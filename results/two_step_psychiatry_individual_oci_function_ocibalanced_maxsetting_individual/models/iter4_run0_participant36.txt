Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior in the two-step task.

### Model 1: OCI-Modulated Structural Learning
This model hypothesizes that OCI symptoms affect how quickly a participant updates their internal model of the spaceship-planet transition probabilities. While the "standard" transition probabilities are fixed (0.7/0.3), high OCI participants might be hyper-sensitive to rare transitions (over-updating) or rigid in their beliefs (under-updating). This "Structural Learning Rate" is modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    RL model with OCI-modulated Structural Learning.
    
    Hypothesis: High OCI participants might obsess over the transition structure,
    updating their internal model of the spaceship-planet transitions dynamically
    based on observed outcomes, rather than relying on a fixed prior.
    
    Parameters:
    - learning_rate: [0, 1] Reward learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - lambda_coeff: [0, 1] Eligibility trace decay.
    - lr_struct_base: [0, 1] Base learning rate for transition matrix.
    - lr_struct_oci: [-1, 1] OCI modulation of structure learning rate.
    """
    learning_rate, beta, w, lambda_coeff, lr_struct_base, lr_struct_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate structure learning rate, bounded [0, 1]
    lr_struct = np.clip(lr_struct_base + lr_struct_oci * oci_score, 0, 1)

    # Initialize transition matrix with instructed probabilities
    # Rows: Action (0 or 1), Cols: State (0 or 1)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Policy (Model-Based uses dynamic trans_probs)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        logits = logits - np.max(logits)
        probs_1 = np.exp(logits) / np.sum(np.exp(logits))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # RL Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Structural Learning Update
        # Update the transition probability for the chosen spaceship towards the observed planet
        trans_probs[a1, s_idx] += lr_struct * (1.0 - trans_probs[a1, s_idx])
        trans_probs[a1, 1-s_idx] = 1.0 - trans_probs[a1, s_idx]
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Counterfactual Rumination
This model hypothesizes that high OCI leads to increased "rumination" or mental simulation of unchosen options. After each trial, the participant updates the Model-Free value of the *unchosen* spaceship towards its Model-Based estimate. This mechanism aligns habits with goal-directed values over time, potentially explaining the rigid "stickiness" or rapid switching observed in the data.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    RL model with OCI-modulated Counterfactual Rumination.
    
    Hypothesis: High OCI participants engage in "mental simulation" or rumination,
    updating the value of the UNCHOSEN spaceship towards its Model-Based estimate.
    This aligns MF values with MB structure over time.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_coeff: [0, 1]
    - alpha_sim_base: [0, 1] Base rate of counterfactual update.
    - alpha_sim_oci: [-1, 1] Modulation by OCI.
    """
    learning_rate, beta, w, lambda_coeff, alpha_sim_base, alpha_sim_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Rate at which unchosen options are updated towards MB values
    alpha_sim = np.clip(alpha_sim_base + alpha_sim_oci * oci_score, 0, 1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # MB Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Choice 1
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta * q_net
        logits = logits - np.max(logits)
        probs_1 = np.exp(logits) / np.sum(np.exp(logits))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # Choice 2
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Standard RL Updates for chosen path
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Counterfactual Update for the unchosen action
        unchosen_a1 = 1 - a1
        # The unchosen action's MF value is pulled towards its current MB value
        q_stage1_mf[unchosen_a1] += alpha_sim * (q_stage1_mb[unchosen_a1] - q_stage1_mf[unchosen_a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Subjective Beliefs
This model proposes that OCI distorts the participant's prior belief about the reliability of the spaceships. Instead of using the objective 0.7 probability, high OCI participants might view the world in "black and white" (probability ~1.0) or as highly unpredictable (probability ~0.5). This subjective probability is used for all Model-Based calculations.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    RL model with OCI-modulated Subjective Transition Beliefs.
    
    Hypothesis: OCI affects the participant's belief about the reliability of the spaceships.
    High OCI might lead to "black and white" thinking (believing probability is 1.0) 
    or distrust (believing probability is 0.5), distorting MB calculations.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_coeff: [0, 1]
    - p_common_base: [0, 1] Base subjective probability of common transition.
    - p_common_oci: [-1, 1] Modulation by OCI.
    """
    learning_rate, beta, w, lambda_coeff, p_common_base, p_common_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective probability of the 'Common' transition
    p_common = np.clip(p_common_base + p_common_oci * oci_score, 0, 1)
    
    # Construct subjective transition matrix
    # Assumes symmetric structure: A->X and U->Y are 'common' with prob p_common
    subj_trans_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # MB Calculation using Subjective Matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subj_trans_matrix @ max_q_stage2
        
        # Choice 1
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta * q_net
        logits = logits - np.max(logits)
        probs_1 = np.exp(logits) / np.sum(np.exp(logits))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # Choice 2
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # RL Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```