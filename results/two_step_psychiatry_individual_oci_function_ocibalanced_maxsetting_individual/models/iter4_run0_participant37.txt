Here are three new cognitive models that explore different mechanisms for how obsessive-compulsive traits (OCI) might influence decision-making in this task.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance
This model hypothesizes that higher OCI scores lead to a reliance on habitual (Model-Free) control over goal-directed (Model-Based) planning. This aligns with psychiatric theories suggesting compulsivity is linked to excessive habit formation. The mixing weight `w` (which balances Model-Based vs. Model-Free) is dynamically adjusted by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Balance.
    Hypothesis: High OCI reduces the weight of Model-Based (planning) value, pushing the 
    agent towards Model-Free (habitual) strategies.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] (Baseline Model-Based weight)
    oci_suppression: [0, 1] (How much OCI suppresses Model-Based control)
    """
    learning_rate, beta, w_base, oci_suppression = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w based on OCI. 
    # If oci_suppression is high, high OCI reduces w_eff towards 0 (pure Model-Free).
    w_eff = w_base * (1.0 - (oci_score * oci_suppression))
    w_eff = np.clip(w_eff, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Data cleaning for indices
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates
This model posits that individuals with high OCI might be hyper-sensitive to negative feedback (punishment/lack of reward) or "missed" rewards, leading to different learning rates for positive vs. negative prediction errors. The OCI score modulates the ratio between learning from positive outcomes and learning from negative outcomes.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Asymmetric Learning Rates.
    Hypothesis: High OCI creates an imbalance in learning from positive vs negative prediction errors.
    Specifically, OCI scales the learning rate for negative prediction errors (disappointments),
    making the agent quicker to abandon options that fail.
    
    Bounds:
    alpha_pos: [0, 1] (Learning rate for positive PE)
    beta: [0, 10]
    w: [0, 1]
    oci_neg_scale: [0, 5] (Multiplier for alpha when PE is negative, scaled by OCI)
    """
    alpha_pos, beta, w, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate based on OCI
    # If oci_neg_scale is > 1, high OCI makes alpha_neg larger than alpha_pos
    alpha_neg = alpha_pos * (1.0 + oci_score * oci_neg_scale)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Choice Determinism (Inverse Temperature)
This model tests the hypothesis that high OCI leads to more rigid, deterministic behavior regardless of the specific values (Model-Based or Model-Free). Instead of affecting *what* is learned, OCI affects *how* choices are made by scaling the softmax beta parameter. A higher beta means the agent exploits small differences in value more aggressively (less exploration).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Choice Determinism.
    Hypothesis: High OCI leads to 'rigid' decision making. The OCI score increases the 
    inverse temperature (beta), making the softmax function sharper. This results in 
    less exploration and more deterministic exploitation of current value estimates.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10] (Baseline inverse temperature)
    w: [0, 1]
    oci_rigidity: [0, 5] (How much OCI increases beta)
    """
    learning_rate, beta_base, w, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective beta increases with OCI
    beta_eff = beta_base * (1.0 + oci_score * oci_rigidity)
    # Cap beta to prevent numerical overflow, though 10 is usually safe in bounds
    beta_eff = np.clip(beta_eff, 0.0, 20.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Use beta_eff here
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Use beta_eff here as well
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```