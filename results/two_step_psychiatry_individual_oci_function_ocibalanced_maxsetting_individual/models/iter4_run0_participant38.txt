Here are 3 new cognitive models that incorporate the OCI score into the decision-making process in novel ways, distinct from the previous attempts.

### Model 1: Hybrid Learner with OCI-modulated Transition Learning
This model hypothesizes that high OCI scores might affect how quickly participants update their internal model of the environment (the transition matrix). While the standard task assumes fixed transition probabilities (70/30), participants often learn these. A high OCI individual might be more rigid or hyper-sensitive to transition updates. Here, OCI modulates the learning rate of the transition matrix itself, affecting the Model-Based calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner where the transition matrix is learned dynamically.
    The OCI score modulates the learning rate of the state-transition model.
    High OCI may lead to over-updating or rigidity in transition estimates.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values (MF).
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting between MB (1) and MF (0) systems.
    trans_lr_base: [0,1] - Base learning rate for the transition matrix.
    oci_trans_mod: [0,5] - How much OCI scales the transition learning rate.
    """
    learning_rate, beta, w, trans_lr_base, oci_trans_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Dynamic transition matrix, initialized to uniform prior
    # Row: spaceship (0 or 1), Col: planet (0 or 1)
    # T[s, s'] = P(s'|s)
    transition_counts = np.ones((2, 2)) 
    
    # Effective transition learning rate scaled by OCI
    # A high OCI might make someone more sensitive to transition surprises (higher LR)
    # or more stuck in their ways. We model it as a boost to the base rate.
    trans_lr = trans_lr_base * (1.0 + oci_score * oci_trans_mod)
    trans_lr = np.clip(trans_lr, 0, 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Calculate transition matrix from counts (normalized)
        row_sums = transition_counts.sum(axis=1, keepdims=True)
        T = transition_counts / row_sums

        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T @ max_q_stage2

        # Hybrid Value
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # 1. Update MF Q-values (Stage 2)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # 2. Update MF Q-values (Stage 1) - using SARSA-like update or TD(0)
        # Standard task analysis often uses Q(s2) for the update target
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # 3. Update Transition Matrix (Model-Based Learning)
        # We use a simple delta rule for probabilities instead of raw counting to allow forgetting
        # T_new(s'|s) = T_old(s'|s) + alpha * (1 - T_old(s'|s)) if transition occurred
        # T_new(other|s) = T_old(other|s) + alpha * (0 - T_old(other|s))
        
        # Current probability of the observed transition
        # We update the row corresponding to action a1
        # Target vector is one-hot for the observed state s_idx
        target = np.zeros(2)
        target[s_idx] = 1.0
        
        # Update row a1
        transition_counts[a1] = transition_counts[a1] + trans_lr * (target - transition_counts[a1])
        # Ensure positivity (though the math above keeps it valid if initialized valid)
        transition_counts[a1] = np.maximum(transition_counts[a1], 0.001)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration with OCI-modulated Decaying Stickiness
This model focuses on "stickiness" (repeating the previous choice), but unlike simple stickiness, this model implements a decaying trace of past choices. The OCI score modulates the *decay rate* of this stickiness trace. The hypothesis is that high OCI individuals might have a harder time "letting go" of past actions, resulting in a slower decay of the choice trace (longer influence of past history).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with an exponentially decaying choice trace (stickiness).
    The decay rate of the choice trace is modulated by OCI.
    High OCI -> Slower decay (memory of past choices persists longer).

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    stickiness_weight: [0,5] - Impact of the choice trace on current decision.
    decay_base: [0,1] - Base decay rate for the choice trace (0=instant decay, 1=no decay).
    oci_decay_mod: [0,1] - Modulates decay based on OCI (reduces decay).
    """
    learning_rate, beta, stickiness_weight, decay_base, oci_decay_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Adjust decay rate: Higher OCI makes decay closer to 1 (slower forgetting of action)
    # decay = base + (1 - base) * oci * mod
    decay_rate = decay_base + (1.0 - decay_base) * oci_score * oci_decay_mod
    decay_rate = np.clip(decay_rate, 0.0, 0.99) # limit to avoid runaway

    # Choice trace for Stage 1 actions (A vs B)
    choice_trace = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Add stickiness to Q-values
        q_net = q_stage1_mf + stickiness_weight * choice_trace

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Update Choice Trace
        # Trace decays, then increments for the chosen action
        choice_trace *= decay_rate
        choice_trace[a1] += 1.0

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates with OCI-driven Punishment Sensitivity
This model implements separate learning rates for positive and negative prediction errors. The OCI score specifically amplifies the learning rate for negative prediction errors (punishment/lack of reward). This aligns with theories that compulsive behavior may be driven by an excessive avoidance of negative outcomes or a hyper-responsiveness to failure signals.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative RPEs.
    OCI score amplifies the learning rate for negative prediction errors (alpha_neg).

    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0,1] - Base learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature.
    oci_neg_amp: [0,5] - Amplification factor for alpha_neg based on OCI.
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # High OCI -> Higher alpha_neg
    alpha_neg = alpha_neg_base * (1.0 + oci_score * oci_neg_amp)
    alpha_neg = np.clip(alpha_neg, 0, 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg * delta_stage2

        # Stage 1 Update
        # Using the updated Q2 value as the target (standard in some 2-step implementations)
        # or simply Q2(s, a2) before update. Here we use Q2(s, a2) AFTER update for stronger coupling
        # but let's stick to the standard TD error form: value of state reached.
        
        # Value of state reached is usually max(Q(s_idx, :)) or Q(s_idx, a2)
        # We use Q(s_idx, a2) as the proxy for V(s_idx)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```