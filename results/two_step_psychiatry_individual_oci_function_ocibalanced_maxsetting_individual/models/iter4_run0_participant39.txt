Here are the 3 proposed cognitive models.

### Model 1: OCI-Modulated Learning Rate and Weight
This model hypothesizes that OCI symptoms simultaneously affect the speed of value updating (learning rate) and the balance between goal-directed and habitual control (w). High OCI might lead to both rigid habits (low w) and altered learning dynamics.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Learning Rate and MB/MF Weight.
    
    Hypothesis:
    OCI symptoms affect both the speed of value updating (learning rate) and the reliance on 
    goal-directed vs habitual strategies (w). High OCI might correlate with rigid habits (low w)
    and altered learning speeds.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate.
    lr_oci: [-1, 1] - OCI modulation of learning rate.
    beta: [0, 10] - Inverse temperature (decision noise).
    w_base: [0, 1] - Base mixing weight (0=MF, 1=MB).
    w_oci: [-1, 1] - OCI modulation of w.
    stickiness: [-5, 5] - Choice perseveration bonus.
    """
    lr_base, lr_oci, beta, w_base, w_oci, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate parameters by OCI
    lr = lr_base + lr_oci * oci_score
    lr = np.clip(lr, 0.0, 1.0)
    
    w = w_base + w_oci * oci_score
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid MB/MF value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Action value updating
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 (with eligibility trace lambda=1 implicitly)
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        
        # Update Stage 2
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stickiness and Stage 2 Beta
This model proposes that OCI symptoms manifest as increased perseveration (stickiness) and altered decision noise specifically in the concrete outcome stage (Stage 2), reflecting a dissociation between abstract planning and concrete reward consumption.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stickiness and Stage 2 Beta.
    
    Hypothesis:
    OCI symptoms manifest as increased perseveration (stickiness) and altered decision noise 
    specifically in the concrete outcome stage (Stage 2), reflecting a dissociation between 
    abstract and concrete decision making.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    stickiness_base: [-5, 5] - Base perseveration.
    stickiness_oci: [-5, 5] - OCI modulation of stickiness.
    beta_stage1: [0, 10] - Inverse temperature for Stage 1.
    beta_stage2_base: [0, 10] - Base inverse temperature for Stage 2.
    beta_stage2_oci: [-5, 5] - OCI modulation of Stage 2 beta.
    """
    learning_rate, w, stickiness_base, stickiness_oci, beta_stage1, beta_stage2_base, beta_stage2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate parameters by OCI
    stickiness = stickiness_base + stickiness_oci * oci_score
    
    beta_stage2 = beta_stage2_base + beta_stage2_oci * oci_score
    beta_stage2 = np.clip(beta_stage2, 0.0, 10.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1

        # Policy for the second choice (using modulated beta_stage2)
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Action value updating
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate and Stickiness
This model hypothesizes that OCI affects the adaptability of the agent (learning rate) and the tendency to repeat choices (stickiness), without necessarily changing the structural balance between Model-Based and Model-Free systems (w is fixed).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Learning Rate and Stickiness.
    
    Hypothesis:
    OCI affects the adaptability of the agent (learning rate) and the tendency to repeat choices 
    regardless of value (stickiness), without necessarily changing the MB/MF balance structure itself.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate.
    lr_oci: [-1, 1] - OCI modulation of learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    stickiness_base: [-5, 5] - Base perseveration.
    stickiness_oci: [-5, 5] - OCI modulation of stickiness.
    """
    lr_base, lr_oci, beta, w, stickiness_base, stickiness_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate parameters by OCI
    lr = lr_base + lr_oci * oci_score
    lr = np.clip(lr, 0.0, 1.0)
    
    stickiness = stickiness_base + stickiness_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Action value updating
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```