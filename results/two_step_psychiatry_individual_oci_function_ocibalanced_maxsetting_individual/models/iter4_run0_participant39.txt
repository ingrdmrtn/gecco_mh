Here are three cognitive models expressed as Python functions that incorporate the participant's OCI score into the decision-making process.

### Model 1: Hybrid RL with OCI-Modulated Stage 2 Precision
This model hypothesizes that high OCI scores correlate with increased rigidity (higher inverse temperature, $\beta$) specifically in the second stage of the task (choosing the alien), while the first stage (planning) remains less affected. This creates a dissociation between planning uncertainty and execution rigidity.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-Modulated Stage 2 Precision.
    
    Hypothesis: OCI affects the precision (inverse temperature) of the second-stage 
    choice (alien selection) differently than the first-stage choice. High OCI 
    participants may be more rigid/deterministic in their direct interactions (Stage 2).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta_base: [0, 10] Baseline inverse temperature (used for Stage 1).
    - beta_2_oci_sens: [-5, 5] Sensitivity of Stage 2 beta to OCI. 
      beta_stage2 = beta_base + (beta_2_oci_sens * oci).
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0) values.
    """
    learning_rate, beta_base, beta_2_oci_sens, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate stage-specific betas
    beta_1 = beta_base
    # Beta 2 is modulated by OCI. We clip to ensure it stays within reasonable positive bounds.
    beta_2 = np.clip(beta_base + (beta_2_oci_sens * participant_oci), 0.0, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax with Beta 1
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Choice ---
        # Softmax with Beta 2 (OCI-modulated)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Prediction errors
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anhedonic Learning (OCI-Dampened Positive Learning)
This model posits that high OCI scores are associated with a reduced sensitivity to positive rewards (anhedonia or dampening), while learning from negative outcomes (failures) remains baseline. This asymmetry causes values to decrease rapidly after failures but increase slowly after successes.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Anhedonic Learning (OCI-Dampened Positive Learning).
    
    Hypothesis: High OCI participants exhibit dampened learning from positive rewards 
    (anhedonia), while learning from lack of reward (0) remains normal.
    This creates an asymmetry where values degrade faster than they build.
    
    Parameters:
    - alpha_base: [0, 1] Baseline learning rate (used for negative prediction errors).
    - alpha_pos_oci_damp: [0, 1] Factor by which OCI dampens the learning rate for positive errors.
      alpha_pos = alpha_base * (1 - (alpha_pos_oci_damp * oci)).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for MB vs MF.
    """
    alpha_base, alpha_pos_oci_damp, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective learning rates
    alpha_neg = alpha_base
    # Dampen positive learning rate based on OCI
    dampening_factor = np.clip(alpha_pos_oci_damp * participant_oci, 0.0, 0.99)
    alpha_pos = alpha_base * (1.0 - dampening_factor)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Determine learning rate based on prediction error sign or reward type
        # Here we use reward type for simplicity (Reward 1 vs Reward 0)
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        lr_s2 = alpha_pos if r > 0 else alpha_neg
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # For stage 1, we use the same effective LR logic based on the final outcome
        lr_s1 = alpha_pos if r > 0 else alpha_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Cumulative Stickiness with OCI-Dependent Decay
This model treats "stickiness" (perseveration) not as a simple 1-back bonus, but as a cumulative memory trace that decays over time. The OCI score determines the *persistence* of this trace: high OCI leads to slower decay, causing choices to become "stuck" for longer periods.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Cumulative Stickiness with OCI-Dependent Decay.
    
    Hypothesis: Perseveration is driven by a choice trace that accumulates with 
    repetition and decays over time. High OCI slows down the decay of this trace, 
    making obsessions/compulsions linger longer.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_weight: [0, 5] Weight of the stickiness trace in the decision.
    - decay_oci_factor: [0, 1] Determines how OCI affects trace decay.
      decay_rate = decay_oci_factor * oci. (Higher OCI -> Higher decay param -> Slower forgetting).
    - w: [0, 1] Mixing weight.
    """
    learning_rate, beta, stick_weight, decay_oci_factor, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Decay rate for the trace. 
    # If decay_rate is 0, trace resets instantly (no memory).
    # If decay_rate is 1, trace never decays (infinite memory).
    decay_rate = np.clip(decay_oci_factor * participant_oci, 0.0, 0.95)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for Stage 1 options
    choice_trace = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF, MB, and Stickiness Trace
        # Note: Trace is added to the net value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_weight * choice_trace
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update Trace
        # Decay existing trace
        choice_trace *= decay_rate
        # Add to chosen action
        choice_trace[a1] += 1.0
        
        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```