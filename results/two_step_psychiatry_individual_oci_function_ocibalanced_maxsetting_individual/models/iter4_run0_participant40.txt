Here are three new cognitive models designed to capture the behavior of a participant with high OCI symptoms in a two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Control
This model hypothesizes that high OCI scores correlate with a rigid reliance on "habitual" (model-free) control, reducing the influence of goal-directed (model-based) planning. The parameter `w` (mixing weight) determines the balance between model-based and model-free values, and this balance is directly modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight is modulated by OCI.
    
    Hypothesis: High OCI participants show reduced model-based control (goal-directedness)
    and increased reliance on model-free (habitual) control.
    w = w_base * (1 - oci_impact * oci)
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1] - Baseline weight for model-based control (0=MF, 1=MB)
    oci_impact: [0,1] - How much OCI reduces the model-based weight
    """
    learning_rate, beta, w_base, oci_impact = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Ensure inputs are integers
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    # Fixed transition matrix (approximate knowledge of task structure)
    # Row 0: Spaceship A -> 70% Planet X (0), 30% Planet Y (1)
    # Row 1: Spaceship B -> 30% Planet X (0), 70% Planet Y (1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate effective mixing weight w
    # If oci_impact is high and oci_score is high, w reduces towards 0 (pure MF)
    w = w_base * (1.0 - oci_impact * oci_score)
    w = np.clip(w, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of stage 2 weighted by transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1) # stability
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 2 Update (TD)
        # Q2(s, a2) += alpha * (r - Q2(s, a2))
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        # Q1(a1) += alpha * (Q2(s, a2) - Q1(a1))
        # Note: Using Q2 value of chosen action as proxy for state value (SARSA-like logic common in 2-step)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates
This model suggests that high OCI leads to an asymmetry in how positive and negative prediction errors are processed. Specifically, it tests if OCI amplifies learning from punishment (or lack of reward) relative to reward, or vice versa. The learning rate is split into positive and negative components, with the negative learning rate scaled by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: High OCI participants may over-learn from negative outcomes (zero reward)
    or positive outcomes.
    alpha_neg = alpha_base_neg * (1 + oci_scale * oci)
    
    Bounds:
    alpha_pos: [0,1] - Learning rate for positive prediction errors
    alpha_base_neg: [0,1] - Base learning rate for negative prediction errors
    beta: [0,10]
    oci_scale: [0, 5] - Scaling factor for OCI effect on negative learning
    """
    alpha_pos, alpha_base_neg, beta, oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate effective negative learning rate
    alpha_neg = alpha_base_neg * (1.0 + oci_scale * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Stage 2 Update
        pe_2 = r - q_stage2[state_idx, a2]
        if pe_2 >= 0:
            q_stage2[state_idx, a2] += alpha_pos * pe_2
        else:
            q_stage2[state_idx, a2] += alpha_neg * pe_2
            
        # Stage 1 Update
        # Using the updated Q2 value to drive Stage 1 update
        pe_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        if pe_1 >= 0:
            q_stage1[a1] += alpha_pos * pe_1
        else:
            q_stage1[a1] += alpha_neg * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature (Decision Noise)
This model posits that OCI affects the exploration-exploitation trade-off. High OCI might lead to "hyper-exploitation" (very high beta, rigid choices) or high uncertainty/anxiety (very low beta, random choices). Here, the inverse temperature `beta` is a function of the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner where the inverse temperature (beta) is modulated by OCI.
    
    Hypothesis: OCI affects decision noise/rigidity. 
    beta = beta_base + (beta_oci_slope * oci)
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10] - Baseline inverse temperature
    beta_oci_slope: [-5, 5] - How OCI changes beta (can increase rigidity or noise)
    w_mb: [0,1] - Fixed weight for model-based control
    """
    learning_rate, beta_base, beta_oci_slope, w_mb = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate effective beta
    beta = beta_base + (beta_oci_slope * oci_score)
    # Beta must be non-negative
    beta = np.maximum(beta, 0.0)
    # Cap beta to prevent overflow issues
    beta = np.minimum(beta, 20.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```