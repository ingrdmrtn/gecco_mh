Here are three new cognitive models that explain the participant's behavior, incorporating their OCI score to account for obsessive-compulsive traits like perseveration and rigidity.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Nonlinear Accumulated Stickiness Model.
    The influence of the stickiness trace is non-linear, governed by a power law.
    OCI modulates the exponent (power) of the stickiness trace.
    High OCI leads to a higher power, creating a 'lock-in' effect where strong habits 
    become disproportionately hard to break, explaining long streaks of repetition.

    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0 = pure MF, 1 = pure MB).
    - stick_weight: [0, 5] Base weight of the stickiness trace.
    - stick_decay: [0, 1] Decay rate of the choice trace.
    - stick_power_base: [0, 5] Base exponent for the trace power law.
    - stick_power_oci: [-2, 2] Effect of OCI on the exponent.
    """
    learning_rate, beta, w, stick_weight, stick_decay, stick_power_base, stick_power_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate power exponent
    power = stick_power_base + stick_power_oci * current_oci
    # Bound power to prevent overflow or invalid values
    if power < 0.1: power = 0.1
    if power > 5.0: power = 5.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Nonlinear stickiness: Stickiness = weight * (trace ^ power)
        stick_term = stick_weight * np.power(choice_trace, power)
        
        logits = beta * q_net + stick_term
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update trace
        choice_trace *= stick_decay
        choice_trace[int(action_1[trial])] += 1.0
        
        # Stage 2 Policy
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Learning
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 1 Learning Rate Model.
    OCI modulates the learning rate specifically for the first stage (spaceship choice).
    High OCI reduces the Stage 1 learning rate, leading to 'frozen' preferences 
    that are insensitive to prediction errors (compulsive rigidity).
    Stage 2 learning is independent, allowing for goal-directed behavior within the selected path.
    
    Parameters:
    - lr_1_base: [0, 1] Base learning rate for Stage 1.
    - lr_1_oci_damp: [0, 10] Damping factor of OCI on LR1. LR1 = Base * exp(-damp * OCI).
    - lr_2: [0, 1] Learning rate for Stage 2.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Simple 1-step choice stickiness.
    - lambda_eligibility: [0, 1] Eligibility trace for MF updates.
    """
    lr_1_base, lr_1_oci_damp, lr_2, beta, w, stickiness, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 1 Learning Rate: High OCI -> Lower Learning Rate
    lr_1 = lr_1_base * np.exp(-lr_1_oci_damp * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        logits = beta * q_net + stick_vec
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        prev_a1 = int(action_1[trial])
        
        # Stage 2
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Learning
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 update (using lambda)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * delta_stage1 + lr_1 * lambda_eligibility * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Habit Modulation Model.
    OCI increases both the Stickiness (motor perseveration) and the Eligibility Trace (lambda).
    High lambda implies stronger Model-Free reinforcement from outcomes directly to Stage 1 choices.
    This models compulsivity as a combination of repetition bias and strong habit formation via direct reinforcement.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick_base: [0, 5] Baseline stickiness.
    - stick_oci: [0, 5] Increase in stickiness per unit OCI.
    - lambda_base: [0, 1] Baseline eligibility trace.
    - lambda_oci: [-1, 1] Change in lambda per unit OCI (bounded [0,1]).
    """
    learning_rate, beta, w, stick_base, stick_oci, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # OCI modulates both mechanisms of habit
    stickiness = stick_base + stick_oci * current_oci
    lambda_val = lambda_base + lambda_oci * current_oci
    
    # Bound lambda
    if lambda_val < 0.0: lambda_val = 0.0
    if lambda_val > 1.0: lambda_val = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        logits = beta * q_net + stick_vec
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        prev_a1 = int(action_1[trial])
        
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```