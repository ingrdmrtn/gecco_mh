Here are three new cognitive models that incorporate the participant's OCI score to explain their decision-making behavior.

### Model 1: OCI-Modulated Memory Decay (Value Hoarding)
This model hypothesizes that OCI symptoms are related to "mental hoarding" or an inability to let go of past associations. In standard reinforcement learning, unchosen options might not change, or they might passively decay. Here, we propose that high OCI leads to **reduced forgetting** of unchosen option values. While a baseline participant might let unchosen Q-values decay towards zero (forgetting), a high-OCI participant retains these values rigidly.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Memory Decay Model.
    
    Hypothesis: High OCI participants exhibit reduced forgetting (value hoarding).
    Unchosen Q-values decay at a rate that decreases as OCI increases.
    
    Parameters:
    - learning_rate: [0,1] Update rate for chosen options.
    - beta: [0,10] Inverse temperature for both stages.
    - w: [0,1] Model-based weight.
    - decay_base: [0,1] Base decay rate for unchosen options.
    - decay_retention: [0,1] OCI modulation. High value means OCI reduces decay (increases retention).
                     decay_rate = decay_base * (1 - decay_retention * oci).
    """
    learning_rate, beta, w, decay_base, decay_retention = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate based on OCI
    # If oci is high and retention is high, decay becomes small (memory is sticky/hoarded)
    decay_rate = decay_base * (1.0 - decay_retention * oci_score)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # 1. Decay unchosen Stage 1 Q-values
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # 2. Standard TD Updates
        # Stage 1 update (TD(1)-like for MB/MF hybrid usually, here simplified SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update for Stage 1 based on Stage 2 outcome
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Weighted Hybrid-Heuristic Model
This model proposes that high OCI participants rely on a rigid **Win-Stay, Lose-Switch (WSLS)** heuristic alongside the complex reinforcement learning calculation. The participant's data shows strong streaks of repeating successful choices and switching immediately upon failure, which is characteristic of this heuristic. The model mixes the probability distribution from the Hybrid RL agent with a deterministic WSLS distribution, where the mixing weight is determined by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Weighted Hybrid-Heuristic Model.
    
    Hypothesis: High OCI participants rely partly on a rigid Win-Stay Lose-Switch (WSLS) 
    heuristic for the first-stage choice, overriding value-based calculations.
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w: [0,1]
    - wsls_mix_k: [0,1] Scaling factor for OCI. The weight of the heuristic is 
                  rho = wsls_mix_k * oci.
    """
    learning_rate, beta, w, wsls_mix_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Heuristic weight
    rho = wsls_mix_k * oci_score
    rho = np.clip(rho, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous trial info for WSLS
    prev_a1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # 1. Calculate Hybrid RL Probabilities
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_rl = exp_q1 / np.sum(exp_q1)
        
        # 2. Calculate WSLS Probabilities
        probs_wsls = np.zeros(2)
        if prev_a1 != -1:
            if prev_reward == 1.0:
                # Win-Stay
                probs_wsls[prev_a1] = 1.0
            else:
                # Lose-Switch (assumes reward 0 is a loss)
                probs_wsls[1 - prev_a1] = 1.0
        else:
            # First trial fallback to uniform
            probs_wsls = np.array([0.5, 0.5])
            
        # 3. Mix
        probs_final = (1 - rho) * probs_rl + rho * probs_wsls
        
        # Safety for log
        probs_final = np.clip(probs_final, 1e-10, 1.0) 
        p_choice_1[trial] = probs_final[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        
        # --- Stage 2 Decision (Standard Softmax) ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        # Update history
        prev_a1 = a1
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Distorted Transition Belief Model
This model posits that high OCI is associated with "black-and-white thinking" or an intolerance of uncertainty. While the true transition probabilities are 0.7/0.3, a high-OCI participant may subjectively perceive these transitions as more deterministic (e.g., 0.9/0.1) or, conversely, more chaotic. This model modulates the **Model-Based transition matrix** based on the OCI score, distorting the internal model used for planning.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Distorted Transition Belief Model.
    
    Hypothesis: High OCI participants possess a distorted belief about the 
    environmental structure (transition matrix). They may perceive the 
    'common' transition as more deterministic (rigidity) than it really is.
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w: [0,1]
    - distortion_param: [0, 1] Scales the distortion of the transition matrix.
                        p_common = 0.7 + distortion_param * oci.
                        (Clipped to ensure valid probability < 1.0).
    """
    learning_rate, beta, w, distortion_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Distort the transition matrix
    # Base is 0.7. High OCI + positive distortion -> perception of 0.8, 0.9, etc.
    p_common = 0.7 + (distortion_param * oci_score)
    p_common = np.clip(p_common, 0.5, 0.99) # Ensure it stays valid and majority
    p_rare = 1.0 - p_common
    
    # Internal model of transitions
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # The Model-Based value is calculated using the DISTORTED matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```