Here are 3 new cognitive models expressed as Python functions.

### Model 1: OCI-Modulated Lose-Shift Bias
This model hypothesizes that high OCI participants have an increased sensitivity to negative outcomes (omission of reward), leading to a heuristic "Lose-Shift" behavior. While standard reinforcement learning gradually decreases the value of unrewarded actions, this model adds an immediate, transient penalty to the decision value of an action if it yielded no reward in the previous trial. The magnitude of this penalty is scaled by the participant's OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Lose-Shift Bias.
    Hypothesis: High OCI participants exhibit a transient "Lose-Shift" bias.
    If the previous choice resulted in 0 reward, a penalty is applied to that 
    choice's value in the next trial, proportional to OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) systems.
    - ls_strength: [0, 5] Strength of the Lose-Shift penalty scaled by OCI.
    """
    learning_rate, beta, w, ls_strength = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    prev_act1 = -1
    prev_reward = 0.0

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Apply Lose-Shift Penalty
        if trial > 0 and prev_reward == 0.0 and prev_act1 != -1:
            bias = np.zeros(2)
            bias[prev_act1] = -1.0 * ls_strength * oci_score
            q_net += bias

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # Value Updates
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

        prev_act1 = act1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Belief
This model suggests that high OCI participants distort the probabilities in their internal model of the task. Specifically, compulsivity may be associated with a desire for certainty or control, leading participants to perceive the probabilistic spaceship transitions as more deterministic than they actually are. The transition matrix used for Model-Based evaluation is modified such that the "common" transition probability increases towards 1.0 as a function of OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Belief.
    Hypothesis: High OCI participants perceive the task transitions as more 
    deterministic (less random) than they are. They overestimate the 
    common transition probability.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free.
    - trans_distortion: [0, 1] Factor increasing the perceived common transition prob.
      0 = standard (0.7), 1 = max distortion (approaching 1.0).
    """
    learning_rate, beta, w, trans_distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Distort transition matrix based on OCI
    # Base is 0.7. Max distortion pushes it to 1.0. 
    # Formula: p = 0.7 + (0.3 * distortion * OCI)
    p_common = 0.7 + (0.3 * trans_distortion * oci_score)
    if p_common > 0.99: p_common = 0.99
    
    # Internal model used by the participant
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # Value Updates
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Loss Aversion
This model proposes that high OCI is linked to specific "Loss Aversion" or punishment sensitivity. In the standard task, a reward of 0 is neutral. This model posits that high OCI participants subjectively experience the 0 outcome as a negative value (a loss), motivating active avoidance. The learning update uses this subjective negative reward instead of 0, driving Q-values lower than simple extinction would.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Loss Aversion.
    Hypothesis: High OCI participants treat the omission of reward (0 coins) 
    as an active punishment (negative reward).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free.
    - loss_sens: [0, 5] Magnitude of subjective negative reward for 0 outcomes.
    """
    learning_rate, beta, w, loss_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate Subjective Reward
        # If reward is 0, it is perceived as negative proportional to OCI
        r_effective = r
        if r == 0.0:
            r_effective = -1.0 * loss_sens * oci_score

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # Value Updates with Effective Reward
        delta_stage2 = r_effective - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```