Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

### Model 1: OCI-Modulated Loss Sensitivity
This model hypothesizes that high OCI scores are associated with an increased sensitivity to negative outcomes (lack of reward). While the participant learns from positive rewards at a baseline rate, unrewarded trials trigger a stronger error signal or "hyper-correction" proportional to their OCI score.

```python
import numpy as np

def cognitive_model1_oci_loss_sensitivity(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Loss Sensitivity.
    
    Hypothesis: High OCI participants exhibit stronger learning from unrewarded outcomes 
    (loss/punishment) due to heightened error monitoring, anxiety, or perfectionism. 
    Standard RL uses a single learning rate. This model splits the learning rate 
    based on the outcome valence: the learning rate for negative outcomes (0 reward)
    is boosted by the OCI score.
    
    alpha_pos = learning_rate
    alpha_neg = learning_rate * (1 + oci * phi)
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for rewarded trials.
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Weight of Model-Based control.
    - phi: [0, 5] Scaling factor for loss sensitivity based on OCI.
    """
    learning_rate, beta, w, phi = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Define learning rates based on OCI modulation
    alpha_pos = learning_rate
    alpha_neg = learning_rate * (1.0 + oci_score * phi)
    
    # Clamp alpha_neg to [0, 1] to ensure stability
    if alpha_neg > 1.0: alpha_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value (Planning)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Determine effective learning rate based on reward valence
        r = reward[trial]
        current_alpha = alpha_pos if r == 1 else alpha_neg
        
        # Stage 1 MF Update (TD)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_alpha * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_alpha * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Uncertainty
This model proposes that high OCI scores reflect a degradation in the internal model of the environment. Specifically, the participant's confidence in the transition structure (common vs. rare) is eroded by OCI, making the Model-Based component effectively use a "fuzzier" or more random transition matrix, thereby reducing the efficacy of planning.

```python
def cognitive_model2_oci_transition_uncertainty(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Uncertainty.
    
    Hypothesis: High OCI participants suffer from a degradation in their internal model 
    of the environment's structure (the transition probabilities). 
    While the true transition matrix is [[0.7, 0.3], [0.3, 0.7]], high OCI leads to 
    perceiving the transitions as more random (closer to [[0.5, 0.5], ...]).
    
    T_perceived = (1 - d) * T_true + d * T_uniform
    where d = oci * distortion.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - distortion: [0, 1.2] Factor scaling the OCI effect on transition matrix entropy.
    """
    learning_rate, beta, w, distortion = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate Distorted Transition Matrix
    # The distortion factor mixes the true matrix with a uniform (random) matrix
    dist_factor = oci_score * distortion
    if dist_factor > 1.0: dist_factor = 1.0
    
    t_true = np.array([[0.7, 0.3], [0.3, 0.7]])
    t_uniform = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    # Linear interpolation between True and Uniform based on OCI
    t_perceived = (1.0 - dist_factor) * t_true + dist_factor * t_uniform
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value using Perceived (Distorted) Matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = t_perceived @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace
This model tests the hypothesis that high OCI correlates with "habitual stickiness" or the tendency to reinforce the entire action chain based on the final outcome. The eligibility trace parameter ($\lambda$), which controls how much the stage 2 reward directly updates the stage 1 choice, is scaled by the OCI score.

```python
def cognitive_model3_oci_eligibility_trace(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace.
    
    Hypothesis: OCI score correlates with the strength of habit formation or the 
    tendency to link outcomes directly to the initial choice (skipping the model-based structure).
    This is modeled as an eligibility trace (lambda) in the Model-Free update, 
    where the trace decay parameter is scaled by OCI.
    
    lambda = oci * lambda_scale
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - lambda_scale: [0, 1] Scales OCI to determine eligibility trace strength.
    """
    learning_rate, beta, w, lambda_scale = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate Eligibility Trace Lambda based on OCI
    lam = oci_score * lambda_scale
    if lam > 1.0: lam = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Decision ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 MF update (TD)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Eligibility Trace Update for Stage 1 
        # Directly reinforces Stage 1 choice based on Stage 2 RPE, scaled by lambda (OCI)
        q_stage1_mf[action_1[trial]] += learning_rate * lam * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```