Here are the 3 proposed cognitive models.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Lose-Stay Stickiness Model.
    
    Hypothesis: High OCI scores are associated with compulsive repetition (stickiness) 
    specifically following unrewarded trials (Lose-Stay behavior). While general stickiness 
    might exist, OCI exacerbates the inability to switch away from a choice after failure.
    
    Parameters:
    lr: [0, 1] Learning rate for Q-value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Weighting parameter (0=Model-Free, 1=Model-Based).
    stick_base: [0, 5] Baseline choice stickiness applied on every trial.
    oci_loss_stick: [0, 5] Additional stickiness added ONLY if the previous trial was unrewarded, scaled by OCI.
    """
    lr, beta, w, stick_base, oci_loss_stick = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0 
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness
        if prev_a1 != -1:
            current_stickiness = stick_base
            # If previous outcome was a loss (0 reward), increase stickiness based on OCI
            if prev_reward == 0:
                current_stickiness += oci_loss_stick * participant_oci
            
            q_net[prev_a1] += current_stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        r = reward[trial]
        
        # MF Stage 2 Update
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        # MF Stage 1 Update
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        q_stage1_mf[a1] += lr * pe_2
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Cognitive Rigidity Model (OCI-modulated Learning Rate).
    
    Hypothesis: High OCI is associated with cognitive rigidity, modeled here as a 
    reduced learning rate. High OCI participants update their value estimates more 
    slowly, making them less responsive to new information.
    
    Parameters:
    lr_base: [0, 1] Base learning rate.
    oci_lr_penalty: [0, 1] Reduction factor for learning rate based on OCI. Effective LR = lr_base * (1 - oci * penalty).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stickiness: [0, 5] Choice stickiness.
    """
    lr_base, oci_lr_penalty, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective learning rate, ensuring it stays non-negative
    lr = lr_base * (1.0 - (participant_oci * oci_lr_penalty))
    lr = np.maximum(lr, 0.001) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        q_stage1_mf[a1] += lr * pe_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Cumulative Habit Model.
    
    Hypothesis: Compulsivity is better modeled by a slowly decaying memory trace of past choices 
    (cumulative habit) rather than just the last choice (simple stickiness). OCI scales the 
    influence (weight) of this accumulated history on the current decision.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    trace_decay: [0, 1] Decay rate of the choice trace (0 = instant forgetting/simple stickiness, 1 = perfect memory).
    oci_trace_weight: [0, 5] Weight of the cumulative trace in the decision, scaled by OCI.
    """
    lr, beta, w, trace_decay, oci_trace_weight = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace vector for the two spaceships
    choice_trace = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate the effective weight for the trace based on OCI
    trace_w = oci_trace_weight * participant_oci
    
    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add cumulative habit bonus
        q_net += trace_w * choice_trace
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update Choice Trace
        # Decay existing trace
        choice_trace *= trace_decay
        # Reinforce chosen action (add 1 to the chosen action's trace)
        choice_trace[a1] += 1.0
        
        # Stage 2
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        q_stage1_mf[a1] += lr * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```