Here are the 3 proposed cognitive models.

### Model 1: OCI-Enhanced Cumulative Habit
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Enhanced Cumulative Habit Model.
    
    Hypothesis: High OCI participants rely heavily on established habits. 
    Unlike simple stickiness (repeating the last choice), this model tracks a 
    cumulative 'habit strength' (exponential moving average of past choices).
    The influence of this habit on the current decision is amplified by the OCI score.

    Parameters:
    - learning_rate: Rate for value updating and habit formation [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight (0=MF, 1=MB) [0, 1]
    - lambd: Eligibility trace for Stage 1 MF update [0, 1]
    - habit_gain: Scaling factor for the influence of habit strength on choice, modulated by OCI [0, 10]
    """
    learning_rate, beta, w, lambd, habit_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit strength for each Stage 1 action, initialized to neutral
    habit_strength = np.zeros(2) 

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value combines MB, MF, and OCI-weighted Habit
        # The habit bonus is added to the log-odds (Q-values)
        habit_bonus = habit_gain * oci_score * habit_strength
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + habit_bonus
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Decision ---
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        a2 = int(action_2[trial])

        # --- Updates ---
        # 1. Update Habit Strength (Trace of choice frequency)
        # Chosen action strengthens, unchosen decays
        habit_strength[a1] += learning_rate * (1.0 - habit_strength[a1])
        habit_strength[1 - a1] += learning_rate * (0.0 - habit_strength[1 - a1])

        # 2. TD Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

### Model 2: Safety Signal Model (Reward Inflation)
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Safety Signal Model.
    
    Hypothesis: For individuals with high OCI, the absence of a negative outcome 
    (or even a neutral outcome like 0 coins) can generate a "safety signal" or relief,
    which is perceived as reinforcing. This effectively inflates the reward value 
    of non-rewarded trials, preventing the extinction of the chosen behavior (compulsion).
    
    Parameters:
    - learning_rate: Base learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight [0, 1]
    - lambd: Eligibility trace [0, 1]
    - safety_bonus: The magnitude of the relief signal added to 0-rewards, scaled by OCI [0, 1]
    """
    learning_rate, beta, w, lambd, safety_bonus = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Decision ---
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        a2 = int(action_2[trial])

        # --- Reward Transformation ---
        # If reward is 0, add safety bonus based on OCI. If reward is 1, keep as 1.
        # R_eff = R + (1-R) * safety_factor
        r_observed = reward[trial]
        safety_val = safety_bonus * oci_score
        r_effective = r_observed + (1.0 - r_observed) * safety_val

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r_effective - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

### Model 3: OCI-Induced Model-Based Impairment
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Induced Model-Based Impairment Model.
    
    Hypothesis: Compulsivity is characterized by a shift from goal-directed (Model-Based) 
    to habitual (Model-Free) control. This model posits that the weight assigned to 
    the Model-Based system (w) is suppressed by the OCI score.
    
    Parameters:
    - learning_rate: Base learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w_base: Baseline mixing weight for a participant with OCI=0 [0, 1]
    - lambd: Eligibility trace [0, 1]
    - mb_decay: The rate at which OCI suppresses the Model-Based weight [0, 1]
    """
    learning_rate, beta, w_base, lambd, mb_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective w based on OCI
    # w decreases as OCI increases
    w_effective = w_base * (1.0 - mb_decay * oci_score)
    # Ensure w stays non-negative
    if w_effective < 0:
        w_effective = 0.0

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Decision ---
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        a2 = int(action_2[trial])

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss