Here are 3 new cognitive models based on the provided participant data and OCI score. These models explore different mechanisms by which high obsessive-compulsive symptoms (OCI) might influence decision-making, specifically focusing on habit formation, model-based control, and learning rates.

### Model 1: OCI-Modulated Habit Strength (Model-Free Dominance)
This model hypothesizes that individuals with high OCI scores rely more heavily on habit-based (Model-Free) learning rather than goal-directed (Model-Based) planning. High OCI acts as a weight that shifts the balance `w` towards the Model-Free system. Instead of a static `w`, the effective mixing weight is dynamically reduced by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Habit Strength.
    
    This model posits that high OCI scores reduce the influence of the Model-Based (planning)
    system, effectively increasing reliance on the Model-Free (habit) system.
    The mixing weight 'w' (which usually balances MB and MF) is dampened by the OCI score.
    
    Bounds:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline mixing weight for Model-Based control (before OCI adjustment).
    oci_dampening: [0, 1] Strength of OCI's reduction of the Model-Based weight.
    """
    lr, beta, w_base, oci_dampening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective w: Higher OCI reduces w, pushing behavior towards MF (habit).
    # w represents the weight of the Model-Based system.
    w_effective = w_base * (1.0 - (oci_dampening * oci_score))
    # Ensure w stays non-negative
    w_effective = max(0.0, w_effective)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: weighted sum of MB and MF
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 1 update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning Rates (Punishment Sensitivity)
This model explores the idea that high OCI is associated with an increased sensitivity to negative outcomes (or lack of reward) as a form of error correction or "checking." Here, the OCI score scales the learning rate specifically for negative prediction errors (when outcomes are worse than expected), potentially leading to faster abandonment of choices that fail to yield gold.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Asymmetric Learning (Punishment Sensitivity).
    
    This model assumes high OCI participants are hyper-sensitive to "failure" (0 reward).
    The learning rate is split into a base rate and a negative-error multiplier.
    The OCI score amplifies the learning rate specifically when prediction errors are negative.
    
    Bounds:
    lr_base: [0, 1] Base learning rate for positive errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_neg_amp: [0, 5] Multiplier for OCI's effect on the learning rate for negative errors.
    """
    lr_base, beta, w, oci_neg_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Calculate prediction errors
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Determine effective learning rate based on sign of error
        # If error is negative (disappointment), OCI amplifies the learning rate
        if delta_stage2 < 0:
            lr_eff_2 = lr_base * (1.0 + oci_neg_amp * oci_score)
            # Cap at 1.0
            lr_eff_2 = min(1.0, lr_eff_2)
        else:
            lr_eff_2 = lr_base
            
        if delta_stage1 < 0:
            lr_eff_1 = lr_base * (1.0 + oci_neg_amp * oci_score)
            lr_eff_1 = min(1.0, lr_eff_1)
        else:
            lr_eff_1 = lr_base

        q_stage2_mf[s_idx, a2] += lr_eff_2 * delta_stage2
        q_stage1_mf[a1] += lr_eff_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Enhanced Eligibility Traces (Compulsive Credit Assignment)
This model posits that high OCI creates a stronger link between the final outcome and the initial choice, bypassing the two-step structure. This is implemented via an eligibility trace parameter `lambda` that is modulated by OCI. High OCI increases `lambda`, meaning the Stage 1 choice is updated more directly by the Stage 2 reward, reflecting a "compulsive" attribution of the outcome to the initial action regardless of the transition.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Enhanced Eligibility Traces.
    
    This model assumes high OCI leads to 'compulsive credit assignment', where the 
    participant strongly links the final reward to the first-stage choice, blurring the 
    two-step structure. This is modeled by an eligibility trace parameter (lambda) 
    that increases with OCI score.
    
    Bounds:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    lambda_base: [0, 1] Baseline eligibility trace decay.
    oci_lambda_boost: [0, 1] How much OCI increases the eligibility trace (linking stage 1 to reward).
    """
    lr, beta, w, lambda_base, oci_lambda_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective lambda
    # Higher OCI increases lambda, making the update more like TD(1) / Monte Carlo
    # meaning the Stage 1 value is updated directly by the final reward more strongly.
    lambda_eff = lambda_base + (oci_lambda_boost * oci_score)
    lambda_eff = min(1.0, lambda_eff) # Cap at 1.0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning with Eligibility Traces ---
        r = reward[trial]
        
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 Prediction Error (Standard TD(0) part)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Update Stage 1: Standard TD update + Eligibility Trace update from Stage 2 error
        # If lambda is high (high OCI), Stage 1 Q-value is strongly affected by the 
        # final reward prediction error (delta_stage2) directly.
        q_stage1_mf[a1] += lr * (delta_stage1 + lambda_eff * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```