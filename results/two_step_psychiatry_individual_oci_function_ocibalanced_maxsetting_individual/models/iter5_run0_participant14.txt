Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in this two-step task.

### Model 1: Hybrid Model-Based/Model-Free with OCI Modulating the Balance ($w$)
This model hypothesizes that OCI symptoms affect the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it tests if higher OCI scores lead to a stronger reliance on Model-Free (habitual) learning, or conversely, if they drive a more rigid Model-Based calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is modulated by OCI.
    
    The parameter 'w' determines the balance between Model-Based (w=1) and Model-Free (w=0) control.
    Here, we model 'w' as a logistic function of OCI, allowing the balance to shift based on symptom severity.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_intercept: [0, 10] Intercept for the logistic function determining w.
    w_slope: [-5, 5] Slope for the logistic function determining w with respect to OCI.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI
    # Using a sigmoid function to keep w between 0 and 1
    # w_logit = w_intercept + w_slope * oci_score
    # w = 1 / (1 + np.exp(-w_logit))
    # Simplified linear approximation clamped to [0,1] for stability in estimation is often preferred,
    # but let's use the sigmoid logic implicitly or a direct linear map.
    # Let's use a direct linear modulation clamped.
    w_raw = w_intercept + w_slope * oci_score
    w = np.clip(w_raw, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1 (spaceships)
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (aliens)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 2 MF Update (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 MF Update (TD error using stage 2 value)
        # Standard TD(0) update for stage 1 MF value
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Pure Model-Based with OCI-Dependent Forgetting
This model suggests that OCI relates to memory persistence. High OCI might correlate with "sticky" thoughts or inability to update beliefs flexibly, or conversely, anxiety might drive rapid forgetting of old safety signals. Here, we model the *decay rate* of the Q-values as a function of OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner where value decay (forgetting) is modulated by OCI.
    
    Instead of standard Q-learning where values stay constant until visited,
    this model decays unchosen option values towards 0 (or a prior) on every trial.
    The rate of this decay is modulated by OCI.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature.
    decay_base: [0, 1] Base decay rate (1 = no decay, 0 = instant forgetting).
    decay_oci_slope: [-1, 1] How OCI modifies the decay rate.
    """
    learning_rate, beta, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate
    # decay = 1.0 means values persist perfectly. decay < 1.0 means they fade.
    eff_decay = decay_base + decay_oci_slope * oci_score
    eff_decay = np.clip(eff_decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens (Planet X: 0,1; Planet Y: 0,1)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Decay ALL values first (passive forgetting)
        q_stage2_mf *= eff_decay
        
        # Update chosen value (Reinforcement)
        # Note: In a decay model, we often update directly towards reward rather than additive error
        # q_new = q_old + lr * (r - q_old)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Pruning Model with OCI-Modulated Threshold
This model proposes that participants simplify the task by "pruning" (ignoring) transitions that they deem unlikely or values that are low. OCI might relate to a higher need for certainty or control, potentially affecting the threshold at which they prune information.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner that 'prunes' the transition matrix based on an OCI-modulated threshold.
    
    If a transition probability is below the threshold, the agent treats it as 0
    and renormalizes, effectively simplifying the mental model of the task.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    pruning_threshold_base: [0, 0.5] Base probability threshold for pruning.
    pruning_oci_factor: [0, 1] How strongly OCI increases the pruning threshold.
    """
    learning_rate, beta, pruning_threshold_base, pruning_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective pruning threshold
    # Higher OCI -> Higher threshold (more rigid/simplified view, ignoring rare transitions)
    threshold = pruning_threshold_base * (1.0 + pruning_oci_factor * oci_score)
    threshold = np.clip(threshold, 0.0, 0.49) # Cap below 0.5 to avoid pruning everything
    
    # Construct subjective transition matrix
    # True matrix: [[0.7, 0.3], [0.3, 0.7]]
    subjective_tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Apply pruning
    if subjective_tm[0, 1] < threshold:
        subjective_tm[0, 1] = 0.0
        subjective_tm[0, 0] = 1.0 # Renormalize
    if subjective_tm[1, 0] < threshold:
        subjective_tm[1, 0] = 0.0
        subjective_tm[1, 1] = 1.0 # Renormalize
        
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2)) 
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy (Using Subjective Pruned Matrix) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_tm @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```