Here are three new cognitive models that incorporate the OCI score to explain participant behavior in the two-step task.

### Model 1: Stage 2 Learning Rate Modulation
This model hypothesizes that OCI symptoms specifically affect the learning rate for the second stage (aliens), reflecting altered processing of concrete, detailed feedback, while the learning rate for the abstract first stage (spaceships) remains distinct.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 2 Learning Rate Modulation Model.

    Hypothesis: OCI scores correlate with the rate of learning detailed outcome probabilities 
    (Stage 2 aliens), reflecting altered processing of concrete feedback, while 
    Stage 1 (abstract spaceship choice) learning remains constant.

    Parameters:
    - alpha_1: [0, 1] Learning rate for Stage 1 (Spaceships).
    - alpha_2_base: [0, 1] Base learning rate for Stage 2 (Aliens).
    - alpha_2_oci: [-1, 1] OCI modulation for Stage 2 learning rate.
        alpha_2 = alpha_2_base + (alpha_2_oci * oci)
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    alpha_1, alpha_2_base, alpha_2_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate modulated alpha_2
    alpha_2 = alpha_2_base + (alpha_2_oci * current_oci)
    alpha_2 = np.clip(alpha_2, 0.0, 1.0)
    
    # Clip alpha_1 
    alpha_1 = np.clip(alpha_1, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 MF Update using alpha_1
        q_stage1_mf[a1] += alpha_1 * (delta_stage1 + delta_stage2)

        # Stage 2 MF Update using modulated alpha_2
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Rare Transition Learning Rate Modulation
This model hypothesizes that OCI modulates how strongly prediction errors from **rare** transitions update the Stage 1 Model-Free values. High OCI might lead to over-interpreting noise (rare events) or discounting them differently than common events.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Rare Transition Learning Rate Modulation Model.

    Hypothesis: OCI modulates how strongly prediction errors from Rare transitions 
    update the Stage 1 MF values. High OCI might lead to over-learning from noise (rare events)
    or discounting them differently than common events.

    Parameters:
    - alpha_common: [0, 1] Learning rate for common transitions.
    - alpha_rare_base: [0, 1] Base learning rate for rare transitions.
    - alpha_rare_oci: [-1, 1] OCI modulation for rare transition learning rate.
        alpha_rare = alpha_rare_base + (alpha_rare_oci * oci)
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    alpha_common, alpha_rare_base, alpha_rare_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    alpha_rare = alpha_rare_base + (alpha_rare_oci * current_oci)
    alpha_rare = np.clip(alpha_rare, 0.0, 1.0)
    alpha_common = np.clip(alpha_common, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Determine if transition was common or rare
        # Common: 0->0, 1->1. Rare: 0->1, 1->0
        is_common = (a1 == s_idx)
        current_lr = alpha_common if is_common else alpha_rare

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 MF update uses transition-dependent LR
        q_stage1_mf[a1] += current_lr * (delta_stage1 + delta_stage2)

        # Stage 2 MF update uses alpha_common (assumed robust base learning)
        q_stage2_mf[s_idx, a2] += alpha_common * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Transition-Dependent Stickiness
This model hypothesizes that OCI affects the tendency to repeat choices (stickiness) differently depending on whether the previous transition was Common or Rare. For example, high OCI might be associated with rigidity (staying) even after rare transitions that usually prompt switching in model-free learners.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Transition-Dependent Stickiness Model.

    Hypothesis: OCI affects the tendency to repeat choices (stickiness) differently 
    depending on whether the previous transition was Common or Rare. 
    High OCI might show rigidity even after rare transitions (which usually cue switching in MF).

    Parameters:
    - alpha: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stick_common: [0, 5] Stickiness after a common transition.
    - stick_rare_base: [0, 5] Base stickiness after a rare transition.
    - stick_rare_oci: [-5, 5] OCI modulation for rare transition stickiness.
        stick_rare = stick_rare_base + (stick_rare_oci * oci)
    """
    alpha, beta, w, stick_common, stick_rare_base, stick_rare_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    stick_rare = stick_rare_base + (stick_rare_oci * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_transition_common = True # Default for first trial

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            # Apply stickiness based on previous transition type
            current_stick = stick_common if last_transition_common else stick_rare
            q_augmented[last_action_1] += current_stick
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += alpha * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        
        last_action_1 = a1
        # Determine transition type for next trial's stickiness
        last_transition_common = (a1 == s_idx)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```