Here are three cognitive models formulated as Python functions.

### Model 1: Subjective Loss Sensitivity Model
This model tests the hypothesis that high OCI individuals exhibit a hypersensitivity to negative outcomes. Instead of treating a lack of reward (0 coins) as a neutral event, they perceive it as a punishment (negative utility). This drives stronger avoidance learning in the Model-Free system.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Loss Sensitivity Model.
    
    Hypothesis: High OCI individuals perceive the lack of reward (0) as an active negative outcome (loss),
    driving stronger avoidance learning than in low OCI individuals who might treat it as neutral.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Choice stickiness (action repetition).
    - loss_base: [0, 5] Base subjective penalty for 0 reward.
    - loss_oci: [0, 5] OCI-dependent increase in subjective penalty.
    """
    lr, beta, w, stickiness, loss_base, loss_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective loss value
    loss_magnitude = loss_base + loss_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    last_action_1 = -1
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Effective reward transformation
        # If reward is 0, it is treated as -loss_magnitude
        if r == 1:
            r_eff = 1.0
        else:
            r_eff = -loss_magnitude

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r_eff - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1

    return -log_likelihood
```

### Model 2: Cumulative Choice Trace (Habit) Model
This model posits that "compulsive" behavior in high OCI individuals is driven by a stronger form of habit formation. Instead of simple 1-back stickiness, this model uses a "choice trace" that accumulates with repetition and decays slowly. High OCI individuals place more weight on this accumulated history, leading to persistent behavior even in the face of changing rewards.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Cumulative Choice Trace (Habit) Model.
    
    Hypothesis: OCI correlates with stronger habit formation, modeled as a 'choice trace' 
    that accumulates with repetition and decays slowly, rather than just 1-back stickiness.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - trace_decay: [0, 1] Decay rate of the choice trace (0 = instant forgetting, 1 = no decay).
    - trace_w_base: [0, 5] Base weight of the choice trace.
    - trace_w_oci: [0, 5] OCI-dependent increase in trace weight.
    """
    lr, beta, w, trace_decay, trace_w_base, trace_w_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    trace_weight = trace_w_base + trace_w_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    choice_trace = np.zeros(2)
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add cumulative choice trace influence
        q_net += trace_weight * choice_trace
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Update choice trace for the next trial
        choice_trace *= trace_decay
        choice_trace[a1] += 1.0

    return -log_likelihood
```

### Model 3: Planet Stickiness (Goal Perseveration) Model
This model differentiates between motor repetition (repeating the same spaceship) and goal perseveration (repeating the same planet). It hypothesizes that high OCI individuals may compulsively try to return to the location (Planet X or Y) they just visited, regardless of whether they received a reward. This is implemented as a Model-Based bias where the "stickiness" bonus is distributed to actions based on their probability of transitioning to the previous state.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Planet Stickiness (Goal Perseveration) Model.
    
    Hypothesis: High OCI individuals exhibit 'Goal Perseveration', preferring to return to 
    the same planet (state) they just visited, regardless of the reward outcome. This is distinct
    from action stickiness.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stick_action: [0, 5] Standard action stickiness.
    - stick_planet_base: [0, 5] Base bias to return to previous planet.
    - stick_planet_oci: [0, 5] OCI-dependent increase in planet stickiness.
    """
    lr, beta, w, stick_action, stick_planet_base, stick_planet_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    stick_planet = stick_planet_base + stick_planet_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_planet = -1
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Standard Action Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stick_action
            
        # Planet Stickiness (Goal Perseveration)
        if last_planet != -1:
            # Add bonus to actions proportional to probability of reaching last_planet
            # transition_matrix[action, next_state]
            bonus_0 = transition_matrix[0, last_planet] * stick_planet
            bonus_1 = transition_matrix[1, last_planet] * stick_planet
            q_net[0] += bonus_0
            q_net[1] += bonus_1
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1
        last_planet = s_idx

    return -log_likelihood
```