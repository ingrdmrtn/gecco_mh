Here are three new cognitive models that incorporate the OCI score into the learning and decision-making process in different ways, specifically focusing on mechanisms relevant to obsessive-compulsive traits like habit formation, uncertainty avoidance, and altered learning from feedback.

### Cognitive Model 1: OCI-Modulated Habit Strength (Model-Free Dominance)
This model hypothesizes that individuals with higher OCI scores rely more heavily on habit-based (model-free) learning rather than goal-directed (model-based) planning. The parameter `w` (mixing weight) is not static but is dynamically shifted towards 0 (pure model-free) based on the OCI score. Specifically, the effective weight `w_eff` is reduced by the OCI score scaled by a sensitivity parameter.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based/Model-Free Hybrid with OCI-driven Habit Bias.
    
    Hypothesis: Higher OCI scores lead to a stronger reliance on Model-Free (habitual) 
    control over Model-Based (goal-directed) control. The mixing weight 'w' is 
    reduced by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w_base: [0, 1] - Baseline mixing weight (1 = pure MB, 0 = pure MF) before OCI modulation.
    oci_habit_strength: [0, 2] - Strength of OCI in reducing 'w' (pushing towards MF).
    """
    learning_rate, beta, w_base, oci_habit_strength = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight: OCI reduces the MB contribution
    # We clip to ensure it stays valid [0, 1]
    w_eff = w_base - (oci_score * oci_habit_strength)
    if w_eff < 0: w_eff = 0.0
    if w_eff > 1: w_eff = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based valuation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mixed valuation using OCI-adjusted weight
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s1_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s1_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # TD(0) updates
        # Stage 1 MF update
        delta_stage1 = q_stage2_mf[s1_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s1_idx, a2]
        q_stage2_mf[s1_idx, a2] = q_stage2_mf[s1_idx, a2] + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Enhanced Punishment Sensitivity (Loss Aversion)
This model posits that OCI traits correlate with an increased sensitivity to negative outcomes (or lack of reward). Instead of a single learning rate, the model uses separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The OCI score specifically amplifies the learning rate for negative prediction errors, making the agent learn faster from failures/zeros, reflecting a "fear of failure" or perfectionism often seen in OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning with OCI-driven Negative Sensitivity.
    
    Hypothesis: OCI is associated with hyper-sensitivity to errors/punishment.
    The learning rate for negative prediction errors (alpha_neg) is boosted 
    by the OCI score, while positive learning is baseline.
    
    Parameters:
    alpha_base: [0, 1] - Base learning rate for positive prediction errors.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    oci_punish_mult: [0, 5] - Multiplier for OCI to boost learning from negative PEs.
    """
    alpha_base, beta, w, oci_punish_mult = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s1_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s1_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s1_idx, a2]
        
        # Determine effective alpha based on sign of PE
        if delta_stage2 >= 0:
            eff_alpha_2 = alpha_base
        else:
            # Negative PE: Learning is boosted by OCI
            # We constrain the boosted alpha to max 1.0
            boosted = alpha_base * (1.0 + oci_score * oci_punish_mult)
            eff_alpha_2 = min(boosted, 1.0)
            
        q_stage2_mf[s1_idx, a2] = q_stage2_mf[s1_idx, a2] + eff_alpha_2 * delta_stage2

        # Stage 1 update (using Stage 2 values)
        delta_stage1 = q_stage2_mf[s1_idx, a2] - q_stage1_mf[a1]
        
        # We apply the same asymmetry logic to stage 1 updates
        if delta_stage1 >= 0:
            eff_alpha_1 = alpha_base
        else:
            boosted = alpha_base * (1.0 + oci_score * oci_punish_mult)
            eff_alpha_1 = min(boosted, 1.0)

        q_stage1_mf[a1] = q_stage1_mf[a1] + eff_alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Uncertainty Intolerance (Exploration Suppression)
This model suggests that high OCI is linked to "Intolerance of Uncertainty." In a softmax decision rule, exploration is driven by the inverse temperature `beta`. A lower beta means more exploration (randomness), while a higher beta means more exploitation (determinism). Here, the OCI score increases the `beta` parameter, making the agent more rigid and less likely to explore options that currently appear suboptimal, effectively suppressing uncertainty.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Uncertainty Intolerance Model.
    
    Hypothesis: OCI correlates with intolerance of uncertainty, leading to 
    reduced exploration. The inverse temperature (beta) is increased by the 
    OCI score, making choices more deterministic/rigid.
    
    Parameters:
    learning_rate: [0, 1] - Standard learning rate.
    beta_base: [0, 10] - Baseline inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    oci_rigidity: [0, 5] - How strongly OCI increases beta (reducing exploration).
    """
    learning_rate, beta_base, w, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta: Base + (OCI * Rigidity Factor)
    # Higher beta = less exploration, more exploitation of current best
    beta_eff = beta_base + (oci_score * oci_rigidity)
    # Cap beta to prevent numerical overflow in exp()
    if beta_eff > 20: beta_eff = 20.0 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use effective beta
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s1_idx = int(state[trial])
        # Use effective beta
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s1_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[s1_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s1_idx, a2]
        q_stage2_mf[s1_idx, a2] = q_stage2_mf[s1_idx, a2] + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```