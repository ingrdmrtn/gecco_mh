Here are three new cognitive models for the two-step task, incorporating the OCI score to explain individual variability.

### Model 1: Dynamic Transition Learning Modulated by OCI
This model hypothesizes that the participant's belief about the spaceship-planet transition probabilities is not fixed (even if instructed), but is updated trial-by-trial. The rate of this update is modulated by their OCI score. High OCI (compulsivity/doubt) might lead to faster updating (instability) or slower updating (rigidity) of their internal model of the environment.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-modulated Transition Learning Rate.
    
    The agent updates the transition matrix (Model-Based component) trial-by-trial.
    The learning rate for this structural update is modulated by OCI.
    
    Parameters:
    lr_reward: [0, 1] - Learning rate for reward values (MF values).
    lr_trans_base: [0, 1] - Base learning rate for transition probabilities.
    lr_trans_oci: [-1, 1] - Modulation of transition learning rate by OCI.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting between Model-Based and Model-Free (0=MF, 1=MB).
    stickiness: [-5, 5] - Choice perseveration bonus for Stage 1.
    """
    lr_reward, lr_trans_base, lr_trans_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate effective transition learning rate
    lr_trans = lr_trans_base + (lr_trans_oci * oci_val)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition matrix (rows=action1, cols=state)
    # Start with instructed/common knowledge (0.7/0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Stickiness
        if last_action_1 != -1:
            logits_1[int(last_action_1)] += stickiness
            
        # Softmax Choice 1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Observe State
        s_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Observe Reward
        r = reward[trial]
        
        # --- Updates ---
        
        # 1. Update Transition Matrix (Model-Based Learning)
        # Increase prob of observed transition (a1 -> s_idx)
        transition_matrix[a1, s_idx] += lr_trans * (1.0 - transition_matrix[a1, s_idx])
        # Decrease prob of unobserved transition
        transition_matrix[a1, 1 - s_idx] = 1.0 - transition_matrix[a1, s_idx]
        
        # 2. Update Action Values (Model-Free Learning)
        # Stage 1 MF update (SARSA-style)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_reward * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_reward * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that OCI affects the balance between learning from positive outcomes (reward) and negative outcomes (omission of reward). OCI is linked to anxiety and error-sensitivity; thus, higher OCI might lead to a stronger learning response to negative prediction errors (avoidance learning).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-modulated Asymmetric Learning Rates.
    
    Learning rates for positive and negative prediction errors are distinct.
    The negative learning rate is defined relative to the positive one,
    modulated by OCI to capture bias towards/away from negative feedback.
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors (delta > 0).
    lr_neg_base: [0, 5] - Base multiplier for negative learning rate relative to lr_pos.
    lr_neg_oci: [-5, 5] - Modulation of the negative multiplier by OCI.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting between Model-Based and Model-Free.
    stickiness: [-5, 5] - Choice perseveration bonus.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate negative learning rate multiplier
    neg_multiplier = lr_neg_base + (lr_neg_oci * oci_val)
    # Ensure multiplier is non-negative
    neg_multiplier = np.maximum(neg_multiplier, 0.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[int(last_action_1)] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates with Asymmetric Learning Rate ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = lr_pos if delta_stage1 >= 0 else (lr_pos * neg_multiplier)
        # Clip effective LR to [0, 1] for stability
        lr_1 = np.clip(lr_1, 0.0, 1.0)
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = lr_pos if delta_stage2 >= 0 else (lr_pos * neg_multiplier)
        lr_2 = np.clip(lr_2, 0.0, 1.0)
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Split Stage Betas Modulated by OCI
This model separates the "Inverse Temperature" (randomness/exploration) parameter for the planning stage (Stage 1) and the harvesting stage (Stage 2). It hypothesizes that OCI specifically affects decision-making under high uncertainty (Stage 1 planning) differently than simple bandit decisions (Stage 2).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Split Stage Betas Modulated by OCI.
    
    The model allows for different levels of choice stochasticity in Stage 1 
    vs Stage 2. The Stage 1 beta is modulated by OCI, reflecting the impact 
    of compulsivity on complex planning/decision-making, while Stage 2 beta 
    is fixed.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for MF values.
    beta_1_base: [0, 10] - Base inverse temperature for Stage 1.
    beta_1_oci: [-5, 5] - Modulation of Stage 1 beta by OCI.
    beta_2: [0, 10] - Inverse temperature for Stage 2.
    w: [0, 1] - Weighting between Model-Based and Model-Free.
    stickiness: [-5, 5] - Choice perseveration bonus.
    """
    learning_rate, beta_1_base, beta_1_oci, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate Stage 1 Beta
    beta_1 = beta_1_base + (beta_1_oci * oci_val)
    beta_1 = np.maximum(beta_1, 0.0) # Ensure non-negative
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy (Uses Beta 1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net
        
        if last_action_1 != -1:
            logits_1[int(last_action_1)] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        
        # --- Stage 2 Policy (Uses Beta 2) ---
        # Stage 2 is pure MF (no transition model relevant here)
        logits_2 = beta_2 * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```