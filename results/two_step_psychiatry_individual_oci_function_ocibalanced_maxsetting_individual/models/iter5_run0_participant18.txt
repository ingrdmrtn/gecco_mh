Here are three new cognitive models designed to capture different potential mechanisms by which obsessive-compulsive symptoms (OCI) might influence decision-making in this two-step task.

### Model 1: OCI-Modulated Model-Based/Model-Free Balance with Pruning
This model hypothesizes that higher OCI scores might lead to a more rigid, habitual (model-free) strategy, or conversely, a more deliberative (model-based) strategy. Furthermore, it introduces a "pruning" mechanism where the participant might ignore the less likely transition path if their OCI score makes them more risk-averse or focused on deterministic rules.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based/Model-Free Balance with Pruning.
    
    This model combines Model-Based (MB) and Model-Free (MF) reinforcement learning.
    The weight 'w' determines the balance (w=1 is pure MB, w=0 is pure MF).
    The 'w' parameter is modulated by the OCI score.
    Additionally, a pruning threshold is applied to the MB transition matrix based on OCI,
    simulating a tendency to simplify the world model (ignoring rare transitions) in high OCI.

    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Base weight for Model-Based control.
    - w_oci_mod: [-1, 1] Modulation of 'w' by OCI score.
    - pruning_factor: [0, 1] If OCI > 0.5, rare transitions < pruning_factor are treated as 0 probability.
    """
    learning_rate, beta, w_base, w_oci_mod, pruning_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective mixing weight w (bounded between 0 and 1)
    w = w_base + (w_oci_mod * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix (common=0.7, rare=0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Pruning mechanism: If OCI is high enough (arbitrary split at median range .33-.66), 
    # simplify the transition matrix to reduce uncertainty.
    # Here we use the raw OCI score to scale the perceived rare transition probability.
    # If pruning is active, the rare transition probability is reduced.
    perceived_transition_matrix = transition_matrix.copy()
    if oci_score > 0.4: # Medium to High OCI
        # Reduce rare transition probability by the pruning factor
        perceived_transition_matrix[0, 1] *= (1.0 - pruning_factor)
        perceived_transition_matrix[1, 0] *= (1.0 - pruning_factor)
        # Normalize rows
        perceived_transition_matrix[0, 0] = 1.0 - perceived_transition_matrix[0, 1]
        perceived_transition_matrix[1, 1] = 1.0 - perceived_transition_matrix[1, 0]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Skip missing data
        if a1 == -1:
            continue

        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1, a1) = Sum P(s2|s1,a1) * max_a2 Q(s2, a2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = perceived_transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Update Stage 1 MF (TD(1) logic usually, here simple SARSA-like update for MF)
        # Note: Standard two-step usually updates Stage 1 MF using Stage 2 max Q (TD(0)) or reward (TD(1))
        # We use a TD(1)-like update where the reward propagates back to stage 1
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Additional update for Stage 1 MF based on final reward (TD(1) aspect)
        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: OCI-Based Asymmetric Learning Rates (Positive/Negative)
This model posits that OCI symptoms relate to how individuals process positive versus negative feedback. Specifically, high OCI might be associated with "hyper-learning" from failure (negative prediction errors) or safety behaviors, while low OCI might be more reward-driven.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Based Asymmetric Learning Rates.
    
    This is a pure Model-Free learner (TD-learning) but with separate learning rates 
    for positive and negative prediction errors. The balance between these rates 
    is modulated by the OCI score.
    
    Hypothesis: Higher OCI leads to stronger learning from negative outcomes (avoidance).
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - lr_asymmetry: [-0.5, 0.5] Bias towards positive or negative learning.
    - oci_sens: [0, 2] How much OCI amplifies the negative learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Eligibility trace decay (connecting stage 2 to stage 1).
    """
    lr_base, lr_asymmetry, oci_sens, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Define learning rates based on OCI
    # lr_pos is for RPE > 0
    # lr_neg is for RPE < 0
    # OCI increases sensitivity to negative errors specifically
    lr_pos = np.clip(lr_base + lr_asymmetry, 0, 1)
    lr_neg = np.clip(lr_base - lr_asymmetry + (oci_sens * oci_score), 0, 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue

        # Stage 1 Policy
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 RPE
        rpe_2 = r - q_stage2[s2, a2]
        
        # Update Stage 2
        alpha_2 = lr_pos if rpe_2 > 0 else lr_neg
        q_stage2[s2, a2] += alpha_2 * rpe_2
        
        # Stage 1 RPE (TD(0) part)
        # The value of the state arrived at (s2) is max(Q(s2, :)) or Q(s2, a2)
        # Standard SARSA uses Q(s2, a2)
        rpe_1 = q_stage2[s2, a2] - q_stage1[a1]
        alpha_1 = lr_pos if rpe_1 > 0 else lr_neg
        q_stage1[a1] += alpha_1 * rpe_1
        
        # Eligibility trace update: Stage 1 also learns from Stage 2's RPE
        q_stage1[a1] += lambda_eligibility * alpha_2 * rpe_2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Exploration (Uncertainty Bonus)
This model suggests that OCI relates to intolerance of uncertainty. Individuals with different OCI scores might handle exploration differently. High OCI might lead to strictly exploiting the best option to reduce anxiety (low exploration), or conversely, checking the other option excessively to verify outcomes. This model adds an "uncertainty bonus" to the values, scaled by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Exploration (Uncertainty Bonus).
    
    This model augments a standard reinforcement learner with an exploration bonus
    based on how long it has been since an option was chosen.
    The magnitude of this bonus is modulated by the OCI score.
    
    Hypothesis: High OCI participants might have 'checking' compulsions (high bonus)
    or extreme rigidity (negative bonus/penalty for unchosen).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - bonus_base: [-1, 1] Base exploration bonus parameter.
    - bonus_oci_scale: [-2, 2] How OCI scales the bonus.
    - decay_rate: [0, 1] How fast the 'time since last chosen' counter grows/decays.
    """
    learning_rate, beta, bonus_base, bonus_oci_scale, decay_rate = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective exploration bonus weight
    w_bonus = bonus_base + (bonus_oci_scale * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track time since last choice for Stage 1 actions
    # Initialized to 0, increments every trial an action is NOT chosen
    time_since_choice_1 = np.zeros(2) 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue

        # --- Stage 1 Choice ---
        # Add exploration bonus to Q-values for decision making
        # Bonus = w_bonus * time_since_last_choice
        q_stage1_augmented = q_stage1 + (w_bonus * time_since_choice_1)
        
        exp_q1 = np.exp(beta * q_stage1_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        # (Standard Softmax)
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update counters
        # Chosen action resets to 0
        # Unchosen action increments by 1 (or decays towards max)
        time_since_choice_1[a1] = 0
        time_since_choice_1[1-a1] += 1
        
        # Q-Learning Updates (Simple SARSA)
        delta1 = q_stage2[s2, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        delta2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * delta2
        
        # TD(1) style propagation
        q_stage1[a1] += learning_rate * delta2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```