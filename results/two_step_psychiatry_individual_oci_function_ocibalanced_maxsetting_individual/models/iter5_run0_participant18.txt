Here are the three cognitive models implemented as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Trace.
    Hypothesis: High OCI scores are associated with altered credit assignment for distal choices.
    This model assumes that OCI interferes with the maintenance of eligibility traces (lambda),
    reducing the extent to which the Stage 2 outcome reinforces the Stage 1 choice.
    High OCI leads to a more fragmented, stage-local learning process.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-Based weight [0,1]
    - lambda_base: Base eligibility trace parameter [0,1]
    - stickiness: Choice perseveration [0,5]
    - oci_decay: Sensitivity of eligibility trace to OCI [0,5]
    """
    lr, beta, w, lambda_base, stickiness, oci_decay = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Modulate lambda based on OCI
    # As OCI increases, lambda decreases (faster decay/less credit assignment to Stage 1)
    lambda_val = lambda_base * np.exp(-oci_decay * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1: # Skip invalid trials
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        # Stage 1 Choice
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        logits_1 = beta * q_net_s1
        
        # Stickiness
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[int(action_1[t])]
        
        # Stage 2 Choice
        s_curr = int(state[t])
        logits_2 = beta * q_mf_s2[s_curr]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[int(action_2[t])]
        
        # Learning
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Stage 1 PE (TD(0))
        delta_1 = q_mf_s2[s_curr, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        # Stage 2 PE
        delta_2 = r - q_mf_s2[s_curr, a2]
        q_mf_s2[s_curr, a2] += lr * delta_2
        
        # Eligibility Trace update for Stage 1
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Belief (Structural Doubt).
    Hypothesis: OCI is associated with doubt regarding the environmental structure.
    High OCI participants perceive the transition probabilities as less deterministic
    (closer to 0.5/0.5) than they actually are (0.7/0.3), effectively dampening 
    the precision of Model-Based planning.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-Based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Choice perseveration [0,5]
    - doubt_param: Distortion of transition matrix by OCI [0,5]
    """
    lr, beta, w, lambda_val, stickiness, doubt_param = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Modulate transition belief
    # p_common goes from 0.7 (no doubt) towards 0.5 (full doubt/randomness) as OCI increases
    doubt_factor = 1.0 / (1.0 + doubt_param * oci_score)
    p_common = 0.5 + 0.2 * doubt_factor
    
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        max_q_s2 = np.max(q_mf_s2, axis=1)
        # Use the distorted transition matrix for MB calculation
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        logits_1 = beta * q_net_s1
        
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[int(action_1[t])]
        
        s_curr = int(state[t])
        logits_2 = beta * q_mf_s2[s_curr]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[int(action_2[t])]
        
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        delta_1 = q_mf_s2[s_curr, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        delta_2 = r - q_mf_s2[s_curr, a2]
        q_mf_s2[s_curr, a2] += lr * delta_2
        
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Conditional Perseveration (Compulsive Repetition).
    Hypothesis: OCI is linked to compulsive behavior, specifically the tendency to repeat 
    actions that resulted in failure (chasing/checking), distinct from general stickiness.
    This model adds an extra stickiness term proportional to OCI only after unrewarded trials.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-Based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stick_base: General choice perseveration [0,5]
    - oci_loss_stick: Additional perseveration after loss, scaled by OCI [0,5]
    """
    lr, beta, w, lambda_val, stick_base, oci_loss_stick = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0.0
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        logits_1 = beta * q_net_s1
        
        # Conditional Stickiness
        if prev_a1 != -1:
            stick_bonus = stick_base
            # If previous outcome was a loss (0.0), add OCI-driven stickiness
            if prev_reward < 0.5:
                stick_bonus += oci_loss_stick * oci_score
            
            logits_1[prev_a1] += stick_bonus
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[int(action_1[t])]
        
        s_curr = int(state[t])
        logits_2 = beta * q_mf_s2[s_curr]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[int(action_2[t])]
        
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        delta_1 = q_mf_s2[s_curr, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        delta_2 = r - q_mf_s2[s_curr, a2]
        q_mf_s2[s_curr, a2] += lr * delta_2
        
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```