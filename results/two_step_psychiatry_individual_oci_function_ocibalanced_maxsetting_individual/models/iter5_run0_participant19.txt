Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

### Model 1: OCI-Conditioned Punishment Stickiness
This model tests the hypothesis that high OCI scores relate to compulsive repetition of actions specifically after negative outcomes ("Lose-Stay"), rather than just general stickiness or reinforced stickiness.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Conditioned Punishment Stickiness (Lose-Stay).
    
    Hypothesis: High OCI is associated with 'compulsive' repetition of actions 
    even when they result in no reward (failure). This contrasts with standard 
    reinforcement learning (Lose-Shift) and the previous 'Reinforced Stickiness' 
    (Win-Stay). Here, a stickiness bonus is applied to the previous Stage 1 choice 
    specifically when the previous outcome was a loss (0 coins).
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight mixing Model-Based (1) and Model-Free (0) values.
    - stick_loss_oci: [0, 5] Magnitude of stickiness bonus applied after a loss, scaled by OCI.
    """
    learning_rate, beta, w, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the stickiness magnitude specific to this participant
    stickiness_mag = stick_loss_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1.0
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness bonus ONLY if the last trial resulted in 0 reward (Loss)
        if last_action_1 != -1 and last_reward == 0.0:
            q_net[last_action_1] += stickiness_mag

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 1 MF value (TD prediction error)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF value (Reward prediction error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Unchosen Value Decay
This model hypothesizes that high OCI leads to a "tunnel vision" effect where the value of the unchosen option decays (is forgotten) more rapidly, leading to rigidity in choice patterns.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Unchosen Value Decay (Tunnel Vision).
    
    Hypothesis: High OCI participants may exhibit a form of 'tunnel vision' or 
    obsessive focus where they rapidly lose track of the value of options they 
    did not choose. This is modeled as a decay of the Q-value for the unchosen 
    spaceship in Stage 1, scaled by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - decay_oci: [0, 1] Decay rate for unchosen action value, scaled by OCI. 
                        (Effective decay = decay_oci * OCI).
    """
    learning_rate, beta, w, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective decay rate depends on OCI
    effective_decay = decay_oci * oci_score
    # Clamp to [0, 1] to ensure stability
    if effective_decay > 1.0: effective_decay = 1.0
    if effective_decay < 0.0: effective_decay = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Standard Update for Chosen Action
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay Update for Unchosen Action (Tunnel Vision)
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - effective_decay)
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Induced Loss Aversion
This model proposes that high OCI participants perceive the absence of reward (0 coins) as an active negative outcome (loss), driving avoidance behavior more strongly than simple extinction.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Induced Loss Aversion (Negative Reward Perception).
    
    Hypothesis: Participants with high OCI symptoms may perceive the lack of a 
    reward (0 coins) not just as a neutral outcome, but as a negative one (failure 
    or loss). This model transforms the reward function such that 0 coins results 
    in a negative reward value proportional to OCI, driving active avoidance rather 
    than just extinction.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - loss_sens_oci: [0, 5] Magnitude of negative reward for 0 outcomes, scaled by OCI.
                            (Effective R for 0 coins = -loss_sens_oci * OCI).
    """
    learning_rate, beta, w, loss_sens_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Define the negative reward value for failure outcomes
    neg_reward_val = -1.0 * loss_sens_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Transform Reward: If r is 0, perceived as negative
        effective_r = r
        if r == 0.0:
            effective_r = neg_reward_val
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update using the potentially negative Q-values from Stage 2
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 using effective reward
        delta_stage2 = effective_r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```