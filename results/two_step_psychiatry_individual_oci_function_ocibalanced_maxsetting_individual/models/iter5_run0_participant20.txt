Here are three cognitive models expressed as Python functions.

### Model 1: OCI-Modulated Subjective Loss Sensitivity
This model hypothesizes that OCI scores correlate with **loss aversion** or **punishment sensitivity**. In this task, receiving 0 coins is the "loss" condition. The model assumes that while the objective reward is 0, high-OCI participants perceive this as a negative value (subjective punishment), driving stronger avoidance learning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Subjective Loss (Punishment Sensitivity).
    
    Hypothesis: Participants with higher OCI scores perceive the absence of reward (0 coins)
    as a subjective loss (negative reward), rather than a neutral outcome. This drives
    avoidance behavior more strongly than standard RL.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature (exploitation vs exploration).
    w: [0,1] - Mixing weight for Model-Based (1) vs Model-Free (0).
    loss_base: [0,2] - Baseline subjective negativity of a 0-coin outcome.
    loss_oci_slope: [0,2] - How much OCI increases this subjective negativity.
    
    Effective Reward Calculation:
    If reward == 1: R_eff = 1
    If reward == 0: R_eff = -1 * (loss_base + loss_oci_slope * oci)
    """
    learning_rate, beta, w, loss_base, loss_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate the subjective magnitude of a "loss" (0 reward)
    subjective_loss_mag = loss_base + loss_oci_slope * participant_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r_observed = reward[trial]
        
        # Apply subjective loss transformation
        if r_observed == 1.0:
            r_eff = 1.0
        else:
            r_eff = -1.0 * subjective_loss_mag

        # Stage 1 Policy (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy (Model-Free)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates using Effective Reward
        delta_stage2 = r_eff - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Learning
This model hypothesizes that OCI affects how participants learn the structure of the world (the transition matrix). Instead of assuming fixed transition probabilities (0.7/0.3), the participant learns them over time. OCI modulates the **learning rate of these transitions**, reflecting either rigidity (low learning rate) or hyper-vigilance to structural change (high learning rate).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Transition Learning Rate.
    
    Hypothesis: The participant does not assume a fixed transition matrix but learns it 
    dynamically. The rate at which they update their belief about spaceship-planet 
    transitions is modulated by their OCI score.
    
    Parameters:
    lr_reward: [0,1] - Learning rate for reward values (Q-values).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight for Model-Based (1) vs Model-Free (0).
    lr_trans_base: [0,1] - Baseline learning rate for transition probabilities.
    lr_trans_oci: [0,1] - How much OCI affects the transition learning rate.
    
    Transition Update:
    P(State|Action) is updated based on observed transitions.
    lr_trans = clip(lr_trans_base + lr_trans_oci * oci, 0, 1)
    """
    lr_reward, beta, w, lr_trans_base, lr_trans_oci = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Dynamic transition learning rate
    lr_trans = lr_trans_base + lr_trans_oci * participant_oci
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition beliefs (start with instruction knowledge 0.7)
    # p_a0_s0: Prob of reaching state 0 given action 0
    # p_a1_s1: Prob of reaching state 1 given action 1
    p_a0_s0 = 0.7
    p_a1_s1 = 0.7
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Construct current estimated transition matrix for MB step
        # T = [[P(S0|A0), P(S1|A0)], [P(S0|A1), P(S1|A1)]]
        # Note: The template computes T @ MaxQ. 
        # T should be (2, 2) where row i is P(State|Action i).
        # Row 0: [p_a0_s0, 1-p_a0_s0]
        # Row 1: [1-p_a1_s1, p_a1_s1]
        
        current_trans_matrix = np.array([
            [p_a0_s0, 1.0 - p_a0_s0],
            [1.0 - p_a1_s1, p_a1_s1]
        ])

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = current_trans_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Value Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_reward * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_reward * delta_stage1
        
        # Transition Probability Updates
        if a1 == 0:
            # Did we go to state 0?
            outcome = 1.0 if s_idx == 0 else 0.0
            p_a0_s0 += lr_trans * (outcome - p_a0_s0)
        elif a1 == 1:
            # Did we go to state 1?
            outcome = 1.0 if s_idx == 1 else 0.0
            p_a1_s1 += lr_trans * (outcome - p_a1_s1)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Habit Trace
This model hypothesizes that OCI correlates with the strength of **long-term habit formation**. Unlike simple 1-step stickiness, this model maintains a "choice trace" (exponential moving average of past choices) that biases future decisions. High OCI leads to a stronger influence of this long-term history on current choices.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Long-term Habit (Choice Trace).
    
    Hypothesis: OCI scores relate to the strength of habits formed over time. 
    A 'choice trace' tracks the history of choices with a decay rate. The influence 
    of this trace (habit strength) on the Q-values is modulated by OCI.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight for MB vs MF.
    trace_decay: [0,1] - Decay rate for the choice trace (0 = only last choice, 1 = no decay).
    habit_oci_slope: [0,5] - Scaling factor for how much OCI weights the habit trace.
    
    Habit Bias:
    bias = habit_oci_slope * oci * choice_trace
    """
    learning_rate, beta, w, trace_decay, habit_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    habit_weight = habit_oci_slope * participant_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for Stage 1 actions (A vs U)
    # Initialized at 0 (no history)
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Habit Trace
        # The trace acts as an additive bias to the Q-values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + (habit_weight * choice_trace)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update Choice Trace (Decay then reinforce chosen)
        # trace(t+1) = decay * trace(t) + (1-decay) * I(choice)
        # This keeps the trace magnitude roughly normalized between 0 and 1
        choice_vec = np.zeros(2)
        choice_vec[a1] = 1.0
        choice_trace = trace_decay * choice_trace + (1.0 - trace_decay) * choice_vec
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Value Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```