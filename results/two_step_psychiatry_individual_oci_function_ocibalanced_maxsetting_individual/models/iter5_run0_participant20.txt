def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Subjective Transition Probability Distortion.
    
    Hypothesis: Participants with higher OCI scores may exhibit intolerance of uncertainty,
    leading to a distorted belief about the stability of the environment. Specifically, 
    they may perceive the 'common' (0.7) transition as less reliable (closer to 0.5), 
    reducing the effectiveness of Model-Based planning.
    
    Parameters:
    - lr: [0, 1] Learning rate for Model-Free values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stick: [0, 5] Perseveration bonus.
    - trans_distortion: [0, 1] OCI-based distortion of transition probability.
      p_common = 0.7 - (trans_distortion * oci). Clamped to [0.5, 0.7].
      
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, stick, trans_distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # Base is 0.7. Distortion reduces it towards 0.5 (randomness).
    p_common = 0.7 - (trans_distortion * oci_score)
    if p_common < 0.5: p_common = 0.5
    if p_common > 0.7: p_common = 0.7
    
    # Subjective transition matrix used for MB calculation
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB = T_subjective * max(Q_S2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Stickiness
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update (SARSA-style)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Stage 1 Learning Rate.
    
    Hypothesis: OCI scores modulate the learning rate specifically for the first-stage 
    action-state associations (Model-Free), leading to differential habit formation 
    strength compared to the reward valuation (Stage 2). High OCI may lead to 
    faster/stronger caching of the Stage 1 choice outcome.
    
    Parameters:
    - lr_s2: [0, 1] Learning rate for Stage 2 (State -> Reward).
    - lr_s1_base: [0, 1] Base learning rate for Stage 1 (Action -> State/Value).
    - lr_s1_oci_factor: [0, 5] Multiplier for OCI effect on Stage 1 LR.
      lr_s1 = lr_s1_base * (1 + lr_s1_oci_factor * oci). Clamped to 1.0.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick: [0, 5] Stickiness.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr_s2, lr_s1_base, lr_s1_oci_factor, beta, w, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 specific learning rate
    lr_s1 = lr_s1_base * (1.0 + lr_s1_oci_factor * oci_score)
    if lr_s1 > 1.0: lr_s1 = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    last_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        # Use OCI-modulated LR for Stage 1
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Use standard LR for Stage 2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_s2 * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Surprise-Modulated Learning (Pearce-Hall style).
    
    Hypothesis: OCI scores correlate with hyper-vigilance to prediction errors (surprise).
    The learning rate dynamically increases when the absolute prediction error is high,
    and this effect is scaled by the OCI score.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate.
    - surp_oci: [0, 10] OCI scaling factor for surprise modulation.
      alpha_effective = lr_base + surp_oci * oci * |delta|
      (Clamped to 1.0).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick: [0, 5] Stickiness.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr_base, surp_oci, beta, w, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    last_choice_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        
        # Dynamic Learning Rate for Stage 1 based on |delta1|
        lr_eff_1 = lr_base + surp_oci * oci_score * np.abs(delta_stage1)
        if lr_eff_1 > 1.0: lr_eff_1 = 1.0
        q_stage1_mf[a1] += lr_eff_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        # Dynamic Learning Rate for Stage 2 based on |delta2|
        lr_eff_2 = lr_base + surp_oci * oci_score * np.abs(delta_stage2)
        if lr_eff_2 > 1.0: lr_eff_2 = 1.0
        q_stage2_mf[s2, a2] += lr_eff_2 * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss