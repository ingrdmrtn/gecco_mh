Here are 3 new cognitive models that explore different mechanisms for how OCI symptoms might influence decision-making in this two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Mixing Weight (w)
This model posits that the balance between Model-Based (planning) and Model-Free (habit) systems is influenced by the OCI score. Specifically, it tests the hypothesis that higher OCI scores might push an individual towards more habitual (Model-Free) control, or conversely, towards more rigid planning.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free Learner where OCI modulates the mixing weight 'w'.
    
    The mixing weight 'w' determines the contribution of the Model-Based system.
    w_total = w_base + w_oci_scale * oci
    This allows the degree of goal-directedness to vary with symptom severity.
    
    Parameters:
    lr : [0, 1] Learning rate for value updates.
    beta : [0, 10] Inverse temperature for softmax choice.
    w_base : [0, 1] Baseline mixing weight (0=Pure MF, 1=Pure MB).
    w_oci_scale : [-1, 1] How much OCI shifts the balance. 
                  (Bounded effectively by sigmoid or clipping logic).
    """
    lr, beta, w_base, w_oci_scale = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate mixing weight and clip to [0, 1]
    w = w_base + w_oci_scale * current_oci
    w = np.clip(w, 0.0, 1.0)
    
    # Fixed transition matrix for MB planning
    # Spaceship 0 -> Planet 0 (70%), Spaceship 1 -> Planet 1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Model-Based Value: V(S') = max(Q_stage2(S', :))
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        # Stage 2 Update (TD)
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr * rpe_2
        
        # Stage 1 Update (TD - Model Free)
        # Note: In a pure hybrid model, MF Q-values are updated via SARSA or Q-learning
        # Here we use the value of the state actually reached (SARSA-like) or max (Q-learning)
        # Standard 2-step usually uses the value of the chosen stage 2 option or the state value.
        # We use the updated stage 2 value of the chosen action as the target.
        rpe_1 = q_stage2[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss Aversion Learner with OCI-Modulated Loss Sensitivity
This model investigates if OCI symptoms are related to an increased sensitivity to negative outcomes (losses). Instead of a single learning rate, this model weighs negative prediction errors (disappointments) differently based on the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with OCI-Modulated Loss Sensitivity.
    
    This model separates the weighting of positive and negative prediction errors.
    The 'loss_multiplier' amplifies the learning signal when the outcome is worse 
    than expected. This multiplier is a function of the OCI score.
    
    Parameters:
    lr : [0, 1] Base learning rate.
    beta : [0, 10] Inverse temperature.
    loss_sens_base : [0, 5] Base multiplier for negative prediction errors.
    loss_sens_oci : [0, 5] Additional multiplier scaling with OCI.
    """
    lr, beta, loss_sens_base, loss_sens_oci = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate loss sensitivity
    loss_sensitivity = loss_sens_base + loss_sens_oci * current_oci
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning Stage 2 ---
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        effective_lr_2 = lr
        if rpe_2 < 0:
            effective_lr_2 *= loss_sensitivity
            
        q_stage2[state_idx, action_2[trial]] += effective_lr_2 * rpe_2
        
        # --- Learning Stage 1 ---
        # Using the value of the state reached (TD(0))
        target_stage1 = np.max(q_stage2[state_idx]) # Q-learning update
        rpe_1 = target_stage1 - q_stage1[action_1[trial]]
        
        effective_lr_1 = lr
        if rpe_1 < 0:
            effective_lr_1 *= loss_sensitivity
            
        q_stage1[action_1[trial]] += effective_lr_1 * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration with OCI Dampening
This model posits that exploration is driven by uncertainty (inverse of visit counts), but high OCI scores might dampen this exploratory drive, leading to more rigid or exploitative behavior despite uncertainty.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with Uncertainty Bonus dampened by OCI.
    
    An exploration bonus is added to actions that have been visited less frequently.
    However, the influence of this exploration bonus is reduced (dampened) 
    as OCI score increases, potentially reflecting rigidity or anxiety-driven 
    avoidance of uncertainty.
    
    Parameters:
    lr : [0, 1] Learning rate.
    beta : [0, 10] Inverse temperature.
    expl_bonus_base : [0, 2] Base exploration bonus parameter.
    expl_oci_damp : [0, 2] Factor that reduces exploration based on OCI.
    """
    lr, beta, expl_bonus_base, expl_oci_damp = model_parameters
    n_trials = len(action_1)
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    current_oci = oci[0]
    
    # Calculate effective exploration bonus weight
    # We ensure it doesn't go below zero
    w_expl = max(0.0, expl_bonus_base - expl_oci_damp * current_oci)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track counts for uncertainty (sqrt(1/count))
    counts_stage1 = np.zeros(2)
    counts_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        
        # --- Stage 1 Choice ---
        # Add uncertainty bonus: 1 / sqrt(count + 1)
        bonus_1 = w_expl * (1.0 / np.sqrt(counts_stage1 + 1))
        
        exp_q1 = np.exp(beta * (q_stage1 + bonus_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        counts_stage1[action_1[trial]] += 1
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        
        bonus_2 = w_expl * (1.0 / np.sqrt(counts_stage2[state_idx] + 1))
        
        exp_q2 = np.exp(beta * (q_stage2[state_idx] + bonus_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        counts_stage2[state_idx, action_2[trial]] += 1
        
        # --- Learning ---
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += lr * rpe_2
        
        target_stage1 = np.max(q_stage2[state_idx])
        rpe_1 = target_stage1 - q_stage1[action_1[trial]]
        q_stage1[action_1[trial]] += lr * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```