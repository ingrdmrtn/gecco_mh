Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

### Cognitive Model 1: Dynamic Perseveration (Stickiness Accumulation)
This model hypothesizes that perseveration (stickiness) is not a static bias but accumulates the longer a participant persists with a choice. The OCI score modulates the **rate** of this accumulation. High OCI leads to a rapid buildup of "compulsive" stickiness, explaining the long blocks of repeated choices observed in the data.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Perseveration Model.
    Perseveration strength accumulates with consecutive choices of the same option.
    OCI modulates the rate at which this stickiness builds up.
    
    Hypothesis: Higher OCI leads to faster accumulation of stickiness (compulsion)
    with repeated choices, making it harder to switch.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    pers_static: [-2, 2] Base static perseveration bonus (1-back stickiness).
    pers_accum_base: [0, 2] Base rate of stickiness accumulation per consecutive trial.
    pers_accum_oci: [-2, 2] OCI modulation of the accumulation rate.
    """
    lr, beta, w, pers_static, pers_accum_base, pers_accum_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate accumulation rate based on OCI
    accum_rate = pers_accum_base + pers_accum_oci * current_oci
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Tracks how many times an action has been chosen consecutively
    consecutive_counts = np.zeros(2)
    last_action = -1
    
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Stage 1 Policy
        q_mb = tm @ np.max(q2, axis=1)
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Apply static perseveration
        if last_action != -1:
            q_net[last_action] += pers_static
            
        # Apply dynamic stickiness (bonus increases with consecutive choices)
        q_net += consecutive_counts * accum_rate
        
        # Softmax choice 1
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # Update consecutive counts
        if a1 == last_action:
            consecutive_counts[a1] += 1
        else:
            consecutive_counts[a1] = 1 # Reset to 1 (current choice)
            consecutive_counts[1-a1] = 0 # Reset other
        last_action = a1
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # Value Updates
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
    return -log_lik
```

### Cognitive Model 2: Uncertainty Modulation
This model posits that OCI relates to "Intolerance of Uncertainty." It introduces an uncertainty term (based on the time elapsed since an option was last chosen) into the decision value. OCI modulates the weight of this term. A negative weight (aversion) would cause the participant to stick to the recently chosen (known) option and avoid the unchosen (uncertain) one.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Uncertainty Modulation Model.
    Participants add a bonus/penalty to options based on how long it has been since 
    they were last chosen (proxy for uncertainty).
    
    Hypothesis: High OCI correlates with uncertainty aversion (negative weight),
    causing participants to avoid exploring unchosen options.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    pers: [-2, 2] Standard perseveration bonus.
    unc_weight_base: [-2, 2] Base weight for uncertainty (time since last choice).
    unc_weight_oci: [-2, 2] OCI modulation of uncertainty weight.
    """
    lr, beta, w, pers, unc_weight_base, unc_weight_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    w_unc = unc_weight_base + unc_weight_oci * current_oci
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_chosen_time = np.zeros(2)
    last_action = -1
    
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Stage 1 Policy
        q_mb = tm @ np.max(q2, axis=1)
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action != -1:
            q_net[last_action] += pers
            
        # Uncertainty: Time elapsed since last choice
        # We scale it (divide by 10) to keep values in a reasonable range relative to Q-values
        uncertainty = (t - last_chosen_time) / 10.0 
        uncertainty = np.minimum(uncertainty, 5.0) # Cap to prevent overflow effects
        
        q_net += w_unc * uncertainty
        
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        last_chosen_time[a1] = t
        last_action = a1
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # Value Updates
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
    return -log_lik
```

### Cognitive Model 3: Action-Habit Learning System
This model introduces a separate "Habit" learning system that runs in parallel with Model-Based and Model-Free RL. Unlike standard perseveration (which is just a 1-trial memory), this system learns a habit value $H(a)$ that strengthens with repetition and decays with disuse, governed by its own learning rate. OCI modulates the **influence (weight)** of this habit system on the final choice.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Action-Habit Learning Model.
    Includes a separate 'Habit' value system that learns purely from action repetition 
    (independent of reward), governed by its own learning rate.
    
    Hypothesis: High OCI individuals rely more heavily on this reward-independent 
    habit system (compulsive repetition), ignoring reward feedback.
    
    Parameters:
    lr: [0, 1] Learning rate for reward-based values (MF/MB).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based values (vs Model-Free).
    habit_lr: [0, 1] Learning rate for the Habit system (stickiness buildup/decay speed).
    habit_w_base: [0, 5] Base weight of the Habit system in decision making.
    habit_w_oci: [-5, 5] OCI modulation of the Habit system weight.
    """
    lr, beta, w, habit_lr, habit_w_base, habit_w_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    w_habit = habit_w_base + habit_w_oci * current_oci
    
    q_mf = np.zeros(2) # Reward-based MF
    q_habit = np.zeros(2) # Action-based Habit (starts at 0)
    q2 = np.zeros((2, 2))
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Stage 1 Policy
        q_mb = tm @ np.max(q2, axis=1)
        
        # Combine MB, MF, and Habit
        # The Habit value acts as an additive bias, weighted by w_habit
        q_net = w * q_mb + (1 - w) * q_mf + w_habit * q_habit
        
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # Update Habit (Law of Effect / Reinforcement of Action)
        # H[a] moves towards 1 if chosen, decays towards 0 if not.
        q_habit[a1] += habit_lr * (1 - q_habit[a1])
        q_habit[1-a1] *= (1 - habit_lr)
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # Update Reward Values
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
    return -log_lik
```