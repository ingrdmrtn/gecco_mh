Here are three new cognitive models expressed as Python functions. These models introduce novel mechanisms for how OCI scores might interact with decision-making parameters, specifically focusing on reward sensitivity modulation, asymmetric learning rates for positive/negative outcomes, and modulation of the model-based/model-free trade-off parameter `w`.

### Model 1: OCI-Modulated Reward Sensitivity
This model hypothesizes that individuals with higher OCI scores might perceive rewards or punishments differently. Instead of modulating the learning rate or the choice stickiness, this model scales the raw reward signal itself based on OCI. A high OCI might lead to hypersensitivity to outcomes (amplifying the reward signal) or blunting.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated reward sensitivity.
    
    Hypothesis: OCI scores modulate the effective magnitude of the reward experienced.
    Higher OCI might amplify the perceived value of feedback (hypersensitivity).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] (Mixing weight)
    reward_sens_base: [0, 5] (Base sensitivity multiplier)
    reward_sens_oci: [-5, 5] (Slope of OCI effect on sensitivity)
    """
    learning_rate, beta, w, reward_sens_base, reward_sens_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective reward sensitivity
    # If reward_sens_oci is positive, higher OCI amplifies rewards.
    effective_sensitivity = reward_sens_base + (reward_sens_oci * current_oci)
    # Ensure sensitivity doesn't go negative or illogical for this context
    effective_sensitivity = max(0.0, effective_sensitivity)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updating ---
        # Scale the reward by the OCI-derived sensitivity factor
        scaled_reward = reward[trial] * effective_sensitivity

        delta_stage2 = scaled_reward - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # TD(1) update for stage 1 using the stage 2 prediction error
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI specifically alters how participants learn from negative versus positive prediction errors. It proposes that OCI modulates the learning rate for *negative* prediction errors (disappointments) specifically, reflecting a potential negativity bias or fear of failure common in compulsive phenotypes.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated asymmetric learning rates.
    
    Hypothesis: OCI affects learning from negative prediction errors (disappointments)
    differently than positive ones. Higher OCI scales the learning rate for negative PEs.
    
    Bounds:
    lr_pos: [0, 1] (Learning rate for positive PEs)
    lr_neg_base: [0, 1] (Base learning rate for negative PEs)
    lr_neg_oci_scale: [0, 5] (Multiplier for OCI effect on negative LR)
    beta: [0, 10]
    w: [0, 1]
    """
    lr_pos, lr_neg_base, lr_neg_oci_scale, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    # We constrain it to be within [0, 1] roughly, though the optimizer handles bounds.
    effective_lr_neg = lr_neg_base * (1 + lr_neg_oci_scale * current_oci)
    effective_lr_neg = min(1.0, max(0.0, effective_lr_neg))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updating Stage 2 ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        
        # Apply asymmetric learning rate
        lr_s2 = lr_pos if delta_stage2 >= 0 else effective_lr_neg
        q_stage2_mf[state_idx, act2] += lr_s2 * delta_stage2
        
        # --- Updating Stage 1 ---
        # Note: We use the updated stage 2 value to drive stage 1 update
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        
        # Apply asymmetric learning rate for stage 1 as well
        lr_s1 = lr_pos if delta_stage1 >= 0 else effective_lr_neg
        q_stage1_mf[act1] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Model-Based Weighting (Logistic Sigmoid)
Previous attempts might have used linear scaling for `w` (the model-based weight). This model uses a logistic sigmoid function to map OCI to `w`. This assumes that the transition from model-free to model-based (or vice versa) based on OCI is non-linear and bounded naturally between 0 and 1, capturing a potential "threshold" effect of symptoms.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the weight 'w' is a sigmoid function of OCI.
    
    Hypothesis: The balance between Model-Based and Model-Free control changes 
    non-linearly with OCI. Low OCI might be flexible (high w), while high OCI 
    might be habitual (low w), or vice versa, modeled by a sigmoid curve.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_inflection: [0, 1] (OCI score where the sigmoid is at 0.5)
    w_slope: [-20, 20] (Steepness and direction of the transition)
    """
    learning_rate, beta, w_inflection, w_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Sigmoid function for w: 1 / (1 + exp(-slope * (oci - inflection)))
    # This maps w naturally to (0, 1)
    # If slope is negative, higher OCI -> lower w (more Model-Free/Habitual)
    w = 1.0 / (1.0 + np.exp(-w_slope * (current_oci - w_inflection)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updating ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```