Here are three new cognitive models for the two-step decision task, incorporating the OCI score to explain individual variability.

### Model 1: OCI-Modulated Perceived Transition Belief
This model tests the hypothesis that OCI symptoms are associated with a distorted belief about the environment's structure. Specifically, it proposes that participants with higher OCI scores may perceive the transition probabilities (Common vs. Rare) differentlyâ€”either becoming more rigid (perceiving the common transition as deterministic) or more uncertain (perceiving transitions as random). This distortion affects the Model-Based value calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Perceived Transition Belief.
    
    Hypothesis: Participants with higher OCI scores have a distorted internal model 
    of the transition structure. They may overestimate the probability of the 
    common transition (rigidity) or underestimate it (uncertainty), affecting 
    Model-Based value calculation.
    
    Parameters:
    - learning_rate: [0,1] Learning rate for value updates.
    - beta: [0,10] Inverse temperature (softness of choice).
    - w: [0,1] Weighting between Model-Based (1) and Model-Free (0) control.
    - stickiness: [0,5] Tendency to repeat the previous Stage 1 choice.
    - trans_prob_base: [0,1] Baseline perceived probability of the common transition.
    - trans_prob_oci_param: [0,1] Effect of OCI on perceived probability (mapped to [-0.5, 0.5]).
    """
    learning_rate, beta, w, stickiness, trans_prob_base, trans_prob_oci_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Map the [0,1] parameter to a slope in [-0.5, 0.5] to allow bidirectional distortion
    slope = trans_prob_oci_param - 0.5
    
    # Calculate subjective transition probability
    # Base is typically around 0.7; OCI shifts this belief
    p_common = trans_prob_base + slope * oci_score
    p_common = np.clip(p_common, 0.0, 1.0)
    
    # Subjective Transition Matrix: T[0,0]=A->X, T[0,1]=A->Y, etc.
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Model-Based: Use subjective transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if prev_action_1 != -1:
            q_net[int(prev_action_1)] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF Update (TD-0)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        prev_action_1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Post-Loss Stickiness
This model investigates if OCI is linked to "compulsive persistence" specifically after negative outcomes. While standard stickiness applies to all trials, this model separates stickiness into "Win-Stay" and "Lose-Stay" components, hypothesizing that OCI specifically modulates the tendency to repeat a choice after receiving no reward (0 coins).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Post-Loss Stickiness.
    
    Hypothesis: OCI scores relate to compulsive repetition of actions specifically 
    after failure (loss/omission of reward). This model separates stickiness 
    based on the previous trial's outcome, with OCI modulating the 'Lose-Stay' tendency.
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF weight.
    - stick_win: [0,5] Stickiness after receiving a reward (Win-Stay).
    - stick_loss_base: [0,5] Baseline stickiness after no reward.
    - stick_loss_oci_param: [0,1] Effect of OCI on post-loss stickiness (scaled).
    """
    learning_rate, beta, w, stick_win, stick_loss_base, stick_loss_oci_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Map parameter to an additive effect. Assuming OCI increases stickiness (positive slope).
    # Scale by 5 to match the range of stickiness parameters.
    stick_loss = stick_loss_base + stick_loss_oci_param * 5.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Outcome-Dependent Stickiness
        if prev_action_1 != -1:
            if prev_reward == 1:
                q_net[int(prev_action_1)] += stick_win
            else:
                q_net[int(prev_action_1)] += stick_loss

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        prev_action_1 = a1
        prev_reward = reward[trial]

    eps = 1e-10
    valid_trials = (action_1 != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Counterfactual Updating
This model tests the hypothesis that OCI is associated with counterfactual thinking ("If I had chosen the other option, what would have happened?"). It posits that participants update the value of the *unchosen* alien assuming an anticorrelated outcome (if chosen gave 0, unchosen would have given 1). OCI modulates the weight of this fictive update.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Counterfactual Updating.
    
    Hypothesis: High OCI scores relate to increased counterfactual processing 
    or "fictitious play". Participants update the unchosen alien's value 
    assuming the outcome would have been the opposite of the chosen alien's reward.
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w: [0,1]
    - stickiness: [0,5]
    - cf_weight_base: [0,1] Baseline weight for counterfactual updating.
    - cf_weight_oci_param: [0,1] Effect of OCI on counterfactual weight.
    """
    learning_rate, beta, w, stickiness, cf_weight_base, cf_weight_oci_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Counterfactual Weight (bounded [0,1])
    # Maps parameter to additive effect
    cf_weight = cf_weight_base + (cf_weight_oci_param - 0.5) * 2.0 * oci_score
    cf_weight = np.clip(cf_weight, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_action_1 != -1:
            q_net[int(prev_action_1)] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Chosen Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Unchosen Counterfactual Update
        # Assume anticorrelation: cf_reward = 1 - actual_reward
        unchosen_a2 = 1 - a2
        cf_reward = 1.0 - reward[trial]
        delta_stage2_cf = cf_reward - q_stage2_mf[state_idx, unchosen_a2]
        
        # Update unchosen with scaled learning rate
        q_stage2_mf[state_idx, unchosen_a2] += (learning_rate * cf_weight) * delta_stage2_cf

        prev_action_1 = a1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```