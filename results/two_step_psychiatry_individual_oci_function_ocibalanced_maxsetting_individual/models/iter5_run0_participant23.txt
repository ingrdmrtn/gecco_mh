Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Surprise Sensitivity
This model hypothesizes that OCI symptoms affect how participants process "surprising" (Rare) transitions. While a pure Model-Based agent ignores the misleading reinforcement from Rare transitions, a Model-Free agent is fooled by them. This model proposes that OCI modulates the learning rate specifically for the first-stage Model-Free update following a Rare transition, potentially reflecting a "gating" mechanism or heightened sensitivity to rule violations.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Surprise Sensitivity Model.

    Hypothesis: OCI symptoms affect how participants learn from 'Rare' transitions.
    High OCI might lead to over-reacting to rare events (treating them as signal rather than noise)
    or under-reacting (rigidity). This model modulates the learning rate for the 
    Stage 1 Model-Free update specifically when a Rare transition occurs.

    Parameters:
    - lr: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Choice perseveration bonus.
    - rare_mult_base: [0, 5] Baseline multiplier for learning rate on Rare transitions.
    - rare_mult_oci: [-5, 5] OCI-dependent modulation of the rare learning rate multiplier.
    """
    lr, beta, w, stickiness, rare_mult_base, rare_mult_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Transition matrix: Action 0 -> State 0 (0.7), Action 1 -> State 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Alien
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Identify if transition was Common or Rare
        # Common: (a1=0 -> s=0) or (a1=1 -> s=1)
        # Rare: (a1=0 -> s=1) or (a1=1 -> s=0)
        is_common = (a1 == s_idx)
        
        # Calculate effective learning rate for Stage 1 update
        if is_common:
            lr_eff = lr
        else:
            # Rare transition
            multiplier = rare_mult_base + rare_mult_oci * oci_score
            # Clamp multiplier to be non-negative to avoid instability
            multiplier = max(0.0, multiplier)
            lr_eff = lr * multiplier

        # Stage 1 MF update (TD)
        # Using the value of the chosen option in stage 2 (SARSA-like logic often used in hybrid models)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_eff * delta_stage1

        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    
    return log_loss
```

### Model 2: OCI-Modulated Counterfactual Learning
This model hypothesizes that OCI participants engage in increased "checking" or counterfactual reasoning. When they observe the outcome of a chosen alien, they also update the value of the *unchosen* alien on the same planet, assuming an anti-correlated reward structure (if one has gold, the other likely doesn't). OCI modulates the strength of this counterfactual update.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Counterfactual Learning Model.

    Hypothesis: OCI participants may engage in more 'checking' or counterfactual thinking.
    This model assumes that when an outcome is received for the chosen alien, the participant 
    also updates the value of the unchosen alien, assuming an anti-correlated reward structure.
    OCI modulates the weight of this counterfactual update.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Choice perseveration.
    - cf_weight_base: [0, 1] Baseline weight for counterfactual updating.
    - cf_weight_oci: [-1, 1] OCI modulation of counterfactual weight.
    """
    lr, beta, w, stickiness, cf_weight_base, cf_weight_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[a1]

        # Stage 2
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[a2]

        # Updates
        # Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

        # Stage 2 Chosen
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 2 Unchosen (Counterfactual)
        cf_w = cf_weight_base + cf_weight_oci * oci_score
        # Clamp cf_w to [0, 1] to represent degree of belief in anti-correlation
        cf_w = np.clip(cf_w, 0.0, 1.0)
        
        a2_unchosen = 1 - a2
        # Assume anti-correlated: if r=1, other is 0. If r=0, other is 1.
        r_cf = 1.0 - r
        delta_cf = r_cf - q_stage2_mf[s_idx, a2_unchosen]
        q_stage2_mf[s_idx, a2_unchosen] += lr * cf_w * delta_cf

        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    
    return log_loss
```

### Model 3: OCI-Modulated Uncertainty Bonus
This model links OCI to intolerance of uncertainty. It adds an exploration bonus term to the value of each option, inversely proportional to the square root of the number of times that option has been visited. OCI modulates the magnitude and sign of this bonus, allowing for either uncertainty avoidance (negative bonus, safety-seeking) or uncertainty resolution (positive bonus).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Uncertainty Bonus Model.

    Hypothesis: OCI symptoms relate to intolerance of uncertainty. This model includes an 
    exploration bonus based on the inverse visit count of states/actions. 
    OCI modulates the magnitude and sign of this bonus (e.g., uncertainty avoidance vs seeking).

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Choice perseveration.
    - bonus_base: [-2, 2] Baseline exploration bonus weight.
    - bonus_oci: [-2, 2] OCI modulation of exploration bonus.
    """
    lr, beta, w, stickiness, bonus_base, bonus_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Visit counts for uncertainty calculation
    counts_1 = np.zeros(2)
    counts_2 = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Calculate Bonus Weight
        phi = bonus_base + bonus_oci * oci_score

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add uncertainty bonus: phi * (1/sqrt(N+1))
        unc_1 = 1.0 / np.sqrt(counts_1 + 1.0)
        q_net_bonus = q_net + phi * unc_1
        
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta * q_net_bonus + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        unc_2 = 1.0 / np.sqrt(counts_2[s_idx] + 1.0)
        q_stage2_bonus = q_stage2_mf[s_idx] + phi * unc_2
        
        logits_2 = beta * q_stage2_bonus
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Update counts
        counts_1[a1] += 1
        counts_2[s_idx, a2] += 1
        
        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    
    return log_loss
```