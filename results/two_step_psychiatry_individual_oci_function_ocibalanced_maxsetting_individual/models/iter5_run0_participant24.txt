Here are the three proposed cognitive models.

### Model 1: Subjective Transition Belief Model
This model hypothesizes that OCI symptoms distort the participant's internal model of the environment's transition structure. While the true common transition probability is 0.7, this model allows the participant's belief to deviate based on their OCI score (e.g., believing transitions are more deterministic or more random), which specifically biases the Model-Based value calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Belief Model.
    
    Hypothesis: OCI modulates the 'Subjective Transition Probability' used in 
    Model-Based planning. High OCI may lead to beliefs that transitions are 
    more deterministic (rigid) or more random (uncertainty), deviating from 
    the true 0.7 probability.
    
    Parameters:
    - learning_rate: [0, 1] Value updating speed.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based values (vs Model-Free).
    - trans_base: [0, 1] Baseline belief of the 'common' transition probability.
    - trans_oci_slope: [-1, 1] Effect of OCI on transition belief.
    """
    learning_rate, beta, w, trans_base, trans_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate subjective transition probability, clipped to [0, 1]
    # We assume the structure is symmetric (A->X is common, U->Y is common)
    subj_trans_prob = trans_base + (trans_oci_slope * participant_oci)
    subj_trans_prob = np.clip(subj_trans_prob, 0.0, 1.0)
    
    # Subjective transition matrix based on OCI-modulated belief
    # Row 0: Space A -> [Prob X, Prob Y]
    # Row 1: Space U -> [Prob X, Prob Y]
    transition_matrix = np.array([
        [subj_trans_prob, 1.0 - subj_trans_prob], 
        [1.0 - subj_trans_prob, subj_trans_prob]
    ])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage using subjective transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Net Value
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # TD(0) update for Stage 1 (Model-Free)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    # Sum negative log likelihoods only for valid trials
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Probability Mixture Strategy Model
Instead of mixing the *values* (Q-values) of the Model-Based and Model-Free systems, this model mixes their *choice probabilities*. This implies that on any given trial, the participant probabilistically selects a "strategy" (MB or MF) to execute. OCI modulates the probability of selecting the Model-Based strategy.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Probability Mixture Strategy Model.
    
    Hypothesis: Instead of integrating values (Hybrid MB/MF), the participant
    switches between using a Model-Based strategy and a Model-Free strategy.
    OCI modulates the probability 'w_prob' of choosing the Model-Based strategy.
    
    Parameters:
    - learning_rate: [0, 1] Value updating speed.
    - beta: [0, 10] Inverse temperature (shared by both strategies).
    - w_prob_base: [0, 1] Baseline probability of using MB strategy.
    - w_prob_slope: [-1, 1] Effect of OCI on MB strategy probability.
    """
    learning_rate, beta, w_prob_base, w_prob_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate probability of using Model-Based strategy
    w_prob = w_prob_base + (w_prob_slope * participant_oci)
    w_prob = np.clip(w_prob, 0.0, 1.0)
    
    # Fixed transition matrix (Standard)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        
        # 1. Model-Based Probability Distribution
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        exp_mb = np.exp(beta * q_stage1_mb)
        probs_mb = exp_mb / np.sum(exp_mb)
        
        # 2. Model-Free Probability Distribution
        exp_mf = np.exp(beta * q_stage1_mf)
        probs_mf = exp_mf / np.sum(exp_mf)
        
        # 3. Mixture of Probabilities
        probs_1 = (w_prob * probs_mb) + ((1 - w_prob) * probs_mf)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Stage 1 Beta Modulation Model
This model posits that OCI specifically affects the exploration-exploitation trade-off (inverse temperature `beta`) in the first stage of the task (Spaceship choice), which involves structural uncertainty, while the second stage (Alien choice) operates with a separate, fixed `beta`.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 1 Beta Modulation Model.
    
    Hypothesis: OCI affects decision noise/exploration specifically in the 
    first stage (planning/transition stage) differently than the second stage.
    We separate Beta for Stage 1 and Stage 2, with Stage 1 Beta modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Value updating speed.
    - w: [0, 1] Model-Based weight.
    - beta_2: [0, 10] Fixed inverse temperature for Stage 2 choice.
    - beta_1_base: [0, 10] Baseline inverse temperature for Stage 1.
    - beta_1_slope: [-5, 5] Effect of OCI on Stage 1 Beta.
    """
    learning_rate, w, beta_2, beta_1_base, beta_1_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate Stage 1 Beta, clipped to reasonable bounds [0, 20]
    beta_1 = beta_1_base + (beta_1_slope * participant_oci)
    beta_1 = np.clip(beta_1, 0.0, 20.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Hybrid Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax using OCI-modulated beta_1
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Softmax using fixed beta_2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```