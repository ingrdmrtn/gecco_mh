Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid RL with OCI-Modulated Mixing Weight ($w$)
This model assumes the participant uses a hybrid strategy combining Model-Based (MB) and Model-Free (MF) reinforcement learning. The core hypothesis is that the balance between these two systems (the mixing weight $w$) is influenced by the participant's OCI score. Specifically, higher OCI might correlate with a stronger reliance on habitual (MF) control or rigid rule-following (MB), shifting the balance.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL where the mixing weight 'w' is modulated by OCI.
    
    The parameter 'w' determines the balance between Model-Based (goal-directed) and 
    Model-Free (habitual) control in the first stage.
    w = 1 implies pure Model-Based.
    w = 0 implies pure Model-Free.
    Here, w is a logistic function of the OCI score to keep it bounded [0, 1].
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for updating value estimates.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w_intercept: [0, 10] - Baseline logit intercept for the mixing weight.
    w_slope: [-5, 5] - Slope determining how OCI affects the mixing weight.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w using a logistic function to ensure it stays in [0, 1]
    # w_logit = w_intercept + w_slope * oci_score
    # w = 1 / (1 + np.exp(-w_logit)) 
    # Simplified linear approximation clamped for stability given bounds often used in simple fits:
    w = w_intercept + w_slope * oci_score
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix as per task description
    # Spaceship A (0) -> Planet X (0) w/ 0.7 prob
    # Spaceship U (1) -> Planet Y (1) w/ 0.7 prob
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)       # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2))  # Model-free values for stage 2 (2 planets, 2 aliens)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value available at each planet
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        if s_idx != -1:
            # Stage 2 is purely model-free (choosing aliens)
            exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                r = reward[trial]
                
                # Prediction errors
                # Stage 2 PE
                delta_stage2 = r - q_stage2_mf[s_idx, a2]
                q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
                
                # Stage 1 PE (Model-Free update)
                if a1 != -1:
                    # TD(0) update: value of chosen spaceship moves toward value of state arrived at
                    # Note: often in 2-step tasks, Q(s1, a1) updates towards Q(s2, a2) or just reward.
                    # Standard SARSA-like update for MF path:
                    delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
                    q_stage1_mf[a1] += learning_rate * delta_stage1
                    
                    # Alternatively, some models update stage 1 with the reward directly (TD(1))
                    # q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free RL with OCI-Dependent Learning Rates for Positive/Negative Outcomes
This model posits that OCI symptoms affect how participants learn from success versus failure. Individuals with higher compulsivity might be hypersensitive to negative feedback (avoiding punishment) or rigid in the face of positive feedback. This model splits the learning rate into positive and negative components, with the negative learning rate modulated by OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with differential learning rates for positive/negative prediction errors.
    The learning rate for negative prediction errors is modulated by OCI.
    
    Hypothesis: High OCI might lead to over-correction or "stickiness" after failures.
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors (reward > expectation).
    lr_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    oci_neg_sensitivity: [-1, 1] - Modulates how OCI affects the negative learning rate.
    """
    lr_pos, lr_neg_base, beta, oci_neg_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # We clip to ensure it stays valid [0, 1]
    lr_neg = lr_neg_base + (oci_neg_sensitivity * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                r = reward[trial]
                
                # --- Learning Stage 2 ---
                pe2 = r - q_stage2[s_idx, a2]
                alpha2 = lr_pos if pe2 >= 0 else lr_neg
                q_stage2[s_idx, a2] += alpha2 * pe2
                
                # --- Learning Stage 1 ---
                if a1 != -1:
                    # TD(1) style update: Stage 1 value updates directly from reward
                    # (Common simplification in 2-step analyses to capture model-free behavior)
                    pe1 = r - q_stage1[a1]
                    alpha1 = lr_pos if pe1 >= 0 else lr_neg
                    q_stage1[a1] += alpha1 * pe1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid RL with OCI-Modulated Choice Randomness (Noise)
This model investigates whether OCI is associated with decision noise or "temperature". Instead of altering the learning mechanism or the balance of strategies, this model suggests OCI affects the consistency of choices. A high OCI score might correlate with lower `beta` (more exploration/randomness) due to uncertainty, or higher `beta` (more deterministic/rigid behavior).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the inverse temperature (beta) is modulated by OCI.
    
    This tests if OCI is linked to the stochasticity of choice.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    w: [0, 1] - Mixing weight (fixed for this model, not OCI dependent).
    beta_base: [0, 10] - Baseline inverse temperature.
    oci_beta_factor: [-5, 5] - How OCI scales the beta parameter.
    """
    learning_rate, w, beta_base, oci_beta_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # beta = beta_base * exp(oci_factor * oci) is one way, 
    # but linear is safer for simple bounds: beta = beta_base + factor * oci
    beta_eff = beta_base + (oci_beta_factor * oci_score)
    # Ensure beta stays non-negative and within reasonable bounds
    beta_eff = np.clip(beta_eff, 0.0, 20.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        if s_idx != -1:
            exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                r = reward[trial]
                
                # Update Stage 2
                delta_stage2 = r - q_stage2_mf[s_idx, a2]
                q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
                
                # Update Stage 1 (Model-Free)
                if a1 != -1:
                    # Using the simpler TD(1) update often found in these analyses
                    delta_stage1 = r - q_stage1_mf[a1]
                    q_stage1_mf[a1] += learning_rate * delta_stage1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```