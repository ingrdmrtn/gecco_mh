Here are three new cognitive models for the two-step decision task, incorporating the OCI score to explain individual variability in distinct ways.

### Model 1: OCI-Modulated Beta Asymmetry (Win vs Loss)
This model hypothesizes that OCI affects the exploration-exploitation balance (inverse temperature, $\beta$) specifically after unrewarded trials (losses). High OCI might lead to "freezing" or rigid behavior (high $\beta$) after a loss, or conversely, erratic behavior. This contrasts with a separate $\beta$ for rewarded trials.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Inverse Temperature Asymmetry (Win vs Loss).
    
    Hypothesis: OCI affects the exploration-exploitation balance (beta) specifically after 
    unrewarded trials (losses). High OCI might lead to 'freezing' or rigidity (high beta) 
    after a loss, distinct from behavior after a win.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - beta_win: [0, 10] Inverse temperature after a rewarded trial.
    - beta_loss_base: [0, 10] Baseline inverse temperature after an unrewarded trial.
    - beta_loss_oci: [-5, 5] Modulation of beta_loss by OCI score.
      Effective beta_loss = beta_loss_base + beta_loss_oci * oci.
    """
    learning_rate, w, beta_win, beta_loss_base, beta_loss_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta for loss trials
    beta_loss = beta_loss_base + (beta_loss_oci * current_oci)
    # Ensure beta is non-negative
    beta_loss = np.maximum(0.0, beta_loss)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_reward = 1 # Initialize as if previous was a win (start confident)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Determine current beta based on previous outcome
        if prev_reward == 1:
            current_beta = beta_win
        else:
            current_beta = beta_loss
            
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        logits_1 = current_beta * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = current_beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        # Stage 1 Update (SARSA-like for MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Model-Based Weighting (Common vs Rare)
This model hypothesizes that OCI affects the reliance on Model-Based planning ($w$) specifically after experiencing a "rare" transition. High OCI participants might react to the structural surprise of a rare transition by either abandoning the model (becoming more Model-Free) or over-relying on it, compared to common transitions.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Model-Based Weighting (Common vs Rare Transitions).
    
    Hypothesis: OCI affects the reliance on Model-Based planning specifically after 
    experiencing a 'rare' transition. High OCI might lead to mistrust of the model 
    (lower w) or hyper-correction (higher w) when the world model is violated.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_common: [0, 1] MB weight after a common transition.
    - w_rare_base: [0, 1] Baseline MB weight after a rare transition.
    - w_rare_oci: [-1, 1] Modulation of w_rare by OCI.
      Effective w_rare = w_rare_base + w_rare_oci * oci.
    """
    learning_rate, beta, w_common, w_rare_base, w_rare_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective w for rare transitions
    w_rare = w_rare_base + (w_rare_oci * current_oci)
    w_rare = np.clip(w_rare, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous transition type: Common=False, Rare=True
    prev_transition_rare = False
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Determine w based on previous transition
        if prev_transition_rare:
            current_w = w_rare
        else:
            current_w = w_common
            
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = current_w * q_stage1_mb + (1 - current_w) * q_stage1_mf
        
        logits_1 = beta * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        # Determine transition type for next trial
        # Action 0 -> State 0 (Common), Action 0 -> State 1 (Rare)
        # Action 1 -> State 1 (Common), Action 1 -> State 0 (Rare)
        if a1 == s_idx:
            prev_transition_rare = False
        else:
            prev_transition_rare = True

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Positive Learning Rate
This model hypothesizes that OCI affects the sensitivity to positive rewards (gains) specifically, while the learning rate for lack of reward (losses) remains baseline. This tests if OCI is related to altered processing of "success" signals.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Positive Learning Rate.
    
    Hypothesis: OCI affects the sensitivity to positive rewards (learning rate for gains),
    while the learning rate for lack of reward (losses) remains baseline.
    
    Parameters:
    - lr_neg: [0, 1] Learning rate for unrewarded trials (R=0).
    - lr_pos_base: [0, 1] Baseline learning rate for rewarded trials (R=1).
    - lr_pos_oci: [-1, 1] Modulation of positive learning rate by OCI.
      Effective lr_pos = lr_pos_base + lr_pos_oci * oci.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    """
    lr_neg, lr_pos_base, lr_pos_oci, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective positive learning rate
    lr_pos = lr_pos_base + (lr_pos_oci * current_oci)
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Determine which learning rate to use
        if r == 1:
            current_lr = lr_pos
        else:
            current_lr = lr_neg
            
        # Learning
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        q_stage1_mf[a1] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```