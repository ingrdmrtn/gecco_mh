Here are three cognitive models expressed as Python functions that incorporate the OCI score into the decision-making process.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Mixing Weight (Goal-Directed Deficit).

    Hypothesis:
    High OCI scores are associated with a deficit in goal-directed (Model-Based) control.
    This model linearly modulates the mixing weight `w` such that higher OCI scores 
    reduce the influence of the Model-Based system, leading to more Model-Free behavior.

    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w_base: [0, 1] Baseline mixing weight (representing w when OCI=0).
    - w_oci_dec: [0, 1] Factor by which OCI reduces w. 
                 Effective w = w_base - (w_oci_dec * oci).
    """
    lr, beta, w_base, w_oci_dec = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective mixing weight, clipped between 0 and 1
    # Higher OCI reduces w, shifting control towards Model-Free (0)
    w_eff = w_base - (w_oci_dec * oci_score)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value using OCI-modulated weight
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Update Stage 1 MF value using Stage 2 Q-value (TD(0))
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Update Stage 2 MF value using Reward
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Choice Stickiness (Compulsive Repetition).

    Hypothesis:
    Higher OCI scores lead to increased perseveration (stickiness). This model adds 
    a "stickiness" bonus to the previously chosen action at Stage 1, where the 
    magnitude of this bonus is directly scaled by the participant's OCI score.
    This captures the 'compulsion to repeat' independent of reward.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (Model-Based vs Model-Free).
    - stick_oci_scale: [0, 5] Scaling factor for stickiness based on OCI.
                       Stickiness Bonus = stick_oci_scale * oci.
    """
    lr, beta, w, stick_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate stickiness bonus proportional to OCI
    stickiness_bonus = stick_oci_scale * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-value of the previous choice
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness_bonus
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Update previous action tracker
        prev_a1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Inverse Temperature (Anxiety-Driven Rigidity).

    Hypothesis:
    OCI is associated with anxiety and rigid behavior. This model posits that 
    the inverse temperature parameter (beta), which controls the determinism of choices 
    (exploration vs exploitation), is increased by higher OCI scores. 
    High OCI leads to more deterministic (rigid) choices and less exploration.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Mixing weight.
    - beta_oci_slope: [0, 5] Slope of OCI effect on beta.
                      Effective Beta = beta_base * (1 + beta_oci_slope * oci).
    """
    lr, beta_base, w, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta: increases with OCI
    beta_eff = beta_base * (1.0 + beta_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use effective beta for Stage 1
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        # Use effective beta here as well for global rigidity
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood
```