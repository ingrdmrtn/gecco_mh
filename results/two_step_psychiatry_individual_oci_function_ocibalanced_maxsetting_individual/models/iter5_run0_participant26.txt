Here are three new cognitive models based on the participant's data and OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Stage 1 Learning Rate.
    
    Hypothesis: 
    Participants with higher OCI scores may update their high-level spaceship 
    preferences (Stage 1) differently than their low-level alien preferences (Stage 2).
    This model assumes the OCI score scales the learning rate specifically for the 
    first stage decision, reflecting a potential disparity in how structural choices 
    vs. immediate reward choices are learned.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate (applied to Stage 2).
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weighting between Model-Based and Model-Free values (0=MF, 1=MB).
    - oci_lr_scale: [-1, 2] Factor scaling the Stage 1 learning rate by OCI.
      alpha_stage1 = lr_base * (1 + oci * oci_lr_scale)
    """
    lr_base, beta, w, oci_lr_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate stage 1 specific learning rate
    # We clip to ensure it stays within valid bounds [0, 1]
    alpha_stage1 = lr_base * (1.0 + current_oci * oci_lr_scale)
    alpha_stage1 = np.clip(alpha_stage1, 0.0, 1.0)
    
    # Stage 2 uses the base learning rate
    alpha_stage2 = lr_base

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Action Selection Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: continue # Handle missing data
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        if s_idx == -1: continue
        
        # Stage 2 is purely Model-Free
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 (Alien choice) using base learning rate
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += alpha_stage2 * delta2
        
        # Update Stage 1 (Spaceship choice) using OCI-modulated learning rate
        # TD(0) update: target is the value of the state we landed in (max Q of that state)
        # Note: In standard hybrid models, MF Stage 1 is updated by Q_stage2_MF of chosen state - Q_stage1
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_stage1 * delta1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Transition Belief (MB Distortion).
    
    Hypothesis: 
    Participants with high OCI may have distorted beliefs about the transition probabilities.
    Higher OCI might correlate with a need for certainty, causing the participant 
    to perceive the 'common' transition (0.7) as more deterministic (e.g., 0.9) 
    or less deterministic than reality, affecting their Model-Based planning.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - transition_bias: [-0.5, 0.5] Distortion factor applied to the transition probability.
      p_common_belief = 0.7 + (transition_bias * oci)
    """
    learning_rate, beta, w, transition_bias = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate subjective transition probability
    # True probability is 0.7. We distort it by OCI.
    p_common = 0.7 + (transition_bias * current_oci)
    p_common = np.clip(p_common, 0.01, 0.99) # Ensure valid probability
    
    # Construct subjective transition matrix
    # Row 0: Spaceship A -> [Planet X (Common), Planet Y (Rare)]
    # Row 1: Spaceship U -> [Planet X (Rare), Planet Y (Common)]
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the distorted transition matrix for MB calculation
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: continue
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        if s_idx == -1: continue
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Reward Sensitivity.
    
    Hypothesis: 
    OCI modulates the subjective valuation of the reward outcome. 
    High OCI scores might be associated with hypersensitivity to positive outcomes 
    (or lack thereof), effectively scaling the reward signal magnitude used 
    in the prediction error calculation.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - reward_sensitivity: [0, 5] Scaling factor for the reward based on OCI.
      effective_reward = reward * (1 + oci * reward_sensitivity)
    """
    learning_rate, beta, w, reward_sensitivity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: continue
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        if s_idx == -1: continue
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Calculate effective reward
        # This scales the feedback signal.
        # If sensitivity is 0, r_eff = r. If positive, r_eff > r (for r=1).
        r_eff = r * (1.0 + current_oci * reward_sensitivity)
        
        delta2 = r_eff - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        # For Stage 1 MF update, we use the updated Q-value from stage 2
        # which now implicitly contains the scaled reward magnitude history
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```