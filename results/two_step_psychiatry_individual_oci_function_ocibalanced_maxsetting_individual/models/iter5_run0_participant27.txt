Here are the 3 proposed cognitive models.

### Model 1: Hybrid MB/MF with OCI-Modulated Eligibility Traces
This model hypothesizes that OCI affects the "credit assignment" process in the brain. Specifically, it tests whether OCI modulates the eligibility trace ($\lambda$), which governs how much the outcome at the second stage (reward) directly updates the value of the first-stage choice. A higher $\lambda$ implies the participant treats the two steps as a single event, while a lower $\lambda$ implies strictly sequential TD learning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated Eligibility Traces.
    
    Hypothesis: OCI affects the eligibility trace parameter (lambda). High OCI might
    lead to 'smearing' of reinforcement across the trial steps (higher lambda), 
    or strict sequential processing (lower lambda).
    
    Parameters:
    - lr: Learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w_mix: Balance between MB (1) and MF (0) control [0, 1]
    - lambda_base: Baseline eligibility trace parameter [0, 1]
    - lambda_oci_slope: Effect of OCI on lambda [-1, 1]
    """
    lr, beta, w_mix, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated lambda
    lambda_param = lambda_base + (lambda_oci_slope * current_oci)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)

    # Transition matrix (fixed for this task structure)
    # Row 0: Spaceship A -> 70% Planet X (0), 30% Planet Y (1)
    # Row 1: Spaceship B -> 30% Planet X (0), 70% Planet Y (1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_mf_s1 = np.zeros(2)      # Model-Free values for Stage 1 (Spaceships)
    q_mf_s2 = np.zeros((2, 2)) # Model-Free values for Stage 2 (Planets/Aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- STAGE 1 CHOICE ---
        # Model-Based Value Calculation
        # V_MB(state) = max_a Q_MF_s2(state, a)
        max_q_s2 = np.max(q_mf_s2, axis=1) # Max value of each planet
        q_mb_s1 = transition_matrix @ max_q_s2
        
        # Hybrid Value
        q_net_s1 = (w_mix * q_mb_s1) + ((1 - w_mix) * q_mf_s1)
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 CHOICE ---
        # Softmax Policy Stage 2 (Purely Model-Free at this stage)
        exp_q2 = np.exp(beta * q_mf_s2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATES ---
        # Prediction Errors
        pe_1 = q_mf_s2[s_idx, a2] - q_mf_s1[a1] # Stage 1 PE
        pe_2 = r - q_mf_s2[s_idx, a2]           # Stage 2 PE
        
        # Update Stage 2 values (Standard Q-Learning)
        q_mf_s2[s_idx, a2] += lr * pe_2
        
        # Update Stage 1 values (TD(lambda))
        # Update includes the immediate Stage 1 PE, plus a portion of the Stage 2 PE
        # scaled by lambda.
        q_mf_s1[a1] += lr * pe_1 + (lr * lambda_param * pe_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Learning Rate Hybrid with OCI-penalized Model-Based Control
This model separates the learning rates for Stage 1 (choosing spaceships) and Stage 2 (choosing aliens). It hypothesizes that the participant might learn the immediate reward associations (Stage 2) at a different rate than the abstract transition values (Stage 1). Furthermore, it posits that OCI specifically degrades the Model-Based contribution ($w$), representing a reliance on habit over complex planning in high-symptom individuals.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Split Learning Rates and OCI-penalized Model-Based Weight.
    
    Hypothesis: 
    1. Learning happens at different speeds for abstract choices (Stage 1) vs 
       concrete outcomes (Stage 2).
    2. OCI acts as a penalty on the Model-Based weight (w), reducing goal-directed planning.
    
    Parameters:
    - lr_s1: Learning rate for Stage 1 choices [0, 1]
    - lr_s2: Learning rate for Stage 2 choices [0, 1]
    - beta: Inverse temperature [0, 10]
    - w_base: Baseline Model-Based weight [0, 1]
    - w_oci_penalty: Reduction in w per unit of OCI [0, 1]
    """
    lr_s1, lr_s2, beta, w_base, w_oci_penalty = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated w
    w_mix = w_base - (w_oci_penalty * current_oci)
    w_mix = np.clip(w_mix, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- STAGE 1 ---
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = (w_mix * q_mb_s1) + ((1 - w_mix) * q_mf_s1)
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 ---
        exp_q2 = np.exp(beta * q_mf_s2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATES ---
        # Update Stage 2 using its specific learning rate
        pe_2 = r - q_mf_s2[s_idx, a2]
        q_mf_s2[s_idx, a2] += lr_s2 * pe_2
        
        # Update Stage 1 using its specific learning rate (SARSA-style update from Stage 2 Q-value)
        # Note: Using Q-value of chosen state/action as the target for Stage 1
        pe_1 = q_mf_s2[s_idx, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr_s1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with OCI-Modulated "Win-Stay" Bonus
This model investigates if OCI drives a specific type of perseveration: "Win-Stay". Unlike general stickiness (repeating any choice), this model proposes that OCI amplifies the tendency to repeat a choice *only if it was rewarded*, effectively adding a "compulsive bonus" to successful actions that overrides the calculated value.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF with OCI-modulated Reward-Dependent Stickiness (Win-Stay).
    
    Hypothesis: OCI increases the 'Win-Stay' heuristic. If a choice resulted in 
    a reward on the previous trial, an additional 'bonus' is added to that choice's 
    logit in the next trial, proportional to the OCI score.
    
    Parameters:
    - lr: Learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w_mix: Model-Based weight [0, 1]
    - win_stay_base: Baseline bonus for repeating a rewarded action [0, 5]
    - win_stay_oci: Additional bonus scaled by OCI [0, 5]
    """
    lr, beta, w_mix, win_stay_base, win_stay_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate total Win-Stay bonus
    win_stay_bonus = win_stay_base + (win_stay_oci * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_r = 0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            prev_a1 = -1 # Reset history on missing data
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- STAGE 1 ---
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = (w_mix * q_mb_s1) + ((1 - w_mix) * q_mf_s1)
        
        # Calculate logits (beta * Q)
        logits_1 = beta * q_net_s1
        
        # Apply Win-Stay Bonus
        # If the previous trial was rewarded, boost the logit of the previous action
        if prev_a1 != -1 and prev_r == 1.0:
            logits_1[prev_a1] += win_stay_bonus
            
        # Softmax
        # Stabilize exp for numerical safety
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- STAGE 2 ---
        exp_q2 = np.exp(beta * q_mf_s2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- UPDATES ---
        pe_2 = r - q_mf_s2[s_idx, a2]
        q_mf_s2[s_idx, a2] += lr * pe_2
        
        pe_1 = q_mf_s2[s_idx, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * pe_1
        
        # Store history for next trial
        prev_a1 = a1
        prev_r = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```