Here are the three cognitive models based on the participant's data and OCI score.

### Model 1: Separate Stage Learning Rates with OCI-Modulated Stage 2 Learning
This model hypothesizes that the participant learns the value of spaceships (Stage 1) and aliens (Stage 2) at different rates. Specifically, it proposes that the OCI score modulates the learning rate for the second stage (`lr_s2`), reflecting a potential sensitivity to the immediate outcomes (coins) characteristic of compulsive monitoring, while the first stage learning rate remains independent.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with separate learning rates for Stage 1 and Stage 2.
    OCI score modulates the Stage 2 learning rate (Alien reward learning).
    
    Parameters:
    lr_s1: [0,1] - Learning rate for Stage 1 (Spaceship values).
    lr_s2_base: [0,1] - Base learning rate for Stage 2, scaled by OCI.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    stickiness: [0,5] - Perseveration bonus for repeated Stage 1 choices.
    lam: [0,1] - Eligibility trace parameter (credit assignment to Stage 1 from Stage 2).
    """
    lr_s1, lr_s2_base, beta, w, stickiness, lam = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate Stage 2 learning rate by OCI
    # Hypothesis: Higher OCI leads to higher sensitivity/updating for immediate rewards
    lr_s2 = lr_s2_base * (1.0 + oci_score)
    # Ensure bounds
    lr_s2 = min(max(lr_s2, 0.0), 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (Planet x Alien)

    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Decision ---
        # Model-Based Value: Expected max value of next stage given transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Net Value: Weighted sum of MB and MF values
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Apply Stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Softmax Choice Probability Stage 1
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision ---
        # Softmax Choice Probability Stage 2 (based on MF values of current state)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning / Updating ---
        # Prediction Error Stage 1 (TD error)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1

        # Prediction Error Stage 2 (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2

        # Eligibility Trace: Update Stage 1 based on Stage 2 error
        q_stage1_mf[a1] += lr_s1 * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates with OCI-Modulated Negative Learning
This model tests the hypothesis that OCI symptoms correlate with an altered sensitivity to negative outcomes (omission of reward). It uses separate learning rates for positive and negative prediction errors, where the negative learning rate is modulated by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with asymmetric learning rates for positive and negative errors.
    OCI score modulates the Negative Learning Rate (sensitivity to 0 reward).
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg_base: [0,1] - Base learning rate for negative prediction errors, scaled by OCI.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    stickiness: [0,5] - Perseveration bonus.
    lam: [0,1] - Eligibility trace.
    """
    lr_pos, lr_neg_base, beta, w, stickiness, lam = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate Negative Learning Rate by OCI
    # Hypothesis: Higher OCI leads to stronger updating on failures (0 coins)
    lr_neg = lr_neg_base * (1.0 + oci_score)
    lr_neg = min(max(lr_neg, 0.0), 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Asymmetric Updating ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2

        # Eligibility Trace Update (using lr_1 to be consistent with Stage 1 update)
        q_stage1_mf[a1] += lr_1 * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Inverse Temperature
This model proposes that the OCI score affects the exploration-exploitation trade-off globally. It suggests that individuals with higher OCI scores might exhibit higher `beta` (more deterministic/rigid choice behavior) or lower `beta` (more anxiety-driven randomness). The model scales the base inverse temperature by the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates the Inverse Temperature (Beta).
    
    Parameters:
    lr: [0,1] - Global learning rate.
    beta_base: [0,10] - Base inverse temperature, scaled by OCI.
    w: [0,1] - Mixing weight.
    lam: [0,1] - Eligibility trace.
    stickiness: [0,5] - Perseveration bonus.
    """
    lr, beta_base, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate Beta by OCI
    # Hypothesis: OCI affects the rigidity of choices (higher beta = more rigid/deterministic)
    beta = beta_base * (1.0 + oci_score)
    # Ensure reasonable upper bound if necessary, though beta can be high
    beta = min(beta, 20.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        # Eligibility Trace
        q_stage1_mf[a1] += lr * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```