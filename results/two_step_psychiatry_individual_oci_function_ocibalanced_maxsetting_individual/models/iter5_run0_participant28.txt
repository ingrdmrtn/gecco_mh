Here are three new cognitive models that incorporate the OCI score to explain the participant's behavior in the two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight (`cognitive_model1`)
This model assumes the participant uses a hybrid of Model-Free (MF) and Model-Based (MB) reinforcement learning. The key hypothesis here is that the balance between these two systems (`w`) is not static but is influenced by the participant's obsessive-compulsive traits. Specifically, higher OCI scores might correlate with a more rigid, habit-based (MF) strategy or an over-reliance on a model-based strategy. This model tests a linear relationship between OCI and the weighting parameter `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MF/MB learner where the weighting parameter 'w' is a linear function of OCI.
    
    This tests if the balance between goal-directed (MB) and habitual (MF) control
    is a function of obsessive-compulsive symptom severity.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_intercept: [0, 1] Baseline model-based weight (at OCI=0).
    w_slope: [0, 1] Sensitivity of 'w' to OCI score.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w, bounded between 0 and 1
    # We allow the slope to be positive or negative implicitly by how the optimizer fits it,
    # but here we implement a simple additive model. 
    # To allow bidirectional influence with positive parameters, we can center OCI or just use additive.
    # Here: w = w_intercept + (w_slope * (oci - 0.5)). 
    # This centers the effect around the middle of the OCI range.
    raw_w = w_intercept + w_slope * (oci_score - 0.5)
    w = np.clip(raw_w, 0.0, 1.0)

    # Fixed transition matrix for the task structure
    # A (0) -> X (0) common (0.7)
    # U (1) -> Y (1) common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        # V_MB(s1) = sum(P(s2|s1, a1) * max_a2 Q_MF(s2, a2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 1 MF update (SARSA-like or TD(0) to stage 2 value)
        # Note: In standard two-step, Stage 1 MF often updates towards Q_stage2(s, a2)
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1
        
        # Stage 2 MF update
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        # Additional TD(lambda) update for stage 1 based on stage 2 RPE is common 
        # but omitted here to isolate the specific hybrid w hypothesis cleanly.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with OCI-Modulated Choice Stickiness (`cognitive_model2`)
Obsessive-compulsive traits are often associated with repetitive behaviors or "stickiness" to previous choices, regardless of reward outcomes. This model implements a pure Model-Free learner but adds a choice persistence (stickiness) bonus to the Stage 1 choice. The magnitude of this stickiness is modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Choice Stickiness (Perseveration) modulated by OCI.
    
    This tests if OCI symptoms increase the tendency to repeat the previous Stage 1 choice
    regardless of the outcome (compulsive repetition).
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    stickiness_base: [0, 5] Baseline bonus for repeating the previous choice.
    oci_stick_mod: [0, 5] Modulation of stickiness by OCI.
    """
    learning_rate, beta, stickiness_base, oci_stick_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness
    # Higher OCI -> Higher stickiness
    stickiness = stickiness_base + (oci_stick_mod * oci_score)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous choice for stickiness
    prev_a1 = -1 
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Add stickiness bonus to Q-values temporarily for choice probability calculation
        q_stage1_aug = q_stage1.copy()
        if prev_a1 != -1:
            q_stage1_aug[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_aug)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous choice
        prev_a1 = a1

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Standard TD updates
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Eligibility trace update (credit assignment to Stage 1 from Stage 2 outcome)
        # Using a fixed lambda=1 assumption for pure MF logic here
        q_stage1[a1] += learning_rate * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates Modulated by OCI (`cognitive_model3`)
Individuals with high anxiety or compulsive traits might process positive and negative feedback differently. This model posits that the OCI score modulates the asymmetry between learning from positive prediction errors (rewards) versus negative prediction errors (omissions). This is applied to the second stage update, which drives the core value learning.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive/negative RPEs,
    where the asymmetry is modulated by OCI.
    
    This tests if OCI symptoms bias learning towards positive or negative outcomes.
    
    Parameters:
    lr_base: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    lambda_param: [0, 1] Eligibility trace parameter.
    oci_asym_bias: [0, 1] Degree to which OCI biases learning rates. 
                      If > 0.5, OCI enhances learning from negative RPEs.
    """
    lr_base, beta, lambda_param, oci_asym_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Define two learning rates based on OCI modulation
    # We construct a bias factor. 
    # If oci_asym_bias is high, OCI increases lr_neg relative to lr_pos
    # Bias is centered around 0.5
    bias_factor = (oci_asym_bias - 0.5) * 2.0 * oci_score # Range roughly -1 to 1 depending on OCI
    
    # Apply bias to base learning rate
    # lr_pos = lr_base * (1 - bias_factor)
    # lr_neg = lr_base * (1 + bias_factor)
    # We clip to ensure valid learning rates
    lr_pos = np.clip(lr_base * (1.0 - bias_factor), 0.01, 1.0)
    lr_neg = np.clip(lr_base * (1.0 + bias_factor), 0.01, 1.0)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Calculate Stage 2 RPE
        delta2 = r - q_stage2[s_idx, a2]
        
        # Select learning rate based on sign of prediction error
        current_lr = lr_pos if delta2 >= 0 else lr_neg
        
        # Update Stage 2
        q_stage2[s_idx, a2] += current_lr * delta2
        
        # Update Stage 1
        # Stage 1 update uses the same LR asymmetry for consistency in this hypothesis
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1] # TD(0) part
        q_stage1[a1] += current_lr * delta1
        
        # Eligibility trace part (TD(lambda))
        q_stage1[a1] += current_lr * lambda_param * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```