Here are three cognitive models that incorporate the OCI score to explain the participant's decision-making behavior, specifically addressing the observed stickiness and potential variations in model-based control and belief structures.

### Model 1: OCI-Modulated Model-Based Weight with Eligibility Trace
This model hypothesizes that higher OCI scores correlate with a reduced reliance on Model-Based (goal-directed) control, shifting behavior towards Model-Free (habitual) control. It includes an eligibility trace parameter ($\lambda$) to allow the Model-Free system to learn from second-stage outcomes, providing a more robust baseline for the habit system.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated MB Weight with Eligibility Trace.
    Hypothesis: OCI reduces the mixing weight 'w', leading to more Model-Free behavior.
    Includes an eligibility trace (lambda) for the MF system.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature (softmax randomness).
    w_base: [0, 1] Baseline Model-Based weight.
    w_oci_damp: [0, 1] Scaling factor for OCI reduction of w.
    stickiness: [0, 5] Choice perseveration bonus.
    lambda_param: [0, 1] Eligibility trace decay (Stage 2 reward effect on Stage 1).
    """
    learning_rate, beta, w_base, w_oci_damp, stickiness, lambda_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w: decreases as OCI increases
    # w represents the weight of the Model-Based system (0=MF, 1=MB)
    w = w_base * (1.0 - w_oci_damp * oci_score)
    # Ensure w stays in bounds
    if w < 0: w = 0.0
    if w > 1: w = 1.0

    # Fixed transition matrix as per task description
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next stage states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Mixture of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace Update (TD(lambda)): Stage 2 reward updates Stage 1 choice
        q_stage1_mf[a1] += learning_rate * lambda_param * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Loss-Chasing Stickiness
This model addresses the participant's tendency to repeat choices even after receiving no reward (0 coins). It hypothesizes that OCI is associated with "loss chasing" or compulsive persistence. Specifically, the stickiness parameter is boosted by the OCI score when the previous trial was unrewarded, making it harder to switch away from a losing option.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Loss-Chasing Stickiness.
    Hypothesis: Stickiness is dynamic. If the previous trial resulted in a loss (0 reward),
    stickiness is increased proportional to OCI, reflecting compulsive persistence.
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF mixing weight.
    stick_base: [0, 5] Baseline stickiness.
    stick_loss_oci: [0, 5] Additional stickiness after a loss, scaled by OCI.
    """
    learning_rate, beta, w, stick_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = 0.0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Dynamic Stickiness Logic
        current_stickiness = stick_base
        if trial > 0 and prev_reward == 0.0:
            # If previous outcome was a loss, increase stickiness based on OCI
            current_stickiness += stick_loss_oci * oci_score
            
        if prev_a1 != -1:
            q_net[prev_a1] += current_stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Belief
This model hypothesizes that high OCI scores distort the internal model of the environment's transition probabilities. While the real transition probability is 0.7, an individual with high OCI might perceive the world as more deterministic (intolerance of uncertainty) or less reliable. This model allows the *subjective* transition probability used in the Model-Based calculation to vary with OCI.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Transition Belief.
    Hypothesis: The internal model of transition probabilities (used for MB calculation)
    is distorted by OCI. High OCI may lead to a belief in more deterministic transitions.
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF mixing weight.
    stickiness: [0, 5] Choice perseveration.
    trans_base: [0.5, 1] Baseline belief of common transition probability.
    trans_oci: [-0.5, 0.5] Modification of transition belief by OCI.
    """
    learning_rate, beta, w, stickiness, trans_base, trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Subjective Transition Matrix
    # The 'real' matrix is [[0.7, 0.3], [0.3, 0.7]]
    # The 'subjective' p is modulated by OCI
    subjective_p = trans_base + trans_oci * oci_score
    
    # Clip to reasonable probability bounds [0.01, 0.99] to avoid log(0) issues in theory,
    # though here we just construct the matrix.
    if subjective_p > 0.99: subjective_p = 0.99
    if subjective_p < 0.5: subjective_p = 0.5 # Assume they identify the 'common' direction correctly at least
    
    # Subjective matrix used for MB valuation
    subjective_trans_matrix = np.array([
        [subjective_p, 1.0 - subjective_p],
        [1.0 - subjective_p, subjective_p]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use SUBJECTIVE matrix for MB calculation
        q_stage1_mb = subjective_trans_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```