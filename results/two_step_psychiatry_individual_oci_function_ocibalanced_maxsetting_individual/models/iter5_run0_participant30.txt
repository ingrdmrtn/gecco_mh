Here are 3 new cognitive models exploring different mechanisms of how Obsessive-Compulsive symptoms (OCI) might influence decision-making in this task.

### Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that individuals with high OCI scores exhibit higher "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward. This reflects the compulsive aspect of OCD, where actions are repeated rigidly. The stickiness parameter is added to the Q-values of the previously chosen action before the softmax step.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Perseveration (Stickiness).
    
    Hypothesis: High OCI leads to increased choice stickiness (tendency to repeat 
    the last action), reflecting compulsive repetition.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - MB/MF balance
    stickiness_base: [0, 5] - Base tendency to repeat choice
    stickiness_oci_slope: [-1, 1] - Modulation of stickiness by OCI
    """
    learning_rate, beta, w, stickiness_base, stickiness_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    stickiness = stickiness_base + (stickiness_oci_slope * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # Update Stage 2 Q-values
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2
            
            # Update Stage 1 MF Q-values (TD(1) style, using actual reward)
            # Standard MF update typically uses the stage 2 value, but simple SARSA/Q-learning often uses r directly if lambda=1
            # Here we use the standard TD(0) for stage 1 based on stage 2 value
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated "Anxiety" (Reward Sensitivity)
This model posits that high OCI is associated with altered sensitivity to outcomes. Specifically, it tests if OCI modulates the subjective utility of the reward. High anxiety might make "winning" (getting a coin) feel more urgent or significant (increasing sensitivity), or conversely, might blunt positive feedback. We model this by scaling the reward `r` before it enters the prediction error calculation.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Reward Sensitivity.
    
    Hypothesis: OCI affects how strongly the participant perceives the reward.
    Higher OCI might amplify the impact of rewards (hyper-sensitivity) or dampen it.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    reward_sens_base: [0, 2] - Base scalar for reward value
    reward_sens_oci_slope: [-1, 1] - Modulation of reward sensitivity by OCI
    """
    learning_rate, beta, w, reward_sens_base, reward_sens_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective reward sensitivity
    reward_sensitivity = reward_sens_base + (reward_sens_oci_slope * oci_score)
    # Ensure sensitivity is non-negative
    reward_sensitivity = max(0.0, reward_sensitivity)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # Scale reward
            effective_reward = r * reward_sensitivity
            
            # Update Stage 2 Q-values
            delta_stage2 = effective_reward - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2
            
            # Update Stage 1 MF Q-values
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Learning Rate Asymmetry (Positive vs Negative)
This model investigates if OCI creates a bias in learning from positive versus negative prediction errors. People with high OCD symptoms might be more sensitive to "errors" (negative outcomes) or failures to receive a reward (0 coins) compared to successes. This model splits the learning rate into two components based on the sign of the prediction error, with OCI modulating the ratio or strength of one relative to the other.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Learning Rate Asymmetry (Pos vs Neg).
    
    Hypothesis: High OCI participants might learn differently from positive 
    outcomes (gains) vs negative outcomes (omissions). OCI modulates the 
    learning rate for negative prediction errors specifically.
    
    Bounds:
    lr_pos: [0, 1] - Learning rate for positive prediction errors
    lr_neg_base: [0, 1] - Base learning rate for negative prediction errors
    lr_neg_oci_mod: [-1, 1] - Modulation of negative LR by OCI
    beta: [0, 10]
    w: [0, 1]
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (lr_neg_oci_mod * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # Update Stage 2 Q-values
            delta_stage2 = r - q_stage2_mf[s, a2]
            current_lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
            q_stage2_mf[s, a2] += current_lr_2 * delta_stage2
            
            # Update Stage 1 MF Q-values
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            current_lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
            q_stage1_mf[a1] += current_lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```