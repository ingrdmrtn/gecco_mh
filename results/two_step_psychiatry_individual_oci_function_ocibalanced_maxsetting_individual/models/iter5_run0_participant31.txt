Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in this task.

### Model 1: OCI-Modulated Learning Rates (Asymmetric Updating)
This model hypothesizes that high OCI scores lead to different learning rates for positive versus negative prediction errors. Individuals with high compulsivity might be more sensitive to "missed" rewards (negative prediction errors) or perhaps over-learn from success. Here, we test if OCI modulates the learning rate specifically for negative outcomes, reflecting a potential fear of failure or error sensitivity common in OCD.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Asymmetric Learning Rates.
    
    This model separates learning rates for positive (lr_pos) and negative (lr_neg) 
    prediction errors. It hypothesizes that the learning rate for negative prediction 
    errors is modulated by the OCI score, reflecting increased sensitivity to 
    worse-than-expected outcomes in high-OCI individuals.
    
    lr_neg = lr_neg_base * (1 + oci * oci_sens)
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors (RPE > 0).
    lr_neg_base: [0, 1] - Base learning rate for negative prediction errors (RPE < 0).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight for MB vs MF control (0=MF, 1=MB).
    oci_sens: [0, 5] - Scaling factor for how much OCI increases negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Modulate negative learning rate by OCI
    lr_neg = lr_neg_base * (1.0 + current_oci * oci_sens)
    # Clamp to ensure valid range [0, 1]
    if lr_neg > 1.0: lr_neg = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial]) 
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha2 * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Choice Stickiness (Perseveration)
This model posits that high OCI is associated with "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of the reward outcome. This is a hallmark of compulsive behavior. The model adds a "stickiness" bonus to the Q-values of the previously chosen action, where the magnitude of this bonus is scaled by the participant's OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Choice Stickiness.
    
    This model assumes that OCI score drives choice perseveration (stickiness).
    A 'stickiness' bonus is added to the Q-value of the action taken in the 
    previous trial. The magnitude of this bonus is determined by a base stickiness 
    plus an OCI-dependent component.
    
    stickiness_bonus = stick_base + (stick_oci_scale * oci)
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight for MB vs MF control.
    stick_base: [0, 5] - Baseline tendency to repeat choices.
    stick_oci_scale: [0, 5] - How much OCI amplifies the stickiness.
    """
    learning_rate, beta, w, stick_base, stick_oci_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    stickiness_bonus = stick_base + (stick_oci_scale * current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 
    
    # Store previous choice for stickiness (initialize with -1 for none)
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the net Q-values before softmax
        q_net_stick = q_net.copy()
        if prev_a1 != -1:
            q_net_stick[prev_a1] += stickiness_bonus

        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous action
        prev_a1 = a1
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial]) 
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Randomness (Inverse Temperature)
This model suggests that high OCI scores might relate to decision noise or exploration/exploitation balance. Specifically, it tests if higher OCI leads to more deterministic (less random) behavior, reflecting rigidity, or perhaps more chaotic behavior. We model the inverse temperature parameter `beta` as a function of OCI. A higher `beta` means choices are more strictly determined by value differences (rigidity), while a lower `beta` implies more randomness.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Inverse Temperature (Rigidity).
    
    This model proposes that the OCI score modulates the inverse temperature (beta)
    of the softmax function. High OCI might lead to higher beta values, 
    representing 'rigidity' or a strong exploitation of small value differences,
    making behavior more deterministic and less exploratory.
    
    beta_effective = beta_base * (1 + oci * oci_beta_factor)
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta_base: [0, 10] - Baseline inverse temperature.
    w: [0, 1] - Mixing weight for MB vs MF control.
    oci_beta_factor: [0, 5] - Scaling factor for how OCI affects beta. 
                              (Positive = more rigid/deterministic with high OCI).
    """
    learning_rate, beta_base, w, oci_beta_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective beta based on OCI
    beta_effective = beta_base * (1.0 + current_oci * oci_beta_factor)
    
    # Cap beta to prevent numerical overflow in exp()
    if beta_effective > 20.0: beta_effective = 20.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial]) 
        exp_q2 = np.exp(beta_effective * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```