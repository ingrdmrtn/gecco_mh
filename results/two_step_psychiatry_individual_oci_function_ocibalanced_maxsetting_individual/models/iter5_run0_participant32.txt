Here are three cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

### Cognitive Model 1: OCI-Modulated Temperature (Rigidity)
This model hypothesizes that OCI scores correlate with "cognitive rigidity," manifested as a reduced tolerance for exploration. High OCI participants are modeled as having a higher inverse temperature ($\beta$), leading to more deterministic (greedy) choices and less random exploration compared to low OCI participants.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Temperature Model.
    
    Hypothesis: High OCI scores are associated with increased cognitive rigidity and 
    intolerance of uncertainty. This is modeled by scaling the inverse temperature (beta)
    with the OCI score, such that higher OCI leads to sharper, more deterministic softmax policies.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta_base: [0, 10] Baseline inverse temperature for a participant with OCI=0.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) control.
    - rigidity: [0, 5] Scaling factor. Effective beta = beta_base * (1 + rigidity * OCI).
    """
    learning_rate, beta_base, w, rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective beta increases with OCI
    beta = beta_base * (1.0 + rigidity * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # TD(0) update for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # TD(0) update for Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Punishment Sensitivity
This model hypothesizes that high OCI participants exhibit a "fear of failure" or perfectionism, leading them to over-weight negative outcomes (lack of reward). The learning rate for negative prediction errors (when reward is 0) is scaled up by the OCI score, causing them to abandon unrewarded options faster than rewarded ones.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Punishment Sensitivity Model.
    
    Hypothesis: High OCI participants are hypersensitive to negative feedback (0 coins).
    This is modeled by increasing the learning rate specifically when the prediction error is negative.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - punish_sens: [0, 5] Multiplier for the learning rate when outcome is negative (scaled by OCI).
      alpha_neg = learning_rate * (1 + punish_sens * OCI).
    """
    learning_rate, beta, w, punish_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Determine alpha for Stage 1 update based on sign of delta
        alpha_1 = learning_rate
        if delta_stage1 < 0:
            alpha_1 = learning_rate * (1.0 + punish_sens * oci_score)
            alpha_1 = min(alpha_1, 1.0) # Bound at 1.0
        q_stage1_mf[a1] += alpha_1 * delta_stage1

        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        # Determine alpha for Stage 2 update based on sign of delta
        alpha_2 = learning_rate
        if delta_stage2 < 0:
            alpha_2 = learning_rate * (1.0 + punish_sens * oci_score)
            alpha_2 = min(alpha_2, 1.0)
        q_stage2_mf[state_idx, a2] += alpha_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Eligibility Trace
This model hypothesizes that OCI affects the mechanism of credit assignment between the second stage outcome and the first stage choice. The parameter `lambda` (eligibility trace) determines how much the reward received at Stage 2 directly updates the value of the Stage 1 choice. Here, `lambda` is directly proportional to the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Eligibility Trace Model.
    
    Hypothesis: High OCI scores relate to altered temporal credit assignment.
    This model assumes the eligibility trace (lambda), which allows the Stage 2 reward 
    to directly update the Stage 1 choice, scales with OCI. 
    High OCI participants may link final outcomes more strongly to initial choices.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_oci: [0, 1] Scaling factor for eligibility trace. 
      Effective lambda = lambda_oci * OCI. (Bounded [0,1]).
    """
    learning_rate, beta, w, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda based on OCI
    eligibility_trace = lambda_oci * oci_score
    eligibility_trace = min(max(eligibility_trace, 0.0), 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Stage 1 Prediction Error (Standard TD)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1: Standard TD + Eligibility Trace from Stage 2 Error
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * eligibility_trace * delta_stage2)

        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```