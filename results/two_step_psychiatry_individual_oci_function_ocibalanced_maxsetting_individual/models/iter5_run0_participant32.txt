Here are three new cognitive models for the two-step task, incorporating OCI scores in novel ways based on your requirements and the participant's data.

### Model 1: Hybrid MB/MF with OCI-Modulated Exploration (Beta)
This model hypothesizes that OCI symptoms primarily affect the **decision noise** or the exploration-exploitation trade-off. High OCI might lead to more rigid, deterministic choices (higher beta) or, conversely, more erratic behavior due to anxiety (lower beta). This modulates `beta` globally for the Hybrid agent.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the Inverse Temperature (Beta) is modulated by OCI.
    Hypothesis: Higher OCI scores lead to systematically different levels of decision noise 
    (rigidity vs. randomness), affecting the exploration-exploitation trade-off.

    Bounds:
    learning_rate: [0,1]
    w: [0,1] (Weight of Model-Based system)
    beta_base: [0,10]
    beta_oci_coeff: [-5, 5] (Modulation of beta by OCI)
    """
    learning_rate, w, beta_base, beta_oci_coeff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate modulated beta
    beta = beta_base + (beta_oci_coeff * oci_score)
    beta = np.clip(beta, 0.0, 10.0) # Ensure beta stays within valid bounds

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    # Fixed transition matrix for MB (standard assumption)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value: V(S') = max(Q_stage2(S', :))
        v_stage2 = np.max(q_stage2_mf, axis=1) # Value of each state (planet)
        q_stage1_mb = transition_matrix @ v_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax with modulated beta
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        # Pure MF for stage 2 (standard)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update (MF)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (MF) - driven by Stage 2 value
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with OCI-Modulated Subjective Transition Belief
This model hypothesizes that high OCI participants have a distorted **internal model** of the environment. Instead of using the objective transition probabilities (0.7/0.3), their Model-Based system uses a subjective probability `p_common` that is a function of their OCI score (e.g., perceiving the world as more deterministic or more volatile than it is).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the Subjective Transition Probability is modulated by OCI.
    Hypothesis: High OCI participants may possess a distorted internal model of the environment's 
    transition structure (e.g., perceiving rare transitions as more or less likely than they are).

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    trans_prob_base: [0,1] (Base belief about common transition probability)
    trans_prob_oci: [-1,1] (Modulation by OCI)
    """
    learning_rate, beta, w, trans_prob_base, trans_prob_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective common transition probability
    p_common = trans_prob_base + (trans_prob_oci * oci_score)
    p_common = np.clip(p_common, 0.0, 1.0) 
    
    # Construct subjective transition matrix
    # Row 0: Space A -> [Prob(X), Prob(Y)] = [p, 1-p]
    # Row 1: Space B -> [Prob(X), Prob(Y)] = [1-p, p]
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        v_stage2 = np.max(q_stage2_mf, axis=1)
        # Use subjective matrix for MB calculation
        q_stage1_mb = transition_matrix @ v_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MB/MF with OCI-Modulated Win-Stay Stickiness
This model refines the concept of "stickiness" (perseveration). Instead of general repetition, it tests if OCI is associated with a specific compulsion to **repeat successful actions** ("Win-Stay"). The magnitude of this specific stickiness is modulated by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with 'Win-Stay' Stickiness modulated by OCI.
    Hypothesis: OCI is associated with compulsive repetition of successful actions (Win-Stay),
    rather than general perseveration. The tendency to repeat a rewarded Stage 1 choice is modulated by OCI.

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    stick_win_base: [0,5] (Base stickiness after a win)
    stick_win_oci: [-5,5] (Modulation by OCI)
    """
    learning_rate, beta, w, stick_win_base, stick_win_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Win-Stickiness
    stick_win = stick_win_base + (stick_win_oci * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        v_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ v_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Logits
        logits = beta * q_net_stage1
        
        # Apply Win-Stay Stickiness directly to logits
        if trial > 0 and reward[trial-1] == 1.0:
            prev_a1 = int(action_1[trial-1])
            logits[prev_a1] += stick_win

        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```