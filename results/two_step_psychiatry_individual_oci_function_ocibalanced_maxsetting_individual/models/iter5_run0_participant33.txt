Here are three new cognitive models, expressed as Python functions, that explore different mechanisms for how high OCI scores might influence decision-making in this two-step task.

### Model 1: Hybrid Model-Based/Model-Free with OCI-Driven Imbalance
This model tests the hypothesis that high OCI scores disrupt the balance between goal-directed (Model-Based) and habitual (Model-Free) control. Specifically, it posits that higher OCI leads to a stronger reliance on Model-Free (habitual) values, potentially ignoring the transition structure of the task.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF Model with OCI-Modulated Weighting (w).
    
    Hypothesis: The balance parameter 'w' (0=pure MF, 1=pure MB) is modulated by OCI.
    High OCI scores might reduce 'w', pushing the agent towards Model-Free habits.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_base: [0, 1] - Baseline weighting for Model-Based control.
    w_oci_slope: [0, 1] - How strongly OCI reduces the MB weight.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the mixing weight 'w'. 
    # We subtract OCI influence from base to model the hypothesis that OCI reduces MB control.
    # We clip to ensure it stays within [0, 1].
    w = w_base - (w_oci_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix (A->X=0.7, U->Y=0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-Free values for stage 1
    q_stage2 = np.zeros((2, 2)) # Values for stage 2 (same for MB and MF usually)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1: # Skip invalid trials
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # 1. Model-Based Value Calculation
        # Max value of next stage states
        max_q_stage2 = np.max(q_stage2, axis=1) 
        # Bellman equation using transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Integrated Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # 3. Softmax Choice
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Standard Q-learning (SARSA-like for Stage 1)
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (Model-Free update)
        # Note: We use the value of the state actually reached (q_stage2[s, a2])
        delta_stage1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates based on OCI
This model explores the idea that OCI is related to enhanced error monitoring or sensitivity to negative feedback (or lack of reward). It proposes that individuals with higher OCI scores learn differently from positive versus negative prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model.
    
    Hypothesis: OCI scores modulate the learning rate for negative prediction errors (forgetting/unlearning).
    High OCI might lead to 'over-learning' from failures or lack of reward compared to successes.
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors (RPE > 0).
    lr_neg_base: [0, 1] - Base learning rate for negative prediction errors (RPE < 0).
    oci_neg_mod: [0, 1] - How much OCI increases the learning rate for negative errors.
    beta: [0, 10] - Inverse temperature.
    """
    lr_pos, lr_neg_base, oci_neg_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # Bounded at 1.0. High OCI increases sensitivity to negative outcomes.
    lr_neg = np.clip(lr_neg_base + (oci_neg_mod * oci_score), 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Choice (Simple Model-Free for this mechanism focus)
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        
        # Stage 2 Update
        delta_2 = r - q_stage2[s, a2]
        if delta_2 >= 0:
            q_stage2[s, a2] += lr_pos * delta_2
        else:
            q_stage2[s, a2] += lr_neg * delta_2
            
        # Stage 1 Update
        # Using the value of the chosen stage 2 state as the target
        delta_1 = q_stage2[s, a2] - q_stage1[a1]
        if delta_1 >= 0:
            q_stage1[a1] += lr_pos * delta_1
        else:
            q_stage1[a1] += lr_neg * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration Modulated by OCI
This model posits that OCI relates to intolerance of uncertainty. Instead of simple softmax exploration, it adds an "uncertainty bonus" (or penalty) to the value of actions. If high OCI correlates with avoidance of uncertainty, the model might penalize actions that haven't been taken recently (or whose values are uncertain), or conversely, if it relates to checking behavior, it might boost them.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Uncertainty/Novelty Bonus Model.
    
    Hypothesis: OCI modulates the 'exploration bonus' for unchosen actions. 
    We track a 'counter' for how long ago an action was taken.
    The effective value Q' = Q + (bonus_weight * log(time_since_chosen)).
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    phi_base: [0, 1] - Base exploration bonus parameter.
    phi_oci: [0, 1] - OCI modulation of exploration. (Can be negative if OCI reduces exploration).
    """
    learning_rate, beta, phi_base, phi_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate exploration weight phi
    # If phi is positive, it encourages exploring old options (curiosity).
    # If phi is negative, it encourages sticking to recent options (safety/habit).
    # We model OCI as potentially reducing this bonus (avoiding the unknown) or increasing it (checking).
    # Let's assume here OCI modifies the base tendency.
    phi = phi_base + (phi_oci * oci_score)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track time since last selection for Stage 1 actions
    # Initialize with 1 to avoid log(0)
    time_since_s1 = np.ones(2) 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice with Exploration Bonus ---
        # Add exploration bonus to Q values
        # We use a simple counter-based uncertainty proxy
        exploration_bonus = phi * np.log(time_since_s1)
        q_effective = q_stage1 + exploration_bonus
        
        exp_q1 = np.exp(beta * q_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update counters: Increment all, reset chosen
        time_since_s1 += 1
        time_since_s1[a1] = 1

        # --- Stage 2 Choice (Standard Softmax) ---
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```