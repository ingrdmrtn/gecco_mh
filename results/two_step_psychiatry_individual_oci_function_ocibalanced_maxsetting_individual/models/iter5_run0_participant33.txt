Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Dynamic Transition Learning
This model hypothesizes that individuals with high OCI scores may have different beliefs about the stability of the environment's transition structure. While the standard model assumes a fixed transition matrix (0.7/0.3), this model allows the agent to update their internal transition probabilities trial-by-trial. The learning rate for this structural update is modulated by the OCI score. A high learning rate implies the agent treats the structure as volatile (constantly checking/updating beliefs), while a low rate implies rigidity.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Dynamic Transition Learning Model.
    
    The agent learns the transition matrix (Stage 1 -> Stage 2) online rather than 
    using a fixed one. The rate at which these structural beliefs are updated is 
    modulated by the OCI score. High OCI might lead to 'chasing' the transition 
    statistics (instability) or rigidity.
    
    Parameters:
    - lr: [0,1] Value learning rate.
    - beta: [0,10] Inverse temperature (softmax noise).
    - w: [0,1] Mixing weight (0=Model-Free, 1=Model-Based).
    - lambd: [0,1] Eligibility trace for Model-Free update.
    - lr_trans_base: [0,1] Baseline learning rate for transition matrix updates.
    - lr_trans_oci: [-1,1] OCI modulation of transition learning rate.
    """
    lr, beta, w, lambd, lr_trans_base, lr_trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated transition learning rate
    lr_trans = lr_trans_base + lr_trans_oci * oci_score
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition matrix with prior knowledge (instructions)
    # Rows: Action 1 (0 or 1), Cols: State 2 (0 or 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state, action
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Skip invalid/missing trials
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation using dynamic transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Learning ---
        # 1. Update Transition Matrix (Dynamic MB)
        # Delta rule: P(s2|a1) <- P(s2|a1) + alpha_trans * (1 - P(s2|a1))
        # The probability of the unobserved state decreases correspondingly.
        transition_matrix[a1, s2] += lr_trans * (1.0 - transition_matrix[a1, s2])
        transition_matrix[a1, 1-s2] = 1.0 - transition_matrix[a1, s2]
        
        # 2. Update Values (MF)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Eligibility Trace Update
        q_stage1_mf[a1] += lr * lambd * delta_2
        
    return log_loss
```

### Model 2: OCI-Modulated Stage 2 Stickiness
Standard models often include "choice stickiness" (perseveration) at Stage 1. This model proposes that OCI symptoms might manifest as perseveration at the second stage (choosing aliens), potentially reflecting compulsive checking or repetition of concrete actions regardless of the path taken to reach them. The strength of this stickiness is modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Stickiness Model.
    
    Introduces a perseveration (stickiness) bonus specifically for the second-stage 
    choice. The agent is more (or less) likely to repeat their previous choice of 
    alien at a given planet, regardless of reward. This tendency is modulated by OCI.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF mixing weight.
    - lambd: [0,1] Eligibility trace.
    - stick2_base: [-5, 5] Baseline stickiness for Stage 2 actions.
    - stick2_oci: [-5, 5] OCI modulation of Stage 2 stickiness.
    """
    lr, beta, w, lambd, stick2_base, stick2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    stick2 = stick2_base + stick2_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for each state (planet 0 and planet 1)
    # Initialize with -1 (no previous action)
    last_a2 = np.array([-1, -1]) 
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue
            
        # Stage 1 Policy (Standard)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # Stage 2 Policy (With Stickiness)
        # Calculate base logits from Q-values
        logits_2 = beta * q_stage2_mf[s2].copy()
        
        # Add stickiness bonus if there was a previous action at this state
        if last_a2[s2] != -1:
            logits_2[last_a2[s2]] += stick2
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # Update history
        last_a2[s2] = a2
        
        # Learning
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2
        
    return log_loss
```

### Model 3: OCI-Modulated Rare Transition Sensitivity
This model posits that OCI affects how the agent integrates "rare" or "surprising" transitions into their Model-Free value updates. The eligibility trace parameter ($\lambda$), which controls how much the Stage 2 outcome updates the Stage 1 choice, is modulated specifically on trials where a rare transition occurred. High OCI might lead to "cutting" the trace on rare trials (ignoring the outcome as a fluke) or over-weighting it.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Rare Transition Eligibility Model.
    
    The eligibility trace (lambda) determines how much credit is assigned to the 
    Stage 1 choice based on the Stage 2 outcome. This model allows the lambda 
    value to differ for Common vs Rare transitions. The 'Rare' lambda is 
    derived from the 'Common' lambda via an OCI-modulated factor.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF mixing weight.
    - lambd_common: [0,1] Eligibility trace for common transitions.
    - rare_mod_base: [0,2] Baseline multiplier for rare transition lambda.
    - rare_mod_oci: [-1,1] OCI modulation of the rare transition multiplier.
    """
    lr, beta, w, lambd_common, rare_mod_base, rare_mod_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the modifier for rare transitions
    # factor > 1 implies boosting learning from rare events
    # factor < 1 implies suppressing learning from rare events
    rare_factor = rare_mod_base + rare_mod_oci * oci_score
    rare_factor = np.clip(rare_factor, 0.0, 5.0) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue
            
        # Determine if transition was common or rare
        # Common: 0->0 or 1->1. Rare: 0->1 or 1->0.
        is_common = (a1 == s2)
        
        # Select lambda based on transition type
        if is_common:
            current_lambd = lambd_common
        else:
            current_lambd = lambd_common * rare_factor
            
        # Ensure lambda stays within standard bounds [0,1]
        current_lambd = np.clip(current_lambd, 0.0, 1.0)
        
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # Learning
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Update Stage 1 MF value using the context-dependent lambda
        q_stage1_mf[a1] += lr * current_lambd * delta_2
        
    return log_loss
```