Here are three new cognitive models that incorporate the OCI score to explain the observed behavior.

### Model 1: Hybrid RL with OCI-Modulated Model-Based Control
This model hypothesizes that high OCI individuals rely more heavily on habit (model-free) learning and have a suppressed ability to use model-based planning (goal-directed behavior). The weighting parameter `w` determines the balance between model-based and model-free values. Here, the effective `w` is reduced as OCI increases, simulating a shift towards habitual control.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-Modulated Model-Based Control.
    
    Hypothesis: High OCI scores are associated with a deficit in goal-directed 
    (model-based) control, leading to a reliance on habitual (model-free) systems.
    The mixing parameter 'w' is dampened by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w_base: [0, 1] Baseline weight for model-based values (0=pure MF, 1=pure MB).
    oci_dampening: [0, 1] Strength of OCI's reduction on 'w'.
    """
    learning_rate, beta, w_base, oci_dampening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective w is reduced by OCI. If oci_dampening is high, high OCI 
    # pushes w towards 0 (pure model-free/habit).
    w_eff = w_base * (1.0 - (oci_score * oci_dampening))
    w_eff = np.clip(w_eff, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_stage1_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        # Pure Model-Free at second stage
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2 Q-values
        delta_stage2 = reward[trial] - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values (TD(1) logic roughly)
        # Using the value of the state reached as the target
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates with OCI-Enhanced Punishment Sensitivity
This model posits that OCI is related to an over-sensitivity to negative outcomes (missing rewards). It splits the learning rate into positive (`alpha_pos`) and negative (`alpha_neg`) components. The negative learning rate is boosted by the OCI score, causing the agent to react more strongly when rewards are not received (reward = 0).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rates with OCI-Enhanced Punishment Sensitivity.
    
    Hypothesis: High OCI individuals are hyper-sensitive to negative feedback 
    (lack of reward). The learning rate for negative prediction errors is 
    scaled up by the OCI score.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    oci_neg_scale: [0, 5] Multiplier for OCI impact on negative learning rate.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_neg_scale, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    alpha_neg_eff = alpha_neg_base * (1.0 + oci_score * oci_neg_scale)
    alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Choice (Model-Free only for simplicity in this variant) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg_eff * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg_eff * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration with OCI-Induced Anxiety (Inverse Temperature Scaling)
This model suggests that OCI relates to anxiety, which can manifest as "freezing" or rigid behavior (high exploitation) under uncertainty. Here, the inverse temperature `beta` is not constant but scales with the OCI score. A higher OCI leads to a higher `beta`, making choices more deterministic (less random exploration) based on current value estimates.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Uncertainty-Driven Exploration with OCI-Induced Rigidity.
    
    Hypothesis: High OCI is associated with anxiety and rigidity. This is modeled
    by scaling the inverse temperature (beta) with the OCI score. Higher OCI
    leads to higher beta, resulting in more deterministic (rigid) choices and 
    reduced exploration.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta_base: [0, 10] Baseline inverse temperature.
    oci_beta_boost: [0, 5] How strongly OCI increases beta (rigidity).
    stickiness: [0, 5] Tendency to repeat the previous choice (perseveration).
    """
    learning_rate, beta_base, oci_beta_boost, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Beta increases with OCI, leading to "sharper" softmax (less exploration)
    beta_eff = beta_base * (1.0 + oci_score * oci_beta_boost)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])

        # --- Stage 1 Choice ---
        # Add stickiness bonus to the previously chosen action
        q_stage1_net = q_stage1_mf.copy()
        if prev_a1 != -1:
            q_stage1_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta_eff * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        prev_a1 = a1

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```