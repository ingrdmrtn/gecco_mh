Here are the three proposed cognitive models.

### Model 1: Subjective Transition Distortion
**Hypothesis:** Individuals with high OCI symptoms may perceive the world as more deterministic or structured than it actually is. In the context of this task, they may overestimate the probability of the "common" transition (distorting the transition matrix used in Model-Based planning), leading to a stronger reliance on the expected structure and potentially slower adaptation to rare transitions.
**Mechanism:** The Model-Based transition probability $P(\text{Common})$ is modulated by OCI.
$P_{common} = 0.7 + (\text{distortion} \times \text{OCI} \times 0.3)$
This value is clamped to a maximum of 0.99 to prevent numerical issues.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Subjective Transition Belief.
    
    Hypothesis: 
    High OCI leads to an overestimation of the transition matrix determinism (Model-Based distortion).
    The agent believes the common transition is more likely than 0.7, scaling with OCI.
    
    Parameters:
    learning_rate: [0,1] - Rate of value updating.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0).
    lambda_elig: [0,1] - Eligibility trace decay.
    oci_trans_bias: [0,1] - Degree to which OCI distorts the perceived transition probability towards 1.0.
    """
    learning_rate, beta, w, lambda_elig, oci_trans_bias = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Modulate transition belief based on OCI
    # Base is 0.7. Max distortion pushes it towards 1.0.
    # We scale the remaining 0.3 range by the bias and OCI.
    perceived_prob = 0.7 + (oci_trans_bias * current_oci * 0.29)
    transition_matrix = np.array([[perceived_prob, 1 - perceived_prob], 
                                  [1 - perceived_prob, perceived_prob]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        # Stage 1 PE
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 (MF) with eligibility trace
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        
        # Update Stage 2 (MF)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Learning Rigidity
**Hypothesis:** High OCI is associated with rigidity and difficulty in updating established values, particularly for concrete outcomes (aliens) rather than abstract navigation (spaceships). This model proposes that OCI dampens the learning rate specifically for Stage 2 (Alien choice), making the participant "stickier" to their valuation of aliens and slower to overwrite them with new reward information.
**Mechanism:** Separate learning rates for Stage 1 and Stage 2, where Stage 2 learning rate is reduced by OCI.
$LR_{stage2} = LR \times (1 - \text{rigidity} \times \text{OCI})$

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Stage 2 Learning Rigidity.
    
    Hypothesis:
    High OCI leads to reduced updating of values at the second stage (Alien choice),
    reflecting a rigidity in changing beliefs about concrete outcomes.
    
    Parameters:
    learning_rate: [0,1] - Base learning rate (applied to Stage 1).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weight.
    lambda_elig: [0,1] - Eligibility trace.
    oci_s2_rigidity: [0,1] - Factor reducing learning rate at Stage 2 based on OCI.
    """
    learning_rate, beta, w, lambda_elig, oci_s2_rigidity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate specific learning rate for stage 2
    # If rigidity is high and OCI is high, lr_s2 becomes small (hard to unlearn).
    lr_s2 = learning_rate * (1.0 - (oci_s2_rigidity * current_oci))
    lr_s2 = max(0.01, lr_s2) # Ensure it doesn't hit 0 completely

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 using base learning rate
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        
        # Update Stage 2 using OCI-modulated learning rate
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage 2 Beta Stiffness
**Hypothesis:** High OCI participants may exhibit a "winner-take-all" or compulsive selection strategy specifically at the final choice stage (Alien selection), regardless of their planning at the first stage. This model posits that OCI increases the inverse temperature (beta) for Stage 2, making choices more deterministic/greedy compared to Stage 1.
**Mechanism:** Two beta parameters, where $\beta_{stage2}$ is an OCI-amplified version of $\beta_{stage1}$.
$\beta_{stage2} = \beta_{stage1} \times (1 + \text{stiffness} \times \text{OCI})$

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Beta Stiffness.
    
    Hypothesis:
    OCI increases the exploitation (inverse temperature) specifically at Stage 2,
    leading to more deterministic/compulsive selection of Aliens, while Stage 1
    remains more flexible or noisy.
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Base inverse temperature (Stage 1).
    w: [0,1] - MB/MF weight.
    lambda_elig: [0,1] - Eligibility trace.
    oci_beta2_stiff: [0,5] - Multiplier increasing Beta at Stage 2 based on OCI.
    """
    learning_rate, beta, w, lambda_elig, oci_beta2_stiff = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 2 specific beta
    beta_s2 = beta * (1.0 + oci_beta2_stiff * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (uses base beta) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Policy (uses stiff beta_s2) ---
        exp_q2 = np.exp(beta_s2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```