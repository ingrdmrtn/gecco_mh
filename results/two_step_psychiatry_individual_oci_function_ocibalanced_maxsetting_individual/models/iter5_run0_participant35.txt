Here are the three proposed cognitive models.

### Model 1: Subjective Transition Belief Model
This model hypothesizes that high OCI scores correlate with a distorted internal model of the environment's causal structure. While the true transition probabilities are fixed (0.7/0.3), this model assumes the participant learns or holds a subjective belief about the "common" transition probability, which is modulated by their OCI score. This alters the Model-Based value calculation in Stage 1.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Belief Model.
    
    Hypothesis: OCI modulates the internal model of transition probabilities used in 
    Model-Based planning. High OCI participants may perceive the 'common' transition 
    as more deterministic (rigid structure) or less deterministic (distrust of structure), 
    altering their MB value calculation.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - stickiness: [0, 5] Perseveration bonus for repeated Stage 1 choices.
    - trans_belief_base: [0, 1] Baseline belief of common transition probability.
    - trans_belief_oci: [-1, 1] Modulation of transition belief by OCI.
    """
    learning_rate, beta, w, stickiness, trans_belief_base, trans_belief_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # Standard is 0.7. Here it's subjective.
    p_common = trans_belief_base + (trans_belief_oci * oci_score)
    if p_common < 0.0: p_common = 0.0
    if p_common > 1.0: p_common = 1.0
    
    # Transition matrix based on subjective belief
    # Action 0 -> Common: State 0, Rare: State 1
    # Action 1 -> Common: State 1, Rare: State 0
    trans_matrix = np.array([[p_common, 1 - p_common], 
                             [1 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Alien
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1: Choice ---
        # Model-Based Value Calculation using Subjective Matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_matrix @ max_q_stage2
        
        # Net Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2: Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 MF Update (TD(0))
        v_stage2 = q_stage2_mf[state_idx, a2]
        delta_stage1 = v_stage2 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Inverse Temperature Model
This model hypothesizes that OCI affects decision noise differently in the planning phase (Stage 1) compared to the harvesting phase (Stage 2). Stage 1 requires integrating Model-Based and Model-Free information, which might be more susceptible to OCI-related indecision or rigidity, modeled by modulating `beta_1`. Stage 2 is a simpler bandit task, modeled by a separate `beta_2`.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Inverse Temperature Model.
    
    Hypothesis: OCI affects the exploration-exploitation balance differently in the 
    planning phase (Stage 1) versus the harvesting phase (Stage 2).
    Stage 1 beta is modulated by OCI, while Stage 2 beta is independent.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_1_base: [0, 10] Baseline inverse temperature for Stage 1.
    - beta_1_oci: [-5, 5] Modulation of Stage 1 beta by OCI.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weighting between MB and MF.
    - stickiness: [0, 5] Stickiness for Stage 1.
    """
    learning_rate, beta_1_base, beta_1_oci, beta_2, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Beta 1
    beta_1 = beta_1_base + (beta_1_oci * oci_score)
    if beta_1 < 0.0: beta_1 = 0.0
    if beta_1 > 20.0: beta_1 = 20.0 
    
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2 - Use Beta 2
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Updates
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        v_stage2 = q_stage2_mf[state_idx, a2]
        delta_stage1 = v_stage2 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Cumulative Habit (Trace) Model
This model replaces simple one-step stickiness with a cumulative habit trace (Choice Kernel). The decay rate of this habit trace is modulated by OCI. This tests the hypothesis that high OCI leads to more persistent habits (slower decay) or perhaps more unstable habits (faster decay), influencing the repetitive nature of choices.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Cumulative Habit (Trace) Model with OCI-Modulated Decay.
    
    Hypothesis: Stickiness is not just about the last trial, but a cumulative habit trace.
    OCI modulates the decay rate of this habit trace. High OCI might lead to 
    slower decay (stronger, persistent habits) or faster decay (instability).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stick_weight: [0, 5] Weight of the habit trace in decision.
    - trace_decay_base: [0, 1] Baseline decay rate of the habit trace.
    - trace_decay_oci: [-1, 1] Modulation of decay by OCI.
    """
    learning_rate, beta, w, stick_weight, trace_decay_base, trace_decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    decay = trace_decay_base + (trace_decay_oci * oci_score)
    if decay < 0.0: decay = 0.0
    if decay > 1.0: decay = 1.0
    
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit trace for Stage 1 actions (0 and 1)
    habit_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add cumulative habit
        q_net += stick_weight * habit_trace
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Updates
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        v_stage2 = q_stage2_mf[state_idx, a2]
        delta_stage1 = v_stage2 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Habit Trace
        # Decay all traces
        habit_trace *= decay
        # Increment chosen trace
        habit_trace[a1] += 1.0
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```