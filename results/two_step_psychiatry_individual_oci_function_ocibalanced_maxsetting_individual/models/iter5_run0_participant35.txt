Here are three new cognitive models for the two-step task, incorporating the OCI score to explain individual variability.

### Model 1: Subjective Transition Uncertainty Model
This model hypothesizes that OCI symptoms correlate with a distorted belief about the reliability of the transition structure (the probability that Spaceship A goes to Planet X). While the true transition probability is 0.7, high OCI participants might perceive this relationship as more uncertain (closer to 0.5) or more deterministic (closer to 1.0) due to "doubt" or rigid thinking. This subjective transition matrix is used for Model-Based value calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Uncertainty Model.
    
    Hypothesis: OCI modulates the agent's belief about the transition matrix reliability.
    Model-Based values are calculated using a subjective transition probability 'p'
    derived from OCI.
    
    p_subjective = 0.5 + 0.2 * k
    where k = k_base + k_oci * OCI.
    
    If k=1, p=0.7 (True probability).
    If k=0, p=0.5 (Random/Uncertain).
    If k>1, p>0.7 (Over-deterministic belief).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    k_base: [0, 5]
    k_oci: [-5, 5]
    """
    learning_rate, beta, w, k_base, k_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition reliability
    k = k_base + k_oci * oci_score
    # Clip to ensure probability stays within [0, 1]
    # 0.5 + 0.2*2.5 = 1.0; 0.5 + 0.2*(-2.5) = 0.0
    k = np.clip(k, -2.5, 2.5) 
    
    p_trans = 0.5 + 0.2 * k
    transition_matrix = np.array([[p_trans, 1-p_trans], [1-p_trans, p_trans]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        # Integrated Value
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 RPE
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2
        
        # Stage 1 RPE (SARSA)
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        
        # Eligibility Trace (Lambda=1 assumed)
        q_mf1[a1] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stress-Dependent Arbitration Model
This model proposes that OCI symptoms affect how the participant balances Model-Based (goal-directed) and Model-Free (habitual) control specifically after experiencing a loss (0 coins). High OCI participants might revert to habitual control ($w \to 0$) under the "stress" of a negative outcome, while maintaining different arbitration after a win.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stress-Dependent Arbitration Model.
    
    Hypothesis: The mixing weight 'w' (MB vs MF) depends on the previous trial's outcome.
    OCI specifically modulates the mixing weight used after a LOSS (0 reward),
    reflecting a tendency to fall back on habits (or MB) under stress.
    
    w_loss = w_loss_base + w_loss_oci * OCI
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_win: [0, 1] (Mixing weight after a reward)
    w_loss_base: [0, 1] (Base mixing weight after a loss)
    w_loss_oci: [-1, 1] (Modulation of loss-weight by OCI)
    """
    learning_rate, beta, w_win, w_loss_base, w_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))
    
    prev_reward = 0.0 # Assume start with "loss" state or neutral

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Determine current mixing weight based on previous outcome
        if prev_reward == 1.0:
            w = w_win
        else:
            w = w_loss_base + w_loss_oci * oci_score
            w = np.clip(w, 0.0, 1.0)

        # --- Stage 1 Policy ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2
        
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        
        q_mf1[a1] += learning_rate * delta_2
        
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Learning Rate Model
This model posits that OCI affects the learning rate for the first-stage choice (Spaceship) differently than the second-stage choice (Alien). High OCI participants might show rigidity (low learning rate) or volatility (high learning rate) specifically in the structural decision (Stage 1), while their ability to learn simple reward associations (Stage 2) remains distinct.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Learning Rate Model.
    
    Hypothesis: OCI modulates the learning rate for Stage 1 (Spaceship choice) 
    differently from Stage 2 (Alien choice). High OCI may impair or enhance
    credit assignment to the initial choice specifically.
    
    lr_1 = lr_1_base + lr_1_oci * OCI
    lr_2 is fixed for the individual (but fitted).
    
    Bounds:
    lr_1_base: [0, 1]
    lr_1_oci: [-1, 1]
    lr_2: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    """
    lr_1_base, lr_1_oci, lr_2, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 specific learning rate
    lr_1 = lr_1_base + lr_1_oci * oci_score
    lr_1 = np.clip(lr_1, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2 values using lr_2
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += lr_2 * delta_2
        
        # Update Stage 1 values using OCI-modulated lr_1
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += lr_1 * delta_1
        
        # Eligibility Trace (using lr_1 for consistency with Stage 1 update)
        q_mf1[a1] += lr_1 * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```