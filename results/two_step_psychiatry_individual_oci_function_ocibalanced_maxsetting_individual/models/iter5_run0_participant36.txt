Here are 3 new cognitive models based on the Two-Step Task and the participant's OCI score.

### Model 1: OCI-modulated Forgetting of Unchosen Options
This model hypothesizes that OCI symptoms relate to how the participant manages information about options they *didn't* choose. High OCI might lead to "obsessive maintenance" (low forgetting) or "doubting/checking" (high forgetting/decay) of the unchosen path's value, independent of the chosen path's update.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Forgetting of Unchosen Options.
    
    Hypothesis: High OCI participants might exhibit different memory retention for 
    options they did not choose (e.g., obsessive monitoring or rapid doubting).
    This model introduces a decay rate for the unchosen spaceship's Q-value 
    in Stage 1, modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - p_stick: [0, 5] Fixed perseveration bonus for Stage 1 choices.
    - forget_base: [0, 1] Base forgetting rate for unchosen action.
    - forget_oci: [-1, 1] Modulation of forgetting rate by OCI.
    """
    learning_rate, beta, w, p_stick, forget_base, forget_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate forgetting rate and clip to [0, 1]
    forget_rate = forget_base + (forget_oci * oci_score)
    forget_rate = np.clip(forget_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        logits_1 = beta * q_net
        
        # Perseveration
        if prev_action_1 != -1:
            logits_1[int(prev_action_1)] += p_stick
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # TD Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Chosen Q-values (Lambda=1 assumed for stage 1)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Forgetting for Unchosen Stage 1 Action
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-modulated Beta Asymmetry (Win vs Loss)
This model hypothesizes that OCI affects how participants react to success versus failure. High OCI might lead to rigid exploitation (high beta) or anxious exploration (low beta) specifically after a negative outcome (0 coins), while behavior after a win remains stable.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Beta Asymmetry (Win vs Loss).
    
    Hypothesis: OCI participants may show different levels of choice rigidity (exploration/exploitation)
    depending on the previous outcome. High OCI might lead to rigid behavior (high beta) 
    after losses (failure to adapt) or wins (safety seeking).
    
    Parameters:
    - learning_rate: [0, 1]
    - w: [0, 1]
    - p_stick: [0, 5] Perseveration bonus.
    - beta_win: [0, 10] Inverse temperature after a reward (1).
    - beta_loss_base: [0, 10] Base inverse temperature after a non-reward (0).
    - beta_loss_oci: [-5, 5] Modulation of beta_loss by OCI.
    """
    learning_rate, w, p_stick, beta_win, beta_loss_base, beta_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    beta_loss = beta_loss_base + (beta_loss_oci * oci_score)
    beta_loss = np.maximum(0.0, beta_loss) # Ensure non-negative
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = 1.0 # Initialize as if previous was win to avoid initial noise

    for trial in range(n_trials):
        
        # Determine current beta based on previous reward
        current_beta = beta_win if prev_reward == 1.0 else beta_loss

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        logits_1 = current_beta * q_net
        
        if prev_action_1 != -1:
            logits_1[int(prev_action_1)] += p_stick
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        logits_2 = current_beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        prev_action_1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-modulated Stage 2 Perseveration
This model tests if OCI symptoms manifest as "motor stickiness" or repetitive behavior at the second stage (alien choice), independent of the first stage planning. This captures compulsive repetition of simple actions.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Stage 2 Perseveration.
    
    Hypothesis: OCI symptoms may manifest as 'motor stickiness' or repetition 
    at the second stage (alien choice), independent of the first stage planning.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - p_stage1: [0, 5] Perseveration bonus for Stage 1.
    - p_stage2_base: [0, 5] Base perseveration bonus for Stage 2.
    - p_stage2_oci: [-5, 5] Modulation of Stage 2 perseveration by OCI.
    """
    learning_rate, beta, w, p_stage1, p_stage2_base, p_stage2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_stage2 = p_stage2_base + (p_stage2_oci * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_action_2 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        logits_1 = beta * q_net
        
        if prev_action_1 != -1:
            logits_1[int(prev_action_1)] += p_stage1
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx]
        
        # Apply Stage 2 Perseveration (Motor stickiness: sticking to action 0 or 1)
        if prev_action_2 != -1:
            logits_2[int(prev_action_2)] += p_stage2
            
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        prev_action_1 = a1
        prev_action_2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```