Here are three new cognitive models based on the participant's data and OCI score.

### Model 1: Pure Model-Free with Asymmetric Stage 1 Learning and Stickiness
This model posits that the participant's high OCI score and perseverative behavior (long streaks of choosing the same spaceship) are driven by an asymmetry in how they process positive versus negative prediction errors in the first stage. Specifically, it tests if OCI modulates the learning rate for negative outcomes (ignoring losses) while maintaining a separate learning rate for positive outcomes.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Pure Model-Free with Asymmetric Stage 1 Learning and Stickiness.
    Hypothesis: OCI modulates the learning rate specifically for negative prediction errors 
    in Stage 1, potentially causing the participant to ignore outcomes that contradict 
    their current habit (perseveration). Stage 2 and Stage 1 (positive) share a base rate.

    Parameters:
    - lr_base: [0, 1] Base learning rate (used for Stage 2 and Stage 1 positive PEs).
    - lr_neg_base: [0, 1] Base learning rate for Stage 1 negative PEs.
    - oci_neg_mod: [-1, 1] Modulation of Stage 1 negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature for softmax.
    - stickiness: [0, 5] Choice perseveration bonus for Stage 1.
    """
    lr_base, lr_neg_base, oci_neg_mod, beta, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate specific learning rate for negative prediction errors in Stage 1
    lr_neg_stage1 = np.clip(lr_neg_base + (oci_score * oci_neg_mod), 0.0, 1.0)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_net = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 2 Update (Standard Q-Learning with base rate)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_base * delta_stage2

        # Stage 1 Update (Asymmetric Learning Rate)
        # Use max Q from stage 2 (Q-learning) or value of chosen (SARSA)? 
        # Standard TD(0) often uses the value of the state arrived at.
        # Here we use the value of the chosen stage 2 action as the target.
        target_stage1 = q_stage2_mf[state_idx, a2]
        delta_stage1 = target_stage1 - q_stage1_mf[a1]
        
        if delta_stage1 < 0:
            current_lr = lr_neg_stage1
        else:
            current_lr = lr_base
            
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Pure Model-Free with Stage-Specific Exploration (Separate Betas)
This model investigates whether the participant's decision noise varies between the high-level planning (Stage 1) and the low-level execution (Stage 2). It proposes that OCI specifically modulates the inverse temperature (`beta`) of the Stage 1 choice, potentially explaining the rigidity (high beta) or randomness (low beta) in spaceship selection, distinct from how they select aliens.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Pure Model-Free with Stage-Specific Exploration (Separate Betas).
    Hypothesis: OCI affects the exploration/exploitation balance (beta) differently 
    for Stage 1 (Spaceships) than for Stage 2 (Aliens). 
    
    Parameters:
    - lr: [0, 1] Learning rate for both stages.
    - stickiness: [0, 5] Choice perseveration bonus for Stage 1.
    - beta_stage2: [0, 10] Inverse temperature for Stage 2 choices.
    - beta_stage1_base: [0, 10] Base inverse temperature for Stage 1.
    - oci_beta1_mod: [-5, 5] Modulation of Stage 1 beta by OCI.
    """
    lr, stickiness, beta_stage2, beta_stage1_base, oci_beta1_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Stage 1 specific beta
    beta_stage1 = np.clip(beta_stage1_base + (oci_score * oci_beta1_mod), 0.0, 10.0)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice (Uses Beta 1) ---
        q_net = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice (Uses Beta 2) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

        # Update Stage 1
        target_stage1 = q_stage2_mf[state_idx, a2]
        delta_stage1 = target_stage1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Sticky Model-Based with OCI-Modulated Transition Belief
This model tests a Model-Based hypothesis where the participant's internal model of the spaceship transitions is distorted by their OCI score. Instead of assuming the true transition probability (0.7), the participant acts according to a subjective belief (e.g., believing transitions are deterministic or random), while `stickiness` accounts for their repetitive behavior.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Sticky Model-Based with OCI-Modulated Transition Belief.
    Hypothesis: The participant uses a Model-Based strategy, but their belief about 
    the transition matrix (probability of Spaceship->Planet) is subjective and 
    modulated by OCI. Stickiness accounts for habit-like persistence.

    Parameters:
    - lr_stage2: [0, 1] Learning rate for updating alien values (Stage 2).
    - beta: [0, 10] Inverse temperature.
    - stickiness: [0, 5] Choice perseveration bonus.
    - trans_belief_base: [0, 1] Base belief of 'common' transition probability (usually 0.7).
    - oci_trans_mod: [-1, 1] Modulation of transition belief by OCI.
    """
    lr_stage2, beta, stickiness, trans_belief_base, oci_trans_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability
    # We clip between 0.0 and 1.0. A value of 1.0 implies deterministic belief.
    # A value of 0.5 implies belief that spaceships are random.
    trans_prob = np.clip(trans_belief_base + (oci_score * oci_trans_mod), 0.0, 1.0)
    
    # Construct subjective transition matrix T
    # T[0,0] = P(Planet 0 | Spaceship 0). Assuming Sp0->Pl0 is common.
    # T[1,1] = P(Planet 1 | Spaceship 1). Assuming Sp1->Pl1 is common.
    transition_matrix = np.array([[trans_prob, 1.0 - trans_prob], 
                                  [1.0 - trans_prob, trans_prob]])

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Only Stage 2 values are learned via RL. Stage 1 values are computed.
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice (Model-Based Calculation) ---
        # Compute MB values: V_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = q_stage1_mb.copy()
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Only Stage 2 values are updated via prediction error
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```