Here are three new cognitive models expressed as Python functions. These models explore different mechanistic roles for the high OCI score (0.733), specifically focusing on how obsessive-compulsive traits might influence learning rates, reward sensitivity, or the balance between model-based and model-free control.

### Cognitive Model 1: OCI-Modulated Learning Rate Asymmetry
This model hypothesizes that high OCI scores lead to an asymmetry in how positive versus negative prediction errors are processed. Specifically, individuals with higher OCI might be more sensitive to "missed" rewards (negative prediction errors) or conversely, overly rigid in maintaining values despite negative outcomes (rigidity). Here, we model OCI as modulating the learning rate for negative prediction errors specifically, potentially leading to "sticky" values that don't update downwards easily.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Asymmetric Learning Rates.
    Hypothesis: High OCI participants exhibit rigidity in updating value estimates 
    when expectations are violated (negative prediction errors). The OCI score 
    scales the learning rate for negative prediction errors (alpha_neg) relative 
    to positive ones (alpha_pos).
    
    Bounds:
    alpha_pos: [0, 1] (Learning rate for positive PE)
    beta: [0, 10] (Inverse temperature)
    w: [0, 1] (Model-based weight)
    oci_dampening: [0, 1] (Factor by which OCI reduces alpha_neg)
    """
    alpha_pos, beta, w, oci_dampening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate based on OCI
    # Higher OCI -> lower alpha_neg (more rigidity/less unlearning of bad options)
    alpha_neg = alpha_pos * (1.0 - (oci_score * oci_dampening))
    
    # Ensure alpha_neg stays within bounds [0, 1]
    alpha_neg = max(0.0, min(1.0, alpha_neg))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (2 states, 2 actions)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Data cleaning
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Policy ---
        # Model-based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Softmax choice 2 based on stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        current_alpha = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += current_alpha * delta_stage2
        
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        current_alpha_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += current_alpha_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Inverse Temperature (Decision Noise)
This model posits that the OCI score relates to the determinism of the choice policy. High OCI might reflect a more "compulsive" or deterministic selection of the perceived best option, reducing exploration. Alternatively, it might reflect high anxiety and noise. Given the participant's data shows long streaks of the same choice, this model tests the hypothesis that higher OCI leads to higher `beta` (lower noise/higher exploitation), making the agent stick rigidly to the option with slightly higher value.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Inverse Temperature.
    Hypothesis: OCI modulates the exploration-exploitation trade-off. 
    Higher OCI scores lead to higher beta values (lower temperature), 
    resulting in more deterministic (compulsive) behavior and reduced 
    random exploration.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10] (Baseline inverse temperature)
    w: [0, 1]
    oci_beta_scale: [0, 5] (Scaling factor for OCI effect on beta)
    """
    learning_rate, beta_base, w, oci_beta_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective beta increases with OCI
    beta_eff = beta_base * (1.0 + oci_score * oci_beta_scale)
    
    # Cap beta to prevent numerical overflows, though 20 is quite high for softmax
    beta_eff = min(20.0, beta_eff)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use effective beta modulated by OCI
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Influenced Decay of Unchosen Options
This model introduces a decay mechanism for unchosen options. In standard RL, values of unchosen actions remain static. However, OCI might relate to "forgetting" or actively discounting alternative paths that aren't being taken (tunnel vision). This model posits that higher OCI leads to faster decay of unchosen action values towards 0, reinforcing the currently chosen path simply because the alternative fades from memory/relevance.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Influenced Value Decay.
    Hypothesis: High OCI is associated with a "tunnel vision" effect where 
    the value of unchosen options decays over time. The rate of this decay 
    is modulated by the OCI score. If the decay is high, the agent forgets 
    the value of the alternative spaceship, making switching less likely 
    if the current one is even marginally positive.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    decay_rate_base: [0, 1] (Base rate of decay for unchosen options)
    """
    learning_rate, beta, w, decay_rate_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Decay rate scaled by OCI. 
    # If OCI is high, decay might be stronger (or weaker, parameter will decide fit).
    # We define decay_eff such that it stays in [0, 1].
    decay_eff = decay_rate_base * oci_score
    decay_eff = min(1.0, max(0.0, decay_eff))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update chosen Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 action in the current state
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay_eff)

        # Update chosen Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1 action
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_eff)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```