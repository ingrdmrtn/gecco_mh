Here are three cognitive models expressed as Python functions.

### Model 1: Pure Model-Free with OCI-modulated Choice Kernel Decay
This model hypothesizes that high OCI scores are associated with increased perseveration (habit persistence). It replaces the standard one-step stickiness with a "Choice Kernel" that accumulates a trace of past choices. The **decay rate** of this kernel is modulated by OCI: participants with higher OCI have a slower decay rate, meaning past choices influence current behavior for longer periods, explaining the long streaks of repeated choices observed in the data.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free with OCI-modulated Choice Kernel Decay.
    
    Hypothesis: High OCI leads to stronger habit persistence (perseveration).
    A Choice Kernel accumulates the history of choices. The decay rate of this
    kernel is reduced by OCI, leading to longer-lasting habits.
    
    decay = decay_base * (1.0 - decay_oci_sens * OCI)
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature.
    - lam: [0,1] Eligibility trace.
    - stick_weight: [0,10] Weight of the choice kernel in decision making.
    - decay_base: [0,1] Base decay rate for the choice kernel.
    - decay_oci_sens: [0,1] Sensitivity of decay to OCI (higher = slower decay with high OCI).
    """
    learning_rate, beta, lam, stick_weight, decay_base, decay_oci_sens = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate decay rate modulated by OCI.
    # High OCI reduces decay, increasing the persistence of the kernel.
    decay = decay_base * (1.0 - decay_oci_sens * oci_score)
    decay = np.clip(decay, 0.0, 1.0)
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    choice_kernel = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # Stage 1 Choice
        # Combine Q-value and Choice Kernel
        # The kernel acts as a bias towards frequently/recently chosen options
        q_net_1 = beta * q_mf_1 + stick_weight * choice_kernel
        exp_q1 = np.exp(q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2 Choice
        q_net_2 = beta * q_mf_2[s_idx] 
        exp_q2 = np.exp(q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Updates
        # 1. Update Choice Kernel
        choice_kernel *= (1.0 - decay)
        choice_kernel[a1] += 1.0
        
        # 2. Update MF Values
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        # Eligibility trace update for Stage 1
        q_mf_1[a1] += learning_rate * lam * delta_2
        
    return log_loss
```

### Model 2: Hybrid MB/MF with OCI-modulated Inverse Temperature
This model hypothesizes that OCI relates to cognitive rigidity or determinism. It proposes that the **inverse temperature (beta)** parameter, which controls the exploration-exploitation balance, increases with OCI score. A higher beta implies more deterministic choices and less random exploration, consistent with compulsive behavior. It maintains the hybrid Model-Based/Model-Free structure to capture goal-directed planning capabilities.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF with OCI-modulated Inverse Temperature (Beta).
    
    Hypothesis: High OCI is associated with higher rigidity/determinism.
    The inverse temperature (beta) increases with OCI score, making choices
    more greedy/deterministic and less noisy.
    
    beta_eff = beta_base + beta_oci_slope * OCI
    
    Parameters:
    - learning_rate: [0,1] Learning rate.
    - beta_base: [0,10] Base inverse temperature.
    - beta_oci_slope: [0,10] Increase in beta per unit OCI.
    - w: [0,1] Weight of Model-Based system (0=MF, 1=MB).
    - lam: [0,1] Eligibility trace.
    - stickiness: [0,10] Simple 1-step stickiness.
    """
    learning_rate, beta_base, beta_oci_slope, w, lam, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate effective beta
    beta_eff = beta_base + beta_oci_slope * oci_score
    # Cap beta to prevent numerical overflow
    if beta_eff > 20.0: beta_eff = 20.0
    
    # Fixed transition matrix for Model-Based system
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # Model-Based Values Calculation
        # V_MB(s') = max_a' Q_MF(s', a')
        max_q2 = np.max(q_mf_2, axis=1)
        # Q_MB(s, a) = T(s,a,s') * V_MB(s')
        q_mb_1 = trans_probs @ max_q2
        
        # Net Value Stage 1 (Weighted mixture of MB and MF)
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Apply simple stickiness
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        # Stage 1 Choice Probability
        exp_q1 = np.exp(beta_eff * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2 Choice Probability
        exp_q2 = np.exp(beta_eff * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Updates
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_a1 = a1
        
    return log_loss
```

### Model 3: Hybrid MB/MF with OCI-modulated Negative Learning Rate
This model hypothesizes that high OCI scores are linked to perfectionism or an altered sensitivity to negative outcomes (omission of reward). It separates the learning rate into positive ($R=1$) and negative ($R=0$) components. The **negative learning rate** is modulated by OCI, allowing the model to capture if high-OCI participants update their beliefs more drastically when they fail to receive a reward.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF with OCI-modulated Negative Learning Rate.
    
    Hypothesis: High OCI participants show altered sensitivity to negative outcomes 
    (omission of reward). The learning rate for unrewarded trials (R=0) is 
    modulated by OCI relative to the positive learning rate.
    
    lr_neg = lr_pos * (1.0 + lr_neg_oci_slope * OCI)
    
    Parameters:
    - lr_pos: [0,1] Learning rate for rewarded trials (R=1).
    - lr_neg_oci_slope: [0, 10] Slope for negative learning rate scaling. 
      (Higher slope = more learning from failure for high OCI).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - lam: [0,1] Eligibility trace.
    - stickiness: [0,10] Simple stickiness.
    """
    lr_pos, lr_neg_oci_slope, beta, w, lam, stickiness = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate negative learning rate based on OCI
    # This allows learning from failure to be disproportionately high for high OCI
    factor = 1.0 + lr_neg_oci_slope * oci_score
    lr_neg = lr_pos * factor
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # Select learning rate based on outcome
        if r > 0.5:
            curr_lr = lr_pos
        else:
            curr_lr = lr_neg
            
        # Model-Based Calculation
        max_q2 = np.max(q_mf_2, axis=1)
        q_mb_1 = trans_probs @ max_q2
        
        # Net Value
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        # Stage 1 Policy
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Updates
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += curr_lr * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += curr_lr * delta_2
        
        q_mf_1[a1] += curr_lr * lam * delta_2
        
        prev_a1 = a1
        
    return log_loss
```