Here are the three cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: OCI-Scaled General Stickiness
This model hypothesizes that OCI symptoms drive a general "urge to repeat" (perseveration) that is independent of the outcome (Win or Loss). While the previous best model looked at "Lose-Stay", this model tests if the stickiness is a global feature of the participant's high OCI score, applying a bonus to the previously chosen spaceship regardless of whether it yielded gold.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Scaled General Stickiness Model.
    
    Hypothesis: High OCI scores leads to generalized perseveration (stickiness) 
    regardless of the outcome (Win or Loss). The stickiness parameter is 
    decomposed into a baseline and an OCI-dependent component.
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature (exploitation vs exploration).
    w: [0, 1] - Mixing weight (0=Model-Free, 1=Model-Based).
    lambda_decay: [0, 1] - Eligibility trace decay for Stage 1 updates.
    stick_base: [0, 5] - Baseline stickiness for any subject.
    stick_oci: [0, 5] - Additional stickiness scaled by OCI score.
    """
    learning_rate, beta, w, lambda_decay, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Total stickiness is baseline plus OCI-dependent component
    total_stickiness = stick_base + (stick_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Stickiness Vector
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = total_stickiness
            
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous action
        prev_a1 = a1

        # --- Stage 2 Policy ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Prediction Errors
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 (Model-Free) with Eligibility Trace
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        
        # Update Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Reward Sensitivity
This model hypothesizes that OCI symptoms alter the subjective valuation of the reward. High OCI might correlate with "perfectionism" (hyper-valuing the gold coin) or "anhedonia" (undervaluing it). This model scales the effective reward `r` used in the prediction error calculation by the OCI score. This affects how fast both the Model-Based (via Stage 2 values) and Model-Free systems learn from success.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Reward Sensitivity Model.
    
    Hypothesis: OCI affects the subjective magnitude of the reward signal.
    The effective reward is scaled by OCI, which amplifies or dampens the 
    prediction errors (delta), thereby changing the learning trajectory 
    for both MB and MF systems.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    lambda_decay: [0, 1]
    reward_sens_oci: [0, 1] - Scales the reward magnitude based on OCI. 
                              (1 + reward_sens_oci * OCI * 2).
                              Allows for up to 3x subjective valuation if maxed.
    """
    learning_rate, beta, w, lambda_decay, reward_sens_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective reward multiplier
    # If reward_sens_oci is 0, multiplier is 1. If high, multiplier increases.
    r_multiplier = 1.0 + (reward_sens_oci * oci_score * 2.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        
        # Subjective Reward
        r_effective = reward[trial] * r_multiplier

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        # Use effective reward for the delta calculation
        delta_stage2 = r_effective - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 3: Stage-Specific OCI Rigidity
This model separates the exploration/exploitation parameter (`beta`) for Stage 1 (Spaceship choice) and Stage 2 (Alien choice). It hypothesizes that OCI specifically increases rigidity (determinism) in the high-level planning stage (Stage 1), while the reaction to aliens (Stage 2) remains at a baseline level of stochasticity. This reflects the idea that the "compulsion" acts on the strategic choice.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific OCI Rigidity Model.
    
    Hypothesis: OCI increases decision rigidity (higher beta) specifically 
    for the first-stage choice (planning/strategy), while the second-stage 
    choice remains at a baseline level of exploration.
    
    Parameters:
    learning_rate: [0, 1]
    beta_base: [0, 10] - Baseline inverse temperature for Stage 2 (and base for Stage 1).
    beta_oci_boost: [0, 5] - Additional beta added to Stage 1 scaled by OCI.
    w: [0, 1]
    lambda_decay: [0, 1]
    stickiness: [0, 5] - Standard choice stickiness (outcome-independent).
    """
    learning_rate, beta_base, beta_oci_boost, w, lambda_decay, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stage 1 Beta is enhanced by OCI (more rigid/deterministic)
    beta_stage1 = beta_base + (beta_oci_boost * oci_score)
    # Stage 2 Beta is baseline
    beta_stage2 = beta_base

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 (High Rigidity) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        # Use beta_stage1
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        prev_a1 = a1

        # --- Stage 2 (Baseline Rigidity) ---
        qs_2 = q_stage2_mf[s_idx]
        # Use beta_stage2
        exp_q2 = np.exp(beta_stage2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```