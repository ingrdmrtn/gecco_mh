Here are 3 new cognitive models based on the participant's data and OCI score.

### Model 1: Dynamic Transition Learning with OCI Modulation
This model hypothesizes that high OCI scores correlate with altered updating of the internal model of the environment (the transition matrix). While the standard model assumes fixed transition probabilities (0.7/0.3), this model assumes participants learn these probabilities over time, and OCI modulates the learning rate of this structural knowledge (`lr_trans`).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Transition Learning with OCI Modulation.
    
    Hypothesis:
    OCI affects the rate at which participants update their internal model of the 
    spaceship-planet transition probabilities. High OCI might lead to 
    rigid beliefs (low lr_trans) or hyper-sensitivity to rare transitions (high lr_trans).
    
    Parameters:
    learning_rate: [0, 1] - Value learning rate (Q-values).
    beta: [0, 10] - Softmax inverse temperature.
    w: [0, 1] - MB/MF balance weight.
    stickiness: [-5, 5] - Choice perseveration bonus.
    lr_trans_base: [0, 1] - Base learning rate for transition matrix.
    lr_trans_oci: [-1, 1] - OCI modulation of transition learning rate.
    """
    learning_rate, beta, w, stickiness, lr_trans_base, lr_trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate modulated transition learning rate
    lr_trans = lr_trans_base + lr_trans_oci * oci_score
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition matrix (belief) with instructed probabilities
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing data
        if a2 < 0 or a2 > 1 or np.isnan(a2):
            p_choice_1[trial] = 0.5 
            p_choice_2[trial] = 0.5
            continue
            
        # --- Stage 1 Policy ---
        # Model-Based Value estimation using DYNAMIC transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mixed Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        
        # 1. Update Transition Matrix Belief
        # P(s|a1) moves towards 1 for the observed transition
        transition_matrix[a1, s_idx] += lr_trans * (1.0 - transition_matrix[a1, s_idx])
        transition_matrix[a1, 1-s_idx] = 1.0 - transition_matrix[a1, s_idx]
        
        # 2. Update Q-values
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Standard Q-learning update for MF (assuming lambda implicitly 1 for reward propagation)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Goal Learning Rate
This model tests the hypothesis that OCI specifically affects the learning rate for the goal states (Stage 2/Aliens), while the habitual Stage 1 learning rate remains fixed. High OCI might be associated with obsessive updating of specific outcome values.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Goal Learning Rate (Stage 2).
    
    Hypothesis:
    OCI modulates the learning rate specifically for the goal states (Stage 2),
    reflecting differences in how obsessive symptoms relate to outcome valuation/updating,
    while the habit formation (Stage 1) remains at a baseline rate.
    
    Parameters:
    lr_st1: [0, 1] - Learning rate for Stage 1 (MF).
    lr_st2_base: [0, 1] - Base learning rate for Stage 2.
    lr_st2_oci: [-1, 1] - OCI modulation of Stage 2 learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF balance.
    stickiness: [-5, 5] - Perseveration.
    """
    lr_st1, lr_st2_base, lr_st2_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate Stage 2 learning rate
    lr_st2 = lr_st2_base + lr_st2_oci * oci_score
    lr_st2 = np.clip(lr_st2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a2 < 0 or a2 > 1 or np.isnan(a2):
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 uses fixed lr_st1
        q_stage1_mf[a1] += lr_st1 * (delta_stage1 + delta_stage2)
        
        # Stage 2 uses modulated lr_st2
        q_stage2_mf[s_idx, a2] += lr_st2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Independent Control Channels with OCI-Modulated MF Strength
This model removes the trade-off parameter `w` and instead models MB and MF as independent channels with separate signal strengths (betas). OCI specifically modulates the strength of the Model-Free (habitual) channel. This allows for the possibility that high OCI increases habitual drive without necessarily decreasing goal-directed planning capacity.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Independent Control Channels with OCI-Modulated MF Strength.
    
    Hypothesis:
    Instead of a mixing weight 'w', the Model-Based and Model-Free systems
    act as independent controllers with their own signal strengths (betas).
    OCI modulates the strength (inverse temperature) of the Model-Free (habitual) controller,
    making habits more or less dominant/deterministic.
    
    Parameters:
    learning_rate: [0, 1] - Shared learning rate.
    lambda_eligibility: [0, 1] - Eligibility trace for MF Stage 1 update.
    stickiness: [-5, 5] - Perseveration.
    beta_mb: [0, 10] - Strength of Model-Based control.
    beta_mf_base: [0, 10] - Base strength of Model-Free control.
    beta_mf_oci: [-5, 5] - OCI modulation of MF strength.
    """
    learning_rate, lambda_eligibility, stickiness, beta_mb, beta_mf_base, beta_mf_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate modulated MF strength
    beta_mf = beta_mf_base + beta_mf_oci * oci_score
    beta_mf = np.maximum(beta_mf, 0.0) # Ensure non-negative beta
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a2 < 0 or a2 > 1 or np.isnan(a2):
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine logits directly (independent channels)
        logits = beta_mb * q_stage1_mb + beta_mf * q_stage1_mf
        
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1

        # Stage 2 Policy
        # Use sum of betas to approximate total precision in stage 2
        beta_stage2 = beta_mb + beta_mf
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx, :])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update MF Q-values with eligibility trace
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```