Here are three new cognitive models designed to capture the behavior of the participant, specifically focusing on how high OCI scores (indicating high obsessive-compulsive symptoms) might influence reinforcement learning processes in a two-step task.

### Model 1: Hybrid Model with OCI-Modulated Model-Based Weight (`w`)
This model hypothesizes that individuals with higher OCI scores might rely more on habitual (model-free) control rather than goal-directed (model-based) planning, perhaps due to cognitive rigidity or a reliance on routine. Here, the weighting parameter `w` (which balances model-based and model-free values) is directly modulated by the OCI score. A base `w` is modified by the OCI score to determine the final trade-off.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Model-Based Weighting.
    
    Hypothesis: High OCI scores relate to a shift in the balance between 
    Goal-Directed (MB) and Habitual (MF) control. This model allows OCI to 
    shift the weighting parameter 'w'.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] Base weight for Model-Based values.
    w_oci_shift: [-1, 1] Adjustment to w based on OCI score.
    """
    learning_rate, beta, w_base, w_oci_shift = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective w, constrained to [0, 1]
    # If w_oci_shift is negative, high OCI reduces MB usage (more habitual).
    w = w_base + (participant_oci * w_oci_shift)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value (Hybrid)
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD)
        # Using the value of the state reached (s_idx) and action taken (a2) as the target
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model proposes that high OCI scores might be associated with a heightened sensitivity to negative outcomes (or lack of reward) versus positive outcomes. This is modeled by splitting the learning rate into `alpha_pos` and `alpha_neg`. The OCI score specifically amplifies or dampens the learning rate for negative prediction errors (when reward is 0), reflecting a potential "fear of failure" or hyper-correction mechanism.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: OCI affects how strongly participants learn from negative outcomes 
    (reward=0) vs positive outcomes. 
    
    Bounds:
    alpha_pos: [0, 1] Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    oci_neg_scale: [0, 5] Multiplier for OCI impact on negative learning rate.
    beta: [0, 10]
    """
    alpha_pos, alpha_neg_base, oci_neg_scale, beta = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective negative learning rate
    # High OCI -> Higher learning from failure?
    alpha_neg = alpha_neg_base * (1 + (participant_oci * oci_neg_scale))
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update
        delta_stage2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * delta_stage2

        # Stage 1 Update
        # TD(0) update using the value of the state reached
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Trace Decay Modulated by OCI
This model investigates if OCI scores influence the "credit assignment" process across the two stages. The eligibility trace parameter (`lambda`) determines how much the Stage 1 choice is updated based on the Stage 2 outcome. A high OCI score might lead to "over-thinking" or stronger causal linking between distal outcomes and initial choices (higher lambda), or conversely, a focus only on immediate surroundings (lower lambda).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(lambda) with OCI-modulated Eligibility Trace.
    
    Hypothesis: OCI modulates the eligibility trace (lambda), affecting how much 
    credit the Stage 1 action receives for the Stage 2 reward directly.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_base: [0, 1] Base eligibility trace decay.
    lambda_oci_mod: [-1, 1] Modulation of lambda by OCI.
    """
    learning_rate, beta, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective lambda
    lam = lambda_base + (participant_oci * lambda_oci_mod)
    lam = np.clip(lam, 0.0, 1.0)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Prediction Errors
        # RPE at stage 2: Reward - Q2
        delta_2 = r - q_stage2[s_idx, a2]
        
        # RPE at stage 1: Q2 - Q1
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]

        # Updates
        # Standard Q-learning for Stage 2
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # TD(lambda) update for Stage 1:
        # It learns from its own immediate transition (delta_1)
        # AND a portion (lambda) of the subsequent error (delta_2)
        q_stage1[a1] += learning_rate * (delta_1 + lam * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```