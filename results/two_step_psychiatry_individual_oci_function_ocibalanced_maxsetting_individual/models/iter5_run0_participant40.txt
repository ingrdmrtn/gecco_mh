Here are the 3 proposed cognitive models.

### Model 1: Forgetting of Unchosen Options Model
This model hypothesizes that high OCI participants exhibit a form of "tunnel vision" or "attentional capture" where they rapidly forget the value of unchosen options. By allowing the value of the alternative spaceship to decay to zero more quickly, the participant becomes locked into their current choice (even if rewards are sparse), as the alternative is perceived as having no value.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Forgetting of Unchosen Options Model.
    
    High OCI participants may exhibit 'tunnel vision', forgetting the value of unchosen options 
    more rapidly. This prevents them from switching back to the alternative even when the current 
    option yields poor rewards, as the alternative's value has decayed to near zero.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Mixing weight (0 = pure MF, 1 = pure MB).
    - stickiness: [0, 5] Choice stickiness bonus (1-back).
    - lambda_eligibility: [0, 1] Eligibility trace for Stage 2 reward to update Stage 1.
    - val_decay_base: [0, 1] Base decay rate for unchosen Q-values.
    - val_decay_oci: [0, 1] Additional decay rate scaled by OCI score.
    """
    learning_rate, beta, w, stickiness, lambda_eligibility, val_decay_base, val_decay_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate decay rate bounded between 0 and 1
    decay_rate = val_decay_base + val_decay_oci * current_oci
    if decay_rate > 1.0: decay_rate = 1.0
    if decay_rate < 0.0: decay_rate = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5  # Initialize at 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus
        logits = beta * q_net
        if prev_choice_1 != -1:
            logits[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # TD Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 MF (with eligibility trace)
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_eligibility * delta_stage2
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Forgetting Mechanism: Decay the unchosen option in Stage 1
        unchosen = 1 - a1
        q_stage1_mf[unchosen] *= (1.0 - decay_rate)
        
        prev_choice_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Compulsive Drift Model
This model posits that for high OCI participants, the mere act of choosing an option intrinsically reinforces it ("compulsive drift"), independent of the external reward. This creates a "habit" that strengthens over time, causing Q-values for the chosen option to drift upwards and locking the participant into streaks even in the absence of monetary reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Compulsive Drift Model.
    
    This model posits that for high OCI participants, the act of choosing an option 
    intrinsically reinforces it (compulsive drift), independent of the external reward.
    This creates a 'habit' that strengthens over time, explaining long streaks.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Standard 1-back stickiness.
    - lambda_eligibility: [0, 1] Eligibility trace.
    - drift_base: [0, 1] Base drift added to Q-value of chosen action.
    - drift_oci: [0, 1] Additional drift scaled by OCI.
    """
    learning_rate, beta, w, stickiness, lambda_eligibility, drift_base, drift_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    drift_amount = drift_base + drift_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        if prev_choice_1 != -1:
            logits[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Standard RL Update
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_eligibility * delta_stage2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Compulsive Drift: Add 'fictitious reward' or drift to the chosen action
        # This increases the value of the chosen action simply because it was chosen.
        q_stage1_mf[a1] += drift_amount
        
        prev_choice_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Rigidity Model
This model separates the decision noise (beta) for the habitual Stage 1 choice and the goal-directed Stage 2 choice. It hypothesizes that OCI specifically increases rigidity (beta) in the first-stage choice (leading to compulsive spaceship selection), while the second-stage choice remains more flexible or standard.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Rigidity Model.
    
    Separates decision noise (beta) for the habitual Stage 1 choice and the goal-directed Stage 2 choice.
    OCI modulates the Stage 1 beta, making the initial choice extremely rigid (compulsive),
    while Stage 2 learning and decision making remains flexible.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta_1_base: [0, 10] Base inverse temp for Stage 1.
    - beta_1_oci: [0, 10] Increase in Stage 1 rigidity due to OCI.
    - beta_2: [0, 10] Inverse temp for Stage 2.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Stickiness bonus.
    - lambda_eligibility: [0, 1] Eligibility trace.
    """
    learning_rate, beta_1_base, beta_1_oci, beta_2, w, stickiness, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 1 Beta
    beta_1 = beta_1_base + beta_1_oci * current_oci
    # Ensure beta doesn't explode too high if params are large (though softmax handles it, bounds are 0-10)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision (Rigidity Modulated by OCI) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta_1 * q_net
        if prev_choice_1 != -1:
            logits[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision (Standard Beta) ---
        state_idx = int(state[trial])
        logits_2 = beta_2 * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_eligibility * delta_stage2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        prev_choice_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```