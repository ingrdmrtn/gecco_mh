Here are the three proposed cognitive models.

### Cognitive Model 1
```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with OCI-modulated Choice Perseveration (Stickiness).
    
    Hypothesis: The participant uses a Model-Based strategy (utilizing the known transition structure)
    rather than Model-Free caching for Stage 1, but exhibits compulsive repetition (stickiness) 
    of their choices. The magnitude of this stickiness is scaled by their OCI score.
    
    Parameters:
    lr: [0, 1] Learning rate for Stage 2 values (alien reward probabilities).
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    stick_base: [0, 5] Base parameter for choice perseveration.
    """
    lr, beta, stick_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: High OCI -> Higher stickiness magnitude
    stickiness = stick_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Stage 2 values are learned via Model-Free TD, but Stage 1 is calculated Model-Based
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Based + Stickiness) ---
        # 1. Calculate MB values: V(s) = sum(T(s,a,s') * max(Q(s',a')))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Add Stickiness to the logits
        logits_1 = beta * q_stage1_mb
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # 3. Softmax
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        last_action_1 = int(action_1[trial])
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        qs2 = q_stage2_mf[state_idx]
        logits_2 = beta * qs2
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        # Update Stage 2 Q-values based on reward
        r = reward[trial]
        a2 = int(action_2[trial])
        
        # Prediction Error for Stage 2
        pe = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * pe
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2
```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated Loss Sensitivity.
    
    Hypothesis: High OCI participants perceive a '0' coin outcome not as a neutral event, 
    but as a negative outcome (subjective loss/punishment), driving active avoidance.
    This model transforms the reward signal based on OCI.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    loss_base: [0, 5] Base magnitude of the subjective penalty for 0 rewards.
    """
    lr, beta, loss_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: The penalty for 0-outcomes scales with OCI
    penalty = loss_base * oci_score

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Model-Free) ---
        logits_1 = beta * q_stage1_mf
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Subjective Reward Transformation
        # If reward is 0, it feels like -penalty. If 1, it is 1.
        if r == 0:
            eff_reward = -penalty
        else:
            eff_reward = r
            
        # TD Updates
        # Stage 2 Update
        pe_2 = eff_reward - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * pe_2
        
        # Stage 1 Update (SARSA-style, using Q-value of next state)
        pe_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3
```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated Value Decay.
    
    Hypothesis: High OCI participants have "stickier" memory for valuations,
    resulting in lower decay rates for unchosen options (obsessive maintenance of value).
    This leads to values persisting longer than in typical learners who might forget unvisited options.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    decay_base: [0, 1] Base decay rate for unchosen actions.
    """
    lr, beta, decay_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI modulation: High OCI reduces forgetting (stickier memory / lower decay)
    # The decay applied is inversely proportional to OCI score.
    decay = decay_base / (1.0 + oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        logits_1 = beta * q_stage1_mf
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Selected actions update (Standard Q-learning)
        pe_2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * pe_2
        
        pe_1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        
        # Decay for unchosen actions
        # This models forgetting or passive value erosion
        
        # Stage 1 unchosen
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)
        
        # Stage 2 unchosen (only for the visited state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```