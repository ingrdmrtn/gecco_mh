Here are three new cognitive models designed to capture the behavior of the participant, specifically addressing the high OCI score and the observed patterns of perseverance and learning.

### Cognitive Model 1: Pure Model-Free RL with OCI-Modulated Asymmetric Learning Rates
This model hypothesizes that high OCI scores correlate with an altered sensitivity to negative outcomes (prediction errors). It splits the learning rate into positive (better than expected) and negative (worse than expected) components. The negative learning rate is linearly interpolated based on the OCI score, allowing the model to capture whether the participant is hyper-sensitive or insensitive to missing rewards (outcomes of 0).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free RL with OCI-Modulated Asymmetric Learning Rates.
    
    Splits learning into positive and negative prediction error updates.
    The learning rate for negative errors is modulated by OCI, testing if 
    symptoms relate to how the agent updates value after lack of reward.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors (R > Q).
    lr_neg_low_oci: [0,1] - Negative learning rate (R < Q) for low OCI scores.
    lr_neg_high_oci: [0,1] - Negative learning rate (R < Q) for high OCI scores.
    beta: [0,10] - Inverse temperature for softmax choice policy.
    stickiness: [0,5] - Choice perseverance bonus added to Q-values.
    """
    lr_pos, lr_neg_low_oci, lr_neg_high_oci, beta, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Interpolate negative learning rate based on OCI
    lr_neg = lr_neg_low_oci * (1 - oci_score) + lr_neg_high_oci * oci_score
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # Handle missing data/timeouts
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = a1
            continue
        
        # Stage 1 Policy
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        # Safety for overflow
        if np.any(np.isinf(exp_q1)):
            probs_1 = np.zeros_like(exp_q1)
            probs_1[np.argmax(q_net_1)] = 1.0
        else:
            probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2[s])
        if np.any(np.isinf(exp_q2)):
            probs_2 = np.zeros_like(exp_q2)
            probs_2[np.argmax(q_stage2[s])] = 1.0
        else:
            probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Stage 1 TD Error (driven by Stage 2 Q-value)
        delta_1 = q_stage2[s, a2] - q_stage1[a1]
        lr_1 = lr_pos if delta_1 >= 0 else lr_neg
        q_stage1[a1] += lr_1 * delta_1
        
        # Stage 2 TD Error (driven by Reward)
        delta_2 = r - q_stage2[s, a2]
        lr_2 = lr_pos if delta_2 >= 0 else lr_neg
        q_stage2[s, a2] += lr_2 * delta_2
        
        # Eligibility trace-like update for Stage 1 using Stage 2 RPE
        lr_1_second = lr_pos if delta_2 >= 0 else lr_neg
        q_stage1[a1] += lr_1_second * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Mixture of Model-Free RL and OCI-Modulated Habitual Perseveration
This model proposes that high OCI behavior is characterized by a "slip" into habitual repetition that overrides value-based decision-making. Instead of adding a bias to the Q-values (which just shifts the softmax curve), this model uses a mixture density: the probability of choice is a weighted average of the RL policy and a pure "repeat last choice" policy. The weight of the habit component is determined by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Mixture of Model-Free RL and OCI-Modulated Habitual Perseveration.
    
    The agent chooses based on a weighted mixture of a value-based (RL) policy and 
    a habitual policy (pure repetition). The weight of the habit is modulated by OCI.
    This captures 'compulsive' repetition that may occur regardless of value.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for RL softmax.
    w_habit_low_oci: [0,1] - Weight of habit policy at OCI=0.
    w_habit_high_oci: [0,1] - Weight of habit policy at OCI=1.
    """
    learning_rate, beta, w_habit_low_oci, w_habit_high_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Interpolate habit weight based on OCI
    w_habit = w_habit_low_oci * (1 - oci_score) + w_habit_high_oci * oci_score
    w_habit = np.clip(w_habit, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = a1
            continue
        
        # Stage 1 Policy: Mixture
        # RL Part
        exp_q1 = np.exp(beta * q_stage1)
        if np.any(np.isinf(exp_q1)):
            probs_rl = np.zeros_like(exp_q1)
            probs_rl[np.argmax(q_stage1)] = 1.0
        else:
            probs_rl = exp_q1 / np.sum(exp_q1)
        
        # Habit Part (Repeat last action)
        probs_habit = np.zeros(2)
        if last_action_1 != -1:
            probs_habit[last_action_1] = 1.0
        else:
            probs_habit = np.array([0.5, 0.5]) # No habit on first trial
            
        probs_1_mixed = (1 - w_habit) * probs_rl + w_habit * probs_habit
        p_choice_1[trial] = probs_1_mixed[a1]
        
        # Stage 2 Policy: Pure RL
        exp_q2 = np.exp(beta * q_stage2[s])
        if np.any(np.isinf(exp_q2)):
            probs_2 = np.zeros_like(exp_q2)
            probs_2[np.argmax(q_stage2[s])] = 1.0
        else:
            probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates (Standard MF with Eligibility Trace)
        delta_1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_2
        
        q_stage1[a1] += learning_rate * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Pure Model-Free RL with OCI-Modulated Eligibility Trace
This model investigates the link between the two stages of the task. It uses an eligibility trace parameter ($\lambda$) that is modulated by OCI. This parameter controls how strongly the immediate reward at Stage 2 reinforces the choice made at Stage 1. High OCI might lead to a disconnection between stages (low $\lambda$) or an excessive focus on the final outcome (high $\lambda$).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free RL with OCI-Modulated Eligibility Trace (Lambda).
    
    Modulates the eligibility trace parameter (lambda) by OCI. This controls 
    how much the Stage 2 reward outcome directly reinforces the Stage 1 choice.
    Tests if high OCI alters the credit assignment between task stages.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    stickiness: [0,5] - Choice stickiness bonus.
    lambda_low_oci: [0,1] - Eligibility trace value at OCI=0.
    lambda_high_oci: [0,1] - Eligibility trace value at OCI=1.
    """
    learning_rate, beta, stickiness, lambda_low_oci, lambda_high_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Interpolate lambda based on OCI
    elig_lambda = lambda_low_oci * (1 - oci_score) + lambda_high_oci * oci_score
    elig_lambda = np.clip(elig_lambda, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        if a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            last_action_1 = a1
            continue
        
        # Stage 1 Policy
        q_net_1 = q_stage1.copy()
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        if np.any(np.isinf(exp_q1)):
            probs_1 = np.zeros_like(exp_q1)
            probs_1[np.argmax(q_net_1)] = 1.0
        else:
            probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2[s])
        if np.any(np.isinf(exp_q2)):
            probs_2 = np.zeros_like(exp_q2)
            probs_2[np.argmax(q_stage2[s])] = 1.0
        else:
            probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Stage 1 Update: driven by Stage 2 value (TD(0))
        delta_1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1
        
        # Stage 2 Update: driven by Reward
        delta_2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_2
        
        # Stage 1 Eligibility Trace Update: driven by Stage 2 RPE
        q_stage1[a1] += learning_rate * elig_lambda * delta_2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```