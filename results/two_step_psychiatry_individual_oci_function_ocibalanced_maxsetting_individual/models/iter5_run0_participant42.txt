Here are 3 new cognitive models, building on the provided template and exploring different mechanisms for how OCI symptoms might influence decision-making in this task.

### Model 1: OCI-Modulated Model-Based Suppression
This model tests the hypothesis that high OCI scores are associated with a degradation or suppression of Model-Based (planning) reasoning, perhaps due to cognitive rigidity or resource constraints. Instead of a linear shift, it uses an exponential suppression factor.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Suppression.
    Hypothesis: High OCI scores suppress the weight (w) given to Model-Based values
    via an exponential decay function, reflecting cognitive rigidity or interference.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_base: [0, 1] Baseline model-based weighting for a theoretical 0-OCI participant.
    - suppression_factor: [0, 5] How strongly OCI reduces the effective w.
      Effective w = w_base * exp(-suppression_factor * oci)
    """
    learning_rate, beta, w_base, suppression_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight 'w' based on OCI
    # If suppression_factor is high and OCI is high, w becomes very small (pure MF)
    w = w_base * np.exp(-suppression_factor * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value using the OCI-suppressed w
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        # Stage 2 update (standard Q-learning)
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Stage 1 MF update (SARSA-style with stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Choice Stickiness)
This model posits that OCI manifests as a "sticky" behaviorâ€”a compulsion to repeat the previous Stage 1 choice regardless of the outcome (reward/no-reward) or the transition type (common/rare). This is different from "Win-Stay" because it applies even after losses.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Perseveration (Choice Stickiness).
    Hypothesis: High OCI leads to motor perseveration (stickiness) at Stage 1.
    The participant gets a 'bonus' added to the value of the spaceship chosen 
    in the previous trial, scaled by their OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - stick_strength: [0, 5] Magnitude of the perseveration bias.
      Bias = stick_strength * oci_score
    """
    learning_rate, beta, w, stick_strength = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus if we have a previous choice
        if prev_choice_1 != -1:
            q_net[prev_choice_1] += stick_strength * oci_score
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        # Store choice for next trial
        prev_choice_1 = act1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Pruning (State Neglect)
This model suggests that high OCI individuals might simplify the task by "pruning" or ignoring the rare transition paths in their mental model. If OCI is high, the participant treats the transition matrix as more deterministic than it actually is (e.g., believing 0->0 is 100% likely and 0->1 is 0%), leading to stronger but potentially erroneous Model-Based values.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Pruning (State Neglect).
    Hypothesis: High OCI participants simplify the transition structure.
    They perceive the transition probability of the common transition as higher 
    than reality (closer to 1.0) and the rare transition as lower (closer to 0.0).
    This distortion is proportional to their OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - distortion: [0, 1] How much OCI distorts the transition matrix.
      Perceived P(Common) = 0.7 + (distortion * oci * 0.3)
      (Bounded at 1.0)
    """
    learning_rate, beta, w, distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate distorted transition matrix based on OCI
    # Base is 0.7. Max possible is 1.0.
    # We add (distortion * oci) portion of the remaining gap (0.3).
    p_common = 0.7 + (distortion * oci_score * 0.3)
    # Ensure bounds [0, 1]
    if p_common > 1.0: p_common = 1.0
    p_rare = 1.0 - p_common
    
    # Distorted transition matrix
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use distorted matrix for planning
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```