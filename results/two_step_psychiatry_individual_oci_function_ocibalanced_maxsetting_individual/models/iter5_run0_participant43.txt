Here are three new cognitive models based on the participant's data and OCI score.

### Model 1: OCI-Modulated Punishment Resistance
This model hypothesizes that high OCI scores are associated with **insensitivity to negative feedback** (punishment). While the participant learns normally from positive rewards, their ability to "unlearn" or devalue an action after receiving no reward (0 coins) is dampened by their OCI score. This could explain the observed behavior of sticking to a spaceship for long streaks despite frequent zero-reward outcomes (e.g., trials 180-200).

```python
def cognitive_model1_oci_punishment_resistance(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Punishment Resistance.
    
    Hypothesis: High OCI individuals exhibit compulsivity by ignoring negative feedback.
    The learning rate for negative prediction errors (punishment/omission of reward) 
    is reduced as OCI increases.
    
    alpha_neg = learning_rate * (1 - resistance * OCI)
    alpha_pos = learning_rate
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for positive updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - resistance: [0, 1] Degree to which OCI dampens learning from negative outcomes. 
                  (1 = OCI fully blocks unlearning; 0 = No effect).
    """
    learning_rate, beta, w, resistance = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Calculate effective learning rates based on Prediction Error sign
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 < 0:
            eff_lr_1 = learning_rate * (1.0 - resistance * oci_score)
            eff_lr_1 = max(0.0, eff_lr_1)
        else:
            eff_lr_1 = learning_rate
        q_stage1_mf[action_1[trial]] += eff_lr_1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 < 0:
            eff_lr_2 = learning_rate * (1.0 - resistance * oci_score)
            eff_lr_2 = max(0.0, eff_lr_2)
        else:
            eff_lr_2 = learning_rate
        q_stage2_mf[state_idx, action_2[trial]] += eff_lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Rigid Exploration
This model posits that OCI affects the **exploration-exploitation balance** (inverse temperature, $\beta$). High OCI scores lead to higher $\beta$ values, resulting in more rigid, deterministic choices and reduced exploration. This aligns with the "compulsive" aspect of OCI, where uncertainty is minimized by rigidly adhering to the perceived best option, even if the value difference is small.

```python
def cognitive_model2_oci_uncertainty_rigidity(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Learner with OCI-Modulated Rigidity (Beta Scaling).
    
    Hypothesis: High OCI is associated with reduced exploration and more deterministic (rigid) behavior.
    The inverse temperature (beta) is scaled up by the OCI score.
    
    beta_effective = beta_base * (1 + rigidity_scale * OCI)
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - rigidity_scale: [0, 5] Scaling factor for OCI's effect on beta. 
                      Higher values mean OCI leads to sharper, more deterministic probabilities.
    """
    learning_rate, beta_base, w, rigidity_scale = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate effective beta once
    beta_eff = beta_base * (1.0 + rigidity_scale * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_eff * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta_eff * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reinforced Perseveration (Win-Stay)
Unlike general perseveration (stickiness), this model hypothesizes that OCI specifically amplifies the **"Win-Stay"** heuristic. High OCI participants may form stronger attachments to actions that were immediately rewarded, leading to a "super-reinforcement" effect where a single reward triggers a compulsive repetition of that choice, regardless of the underlying Model-Based or Model-Free values.

```python
def cognitive_model3_oci_reinforced_perseveration(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Learner with OCI-Modulated Reinforced Perseveration (Win-Stay).
    
    Hypothesis: OCI amplifies the tendency to repeat a choice specifically if it was rewarded 
    on the previous trial (Win-Stay). This is distinct from general stickiness.
    
    Logits += win_stay_strength * OCI * I(action == last_action) * last_reward
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - win_stay_strength: [0, 5] Bonus added to the previous choice IF it was rewarded, scaled by OCI.
    """
    learning_rate, beta, w, win_stay_strength = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    last_action = -1
    last_reward = 0.0
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        
        # Apply Win-Stay bonus scaled by OCI
        if last_action != -1 and last_reward > 0:
            logits_1[last_action] += win_stay_strength * oci_score * last_reward
            
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update history
        last_action = action_1[trial]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_reward = reward[trial]
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```