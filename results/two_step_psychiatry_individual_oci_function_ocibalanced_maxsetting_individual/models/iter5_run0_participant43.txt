Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Reward Valuation Model.
    
    Hypothesis: Participants with high OCI symptoms may exhibit hypersensitivity 
    to positive feedback (or relief). This model scales the effective reward 
    value based on the OCI score. A higher effective reward increases the 
    asymptotic value of the chosen options, effectively increasing the drive 
    to repeat rewarded actions (interacting with beta) and making habits stronger.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) strategies.
    stickiness: [0, 5] Tendency to repeat the previous Stage 1 choice.
    oci_val_boost: [0, 5] Scaling factor for reward valuation. 
                   Effective Reward = Reward * (1 + OCI * oci_val_boost).
    """
    lr, beta, w, stickiness, oci_val_boost = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Transition matrix for MB (assumed fixed structural knowledge)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the previously chosen action
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Scale reward based on OCI
        r = reward[trial]
        r_effective = r * (1.0 + participant_oci * oci_val_boost)
        
        # Stage 2 Update (TD)
        pe_2 = r_effective - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        # Stage 1 Update (TD)
        # Using the standard aggregation of prediction errors
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        q_stage1_mf[a1] += lr * pe_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Distorted Transition Belief Model.
    
    Hypothesis: High OCI is associated with an 'Intolerance of Uncertainty'. 
    These participants may distort the transition probabilities in their 
    Model-Based planning, perceiving the 'common' transition as more deterministic 
    (or less, if anxious/doubting) than it actually is (0.7).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stickiness: [0, 5] Choice stickiness.
    oci_trans_bias: [0, 1] Distortion of transition belief. 
                    Maps to a shift in probability roughly +/- 0.3 scaled by OCI.
                    p_common = 0.7 + (OCI * (oci_trans_bias - 0.5) * 0.6).
    """
    lr, beta, w, stickiness, oci_trans_bias = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate subjective transition probability
    # Map oci_trans_bias (0 to 1) to a shift range roughly [-0.3, 0.3]
    shift = (oci_trans_bias - 0.5) * 0.6
    p_common = 0.7 + (participant_oci * shift)
    p_common = np.clip(p_common, 0.01, 0.99) # Ensure valid prob
    p_rare = 1.0 - p_common
    
    # Subjective Transition Matrix
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    prev_a1 = -1
    
    for trial in range(n_trials):
        # MB Planning with distorted matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        q_stage1_mf[a1] += lr * pe_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Surprise Sensitivity Model.
    
    Hypothesis: High OCI participants may react more strongly to 'surprise' 
    (rare transitions). When a rare transition occurs (e.g., Spaceship A -> Planet Y), 
    their learning rate for the Stage 1 Model-Free value is amplified, representing 
    an anxiety-driven over-correction or 'startle' learning response.
    
    Parameters:
    lr_base: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stickiness: [0, 5] Choice stickiness.
    oci_surprise_shock: [0, 10] Multiplier for learning rate on rare transitions.
                        LR_rare = LR_base * (1 + OCI * oci_surprise_shock).
    """
    lr_base, beta, w, stickiness, oci_surprise_shock = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    prev_a1 = -1
    
    for trial in range(n_trials):
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Determine if transition was rare
        # Common: 0->0, 1->1. Rare: 0->1, 1->0.
        is_rare = (a1 != s_curr)
        
        # Calculate context-dependent Learning Rate for Stage 1
        if is_rare:
            lr_s1 = lr_base * (1.0 + participant_oci * oci_surprise_shock)
            lr_s1 = np.minimum(lr_s1, 1.0) # Cap at 1.0
        else:
            lr_s1 = lr_base
            
        # Stage 2 update uses base LR
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr_base * pe_2
        
        # Stage 1 update
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * pe_1
        q_stage1_mf[a1] += lr_s1 * pe_2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```