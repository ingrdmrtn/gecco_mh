Here are three new cognitive models that incorporate the OCI-R score to explain the participant's behavior, specifically focusing on mechanisms related to obsessive-compulsive traits like habit formation, outcome sensitivity, and uncertainty avoidance.

### Model 1: OCI-Modulated Habit Strength (Model-Free Dominance)
This model hypothesizes that high OCI scores lead to a dominance of the habitual (Model-Free) system over the goal-directed (Model-Based) system. Instead of a fixed mixing weight `w`, the balance between MB and MF control is dynamically shifted by the OCI score. Specifically, higher OCI acts as a "habit amplifier," pushing the agent to rely more on cached values (MF) rather than the transition structure (MB), reflecting the compulsive nature of repetition seen in the data.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Habit Strength.
    
    This model posits that OCI symptoms shift the balance between Model-Based (MB)
    and Model-Free (MF) control. A baseline mixing weight `w_base` is modified
    by the OCI score. Higher OCI reduces the influence of the MB system (planning)
    and increases reliance on the MF system (habit), potentially explaining the
    repetitive choice patterns.

    Bounds:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (softness of choice).
    w_base: [0, 1] Baseline mixing weight for MB control (1=pure MB, 0=pure MF).
    oci_habit_strength: [0, 1] Strength of OCI's push towards MF control.
    """
    lr, beta, w_base, oci_habit_strength = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w based on OCI. 
    # High OCI reduces w (less MB, more MF).
    # If oci_habit_strength is high, w approaches 0.
    w = w_base * (1.0 - (oci_score * oci_habit_strength))
    # Ensure w stays in [0, 1]
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value: V(s') = max(Q(s', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mixed Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 1 update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Asymmetric Learning (Loss Aversion)
This model investigates if high OCI scores relate to an increased sensitivity to failure or "zero reward" outcomes. People with high OCI often exhibit perfectionism or fear of errors. Here, the learning rate is split into positive and negative components, but the negative learning rate is specifically amplified by the OCI score. This means the participant "over-learns" from non-rewarded trials, potentially leading to rapid switching or rigid avoidance after a loss.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Asymmetric Learning (Loss Sensitivity).
    
    This model proposes that OCI symptoms amplify learning from negative outcomes 
    (absence of reward). The learning rate for unrewarded trials (lr_neg) is 
    calculated as a base rate plus an OCI-dependent boost. This reflects a 
    hyper-sensitivity to "failure" or missing gold coins.

    Bounds:
    lr_pos: [0, 1] Learning rate for rewarded trials.
    lr_neg_base: [0, 1] Base learning rate for unrewarded trials.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    oci_loss_amp: [0, 5] Amplification of negative learning rate by OCI.
    """
    lr_pos, lr_neg_base, beta, w, oci_loss_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Determine learning rate based on outcome
        if r > 0:
            current_lr = lr_pos
        else:
            # OCI amplifies the impact of negative outcomes (0 reward)
            # We clip to ensure it doesn't exceed 1.0 or become unstable
            current_lr = np.clip(lr_neg_base + (oci_score * oci_loss_amp), 0.0, 1.0)
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Scaled Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) commonly used in reinforcement learning to bridge the gap between the first action and the final reward. Here, we hypothesize that OCI affects how much credit is assigned to the first-stage choice based on the second-stage outcome. A high OCI might lead to "magical thinking" or over-attribution of causality, where the initial choice is seen as strictly determining the final outcome, regardless of the intermediate probabilistic transition.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Scaled Eligibility Trace (Lambda).
    
    This model uses an eligibility trace parameter (lambda) to update Stage 1 values
    directly from the reward, skipping the Stage 2 value estimate partially.
    We hypothesize that OCI affects this 'causal bridging'. High OCI might
    increase lambda, making the agent update the first stage choice strongly based
    on the final reward, effectively ignoring the two-step structure (flattening the task).

    Bounds:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    lambda_base: [0, 1] Base eligibility trace decay parameter.
    oci_lambda_scale: [0, 1] Scaling factor for OCI on lambda.
    """
    lr, beta, w, lambda_base, oci_lambda_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective lambda
    # High OCI increases lambda -> stronger direct connection between Choice 1 and Reward
    lam = np.clip(lambda_base + (oci_score * oci_lambda_scale), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Standard TD error for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Eligibility Trace update for Stage 1
        # Instead of just updating from Q_stage2 (TD(0)), we mix in the actual reward
        # This is equivalent to TD(lambda) logic where Stage 1 gets credit for R
        
        # TD error at stage 1 usually: Q2 - Q1
        # With lambda=1, it becomes: R - Q1 (ignoring Q2 change)
        # We implement a simplified update: Q1 += lr * ( (1-lambda)*(Q2-Q1) + lambda*(R-Q1) )
        # Which simplifies to: Q1 += lr * ( (1-lambda)*Q2 + lambda*R - Q1 )
        
        target = (1 - lam) * q_stage2_mf[s_idx, a2] + lam * r
        delta_stage1_trace = target - q_stage1_mf[a1]
        
        q_stage1_mf[a1] += lr * delta_stage1_trace

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```