Here are three new cognitive model proposals based on the provided task and participant data.

### Model 1: Model-Based/Model-Free Hybrid with OCI-modulated Mixing Weight
This model assumes participants use a mixture of Model-Based (planning) and Model-Free (habitual) strategies. The core hypothesis is that OCI symptoms might relate to the balance between these systems (e.g., higher compulsivity leading to more habitual/model-free control). Since the participant has a low OCI score (0.3), this model tests if `w` (the weight of model-based control) is dynamically set by their OCI score using a logistic function to keep it bounded.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight w is determined by OCI.
    
    The mixing weight w [0, 1] determines the contribution of MB vs MF values.
    w = 1 is fully Model-Based, w = 0 is fully Model-Free.
    We model w as a logistic function of OCI: w = 1 / (1 + exp(-(w_intercept + w_slope * oci)))
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_intercept: [-5, 5] Intercept for the logistic function determining w.
    w_slope: [-5, 5] Slope for the logistic function determining w.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI
    # Using logistic function to bound w between 0 and 1
    logit_w = w_intercept + w_slope * oci_score
    w = 1.0 / (1.0 + np.exp(-logit_w))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (Planet) x Action (Alien)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy: Mix MB and MF
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        # TD(0) for Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD(1) / Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update for Stage 1 MF (connecting outcome to stage 1 choice)
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model proposes that OCI scores influence how participants learn from positive versus negative prediction errors. Compulsive tendencies might be associated with an over-sensitivity to negative feedback (or lack of reward) or a rigidity in updating. Here, we define a base learning rate and an asymmetry parameter that scales the learning rate for negative prediction errors, with the asymmetry itself modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with asymmetric learning rates for positive/negative RPEs.
    The asymmetry is modulated by OCI.
    
    alpha_pos = lr_base
    alpha_neg = lr_base * (1 + asymmetry_oci_factor * oci)
    
    If asymmetry_oci_factor > 0, higher OCI leads to faster learning from negative outcomes.
    
    Parameters:
    lr_base: [0, 1] Base learning rate (for positive RPEs).
    beta: [0, 10] Inverse temperature.
    asymmetry_oci_factor: [-2, 2] How much OCI scales the negative learning rate relative to positive.
    """
    lr_base, beta, asymmetry_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Define learning rates
    alpha_pos = lr_base
    # Factor to scale negative learning rate. 
    # We clip to ensure the resulting alpha_neg is reasonable [0, 1]
    neg_scale = 1.0 + asymmetry_oci_factor * oci_score
    alpha_neg = np.clip(lr_base * neg_scale, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1: Pure Model-Based for simplicity to isolate learning rate effects
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Update Stage 2 Values
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Uncertainty-Weighted Exploration with OCI Modulation
This model posits that OCI affects how much uncertainty (variance) in value estimates drives exploration. Instead of just a softmax on Q-values, we track the variance of the Q-values (uncertainty). The choice policy includes an "uncertainty bonus" (or penalty). The magnitude of this bonus is scaled by the OCI score, testing the hypothesis that OCI relates to intolerance of uncertainty or specific exploratory drives.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with Uncertainty-Weighted exploration (Kalman Filter-like updates).
    The exploration bonus (phi) attached to uncertainty is modulated by OCI.
    
    Q-values and variances (sigma) are tracked.
    Policy uses Q + phi * sqrt(sigma).
    phi = phi_base + phi_oci_slope * oci
    
    Parameters:
    learning_rate: [0, 1] Serves as the Kalman gain proxy or fixed learning rate for mean.
    beta: [0, 10] Inverse temperature.
    phi_base: [-5, 5] Base exploration bonus parameter.
    phi_oci_slope: [-5, 5] Modulation of exploration bonus by OCI.
    """
    learning_rate, beta, phi_base, phi_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Exploration bonus parameter
    phi = phi_base + phi_oci_slope * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Means and Variances for Stage 2
    q_stage2_mean = np.zeros((2, 2)) + 0.5 # Initialize at 0.5
    q_stage2_var = np.ones((2, 2))         # Initialize high uncertainty
    
    # Simple decay for variance to simulate non-stationary environment
    var_decay = 0.95 
    innovation_var = 0.1 # Added uncertainty per trial

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Planning
        # We propagate the "uncertainty-augmented" values
        # UCB value = Mean + phi * std_dev
        stage2_ucb = q_stage2_mean + phi * np.sqrt(q_stage2_var)
        max_stage2_val = np.max(stage2_ucb, axis=1)
        
        q_stage1_mb = transition_matrix @ max_stage2_val
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        # Choose based on UCB values in the current state
        current_state_vals = stage2_ucb[s_idx]
        exp_q2 = np.exp(beta * current_state_vals)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Update Stage 2 Mean and Variance
        # Approximate Kalman update
        # Prediction error
        pe = r - q_stage2_mean[s_idx, a2]
        
        # Update mean
        q_stage2_mean[s_idx, a2] += learning_rate * pe
        
        # Update variance: Reduce variance for chosen option
        q_stage2_var[s_idx, a2] *= (1.0 - learning_rate)
        
        # Diffuse variance for all options (forgetting/drift)
        q_stage2_var = q_stage2_var * var_decay + innovation_var

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```