Here are three new cognitive models that incorporate OCI scores to explain decision-making behavior in the two-step task.

### Model 1: OCI-Modulated Choice Trace Stickiness
This model hypothesizes that obsessive-compulsive traits relate to a "habit strength" or perseveration that accumulates over time, rather than just repeating the immediately previous choice. It uses an exponentially decaying trace of past choices. The influence of this trace on current choice is scaled by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Choice Trace Stickiness.
    
    Hypothesis: Compulsivity (OCI) modulates the weight of a longer-term 
    perseverative bias (trace stickiness). High OCI leads to stronger 
    adherence to accumulated past choices.
    
    Q_net(a) = w * Q_mb(a) + (1-w) * Q_mf(a) + stick_weight * oci * trace(a)
    
    The trace accumulates when an action is chosen and decays on every trial.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - trace_decay: [0, 1] Decay rate of the choice trace (1 = no decay, 0 = instant decay).
    - stick_weight: [0, 10] Scaling factor for the influence of the trace, modulated by OCI.
    """
    learning_rate, beta, w, trace_decay, stick_weight = model_parameters
    oci_val = oci[0]
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize choice trace (value 0 to 1 usually, but can accumulate > 1)
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Stickiness Bonus
        stick_bonus = stick_weight * oci_val * choice_trace
        
        # Net Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_bonus
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        p_choice_1[trial] = exp_q1[a1] / np.sum(exp_q1)
        
        # Update Trace
        choice_trace *= trace_decay
        choice_trace[a1] += 1.0
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        p_choice_2[trial] = exp_q2[a2] / np.sum(exp_q2)
        
        # --- Learning Updates ---
        # TD Error Stage 1
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # TD Error Stage 2
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        # MF Stage 1 update from Stage 2 RPE
        q_stage1_mf[a1] += learning_rate * delta_2
        
    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Rare Transition Learning
This model investigates whether OCI scores affect how participants learn from "surprising" (rare) transitions. It proposes that the learning rate during rare transitions is modulated by OCI, potentially reflecting hyper-vigilance or altered error processing in obsessive-compulsive traits.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Rare Transition Learning.
    
    Hypothesis: OCI modulates the learning rate specifically when a rare transition occurs.
    High OCI may lead to over-updating (or under-updating) beliefs after a model violation.
    
    If rare transition: effective_lr = learning_rate * (1 + rare_mod * oci)
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - rare_mod: [0, 10] Multiplier for the learning rate on rare transitions, scaled by OCI.
    """
    learning_rate, beta, w, rare_mod = model_parameters
    oci_val = oci[0]
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        p_choice_1[trial] = exp_q1[a1] / np.sum(exp_q1)
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        p_choice_2[trial] = exp_q2[a2] / np.sum(exp_q2)
        
        # --- Learning Updates ---
        # Identify Rare Transition
        # Common: (A->X/0->0), (U->Y/1->1). Rare: (A->Y/0->1), (U->X/1->0).
        is_rare = False
        if (a1 == 0 and s_idx == 1) or (a1 == 1 and s_idx == 0):
            is_rare = True
            
        # Calculate Effective Learning Rate
        current_lr = learning_rate
        if is_rare:
            # Scale LR by OCI factor, clamp to 1.0 to prevent instability
            factor = 1.0 + rare_mod * oci_val
            current_lr = min(1.0, learning_rate * factor)
        
        # TD Stage 1
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_1
        
        # TD Stage 2 (Standard LR)
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        # MF Stage 1 update from Stage 2 RPE (using effective LR)
        q_stage1_mf[a1] += current_lr * delta_2
        
    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Sigmoid MB/MF Balance
This model proposes a non-linear, sigmoidal relationship between OCI and the balance between Model-Based and Model-Free control (`w`). This allows for a smooth transition where low OCI individuals might be high MB, and as OCI increases past a threshold, behavior shifts rapidly towards MF (habitual).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Sigmoid OCI-Modulated MB/MF Balance.
    
    Hypothesis: The balance 'w' between goal-directed (MB) and habitual (MF) control
    is a logistic function of OCI. This models a threshold effect where compulsivity 
    shifts control systems.
    
    w = 1 / (1 + exp(slope * (oci - center)))
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_slope: [0, 10] Steepness of the transition. Positive slope implies High OCI -> Low w (MF).
    - w_center: [0, 1] The OCI threshold where w = 0.5.
    """
    learning_rate, beta, w_slope, w_center = model_parameters
    oci_val = oci[0]
    
    # Calculate w using sigmoid function
    # logit = slope * (oci - center)
    # If slope > 0: OCI > center -> positive logit -> exp > 1 -> w < 0.5 (More MF)
    logit = w_slope * (oci_val - w_center)
    # Clamp logit to avoid overflow in exp
    logit = np.clip(logit, -20, 20)
    w = 1.0 / (1.0 + np.exp(logit))
    
    n_trials = len(action_1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        p_choice_1[trial] = exp_q1[a1] / np.sum(exp_q1)
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        p_choice_2[trial] = exp_q2[a2] / np.sum(exp_q2)
        
        # --- Learning Updates ---
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        q_stage1_mf[a1] += learning_rate * delta_2
        
    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```