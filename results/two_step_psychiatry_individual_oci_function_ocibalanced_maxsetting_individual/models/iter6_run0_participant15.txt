Here are three new cognitive models that explore different mechanisms by which OCI scores might influence decision-making in the two-step task, specifically focusing on how obsessive-compulsive traits might alter value updating, exploration, or sensitivity to rare events.

### Model 1: OCI-Modulated "Win-Stay, Lose-Shift" Sensitivity
This model hypothesizes that individuals with higher OCI scores might exhibit an altered sensitivity to outcomes, specifically a heightened "punishment" signal (loss aversion) or rigidity. Instead of just modulating the learning rate, OCI here modulates the effective reward perception. High OCI might make a lack of reward (0) feel like a punishment (negative value), or amplify the value of a reward, driving more rigid behavior. Here, we test if OCI modulates the effective magnitude of the reward signal itself before it enters the prediction error calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Reward Sensitivity.
    Hypothesis: High OCI individuals perceive rewards/losses differently.
    This model scales the reward input by an OCI-dependent factor.
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between MB (1) and MF (0).
    stick: [0, 5] Choice stickiness.
    rew_sens_oci: [0, 5] OCI-dependent modulation of reward magnitude.
                  effective_reward = reward * (1 + rew_sens_oci * OCI)
    """
    learning_rate, beta, w, stick, rew_sens_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Initialize stickiness variable
    last_action_1 = -1

    for trial in range(n_trials):
        # Skip invalid trials if any (though data seems clean, good practice)
        if np.isnan(action_1[trial]) or np.isnan(action_2[trial]):
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy for Choice 1 ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Apply OCI modulation to reward perception
        # High OCI might amplify the signal of the reward
        effective_reward = r * (1.0 + rew_sens_oci * oci_score)

        # Update Stage 1 MF (TD(1) style or simple TD(0) to stage 2 value)
        # Standard approach: update Q1 based on Q2
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = effective_reward - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    # Filter out 0 probabilities to avoid log(0)
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Eligibility Trace (Lambda)
This model posits that OCI affects how credit is assigned to the first-stage choice based on the second-stage outcome. In reinforcement learning terms, this is the eligibility trace parameter $\lambda$. A high OCI score might lead to "over-thinking" or excessive attribution of the final outcome to the initial choice (high $\lambda$), effectively blurring the distinction between the two stages and updating the first stage directly from the reward. Conversely, it could represent a failure to disconnect the stages.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Eligibility Trace (Lambda).
    OCI modulates the eligibility trace parameter lambda.
    Lambda controls how much the Stage 2 Reward directly updates Stage 1 values.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    stick: [0, 5] Stickiness.
    lambda_base: [0, 1] Base eligibility trace.
    """
    learning_rate, beta, w, stick, lambda_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # OCI modulates lambda. We assume OCI increases the trace (more credit assignment to stage 1)
    # Mapping OCI [0,1] to influence lambda. 
    # We formulate lambda = lambda_base + (1-lambda_base) * OCI * 0.5 (scaled to keep bounds safe)
    # Alternatively, simply let the optimizer find a lambda, but here we want OCI to force it.
    # Let's try: lambda_eff = lambda_base + (0.5 * oci_score)
    # We clip it to [0,1].
    lambda_eff = lambda_base + (0.5 * oci_score)
    if lambda_eff > 1.0: lambda_eff = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if np.isnan(action_1[trial]) or np.isnan(action_2[trial]):
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Stage 2 PE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 1 PE (Standard MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1: 
        # Standard Q-learning part: learning_rate * delta_stage1
        # Eligibility trace part: learning_rate * lambda * delta_stage2
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_eff * delta_stage2)
        
        last_action_1 = a1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: OCI-Driven Decay of Unchosen Options
This model suggests that high OCI is associated with "forgetting" or discounting the value of options that are not currently being engaged with. This reflects a form of attentional narrowing or obsession with the current path. When an action is taken, unchosen actions decay toward 0. The rate of this decay is modulated by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Decay of Unchosen Options.
    High OCI accelerates the decay of unchosen action values (forgetting).
    This simulates an 'obsession' with the current choice path and neglect of alternatives.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    stick: [0, 5] Stickiness.
    decay_rate_oci: [0, 1] OCI-dependent decay rate for unchosen options.
                    decay = decay_rate_oci * OCI.
    """
    learning_rate, beta, w, stick, decay_rate_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    decay = decay_rate_oci * oci_score
    if decay > 1.0: decay = 1.0
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if np.isnan(action_1[trial]) or np.isnan(action_2[trial]):
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Update chosen Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)

        # Update chosen Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 (in the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay)
        
        last_action_1 = a1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```