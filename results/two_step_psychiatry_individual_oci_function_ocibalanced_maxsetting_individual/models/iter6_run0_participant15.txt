Here are three cognitive models designed to explain the participant's behavior, incorporating their OCI score into distinct mechanisms not previously explored in the provided list.

### Model 1: Subjective Valuation of Non-Reward (Rho-Model)
This model hypothesizes that individuals with varying OCI levels perceive the "absence of reward" (0 coins) differently. While standard Q-learning treats 0 as a neutral baseline, compulsive individuals might perceive it as an aversive failure (punishment) or a relief, altering the effective reward signal driving the updates.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Valuation of Non-Reward (Rho-Model).

    Hypothesis: OCI modulates the subjective value assigned to non-rewarded outcomes (0 coins).
    Instead of 0, the agent perceives a value 'rho'. If rho < 0, non-reward is punishing.
    This effective reward propagates through both Stage 2 and Stage 1 updates.

    Parameters:
    - alpha: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Model-Based (1) vs Model-Free (0) weighting.
    - stickiness: [0, 5] Choice perseveration bonus.
    - rho_base: [-1, 1] Baseline subjective value of 0 coins.
    - rho_oci_mod: [-1, 1] Modulation of rho by OCI score.
      rho = rho_base + (rho_oci_mod * oci)
    """
    alpha, beta, w, stickiness, rho_base, rho_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate subjective non-reward value
    rho = rho_base + (rho_oci_mod * current_oci)
    # rho represents the value experienced when reward is 0. 
    # It is not clipped to [0,1] because it could be negative (punishment).

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 1.0/2.0
            p_choice_2[trial] = 1.0/2.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Determine effective reward
        # If r > 0 (1 coin), use actual reward. If r == 0, use subjective rho.
        r_effective = r if r > 0.0 else rho

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r_effective - q_stage2_mf[s_idx, a2]

        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        q_stage1_mf[a1] += alpha * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dynamic Transition Learning
This model hypothesizes that OCI affects the stability of the participant's internal model of the environment. Instead of assuming a fixed transition matrix (0.7/0.3), the agent actively learns the transition probabilities. OCI modulates how rapidly they update this belief (`alpha_trans`), reflecting potential hyper-sensitivity to structural changes (or conversely, rigidity).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Transition Learning Model.

    Hypothesis: The agent does not assume a fixed 70/30 transition structure but learns
    it from experience. OCI modulates the learning rate of this transition model (`alpha_trans`).
    High OCI may lead to over-updating the structural model based on rare transitions.

    Parameters:
    - alpha_val: [0, 1] Learning rate for Q-values (reward learning).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weighting.
    - stickiness: [0, 5] Choice perseveration.
    - alpha_trans_base: [0, 1] Base learning rate for transition probabilities.
    - alpha_trans_oci: [-1, 1] OCI modulation of transition learning rate.
    """
    alpha_val, beta, w, stickiness, alpha_trans_base, alpha_trans_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate transition learning rate
    alpha_trans = alpha_trans_base + (alpha_trans_oci * current_oci)
    alpha_trans = np.clip(alpha_trans, 0.0, 1.0)

    # Initialize dynamic belief about common transition prob (starts at ground truth 0.7)
    p_common_belief = 0.7 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 1.0/2.0
            p_choice_2[trial] = 1.0/2.0
            continue

        # --- Construct Dynamic Transition Matrix ---
        # Matrix: [[P(S0|A0), P(S1|A0)], [P(S0|A1), P(S1|A1)]]
        # Assuming symmetry in belief: P(S0|A0) = P(S1|A1) = p_common_belief
        transition_matrix = np.array([
            [p_common_belief, 1.0 - p_common_belief],
            [1.0 - p_common_belief, p_common_belief]
        ])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]

        q_stage2_mf[s_idx, a2] += alpha_val * delta_stage2
        q_stage1_mf[a1] += alpha_val * (delta_stage1 + delta_stage2)

        # --- Transition Model Update ---
        # Did the transition match the "common" structure?
        # A0(0) -> S0(0) is Common. A1(1) -> S1(1) is Common.
        is_common_transition = 1.0 if a1 == s_idx else 0.0
        
        # Update belief about p_common
        p_common_belief += alpha_trans * (is_common_transition - p_common_belief)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Passive Forgetting
This model proposes that OCI influences the rate at which value information decays for *unchosen* options. Standard Q-learning holds unchosen values constant. Here, unchosen Stage 1 Q-values decay toward zero. OCI modulates this forgetting rate, reflecting either obsessive retention of old values (low forgetting) or cognitive noise/uncertainty (high forgetting).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Passive Forgetting Model.

    Hypothesis: Unchosen action values in Stage 1 decay over time (passive forgetting).
    OCI modulates this forgetting rate. High OCI might imply inability to forget/disengage 
    (low decay) or high noise (high decay).

    Parameters:
    - alpha: [0, 1] Learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weighting.
    - stickiness: [0, 5] Perseveration bonus.
    - forget_base: [0, 1] Base decay rate for unchosen Q-values.
    - forget_oci: [-1, 1] OCI modulation of forgetting rate.
      forget_rate = forget_base + (forget_oci * oci)
    """
    alpha, beta, w, stickiness, forget_base, forget_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate forgetting rate
    forget_rate = forget_base + (forget_oci * current_oci)
    forget_rate = np.clip(forget_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            p_choice_1[trial] = 1.0/2.0
            p_choice_2[trial] = 1.0/2.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        q_augmented = q_net.copy()
        if last_action_1 != -1:
            q_augmented[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Active Updates (Chosen) ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]

        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        q_stage1_mf[a1] += alpha * (delta_stage1 + delta_stage2)

        # --- Passive Updates (Unchosen) ---
        # The unchosen spaceship's MF value decays towards 0
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```