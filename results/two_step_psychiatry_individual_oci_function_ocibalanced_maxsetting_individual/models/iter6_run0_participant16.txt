Here are three new cognitive models that incorporate the Obsessive-Compulsive Inventory (OCI) score to explain participant behavior in the two-step task.

### Model 1: Unchosen Value Decay Model
This model hypothesizes that OCI severity affects the "forgetting" or decay rate of the *unchosen* spaceship's value (Stage 1). While standard Q-learning only updates the chosen option, this model assumes the unchosen option's value decays towards 0 over time. High OCI participants may exhibit different decay rates (e.g., rigid memory vs. rapid forgetting/anxiety).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Unchosen Value Decay Model.
    
    Hypothesis: OCI severity modulates the decay rate of the unchosen spaceship's value in Stage 1.
    This captures whether participants maintain values of unvisited states (rigid memory) 
    or let them fade (uncertainty/anxiety).
    
    Parameters:
    - lr: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight (0 = Model-Free, 1 = Model-Based).
    - stickiness: [0, 5] General choice stickiness bonus.
    - q_decay_low: [0, 1] Decay rate for unchosen Q-value (Low OCI).
    - q_decay_high: [0, 1] Decay rate for unchosen Q-value (High OCI).
    """
    lr, beta, w, stickiness, q_decay_low, q_decay_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate decay rate based on OCI score
    q_decay = q_decay_low * (1 - current_oci) + q_decay_high * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Updates
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 1 Update (Chosen)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 1 Decay (Unchosen)
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - q_decay)
        
        last_action_1 = a1

    return -log_likelihood
```

### Model 2: Dynamic Learning Rate (Pearce-Hall style) Model
This model implements a dynamic learning rate that scales with the magnitude of the previous prediction error (surprise). The sensitivity to this surprise (associability) is modulated by OCI. This tests if OCI participants are hyper-vigilant to errors, increasing their learning rate drastically when outcomes are unexpected.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Learning Rate (Pearce-Hall style) Model.
    
    Hypothesis: The learning rate is not fixed but dynamic, scaling with the absolute value 
    of the previous prediction error (surprise). OCI modulates the 'associability' (k), 
    determining how much surprise increases the learning rate.
    
    Parameters:
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Choice stickiness.
    - lr_base: [0, 1] Baseline learning rate component.
    - k_low: [0, 5] Associability parameter for Low OCI (sensitivity to PE).
    - k_high: [0, 5] Associability parameter for High OCI.
    """
    beta, w, stickiness, lr_base, k_low, k_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate associability k
    k = k_low * (1 - current_oci) + k_high * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_delta_1 = 0.0 # Initialize previous PE
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate dynamic learning rate based on previous surprise
        current_lr = lr_base + k * np.abs(last_delta_1)
        current_lr = np.clip(current_lr, 0.0, 1.0)

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        last_action_1 = a1
        last_delta_1 = delta_stage1

    return -log_likelihood
```

### Model 3: Transition-Dependent Stickiness Model
This model differentiates stickiness (perseveration) based on whether the previous trial's transition was "Common" or "Rare". It hypothesizes that rare transitions might disrupt the repetitive habit in some participants, but high OCI participants might maintain stickiness even after surprising transitions (or vice versa).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Transition-Dependent Stickiness Model.
    
    Hypothesis: Stickiness strength depends on whether the previous transition was Common or Rare.
    OCI modulates the stickiness specifically after Rare transitions, testing if 'surprising'
    transitions disrupt compulsive repetition differently for high OCI individuals.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stick_common: [0, 5] Stickiness after a Common transition.
    - stick_rare_low: [0, 5] Stickiness after a Rare transition (Low OCI).
    - stick_rare_high: [0, 5] Stickiness after a Rare transition (High OCI).
    """
    lr, beta, w, stick_common, stick_rare_low, stick_rare_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    stick_rare = stick_rare_low * (1 - current_oci) + stick_rare_high * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_transition_rare = False # Initialize
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            if last_transition_rare:
                q_net[last_action_1] += stick_rare
            else:
                q_net[last_action_1] += stick_common
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)
        
        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1
        
        # Determine if current transition was rare
        # Common: a1=0->s=0, a1=1->s=1. Rare: a1=0->s=1, a1=1->s=0.
        if a1 == s_idx:
            last_transition_rare = False
        else:
            last_transition_rare = True

    return -log_likelihood
```