Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task. These models focus on how OCI might affect the balance between model-based and model-free control, as well as how it might modulate learning rates for positive versus negative outcomes.

### Model 1: OCI-Modulated Mixing Weight ($w$)
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) strategies is directly influenced by the participant's OCI score. A parameter $w$ controls this trade-off ($w=1$ is fully Model-Based, $w=0$ is fully Model-Free). Here, $w$ is a linear function of the OCI score, testing if higher OCI leads to more habitual or more planning-based behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where the mixing weight w is modulated by OCI.
    w determines the balance: 1 = fully Model-Based, 0 = fully Model-Free.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1]
    w_oci_mod: [-1, 1]
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate mixing weight w, clamped between 0 and 1
    w = w_base + (w_oci_mod * current_oci)
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (also used for MB planning)

    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue # Skip update if data missing

        s_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]

            # --- Learning Updates ---
            
            # Stage 2 Update (standard Q-learning)
            # PE2 = r - Q(s2, a2)
            pe_2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += learning_rate * pe_2
            
            # Stage 1 MF Update (SARSA-style or TD(1))
            # Here we use the standard TD(1) style often used in Daw task: 
            # Update stage 1 MF value using the stage 2 prediction error
            q_stage1_mf[a1] += learning_rate * pe_2 
            
            # Note: We could also do a TD(0) update for Stage 1 (Q(s1,a1) -> Q(s2,a2)), 
            # but in the standard analysis, the eligibility trace implies the Stage 2 PE 
            # drives the Stage 1 MF update directly.
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Asymmetric Learning Rates (Pos/Neg)
This model investigates if OCI affects how participants learn from positive versus negative outcomes. It implements a Model-Free learner where the learning rate splits into $\alpha_{pos}$ and $\alpha_{neg}$. The "negative" learning rate (used when prediction error is negative, i.e., disappointment) is modulated by the OCI score. This reflects the hypothesis that compulsive individuals might be hyper-sensitive (or hypo-sensitive) to errors or lack of reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative prediction errors.
    The learning rate for negative PEs is modulated by OCI.
    
    Bounds:
    lr_pos: [0, 1]
    lr_neg_base: [0, 1]
    lr_neg_oci_mod: [-1, 1]
    beta: [0, 10]
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (lr_neg_oci_mod * current_oci)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue

        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]

            # --- Learning ---
            
            # Stage 2 PE
            pe_2 = r - q_stage2[s_idx, a2]
            
            # Choose learning rate based on sign of PE
            alpha_2 = lr_pos if pe_2 >= 0 else lr_neg
            q_stage2[s_idx, a2] += alpha_2 * pe_2

            # Stage 1 PE (TD(1) logic: update Stage 1 with Stage 2 outcome)
            # The simple MF update often uses the stage 2 PE directly for stage 1
            # or uses the sum of stage 1 PE (usually 0 here as rewards are at end) + stage 2 PE.
            # Here we apply the same asymmetry to the propagation.
            
            alpha_1 = lr_pos if pe_2 >= 0 else lr_neg
            q_stage1[a1] += alpha_1 * pe_2
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Second-Stage Stickiness
This model introduces a specific "stickiness" parameter for the *second* stage choices (choosing the alien). OCI is hypothesized to specifically increase repetitive behavior at the level of the concrete outcome (the alien), rather than the abstract spaceship choice. This model is purely Model-Based for the first stage (to isolate the effect), but adds a perseveration bonus to the second stage Q-values based on the previous choice in that specific state.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with OCI-modulated stickiness specifically on the second stage choices.
    This tests if OCI drives repetition of concrete actions (aliens) rather than abstract plans.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    stick_s2_base: [-5, 5]
    stick_s2_oci_mod: [-5, 5]
    """
    learning_rate, beta, stick_s2_base, stick_s2_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    eff_stickiness_s2 = stick_s2_base + (stick_s2_oci_mod * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2 = np.zeros((2, 2))
    
    # Track previous action for each state (planet)
    prev_a2_per_state = np.array([-1, -1]) 

    for trial in range(n_trials):

        # --- Stage 1: Pure Model-Based ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue

        s_idx = int(state[trial])

        # --- Stage 2: Value + Stickiness ---
        q_net_2 = q_stage2[s_idx].copy()
        
        # Add stickiness if we have visited this state before and made a choice
        if prev_a2_per_state[s_idx] != -1:
            q_net_2[prev_a2_per_state[s_idx]] += eff_stickiness_s2
            
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            # Update history for this state
            prev_a2_per_state[s_idx] = a2
            
            r = reward[trial]
            
            # Update Q-values
            pe_2 = r - q_stage2[s_idx, a2]
            q_stage2[s_idx, a2] += learning_rate * pe_2
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```