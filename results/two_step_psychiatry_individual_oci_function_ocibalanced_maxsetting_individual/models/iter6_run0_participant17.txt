def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based Transition Probability Distortion.
    
    This model assumes that OCI symptoms correlate with a distorted belief about the 
    reliability of the spaceship transitions (rigidity). High OCI leads to a belief 
    that the transitions are more deterministic than they actually are (base 0.7).
    The agent uses this subjective transition matrix for Model-Based planning.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for MF values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting between MB (1) and MF (0).
    distortion: [0, 1] - Magnitude of OCI-driven distortion of transition probabilities. 
                         (0 = Accurate 0.7, 1 = Strongly rigid).
    """
    learning_rate, beta, w, distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Subjective transition probability for the 'common' transition
    # Base is 0.7. OCI increases this towards 1.0 (rigidity).
    # We scale so that max distortion (1.0) * max oci (approx 1.0) adds 0.3.
    p_common = 0.7 + (distortion * oci_score * 0.3)
    # Clamp to ensure probability validity
    if p_common > 0.99: p_common = 0.99
    p_rare = 1.0 - p_common
    
    # Transition matrix: A->X (common), A->Y (rare); U->Y (common), U->X (rare)
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # MF values for stage 1 (Spaceships)
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of best option in next state using distorted matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Action Selection Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # TD(0) for Stage 1 (MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # TD(0) for Stage 2 (MF)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Counterfactual Regret Update Model.
    
    This model posits that OCI symptoms drive "regret" or obsessive "what if" thinking.
    When an outcome is observed, the participant also updates the value of the 
    unchosen spaceship, assuming the opposite outcome would have occurred (e.g., if I lost,
    I assume I would have won with the other choice). This drives switching behavior 
    mediated by OCI.
    
    Parameters:
    learning_rate: [0, 1] - Standard learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    regret_scale: [0, 1] - Strength of the counterfactual update, modulated by OCI.
    """
    learning_rate, beta, w, regret_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Standard Update Chosen
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Counterfactual Update Unchosen
        # If I won (1), assume unchosen would have lost (0).
        # If I lost (0), assume unchosen would have won (1).
        unchosen_a1 = 1 - a1
        counterfactual_r = 1.0 - r 
        
        # We update the unchosen Stage 1 value towards the counterfactual outcome
        effective_regret_lr = learning_rate * regret_scale * oci_score
        
        delta_regret = counterfactual_r - q_stage1_mf[unchosen_a1]
        q_stage1_mf[unchosen_a1] = q_stage1_mf[unchosen_a1] + effective_regret_lr * delta_regret
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Risk-Sensitive Model-Based Planning.
    
    This model assumes OCI introduces 'pessimism' or risk-aversion into the Model-Based 
    valuation step. Instead of assuming the agent will always get the max value 
    from the second stage (aliens), the agent mixes in the minimum value of the aliens
    in that state, representing a fear of the bad outcome (or checking behavior).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    pessimism: [0, 1] - Weight of the minimum value in planning (0=Max only, 1=Min only).
    """
    learning_rate, beta, w, pessimism = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # Calculate Risk-Sensitive Stage 2 Values for MB planning
        # Standard MB: val = max(Q_s2)
        # Risk-Sensitive: val = (1-rho)*max + rho*min
        rho = pessimism * oci_score
        if rho > 1.0: rho = 1.0
        
        max_vals = np.max(q_stage2_mf, axis=1)
        min_vals = np.min(q_stage2_mf, axis=1)
        
        risk_sensitive_vals = (1 - rho) * max_vals + rho * min_vals
        
        q_stage1_mb = transition_matrix @ risk_sensitive_vals
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss