Here are three new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: Uncertainty-Bonus Exploration
This model hypothesizes that the OCI score modulates "checking" behavior or intolerance of uncertainty. It adds an exploration bonus to the Stage 1 Q-values based on how long it has been since an option was last chosen. High OCI might lead to higher "checking" of unchosen options (uncertainty intolerance) or lower checking (rigid avoidance).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Uncertainty Bonus (Exploration).
    
    Hypothesis: OCI symptoms relate to 'checking' behavior or intolerance of uncertainty.
    This model adds a bonus to the Q-values of Stage 1 options proportional to the 
    time elapsed since they were last chosen. The magnitude of this bonus is modulated by OCI.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stickiness: [-5, 5]
    expl_bonus_base: [0, 2]
    expl_bonus_oci: [-2, 2]
    """
    learning_rate, beta, w, stickiness, expl_bonus_base, expl_bonus_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate exploration bonus parameter
    expl_bonus_param = expl_bonus_base + (expl_bonus_oci * oci_val)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track when each action was last chosen to calculate uncertainty
    # Initialize with -1 so first trial has some bonus if desired, or 0
    last_chosen_trial = np.array([-1, -1]) 

    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Add Uncertainty Bonus
        # Bonus increases with time since last choice
        # Calculate time since last choice for both options
        time_since = trial - last_chosen_trial
        # Logarithmic scaling for diminishing returns on uncertainty
        uncertainty = np.log(time_since + 2) # +2 to avoid log(0) or log(1)=0 issues at start
        
        logits_1 += expl_bonus_param * uncertainty
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update trackers
        a1 = int(action_1[trial])
        last_action_1 = a1
        last_chosen_trial[a1] = trial
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # TD Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]

        # Update Stage 1 (TD(1) logic: includes stage 2 error)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Independent Stage 2 Beta
This model proposes that OCI specifically modulates the decision noise (temperature) at the second stage (goal-seeking) independently of the first stage (planning). A high OCI might be associated with a more rigid (deterministic) or more erratic (noisy) selection of aliens, distinct from how they select spaceships.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Stage 2 Inverse Temperature.
    
    Hypothesis: OCI affects the exploration/exploitation balance differently at the 
    planning stage (Stage 1) versus the goal-seeking stage (Stage 2).
    This model uses separate Beta (inverse temperature) parameters for each stage,
    with the Stage 2 Beta modulated by OCI.
    
    Bounds:
    learning_rate: [0, 1]
    beta_1: [0, 10]
    beta_2_base: [0, 10]
    beta_2_oci: [-5, 5]
    w: [0, 1]
    stickiness: [-5, 5]
    """
    learning_rate, beta_1, beta_2_base, beta_2_oci, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate Stage 2 Beta
    beta_2 = beta_2_base + (beta_2_oci * oci_val)
    # Ensure beta_2 stays non-negative
    beta_2 = np.maximum(beta_2, 0.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        last_action_1 = a1
        state_idx = int(state[trial])

        # --- Stage 2 Policy (Uses beta_2) ---
        logits_2 = beta_2 * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]

        # Standard TD(1) updates
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Rare Transition Learning Modulation
This model posits that OCI affects how participants learn from "Rare" (unexpected) transitions. High OCI might lead to over-reacting to rare events (treating them as structural changes) or under-reacting (ignoring them as noise). The model applies a different learning rate for Stage 1 updates when a rare transition occurs.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Learning Rate for Rare Transitions.
    
    Hypothesis: OCI modulates the learning rate specifically when the transition
    from Stage 1 to Stage 2 is 'Rare' (unexpected). This reflects how the participant
    integrates prediction errors generated by low-probability events.
    
    Bounds:
    lr_common: [0, 1] - Learning rate for common transitions and stage 2.
    lr_rare_base: [0, 1] - Base learning rate for rare transitions.
    lr_rare_oci: [-1, 1] - Modulation of rare transition learning rate by OCI.
    beta: [0, 10]
    w: [0, 1]
    stickiness: [-5, 5]
    """
    lr_common, lr_rare_base, lr_rare_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate effective learning rate for rare transitions
    lr_rare = lr_rare_base + (lr_rare_oci * oci_val)
    lr_rare = np.clip(lr_rare, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        last_action_1 = a1
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]

        # Determine if transition was Common or Rare
        # Common: (Action 0 -> State 0) or (Action 1 -> State 1)
        # Rare: (Action 0 -> State 1) or (Action 1 -> State 0)
        is_common = (a1 == state_idx)
        
        current_lr = lr_common if is_common else lr_rare

        # Update Stage 1 using the specific learning rate for this transition type
        q_stage1_mf[a1] += current_lr * (delta_stage1 + delta_stage2)
        
        # Update Stage 2 (always use common/base learning rate)
        q_stage2_mf[state_idx, a2] += lr_common * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```