Here are three new cognitive models that introduce different mechanisms for how OCI scores might influence decision-making in this task, specifically focusing on perseveration, learning rate asymmetries, and uncertainty-driven exploration.

### Cognitive Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that individuals with higher OCI scores exhibit "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward. This compulsion to repeat actions is modeled as an additional bonus added to the Q-value of the previously chosen action, where the magnitude of this bonus is scaled by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with OCI-modulated choice perseveration (stickiness).
    Hypothesis: OCI is linked to compulsive repetition of actions. This model adds a 
    "stickiness" bonus to the previously chosen action at Stage 1, scaled by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Rate of value updating.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0) strategies.
    persev_base: [0, 5] Baseline tendency to repeat the previous choice.
    persev_oci: [0, 5] Additional perseveration scaled by OCI score.
    """
    learning_rate, beta, w, persev_base, persev_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate total stickiness bonus
    stickiness = persev_base + (persev_oci * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for stickiness (initialize to -1 or None)
    prev_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MF and MB values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[prev_action_1] += stickiness
        
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous action
        prev_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 update (TD learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD learning using Stage 2 value as proxy for reward)
        # Note: Standard Daw et al. 2011 uses Q(s2, a2) - Q(s1, a1)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Asymmetric Learning Rates
This model suggests that OCI affects how individuals process positive versus negative prediction errors. Specifically, it tests if higher OCI leads to a stronger reaction to negative outcomes (avoidance learning) compared to positive ones. The learning rate for negative prediction errors (disappointment) is scaled by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with OCI-modulated asymmetric learning rates.
    Hypothesis: High OCI individuals may be more sensitive to negative prediction errors 
    (failures) than positive ones. This model splits the learning rate into a positive 
    component (fixed) and a negative component that scales with OCI.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    lr_neg_oci: [0, 1] Additional sensitivity to negative errors scaled by OCI.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate negative learning rate based on OCI
    lr_neg = lr_neg_base + (lr_neg_oci * current_oci)
    lr_neg = np.clip(lr_neg, 0, 1) # Ensure bounds
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates with Asymmetric Learning Rates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        current_lr = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        current_lr = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Uncertainty Bonus (Exploration)
This model posits that OCI relates to intolerance of uncertainty. Instead of standard softmax exploration, this model includes an "uncertainty bonus" (or penalty) added to the Q-values. The magnitude of this bonus is determined by the inverse of the number of times an option has been visited (a simple proxy for uncertainty), and the weight of this bonus is modulated by the OCI score. A negative weight implies uncertainty avoidance.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with OCI-modulated uncertainty bonus/penalty.
    Hypothesis: OCI is linked to intolerance of uncertainty. This model adds an exploration 
    bonus based on visit counts (uncertainty) to the Q-values. The strength and direction 
    (seeking vs avoiding) of this bonus is modulated by OCI.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    uncert_base: [-1, 1] Baseline reaction to uncertainty (negative=avoidance, positive=seeking).
    uncert_oci: [-1, 1] OCI-dependent modulation of uncertainty reaction.
    """
    lr, beta, w, uncert_base, uncert_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate uncertainty weight
    # If (uncert_base + uncert_oci * oci) is negative, the agent avoids unvisited options.
    k_uncertainty = uncert_base + (uncert_oci * current_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track visit counts for uncertainty calculation
    # Initialize with 1 to avoid division by zero
    counts_stage1 = np.ones(2) 
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate uncertainty bonus: 1/sqrt(count) is a common exploration bonus
        uncert_bonus = 1.0 / np.sqrt(counts_stage1)
        q_net_uncert = q_net + (k_uncertainty * uncert_bonus)
        
        exp_q1 = np.exp(beta * q_net_uncert)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update counts
        counts_stage1[a1] += 1

        # --- Stage 2 Policy ---
        # Note: Uncertainty applied only to Stage 1 for simplicity in this model
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```