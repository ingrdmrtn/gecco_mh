Here are three new cognitive models for the two-step task, incorporating the OCI score to modulate specific mechanisms related to compulsivity, uncertainty, and learning.

### Model 1: OCI-Modulated Transition Sharpening (MB Distortion)
This model hypothesizes that individuals with higher OCI scores exhibit an "Intolerance of Uncertainty." In the Model-Based (MB) planning phase, they may perceive the probabilistic state transitions as more deterministic (sharpened) than they really are, effectively ignoring the rare transition probability.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Transition Sharpening (MB Distortion).
    
    Hypothesis: Participants with higher OCI scores exhibit an "Intolerance of Uncertainty,"
    leading them to perceive the probabilistic transition structure of the task as more 
    deterministic (sharpened) than it actually is. This distorts the Model-Based value calculation.
    
    Parameters:
    - lr: [0, 1] Learning rate for MF Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stick: [0, 5] Choice stickiness/perseveration.
    - sharp_k: [0, 5] Sharpening factor sensitivity to OCI.
      gamma = exp(sharp_k * oci).
      p_perceived = p^gamma / (p^gamma + (1-p)^gamma).
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, stick, sharp_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate distorted transition matrix based on OCI
    # True matrix probabilities: 0.7 (common) and 0.3 (rare)
    gamma = np.exp(sharp_k * oci_score)
    p = 0.7
    # Apply sharpening distortion
    p_sharp = (p**gamma) / (p**gamma + (1-p)**gamma)
    transition_matrix = np.array([[p_sharp, 1-p_sharp], [1-p_sharp, p_sharp]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next stage using distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Mixture of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Add Stickiness
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 MF Update (TD(0) using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF Update (using Reward)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated General Perseveration
This model tests the hypothesis that compulsivity (OCI score) is directly linked to behavioral rigidity or "stickiness." Instead of a fixed stickiness parameter, the tendency to repeat the previous choice is linearly modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated General Perseveration.
    
    Hypothesis: Compulsivity is characterized by repetitive behavior. This model posits that
    OCI score linearly increases the stickiness (choice perseveration) parameter, making
    high-OCI participants more likely to repeat their previous Stage 1 choice regardless of 
    reward history.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick_base: [0, 5] Baseline stickiness.
    - stick_oci: [0, 5] OCI sensitivity of stickiness.
      stick_eff = stick_base + stick_oci * oci.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    stick_eff = stick_base + stick_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Apply OCI-modulated stickiness
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick_eff
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1
        
    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Gating of Learning from Rare Transitions
This model suggests that high-OCI individuals may be "noise-averse" or rigid in their belief updating. They might discount the information gained from trials where a "rare" transition occurred (e.g., Spaceship A going to Planet Y), treating these events as outliers or bad luck rather than valid signals for value updates.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Gating of Learning from Rare Transitions.
    
    Hypothesis: High OCI participants may be more rigid or "noise-averse," treating 
    outcomes following rare transitions as outliers or "bad luck" rather than informative signals.
    Thus, they reduce their learning rate on trials where the transition was rare.
    
    Parameters:
    - lr: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick: [0, 5] Stickiness.
    - rare_gating_oci: [0, 2] Factor reducing LR on rare trials based on OCI.
      lr_trial = lr * (1 - rare_gating_oci * oci) if transition was rare.
      (Result clipped to 0).
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
        
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, stick, rare_gating_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Determine Effective Learning Rate ---
        # Common transitions: 0->0, 1->1. Rare transitions: 0->1, 1->0.
        is_rare = (a1 != s2)
        
        lr_eff = lr
        if is_rare:
            # Reduce learning rate if transition was rare, scaled by OCI
            factor = 1.0 - (rare_gating_oci * oci_score)
            if factor < 0: factor = 0
            lr_eff = lr * factor
        
        # --- Updates ---
        # Update Stage 1 MF using Stage 2 value (gated by rare transition status)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_eff * delta_stage1
        
        # Update Stage 2 MF using Reward (gated by rare transition status)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_eff * delta_stage2
        
        last_choice_1 = a1
        
    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```