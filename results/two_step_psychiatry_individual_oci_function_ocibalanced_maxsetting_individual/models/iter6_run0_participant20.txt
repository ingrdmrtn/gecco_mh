Here are the three proposed cognitive models.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Counterfactual Regret Model.
    
    Hypothesis: Participants with higher OCI scores engage in more counterfactual 
    thinking ("If I had chosen the other spaceship, I would have won"). 
    When a reward is not received (reward == 0), the model updates the value of the 
    UN-chosen Stage 1 option as if it had been rewarded. This 'regret' update is 
    scaled by the OCI score.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Mixing weight for Model-Based (1) vs Model-Free (0).
    regret_oci_param: [0,5] - Scales the counterfactual update based on OCI.
                              Higher values mean stronger assumption that the 
                              unchosen option was better after a loss.
    """
    learning_rate, beta, w, regret_oci_param = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Pre-calculate transition matrix for MB
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Softmax Stage 2 (pure MF)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 2 Update (TD-0)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD-0 using Stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Counterfactual Regret Update
        # If outcome was bad (0), boost the unchosen option
        if r == 0:
            other_a1 = 1 - a1
            # Assume the other option would have yielded reward 1.0
            # We use a scaled learning rate for this imaginary update
            regret_lr = learning_rate * regret_oci_param * participant_oci
            # Update unchosen option towards 1.0 (phantom reward)
            q_stage1_mf[other_a1] += regret_lr * (1.0 - q_stage1_mf[other_a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Linked Transition Uncertainty Model.
    
    Hypothesis: High OCI is associated with doubt and uncertainty about the environment's 
    structure. While the true transition probability is 0.7, this model assumes that 
    participants with higher OCI perceive the transitions as noisier (closer to 0.5).
    This degrades the accuracy of the Model-Based component.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight for Model-Based (1) vs Model-Free (0).
    uncertainty_oci_param: [0,1] - Determines how much OCI degrades the perceived 
                                   transition probability. 
                                   p_perceived = 0.7 - (0.2 * param * oci).
    """
    learning_rate, beta, w, uncertainty_oci_param = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate subjective transition probability
    # Base is 0.7. Max degradation (if param=1, oci=1) brings it to 0.5.
    # We clip to ensure it stays >= 0.5
    distortion = 0.2 * uncertainty_oci_param * participant_oci
    p_common = 0.7 - distortion
    if p_common < 0.5: p_common = 0.5
    
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # The MB value calculation now uses the distorted matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Indecision Model.
    
    Hypothesis: OCI manifests as specific difficulty or "doubt" at the high-level 
    planning stage (Stage 1), resulting in more stochastic (exploratory) choices 
    there, while the lower-level response (Stage 2) remains more deterministic.
    This is modeled by scaling the inverse temperature (beta) at Stage 1 inversely 
    with OCI.

    Parameters:
    learning_rate: [0,1] - Update rate.
    beta_s2: [0,10] - Inverse temperature for Stage 2 (baseline precision).
    w: [0,1] - MB/MF weight.
    beta_s1_damp_param: [0,5] - Parameter scaling how much OCI reduces Beta at Stage 1.
                                beta_s1 = beta_s2 / (1 + param * oci).
    """
    learning_rate, beta_s2, w, beta_s1_damp_param = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate Stage 1 Beta
    # Higher OCI -> Lower Beta 1 -> More randomness/doubt at Stage 1
    beta_s1 = beta_s2 / (1.0 + beta_s1_damp_param * participant_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy (Uses Beta 1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_s1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy (Uses Beta 2)
        exp_q2 = np.exp(beta_s2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```