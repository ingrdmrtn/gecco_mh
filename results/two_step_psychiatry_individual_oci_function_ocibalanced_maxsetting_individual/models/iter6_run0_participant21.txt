Here are three new cognitive models inspired by the participant's data and OCI score, exploring different mechanisms like model-based/model-free trade-offs, learning rate asymmetries, and uncertainty modulation.

### Cognitive Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight
This model hypothesizes that individuals with higher OCI scores might rely more on habitual (model-free) control rather than goal-directed (model-based) planning, or vice versa. Here, the weighting parameter `w` (mixing fraction) is a function of the OCI score. A base weight is adjusted by the OCI score to determine the balance between MB and MF systems.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner where the balance between Model-Based and Model-Free control
    is modulated by the OCI score.
    
    The mixing weight 'w' determines the contribution of Model-Based values.
    w = w_base + w_oci_factor * oci
    If w is high, the agent is more Model-Based. If low, more Model-Free.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Base weight for Model-Based system.
    w_oci_factor: [-1, 1] How strongly OCI affects the MB weight.
    """
    learning_rate, beta, w_base, w_oci_factor = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate mixing weight w, constrained to [0, 1]
    w = w_base + w_oci_factor * current_oci
    w = np.clip(w, 0.0, 1.0)
    
    # Transition matrix (fixed structure of the task)
    # 0 -> 0 (70%), 0 -> 1 (30%); 1 -> 0 (30%), 1 -> 1 (70%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-Free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free Q-values for stage 2 (2 states, 2 actions)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based calculation: V_MB(s1) = T * max(Q_MF(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Q-value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Decision ---
        
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Learning ---
        
        # Stage 2 update (TD error)
        # RPE_2 = r - Q_stage2(s, a2)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD(0) / SARSA-like update using stage 2 value)
        # We use the value of the chosen state-action pair in stage 2 as the target
        # RPE_1 = Q_stage2(s, a2) - Q_stage1(a1)
        # Note: Some models use max(Q_stage2) (Q-learning) or Q_stage2(a2) (SARSA).
        # Given the template implies TD, we use the value of the state reached.
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Asymmetric Learning Rates with OCI-Driven Punishment Sensitivity
This model investigates if OCI symptoms relate to an altered sensitivity to negative outcomes (missing rewards). It posits that the participant learns differently from positive prediction errors (gains) versus negative prediction errors (omissions), and that the learning rate for negative errors is specifically scaled by their OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    The learning rate for negative prediction errors (punishment/omission) 
    is scaled by OCI, reflecting potential hypersensitivity to failure.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    lr_neg_oci_scale: [0, 2] Scaling factor for OCI on negative learning rate.
    beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci_scale, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + lr_neg_oci_scale * current_oci
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Stage 2 ---
        delta_2 = r - q_stage2[s_idx, a2]
        
        if delta_2 >= 0:
            q_stage2[s_idx, a2] += lr_pos * delta_2
        else:
            q_stage2[s_idx, a2] += lr_neg * delta_2
            
        # --- Learning Stage 1 ---
        # Using the updated Q-value from stage 2 to drive stage 1 update (TD)
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        if delta_1 >= 0:
            q_stage1[a1] += lr_pos * delta_1
        else:
            q_stage1[a1] += lr_neg * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Inverse Temperature Modulation by OCI (Exploration/Exploitation)
This model posits that OCI symptoms affect the exploration-exploitation trade-off. High OCI might be associated with more rigid, deterministic behavior (higher beta), or conversely, more uncertainty-driven checking (lower beta). Here, the inverse temperature `beta` is a linear function of the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner where the Inverse Temperature (Beta) is modulated by OCI.
    This tests if OCI is associated with more deterministic (high beta) or 
    more random/exploratory (low beta) choice policies.
    
    beta_effective = beta_base + beta_oci_slope * oci
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature.
    beta_oci_slope: [-5, 5] Slope of OCI effect on beta.
    eligibility_trace: [0, 1] Eligibility trace parameter (lambda).
    """
    lr, beta_base, beta_oci_slope, eligibility_trace = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective beta
    beta = beta_base + beta_oci_slope * current_oci
    # Ensure beta stays within reasonable bounds (non-negative)
    beta = np.maximum(beta, 0.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning with Eligibility Traces (TD(lambda)) ---
        # This allows the Stage 2 reward to directly impact Stage 1 Q-values
        
        # Prediction Error Stage 2
        delta_2 = r - q_stage2[s_idx, a2]
        
        # Prediction Error Stage 1
        # We use the value of the state chosen in stage 2 as the proxy for state 1 value
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[s_idx, a2] += lr * delta_2
        
        # Update Stage 1
        # Direct update from stage 1 error
        q_stage1[a1] += lr * delta_1 
        # Additional update from stage 2 error via eligibility trace
        q_stage1[a1] += lr * eligibility_trace * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```