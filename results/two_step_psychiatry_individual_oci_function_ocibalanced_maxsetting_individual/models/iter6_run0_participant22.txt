Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in this two-step task, focusing on learning rates, model-based control, and exploration.

### Model 1: OCI-Modulated Model-Based Weighting (Sigmoid)
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is non-linearly related to OCI symptoms. Specifically, it tests if higher OCI scores push the mixing weight $w$ towards one extreme (e.g., rigid habits or over-planning) using a sigmoidal transformation centered on the symptom scale.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the weighting parameter 'w' is determined by a sigmoid 
    function of the OCI score. This allows for non-linear modulation of the 
    planning vs. habit balance.

    Hypothesis: The degree of model-based control changes non-linearly with OCI severity.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_slope: [-10, 10] (Steepness and direction of OCI effect)
    w_midpoint: [0, 1] (OCI score where w is 0.5)
    """
    learning_rate, beta, w_slope, w_midpoint = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Sigmoid function to determine w based on OCI
    # w represents the weight of Model-Based control (0 = pure MF, 1 = pure MB)
    w = 1 / (1 + np.exp(-w_slope * (current_oci - w_midpoint)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        # Stage 2 Update (TD)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD using Stage 2 value as proxy for reward)
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Second-Stage Learning Rate
This model proposes that Obsessive-Compulsive traits specifically affect how quickly participants update their beliefs about the immediate rewards (aliens) in the second stage. This could reflect a "stickiness" of belief or hyper-plasticity regarding concrete outcomes, distinct from the higher-level planning.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the learning rate for the second stage (aliens) 
    is scaled by the OCI score.
    
    Hypothesis: OCI affects the sensitivity to immediate reward prediction errors 
    (Stage 2), potentially making updating more rigid (lower LR) or more volatile 
    (higher LR) compared to the base rate.

    Bounds:
    lr_base: [0, 1]
    lr_oci_sens: [-1, 1] (Sensitivity of LR to OCI)
    beta: [0, 10]
    w: [0, 1]
    """
    lr_base, lr_oci_sens, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective learning rate for stage 2
    # Bounded between 0 and 1
    lr_stage2 = lr_base + (lr_oci_sens * current_oci)
    lr_stage2 = np.clip(lr_stage2, 0.0, 1.0)
    
    # Stage 1 uses the base rate
    lr_stage1 = lr_base

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        # Stage 2 Update with OCI-modulated rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += lr_stage2 * delta_stage2
        
        # Stage 1 Update with base rate
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += lr_stage1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration Modulated by OCI
This model introduces an "uncertainty bonus" (exploration) that is modulated by OCI. It posits that individuals with higher OCI scores might be more intolerant of uncertainty, leading them to either avoid uncertain options (negative bonus) or compulsively check them (positive bonus).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with an uncertainty bonus added to Stage 1 choices.
    The magnitude of this bonus is scaled by OCI.
    
    Hypothesis: OCI modulates 'uncertainty intolerance'. High OCI might lead to 
    avoiding options that haven't been visited recently (safety seeking) or 
    seeking them out (checking).
    
    Uncertainty is approximated by a counter of trials since the option was last chosen.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    uncert_scale: [-2, 2] (Bonus per trial unvisited, scaled by OCI)
    """
    learning_rate, beta, w, uncert_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track trials since last visit for Stage 1 actions (0 and 1)
    trials_since_visit = np.zeros(2) 

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Uncertainty Bonus: Scale * OCI * (Trials Since Visit)
        # We normalize trials_since_visit to prevent explosion, e.g., divide by 10 or log
        uncertainty_bonus = uncert_scale * current_oci * np.log(trials_since_visit + 1)
        
        exp_q1 = np.exp(beta * (q_net + uncertainty_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]

        # Update visit counters
        trials_since_visit += 1
        trials_since_visit[act1] = 0

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```