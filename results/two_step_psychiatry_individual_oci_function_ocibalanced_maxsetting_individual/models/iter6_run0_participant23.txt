Here are three new cognitive models that explain the participant's behavior, incorporating their OCI score into the decision-making process in novel ways.

### Model 1: OCI-Modulated Choice Trace Decay
This model hypothesizes that OCI symptoms affect the **duration** of habit formation (perseveration), rather than just the strength. High OCI leads to a slower decay of the "choice trace," meaning past choices exert a longer-lasting influence on current decisions, creating deeper "ruts" of behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Choice Trace Decay.
    
    Hypothesis: OCI modulates the time-constant of the stickiness memory trace.
    High OCI participants have a choice trace that decays more slowly, integrating 
    history over a longer window (stronger habit formation).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stick_weight: [0, 5] Magnitude of the stickiness bias.
    decay_slope: [0, 1] Modulates trace decay. Decay = decay_slope * OCI.
    """
    lr, beta, w, stick_weight, decay_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the decay rate based on OCI. 
    # If OCI is 0, decay is 0 (only immediate last choice matters).
    # If OCI is high, decay is high (long history matters).
    trace_decay = decay_slope * oci_score
    # Clip to ensure stability
    if trace_decay > 0.95: trace_decay = 0.95
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    # Choice trace vector
    choice_trace = np.zeros(2)
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue # Skip missing trials
        
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # 1. Calculate Stage 1 Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 2. Add Stickiness based on the decaying trace
        q_net_1 += stick_weight * choice_trace
        
        # 3. Stage 1 Choice Probability
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # 4. Stage 2 Choice Probability
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # 5. Updates
        # Update Stage 2 MF
        q_stage2_mf[s, a2] += lr * (r - q_stage2_mf[s, a2])
        
        # Update Stage 1 MF (TD(0)-like update using Stage 2 value)
        q_stage1_mf[a1] += lr * (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        
        # Update Choice Trace
        # Decay existing trace and increment chosen action
        choice_trace = choice_trace * trace_decay
        choice_trace[a1] += 1.0
        
    return log_loss
```

### Model 2: OCI-Modulated Rare Transition Dampening
This model hypothesizes that high OCI is associated with **cognitive rigidity**. When a "rare" transition occurs (the spaceship goes to the unlikely planet), high OCI participants may treat this as an anomaly or error and suppress learning from it, effectively dampening the learning rate for that trial.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Rare Transition Dampening.
    
    Hypothesis: High OCI participants exhibit rigidity. When a rare transition occurs 
    (Action 0 -> State 1, or Action 1 -> State 0), they dampen the learning update,
    trusting their model of the world over the immediate 'aberrant' observation.
    
    Parameters:
    lr: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stickiness: [0, 5] Standard choice stickiness.
    rare_damp_slope: [0, 1] Dampening factor slope. 
                      Effective LR = LR * (1 - slope * OCI) on rare trials.
    """
    lr, beta, w, stickiness, rare_damp_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: 
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # 1. Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # 2. Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # 3. Determine if transition was rare
        # Common: 0->0, 1->1. Rare: 0->1, 1->0.
        is_rare = (a1 != s)
        
        # 4. Modulate Learning Rate
        current_lr = lr
        if is_rare:
            dampening = rare_damp_slope * oci_score
            if dampening > 1.0: dampening = 1.0
            current_lr = lr * (1.0 - dampening)
            
        # 5. Updates
        q_stage2_mf[s, a2] += current_lr * (r - q_stage2_mf[s, a2])
        q_stage1_mf[a1] += current_lr * (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        
        last_action_1 = a1
        
    return log_loss
```

### Model 3: OCI-Modulated Planet Stickiness
This model proposes that high OCI participants feel a compulsion to return to the **same state** (planet) they visited previously, regardless of which spaceship (action) is best. This "Model-Based Stickiness" uses the transition matrix to boost the action most likely to return to the previous planet.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Planet (State) Stickiness.
    
    Hypothesis: OCI drives a compulsion to repeat outcomes (states), not just actions.
    The agent calculates which spaceship is most likely to lead back to the *previous planet*
    and adds a bonus to that spaceship.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stick_action: [0, 5] Standard action stickiness.
    stick_planet_slope: [0, 5] Weight for planet stickiness = slope * OCI.
    """
    lr, beta, w, stick_action, stick_planet_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    last_planet = -1
    log_loss = 0.0
    
    # Calculate planet stickiness weight based on OCI
    w_planet_stick = stick_planet_slope * oci_score
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            last_action_1 = -1
            last_planet = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # 1. Base Q-values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # 2. Add Action Stickiness (Standard)
        if last_action_1 != -1:
            q_net_1[last_action_1] += stick_action
            
        # 3. Add Planet Stickiness (Novel)
        # If we visited a planet last time, boost the action likely to take us there again.
        if last_planet != -1:
            # For each action, probability of reaching last_planet
            prob_reach_last = transition_matrix[:, last_planet]
            q_net_1 += w_planet_stick * prob_reach_last
            
        # 4. Probabilities and Loss
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # 5. Updates
        q_stage2_mf[s, a2] += lr * (r - q_stage2_mf[s, a2])
        q_stage1_mf[a1] += lr * (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        
        last_action_1 = a1
        last_planet = s
        
    return log_loss
```