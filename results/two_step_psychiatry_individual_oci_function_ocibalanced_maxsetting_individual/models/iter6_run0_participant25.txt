Here are the 3 new cognitive models based on the participant data and OCI scores.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Subjective Transition Belief.
    
    Hypothesis: OCI scores influence the participant's internal model of the transition 
    probabilities (the 'common' transition probability). High OCI might lead to 
    a distorted belief (e.g., perceiving the world as more deterministic or more random), 
    which affects the Model-Based value calculation.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Model-Free value updates.
    - beta: [0, 10] Inverse temperature for choice selection (both stages).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - p_base: [0, 1] Baseline subjective probability of the common transition.
    - p_oci: [-1, 1] Modulation of the subjective probability by OCI score.
      Effective p = clip(p_base + p_oci * oci, 0, 1).
    """
    learning_rate, beta, w, p_base, p_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate subjective transition probability
    p_common = p_base + (p_oci * current_oci)
    p_common = np.clip(p_common, 0.0, 1.0)
    
    # Subjective Transition Matrix
    # T[0,0] = p_common (A -> X), T[1,1] = p_common (U -> Y)
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Choice ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        logits_1 = beta * q_net_1
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        # Pure Model-Free (Bandit)
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF Update (TD-learning)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace: Stage 1 also learns from Stage 2 RPE
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Stage 2 Inverse Temperature.
    
    Hypothesis: OCI affects the exploration-exploitation balance differently at the 
    second stage (immediate reward consumption) compared to the first stage (planning).
    High OCI might lead to more deterministic (high beta) or more erratic (low beta) 
    behavior specifically when facing the aliens.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2_base: [0, 10] Baseline inverse temperature for Stage 2.
    - beta2_oci: [-5, 5] Modulation of Stage 2 beta by OCI score.
    """
    learning_rate, w, beta1, beta2_base, beta2_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    beta2 = beta2_base + (beta2_oci * current_oci)
    beta2 = np.maximum(0.0, beta2) # Ensure non-negative
    
    # Fixed transition matrix for standard MB
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta1 * q_net_1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 ---
        # Uses OCI-modulated beta2
        logits_2 = beta2 * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Rare Transition Learning Rate.
    
    Hypothesis: OCI affects how the participant learns from 'rare' transitions (unexpected state transitions).
    They may ignore them (low learning rate) or over-update from them (high learning rate).
    This specifically modulates the Stage 1 Model-Free update following a rare transition.
    
    Parameters:
    - lr_common: [0, 1] Learning rate after a common transition (and for reward updates).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - lr_rare_base: [0, 1] Baseline learning rate after a rare transition.
    - lr_rare_oci: [-1, 1] Modulation of rare learning rate by OCI.
    """
    lr_common, beta, w, lr_rare_base, lr_rare_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    lr_rare = lr_rare_base + (lr_rare_oci * current_oci)
    lr_rare = np.clip(lr_rare, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Determine if transition was common or rare
        # 0->0 and 1->1 are common. 0->1 and 1->0 are rare.
        is_common = (a1 == s_idx)
        
        current_lr = lr_common if is_common else lr_rare
        
        # Stage 1 MF Update (Transition Prediction Error)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Stage 2 MF Update (Reward Prediction Error)
        # Using lr_common as the standard reward learning rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_common * delta_stage2
        
        # Eligibility trace update for Stage 1 (Reward Prediction Error)
        q_stage1_mf[a1] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```