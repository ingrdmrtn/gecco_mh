Here are three cognitive models implemented as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner with Choice Perseveration (Stickiness).
    The mixing weight 'w' (balance between MB and MF) is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - stickiness: [-5, 5] Choice perseveration bonus for repeating the previous stage 1 choice.
    - w_base: [-10, 10] Base parameter for the mixing weight (before sigmoid).
    - oci_w_mod: [-10, 10] Effect of OCI score on the mixing weight.
    """
    learning_rate, beta, stickiness, w_base, oci_w_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w, constrained to [0, 1] via sigmoid
    # High w -> Model-Based, Low w -> Model-Free
    w_logit = w_base + (oci_score * oci_w_mod)
    w = 1.0 / (1.0 + np.exp(-w_logit))

    # Transition matrix: Row 0=Action A, Row 1=Action U. 
    # Col 0=Planet X, Col 1=Planet Y.
    # A->X (0.7), U->Y (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf_stage1 = np.zeros(2)
    q_mf_stage2 = np.zeros((2, 2)) # State x Action
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: T * max(Q_stage2)
        # Note: In this task, MB uses the MF stage 2 values as estimates of planet value
        max_q_stage2 = np.max(q_mf_stage2, axis=1)
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Add stickiness to the previously chosen action
        if prev_a1 != -1:
            q_net_stage1[prev_a1] += stickiness
            
        # Softmax
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        # SARSA-style update for Stage 1 MF
        # The value of the state reached (Q2[s, a2]) is the target for Q1[a1]
        delta_1 = q_mf_stage2[s_idx, a2] - q_mf_stage1[a1]
        q_mf_stage1[a1] += learning_rate * delta_1
        
        # Prediction error for Stage 2
        delta_2 = r - q_mf_stage2[s_idx, a2]
        q_mf_stage2[s_idx, a2] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Eligibility Traces (Lambda) and Stickiness.
    The eligibility trace parameter 'lambda' is modulated by the OCI score.
    This controls how much the Stage 2 outcome reinforces the Stage 1 choice directly.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stickiness: [-5, 5] Choice perseveration bonus.
    - lambda_base: [-10, 10] Base parameter for lambda (before sigmoid).
    - oci_lambda_mod: [-10, 10] Effect of OCI score on lambda.
    """
    learning_rate, beta, stickiness, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lambda, constrained to [0, 1]
    lam_logit = lambda_base + (oci_score * oci_lambda_mod)
    lam = 1.0 / (1.0 + np.exp(-lam_logit))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_stage1_effective = q_stage1.copy()
        if prev_a1 != -1:
            q_stage1_effective[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        # Stage 2 RPE
        delta_2 = r - q_stage2[s_idx, a2]
        
        # Update Stage 2
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1
        # Standard TD(0) part: driven by Q-value of next state
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1] # Note: using updated Q2 is common in some implementations, but here we use the value *before* Q2 update implicitly if we wanted strict SARSA, but here we follow the logic that Q2 represents the value of the state. 
        # Actually, let's stick to the standard decomposition:
        # Q1 update = alpha * (Q2_old - Q1) + alpha * lambda * (r - Q2_old)
        # But since we just updated Q2, let's use the RPEs directly.
        # Total update to Q1 = alpha * (Q2_pre + lambda * delta_2 - Q1) ?
        # A cleaner implementation of TD(lambda) for 2-step:
        # Q1 += lr * (Q2_pre - Q1) + lr * lambda * delta_2
        
        # Re-calculating delta_1 based on pre-update Q2 would require storing it. 
        # Let's approximate by using the Q2 value *before* the delta_2 update was applied?
        # Q2_pre = q_stage2[s_idx, a2] - learning_rate * delta_2
        # delta_1 = Q2_pre - q_stage1[a1]
        
        # Simplified update logic often used in these tasks:
        # Q1_new = Q1 + lr * (Q2_observed - Q1) + lr * lambda * (Reward - Q2_observed)
        # Here Q2_observed is the value of the state chosen in stage 2.
        
        q2_val = q_stage2[s_idx, a2] - (learning_rate * delta_2) # Recover pre-update value
        
        q_stage1[a1] += learning_rate * (q2_val - q_stage1[a1]) + learning_rate * lam * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Choice Perseveration and OCI-modulated Reward Sensitivity.
    This model tests if OCI affects the subjective utility of the reward (0 vs 1),
    making the participant more or less sensitive to the difference between outcomes.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stickiness: [-5, 5] Choice perseveration bonus.
    - rho_base: [0, 5] Base reward sensitivity multiplier.
    - oci_rho_mod: [-5, 5] Effect of OCI on reward sensitivity.
    """
    learning_rate, beta, stickiness, rho_base, oci_rho_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Reward Sensitivity (rho)
    # We use a softplus or abs to ensure sensitivity is non-negative, or simply linear if bounds handle it.
    # Let's use linear but assume bounds keep it reasonable.
    rho = rho_base + (oci_score * oci_rho_mod)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        q_stage1_effective = q_stage1.copy()
        if prev_a1 != -1:
            q_stage1_effective[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        # Modulate reward by sensitivity rho
        effective_r = r * rho
        
        # Update Stage 2
        delta_2 = effective_r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1 (Standard SARSA-style MF)
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```