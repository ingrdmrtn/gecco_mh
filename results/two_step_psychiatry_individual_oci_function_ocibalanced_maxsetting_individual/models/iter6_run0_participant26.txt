Here are three cognitive models implemented as Python functions, designed to explain the participant's behavior in the two-step task using their OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weighting (Habit Hypothesis).
    
    Hypothesis: High OCI scores correlate with reduced goal-directed (Model-Based) control,
    shifting behavior towards Model-Free (habitual) control. This models the "compulsive habit"
    theory of OCD.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax choice policy.
    - lambda_eligibility: [0, 1] Eligibility trace decay for Stage 1 MF update.
    - w_max: [0, 1] The maximum Model-Based weight (theoretical w at OCI=0).
    """
    learning_rate, beta, lambda_eligibility, w_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate w based on OCI
    # As OCI increases, w decreases, representing a shift to habitual control.
    w = w_max * (1.0 - oci_score)
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation (Bellman equation using transition matrix)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value (weighted mix of MB and MF)
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: continue # Skip missing data
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Transition ---
        s_idx = int(state[trial])
        if s_idx == -1: continue
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (TD Error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(lambda))
        # Updates Stage 1 Q-value based on Stage 2 value and the reward (via eligibility trace)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_eligibility * delta_stage2

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Negative Learning Rate (Asymmetric Learning).
    
    Hypothesis: High OCI individuals are hypersensitive to negative prediction errors 
    (omission of reward), learning faster from failures than low OCI individuals.
    This reflects anxiety-driven avoidance learning.
    
    Parameters:
    - learning_rate_pos: [0, 1] Learning rate for positive outcomes (Reward=1).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight (fixed parameter).
    - neg_bias_scale: [0, 5] Scaling factor for negative learning rate based on OCI.
      lr_neg = lr_pos * (1 + neg_bias_scale * oci).
    """
    learning_rate_pos, beta, w, neg_bias_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate (for Reward=0)
    # If OCI is high, lr_neg is boosted relative to lr_pos
    learning_rate_neg = learning_rate_pos * (1.0 + neg_bias_scale * oci_score)
    learning_rate_neg = np.clip(learning_rate_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: continue
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        if s_idx == -1: continue
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Select Learning Rate based on Reward (proxy for PE sign in this task)
        current_lr = learning_rate_pos if r == 1.0 else learning_rate_neg
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        # Stage 1 Update (Direct MF update from Stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace (Rumination/Stickiness).
    
    Hypothesis: High OCI is associated with 'mental stickiness' or rumination.
    In RL terms, this is modeled as a higher eligibility trace (lambda), meaning the 
    outcome of the second stage is more strongly credited to the first stage choice,
    bridging the gap between the two steps more effectively (or rigidly).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_base: [0, 1] Base eligibility trace.
    - lambda_oci_scale: [0, 1] How much OCI increases the trace.
      lambda = lambda_base + lambda_oci_scale * oci.
    """
    learning_rate, beta, w, lambda_base, lambda_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate lambda based on OCI
    lambda_val = lambda_base + lambda_oci_scale * oci_score
    lambda_val = np.clip(lambda_val, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: continue
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        if s_idx == -1: continue
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update with OCI-modulated eligibility trace
        # Q1_new = Q1 + lr * (Q2 - Q1) + lr * lambda * (R - Q2)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        
    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```