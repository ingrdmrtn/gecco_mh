Here are three new cognitive models for the two-step decision task, incorporating the OCI-R score into different mechanisms: Subjective Transition Belief, Uncertainty Exploration, and Memory Decay.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Transition Belief Distortion.
    
    Hypothesis:
    Participants with higher OCI scores may exhibit intolerance of uncertainty, 
    leading them to perceive the environment's transition structure as less reliable 
    than it actually is. This model posits that OCI "flattens" the transition matrix 
    used for Model-Based planning, bringing the perceived probability of common 
    transitions closer to chance (0.5).
    
    Parameters:
    - lr: [0, 1] Learning rate for MF values.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between MB and MF (1 = pure MB).
    - distortion_k: [0, 1] Scaling factor for OCI-induced transition distortion.
    """
    lr, beta, w, distortion_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Standard common transition probability is 0.7.
    # We model a distortion where high OCI reduces this belief towards 0.5 (randomness).
    # distortion_k * oci_score determines the magnitude of this reduction.
    # We use a scaling factor of 0.2 because 0.7 - 0.2 = 0.5.
    p_common_belief = 0.7 - (0.2 * distortion_k * oci_score)
    p_common_belief = np.clip(p_common_belief, 0.5, 0.7)
    
    # Subjective transition matrix used for MB planning
    transition_matrix = np.array([[p_common_belief, 1 - p_common_belief], 
                                  [1 - p_common_belief, p_common_belief]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # MB Value Calculation using distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # MF Stage 1 Update (SARSA-like using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # MF Stage 2 Update (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Uncertainty Bonus (Dynamic Exploration).
    
    Hypothesis:
    OCI scores may correlate with "checking" behaviors or a drive to reduce uncertainty.
    This model adds an exploration bonus to Stage 1 options based on how long it has 
    been since they were last chosen. The magnitude of this bonus is scaled by the OCI score.
    Unlike simple stickiness (which is constant), this bonus grows over time.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - bonus_k: [0, 1] Scaling factor for the uncertainty bonus.
    """
    lr, beta, w, bonus_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the trial number when each Stage 1 action was last chosen.
    # Initialize to -1 to represent "never chosen".
    last_chosen_s1 = np.array([-1, -1])
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            # Time advances even if no choice is recorded, but we can't update Q-values.
            # We assume 'missing' implies no update to last_chosen.
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Calculate Uncertainty Bonus ---
        # Bonus is proportional to log(time since last chosen).
        time_since = np.zeros(2)
        for a in range(2):
            if last_chosen_s1[a] == -1:
                # If never chosen, treat as high uncertainty (time = current trial + 1)
                time_since[a] = trial + 1
            else:
                time_since[a] = trial - last_chosen_s1[a]
        
        # Bonus added to Q-values
        exploration_bonus = bonus_k * oci_score * np.log(time_since + 1)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + exploration_bonus
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Update recency tracker
        last_chosen_s1[a1] = trial

    return -log_likelihood

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Memory Decay.
    
    Hypothesis:
    This model hypothesizes that OCI symptoms relate to the persistence of value representations.
    A high OCI score might lead to either obsessive retention (low decay) or instability/doubt 
    (high decay) of learned values for unchosen options. Here, we model decay of unchosen 
    options as a function of OCI.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - decay_k: [0, 1] Scaling factor for decay rate.
    """
    lr, beta, w, decay_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Decay rate depends on OCI
    decay_rate = decay_k * oci_score
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Update Chosen Stage 1
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Decay Unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # Update Chosen Stage 2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Decay Unchosen Stage 2 (for the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s2, unchosen_a2] *= (1.0 - decay_rate)

    return -log_likelihood
```