Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Side Bias
This model hypothesizes that the participant's strong preference for one spaceship (as seen in the data, heavily favoring spaceship 0.0) is driven by an intrinsic "side bias" that is exacerbated by obsessive-compulsive traits. High OCI scores act to amplify a fixed bias towards a specific "safe" or default option, independent of reward history.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Side Bias.
    
    Hypothesis: The participant has a baseline preference (bias) for spaceship 0 (Action 0)
    over spaceship 1. The magnitude of this bias is modulated by their OCI score. 
    Higher OCI scores lead to a stronger rigid preference for the biased side, 
    explaining the overall skew in choice probability despite reward fluctuations.
    
    Parameters:
    lr: [0,1] - Learning rate for value updates.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    lam: [0,1] - Eligibility trace parameter.
    stickiness: [0,5] - Basic choice perseveration (repeat last choice).
    bias_base: [0,2] - Magnitude of side bias towards option 0, scaled by OCI.
    """
    lr, beta, w, lam, stickiness, bias_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate bias: Higher OCI -> Stronger bias towards option 0
    # We assume the bias favors action 0 based on participant data.
    current_bias = bias_base * (1.0 + oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)      # MF values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # MF values for aliens
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Apply OCI-modulated Side Bias (Favoring 0)
        q_net_1[0] += current_bias
        q_net_1[1] -= current_bias

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 TD error
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 TD error
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[a1] += lr * lam * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Exponential Stickiness
This model addresses the "blocky" nature of the participant's choices (long streaks of choosing the same spaceship). Instead of simple one-step stickiness, it uses an accumulating "choice trace" (habit strength). The decay rate of this trace is modulated by OCI, hypothesizing that higher OCI leads to slower decay of habits, resulting in stronger, longer-lasting perseveration.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Exponential Stickiness Decay.
    
    Hypothesis: The participant builds up a 'choice trace' (habit strength) over multiple trials. 
    OCI modulates the decay rate of this trace: higher OCI leads to slower decay (longer memory), 
    resulting in deeper habit formation and longer blocks of perseveration.
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    lam: [0,1] - Eligibility trace.
    stick_weight: [0,5] - Weight of the exponential habit trace.
    decay_base: [0,1] - Base decay rate for the choice trace. OCI increases this towards 1.
    """
    lr, beta, w, lam, stick_weight, decay_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay: High OCI -> Decay closer to 1 (slower forgetting of habit)
    decay = decay_base + (1.0 - decay_base) * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for exponential stickiness
    choice_trace = np.zeros(2) 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Exponential Stickiness Bonus
        q_net_1 += stick_weight * choice_trace

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update Choice Trace
        choice_trace *= decay
        choice_trace[a1] += 1.0
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Forgetting
This model suggests that OCI relates to how value information is maintained. Specifically, it implements a forgetting mechanism where the value of the *unchosen* spaceship decays towards zero. The rate of this forgetting is modulated by OCI. If OCI leads to faster forgetting of the alternative option, the participant may stick with their current choice simply because the alternative's value has degraded.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Forgetting.
    
    Hypothesis: OCI is associated with altered memory maintenance. This model implements 
    passive forgetting of unchosen option values. OCI modulates the forgetting rate, 
    potentially causing the participant to stick with the current option because 
    the alternative's value decays to neutral/zero.
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    lam: [0,1] - Eligibility trace.
    stickiness: [0,5] - Basic perseveration.
    forget_base: [0,1] - Base forgetting rate for unchosen options. Scaled by OCI.
    """
    lr, beta, w, lam, stickiness, forget_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate forgetting rate: modulated by OCI
    # We constrain forget to be at most 1.0
    forget = forget_base * (1.0 + oci_score)
    if forget > 1.0: forget = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * lam * delta_stage2
        
        # --- Forgetting Mechanism ---
        # Decay the value of the unchosen spaceship towards 0
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - forget)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```