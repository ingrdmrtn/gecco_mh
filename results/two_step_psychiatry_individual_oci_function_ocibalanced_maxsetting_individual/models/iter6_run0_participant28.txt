Here are 3 new cognitive models as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Probability Model.
    
    The participant's belief about the transition matrix (used in the Model-Based 
    component) is modulated by their OCI score. High OCI may correlate with 
    perceiving the environment as more deterministic or more random than reality,
    distorting the Model-Based value calculation.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values
    beta: [0,10] - Inverse temperature
    w: [0,1] - Weight of Model-Based system (0=MF, 1=MB)
    p_stick: [0,1] - Stage 1 Stickiness (scaled by 5)
    lambda_val: [0,1] - Eligibility trace
    trans_prob_base: [0,1] - Base subjective transition probability
    trans_prob_oci_mod: [0,1] - Modulation of transition probability by OCI
    """
    learning_rate, beta, w, p_stick, lambda_val, tp_base, tp_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # tp is modulated by OCI around the base value
    tp = tp_base + (tp_oci_mod - 0.5) * oci_score
    tp = np.clip(tp, 0.0, 1.0)
    
    # Subjective transition matrix
    # Assumes symmetric structure: A->X is tp, A->Y is 1-tp, etc.
    transition_matrix = np.array([[tp, 1-tp], [1-tp, tp]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stick * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 2 Stickiness Model.
    
    Introduces a stickiness parameter for the second stage choice (aliens),
    which is modulated by OCI. This hypothesis suggests that compulsivity 
    may manifest as motor perseveration in the second stage of the task, 
    independent of reward history.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    p_stage1: [0,1] - Stage 1 stickiness (scaled by 5)
    lambda_val: [0,1]
    p_stage2_base: [0,1] - Base Stage 2 stickiness (scaled by 5)
    p_stage2_oci_mod: [0,1] - Modulation of Stage 2 stickiness by OCI
    """
    learning_rate, beta, w, p_stage1, lambda_val, p_s2_base, p_s2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_s2 = p_s2_base + (p_s2_oci - 0.5) * oci_score
    p_s2 = np.clip(p_s2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2 = -1 
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stage1 * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2
        logits_2 = beta * q_stage2_mf[s_idx]
        
        # Apply Stage 2 stickiness (motor stickiness on action index 0 or 1)
        if last_action_2 != -1:
            logits_2[last_action_2] += p_s2 * 5.0
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_action_2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Rare Transition Learning Rate Model.
    
    The learning rate for the Stage 1 Model-Free values is modulated by whether 
    the transition was Common or Rare. OCI specifically modulates the learning 
    rate for Rare transitions, reflecting a potential difference in how OCI 
    individuals react to prediction errors driven by transition structure violations.
    
    Parameters:
    lr_common: [0,1] - Learning rate for common transitions (and Stage 2)
    beta: [0,10]
    w: [0,1]
    p_stick: [0,1]
    lambda_val: [0,1]
    lr_rare_base: [0,1] - Base learning rate for rare transitions
    lr_rare_oci_mod: [0,1] - Modulation of rare LR by OCI
    """
    lr_common, beta, w, p_stick, lambda_val, lr_rare_base, lr_rare_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    lr_rare = lr_rare_base + (lr_rare_oci - 0.5) * oci_score
    lr_rare = np.clip(lr_rare, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stick * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Determine if transition was common or rare
        # Common: 0->0 or 1->1
        is_common = (a1 == s_idx)
        current_lr = lr_common if is_common else lr_rare
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Use current_lr for Stage 1 update
        q_stage1_mf[a1] += current_lr * delta_stage1 + current_lr * lambda_val * delta_stage2
        
        # Use lr_common for Stage 2 (assuming standard updating for the bandit part)
        q_stage2_mf[s_idx, a2] += lr_common * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```