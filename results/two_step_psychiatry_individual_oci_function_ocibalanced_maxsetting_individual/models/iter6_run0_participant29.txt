Here are three cognitive models that incorporate the OCI score into the decision-making process, based on the provided participant data and feedback.

### Model 1: OCI-Modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace ($\lambda$) to the hybrid RL framework. The parameter $\lambda$ controls how much the outcome of the second stage directly updates the value of the first stage choice (Model-Free credit assignment). We hypothesize that OCI modulates this credit assignment mechanismâ€”potentially leading to stronger "habitual" chaining of choices (high $\lambda$) or disrupted credit assignment (low $\lambda$).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Eligibility Trace Model.
    
    Hypothesis: OCI scores modulate the eligibility trace (lambda), affecting the 
    direct updating of Stage 1 values by Stage 2 reward prediction errors. 
    This captures potential differences in credit assignment or superstitious 
    learning associated with OCI symptoms.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weighting parameter (0=Model-Free, 1=Model-Based).
    stickiness: [0, 5] - Tendency to repeat the previous Stage 1 choice.
    lambda_base: [0, 1] - Base eligibility trace decay parameter.
    lambda_oci: [-1, 1] - Modulation of lambda by OCI score.
    """
    learning_rate, beta, w, stickiness, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated eligibility trace, clipped to valid range [0, 1]
    lambda_val = lambda_base + lambda_oci * oci_score
    lambda_val = np.clip(lambda_val, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Shape: (State, Action)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based valuation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted sum of MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness to Stage 1
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 Prediction Error (driven by State 2 value)
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        
        # Stage 2 Prediction Error (driven by Reward)
        delta_stage2 = r - q_stage2_mf[s, a2]
        
        # Update Stage 2 MF Value
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Value with Eligibility Trace
        # Combines immediate transition error and discounted reward error
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Loss Sensitivity
This model hypothesizes that OCI affects the subjective valuation of outcomes, specifically the "no reward" outcome. Instead of treating 0 coins as neutral, the model treats it as a loss (negative utility), the magnitude of which is modulated by OCI. This can explain persistence (if they perceive 0 as less negative than switching to an unknown) or avoidance behavior.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Loss Sensitivity Model.
    
    Hypothesis: OCI affects the subjective utility of the 'no reward' outcome (0 coins).
    High OCI might lead to an altered perception of loss (e.g., treating 0 as a 
    significant punishment or conversely, being desensitized to it). This effectively
    shifts the baseline against which rewards are compared.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stickiness: [0, 5]
    loss_base: [0, 5] - Base magnitude of perceived loss when reward is 0.
    loss_oci: [-5, 5] - Modulation of loss magnitude by OCI.
    """
    learning_rate, beta, w, stickiness, loss_base, loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective loss magnitude
    loss_mag = loss_base + loss_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate Effective Reward
        # If r=1, utility is 1. If r=0, utility is negative (loss).
        if r > 0.5:
            eff_reward = 1.0
        else:
            eff_reward = -loss_mag
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Using effective reward for value updates
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = eff_reward - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Dual-Stage Stickiness
This model posits that the rigidity associated with OCI manifests as stickiness not just in the spaceship choice (Stage 1) but also in the alien choice (Stage 2). The data shows the participant often repeats the same alien choice on a planet. This model adds a specific stickiness parameter for Stage 2, modulated by OCI.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Dual-Stage Stickiness Model.
    
    Hypothesis: OCI-related rigidity leads to perseveration at both levels of the task.
    This model includes a specific stickiness parameter for Stage 2 (Alien choice),
    modulated by OCI, allowing for different degrees of routine formation on planets.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stick_1: [0, 5] - Stickiness for Stage 1 (Spaceship).
    stick_2_base: [0, 5] - Base stickiness for Stage 2 (Alien).
    stick_2_oci: [-5, 5] - Modulation of Stage 2 stickiness by OCI.
    """
    learning_rate, beta, w, stick_1, stick_2_base, stick_2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 stickiness
    stick_2 = stick_2_base + stick_2_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    # Track previous choice per state (0 or 1) to apply stickiness correctly
    prev_a2 = [-1, -1] 
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stick_1
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        q_s2 = q_stage2_mf[s].copy()
        
        # Apply Stage 2 Stickiness if a previous choice exists for this state
        if prev_a2[s] != -1:
            q_s2[prev_a2[s]] += stick_2
            
        exp_q2 = np.exp(beta * q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```