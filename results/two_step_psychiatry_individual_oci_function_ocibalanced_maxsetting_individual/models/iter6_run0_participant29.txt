Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Subjective Transition Belief.
    
    This model assumes that the participant's internal model of the spaceship-planet 
    transition probabilities is subjective and modulated by their OCI score.
    While the true common transition probability is 0.7, participants with different
    OCI levels might perceive the world as more deterministic (higher p) or more 
    chaotic (lower p), affecting their Model-Based value calculation.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - trans_p_base: [0, 1] Baseline subjective probability of common transition.
    - trans_p_oci: [-1, 1] Modulation of transition belief by OCI.
    """
    learning_rate, beta, w, trans_p_base, trans_p_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability
    # We allow the probability to deviate from 0.7 based on OCI
    subjective_p = trans_p_base + (trans_p_oci * oci_score)
    # Clamp to ensure valid probability [0, 1]
    subjective_p = min(1.0, max(0.0, subjective_p))
    
    # Construct subjective transition matrix
    # Row 0: Space A to [X, Y]; Row 1: Space B to [X, Y]
    # Assuming symmetry in belief
    transition_matrix = np.array([[subjective_p, 1 - subjective_p], 
                                  [1 - subjective_p, subjective_p]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of next stage based on subjective transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Update (Standard Q-learning)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD(0) Model-Free)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Stage-Specific Learning Rates.
    
    This model posits that OCI affects the learning rate for the first-stage (abstract)
    choices differently than the second-stage (concrete) choices. A ratio parameter 
    modulated by OCI determines the relationship between the two rates. This captures
    if OCI relates to faster/slower updating of high-level plans vs immediate outcomes.
    
    Parameters:
    - lr_stage2: [0, 1] Base learning rate for the second stage (aliens).
    - lr_ratio_base: [0, 5] Base ratio of Stage 1 LR to Stage 2 LR.
    - lr_ratio_oci: [-5, 5] Modulation of the ratio by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    """
    lr_stage2, lr_ratio_base, lr_ratio_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Stage 1 learning rate
    ratio = lr_ratio_base + (lr_ratio_oci * oci_score)
    # Clamp ratio to be non-negative
    ratio = max(0.0, ratio)
    # Ensure learning rate stays in bounds
    lr_stage1 = min(1.0, lr_stage2 * ratio)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Update using lr_stage2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_stage2 * delta_stage2
        
        # Stage 1 Update using lr_stage1
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Learning from Rare Transitions.
    
    This model suggests that OCI affects how much participants learn from 
    'Rare' transitions (unexpected state outcomes). High OCI might lead to 
    ignoring rare events (rigidity/discounting outliers) or over-weighting them.
    The learning rate for the MF update in Stage 1 is scaled when a rare transition occurs.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - rare_mod_base: [0, 5] Base multiplier for learning from rare transitions.
    - rare_mod_oci: [-5, 5] Modulation of the rare multiplier by OCI.
    """
    learning_rate, beta, w, rare_mod_base, rare_mod_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate rare transition modifier
    rare_mod = max(0.0, rare_mod_base + (rare_mod_oci * oci_score))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Update (always concrete outcome)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (Dependent on transition type)
        # Determine if transition was common or rare
        # Common: A(0)->X(0), U(1)->Y(1). Rare: A(0)->Y(1), U(1)->X(0).
        is_common = (a1 == s2)
        
        current_lr = learning_rate
        if not is_common:
            current_lr *= rare_mod
            
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```