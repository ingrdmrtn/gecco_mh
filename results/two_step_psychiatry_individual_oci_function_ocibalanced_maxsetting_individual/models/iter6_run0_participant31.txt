Here are the 3 proposed cognitive models.

### Cognitive Model 1: OCI-Modulated Value Decay (Forgetting)
This model hypothesizes that OCI symptoms modulate the rate at which participants "forget" or devalue unchosen options. High OCI scores may be associated with either obsessive retention of old values (low decay) or anxious uncertainty leading to rapid forgetting (high decay). The model introduces a decay parameter for the unchosen action's Q-value in both stages.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Value Decay Model.
    
    Unchosen Q-values decay towards zero on each trial. The rate of decay
    is modulated by the participant's OCI score.
    
    Parameters:
    - learning_rate: [0,1] Standard learning rate for chosen options.
    - beta: [0,10] Inverse temperature for softmax.
    - w: [0,1] Weighting between Model-Based and Model-Free values (1=Pure MB).
    - decay_base: [0,1] Baseline decay rate for unchosen options.
    - decay_oci: [0,1] Modulation of decay rate by OCI (mapped to slope [-1, 1]).
    """
    learning_rate, beta, w, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map decay_oci to a slope between -1 and 1
    # Effective decay is clipped between 0 and 1
    decay_slope = (decay_oci * 2.0) - 1.0
    decay_rate = np.clip(decay_base + decay_slope * oci_score, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        q_vals_2 = q_stage2_mf[s_idx]
        
        # Softmax Stage 2
        exp_q2 = np.exp(beta * (q_vals_2 - np.max(q_vals_2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Update Stage 1 Chosen
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay Stage 1 Unchosen
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        # Update Stage 2 Chosen
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay Stage 2 Unchosen (for the current state only)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Stage 2 Stickiness
This model proposes that high OCI scores lead to repetitive behaviors specifically in the second stage (the "alien" bandit task), independent of the spaceship choice. This captures potential low-level motor or object-based perseveration.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Stickiness Model.
    
    Adds a 'stickiness' bonus to the previously chosen alien (Stage 2 action)
    within the specific planet context. The magnitude is modulated by OCI.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF weight.
    - stick2_base: [0,1] Baseline stickiness for Stage 2 (mapped to [0, 5]).
    - stick2_oci: [0,1] Modulation of stickiness by OCI (mapped to [-5, 5]).
    """
    learning_rate, beta, w, stick2_base, stick2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stickiness magnitude
    # Base is 0 to 5
    # OCI effect is -5 to 5
    stick_mag = (stick2_base * 5.0) + ((stick2_oci * 2.0 - 1.0) * 5.0 * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last action taken in each state (planet)
    # Initialize with -1 (no previous action)
    last_a2_per_state = [-1, -1]

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        q_vals_2 = q_stage2_mf[s_idx]
        
        # Calculate logits for Stage 2 with stickiness
        logits_2 = beta * q_vals_2
        
        # Apply stickiness if there is a history for this state
        prev_a2 = last_a2_per_state[s_idx]
        if prev_a2 != -1:
            logits_2[prev_a2] += stick_mag
            
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Update history
        last_a2_per_state[s_idx] = a2
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Post-Error Beta Adaptation
This model hypothesizes that OCI influences how participants react to negative outcomes (0 coins). Specifically, it modulates the inverse temperature (beta) on the trial immediately following a loss. High OCI participants might become more rigid (higher beta) or more anxious/exploratory (lower beta) after failing to get a reward.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Post-Error Beta Adaptation Model.
    
    The inverse temperature (beta) changes based on the previous trial's outcome.
    If the previous outcome was 0 (loss), beta is adjusted. The magnitude and
    direction of this adjustment is modulated by OCI.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta_base: [0,10] Baseline inverse temperature (used after wins).
    - w: [0,1] MB/MF weight.
    - beta_loss_shift_base: [0,1] Base shift in beta after loss (mapped to [-5, 5]).
    - beta_loss_shift_oci: [0,1] OCI modulation of the shift (mapped to [-5, 5]).
    """
    learning_rate, beta_base, w, beta_loss_shift_base, beta_loss_shift_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the shift in beta after a loss
    # Base shift: map [0,1] -> [-5, 5]
    base_shift = (beta_loss_shift_base * 10.0) - 5.0
    # OCI modulation: map [0,1] -> [-5, 5] scaled by OCI score
    oci_mod = ((beta_loss_shift_oci * 10.0) - 5.0) * oci_score
    
    total_loss_shift = base_shift + oci_mod
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_reward = 1.0 # Assume win for trial 0 context (neutral start)

    for trial in range(n_trials):
        # Determine current beta based on previous reward
        current_beta = beta_base
        if last_reward == 0.0:
            current_beta += total_loss_shift
        
        # Ensure beta stays non-negative and reasonable
        current_beta = np.clip(current_beta, 0.0, 20.0)
        
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * (q_net_1 - np.max(q_net_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        q_vals_2 = q_stage2_mf[s_idx]
        
        exp_q2 = np.exp(current_beta * (q_vals_2 - np.max(q_vals_2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        last_reward = r
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```