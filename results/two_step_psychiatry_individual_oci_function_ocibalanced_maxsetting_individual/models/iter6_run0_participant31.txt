Here are three new cognitive models for the two-step decision task, incorporating OCI scores into the decision-making process.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Stage 2 Precision.
    
    This model hypothesizes that high OCI scores lead to more rigid, deterministic 
    behavior specifically in the second stage of the task (the "habitual" 
    response to aliens), while the first stage (planning) remains distinct.
    
    The inverse temperature (beta) for the second stage is modulated by OCI.
    High OCI increases beta_2, making choices at the planet level more exploitative.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta_2_base: [0, 10] - Baseline inverse temperature for Stage 2.
    beta_2_oci: [0, 10] - Sensitivity of Stage 2 beta to OCI.
    w: [0, 1] - Weighting between Model-Based and Model-Free values at Stage 1.
    """
    learning_rate, beta_1, beta_2_base, beta_2_oci, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated Stage 2 Beta
    # We constrain the effective beta to be non-negative
    beta_2 = beta_2_base + beta_2_oci * current_oci

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 for stage 1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        # Use OCI-modulated beta_2 for stage 2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Eligibility Trace (Lambda).
    
    This model investigates credit assignment. It hypothesizes that OCI affects 
    how much the final reward outcome is "traced back" to update the value of the 
    initial spaceship choice. 
    
    A parameter `lambda` controls this eligibility trace. 
    High OCI might lead to stronger associations between final outcomes and initial 
    choices (higher lambda), potentially due to hyper-vigilance or obsessiveness.
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature for both stages.
    w: [0, 1] - MB/MF balance.
    lambda_base: [0, 1] - Baseline eligibility trace.
    lambda_oci: [0, 1] - Modulation of lambda by OCI.
    """
    learning_rate, beta, w, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective lambda, clipped to [0, 1]
    lambda_eff = lambda_base + lambda_oci * current_oci
    if lambda_eff > 1.0: lambda_eff = 1.0
    if lambda_eff < 0.0: lambda_eff = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Prediction Error (driven by Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Stage 2 Prediction Error (driven by Reward)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1: Standard TD(0) update + Eligibility Trace term
        # The trace term allows the stage 2 RPE (delta_stage2) to update Stage 1 values directly
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eff * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated MB/MF Weight with Constant Stickiness.
    
    This model posits that OCI specifically degrades the Model-Based weight (w),
    shifting the participant towards Model-Free control, but also includes a 
    constant "stickiness" (perseveration) parameter that is independent of OCI.
    
    This separates the pathological effect (loss of MB control) from general 
    response repetition bias.
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    stick: [0, 5] - Choice stickiness bonus (added to Q-value of previous choice).
    w_base: [0, 1] - Baseline MB weight.
    w_oci_slope: [0, 1] - Sensitivity of w to OCI (reduces w).
    """
    learning_rate, beta, stick, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective w: w decreases as OCI increases
    w = w_base * (1.0 - w_oci_slope * current_oci)
    if w < 0: w = 0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_choice = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate net Q-values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_choice != -1:
            q_net[last_choice] += stick
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        last_choice = a1
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```