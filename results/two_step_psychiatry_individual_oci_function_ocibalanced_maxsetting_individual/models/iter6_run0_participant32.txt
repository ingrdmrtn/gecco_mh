Here are three new cognitive models that explore different mechanisms for how high OCI scores might influence decision-making in this task, specifically focusing on perseveration, model-based/model-free arbitration, and reward sensitivity.

### Cognitive Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that high OCI scores lead to "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of the outcome. This is a common finding in compulsivity research. The model adds a stickiness bonus to the Q-values of the previously chosen action, and the magnitude of this bonus is scaled by the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated choice stickiness (perseveration).
    High OCI is hypothesized to increase the tendency to repeat the previous Stage 1 choice.

    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1] (Weighting parameter: 0 = pure Model-Free, 1 = pure Model-Based)
    stickiness_base: [-5, 5] (Base tendency to repeat choice)
    stickiness_oci_scale: [-5, 5] (How strongly OCI amplifies stickiness)
    """
    learning_rate, beta, w, stickiness_base, stickiness_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate total stickiness based on OCI
    stickiness = stickiness_base + (stickiness_oci_scale * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (2 states, 2 actions)
    
    # Track previous choice for stickiness
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        # Max Q-value at stage 2 for each state
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Expected value based on transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted integrated Q-value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_net_stick = q_net.copy()
        if prev_a1 != -1:
            q_net_stick[prev_a1] += stickiness
        
        # Softmax for Stage 1
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        # Standard Model-Free Q-learning for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        # Stage 2 TD Error and Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 TD Error and Update (Model-Free)
        # Using the value of the state reached (s_idx) as the target
        # A common variation uses max(Q(s_idx, :)) or just Q(s_idx, a2)
        # Here we use Q(s_idx, a2) which is SARSA-like for the second step
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Reward Sensitivity
This model tests the hypothesis that high OCI individuals might have altered sensitivity to rewards. Instead of modulating the learning rate or the balance between systems, the OCI score directly scales the effective reward value `r`. This could represent either a heightened sensitivity (hyper-valuation) or a blunted response (anhedonia-like) to the gold coins, which affects how much Q-values are updated.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Standard Hybrid Model with OCI-modulated Reward Sensitivity.
    The effective reward experienced is scaled by OCI.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    reward_sens_base: [0, 5] (Base scaling factor for reward)
    reward_sens_oci_slope: [-5, 5] (How OCI changes reward sensitivity)
    """
    learning_rate, beta, w, reward_sens_base, reward_sens_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective reward sensitivity
    # We clip to ensure it doesn't become negative which would flip avoidance/seeking
    rho = np.maximum(0, reward_sens_base + (reward_sens_oci_slope * oci_score))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Apply OCI-modulated sensitivity
        effective_reward = r * rho
        
        # --- Updates ---
        # Update Stage 2 Q-values using effective reward
        delta_stage2 = effective_reward - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF Q-values
        # Note: The 'value' of the second stage state is now implicitly scaled by rho
        # because q_stage2_mf tracks the scaled rewards.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Dependent Eligibility Trace (Lambda)
This model proposes that OCI affects the "eligibility trace" parameter ($\lambda$) in a TD($\lambda$) framework. In the two-step task, $\lambda$ effectively controls the direct reinforcement of the first-stage choice by the final reward. If $\lambda=0$, the first stage is only updated via the second stage value (standard TD). If $\lambda=1$, the first stage is updated directly by the reward (Monte Carlo-like). High OCI might correlate with relying more on the final outcome to update the initial choice (high $\lambda$), bypassing the model structure of the middle state.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with Eligibility Traces (TD(lambda)), where lambda is modulated by OCI.
    Lambda controls how much the Stage 2 Reward directly updates Stage 1 Q-values.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    lambda_base: [0, 1]
    lambda_oci_slope: [-1, 1]
    """
    learning_rate, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated lambda
    lam = lambda_base + (lambda_oci_slope * oci_score)
    lam = np.clip(lam, 0.0, 1.0) # Ensure lambda stays valid
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates with Eligibility Trace ---
        
        # 1. Prediction error at Stage 2 (Reward - Q_stage2)
        delta2 = r - q_stage2[s_idx, a2]
        
        # 2. Prediction error at Stage 1 (Q_stage2 - Q_stage1)
        # Note: We use the value of the CHOSEN second stage action as the target (SARSA style)
        delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1
        # In TD(lambda) for this simple chain:
        # Q1 is updated by its own error (delta1) AND a portion (lambda) of the subsequent error (delta2)
        # If lambda=1, Q1 += alpha * (delta1 + delta2) = alpha * (r - Q1) -> Direct update from reward
        # If lambda=0, Q1 += alpha * delta1 -> Only update from next state value
        q_stage1[a1] += learning_rate * (delta1 + (lam * delta2))

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```