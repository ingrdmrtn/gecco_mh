Here are the three proposed cognitive models.

### Model 1: OCI-Distorted Transition Beliefs
This model hypothesizes that high OCI scores relate to an "Intolerance of Uncertainty" or a rigid belief structure. Specifically, it proposes that participants with high OCI symptoms perceive the environment as more deterministic than it actually is, effectively inflating the probability of "common" transitions in their Model-Based calculations and ignoring "rare" paths.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Distorted Transition Beliefs (Rigidity Bias).
    
    Hypothesis: Participants with high OCI scores may perceive the environment as more deterministic 
    than it is, effectively ignoring the possibility of rare transitions in their Model-Based evaluation.
    This reflects an 'Intolerance of Uncertainty' or rigid belief structure.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight of Model-Based system.
    - distortion_sens: [0, 1] Scales OCI to increase the perceived probability of common transitions.
      Effective P(Common) = 0.7 + (0.3 * distortion_sens * oci).
    """
    learning_rate, beta, w, distortion_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition matrix based on OCI
    # Standard P(common) is 0.7. Distortion pushes it towards 1.0 based on OCI.
    # If distortion_sens is high and OCI is high, they may treat transitions as deterministic.
    p_common = 0.7 + (0.3 * distortion_sens * oci_score)
    p_common = np.clip(p_common, 0.7, 0.999) 
    
    # Transition matrix: Row 0 -> [P(0|0), P(1|0)], Row 1 -> [P(0|1), P(1|1)]
    # Spaceship 0 (A) commonly goes to Planet 0 (X).
    # Spaceship 1 (U) commonly goes to Planet 1 (Y).
    transition_matrix = np.array([
        [p_common, 1.0 - p_common], 
        [1.0 - p_common, p_common]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        # MB Values: Expected value of next stage states given subjective transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        # Stage 1 MF Update (SARSA style using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Unchosen Value Decay
This model suggests that high OCI symptoms are associated with an obsessive focus on the chosen path or outcome. Consequently, the value of the *unchosen* spaceship decays (is forgotten) more rapidly for individuals with high OCI scores, making it harder for them to switch back to neglected options.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Dependent Unchosen Value Decay.
    
    Hypothesis: High OCI is associated with obsessive focus on the chosen path/outcome, 
    resulting in rapid forgetting (decay) of the value of unchosen options.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - decay_sens: [0, 1] Scales OCI to determine the decay rate of unchosen Stage 1 Q-values.
      Decay = decay_sens * oci.
    """
    learning_rate, beta, w, decay_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate specific to this participant
    decay_rate = decay_sens * oci_score
    decay_rate = np.clip(decay_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay the unchosen action's value
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Attentional Lapses
This model hypothesizes that OCI symptoms correlate with "mental noise" or checking compulsions that manifest as random lapses in decision-making. It models this as an epsilon-greedy mixture where the epsilon (lapse rate) is proportional to the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with OCI-Modulated Attentional Lapses (Mixture Model).
    
    Hypothesis: OCI symptoms correlate with 'mental noise' or checking compulsions that 
    manifest as random lapses in decision making, modeled as an epsilon-greedy mixture.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature for the core policy.
    - w: [0, 1] MB/MF weight.
    - lapse_sens: [0, 1] Scales OCI to determine the random lapse probability.
      Lapse Rate = lapse_sens * oci.
    """
    learning_rate, beta, w, lapse_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lapse rate
    lapse_rate = lapse_sens * oci_score
    lapse_rate = np.clip(lapse_rate, 0.0, 0.99)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1_softmax = exp_q1 / np.sum(exp_q1)
        
        # Mixture: (1-epsilon)*Softmax + epsilon*Random
        # Random choice prob is 0.5 for binary choice
        probs_1 = (1 - lapse_rate) * probs_1_softmax + (lapse_rate * 0.5)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2_softmax = exp_q2 / np.sum(exp_q2)
        
        # Mixture for Stage 2 as well
        probs_2 = (1 - lapse_rate) * probs_2_softmax + (lapse_rate * 0.5)
        
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```