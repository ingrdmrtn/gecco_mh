Here are the 3 proposed cognitive models.

### Model 1: OCI-Modulated Persistent Habit
This model hypothesizes that high OCI scores are associated with stronger "persistent habits." Unlike simple stickiness (repeating the last choice), a persistent habit is modeled as a memory trace of past choices that decays slowly. The influence of this habit trace on the current decision is modulated by the participant's OCI score, reflecting the compulsive nature of the disorder.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Persistent Habit Model.
    
    Hypothesis: High OCI is associated with strong habit formation that persists 
    beyond immediate repetition (perseveration). This model tracks a 'habit trace' 
    that accumulates with choices and decays over time. The weight of this habit 
    on the decision policy is modulated by the OCI score.
    
    Parameters:
    - lr: [0,1] Learning rate for Q-values.
    - beta: [0,10] Inverse temperature (softmax sensitivity).
    - w: [0,1] Weighting between Model-Based and Model-Free values (0=MF, 1=MB).
    - lambd: [0,1] Eligibility trace parameter for stage 1 update.
    - habit_decay: [0,1] Decay rate of the habit trace per trial (1=instant forgetting, 0=no forgetting).
    - habit_w_base: [0,5] Baseline weight of the habit trace in decision making.
    - habit_w_oci: [0,5] Additional weight of the habit trace scaled by OCI score.
    """
    lr, beta, w, lambd, habit_decay, habit_w_base, habit_w_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective habit weight
    habit_weight = habit_w_base + habit_w_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit trace for stage 1 actions (A/U)
    habit_trace = np.zeros(2)
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing data
        if a1 < 0 or s2 < 0 or a2 < 0 or r < 0:
            continue
            
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value = MB/MF mix + Habit
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add habit influence
        q_net_with_habit = q_net + habit_weight * habit_trace
        
        exp_q1 = np.exp(beta * q_net_with_habit)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updates ---
        # 1. Update Habit Trace
        # Decay all traces
        habit_trace *= (1.0 - habit_decay)
        # Strengthen chosen trace (Leaky integrator)
        habit_trace[a1] += 1.0
        
        # 2. Update Q-values
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Eligibility trace update for stage 1
        q_stage1_mf[a1] += lr * lambd * delta_2

    return log_loss
```

### Model 2: OCI-Modulated Pessimism
This model proposes that high OCI participants exhibit a "pessimism bias" or loss aversion regarding neutral outcomes. Specifically, they may perceive the absence of a reward (0 coins) not as a neutral event, but as a negative outcome (punishment). This effectively shifts the reference point for learning, making the agent more risk-averse or prone to avoidance behaviors when success is uncertain.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Pessimism (Zero-Outcome Valuation) Model.
    
    Hypothesis: High OCI participants may interpret the lack of reward (0 coins) 
    as a negative outcome (punishment) rather than a neutral zero. This model 
    transforms the reward signal: R_effective = R if R=1, else -Pessimism.
    The magnitude of this pessimism is modulated by the OCI score.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF weight.
    - lambd: [0,1] Eligibility trace.
    - pess_base: [0,2] Base pessimism magnitude (penalty for 0 reward).
    - pess_oci: [0,2] Additional pessimism magnitude scaled by OCI.
    """
    lr, beta, w, lambd, pess_base, pess_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective pessimism
    pessimism = pess_base + pess_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r_raw = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0 or r_raw < 0:
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Reward Transformation ---
        # If reward is 1, use 1. If reward is 0, use -pessimism.
        r_effective = r_raw
        if r_raw == 0.0:
            r_effective = -pessimism
            
        # --- Updates ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r_effective - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2

    return log_loss
```

### Model 3: OCI-Modulated Transition Uncertainty
This model suggests that high OCI is linked to an "intolerance of uncertainty" or a mistrust of the environmental structure. Even if the transition probabilities are stable (70/30), high OCI participants may cognitively "blur" these probabilities towards uniformity (50/50), effectively degrading the accuracy of their Model-Based system. This results in a Model-Based value calculation that is less distinct and less useful.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Transition Uncertainty (Blurring) Model.
    
    Hypothesis: High OCI participants have higher uncertainty about the 
    transition structure, effectively 'blurring' the transition matrix 
    used for Model-Based valuation towards a uniform distribution (maximum entropy).
    This degrades the fidelity of the model-based contribution.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF weight.
    - lambd: [0,1] Eligibility trace.
    - blur_base: [0,1] Base blurring factor (0=True Matrix, 1=Uniform Matrix).
    - blur_oci: [0,1] OCI modulation of the blurring factor.
    """
    lr, beta, w, lambd, blur_base, blur_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate blurring factor k, clipped to [0, 1]
    k = blur_base + blur_oci * oci_score
    k = np.clip(k, 0.0, 1.0)
    
    # Define True and Uniform transition matrices
    T_true = np.array([[0.7, 0.3], [0.3, 0.7]])
    T_flat = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    # The effective transition matrix is a mixture
    T_used = (1.0 - k) * T_true + k * T_flat
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0 or r < 0:
            continue
            
        # --- Stage 1 Policy ---
        # Calculate MB values using the BLURRED transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T_used @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updates ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2

    return log_loss
```