Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making in this two-step task.

### Model 1: Model-Based/Model-Free Hybrid with OCI-Driven Imbalance
This model tests the hypothesis that high OCI scores reflect a deficit in Model-Based (goal-directed) control, leading to a reliance on Model-Free (habitual) systems. The weighting parameter `w` determines the balance between these two systems, and we model `w` as being suppressed by high OCI scores.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated mixing weight.
    
    Hypothesis: Higher OCI scores reduce the contribution of the Model-Based system (w),
    biasing the agent towards Model-Free (habitual) control.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_max: [0, 1] - Maximum weight for Model-Based control when OCI is 0.
    """
    learning_rate, beta, w_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI reduces the model-based weight.
    # If OCI is high (near 1), w approaches 0 (pure Model-Free).
    # If OCI is low, w approaches w_max.
    w = w_max * (1.0 - oci_score)

    # Transition matrix (fixed for the task structure)
    # A (0) -> X (0) (common), U (1) -> Y (1) (common)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf = np.zeros((2, 2))  # Model-Free Q-values for stage 1 actions (implicitly) and stage 2
    # But standard hybrid models usually separate Stage 1 MF and MB.
    # Let's use standard notation:
    Q_TD = np.zeros(2)       # Stage 1 Model-Free values
    Q_MB = np.zeros(2)       # Stage 1 Model-Based values
    Q2 = np.zeros((2, 2))    # Stage 2 values (common to both)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1: # Handle missing data if any
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Calculate Model-Based values: V(s') = max_a Q2(s', a)
        max_q2 = np.max(Q2, axis=1) # [max(Q(X)), max(Q(Y))]
        Q_MB = transition_matrix @ max_q2
        
        # Hybrid Value
        Q_net = w * Q_MB + (1 - w) * Q_TD
        
        exp_q1 = np.exp(beta * Q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * Q2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # 1. Update Stage 2 Q-values (Q2) based on reward
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += learning_rate * delta2
        
        # 2. Update Stage 1 Model-Free values (Q_TD)
        # Using SARSA-like update or simple TD(0) from Stage 2 value
        # Standard 2-step task models often update Q_TD using Q2 of the state arrived at.
        delta1 = Q2[s, a2] - Q_TD[a1]
        Q_TD[a1] += learning_rate * delta1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates based on OCI
This model investigates whether OCI is associated with a negativity bias in learning. Specifically, it proposes that high OCI scores amplify the learning rate for negative prediction errors (disappointments), making the agent quicker to abandon actions that yield worse-than-expected outcomes, while positive learning remains baseline.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-modulated Asymmetric Learning Rates.
    
    Hypothesis: High OCI participants have a 'negativity bias', learning more 
    strongly from negative prediction errors than positive ones.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate for positive prediction errors.
    beta: [0, 10] - Inverse temperature.
    neg_amp: [0, 5] - Amplification factor for negative prediction errors based on OCI.
    """
    lr_base, beta, neg_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate: base + amplification by OCI
    # If prediction error is negative, effective LR = lr_base * (1 + neg_amp * OCI)
    # We clamp it to 1.0 to ensure stability.
    lr_neg_factor = 1.0 + (neg_amp * oci_score)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1: 
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Choice 1
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Choice 2
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates with Asymmetry
        
        # Stage 2 Update
        delta2 = r - q_stage2[s, a2]
        current_lr2 = lr_base
        if delta2 < 0:
            current_lr2 = min(1.0, lr_base * lr_neg_factor)
        q_stage2[s, a2] += current_lr2 * delta2

        # Stage 1 Update
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        current_lr1 = lr_base
        if delta1 < 0:
            current_lr1 = min(1.0, lr_base * lr_neg_factor)
        q_stage1[a1] += current_lr1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration Penalty (OCI-Modulated)
This model posits that high OCI is linked to an intolerance of uncertainty. Instead of standard exploration bonuses, this model introduces an "uncertainty penalty." The model tracks how often an action has been taken recently (inverse of uncertainty). High OCI participants are penalized for choosing actions they are uncertain about (i.e., they stick to what they know).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    RL with OCI-modulated Uncertainty Avoidance.
    
    Hypothesis: OCI is associated with intolerance of uncertainty. 
    The model tracks a 'familiarity' count for Stage 1 actions.
    High OCI adds a penalty to actions with low familiarity (high uncertainty).
    
    Parameters:
    learning_rate: [0, 1] - Q-value update rate.
    beta: [0, 10] - Inverse temperature.
    unc_penalty_scale: [0, 5] - Scaling factor for the uncertainty penalty.
    decay_rate: [0, 1] - Rate at which familiarity decays (forgetting).
    """
    learning_rate, beta, unc_penalty_scale, decay_rate = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track familiarity of Stage 1 actions. Initialize at 1 to avoid div by zero initially.
    familiarity = np.ones(2) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1: 
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Calculate Uncertainty Penalty
        # Uncertainty is modeled as inverse of familiarity.
        # Penalty = unc_penalty_scale * OCI * (1/familiarity)
        # Higher OCI -> Higher penalty for exploring less familiar options.
        uncertainty = 1.0 / familiarity
        penalty = unc_penalty_scale * oci_score * uncertainty
        
        # Effective Q for decision includes the penalty
        q_effective = q_stage1 - penalty
        
        exp_q1 = np.exp(beta * q_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice (standard)
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta2

        delta1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1
        
        # Update Familiarity
        # Decay all familiarity traces
        familiarity *= (1.0 - decay_rate)
        # Increment chosen action
        familiarity[a1] += 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```