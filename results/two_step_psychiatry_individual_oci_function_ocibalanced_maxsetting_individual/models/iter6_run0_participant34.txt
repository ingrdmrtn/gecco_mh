Here are 3 new cognitive models that incorporate the OCI score into the decision-making process, specifically tailored to the high OCI participant's tendency towards perseveration or rigid strategy maintenance.

### Model 1: Hybrid RL with OCI-Modulated Model-Based Weight
This model tests the hypothesis that high OCI individuals might rely more on habitual (model-free) control and less on flexible (model-based) planning, or vice versa. Specifically, it posits that OCI score modulates the mixing weight ($w$) between model-based and model-free systems. A high OCI score might suppress the model-based contribution, leading to more rigid, habit-like behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-Modulated Model-Based Weight.

    Hypothesis: The balance between model-based (MB) and model-free (MF) control is
    modulated by OCI. High OCI might lead to a dominance of one system. Here, 
    w (weight of MB) is defined as w_base modulated by OCI.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w_base: [0, 1] Baseline model-based weight.
    oci_w_mod: [0, 1] Strength of OCI modulation on w. 
                  (e.g., w_final = w_base * (1 - oci * oci_w_mod))
    """
    learning_rate, beta, w_base, oci_w_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective weight. If oci_w_mod is high, high OCI reduces MB influence.
    w = w_base * (1.0 - oci_score * oci_w_mod)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy: Mixture of MB and MF
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy: Pure MF
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        # Stage 2 update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates with OCI-Driven Punishment Sensitivity
This model explores the idea that OCI is related to altered sensitivity to negative outcomes (omission of reward). Instead of a single learning rate, we use separate rates for positive and negative prediction errors. The OCI score specifically amplifies the learning rate for negative outcomes (`alpha_neg`), reflecting a hypersensitivity to "failure" or missing gold, which drives rapid switching or anxiety-driven corrections.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rates with OCI-Driven Punishment Sensitivity.

    Hypothesis: High OCI individuals may overreact to lack of reward (0 coins).
    This model splits the learning rate into alpha_pos and alpha_neg.
    The negative learning rate is boosted by the OCI score.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    oci_neg_sens: [0, 5] Multiplier for OCI impact on negative learning rate.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_neg_sens, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base * (1.0 + oci_score * oci_neg_sens)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy (Pure MF for simplicity to isolate learning rate effect)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg * delta_stage2
        
        # Learning Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based/Model-Free Hybrid with OCI-Dependent Stickiness
This model combines the standard hybrid RL framework with a "stickiness" parameter (tendency to repeat the last choice regardless of reward). Here, the OCI score directly scales the stickiness parameter. This captures the hypothesis that high OCI correlates with behavioral rigidity or compulsivity, manifesting as a higher probability of simply repeating the previous motor action.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-Dependent Stickiness.

    Hypothesis: OCI is associated with behavioral rigidity (compulsivity).
    This model adds a 'stickiness' bonus to the Q-values of the previously chosen
    action. The magnitude of this stickiness is scaled by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Update rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for model-based control (static).
    oci_stickiness_gain: [0, 5] How much OCI increases the stickiness bias.
    """
    learning_rate, beta, w, oci_stickiness_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective stickiness
    stickiness = oci_score * oci_stickiness_gain

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choices
    last_a1 = -1
    last_a2 = np.full(2, -1) # Track last choice per state

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to stage 1
        q_net_1_sticky = q_net_1.copy()
        if last_a1 != -1:
            q_net_1_sticky[last_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        q_net_2 = q_stage2_mf[s_idx].copy()
        # Add stickiness to stage 2 (per state)
        if last_a2[s_idx] != -1:
             q_net_2[last_a2[s_idx]] += stickiness

        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update history
        last_a1 = a1
        last_a2[s_idx] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```