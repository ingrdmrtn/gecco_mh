Here are the 3 new cognitive models as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF model with OCI-modulated Trace Stickiness.
    
    Hypothesis:
    High OCI individuals form stronger, longer-lasting habits (perseveration).
    Unlike simple repetition of the last choice, they build up an exponentially 
    decaying 'trace' of past choices. OCI modulates the weight of this accumulated 
    trace in the decision process, making established habits harder to break.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    lambda_elig: [0,1]
    trace_decay: [0,1] - Persistence of the choice trace (higher = longer memory).
    stick_oci: [0,5] - Weight of the trace in decision making, scaled by OCI.
    """
    learning_rate, beta, w, lambda_elig, trace_decay, stick_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Trace of past choices for Stage 1 (initialized to 0)
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add trace stickiness: OCI scales the impact of the accumulated history
        stickiness_bonus = stick_oci * current_oci * choice_trace
        q_net_stage1 += stickiness_bonus
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update Choice Trace
        a1 = int(action_1[trial])
        # Decay existing trace for all actions
        choice_trace *= trace_decay
        # Increment trace for chosen action
        choice_trace[a1] += 1.0
        
        # Stage 2 Policy
        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF model with OCI-modulated Rare Transition Gating.
    
    Hypothesis:
    High OCI individuals may view rare transitions as 'noise' or 'errors' 
    and suppress learning from them to protect their Stage 1 value estimates.
    This creates rigidity by ignoring contradictory evidence (RPEs) arising from 
    rare states, effectively filtering out 'luck' from their habit learning.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    lambda_elig: [0,1]
    oci_rare_gating: [0,1] - Degree to which OCI reduces learning rate on rare transitions.
    """
    learning_rate, beta, w, lambda_elig, oci_rare_gating = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2 Policy
        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        a1 = int(action_1[trial])
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Determine if transition was rare
        # Common transitions: Action 0 -> State 0, Action 1 -> State 1
        is_rare = (a1 != state_idx)
        
        # Modulate learning rate for Stage 1 based on transition type and OCI
        lr_s1 = learning_rate
        if is_rare:
            lr_s1 *= (1.0 - current_oci * oci_rare_gating)
        
        # Update Stage 1 MF (gated by transition type)
        q_stage1_mf[a1] += lr_s1 * (delta_stage1 + lambda_elig * delta_stage2)
        
        # Update Stage 2 MF (standard learning)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF model with OCI-modulated Learning Rate Decay (Crystallization).
    
    Hypothesis:
    High OCI individuals exhibit 'behavioral crystallization', where they lose the 
    ability to update values over time, becoming stuck in early learned behaviors.
    The learning rate decays as a function of trial number and OCI score, leading
    to progressive rigidity.
    
    Parameters:
    learning_rate: [0,1] - Initial learning rate.
    beta: [0,10]
    w: [0,1]
    lambda_elig: [0,1]
    oci_lr_decay: [0,1] - Rate at which learning rate decays over trials for high OCI.
    """
    learning_rate, beta, w, lambda_elig, oci_lr_decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Internal scaling factor to make the decay parameter meaningful over ~200 trials
    decay_scale = 0.1 
    
    for trial in range(n_trials):
        # Calculate current learning rate: decreases as trial count increases, scaled by OCI
        decay_factor = 1.0 + (decay_scale * current_oci * oci_lr_decay * trial)
        current_lr = learning_rate / decay_factor
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2 Policy
        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning using the decayed learning rate
        a1 = int(action_1[trial])
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += current_lr * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```