Here are 3 new cognitive models based on the provided participant data and the two-step task structure.

### Model 1: OCI-Modulated Eligibility Trace (Lambda)
This model hypothesizes that OCI affects the "eligibility trace" ($\lambda$) in reinforcement learning. In the two-step task, $\lambda$ determines how much the reward at the second stage reinforces the choice at the first stage directly (model-free credit assignment). A higher $\lambda$ connects the outcome more strongly to the initial action, a hallmark of habitual learning. Since high OCI is associated with compulsivity and habits, this model posits that higher OCI scores lead to a higher eligibility trace parameter.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reinforcement Learning with OCI-modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI score modulates the eligibility trace parameter (lambda).
    Higher OCI leads to a stronger eligibility trace, meaning the second-stage outcome
    reinforces the first-stage choice more strongly (characteristic of habit/model-free).
    
    lambda_val = lambda_base + lambda_oci * oci_score
    (Clipped to [0, 1])

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] (Mixing weight, 1=Model-Based, 0=Model-Free)
    lambda_base: [0, 1]
    lambda_oci: [0, 1]
    """
    learning_rate, beta, w, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective lambda based on OCI
    lambda_val = lambda_base + lambda_oci * oci_score
    if lambda_val > 1.0: lambda_val = 1.0
    if lambda_val < 0.0: lambda_val = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)      # Model-free values for stage 1
    q_mb1 = np.zeros(2)      # Model-based values for stage 1 (derived)
    q_mf2 = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2

        # Integrated Value (Hybrid)
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 RPE
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2

        # Stage 1 RPE (TD(0) part)
        delta_1 = q_mf2[s2, a2] - q_mf1[a1] # Note: using updated Q2 is common in some implementations, or pre-update
        q_mf1[a1] += learning_rate * delta_1
        
        # Eligibility Trace Update (TD(1) part)
        # The reward at stage 2 reinforces stage 1 choice scaled by lambda
        q_mf1[a1] += learning_rate * lambda_val * delta_2

    eps = 1e-10
    # Handle missing trials if any (though data seems clean, good practice)
    p_choice_1 = p_choice_1[p_choice_1 > 0] 
    p_choice_2 = p_choice_2[p_choice_2 > 0]
    
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Second-Stage Learning Rate Asymmetry
This model suggests that high OCI participants might learn differently from positive versus negative outcomes at the second stage (the immediate reward). Specifically, compulsivity might be linked to a heightened sensitivity to punishments (or lack of reward) or a rigidity that prevents updating after rewards. Here, we split the learning rate for the second stage into a base rate and an OCI-modulated boost specifically for positive rewards, testing if high OCI participants "lock in" on rewards differently.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Learning Rate Asymmetry for Rewards.
    
    Hypothesis: OCI affects how strongly participants update values after receiving a 
    reward (positive prediction error) versus no reward.
    lr_pos = lr_base + lr_oci_mod * oci_score
    lr_neg = lr_base
    
    This tests if high OCI leads to over-learning from positive outcomes (or under-learning).

    Bounds:
    lr_base: [0, 1]
    lr_oci_mod: [-1, 1] (Can increase or decrease sensitivity)
    beta: [0, 10]
    w: [0, 1]
    stiffness: [0, 1] (Unused parameter placeholder to keep count to 5 if needed, but here we use 4 active params)
    """
    lr_base, lr_oci_mod, beta, w, _ = model_parameters # 5th param unused
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate learning rates
    lr_pos = lr_base + lr_oci_mod * oci_score
    lr_neg = lr_base
    
    # Clip to valid range
    if lr_pos > 1.0: lr_pos = 1.0
    if lr_pos < 0.0: lr_pos = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Determine specific learning rate for this trial based on RPE sign
        # Note: Strictly speaking RPE isn't known yet, but usually asymmetry is defined by the Outcome (Reward vs No Reward)
        # or the sign of the RPE. Here we use the Reward outcome for simplicity/robustness in this task (0 or 1).
        current_lr = lr_pos if r > 0 else lr_neg

        # Stage 2 Update
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += current_lr * delta_2

        # Stage 1 Update (Simple SARSA-like for MF)
        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += current_lr * delta_1
        
        # Lambda=1 implicit assumption for MF path often used in simple hybrid models
        q_mf1[a1] += current_lr * delta_2

    eps = 1e-10
    p_choice_1 = p_choice_1[p_choice_1 > 0]
    p_choice_2 = p_choice_2[p_choice_2 > 0]
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Stickiness (Perseveration)
High OCI is strongly linked to repetitive behaviors and difficulty switching sets (perseveration). This model incorporates a "stickiness" parameter that biases the agent to repeat the previous choice at the first stage. The magnitude of this stickiness is a function of the OCI score. A higher OCI score results in higher stickiness, regardless of the reward history (pure repetition).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Choice Stickiness (Perseveration).
    
    Hypothesis: High OCI participants exhibit higher choice perseveration (stickiness),
    tending to repeat the previous Stage 1 action regardless of reward.
    
    stickiness = stick_base + stick_oci * oci_score
    
    This stickiness bonus is added to the Q-value of the previously chosen action
    at Stage 1.

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stick_base: [0, 5] (Magnitude of base stickiness)
    stick_oci: [0, 5] (Additional stickiness per unit of OCI)
    """
    learning_rate, beta, w, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    stickiness = stick_base + stick_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            last_action_1 = -1
            continue

        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        # Calculate net Q-values
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Add Stickiness Bonus
        # We create a copy to not corrupt the actual learned Q-values
        q_choice = q_net.copy()
        if last_action_1 != -1:
            q_choice[last_action_1] += stickiness
        
        exp_q1 = np.exp(beta * q_choice)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_2 = r - q_mf2[s2, a2]
        q_mf2[s2, a2] += learning_rate * delta_2

        delta_1 = q_mf2[s2, a2] - q_mf1[a1]
        q_mf1[a1] += learning_rate * delta_1
        q_mf1[a1] += learning_rate * delta_2 # Lambda=1

        last_action_1 = a1

    eps = 1e-10
    p_choice_1 = p_choice_1[p_choice_1 > 0]
    p_choice_2 = p_choice_2[p_choice_2 > 0]
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```