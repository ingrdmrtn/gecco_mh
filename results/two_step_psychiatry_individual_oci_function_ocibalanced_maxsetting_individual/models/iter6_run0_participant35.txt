Here are the 3 proposed cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Transition Learning Model.
    
    Hypothesis: High OCI individuals may exhibit abnormalities in how they learn the structure 
    of the environment (transition probabilities). They might be overly rigid (low transition 
    learning rate) or overly sensitive to rare transitions (high transition learning rate), 
    constantly updating their belief about which spaceship goes to which planet.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates (MF and Stage 2).
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - stickiness: [0, 5] Perseveration bonus for the previously chosen spaceship.
    - lr_trans_base: [0, 1] Baseline learning rate for the transition matrix.
    - lr_trans_oci: [-1, 1] OCI modulation of the transition learning rate.
    """
    learning_rate, beta, w, stickiness, lr_trans_base, lr_trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated transition learning rate
    lr_trans = lr_trans_base + (lr_trans_oci * oci_score)
    if lr_trans < 0: lr_trans = 0.0
    if lr_trans > 1: lr_trans = 1.0
    
    # Initialize transition beliefs (Probability of 'Common' transition)
    # T[0] is P(Planet 0 | Spaceship 0), T[1] is P(Planet 1 | Spaceship 1)
    # Initialize with instruction knowledge (0.7)
    trans_prob_0 = 0.7
    trans_prob_1 = 0.7
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Construct current transition matrix from dynamic beliefs
        transition_matrix = np.array([
            [trans_prob_0, 1.0 - trans_prob_0],
            [1.0 - trans_prob_1, trans_prob_1]
        ])
        
        # Policy for first choice (Stage 1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Policy for second choice (Stage 2)
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Data for updates
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # MF Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # MF Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Dynamic Transition Model Update
        # If a1=0, target is 1 if we went to Planet 0, else 0
        # If a1=1, target is 1 if we went to Planet 1, else 0
        if a1 == 0:
            target = 1.0 if state_idx == 0 else 0.0
            trans_prob_0 += lr_trans * (target - trans_prob_0)
        elif a1 == 1:
            target = 1.0 if state_idx == 1 else 0.0
            trans_prob_1 += lr_trans * (target - trans_prob_1)
            
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 2 Beta Modulation Model.
    
    Hypothesis: OCI affects the decision noise specifically at the second stage (choosing aliens),
    distinct from the first stage. High OCI might correlate with either more random behavior (low beta)
    or more rigid exploitation (high beta) at the point of reward consumption, independent of
    planning capability.
    
    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 choice.
    - beta_2_base: [0, 10] Baseline inverse temperature for Stage 2 choice.
    - beta_2_oci: [-5, 5] OCI modulation of Stage 2 beta.
    - w: [0, 1] MB/MF weighting.
    - stickiness: [0, 5] Stage 1 perseveration.
    """
    learning_rate, beta_1, beta_2_base, beta_2_oci, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated Beta 2
    beta_2 = beta_2_base + (beta_2_oci * oci_score)
    if beta_2 < 0: beta_2 = 0.0
    if beta_2 > 20: beta_2 = 20.0 # Safety upper bound
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Stage 1 Policy (Uses beta_1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2 Policy (Uses beta_2)
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Updates
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stickiness Decay Model.
    
    Hypothesis: High OCI individuals may have 'stickier' mental traces of their previous choices.
    Instead of stickiness being just 'last choice', this model implements a decaying choice trace.
    OCI modulates the decay rate: High OCI might lead to slower decay (stronger history dependence)
    or faster decay (instability).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stick_mag: [0, 5] Magnitude of the stickiness bias applied to the Q-values.
    - decay_base: [0, 1] Baseline decay rate of the choice trace (1=instant forget, 0=perfect memory).
    - decay_oci: [-1, 1] OCI modulation of the decay rate.
    """
    learning_rate, beta, w, stick_mag, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated decay rate
    decay = decay_base + (decay_oci * oci_score)
    if decay < 0: decay = 0.0
    if decay > 1: decay = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace vector [trace_A, trace_U]
    choice_trace = np.zeros(2)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness based on accumulated choice trace
        q_net += stick_mag * choice_trace
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2 Policy
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Updates
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update choice trace: Decay then increment chosen
        choice_trace *= (1.0 - decay)
        choice_trace[a1] += 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```