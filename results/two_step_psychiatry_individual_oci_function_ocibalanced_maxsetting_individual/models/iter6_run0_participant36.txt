Here are 3 new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: Subjective Transition Distortion
**Hypothesis:** High OCI participants may possess a distorted internal model of the environment's transition probabilities. While the true transition probability is 0.7, obsessive-compulsive traits might lead to "magical thinking" (perceiving the transitions as deterministic, $P \approx 1.0$) or doubt/uncertainty (perceiving them as random, $P \approx 0.5$). This model allows the OCI score to shift the $P(State|Action)$ used in the Model-Based calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Distortion Model.
    
    The Model-Based system uses a subjective transition matrix modulated by OCI.
    Standard transition is 0.7. This model allows the participant to act as if
    the transition is more deterministic or more random based on their OCI.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - lambda_coeff: [0, 1] Eligibility trace decay.
    - dist_base: [-0.5, 0.5] Base distortion of transition probability (relative to 0.7).
    - dist_oci: [-0.5, 0.5] Scaling of distortion by OCI.
    """
    learning_rate, beta, w, lambda_coeff, dist_base, dist_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # True p is 0.7. Subjective p is 0.7 + base + oci * scale.
    # Clipped to avoid invalid probabilities.
    p_subjective = 0.7 + dist_base + (dist_oci * oci_score)
    p_subjective = np.clip(p_subjective, 0.01, 0.99)
    
    # Construct subjective transition matrix
    # Row 0: Action 0 (A) -> [Prob X, Prob Y]
    # Row 1: Action 1 (U) -> [Prob X, Prob Y] (Standard: U goes to Y, so [0.3, 0.7])
    # We assume symmetry in the distortion: if A->X is p, U->Y is p.
    transition_matrix = np.array([
        [p_subjective, 1 - p_subjective], 
        [1 - p_subjective, p_subjective]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max value of next stage using subjective transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Stage 1 PE (using chosen stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Stage 2 PE (using reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Update Stage 1 (MF) with eligibility trace
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        
        # Update Stage 2 (MF)
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Cumulative Frustration (Dynamic Beta)
**Hypothesis:** High OCI participants may experience higher emotional dysregulation or "frustration" in response to losses (0 coins). This model tracks a cumulative "frustration" state variable that decays over time but increases with every unrewarded trial. This frustration modulates the inverse temperature (`beta`), causing the participant to become either more rigid (high beta) or more erratic (low beta) as frustration accumulates.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Cumulative Frustration Model.
    
    Tracks a latent 'frustration' variable that increases when reward is 0 
    and decays over trials. High OCI modulates how strongly this frustration 
    affects the exploration parameter (beta).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Mixing weight.
    - lambda_coeff: [0, 1] Eligibility trace decay.
    - frust_decay: [0, 1] Decay rate of the frustration variable (persistence of affect).
    - frust_sens: [-5, 5] Sensitivity to frustration modulated by OCI. 
                  Positive = frustration makes behavior more rigid. 
                  Negative = frustration makes behavior more random.
    """
    learning_rate, beta_base, w, lambda_coeff, frust_decay, frust_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    frustration = 0.0
    
    for trial in range(n_trials):
        # Dynamic Beta Calculation
        # Beta scales based on accumulated frustration and OCI
        beta_current = beta_base * (1.0 + (frust_sens * oci_score * frustration))
        # Ensure beta stays non-negative
        beta_current = max(0.0, beta_current)

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_current * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        logits_2 = beta_current * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # --- Update Frustration ---
        # If reward is 0, frustration increases by 1. Always decays.
        is_loss = 1.0 if reward[trial] == 0 else 0.0
        frustration = (frust_decay * frustration) + is_loss

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Choice Mixture Stickiness
**Hypothesis:** Standard stickiness adds a bonus to the value (Q-value) of the previous action. This model proposes that high OCI leads to a *compulsive repetition* that is independent of value calculations. This is modeled as a mixture process: with probability $\kappa$ (modulated by OCI), the participant simply repeats the last action regardless of its value; with probability $1-\kappa$, they use the RL policy. This places a "floor" on the probability of repetition, creating hard constraints on switching behavior.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Choice Mixture Stickiness Model.
    
    Models stickiness as a mixture of the RL policy and a pure repetition policy.
    The weight of the repetition policy (kappa) is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_coeff: [0, 1]
    - kappa_base: [0, 1] Base mixture weight for repetition.
    - kappa_oci: [0, 1] OCI scaling for repetition weight.
    """
    learning_rate, beta, w, lambda_coeff, kappa_base, kappa_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixture weight kappa
    kappa = kappa_base + (kappa_oci * oci_score)
    kappa = np.clip(kappa, 0.0, 0.99) # Ensure valid probability
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # No previous action for first trial
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # RL Policy
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_rl = np.exp(logits_1) / np.sum(np.exp(logits_1))
        
        # Mixture Policy
        if prev_action_1 != -1:
            # Mix RL probabilities with pure repetition (1.0 on prev action, 0 on others)
            probs_final = (1 - kappa) * probs_rl
            probs_final[int(prev_action_1)] += kappa
        else:
            probs_final = probs_rl
            
        p_choice_1[trial] = probs_final[int(action_1[trial])]
        prev_action_1 = action_1[trial]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```