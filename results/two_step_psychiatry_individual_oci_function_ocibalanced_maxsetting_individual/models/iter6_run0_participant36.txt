Here are the 3 proposed cognitive models.

### Model 1: Hybrid Model with OCI-Modulated Asymmetric Learning in Stage 2
This model hypothesizes that high OCI participants exhibit biased learning from outcomes (aliens) in the second stage. Specifically, OCI modulates the learning rate for negative outcomes (0 coins), reflecting a potential hypersensitivity to failure or lack of reward, which then propagates to the first stage via both Model-Based (MB) planning and Model-Free (MF) updates.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model with OCI-Modulated Asymmetric Learning in Stage 2.
    Hypothesis: OCI modulates the learning rate for negative prediction errors (reward=0) 
    in Stage 2. The participant combines Model-Based and Model-Free values (Hybrid), 
    but the values they learn from aliens are skewed by this asymmetry.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive outcomes (Reward=1) in Stage 2.
    - lr_neg_base: [0, 1] Base learning rate for negative outcomes (Reward=0) in Stage 2.
    - oci_neg_mod: [-1, 1] Modulation of the negative learning rate by OCI score.
    - beta: [0, 10] Inverse temperature for softmax (choice consistency).
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    """
    lr_pos, lr_neg_base, oci_neg_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-modulated negative learning rate
    lr_neg = np.clip(lr_neg_base + (oci_score * oci_neg_mod), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (State x Action)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected value of next stage states (max over actions)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Standard Softmax on Stage 2 values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 MF Update (TD(0))
        # Using lr_pos as a generic learning rate for Stage 1 MF to save parameters
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_pos * delta_stage1
        
        # Stage 2 MF Update with Asymmetry
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Select learning rate based on outcome
        if r > 0.5: # Reward 1
            current_lr = lr_pos
        else:       # Reward 0
            current_lr = lr_neg
            
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-Modulated Win-Stay Bias
This model proposes that high OCI drives a specific form of perseveration: repeating successful actions ("Win-Stay"). Unlike general stickiness which repeats any choice, this bonus is only applied if the previous trial was rewarded. OCI modulates the magnitude of this "success-driven compulsion" in the first stage.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Model with OCI-Modulated Win-Stay Bias (Reward-Dependent Stickiness).
    Hypothesis: OCI modulates the tendency to repeat the previous Stage 1 choice 
    specifically when it resulted in a reward (Win-Stay). This captures a 'compulsive 
    repetition of success' heuristic overlaying the hybrid RL process.

    Parameters:
    - lr: [0, 1] Learning rate for both stages.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - wsls_base: [0, 5] Base Win-Stay bonus added to Q-value.
    - oci_wsls_mod: [-2, 2] Modulation of Win-Stay bonus by OCI.
    """
    lr, beta, w, wsls_base, oci_wsls_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated Win-Stay bonus
    wsls_bonus = wsls_base + (oci_score * oci_wsls_mod)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_reward = -1

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Win-Stay Bonus
        if last_action_1 != -1 and last_reward == 1:
            q_net_stage1[last_action_1] += wsls_bonus
            
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        last_reward = r

        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Modulated Stage 2 Response Stickiness
This model investigates if OCI affects perseveration at the *second* stage (the alien choice). It hypothesizes that high OCI participants may compulsively repeat the same motor response (e.g., "Left" vs "Right" alien) across trials, regardless of which planet they are on. This is a form of "Response Stickiness" rather than "Choice Stickiness".

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Model with OCI-Modulated Stage 2 Response Stickiness.
    Hypothesis: OCI modulates the tendency to repeat the same motor response (action index)
    at Stage 2, regardless of the planet visited. This represents a low-level motor 
    perseveration or rigidity in the final selection step.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0 = Pure MF, 1 = Pure MB).
    - stick_s2_base: [0, 5] Base stickiness for Stage 2 actions.
    - oci_stick_s2_mod: [-2, 2] Modulation of Stage 2 stickiness by OCI.
    """
    lr, beta, w, stick_s2_base, oci_stick_s2_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated Stage 2 stickiness
    stick_s2 = stick_s2_base + (oci_score * oci_stick_s2_mod)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_2 = -1

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # MB Planning: Agent anticipates its own Stage 2 stickiness? 
        # For simplicity, we assume MB uses max Q values without stickiness bias,
        # or that stickiness is a motor bias applied at decision time.
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        q_net_stage2 = q_stage2_mf[state_idx].copy()
        
        # Apply Response Stickiness (same button index as last trial)
        if last_action_2 != -1:
            q_net_stage2[last_action_2] += stick_s2
            
        exp_q2 = np.exp(beta * q_net_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        last_action_2 = action_2[trial]

        # --- Updating ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]

        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```