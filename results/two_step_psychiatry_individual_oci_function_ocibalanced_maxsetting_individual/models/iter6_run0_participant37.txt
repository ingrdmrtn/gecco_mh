Here are three new cognitive models for the two-step task, incorporating the OCI score to explain individual variability in decision-making.

### Cognitive Model 1: OCI-Modulated Choice Trace Memory
This model hypothesizes that high OCI scores are associated with a more persistent "memory" of past choices, leading to stronger habit formation (perseveration). Instead of a simple one-step repetition bonus, the model uses an exponential moving average (EMA) of past choices. The decay rate of this memory trace is modulated by OCI: higher OCI leads to slower decay, meaning past choices influence current behavior for longer.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with OCI-modulated Stickiness Memory (Trace Decay).
    
    Hypothesis: High OCI participants have a longer 'memory' for past choices,
    leading to stronger habit formation. The decay rate of the choice trace
    is modulated by OCI.
    
    trace_t = decay * trace_{t-1} + (1-decay) * I(choice_t)
    decay = decay_sensitivity * oci
    
    Parameters:
    - learning_rate: [0,1] Standard learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Weighting between Model-Based and Model-Free values.
    - lam: [0,1] Eligibility trace for Stage 2 outcome effect on Stage 1.
    - stick_weight: [0,10] Weight of the choice trace in the decision.
    - decay_sensitivity: [0,1] Sensitivity of the memory decay to OCI.
    """
    learning_rate, beta, w, lam, stick_weight, decay_sensitivity = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate decay rate based on OCI.
    # Higher OCI -> Higher decay (closer to 1) -> Longer memory.
    # We cap decay at 0.95 to ensure stability.
    decay = decay_sensitivity * oci_score
    if decay > 0.95: decay = 0.95
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    choice_trace = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q_stage2
        
        # Net value includes weighted choice trace
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1 + stick_weight * choice_trace
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        
        # 1. Update Choice Trace (Exponential Moving Average)
        # Decay all traces
        choice_trace *= decay
        # Reinforce chosen action
        choice_trace[a1] += (1.0 - decay)
        
        # 2. Update Q-Values
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        q_mf_1[a1] += learning_rate * lam * delta_2
        
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Transition Distortion
This model suggests that high OCI scores are linked to uncertainty or distrust regarding the "common" vs. "rare" transition structure of the task. High OCI participants may "flatten" their internal model of the transition matrix, treating transitions as more random (closer to 50/50) than they actually are. This degrades the accuracy of Model-Based planning, potentially leading to reliance on other strategies.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with OCI-modulated Transition Belief Distortion.
    
    Hypothesis: High OCI participants have distorted beliefs about the 
    transition structure (uncertainty or distrust of 'common' vs 'rare').
    The transition matrix used for MB planning is flattened towards [0.5, 0.5]
    based on OCI.
    
    p_common = 0.7 - (0.2 * distortion_factor * oci)
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF mixing weight.
    - lam: [0,1] Eligibility trace.
    - stickiness: [0,10] Simple 1-step stickiness bonus.
    - distortion_factor: [0,1] Degree of transition flattening per OCI unit.
    """
    learning_rate, beta, w, lam, stickiness, distortion_factor = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Adjust transition matrix based on OCI
    # Maximum distortion (factor=1, oci=1) reduces 0.7 to 0.5.
    dist = distortion_factor * oci_score
    if dist > 1.0: dist = 1.0
    
    p_common = 0.7 - (0.2 * dist)
    p_rare = 1.0 - p_common
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_mf_2, axis=1)
        # Use the distorted transition matrix for planning
        q_mb_1 = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_a1 = a1
        
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Stage 1 Learning Dampening
This model hypothesizes that high OCI is associated with rigidity in top-level choices (habits). It implements this by dampening the learning rate specifically for Stage 1 Model-Free updates based on the OCI score. A lower learning rate at Stage 1 makes it harder for the agent to update its preferences for the spaceships, leading to perseveration on established choices even when outcomes vary.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid model with OCI-modulated Stage 1 Learning Dampening.
    
    Hypothesis: High OCI participants exhibit rigidity in their top-level choices (habits).
    This is modeled as a reduced learning rate for Stage 1 updates, making
    it harder to unlearn established preferences (perseveration).
    
    lr_stage1 = learning_rate / (1 + dampening * oci)
    
    Parameters:
    - learning_rate: [0,1] Base learning rate (applied fully to Stage 2).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF mixing weight.
    - lam: [0,1] Eligibility trace.
    - stickiness: [0,10] Simple 1-step stickiness.
    - dampening: [0,10] Scaling factor for OCI-based LR reduction at Stage 1.
    """
    learning_rate, beta, w, lam, stickiness, dampening = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Calculate Stage 1 Learning Rate
    # High OCI reduces lr_1, causing slower adaptation/perseveration at Stage 1.
    lr_1 = learning_rate / (1.0 + dampening * oci_score)
    lr_2 = learning_rate
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Use dampened lr_1 for Stage 1 MF update
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += lr_1 * delta_1
        
        # Use standard lr_2 for Stage 2 MF update
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += lr_2 * delta_2
        
        # Lambda update also uses lr_1 (updating Stage 1 value)
        q_mf_1[a1] += lr_1 * lam * delta_2
        
        prev_a1 = a1
        
    return log_loss
```