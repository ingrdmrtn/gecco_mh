Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence reinforcement learning in this two-step task.

### Model 1: Stickiness Modulation by OCI
This model hypothesizes that high OCI scores lead to increased "stickiness" or perseveration in choice behavior. Rather than just modulating learning rates or mixing weights, OCI here directly scales a stickiness parameter, making the participant more likely to repeat their previous Stage 1 choice regardless of the outcome. This aligns with the compulsive repetition often seen in OCD.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner with OCI-modulated Choice Stickiness.
    
    Hypothesis: High OCI leads to higher perseveration (stickiness), making the 
    participant more likely to repeat the previous Stage 1 action regardless of reward.
    
    Parameters:
    learning_rate: [0,1] - Value update rate.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Model-based weight.
    stickiness_base: [0,5] - Baseline tendency to repeat the last choice.
    oci_stickiness_boost: [0,5] - Additional stickiness scaled by OCI score.
    """
    learning_rate, beta, w, stickiness_base, oci_stickiness_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective stickiness based on OCI
    stickiness = stickiness_base + (oci_stickiness_boost * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness
    prev_a1 = -1

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_hybrid_sticky = q_hybrid.copy()
        if prev_a1 != -1:
            q_hybrid_sticky[prev_a1] += stickiness

        exp_q1 = np.exp(beta * q_hybrid_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning Updates
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD(1) logic usually implies updating Stage 1 with Stage 2's value)
        # Here we use the standard TD(0) update from the template logic logic
        # But using the updated Stage 2 value as the target is common in these MB/MF models
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model proposes that OCI specifically affects how participants learn from negative outcomes (punishment/omission of reward). High OCI might lead to "hyper-learning" from failure, causing rapid shifts in value estimation when rewards are not received. This reflects the anxiety and error-sensitivity often associated with OCD.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner with OCI-modulated Punishment Sensitivity.
    
    Hypothesis: High OCI leads to a higher learning rate specifically for negative prediction errors 
    (when reward is less than expected), reflecting hypersensitivity to errors/omissions.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0,1] - Baseline learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Model-based weight.
    oci_neg_boost: [0,1] - Factor increasing the negative learning rate based on OCI.
    """
    alpha_pos, alpha_neg_base, beta, w, oci_neg_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate, capped at 1.0
    alpha_neg = min(1.0, alpha_neg_base + (oci_neg_boost * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning Updates
        # Determine which learning rate to use for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_stage2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2
        
        # Determine which learning rate to use for Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_stage1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr_stage1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Trace Decay Modulated by OCI
This model introduces an eligibility trace parameter ($\lambda$) for the Model-Free system, which controls how much credit the first-stage choice gets for the second-stage outcome. The hypothesis is that OCI modulates this credit assignment. Specifically, high OCI might lead to a failure to properly assign credit across time steps (lower $\lambda$), or conversely, an over-attribution (higher $\lambda$). Here we test if OCI modifies the strength of the eligibility trace `lambda`.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner with OCI-modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI affects the efficiency of credit assignment in the model-free system.
    Instead of a simple TD(0) update, this model uses TD(lambda).
    The lambda parameter is modulated by OCI.
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Model-based weight.
    lambda_base: [0,1] - Baseline eligibility trace decay rate.
    oci_lambda_mod: [-0.5, 0.5] - Modulation of lambda by OCI (can increase or decrease).
    """
    learning_rate, beta, w, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective lambda, clamped between 0 and 1
    # We allow oci_lambda_mod to be negative or positive relative to base
    # Since bounds are usually [0,1], we map the param range carefully or use a transform
    # Here we assume the optimizer provides a value that we interpret as a shift
    # To keep within bounds [0,1], we use a sigmoid or simple clamping.
    # Let's use simple clamping.
    
    # Note: Since the optimizer usually gives positive values, we interpret 
    # oci_lambda_mod as a magnitude. To allow directionality without negative params,
    # we can say: lambda = base * (1 +/- oci * mod). 
    # Let's assume high OCI *reduces* credit assignment (disrupted continuity).
    
    effective_lambda = lambda_base * (1.0 - (oci_lambda_mod * oci_score))
    effective_lambda = np.clip(effective_lambda, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning Updates with Eligibility Trace
        
        # 1. Prediction error at stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # 2. Prediction error at stage 1
        # In TD(lambda), the stage 1 value is updated by both its own immediate error (usually 0 here)
        # plus the discounted error from stage 2.
        # Simple view: Q(s1,a1) updates towards Q(s2,a2) (TD(0)) OR towards r (Monte Carlo / TD(1))
        # Delta_1 = Q(s2,a2) - Q(s1,a1)
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1
        # The update combines the immediate transition error and the stage 2 error weighted by lambda
        # Update = alpha * (delta_stage1 + lambda * delta_stage2)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + effective_lambda * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```