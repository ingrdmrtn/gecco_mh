Here are three new cognitive models based on the participant's data and OCI score.

### Model 1: Hybrid Model with OCI-Modulated Loss-Perseveration
This model posits that high OCI scores specifically increase "loss-chasing" or the inability to switch away from a choice after a lack of reward. While general stickiness explains repetition, this model hypothesizes that OCI drives a compulsive repetition specifically after failure (reward = 0), preventing the usual "win-stay, lose-switch" strategy.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated 'Loss-Perseveration'.
    
    Hypothesis: High OCI leads to compulsive repetition of the previous choice 
    specifically when that choice resulted in NO reward (Loss-Stay), 
    overriding the Model-Based plan to switch.

    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control (0=MF, 1=MB).
    - stick_base: [0, 5] General choice stickiness.
    - stick_loss_oci_add: [0, 5] Additional stickiness applied only after a loss, scaled by OCI.
    """
    learning_rate, beta, w, stick_base, stick_loss_oci_add = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Transition matrix (fixed structure for hybrid model)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if prev_a1 != -1:
            # Base stickiness applies always
            stick_val = stick_base
            # OCI-driven stickiness applies only if previous trial was unrewarded (loss)
            if prev_reward == 0:
                stick_val += (participant_oci * stick_loss_oci_add)
            
            q_net[prev_a1] += stick_val
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # TD Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # TD Update Stage 1 (MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Beta Hybrid Model
This model hypothesizes that OCI affects the precision (inverse temperature) of the high-level planning stage (Stage 1) differently than the local harvesting stage (Stage 2). High OCI participants might exhibit rigid, deterministic behavior in their spaceship choice (high Beta 1) due to obsessive adherence to a routine, while their alien choice (Stage 2) remains flexible or noisy.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with separate, OCI-modulated Stage 1 precision.
    
    Hypothesis: High OCI increases rigidity (inverse temperature) specifically 
    in the first-stage choice (spaceship), reflecting a compulsive need for 
    certainty in the 'plan', while stage 2 behavior remains distinct.

    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - w: [0, 1] Weight of Model-Based control.
    - beta_stage2: [0, 10] Inverse temperature for Stage 2 (Aliens).
    - beta_stage1_base: [0, 10] Base inverse temperature for Stage 1.
    - beta_stage1_oci_sens: [0, 5] Increase in Stage 1 Beta per unit of OCI.
    """
    learning_rate, w, beta_stage2, beta_stage1_base, beta_stage1_oci_sens = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective Stage 1 Beta
    beta_stage1 = beta_stage1_base + (participant_oci * beta_stage1_oci_sens)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use OCI-modulated Beta for Stage 1
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        # Use standard Beta for Stage 2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dynamic Transition Learning Modulated by OCI
Standard models assume participants know the fixed 70/30 transition probabilities. This model proposes that participants *learn* these transitions over time, and OCI modulates the rate of this learning. High OCI might be associated with "hyper-plasticity" (updating beliefs about the world structure too fast, seeing patterns in noise) or "rigidity" (updating too slow).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where the Model-Based controller learns the transition matrix,
    with the learning rate of the world-model modulated by OCI.
    
    Hypothesis: OCI affects how quickly the participant updates their belief about
    spaceship-planet transitions. 

    Parameters:
    - lr_value: [0, 1] Learning rate for reward values (Q-learning).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    - lr_trans_oci_mod: [-1, 1] Modulation of transition learning rate by OCI.
    """
    lr_value, beta, w, lr_trans_base, lr_trans_oci_mod = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective transition learning rate, bounded [0, 1]
    lr_trans = lr_trans_base + (participant_oci * lr_trans_oci_mod)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition belief (start with uniform uncertainty)
    # T[0] = Prob(Planet 0 | Spaceship 0), T[1] = Prob(Planet 0 | Spaceship 1)
    # We assume symmetry: P(Y|A) = 1 - P(X|A)
    trans_probs = np.array([0.5, 0.5]) 
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Construct full 2x2 transition matrix from current beliefs
        # Row 0: Spaceship 0 -> [P(X), P(Y)]
        # Row 1: Spaceship 1 -> [P(X), P(Y)]
        # Note: In task, Planet 0 is X, Planet 1 is Y.
        T_matrix = np.array([
            [trans_probs[0], 1.0 - trans_probs[0]],
            [trans_probs[1], 1.0 - trans_probs[1]]
        ])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # 1. Update Transition Beliefs (Model-Based Learning)
        # If a1=0 and we went to s_idx=0 (Planet X), we update trans_probs[0] towards 1.
        # If a1=0 and we went to s_idx=1 (Planet Y), we update trans_probs[0] towards 0.
        if a1 == 0:
            outcome = 1.0 if s_idx == 0 else 0.0
            trans_probs[0] += lr_trans * (outcome - trans_probs[0])
        elif a1 == 1:
            outcome = 1.0 if s_idx == 0 else 0.0
            trans_probs[1] += lr_trans * (outcome - trans_probs[1])

        # 2. Update Values (Model-Free Learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_value * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_value * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```