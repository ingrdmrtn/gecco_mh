Here are the 3 proposed cognitive models.

### Model 1: OCI-Modulated Mixing Weight and Stickiness
This model hypothesizes that obsessive-compulsive symptoms (OCI) primarily drive the balance between goal-directed and habitual control (Mixing Weight `w`) and the tendency to repeat choices regardless of value (Stickiness). High OCI is expected to increase habit reliance (lower `w`) and increase perseveration (higher `stickiness`).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Mixing Weight and Stickiness.
    
    Hypothesis: 
    OCI modulates the trade-off between Model-Based and Model-Free control (w)
    and the tendency to perseverate (stickiness).
    
    Parameters:
    learning_rate: [0, 1] - General learning rate for value updates.
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    lambda_eligibility: [0, 1] - Eligibility trace for Stage 1 updates.
    w_base: [0, 1] - Baseline mixing weight (1=MB, 0=MF).
    w_oci: [-1, 1] - Effect of OCI on mixing weight.
    stick_base: [-5, 5] - Baseline choice stickiness.
    stick_oci: [-5, 5] - Effect of OCI on stickiness.
    """
    learning_rate, beta, lambda_eligibility, w_base, w_oci, stick_base, stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate modulated parameters
    w = w_base + w_oci * oci_score
    w = np.clip(w, 0.0, 1.0)
    
    stickiness = stick_base + stick_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize values
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing data
        if a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 MF values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        # Prediction errors
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 MF (TD(lambda))
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)
        
        # Update Stage 2 MF
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Learning Rate and Beta
This model proposes that OCI influences the general "plasticity" (Learning Rate) and "precision" (Beta) of the decision process. Compulsive individuals might show altered sensitivity to reward history (Learning Rate) and different levels of choice determinism (Beta), while stickiness remains a fixed trait.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Learning Rate and Beta.
    
    Hypothesis:
    OCI affects the speed of learning (learning_rate) and the deterministic 
    nature of choices (beta), representing a shift in cognitive flexibility 
    and sensitivity.
    
    Parameters:
    w: [0, 1] - Mixing weight (MB vs MF).
    stickiness: [-5, 5] - Choice perseveration.
    lambda_eligibility: [0, 1] - Eligibility trace.
    lr_base: [0, 1] - Base learning rate.
    lr_oci: [-1, 1] - Modulation of learning rate by OCI.
    beta_base: [0, 10] - Base inverse temperature.
    beta_oci: [-5, 5] - Modulation of beta by OCI.
    """
    w, stickiness, lambda_eligibility, lr_base, lr_oci, beta_base, beta_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate modulated parameters
    learning_rate = lr_base + lr_oci * oci_score
    learning_rate = np.clip(learning_rate, 0.0, 1.0)
    
    beta = beta_base + beta_oci * oci_score
    beta = np.clip(beta, 0.0, 20.0) # Allow slightly higher bound for high OCI rigidity
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_eligibility * delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Positive Learning Rate (Asymmetric Learning)
This model tests the hypothesis that OCI specifically alters how individuals learn from positive outcomes (rewards), potentially leading to faster habit formation or "chasing" behavior. It uses separate learning rates for positive and negative prediction errors, with OCI modulating the positive rate.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Positive Learning Rate (Asymmetric Learning).
    
    Hypothesis:
    OCI specifically impacts learning from positive prediction errors (rewards),
    leading to asymmetric updating of values. 
    
    Parameters:
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight.
    stickiness: [-5, 5] - Choice perseveration.
    lambda_eligibility: [0, 1] - Eligibility trace.
    lr_neg: [0, 1] - Learning rate for negative prediction errors.
    lr_pos_base: [0, 1] - Base learning rate for positive prediction errors.
    lr_pos_oci: [-1, 1] - Modulation of positive learning rate by OCI.
    """
    beta, w, stickiness, lambda_eligibility, lr_neg, lr_pos_base, lr_pos_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate modulated positive learning rate
    lr_pos = lr_pos_base + lr_pos_oci * oci_score
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a2 == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates with Asymmetric Learning Rate ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Combined error for Stage 1 (TD(lambda))
        total_delta_1 = delta_stage1 + lambda_eligibility * delta_stage2
        
        # Apply asymmetric learning rate for Stage 1
        lr_1 = lr_pos if total_delta_1 > 0 else lr_neg
        q_stage1_mf[a1] += lr_1 * total_delta_1
        
        # Apply asymmetric learning rate for Stage 2
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```