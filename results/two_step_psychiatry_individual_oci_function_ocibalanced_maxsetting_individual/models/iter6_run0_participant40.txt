Here are three new cognitive models based on the participant's data and OCI score.

### Model 1: Hybrid Learner with OCI-Modulated Rigidity
**Hypothesis:** High OCI scores are associated with cognitive rigidity. In this model, OCI acts as a dampener on the learning rate. Participants with high OCI update their value estimates much more slowly than low OCI participants, leading to the observed "streaks" of behavior where they stick to a choice despite recent outcomes. This model uses a hybrid Model-Based/Model-Free architecture.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where learning rate is dampened by OCI (Rigidity).
    
    Hypothesis: High OCI leads to lower learning rates (slower updating),
    explaining the long streaks of behavior.
    
    Bounds:
    alpha_base: [0,1]
    beta: [0,10]
    w: [0,1]
    oci_rigidity: [0,1]
    """
    alpha_base, beta, w, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate: High OCI -> Lower Alpha
    # We clip the reduction factor to ensure alpha doesn't go negative.
    learning_rate = alpha_base * (1.0 - min(1.0, oci_rigidity * oci_score))
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    # Fixed transition matrix for MB
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Choice ---
        s_idx = state[t]
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Updates ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Stage 2 Update (MF)
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * pe_2
        
        # Stage 1 Update (MF) - TD(0)
        pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with OCI-Driven Stickiness
**Hypothesis:** This model posits that high OCI scores drive a specific compulsion to repeat the previous action ("stickiness"), distinct from reward-based learning. While the participant uses a standard Hybrid (MB/MF) strategy to evaluate value, an additional OCI-weighted "stickiness" bonus is added to the previously chosen option, biasing them heavily towards repetition regardless of the calculated value.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated choice stickiness.
    
    Hypothesis: OCI acts as a driver for perseveration (stickiness). 
    This is added to a standard Hybrid agent.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    stickiness_oci_sens: [0,5]
    """
    learning_rate, beta, w, stickiness_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    # Stickiness bonus scales with OCI
    stickiness_bonus = stickiness_oci_sens * oci_score

    for t in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the logits (not the Q-values directly)
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness_bonus
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        last_action_1 = action_1[t]
        
        # --- Stage 2 Choice ---
        s_idx = state[t]
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Updates ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * pe_2
        
        pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with OCI-Modulated Loss Insensitivity
**Hypothesis:** High OCI participants may exhibit "loss chasing" or stubbornness, effectively ignoring negative feedback. This model implements asymmetric learning rates where the learning rate for negative prediction errors (losses) is suppressed by the OCI score. A high OCI score results in a near-zero learning rate for losses, causing the participant to maintain high value estimates for an option even when it yields no coins, explaining the long streaks of unrewarded choices.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated insensitivity to losses.
    
    Hypothesis: High OCI reduces learning from negative prediction errors (losses),
    causing participants to stick with choices even when they don't pay out.
    
    Bounds:
    alpha_pos: [0,1]
    beta: [0,10]
    w: [0,1]
    loss_insensitivity_oci: [0,1]
    """
    alpha_pos, beta, w, loss_insensitivity_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate
    # If loss_insensitivity_oci is high and OCI is high, alpha_neg becomes very small.
    alpha_neg = alpha_pos * (1.0 - (loss_insensitivity_oci * oci_score))
    alpha_neg = max(0.0, alpha_neg) # Ensure non-negative
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Choice ---
        s_idx = state[t]
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Updates with Asymmetric Learning Rates ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Stage 2 Update
        pe_2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if pe_2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * pe_2
        
        # Stage 1 Update
        pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if pe_1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```