To explain the behavior of the participant with high OCI symptoms, who exhibits patterns of repetition and specific responses to reward/punishment, I propose three cognitive models. These models introduce mechanisms related to habit formation, heuristic usage, and risk aversion, which are theoretically linked to obsessive-compulsive traits (e.g., intolerance of uncertainty, reliance on rigid habits).

### Model 1: Frequency-Dependent Habit Model
This model hypothesizes that OCI symptoms correlate with a reliance on "deep" habits formed by the frequency of past choices, rather than just the most recent choice (simple stickiness). It maintains a decaying trace of past choices. High OCI individuals are posited to have a stronger weight on this frequency-based habit.

### Model 2: Heuristic Mixture Model (WSLS)
This model posits that the participant arbitrates between a complex reinforcement learning strategy (MB/MF) and a simple, rigid "Win-Stay, Lose-Shift" (WSLS) heuristic. OCI symptoms modulate the weight ($\epsilon$) given to the rigid heuristic, reflecting a compulsion to follow simple rules to reduce cognitive load or anxiety.

### Model 3: Uncertainty Intolerance (Variance Penalty) Model
This model incorporates "Intolerance of Uncertainty," a core component of OCI. The agent penalizes options based on the variance of their expected rewards. Since outcomes are binary, variance is highest when the probability of reward is 0.5. High OCI leads to a stronger penalty for uncertain outcomes, driving the agent towards options that are either consistently good or consistently bad (predictable).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF Hybrid with OCI-Modulated Frequency-Dependent Habit.
    
    Unlike simple 1-back stickiness, this model maintains an exponentially decaying 
    trace of past choices (frequency). The influence of this trace on the decision 
    is modulated by OCI, capturing 'deep' habit formation.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weight of Model-Based system (0=MF, 1=MB).
    elig_lambda: [0,1] - Eligibility trace for Stage 1 MF update.
    trace_decay: [0,1] - Decay rate of the choice frequency trace (0=instant decay, 1=no decay).
    trace_w_oci: [0,10] - Scaling factor for the habit weight based on OCI. Weight = trace_w_oci * OCI.
    """
    learning_rate, beta, w, elig_lambda, trace_decay, trace_w_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition matrix (fixed as per task structure)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    # Frequency trace for Stage 1 choices
    choice_trace = np.zeros(2)
    
    # Habit weight depends on OCI
    w_habit = trace_w_oci * oci_score

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # --- Stage 1 Decision ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value: Weighted sum of MB, MF, and Habit Trace
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + w_habit * choice_trace
        
        # Softmax
        exp_q1 = np.exp(beta * q_net_1)
        if np.any(np.isinf(exp_q1)):
            probs_1 = np.zeros_like(exp_q1)
            probs_1[np.argmax(q_net_1)] = 1.0
        else:
            probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        q_net_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * q_net_2)
        if np.any(np.isinf(exp_q2)):
            probs_2 = np.zeros_like(exp_q2)
            probs_2[np.argmax(q_net_2)] = 1.0
        else:
            probs_2 = exp_q2 / np.sum(exp_q2)
            
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # 1. Update Choice Trace (Frequency Habit)
        # Decay all traces
        choice_trace *= trace_decay
        # Increment chosen trace
        choice_trace[a1] += 1.0
        
        # 2. Q-Learning Updates
        # Stage 1 PE (using Stage 2 Q-value)
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        # Stage 2 PE (using Reward)
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        # Eligibility Trace: Stage 2 RPE updates Stage 1 MF value
        q_stage1_mf[a1] += learning_rate * elig_lambda * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Mixture of MB/MF Hybrid and Win-Stay/Lose-Shift (WSLS) Heuristic.
    
    The agent's policy is a mixture of a reinforcement learning policy (Softmax) 
    and a deterministic WSLS heuristic. The reliance on the heuristic (epsilon)
    is modulated by OCI, reflecting a compulsion to follow rigid rules.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1] - MB/MF weight.
    elig_lambda: [0,1]
    epsilon_base: [0,1] - Base weight of the heuristic.
    epsilon_oci: [0,1] - OCI modulation of heuristic weight.
    """
    learning_rate, beta, w, elig_lambda, epsilon_base, epsilon_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Mixture weight for heuristic
    epsilon = epsilon_base + epsilon_oci * oci_score
    epsilon = np.clip(epsilon, 0.0, 1.0)
    
    last_a1 = -1
    last_r = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # --- Stage 1 Policy (RL Component) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        if np.any(np.isinf(exp_q1)):
            probs_rl = np.zeros_like(exp_q1)
            probs_rl[np.argmax(q_net_1)] = 1.0
        else:
            probs_rl = exp_q1 / np.sum(exp_q1)
            
        # --- Stage 1 Policy (Heuristic Component: WSLS) ---
        probs_wsls = np.array([0.5, 0.5]) # Default for first trial
        if last_a1 != -1:
            probs_wsls = np.zeros(2)
            if last_r >= 1.0: # Win -> Stay
                probs_wsls[last_a1] = 1.0
            else: # Lose -> Shift
                probs_wsls[1 - last_a1] = 1.0
        
        # Mixture
        probs_1 = (1 - epsilon) * probs_rl + epsilon * probs_wsls
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy (Pure RL) ---
        q_net_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * q_net_2)
        if np.any(np.isinf(exp_q2)):
            probs_2 = np.zeros_like(exp_q2)
            probs_2[np.argmax(q_net_2)] = 1.0
        else:
            probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        q_stage1_mf[a1] += learning_rate * elig_lambda * delta_2
        
        last_a1 = a1
        last_r = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF Hybrid with OCI-Modulated Uncertainty Intolerance (Variance Penalty).
    
    OCI is associated with intolerance of uncertainty. This model penalizes options
    that have high outcome variance (uncertainty). For binary rewards, variance is 
    V * (1-V). High OCI individuals will avoid options with probabilities near 0.5.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    elig_lambda: [0,1]
    var_penalty_oci: [0,10] - Strength of the penalty for variance, scaled by OCI.
    """
    learning_rate, beta, w, elig_lambda, var_penalty_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    penalty_strength = var_penalty_oci * oci_score
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # Calculate Variance Penalty for Stage 2 Options
        # Assuming Q-values approximate the probability of reward (0 to 1)
        # We clip Q to [0,1] for variance calculation to handle initial/noisy values
        q_probs = np.clip(q_stage2_mf, 0.0, 1.0)
        variance = q_probs * (1.0 - q_probs)
        
        # Effective Q-values for Stage 2 (Value - Penalty)
        q_stage2_effective = q_stage2_mf - (penalty_strength * variance)
        
        # --- Stage 1 Decision ---
        # MB uses the effective (penalized) Stage 2 values
        max_q_stage2 = np.max(q_stage2_effective, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # MF values are also penalized implicitly if we wanted, but here we keep MF raw 
        # or we could penalize Stage 1 MF variance. Let's penalize Stage 1 MF variance too.
        q1_probs = np.clip(q_stage1_mf, 0.0, 1.0)
        var_1 = q1_probs * (1.0 - q1_probs)
        q_stage1_mf_effective = q_stage1_mf - (penalty_strength * var_1)
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf_effective
        
        exp_q1 = np.exp(beta * q_net_1)
        if np.any(np.isinf(exp_q1)):
            probs_1 = np.zeros_like(exp_q1)
            probs_1[np.argmax(q_net_1)] = 1.0
        else:
            probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        q_net_2 = q_stage2_effective[s_idx]
        exp_q2 = np.exp(beta * q_net_2)
        if np.any(np.isinf(exp_q2)):
            probs_2 = np.zeros_like(exp_q2)
            probs_2[np.argmax(q_net_2)] = 1.0
        else:
            probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Standard Q-learning updates on the RAW Q-values (not penalized ones)
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
        
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        q_stage1_mf[a1] += learning_rate * elig_lambda * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```