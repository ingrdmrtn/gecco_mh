Here are three new cognitive models inspired by the participant's high OCI score (1.0) and the observed repetitive behavior. These models explore different mechanisms for how obsessive-compulsive traits might influence reinforcement learning, specifically focusing on mixing model-based/model-free strategies, learning rate asymmetries, and uncertainty-driven exploration.

### Cognitive Model 1: OCI-Modulated Model-Based/Model-Free Hybrid
This model tests the hypothesis that high OCI scores correlate with a rigid reliance on habitual (Model-Free) control over goal-directed (Model-Based) control. The mixing parameter `w` determines the balance between these two systems. Here, the `w` parameter is directly modulated by the OCI score, where higher OCI pushes the agent towards Model-Free (habitual) behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where the weighting (w) is modulated by OCI.
    
    Hypothesis: Higher OCI scores lead to a dominance of the Model-Free (habitual) system 
    over the Model-Based (goal-directed) system.
    w = w_base * (1 - oci_impact * OCI)
    If w is closer to 1, behavior is Model-Based. If w is closer to 0, behavior is Model-Free.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline mixing weight for Model-Based system (before OCI modulation).
    oci_impact: [0, 1] How strongly OCI reduces the Model-Based weight.
    """
    learning_rate, beta, w_base, oci_impact = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the mixing weight w based on OCI
    # High OCI reduces w, making the agent more Model-Free
    w = w_base * (1.0 - (oci_impact * oci_score))
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-Free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for stage 2 (same for MF and MB)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update (TD)
        # delta_2 = r - Q(s2, a2)
        delta_stage2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD - Model Free)
        # delta_1 = Q(s2, a2) - Q(s1, a1)  <-- SARSA-style update often used in 2-step
        delta_stage1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Asymmetric Learning Rates with OCI-Driven Punishment Sensitivity
This model posits that individuals with high OCI scores are hypersensitive to negative outcomes (or lack of reward), leading to "unlearning" behaviors that might manifest as switching or checking. However, since the participant shows stickiness, this model tests if they have a specific *asymmetry*: they learn from positive feedback normally, but their learning from negative feedback (0 reward) is amplified by their OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative prediction errors.
    The learning rate for negative prediction errors (punishment/omission) is scaled by OCI.
    
    Hypothesis: High OCI participants have an altered sensitivity to negative outcomes 
    (learning_rate_neg), potentially driving different update dynamics compared to rewards.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (R > Q).
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors (R < Q).
    beta: [0, 10] Inverse temperature.
    oci_sens: [0, 5] Multiplier for OCI to increase the negative learning rate.
    """
    lr_pos, lr_neg_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # High OCI increases sensitivity to negative prediction errors
    lr_neg = lr_neg_base * (1.0 + oci_sens * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 PE
        pe2 = r - q_stage2[s_curr, a2]
        if pe2 >= 0:
            q_stage2[s_curr, a2] += lr_pos * pe2
        else:
            q_stage2[s_curr, a2] += lr_neg * pe2
            
        # Stage 1 PE (TD(0))
        # Using the value of the chosen stage 2 state-action as the target
        pe1 = q_stage2[s_curr, a2] - q_stage1[a1]
        if pe1 >= 0:
            q_stage1[a1] += lr_pos * pe1
        else:
            q_stage1[a1] += lr_neg * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Uncertainty-Avoidance with OCI Modulation
This model suggests that high OCI individuals are motivated to reduce uncertainty. Instead of just maximizing reward, the agent also assigns value to options that have been chosen recently (and are thus "known"), penalizing uncertainty. The "uncertainty penalty" is added to the Q-values and is scaled by the OCI score. This mechanism can produce repetitive "checking" behavior because the agent prefers the known option over the unknown, even if the reward is slightly lower.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner that includes an uncertainty penalty (or familiarity bonus) 
    modulated by OCI.
    
    Hypothesis: High OCI participants are averse to uncertainty. They add a bonus 
    to options they have selected recently (reducing uncertainty) or penalize 
    options they haven't seen in a while. 
    Here, we implement a 'trace' of recent usage. High trace = low uncertainty.
    
    Q_net = Q_MF + (uncertainty_weight * OCI * Trace)
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta: [0, 10] Inverse temperature.
    trace_decay: [0, 1] Decay rate of the familiarity trace (0=instant forgetting, 1=no decay).
    uncert_w: [0, 5] Weight of the familiarity/uncertainty term.
    """
    learning_rate, beta, trace_decay, uncert_w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Trace tracks how recently an action was taken. 
    # Initialize at 0. Updates: trace = trace * decay + 1 (if chosen)
    trace_stage1 = np.zeros(2) 

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Augment Q-values with the familiarity trace scaled by OCI
        # If OCI is high, the agent prefers actions with high trace (familiar actions)
        familiarity_bonus = uncert_w * oci_score * trace_stage1
        
        logits_1 = beta * (q_stage1 + familiarity_bonus)
        # Numerical stability
        logits_1 = logits_1 - np.max(logits_1)
        
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update traces
        trace_stage1 *= trace_decay
        trace_stage1[a1] += 1.0
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Standard Q-learning updates
        delta2 = r - q_stage2[s_curr, a2]
        q_stage2[s_curr, a2] += learning_rate * delta2
        
        delta1 = q_stage2[s_curr, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```