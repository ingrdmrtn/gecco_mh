Here are three new cognitive models that explore different mechanisms by which high OCI scores might influence decision-making in this two-step task. These models focus on altered learning rates for negative outcomes, a bias towards "safe" or habitual choices (perseveration), and a modulation of the model-based weight.

### Cognitive Model 1: OCI-Modulated Punishment Sensitivity
This model hypothesizes that individuals with high OCI symptoms are hypersensitive to negative outcomes (punishment). Instead of a single learning rate, this model splits the learning rate into positive and negative components. The negative learning rate is scaled by the OCI score, implying that high OCI participants update their values more drastically after a loss (0 coins) than low OCI participants.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Punishment Sensitivity.
    Hypothesis: High OCI participants have an increased sensitivity to negative prediction errors
    (losses), leading to faster updating when outcomes are worse than expected.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based vs Model-free weight.
    - oci_sens: [0, 5] Multiplier for OCI impact on negative learning rate.
    """
    alpha_pos, alpha_neg_base, beta, w, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective negative learning rate scales with OCI
    # We clip to ensure it stays within valid bounds [0, 1]
    alpha_neg = alpha_neg_base + (oci_sens * oci_score)
    if alpha_neg > 1.0: alpha_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        lr_s2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[state_idx, act2] += lr_s2 * delta_stage2
        
        # Update Stage 1
        # Note: Standard TD updates stage 1 based on stage 2 value, not reward directly
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        lr_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[act1] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Choice Perseveration
This model posits that high OCI scores correlate with rigid, repetitive behaviors (perseveration or "stickiness"). Unlike the previous "Win-Stay" model which depended on reward, this model implements a raw repetition bias regardless of the outcome. The strength of this stickiness parameter is directly modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Choice Perseveration (Stickiness).
    Hypothesis: High OCI participants exhibit "sticky" behavior, repeating previous choices
    regardless of reward, due to compulsive repetition tendencies.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based vs Model-free weight.
    - stick_base: [0, 5] Base level of choice stickiness.
    - stick_oci_factor: [0, 5] Additional stickiness scaled by OCI score.
    """
    learning_rate, beta, w, stick_base, stick_oci_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate total stickiness
    total_stickiness = stick_base + (stick_oci_factor * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if prev_choice_1 != -1:
            q_net[prev_choice_1] += total_stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        prev_choice_1 = act1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Dependent Model-Based Suppression
This model investigates the trade-off between goal-directed (Model-Based) and habitual (Model-Free) control. It hypothesizes that high OCI serves as a cognitive load or a driver of habit formation, suppressing the Model-Based weight ($w$). As OCI increases, the agent relies less on the transition structure of the task and more on simple temporal difference learning.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Model-Based Suppression.
    Hypothesis: High OCI scores are associated with a reduction in goal-directed (model-based)
    planning, leading to a suppression of the 'w' parameter.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_max: [0, 1] The maximum theoretical model-based weight (for OCI=0).
    - oci_suppression: [0, 1] Factor by which OCI reduces w.
    """
    learning_rate, beta, w_max, oci_suppression = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w: w decreases as OCI increases.
    # We use a simple linear penalty: w = w_max * (1 - k * OCI)
    # Clipped at 0.
    w_effective = w_max * (1.0 - (oci_suppression * oci_score))
    if w_effective < 0: w_effective = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```