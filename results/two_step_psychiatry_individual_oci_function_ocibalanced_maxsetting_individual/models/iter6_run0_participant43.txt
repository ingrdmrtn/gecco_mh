Here are three cognitive models formulated as Python functions. They are designed to explain the decision-making behavior of the participant, specifically addressing the high OCI score (0.86) and the observed "sticky" or perseverative behavior (e.g., long streaks of choosing the same spaceship despite mixed outcomes).

### Model 1: Hybrid Learner with OCI-Modulated Dampened Negative Learning
**Hypothesis**: High OCI scores are associated with a "resistance to extinction" or compulsion to persist in a behavior despite lack of reward. In this model, the OCI score dampens the learning rate specifically when the outcome is worse than expected (negative prediction error). This causes the participant to maintain high value estimates for a chosen spaceship even after receiving 0 coins, leading to the observed perseveration.

```python
import numpy as np

def cognitive_model1_oci_dampened_negative_learning(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Dampened Negative Learning.
    
    Hypothesis: High OCI scores lead to persistence in choices by reducing the learning 
    from negative prediction errors (outcomes worse than expected). This models the 
    'stickiness' or resistance to switching observed in the participant data.
    
    Update Logic for Stage 1:
    delta = Target - Q(s1, a1)
    if delta < 0:
        alpha_eff = learning_rate * (1 - dampening * oci)
    else:
        alpha_eff = learning_rate
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weight of Model-Based control (1=MB, 0=MF).
    - dampening: [0, 1] Scaling factor; higher values mean OCI strongly reduces learning from failure.
    """
    learning_rate, beta, w, dampening = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1) # Stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 MF Update: Dampen learning if error is negative
        target_stage1 = q_stage2_mf[state_idx, action_2[trial]]
        delta_stage1 = target_stage1 - q_stage1_mf[action_1[trial]]
        
        eff_lr = learning_rate
        if delta_stage1 < 0:
            eff_lr = learning_rate * (1.0 - dampening * oci_score)
            if eff_lr < 0: eff_lr = 0
            
        q_stage1_mf[action_1[trial]] += eff_lr * delta_stage1
        
        # Stage 2 MF Update: Standard Q-learning
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with OCI-Mediated Direct Reinforcement
**Hypothesis**: High OCI is linked to habitual behavior. In a two-step task, "pure habits" can be modeled as direct reinforcement of the first action by the final reward, bypassing the model of the second stage (state-action values). This model posits that OCI controls the mixture of standard TD learning (using Stage 2 values) and direct reinforcement (using Reward) for updating Stage 1 values.

```python
def cognitive_model2_oci_direct_reinforcement(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Learner with OCI-Mediated Direct Reinforcement (Habit).
    
    Hypothesis: High OCI scores correlate with 'flat' model-free learning where Stage 1 choices 
    are reinforced directly by the outcome (Reward), ignoring the intermediate state structure.
    The update target for Stage 1 is a mix of the Stage 2 value and the direct Reward, 
    weighted by OCI.
    
    Target = (1 - mix) * Q_Stage2 + mix * Reward
    where mix = direct_weight * OCI
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - direct_weight: [0, 1] How strongly OCI promotes direct reinforcement (0=Standard MF, 1=Pure Direct).
    """
    learning_rate, beta, w, direct_weight = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Calculate mix factor based on OCI (clamped to [0,1])
    mix_factor = direct_weight * oci_score
    if mix_factor > 1.0: mix_factor = 1.0
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 MF Update: Mix of TD(0) (Q2) and Direct Reinforcement (Reward)
        val_stage2 = q_stage2_mf[state_idx, action_2[trial]]
        outcome = reward[trial]
        
        target = (1.0 - mix_factor) * val_stage2 + mix_factor * outcome
        
        delta_stage1 = target - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with OCI-Scaled Accumulated Perseveration
**Hypothesis**: Obsessive-compulsive behavior is often characterized by repetition that becomes stronger the more it is performed. Unlike simple 1-step stickiness, this model uses an accumulating "choice trace" to represent the buildup of a compulsion. The influence of this accumulated history on current choice is scaled by the OCI score.

```python
def cognitive_model3_oci_accumulated_perseveration(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Learner with OCI-Scaled Accumulated Perseveration.
    
    Hypothesis: OCI relates to a 'buildup' of compulsion. This model tracks a 
    decaying trace of past choices. The OCI score determines the strength with which 
    this accumulated history biases the current decision, explaining long streaks.
    
    Logits += persev_strength * OCI * ChoiceTrace
    ChoiceTrace(t) = decay * ChoiceTrace(t-1) + I(choice)
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - persev_strength: [0, 5] Weight of the accumulated compulsion trace.
    - decay: [0, 1] Persistence/memory of the compulsion trace.
    """
    learning_rate, beta, w, persev_strength, decay = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 
    
    # Trace of past choices (accumulates)
    choice_trace = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add OCI-scaled accumulated perseveration
        logits_1 = beta * q_net + (persev_strength * oci_score * choice_trace)
        
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Trace
        choice_trace *= decay
        choice_trace[action_1[trial]] += 1.0
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```