Here are the 3 proposed cognitive models.

### Model 1: Dynamic Transition Uncertainty (Structural Instability)
This model hypothesizes that high OCI participants do not perceive the spaceship-planet transition probabilities as fixed (e.g., 0.7/0.3). Instead, they constantly update their internal model of these transitions based on observed outcomes. High OCI is associated with intolerance of uncertainty and "doubting" the structure, modeled here as a higher learning rate for the transition matrix itself. This leads to instability in Model-Based value calculation as they over-react to rare transitions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Transition Uncertainty Model.
    
    Hypothesis: High OCI participants have an unstable internal model of the 
    environment's structure. They update their belief about transition probabilities 
    (Spaceship -> Planet) more rapidly, over-reacting to rare transitions.
    
    Parameters:
    lr_q: [0, 1] Learning rate for reward values (Q-values).
    lr_trans_base: [0, 1] Base learning rate for updating the transition matrix.
    oci_trans_mod: [0, 1] OCI modulation of transition learning rate. 
                          Effective LR_trans = lr_trans_base * (1 + oci * oci_trans_mod).
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Model-Based weight.
    """
    lr_q, lr_trans_base, oci_trans_mod, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Effective transition learning rate
    lr_trans = lr_trans_base * (1.0 + participant_oci * oci_trans_mod)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)

    # Initialize transition matrix beliefs: [P(X|A), P(X|U)]
    # We track P(Planet 0 | Spaceship)
    # Start with the true average or a prior (0.7 for A, 0.3 for U)
    # Rows: Spaceship 0, Spaceship 1. Cols: Planet 0, Planet 1.
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value Calculation using dynamic transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_curr = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # 1. Update Transition Matrix Beliefs
        # If we are in state s_curr (0 or 1) after choosing a1 (0 or 1)
        # Update row a1. Target is 1 for the observed state, 0 for unobserved.
        # trans_probs[a1, s_curr] moves towards 1
        # trans_probs[a1, 1-s_curr] moves towards 0
        trans_probs[a1, s_curr] += lr_trans * (1.0 - trans_probs[a1, s_curr])
        trans_probs[a1, 1-s_curr] = 1.0 - trans_probs[a1, s_curr]
        
        # 2. Update Q-Values (Standard TD)
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr_q * pe_2
        
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_q * pe_1 # Standard SARSA-like update for MF
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Conflict-Induced Hesitation (Beta Modulation)
This model proposes that high OCI participants experience decision paralysis when their habitual (Model-Free) and goal-directed (Model-Based) systems provide conflicting information. We model this as a reduction in the inverse temperature parameter `beta` (increasing randomness/hesitation) proportional to the magnitude of the conflict between MB and MF values.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Conflict-Induced Hesitation Model.
    
    Hypothesis: High OCI participants experience anxiety-driven hesitation when 
    Model-Based and Model-Free systems disagree. This is modeled as a reduction 
    in beta (inverse temperature) proportional to the conflict between systems.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature.
    w: [0, 1] Model-Based weight.
    stickiness: [0, 5] Choice stickiness.
    oci_conflict_scale: [0, 1] Scaling factor for conflict-based beta reduction.
                               beta = beta_base / (1 + oci * scale * conflict).
    """
    lr, beta_base, w, stickiness, oci_conflict_scale = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Conflict: Absolute difference in relative preferences
        # Preference(A vs U) for MB vs MF
        diff_mb = q_stage1_mb[0] - q_stage1_mb[1]
        diff_mf = q_stage1_mf[0] - q_stage1_mf[1]
        conflict = np.abs(diff_mb - diff_mf)
        
        # Modulate Beta based on Conflict and OCI
        # High conflict + High OCI -> Lower Beta (more noise/hesitation)
        beta_eff = beta_base / (1.0 + participant_oci * oci_conflict_scale * conflict)
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_curr = int(state[trial])
        
        # Stage 2 uses base beta (conflict is a planning problem, not reaction)
        exp_q2 = np.exp(beta_base * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * pe_2
        
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe_1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hypersensitive Stage 2 Learning (Immediate Feedback Focus)
This model hypothesizes that high OCI participants are hyper-focused on the immediate outcome of the trial (the alien reward) rather than the abstract spaceship choice. Consequently, their learning rate for the second stage (aliens) is boosted relative to the first stage, leading to a "greedy" value estimation that might fluctuate rapidly.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hypersensitive Stage 2 Learning Model.
    
    Hypothesis: High OCI participants are hypersensitive to the immediate feedback 
    (alien reward) in the second stage. Their learning rate for Stage 2 is 
    boosted by their OCI score, while Stage 1 learning remains baseline.
    
    Parameters:
    lr_s1: [0, 1] Learning rate for Stage 1 (Spaceships).
    lr_s2_base: [0, 1] Base learning rate for Stage 2 (Aliens).
    oci_s2_boost: [0, 1] OCI boost factor for Stage 2 learning rate.
                         LR_s2 = lr_s2_base * (1 + oci * boost).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    """
    lr_s1, lr_s2_base, oci_s2_boost, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate specific Stage 2 learning rate
    lr_s2 = lr_s2_base * (1.0 + participant_oci * oci_s2_boost)
    lr_s2 = np.clip(lr_s2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_curr = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Update Stage 2 with boosted rate
        pe_2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr_s2 * pe_2
        
        # Update Stage 1 with baseline rate (using the updated S2 value as target usually, 
        # or the value before update? Standard SARSA uses value before or reward. 
        # We use TD(0) style: r + gamma*max_q(s') - Q(s). Here effectively Q(s2,a2) is the value of state s2)
        # We use the value *before* update for the TD error to be consistent with standard TD(0)
        # But for the target, we use the value of the state we landed in.
        # Let's use the standard TD error calculated from the value *before* the S2 update to avoid leakage,
        # but note that the S2 value itself changes faster across trials.
        
        # Re-calculate value of S2 state? No, standard is Q(s2, a2) represents V(s2).
        # We use the Q-value that was just used for prediction error.
        val_s2 = q_stage2_mf[s_curr, a2] - (lr_s2 * pe_2) # Back to pre-update value for consistency?
        # Actually, let's use the standard update: Q1 += alpha * (Q2 - Q1).
        # We use the current estimate of Q2.
        
        pe_1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```