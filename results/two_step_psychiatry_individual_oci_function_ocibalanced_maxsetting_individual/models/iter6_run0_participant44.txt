Here are 3 new cognitive models based on the participant's profile and data.

### Model 1: OCI-Distorted Transition Beliefs
This model posits that high OCI scores correlate with "doubting" the stability of the environment structure. While the true transition probability is 0.7, the participant's internal model (used for Model-Based planning) is distorted by their OCI score, effectively flattening the transition matrix (making it closer to 0.5/0.5). This represents an anxiety-driven uncertainty about the causal structure of the task, reducing the effectiveness of Model-Based control without changing the weighting parameter $w$ itself.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Distorted Transition Beliefs.
    
    Hypothesis: OCI symptoms introduce uncertainty into the internal model of the 
    task structure. High OCI reduces the subjective probability of the common 
    transition in the Model-Based calculation, effectively 'flattening' the 
    transition matrix towards chance (0.5), representing doubt.

    Bounds:
    lr: [0, 1] Learning rate for Model-Free updates.
    beta: [0, 10] Inverse temperature (softmax).
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    distortion: [0, 1] How much OCI reduces the subjective common transition prob.
                       (0 = no distortion, 1 = max distortion towards 0.5).
    """
    lr, beta, w, distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # True transition matrix for the task
    # (Used implicitly by the environment, but here we define the Subjective one)
    # Base common probability is 0.7.
    # Max distortion reduces 0.7 towards 0.5. Range of reduction is 0.2.
    subjective_p = 0.7 - (oci_score * distortion * 0.2)
    
    # Ensure bounds [0.5, 0.7]
    if subjective_p < 0.5: subjective_p = 0.5
    
    # Subjective transition matrix used for MB planning
    # T[0,0] is A->X, T[0,1] is A->Y
    trans_mat_mb = np.array([[subjective_p, 1-subjective_p], 
                             [1-subjective_p, subjective_p]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation using Distorted Matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_mat_mb @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # TD Errors
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update MF values
        q_stage1_mf[a1] += lr * delta_stage1
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Amplified Stage 2 Learning
This model proposes that OCI symptoms lead to a hypersensitivity to immediate, concrete outcomes (Stage 2 rewards) relative to abstract predictors (Stage 1 choices). While standard models use a single learning rate, this model splits the learning rate. The learning rate for the final outcome (Stage 2) is boosted by the OCI score, reflecting a tendency to over-update value estimates based on the most recent concrete feedback (the coin), potentially causing volatility or "chasing" behavior.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Amplified Stage 2 Learning.
    
    Hypothesis: High OCI individuals are hypersensitive to immediate, concrete 
    rewards (Stage 2) compared to abstract state transitions (Stage 1). 
    This model boosts the learning rate specifically for the second stage 
    update based on the OCI score.

    Bounds:
    lr_base: [0, 1] Base learning rate (applied to Stage 1).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    oci_s2_boost: [0, 5] Multiplier for OCI to boost Stage 2 learning rate.
    """
    lr_base, beta, w, oci_s2_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 specific learning rate
    # LR2 = LR1 * (1 + OCI * Boost)
    lr_stage2 = lr_base * (1.0 + oci_score * oci_s2_boost)
    if lr_stage2 > 1.0: lr_stage2 = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 with base LR
        q_stage1_mf[a1] += lr_base * delta_stage1
        
        # Update Stage 2 with OCI-boosted LR
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Stage 2 Perseveration
Previous models explored stickiness (repetition) on the first-stage choice (Spaceship). This model hypothesizes that OCI-driven ritualistic behavior manifests at the level of the *concrete action* (choosing the alien) regardless of the spaceship or planet. It adds a "stickiness" bonus to the Stage 2 Q-values for the previously chosen alien, with the magnitude of this bonus scaled by the OCI score. This captures the tendency to compulsively ask the same alien for gold.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Stage 2 Perseveration.
    
    Hypothesis: OCI symptoms drive compulsive repetition of concrete actions.
    This model adds a 'stickiness' bonus to the choice of Alien (Stage 2),
    making the participant more likely to repeat the same alien choice 
    regardless of reward history.

    Bounds:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    stick_s2_base: [0, 5] Base stickiness for Stage 2 choice.
    oci_stick_s2_amp: [0, 5] Amplification of Stage 2 stickiness by OCI.
    """
    lr, beta, w, stick_s2_base, oci_stick_s2_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the last chosen alien (0 or 1). Initialize to -1 (none).
    last_action_2 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        
        # Calculate stickiness bonus
        # If last_action_2 matches the index (0 or 1), add bonus
        stickiness_val = stick_s2_base + (oci_score * oci_stick_s2_amp)
        
        # Create temporary Q-values for decision to include stickiness
        q_stage2_decision = q_stage2_mf[state_idx].copy()
        
        if last_action_2 != -1:
            q_stage2_decision[last_action_2] += stickiness_val

        exp_q2 = np.exp(beta * q_stage2_decision)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update last action
        last_action_2 = a2
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        q_stage1_mf[a1] += lr * delta_stage1
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```