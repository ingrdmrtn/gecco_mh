Here are the 3 cognitive models expressed as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Loss-Stickiness Model.
    
    This model hypothesizes that OCI score correlates with "compulsive" perseveration 
    specifically after unrewarded trials (Loss-Stickiness). While all participants 
    may have a base stickiness, high OCI individuals are expected to have a higher 
    tendency to repeat actions that just led to a loss (chasing).
    
    Stickiness = stick_base + stick_loss_oci * OCI * (1 - prev_reward)
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF Q-values.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stick_base: [0, 10] General perseveration bonus applied to the previously chosen action.
    - stick_loss_oci: [0, 10] Additional stickiness bonus triggered only after a loss, scaled by OCI.
    """
    learning_rate, beta, w, stick_base, stick_loss_oci = model_parameters
    oci_val = oci[0]
    n_trials = len(action_1)
    
    # Fixed transition matrix for MB (Spaceship A->X 0.7, Spaceship U->Y 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = 0 
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            prev_a1 = -1
            prev_reward = 0
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max Q of stage 2 weighted by transition probabilities
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value: Weighted average of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Stickiness Bonus
        stick_bonus = np.zeros(2)
        if prev_a1 != -1:
            bonus = stick_base
            # Add OCI-modulated component if previous trial was a loss
            if prev_reward == 0:
                bonus += stick_loss_oci * oci_val
            stick_bonus[prev_a1] = bonus
            
        # Softmax choice probability
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        # TD Error for Stage 1 (using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD Error for Stage 2 (using Reward)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility Trace: Update Stage 1 MF value again with Stage 2 error (TD(1))
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        prev_a1 = a1
        prev_reward = r

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Dynamic Transition Learning Model.
    
    This model assumes the participant does not use a fixed transition matrix but 
    learns it over time. The rate at which they update their internal model of the 
    environment (Transition Learning Rate) is modulated by OCI.
    
    Hypothesis: High OCI is associated with cognitive rigidity, leading to a reduced 
    ability to update structural knowledge (lower transition learning rate).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for reward-based Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    - lr_trans_oci_impairment: [0, 1] Factor by which OCI reduces transition learning (Rigidity).
    - stickiness: [0, 10] General stickiness.
    """
    learning_rate, beta, w, lr_trans_base, lr_trans_oci_impairment, stickiness = model_parameters
    oci_val = oci[0]
    n_trials = len(action_1)
    
    # Initialize transition matrix with prior knowledge (0.7/0.3)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Calculate effective transition learning rate: Base * (1 - Impairment * OCI)
    # High OCI -> Lower learning rate (Rigidity)
    impairment = lr_trans_oci_impairment * oci_val
    if impairment > 1.0: impairment = 1.0
    lr_trans = lr_trans_base * (1.0 - impairment)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            prev_a1 = -1
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # MB Values using DYNAMIC transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_a1 != -1:
            stick_bonus[prev_a1] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        # --- Structural Learning (Transition Matrix Update) ---
        # Update P(State|Action) towards the observed state
        # Vectorized update ensuring row sums remain 1
        target_dist = np.zeros(2)
        target_dist[s_idx] = 1.0
        transition_matrix[a1] += lr_trans * (target_dist - transition_matrix[a1])
        
        prev_a1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Independent Systems Model.
    
    This model decouples the Model-Based and Model-Free systems, assigning them 
    independent inverse temperatures (strengths) instead of a mixing weight 'w'.
    It hypothesizes that OCI selectively impairs the reliability/strength of the 
    Model-Based system.
    
    beta_mb = beta_mb_base * (1 - oci_mb_impairment * OCI)
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_mf: [0, 10] Strength (inverse temp) of the Model-Free system.
    - beta_mb_base: [0, 10] Base strength of the Model-Based system.
    - oci_mb_impairment: [0, 1] Factor by which OCI reduces the MB beta.
    - stickiness: [0, 10]
    """
    learning_rate, beta_mf, beta_mb_base, oci_mb_impairment, stickiness = model_parameters
    oci_val = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Calculate effective MB beta
    # High OCI reduces beta_mb (less reliance on MB)
    factor = 1.0 - (oci_mb_impairment * oci_val)
    if factor < 0: factor = 0
    beta_mb = beta_mb_base * factor
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            prev_a1 = -1
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # MB Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine logits directly (independent contributions)
        # Logit = beta_mb * Q_mb + beta_mf * Q_mf + stickiness
        logits = beta_mb * q_stage1_mb + beta_mf * q_stage1_mf
        
        if prev_a1 != -1:
            logits[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Driven by MF values. Using beta_mf for consistency/parsimony.
        exp_q2 = np.exp(beta_mf * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        prev_a1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```