Here are three new cognitive models as Python functions, designed to explain the participant's behavior with specific mechanisms modulated by their OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with Stage 2 Stickiness modulated by OCI.
    
    Hypothesis: Obsessive-compulsive traits (OCI) manifest as rigid perseveration 
    specifically in the second stage (alien choice), regardless of reward.
    While Stage 1 has a baseline stickiness, Stage 2 stickiness scales with OCI.
    
    Parameters:
    lr: [0, 1] Learning rate for Stage 2 values.
    beta: [0, 10] Inverse temperature.
    stick_s1: [0, 5] Base stickiness for Stage 1 (Spaceship).
    stick_s2_oci: [0, 10] Scaling factor for OCI-driven stickiness in Stage 2 (Alien).
    """
    lr, beta, stick_s1, stick_s2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values for Stage 2 (Aliens): 2 states (Planets) x 2 actions (Aliens)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last choice made in each state for Stage 2 stickiness
    # Initialize with -1 (no choice made yet)
    last_choice_s2 = np.array([-1, -1]) 
    last_choice_s1 = -1

    # Effective stickiness for stage 2 scales with OCI
    eff_stick_s2 = stick_s2_oci * oci_score

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Model-Based + S1 Stickiness) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add stickiness to Stage 1 logits
        logits_1 = beta * q_stage1_mb
        if last_choice_s1 != -1:
            logits_1[last_choice_s1] += stick_s1
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (Model-Free + OCI-Modulated S2 Stickiness) ---
        logits_2 = beta * q_stage2_mf[s_idx]
        
        # Add stickiness if we have visited this state and chosen an alien before
        if last_choice_s2[s_idx] != -1:
            logits_2[last_choice_s2[s_idx]] += eff_stick_s2
            
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 2 Q-values (Standard Q-learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Update history
        last_choice_s1 = a1
        last_choice_s2[s_idx] = a2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with OCI-modulated Loss Sensitivity.
    
    Hypothesis: High OCI individuals perceive the absence of reward (0 coins) 
    as an active punishment (loss). This negativity bias scales with OCI score
    and affects value estimation in both stages.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    loss_sensitivity: [0, 5] Magnitude of negative reward for 0-coin outcomes, scaled by OCI.
    """
    lr, beta, loss_sensitivity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))
    
    # Effective punishment value for 0 reward
    # If reward is 0, effective reward is -1 * loss_sensitivity * oci
    neg_reward_val = -1.0 * loss_sensitivity * oci_score

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Based) ---
        # Uses the Q-values which are tainted by the loss sensitivity
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta * q_stage1_mb
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Calculate effective reward
        r_eff = r if r == 1.0 else neg_reward_val
        
        delta_stage2 = r_eff - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with OCI-modulated Outcome-Dependent Stickiness (Win-Stay/Lose-Shift).
    
    Hypothesis: OCI modulates the tendency to switch strategies after a loss (Lose-Shift).
    While Win-Stay is constant, high OCI might drive an over-reaction to failure 
    (or conversely, persistence despite failure), represented as a bias against the 
    previously chosen action if unrewarded.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    win_stay: [0, 5] Bonus added to previous choice if rewarded (Stage 1).
    lose_shift_oci: [0, 10] Penalty subtracted from previous choice if unrewarded, scaled by OCI.
    """
    lr, beta, win_stay, lose_shift_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))
    
    last_a1 = -1
    last_reward = -1

    eff_lose_shift = lose_shift_oci * oci_score

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (MB + Outcome-Dependent Stickiness) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta * q_stage1_mb
        
        # Apply Outcome-Dependent Stickiness
        if last_a1 != -1:
            if last_reward == 1:
                logits_1[last_a1] += win_stay
            else:
                # If loss, subtract penalty (equivalent to shifting)
                logits_1[last_a1] -= eff_lose_shift
        
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        last_a1 = a1
        last_reward = r

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```