Here are the three proposed cognitive models.

### Model 1: Surprise-Boosted Learning
```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Surprise Learning.
    
    This model posits that OCI symptoms correlate with a hyper-reactivity to 
    "surprising" or rare events. When a rare transition occurs (e.g., Spaceship A 
    going to Planet Y), the learning rate for both stages is boosted by the OCI score.
    This leads to faster updating (and potentially over-correction) after rare events.

    Parameters:
    lr_base: [0, 1] Baseline learning rate for common transitions.
    lr_rare_oci: [0, 1] OCI-dependent boost to learning rate for rare transitions.
                   Effective LR = lr_base + (lr_rare_oci * OCI) if transition was rare.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Model-based weight (0=MF, 1=MB).
    stick: [0, 5] Choice stickiness/perseverance.
    """
    lr_base, lr_rare_oci, beta, w, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Fixed transition matrix for the Model-Based system
    # Row 0: Action 0 (U) -> [Prob(Planet0), Prob(Planet1)] = [0.7, 0.3]
    # Row 1: Action 1 (A) -> [Prob(Planet0), Prob(Planet1)] = [0.3, 0.7]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # (Planet, Alien)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        # Skip invalid data
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Determine if the transition was rare
        # Common: 0->0 or 1->1. Rare: 0->1 or 1->0.
        is_rare = (a1 != s_idx)
        
        # Calculate dynamic learning rate
        current_lr = lr_base
        if is_rare:
            current_lr += (lr_rare_oci * oci_score)
        
        # Bound LR
        if current_lr > 1.0: current_lr = 1.0
        
        # Update Stage 1 MF values (TD learning)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Update Stage 2 MF values (Reward learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: Win-Stay Stickiness
```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Win-Stay Stickiness.
    
    This model hypothesizes that OCI symptoms increase the tendency to compulsively 
    repeat a successful action ("Win-Stay"). Stickiness is not constant but is 
    boosted by OCI specifically when the previous trial was rewarded.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based weight.
    stick_base: [0, 5] Baseline stickiness (applied regardless of outcome).
    stick_win_oci: [0, 5] OCI-dependent boost to stickiness after a reward.
                   Total Stick = stick_base + (stick_win_oci * OCI * Prev_Reward).
    """
    lr, beta, w, stick_base, stick_win_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0.0
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate dynamic stickiness
        if last_action_1 != -1:
            # Stickiness is boosted if previous trial was rewarded
            stick_val = stick_base + (stick_win_oci * oci_score * last_reward)
            q_net[last_action_1] += stick_val
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        last_action_1 = a1
        last_reward = r if r >= 0 else 0 # Handle potential -1 data by treating as 0

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: Dynamic Transition Learning
```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Dynamic Transition Learning.
    
    This model suggests that OCI patients do not rely on a fixed model of the world 
    (70/30 probabilities) but actively update their internal transition matrix based 
    on observed transitions. The rate at which they update this matrix (changing their
    beliefs about which spaceship goes where) is determined by their OCI score.
    High OCI implies a more volatile belief system regarding the environment's structure.
    
    Parameters:
    lr_rew: [0, 1] Learning rate for reward values (Q-values).
    lr_trans_oci: [0, 1] OCI-dependent learning rate for the Transition Matrix.
                    lr_trans = lr_trans_oci * OCI.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based weight.
    stick: [0, 5] Choice stickiness.
    """
    lr_rew, lr_trans_oci, beta, w, stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Initialize Transition Matrix with standard priors
    # This matrix is now dynamic and distinct per subject
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    # Calculate the transition learning rate for this participant
    lr_trans = lr_trans_oci * oci_score
    if lr_trans > 1.0: lr_trans = 1.0
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Use the dynamic transition_matrix for MB valuation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # 1. Update Transition Matrix based on observed transition (a1 -> s_idx)
        # Increase prob of observed state, decrease prob of other state
        transition_matrix[a1, s_idx] += lr_trans * (1.0 - transition_matrix[a1, s_idx])
        # Ensure rows sum to 1
        transition_matrix[a1, 1 - s_idx] = 1.0 - transition_matrix[a1, s_idx]
        
        # 2. Update Action Values
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_rew * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_rew * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```