Here are three cognitive models proposed to explain the participant's behavior, incorporating the OCI score to modulate specific decision-making mechanisms.

### Model 1: Outcome-Dependent Inverse Temperature (Win/Loss Beta)
This model hypothesizes that the participant's level of decision noise (exploration vs. exploitation) changes based on the outcome of the previous trial. OCI modulates the "winning" beta, testing if high OCI leads to rigid exploitation (high beta) or anxiety-driven randomization (low beta) specifically after a reward.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Outcome-Dependent Inverse Temperature Model.
    
    Hypothesis: The participant's decision noise (beta) depends on whether the previous 
    trial was rewarded. OCI modulates the beta specifically after a win (beta_win).
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_loss: [0, 10] Inverse temperature after an unrewarded trial.
    - beta_win_base: [0, 10] Base inverse temperature after a rewarded trial.
    - beta_win_oci_mod: [0, 10] OCI modulation on beta_win.
        beta_win = beta_win_base + (beta_win_oci_mod - 5.0) * oci
        (Allows +/- modulation).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr, beta_loss, beta_win_base, beta_win_oci_mod, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Map modulation parameter to centered range [-5, 5]
    mod_val = beta_win_oci_mod - 5.0
    beta_win = beta_win_base + (mod_val * current_oci)
    beta_win = np.clip(beta_win, 0.0, 10.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    prev_reward = 0.0 # Initialize previous reward as 0 (loss) for first trial

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Determine current beta based on previous outcome
        current_beta = beta_win if prev_reward > 0 else beta_loss

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]

        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Surprise-Dependent Stage 2 Precision
This model focuses on how the participant reacts to "rare" transitions (surprise). It hypothesizes that when a rare transition occurs, the decision-making at Stage 2 is disrupted. OCI modulates the inverse temperature specifically for these "surprise" states, potentially capturing a tendency to become more rigid (high beta) or confused (low beta) when expectations are violated.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Surprise-Dependent Stage 2 Precision Model.
    
    Hypothesis: The precision (beta) of the Stage 2 choice depends on whether the 
    transition from Stage 1 was Common or Rare. OCI modulates the response to Rare transitions.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_common: [0, 10] Beta used for Stage 1 and Stage 2 (after Common transitions).
    - beta_rare_base: [0, 10] Base beta used for Stage 2 after Rare transitions.
    - beta_rare_oci_mod: [0, 10] OCI modulation on beta_rare.
        beta_rare = beta_rare_base + (beta_rare_oci_mod - 5.0) * oci
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Choice perseveration.
    """
    lr, beta_common, beta_rare_base, beta_rare_oci_mod, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    mod_val = beta_rare_oci_mod - 5.0
    beta_rare = beta_rare_base + (mod_val * current_oci)
    beta_rare = np.clip(beta_rare, 0.0, 10.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Always uses beta_common) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_common * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Detect Transition Type ---
        # Common: (Action 0 -> State 0) or (Action 1 -> State 1)
        # Rare:   (Action 0 -> State 1) or (Action 1 -> State 0)
        is_rare = (a1 == 0 and s_idx == 1) or (a1 == 1 and s_idx == 0)
        
        current_beta_s2 = beta_rare if is_rare else beta_common

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(current_beta_s2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]

        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Temporal Beta Evolution (Fatigue/Settling)
This model posits that decision noise evolves linearly over time (e.g., participants "settle in" and become more deterministic, or "fatigue" and become more random). OCI modulates the *rate* of this change. High OCI might be associated with a failure to settle (continued anxiety/exploration) or rapid rigidification.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Temporal Beta Evolution Model.
    
    Hypothesis: Inverse temperature (beta) changes linearly over trials.
    OCI modulates the slope of this change (beta_slope).
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_intercept: [0, 10] Initial beta at trial 0.
    - beta_slope_base: [0, 1] Base slope parameter.
    - beta_slope_oci_mod: [0, 1] OCI modulation on slope.
        slope = (beta_slope_base + (beta_slope_oci_mod - 0.5)*2 * oci - 0.5) * 0.2
        (Scaled to allow small positive/negative slopes).
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Choice perseveration.
    """
    lr, beta_intercept, beta_slope_base, beta_slope_oci_mod, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Map inputs [0,1] to a reasonable slope range, e.g., [-0.1, 0.1] per trial
    # slope_base centered at 0.5 -> 0.0
    base_sl = beta_slope_base - 0.5
    mod_sl = (beta_slope_oci_mod - 0.5) * current_oci
    
    # Scale factor to prevent beta from exploding too fast
    slope = (base_sl + mod_sl) * 0.2 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate time-dependent beta
        current_beta = beta_intercept + (slope * trial)
        current_beta = np.clip(current_beta, 0.0, 10.0)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]

        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```