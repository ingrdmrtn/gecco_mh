Here are three new cognitive models that hypothesize different mechanisms for how Obsessive-Compulsive symptoms (OCI) influence decision-making in the two-step task.

### Model 1: Dynamic Transition Learning
This model hypothesizes that OCI severity affects the **flexibility of the Model-Based system**, specifically how quickly participants update their internal model of the spaceship-planet transition probabilities.
- **Hypothesis**: Low OCI participants may maintain a stable, instructed belief about transitions (learning rate $\approx$ 0), while High OCI participants might overly adjust their beliefs based on recent rare transitions (high learning rate, "chasing noise") or be excessively rigid. This model tests if the transition learning rate varies with OCI.
- **Mechanism**: The transition matrix is not fixed. It is updated on every trial using a delta rule with a learning rate modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Transition Learning Model.
    
    Hypothesis: OCI severity modulates the learning rate of the transition matrix (Model-Based component).
    While the transition probabilities are objectively stable, participants may subjectively update 
    their beliefs about spaceship-planet links. High OCI might correlate with hyper-updating (instability)
    or rigidity.
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-values (MF).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Choice stickiness (perseveration).
    - lr_trans_low: [0, 1] Transition matrix learning rate for Low OCI.
    - lr_trans_high: [0, 1] Transition matrix learning rate for High OCI.
    """
    lr, beta, w, stickiness, lr_trans_low, lr_trans_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Map OCI to transition learning rate
    lr_trans = lr_trans_low + (lr_trans_high - lr_trans_low) * current_oci
    
    # Initialize estimated transition matrix (p(State|Action))
    # Rows: Actions (0, 1), Cols: States (0, 1)
    # Start with the instructed prior (A->0 common, U->1 common)
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    last_action_1 = -1
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # Model-Based Value Calculation using dynamic transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Stage 1 Choice Probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # Stage 2 Choice Probability
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        # 1. Update Transition Matrix (MB learning)
        # Create one-hot vector for the observed state
        state_onehot = np.zeros(2)
        state_onehot[s_idx] = 1.0
        
        # Update belief for the chosen spaceship
        # Delta = Observed_State - Predicted_Probs
        trans_delta = state_onehot - trans_matrix[a1]
        trans_matrix[a1] += lr_trans * trans_delta
        
        # Normalize to ensure valid probabilities
        trans_matrix[a1] = np.maximum(trans_matrix[a1], 0) 
        row_sum = np.sum(trans_matrix[a1])
        if row_sum > 0:
            trans_matrix[a1] /= row_sum

        # 2. Update Q-values (MF learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1

    return -log_likelihood
```

### Model 2: Counterfactual Updating
This model hypothesizes that OCI severity relates to **counterfactual thinking** ("What if I had chosen the other spaceship?").
- **Hypothesis**: High OCI participants may be more prone to compulsively updating the value of the unchosen option. Specifically, they might assume that if the chosen option was bad, the unchosen one must have been good (or vice versa), leading to different switching behaviors.
- **Mechanism**: After observing the outcome of the chosen path, the model also updates the *unchosen* Stage 1 action towards the *opposite* value of the received outcome (or the Stage 2 value), weighted by an OCI-dependent parameter.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Counterfactual Updating Model.
    
    Hypothesis: OCI severity modulates the extent of counterfactual updating.
    Participants may update the unchosen spaceship's value based on the outcome of the chosen one 
    (inverse correlation assumption: "If I lost here, the other one was likely a winner").
    
    Parameters:
    - lr: [0, 1] Learning rate for chosen option.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Choice stickiness.
    - cf_weight_low: [0, 1] Counterfactual weight for Low OCI.
    - cf_weight_high: [0, 1] Counterfactual weight for High OCI.
    """
    lr, beta, w, stickiness, cf_weight_low, cf_weight_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate counterfactual weight based on OCI
    cf_weight = cf_weight_low + (cf_weight_high - cf_weight_low) * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        # Stage 1 Chosen Update
        value_s2 = q_stage2_mf[s_idx, a2]
        delta_stage1 = value_s2 - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 1 Unchosen Counterfactual Update
        # We assume the unchosen option would have yielded the opposite value (1 - current_value).
        # This models the thought "The grass is greener on the other side" or "I missed out".
        unchosen_a1 = 1 - a1
        cf_target = 1.0 - value_s2 
        delta_cf = cf_target - q_stage1_mf[unchosen_a1]
        
        # Apply update scaled by cf_weight
        q_stage1_mf[unchosen_a1] += lr * cf_weight * delta_cf
        
        last_action_1 = a1

    return -log_likelihood
```

### Model 3: Ambiguity Aversion / Novelty Seeking
This model hypothesizes that OCI severity drives **Ambiguity Aversion**, manifesting as a bias against options that haven't been chosen recently.
- **Hypothesis**: High OCI (often associated with anxiety and harm avoidance) leads to a penalty for options with high temporal uncertainty (long time since last chosen), causing "compulsive" stickiness to the current option. Low OCI might show novelty seeking (bonus for unchosen).
- **Mechanism**: A bias term is added to the Q-values proportional to the logarithm of time since the action was last chosen. The direction (bonus vs penalty) and magnitude are determined by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Ambiguity Aversion / Novelty Seeking Model.
    
    Hypothesis: OCI severity modulates a bias based on the time since an option was last chosen.
    High OCI (anxiety) may lead to ambiguity aversion (penalty for unchosen/unknown options), 
    driving stickiness. Low OCI might show novelty seeking (bonus for unchosen options).
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stickiness: [0, 5] Standard 1-back stickiness.
    - ambig_low: [0, 1] Ambiguity parameter for Low OCI (<0.5 = Aversion, >0.5 = Novelty).
    - ambig_high: [0, 1] Ambiguity parameter for High OCI.
    """
    lr, beta, w, stickiness, ambig_low, ambig_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Map [0, 1] param to a weight centered at 0
    # If param > 0.5 -> Positive Bonus (Novelty Seeking)
    # If param < 0.5 -> Negative Penalty (Ambiguity Aversion)
    # Scale factor 2.0 allows range [-1.0, 1.0] roughly
    ambig_param = ambig_low + (ambig_high - ambig_low) * current_oci
    ambig_weight = (ambig_param - 0.5) * 2.0 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track time since last choice
    last_chosen_time = np.zeros(2) 
    
    last_action_1 = -1
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Standard Stickiness (Short-term repetition)
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Add Ambiguity/Novelty Bonus (Long-term exploration/avoidance)
        # Calculates time elapsed since each option was last chosen
        time_diff = trial - last_chosen_time
        # Logarithmic scaling of time to represent diminishing sensitivity to uncertainty
        ambig_bonus = ambig_weight * np.log(time_diff + 1.0)
        q_net += ambig_bonus
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1
        last_chosen_time[a1] = trial

    return -log_likelihood
```