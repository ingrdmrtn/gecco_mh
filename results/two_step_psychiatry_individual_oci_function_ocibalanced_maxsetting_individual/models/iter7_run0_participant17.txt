Here are three new cognitive models for the two-step decision task, incorporating the OCI score into different mechanisms: **Accumulated Stickiness**, **Dynamic Strategy Shifting**, and **Subjective Reward Valuation**.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Accumulated Stickiness (Choice Trace) Modulated by OCI.
    
    Hypothesis: Stickiness is not just about the immediate last choice, but an 
    accumulated trace of past choices (habit strength). The influence (weight) 
    of this accumulated habit on the current choice is modulated by OCI.
    High OCI may lead to stronger reliance on established motor patterns (compulsivity).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for MF values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting for Model-Based values (0=MF, 1=MB).
    trace_decay: [0, 1] - Decay rate of the choice trace (0=instant decay, 1=no decay).
    stick_weight_base: [-5, 5] - Base weight of the choice trace.
    stick_weight_oci: [-5, 5] - Modulation of trace weight by OCI.
    """
    learning_rate, beta, w, trace_decay, stick_weight_base, stick_weight_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    stick_weight = stick_weight_base + (stick_weight_oci * oci_val)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Accumulated choice trace for Stage 1 actions (0 and 1)
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        # Add accumulated stickiness bonus
        logits_1 = beta * q_net + stick_weight * choice_trace
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        
        # Update Trace: decay existing trace, then add current choice
        choice_trace *= trace_decay
        choice_trace[a1] += 1.0

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 MF with both stage 1 and stage 2 errors (TD(1)-like)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Dynamic Model-Based Weight Decay Modulated by OCI.
    
    Hypothesis: Participants may start with a goal-directed (Model-Based) strategy 
    but shift towards habits (Model-Free) over time. OCI modulates the rate of this 
    decay. High OCI might accelerate the transition to habitual control (faster decay of w).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w_init: [0, 1] - Initial Model-Based weight.
    w_decay_base: [0, 10] - Base decay rate of MB weight over trials.
    w_decay_oci: [-5, 5] - Modulation of decay rate by OCI.
    stickiness: [-5, 5] - Simple choice perseveration bonus.
    """
    learning_rate, beta, w_init, w_decay_base, w_decay_oci, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    decay_rate = w_decay_base + (w_decay_oci * oci_val)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Calculate dynamic w based on trial progress
        t_norm = trial / n_trials
        # w decays exponentially over the session
        w_curr = w_init * np.exp(-decay_rate * t_norm * 5.0) 
        w_curr = np.clip(w_curr, 0.0, 1.0)
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_curr * q_stage1_mb + (1 - w_curr) * q_stage1_mf
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        last_action_1 = a1
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with Subjective Zero-Reward Valuation Modulated by OCI.
    
    Hypothesis: OCI affects how the absence of reward (0 coins) is valued.
    High OCI might associate 0 coins with failure/punishment (negative value) 
    rather than neutral, altering the reinforcement learning dynamics.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    stickiness: [-5, 5] - Choice perseveration.
    rho_base: [-1, 1] - Base subjective value for 0 reward.
    rho_oci: [-1, 1] - Modulation of rho by OCI.
    """
    learning_rate, beta, w, stickiness, rho_base, rho_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate subjective value for 0 outcome
    rho = rho_base + (rho_oci * oci_val)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        last_action_1 = a1
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        # Apply subjective valuation
        r_eff = r
        if r == 0.0:
            r_eff = rho
            
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r_eff - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```