Here are three new cognitive models based on the participant's data and OCI profile.

### Model 1: Subjective Loss Valuation
This model tests the hypothesis that participants with higher OCI symptoms do not perceive the absence of a reward (0 coins) as a neutral event, but rather as a negative outcome (a loss). This "perfectionistic" or "harm avoidance" tendency implies that the effective reward signal is distorted.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF RL with OCI-modulated Subjective Loss Magnitude.
    
    Hypothesis: Participants with high OCI symptoms perceive a lack of reward (0 coins) 
    not as a neutral outcome, but as a negative outcome (loss). 
    The magnitude of this subjective loss is proportional to their OCI score.
    
    Parameters:
    learning_rate: [0, 1] - Q-learning update rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-Based / Model-Free mixing weight.
    loss_magnitude: [0, 10] - Scaling factor for the negative reward when R=0. 
                              Effective Reward = -1 * OCI * loss_magnitude (if R=0).
    """
    learning_rate, beta, w, loss_magnitude = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Reward Calculation ---
        r_observed = reward[trial]
        if r_observed == 0:
            # Subjective loss valuation based on OCI
            r_effective = -1.0 * oci_score * loss_magnitude
        else:
            r_effective = r_observed

        # --- Updates ---
        # Stage 1 update (using Stage 2 Q-value as proxy for value of state)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # Stage 2 update (using effective reward)
        delta_stage2 = r_effective - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Win-Stay Lose-Shift Mixture
This model proposes that high OCI symptoms lead to a reliance on rigid, heuristic strategies that compete with probabilistic reinforcement learning. Specifically, it mixes the standard RL policy with a deterministic "Win-Stay, Lose-Shift" (WSLS) strategy, where the weight of the WSLS strategy is determined by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF RL mixed with OCI-modulated Win-Stay Lose-Shift (WSLS) Strategy.
    
    Hypothesis: High OCI symptoms lead to a reliance on rigid, heuristic strategies 
    like Win-Stay Lose-Shift, competing with the probabilistic RL computations.
    This model mixes the RL policy with a deterministic WSLS policy.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    wsls_oci_scale: [0, 5] - Determines the mixing weight of the WSLS strategy based on OCI.
                             rho = clip(OCI * wsls_oci_scale, 0, 1).
    """
    learning_rate, beta, w, wsls_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # WSLS state variables
    last_reward = None
    last_action_1 = None
    
    # Calculate mixing weight for WSLS
    rho = np.clip(oci_score * wsls_oci_scale, 0.0, 1.0)

    for trial in range(n_trials):

        # --- Stage 1 Policy (RL Component) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_rl = exp_q1 / np.sum(exp_q1)
        
        # --- Stage 1 Policy (WSLS Component) ---
        probs_wsls = np.zeros(2)
        if last_reward is not None:
            if last_reward == 1:
                # Win-Stay
                probs_wsls[last_action_1] = 1.0
            else:
                # Lose-Shift
                probs_wsls[1 - last_action_1] = 1.0
        else:
            # First trial or undefined, fallback to uniform (or pure RL, here implemented as 0 influence)
            probs_wsls = probs_rl.copy() 
            
        # --- Mixture ---
        # If it's the first trial, rho effectively doesn't apply (probs_wsls = probs_rl)
        probs_1 = (1 - rho) * probs_rl + rho * probs_wsls
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # Update history for WSLS
        last_reward = r
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty Avoidance
This model posits that OCI symptoms correlate with an "Intolerance of Uncertainty." The agent tracks the history of prediction errors (surprisal) associated with each Stage 1 action. High OCI leads to a penalty for actions that have historically resulted in high surprise (unreliable outcomes), biasing the agent towards "safer," more predictable paths.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF RL with OCI-modulated Uncertainty Avoidance.
    
    Hypothesis: Participants with high OCI are intolerant of uncertainty. 
    They track the 'surprisal' (prediction error magnitude) of actions and 
    penalize actions that have historically produced high prediction errors.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    uncert_lr: [0, 1] - Learning rate for the uncertainty trace (accumulation of RPEs).
    uncert_penalty: [0, 5] - Penalty weight for uncertainty, scaled by OCI.
    """
    learning_rate, beta, w, uncert_lr, uncert_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track accumulated uncertainty (average magnitude of RPEs) for each Stage 1 action
    uncertainty_trace = np.zeros(2) 

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply uncertainty penalty
        # High OCI -> Stronger avoidance of actions with high historical prediction errors
        penalty = oci_score * uncert_penalty * uncertainty_trace
        q_net_penalized = q_net - penalty
        
        exp_q1 = np.exp(beta * q_net_penalized)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # Update Uncertainty Trace
        # We define uncertainty here as the total unsigned prediction error experienced
        total_surprise = np.abs(delta_stage1) + np.abs(delta_stage2)
        uncertainty_trace[a1] = uncertainty_trace[a1] + uncert_lr * (total_surprise - uncertainty_trace[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```