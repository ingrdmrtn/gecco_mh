Here are three new cognitive models exploring different mechanisms for how OCI might influence decision-making in this two-step task, specifically focusing on the balance between model-based/model-free control, learning rates, and exploration.

### Cognitive Model 1: OCI-Modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores relate to a more rigid reliance on habit (model-free) over goal-directed planning (model-based), or conversely, an over-reliance on explicit rules. It posits that the mixing parameter `w` (which balances MB and MF systems) is a linear function of the OCI score. Unlike previous attempts, this model simplifies the learning update to a standard hybrid TD-learning approach without eligibility traces to isolate the mixing weight effect.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Weighting.
    
    This model assumes a hybrid Model-Based (MB) and Model-Free (MF) learner.
    The balance between these systems (w) is determined by the OCI score.
    w = w_base + w_oci_slope * oci
    w is clamped between 0 (pure MF) and 1 (pure MB).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Baseline weight for Model-Based system.
    - w_oci_slope: [-1, 1] How OCI score changes the MB weight.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective mixing weight w based on OCI
    w = w_base + w_oci_slope * oci_score
    w = np.clip(w, 0.0, 1.0) # Ensure w stays valid
    
    # Transition matrix (fixed for this task structure)
    # 0 -> 0 (common), 0 -> 1 (rare)
    # 1 -> 1 (common), 1 -> 0 (rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # MF Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for stage 2 (states 0,1; actions 0,1)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Skip invalid trials
        if a1 == -1:
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        # V_MB(s1, a) = sum(P(s'|s1,a) * max_a'(Q_MF(s', a')))
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value for each state (0 and 1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update (standard Q-learning)
        # Q2(s2, a2) += alpha * (r - Q2(s2, a2))
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD-0 for MF)
        # The MF system updates stage 1 based on the value of the chosen stage 2 state
        # Usually Q1(a1) += alpha * (Q2(s2, a2) - Q1(a1))
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Inverse Temperature (Exploration/Exploitation)
This model hypothesizes that OCI symptoms manifest as altered exploration/exploitation trade-offs. Specifically, it tests if higher OCI leads to more deterministic (higher beta) or more stochastic (lower beta) behavior in the second stage, where the reward is immediate. It assumes a pure Model-Free framework to isolate the decision noise effect.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Inverse Temperature.
    
    This model assumes a pure Model-Free learner.
    The inverse temperature (beta) depends on OCI.
    High OCI might lead to more rigid/deterministic choices (high beta)
    or more anxious/noisy choices (low beta).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_oci_scale: [-5, 5] Scaling factor for OCI effect on beta.
    """
    learning_rate, beta_base, beta_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # We use an exponential formulation to ensure beta stays positive naturally
    # or simple linear clipping. Let's use linear with clipping for interpretability.
    beta = beta_base + beta_oci_scale * oci_score
    beta = max(0.0, beta) # Beta cannot be negative
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning (SARSA-like / TD for Stage 1)
        # Update Stage 1 based on Stage 2 value
        # Q1(a1) <- Q1(a1) + lr * (Q2(s2, a2) - Q1(a1))
        # Note: In pure MF models of this task, often the Stage 1 value is updated 
        # towards the value of the state reached or the reward received. 
        # Standard TD(0):
        delta_stage1 = q_stage2[s2, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 based on reward
        delta_stage2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Dependent Learning Rate Asymmetry (Pos/Neg)
This model investigates if OCI scores correlate with an asymmetry in learning from positive versus negative prediction errors. For example, high OCI might be associated with hypersensitivity to negative outcomes (or lack of reward) or rigid maintenance of values despite negative feedback.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Learning Rate Asymmetry.
    
    This model assumes a pure Model-Free learner but allows for different 
    learning rates for positive and negative prediction errors.
    The learning rate for negative prediction errors (alpha_neg) is modulated by OCI.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - lr_neg_base: [0, 1] Base learning rate for negative RPEs.
    - lr_neg_oci_slope: [-1, 1] How OCI modifies the negative learning rate.
    - beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + lr_neg_oci_slope * oci_score
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1:
            continue

        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update
        delta_stage2 = r - q_stage2[s2, a2]
        if delta_stage2 >= 0:
            q_stage2[s2, a2] += lr_pos * delta_stage2
        else:
            q_stage2[s2, a2] += lr_neg * delta_stage2
            
        # Stage 1 Update
        # Using the updated Q2 value for TD(0)
        delta_stage1 = q_stage2[s2, a2] - q_stage1[a1]
        if delta_stage1 >= 0:
            q_stage1[a1] += lr_pos * delta_stage1
        else:
            q_stage1[a1] += lr_neg * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```