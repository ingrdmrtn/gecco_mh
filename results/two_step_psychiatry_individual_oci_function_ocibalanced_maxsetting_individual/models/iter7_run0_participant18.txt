Here are the three cognitive models as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Distorted Model-Based Beliefs.
    Hypothesis: High OCI scores are associated with 'black-and-white' thinking or rigidity in probabilistic beliefs.
    This model posits that OCI distorts the internal model of the transition matrix used for Model-Based control.
    Specifically, high OCI increases the perceived probability of the 'common' transition, making the agent
    underestimate the likelihood of rare transitions during planning (treating the world as more deterministic than it is).

    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Perseveration bonus [0,5]
    - distortion: Degree to which OCI exaggerates common transition probability [0, 0.5]
    """
    lr, beta, w, lambda_val, stickiness, distortion = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Subjective transition matrix calculation
    # Base common prob is 0.7. OCI increases this towards 1.0.
    subjective_common_p = 0.7 + (distortion * oci_score)
    # Clip to ensure valid probability
    if subjective_common_p > 0.99: 
        subjective_common_p = 0.99
    
    # The agent's internal model of the world is distorted by OCI
    transition_matrix = np.array([
        [subjective_common_p, 1 - subjective_common_p], 
        [1 - subjective_common_p, subjective_common_p]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1: # Skip missing data
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 TD error (using Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 TD error (using Reward)
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[a1] += lr * lambda_val * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Asymmetric Punishment Learning.
    Hypothesis: Individuals with higher OCI scores exhibit increased sensitivity to negative prediction errors
    (disappointment/failure), reflecting a fear of failure or perfectionism.
    This model boosts the learning rate when the prediction error is negative, proportional to the OCI score.
    This results in "loss chasing" or rapid value depression after failures.

    Parameters:
    - lr: Base learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Perseveration bonus [0,5]
    - neg_lr_boost: Multiplier for learning rate on negative errors [0, 5]
    """
    lr, beta, w, lambda_val, stickiness, neg_lr_boost = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Asymmetric Learning Rate for Stage 1
        current_lr_s1 = lr
        if delta_stage1 < 0:
            current_lr_s1 = lr * (1 + neg_lr_boost * oci_score)
            
        q_stage1_mf[a1] += current_lr_s1 * delta_stage1
        
        # Update Stage 2
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Asymmetric Learning Rate for Stage 2
        current_lr_s2 = lr
        if delta_stage2 < 0:
            current_lr_s2 = lr * (1 + neg_lr_boost * oci_score)
            
        q_stage2_mf[state_idx, a2] += current_lr_s2 * delta_stage2
        
        # Eligibility trace update for Stage 1 (driven by Stage 2 error)
        # We use the Stage 2 learning rate logic here as the error comes from Stage 2
        q_stage1_mf[a1] += current_lr_s2 * lambda_val * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Induced Post-Error Rigidity.
    Hypothesis: High OCI is linked to an intolerance of uncertainty or failure. When the participant experiences
    a lack of reward (error/failure), they become more rigid in their subsequent choice strategy (freezing/compulsivity).
    This is modeled by increasing the inverse temperature (beta) on the trial following a zero reward,
    scaled by the OCI score.

    Parameters:
    - lr: Learning rate [0,1]
    - beta_base: Baseline inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Perseveration bonus [0,5]
    - rigidity_factor: OCI-dependent increase in beta after failure [0, 5]
    """
    lr, beta_base, w, lambda_val, stickiness, rigidity_factor = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    prev_a1 = -1
    prev_reward = 1.0 # Initialize as if previous was successful to start neutral

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        # Determine current beta based on previous reward
        current_beta = beta_base
        if prev_reward == 0.0:
            # Increase beta (rigidity) if previous outcome was failure
            current_beta = beta_base * (1 + rigidity_factor * oci_score)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = current_beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = current_beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * lambda_val * delta_stage2
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```