Here are the 3 proposed cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Trace (TD-Lambda).
    
    Hypothesis: High OCI individuals may exhibit a more "direct" reinforcement learning style 
    driven by the final outcome rather than the intermediate state value. This is modeled 
    by modulating the eligibility trace parameter (lambda) with the OCI score.
    A higher lambda (closer to 1) implies that the Stage 1 value update is driven more 
    by the Stage 2 reward (Monte Carlo-like), whereas a lower lambda (closer to 0) 
    relies on the bootstrapped Stage 2 value (TD).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based and Model-Free values (1 = Pure MB).
    - lambda_oci: [0, 5] Scaling factor for eligibility trace lambda based on OCI. 
                   lambda = clip(lambda_oci * oci, 0, 1).
    """
    learning_rate, beta, w, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lambda based on OCI, clipped to [0, 1]
    lambda_val = lambda_oci * oci_score
    if lambda_val > 1.0: lambda_val = 1.0
    if lambda_val < 0.0: lambda_val = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        
        # TD Errors
        # Error for Stage 1 prediction of Stage 2 Value
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        # Error for Stage 2 prediction of Reward
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Stage 2 Update (Standard Q-learning)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update with OCI-modulated Eligibility Trace
        # Q1 update combines the immediate TD error (delta_stage1) 
        # and a portion of the subsequent error (delta_stage2) scaled by lambda.
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_val * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Anxiety (Rare Transition Penalty).
    
    Hypothesis: High OCI individuals may have an intolerance for uncertainty or 
    unexpected outcomes. When a 'Rare' transition occurs (violating the common 
    transition structure), the participant penalizes the chosen spaceship, 
    reducing the likelihood of choosing it again, regardless of the final reward. 
    This "Transition Anxiety" penalty is scaled by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - rare_pen_oci: [0, 2] Penalty magnitude subtracted from Q-value upon rare transition, scaled by OCI.
    """
    learning_rate, beta, w, rare_pen_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    penalty = rare_pen_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        
        # Standard Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Rare Transition Penalty
        # Common: 0->0, 1->1. Rare: 0->1, 1->0.
        is_rare = (a1 == 0 and s_idx == 1) or (a1 == 1 and s_idx == 0)
        if is_rare:
            q_stage1_mf[a1] -= penalty

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Beta Growth (Rigidity Formation).
    
    Hypothesis: Compulsivity (OCI) manifests as an increasing rigidity in choice behavior 
    over time. While the participant may start with a baseline level of exploration 
    (inverse temperature), the beta parameter increases over the course of the task 
    for high OCI individuals, representing the "setting in" of habits or rituals.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature at trial 0.
    - w: [0, 1] MB/MF weight.
    - beta_growth_oci: [0, 10] Rate at which beta increases over time, scaled by OCI.
    """
    learning_rate, beta_base, w, beta_growth_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Calculate dynamic beta
        # Beta increases linearly with time proportional to OCI score
        current_beta = beta_base * (1.0 + beta_growth_oci * oci_score * (trial / n_trials))
        
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2
        s_idx = int(state[trial])
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```