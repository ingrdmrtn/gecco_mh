def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Fictitious Update Model modulated by OCI.
    Hypothesis: OCI is associated with over-generalization or fear of missing out. 
    Participants update the value of the *unchosen* Stage 1 spaceship based on the reward received 
    from the *chosen* path, as if they had chosen the other option.
    
    Parameters:
    lr: Learning rate for chosen options [0, 1]
    beta: Inverse temperature [0, 10]
    w: Model-based weight [0, 1]
    fict_base: Baseline learning rate for fictitious update of unchosen option [0, 1]
    fict_oci_param: Effect of OCI on fictitious learning rate [0, 1]
                    (Mapped to range [-0.5, 0.5] internally to allow positive/negative modulation)
    """
    lr, beta, w, fict_base, fict_oci_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Map 0-1 param to -0.5 to 0.5 to allow increasing or decreasing effect
    fict_mod = (fict_oci_param - 0.5)
    lr_fict = fict_base + fict_mod * current_oci
    lr_fict = np.clip(lr_fict, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Fictitious update of unchosen Stage 1 option
        # Update unchosen option towards the SAME reward (Generalization)
        # or towards 0 if that's the hypothesis (but here we model generalization of outcome)
        unchosen_a1 = 1 - a1
        # We assume the agent generalizes the outcome 'r' to the unchosen spaceship
        delta_fict = r - q_stage1_mf[unchosen_a1]
        q_stage1_mf[unchosen_a1] += lr_fict * delta_fict

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Surprise Sensitivity Model modulated by OCI.
    Hypothesis: OCI affects how participants learn from "Rare" transitions (surprise).
    High OCI might lead to over-reaction (high LR) or under-reaction (ignoring outliers) 
    to outcomes following a rare transition.
    
    Parameters:
    lr_common: Learning rate for common transitions [0, 1]
    beta: Inverse temperature [0, 10]
    w: Model-based weight [0, 1]
    lr_rare_base: Baseline learning rate for rare transitions [0, 1]
    lr_rare_oci_param: Effect of OCI on rare transition learning rate [0, 1]
                       (Mapped to [-0.5, 0.5] internally)
    """
    lr_common, beta, w, lr_rare_base, lr_rare_oci_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    lr_rare_mod = (lr_rare_oci_param - 0.5)
    lr_rare = lr_rare_base + lr_rare_mod * current_oci
    lr_rare = np.clip(lr_rare, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Determine if transition was rare
        # Common: 0->0, 1->1 (prob 0.7). Rare: 0->1, 1->0 (prob 0.3)
        is_common = (a1 == s_idx)
        current_lr = lr_common if is_common else lr_rare

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        # Note: Standard MB/MF models usually use the same LR for S2 and S1 updates.
        # Here we modulate the LR based on the transition type that just occurred.
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Initial Belief Bias Model modulated by OCI.
    Hypothesis: OCI is associated with pessimism (low initial expectations) or 
    optimism/magical thinking (high initial expectations), affecting exploration in early trials.
    
    Parameters:
    lr: Learning rate [0, 1]
    beta: Inverse temperature [0, 10]
    w: Model-based weight [0, 1]
    q0_base: Baseline initial Q-value [0, 1]
    q0_oci_param: Effect of OCI on initial Q-value [0, 1]
                  (Mapped to [-0.5, 0.5] internally)
    """
    lr, beta, w, q0_base, q0_oci_param = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    q0_mod = (q0_oci_param - 0.5)
    q_init = q0_base + q0_mod * current_oci
    q_init = np.clip(q_init, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values with OCI-modulated bias
    q_stage1_mf = np.full(2, q_init)
    q_stage2_mf = np.full((2, 2), q_init)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss