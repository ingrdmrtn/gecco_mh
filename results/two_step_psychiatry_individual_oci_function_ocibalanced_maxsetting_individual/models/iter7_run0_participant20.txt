Here are the 3 cognitive models as requested.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Structural Learning (Adaptive Transition Matrix).
    
    Hypothesis: Participants may not assume fixed transition probabilities (0.7/0.3) 
    but learn them over time. OCI symptoms (e.g., intolerance of uncertainty or 
    hyper-vigilance) may modulate the rate at which they update their internal model 
    of the spaceship-planet transitions.
    
    Parameters:
    - lr_val: [0, 1] Learning rate for value updates (TD learning).
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting of Model-Based system (0=MF, 1=MB).
    - stick: [0, 5] Choice stickiness/perseveration.
    - lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    - lr_trans_oci: [0, 1] Modulation of transition learning rate by OCI.
      Effective lr_trans = lr_trans_base * (1 + lr_trans_oci * OCI) (clipped at 1).
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
    
    Returns:
        float: Negative log-likelihood.
    """
    lr_val, beta, w, stick, lr_trans_base, lr_trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Initialize transition matrix belief (start with priors or flat?)
    # Assuming instruction knowledge: start at [0.7, 0.3]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    
    # Calculate effective transition learning rate
    # We allow OCI to increase the learning rate (hyper-vigilance)
    lr_trans_eff = lr_trans_base * (1.0 + lr_trans_oci * oci_score)
    if lr_trans_eff > 1.0: lr_trans_eff = 1.0
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Uses current estimated transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        
        # 1. Update MF Values
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_val * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_val * delta_stage2
        
        # 2. Update Transition Matrix (Structural Learning)
        # Update probability of s2 given a1
        # T[a1, s2] moves towards 1
        transition_matrix[a1, s2] += lr_trans_eff * (1.0 - transition_matrix[a1, s2])
        # The other state probability must be 1 - T[a1, s2]
        transition_matrix[a1, 1-s2] = 1.0 - transition_matrix[a1, s2]
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Reward Sensitivity.
    
    Hypothesis: OCI scores may correlate with an altered subjective valuation of rewards.
    High OCI individuals might perceive the successful attainment of gold coins as 
    more salient or valuable (hypersensitivity to reward), leading to more deterministic 
    behavior (higher effective beta) when values are high.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature (base).
    - w: [0, 1] MB/MF weight.
    - stick: [0, 5] Stickiness.
    - reward_scale_oci: [0, 5] Scaling factor for reward based on OCI.
      Effective Reward = Reward * (1 + reward_scale_oci * OCI).
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
    
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, stick, reward_scale_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r_raw = int(reward[trial])
        
        # Modulate Reward Perception
        r_eff = r_raw * (1.0 + reward_scale_oci * oci_score)
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Note: We update using the effective reward. This scales the Q-values.
        # Since Beta is constant, larger Q-values imply more deterministic choice (exploitation).
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r_eff - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stickiness Contextualized by Rare Transitions.
    
    Hypothesis: High OCI participants may exhibit different perseverative behavior (stickiness)
    specifically after experiencing a rare transition. While "Rare Gating" modulates learning,
    this model modulates the tendency to repeat the previous choice regardless of value.
    This could reflect a "safety seeking" repetition when the environment behaves unexpectedly.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick_base: [0, 5] Baseline stickiness.
    - stick_rare_oci: [0, 5] Additional stickiness added after a rare transition, scaled by OCI.
      If prev trial was rare: stick = stick_base + stick_rare_oci * OCI.
    
    Args:
        action_1, state, action_2, reward, oci, model_parameters
    
    Returns:
        float: Negative log-likelihood.
    """
    lr, beta, w, stick_base, stick_rare_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    prev_was_rare = False
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        
        # Apply Stickiness
        if last_choice_1 != -1:
            stick_val = stick_base
            # If the previous trial involved a rare transition, add OCI-modulated stickiness
            if prev_was_rare:
                stick_val += stick_rare_oci * oci_score
            
            logits_1[last_choice_1] += stick_val
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1
        # Determine if current transition was rare for next trial's context
        # Common: 0->0, 1->1. Rare: 0->1, 1->0.
        prev_was_rare = (a1 != s2)

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```