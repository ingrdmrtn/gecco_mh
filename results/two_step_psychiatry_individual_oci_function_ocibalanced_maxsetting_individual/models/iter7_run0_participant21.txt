Here are the three proposed cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Compulsive Persistence Model (Stickiness Reset on Loss).
    
    Hypothesis: OCI modulates the ability to "reset" or drop a perseverative habit 
    after a negative outcome (loss). High OCI participants may fail to reset 
    their stickiness trace even after receiving 0 coins, leading to compulsive repetition.
    
    Parameters:
    lr: [0, 1] Learning rate for Q-values.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Weight for Model-Based control (1=MB, 0=MF).
    pers_accum: [0, 2] Accumulation of stickiness per repetition (habit strength).
    reset_base: [0, 1] Base proportion of stickiness removed after a loss.
    reset_oci: [-2, 2] Effect of OCI on the reset proportion. 
               (Negative -> High OCI reduces reset -> Compulsion).
    """
    lr, beta, w, pers_accum, reset_base, reset_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate reset factor bounded in [0, 1]
    # High OCI with negative slope reduces the reset factor (less forgetting of habit on loss)
    raw_reset = reset_base + reset_oci * current_oci
    reset_factor = np.clip(raw_reset, 0.0, 1.0)
    
    # Initialize Q-values (0.0 optimistic/neutral init)
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    # Transition matrix (fixed structure)
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Stickiness trace for the two spaceships
    stickiness = np.zeros(2)
    last_action = -1
    
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max value of next stage
        max_q2 = np.max(q2, axis=1)
        q_mb = tm @ max_q2
        
        # Net Value: MB + MF + Stickiness
        q_net = w * q_mb + (1 - w) * q_mf + stickiness
        
        # Softmax Policy
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # --- Updates ---
        
        # 1. Update Stickiness (Accumulation)
        if last_action != -1:
            if a1 == last_action:
                stickiness[a1] += pers_accum
            else:
                stickiness[a1] = pers_accum # Start new accumulation
                stickiness[1-a1] = 0.0      # Reset unchosen
        else:
            stickiness[a1] = pers_accum
            
        last_action = a1
        
        # 2. Reset Stickiness on Loss (Modulated by OCI)
        if r == 0:
            stickiness[a1] *= (1.0 - reset_factor)
            
        # 3. Value Updates (SARSA-like for MF)
        # Stage 1 MF Update
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        # Stage 2 Update
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
    return -log_lik

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic MB-Reliability Model (Doubt after Surprise).
    
    Hypothesis: OCI modulates the stability of Model-Based control after unexpected events.
    High OCI participants may experience "doubt" after a rare transition, 
    temporarily reducing the weight of the Model-Based system (w) and reverting 
    to Model-Free habits for the next trial.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w_base: [0, 1] Baseline Model-Based weight.
    pers: [0, 1] General perseveration bonus.
    doubt_base: [0, 1] Base reduction in 'w' after a rare transition.
    doubt_oci: [-2, 2] Effect of OCI on doubt. 
               (Positive -> High OCI reduces w more -> High Doubt).
    """
    lr, beta, w_base, pers, doubt_base, doubt_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate doubt magnitude
    raw_doubt = doubt_base + doubt_oci * current_oci
    doubt_magnitude = np.clip(raw_doubt, 0.0, 1.0)
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    prev_choice = -1
    prev_transition_rare = False
    
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Determine current w based on previous transition type
        # If previous was rare, reduce w by doubt_magnitude
        current_w = w_base
        if prev_transition_rare:
            current_w -= doubt_magnitude
            
        # Clip w to [0, 1]
        current_w = np.clip(current_w, 0.0, 1.0)
        
        # --- Stage 1 Choice ---
        max_q2 = np.max(q2, axis=1)
        q_mb = tm @ max_q2
        
        # Add simple perseveration
        pers_vec = np.zeros(2)
        if prev_choice != -1:
            pers_vec[prev_choice] = pers
            
        q_net = current_w * q_mb + (1 - current_w) * q_mf + pers_vec
        
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # --- Updates ---
        # Determine if current transition was rare
        # Space 0 -> Planet 0 (Common), 1 (Rare)
        # Space 1 -> Planet 1 (Common), 0 (Rare)
        is_rare = False
        if a1 == 0 and s == 1: is_rare = True
        if a1 == 1 and s == 0: is_rare = True
        
        prev_transition_rare = is_rare
        prev_choice = a1
        
        # Value Updates
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
    return -log_lik

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Superstitious MF Learning Model (Rare Transition Scaling).
    
    Hypothesis: OCI modulates how Model-Free values are updated after Rare transitions.
    High OCI might lead to "superstitious" learning where rare paths are treated as 
    significant predictors (over-learning from rare events) or ignored (under-learning).
    This model scales the MF prediction error when the transition was rare.
    
    Parameters:
    lr: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB weight.
    pers: [0, 1] Perseveration bonus.
    rare_scale_base: [0, 3] Scaling factor for MF learning rate on rare transitions.
    rare_scale_oci: [-2, 2] Effect of OCI on the rare scaling factor.
    """
    lr, beta, w, pers, rare_scale_base, rare_scale_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate rare scaling factor
    # Can be > 1 (over-reaction) or < 1 (dampening)
    raw_scale = rare_scale_base + rare_scale_oci * current_oci
    rare_scale = np.clip(raw_scale, 0.0, 3.0) 
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    prev_choice = -1
    
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice ---
        max_q2 = np.max(q2, axis=1)
        q_mb = tm @ max_q2
        
        pers_vec = np.zeros(2)
        if prev_choice != -1:
            pers_vec[prev_choice] = pers
            
        q_net = w * q_mb + (1 - w) * q_mf + pers_vec
        
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # --- Updates ---
        prev_choice = a1
        
        # Determine transition type
        is_rare = False
        if a1 == 0 and s == 1: is_rare = True
        if a1 == 1 and s == 0: is_rare = True
        
        # Calculate effective learning rate scaling
        current_scale = rare_scale if is_rare else 1.0
        
        # Stage 1 MF Update with Scaling
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * current_scale * pe1
        
        # Stage 2 Update (Standard)
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
    return -log_lik
```