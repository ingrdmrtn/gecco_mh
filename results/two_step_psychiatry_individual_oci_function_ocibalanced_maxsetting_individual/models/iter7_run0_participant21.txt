Here are three new cognitive models exploring different mechanisms for how OCI scores might influence learning and decision-making in the two-step task.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model proposes that individuals with higher OCI scores might rely more (or less) on the model-based system. The standard hybrid model combines model-free (MF) and model-based (MB) values using a weighting parameter `w`. Here, `w` is a function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where the balance between Model-Based (MB) and Model-Free (MF) 
    strategies is modulated by the OCI score.
    
    The weighting parameter 'w' determines the mix: w=1 is purely MB, w=0 is purely MF.
    Here w is defined as a logistic function of a base parameter and an OCI-scaling parameter.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for MF value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_logit_base: [-5, 5] Base logit for the weighting parameter w.
    w_logit_oci: [-5, 5] How OCI score shifts the logit of w.
    """
    learning_rate, beta, w_logit_base, w_logit_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate w using a sigmoid transform to keep it in [0, 1]
    # w_logit = base + slope * OCI
    w_logit = w_logit_base + w_logit_oci * current_oci
    w = 1.0 / (1.0 + np.exp(-w_logit))
    
    # Transition matrix (fixed for this task structure)
    # A -> X (0.7), A -> Y (0.3); U -> Y (0.7), U -> X (0.3)
    # Mapping: Action 0 -> State 0 (0.7); Action 1 -> State 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # MF Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value calculation
        # V_MB(s1) = sum(P(s2|s1, a1) * max_a2 Q_MF(s2, a2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value for each state (planet)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        # Update Stage 2 MF values
        # RPE2 = Reward - Q_MF(s2, a2)
        rpe2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * rpe2
        
        # Update Stage 1 MF values using TD(0)
        # RPE1 = Q_MF(s2, a2) - Q_MF(s1, a1) (SARSA-style or using max for Q-learning)
        # Here we use the value of the state chosen in stage 2
        value_stage2 = q_stage2_mf[state_idx, int(action_2[trial])]
        rpe1 = value_stage2 - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * rpe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with OCI-Dependent Forgetting
This model hypothesizes that high OCI might be associated with a different rate of memory decay or "forgetting" for unchosen options. While standard RL assumes values of unchosen options stay static, this model assumes they decay towards 0 (or a baseline), and the rate of this decay is modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner where unchosen action values decay over time.
    The decay rate is modulated by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate for chosen options.
    beta: [0, 10] Inverse temperature.
    decay_base: [0, 1] Base decay rate for unchosen options (1 = no decay, 0 = instant forget).
    decay_oci_slope: [-1, 1] How OCI affects decay. Positive = high OCI retains memory better.
    """
    learning_rate, beta, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate decay factor. Constrained to [0, 1].
    # decay_factor = base + slope * OCI
    raw_decay = decay_base + decay_oci_slope * current_oci
    decay_factor = np.clip(raw_decay, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning & Decay ---
        
        # Stage 2 Update
        # Update chosen
        q_stage2[s_idx, a2] += learning_rate * (r - q_stage2[s_idx, a2])
        # Decay unchosen in the current state
        unchosen_a2 = 1 - a2
        q_stage2[s_idx, unchosen_a2] *= decay_factor
        # Note: We do not decay the values of the unvisited state in this simple implementation
        
        # Stage 1 Update
        # TD(0) update
        # Value of the second stage state is often approximated by the value of the chosen option
        v_next = q_stage2[s_idx, a2] 
        q_stage1[a1] += learning_rate * (v_next - q_stage1[a1])
        
        # Decay unchosen Stage 1 action
        unchosen_a1 = 1 - a1
        q_stage1[unchosen_a1] *= decay_factor

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Reward and Punishment (OCI-modulated)
This model investigates if OCI scores correlate with an asymmetry in learning from positive versus negative outcomes. Specifically, it tests if high OCI leads to hypersensitivity to punishment (zero reward) or reward (gold coin), affecting the learning rate for negative prediction errors specifically.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for positive and negative prediction errors.
    The learning rate for negative prediction errors (disappointment) is modulated by OCI.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive RPEs.
    lr_neg_base: [0, 1] Base learning rate for negative RPEs.
    lr_neg_oci_slope: [-1, 1] How OCI modifies the negative learning rate.
    beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated negative learning rate, clipped to [0, 1]
    raw_lr_neg = lr_neg_base + lr_neg_oci_slope * current_oci
    lr_neg = np.clip(raw_lr_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        
        # Stage 2 RPE
        rpe2 = r - q_stage2[s_idx, a2]
        
        # Apply asymmetric learning rates
        if rpe2 >= 0:
            eff_lr = lr_pos
        else:
            eff_lr = lr_neg
            
        q_stage2[s_idx, a2] += eff_lr * rpe2
        
        # Stage 1 RPE
        # Using the updated Q-value of stage 2 as the target
        target_val = q_stage2[s_idx, a2]
        rpe1 = target_val - q_stage1[a1]
        
        if rpe1 >= 0:
            eff_lr_1 = lr_pos
        else:
            eff_lr_1 = lr_neg
            
        q_stage1[a1] += eff_lr_1 * rpe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```