Here are 3 new cognitive models based on the participant's data and OCI score.

### Model 1: "Safety Seeking" Model (Avoidance Learning)
**Hypothesis:** High OCI scores are associated with avoidance behaviors and an over-sensitivity to negative outcomes (punishment). This model posits that the participant learns differently from rewards (coins) versus the absence of rewards (0 coins or negative coins), and that their OCI score amplifies the learning rate specifically for negative outcomes (avoidance learning), making them quicker to abandon choices that yield nothing.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Safety Seeking Model: Differentiates learning rates for positive vs. non-positive outcomes.
    The learning rate for non-positive outcomes (avoidance) is modulated by OCI.
    
    Hypothesis: Higher OCI leads to faster learning/overreaction to 'punishment' (0 or negative reward),
    reflecting a 'safety seeking' or harm avoidance strategy.
    
    Bounds:
    lr_pos: [0, 1] (Learning rate for positive rewards)
    lr_neg_base: [0, 1] (Base learning rate for non-positive outcomes)
    lr_neg_oci_mod: [0, 5] (Multiplier for OCI effect on negative learning rate)
    beta: [0, 10] (Inverse temperature)
    w: [0, 1] (MB/MF weighting)
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate effective negative learning rate, capped at 1.0
    lr_neg = min(1.0, lr_neg_base + (lr_neg_oci_mod * current_oci))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        # Determine which learning rate to use based on reward
        current_lr = lr_pos if reward[trial] > 0 else lr_neg

        # Update Stage 2 Q-values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += current_lr * delta_stage2
        
        # Update Stage 1 MF Q-values
        # Note: Using the same LR for stage 1 update for consistency in reaction
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: "Habitual Rigidity" Model (Inverse Temperature Modulation)
**Hypothesis:** OCI is often linked to rigidity and difficulty shifting set. This model hypothesizes that OCI modulates the exploration-exploitation balance (beta). Specifically, higher OCI leads to a higher `beta` (lower temperature), causing the participant to be more deterministic and "stuck" in their current best guess, exploring less even when uncertainty exists.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Habitual Rigidity Model: OCI scales the inverse temperature (beta).
    
    Hypothesis: Higher OCI leads to higher beta (more deterministic/rigid choices),
    reducing exploration. The participant sticks rigidly to the option with slightly 
    higher value.
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10] (Base inverse temperature)
    beta_oci_scale: [0, 10] (Scaling factor for OCI's effect on beta)
    w: [0, 1] (MB/MF weighting)
    """
    learning_rate, beta_base, beta_oci_scale, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Effective Beta: Base + (Scale * OCI)
    # Higher OCI = Higher Beta = More rigid/deterministic
    beta_effective = beta_base + (beta_oci_scale * current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: "Doubt-Induced Model-Based Suppression" Model
**Hypothesis:** Obsessive-compulsive tendencies often involve pathological doubt. While standard theory suggests OCI might correlate with Model-Based (MB) deficits, this model specifically tests if OCI *suppresses* the MB contribution (`w`). The hypothesis is that high OCI participants mistrust their internal model of the task structure (the transition probabilities) and rely more on raw Model-Free (MF) experience.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Doubt-Induced Suppression Model: OCI negatively modulates the Model-Based weight (w).
    
    Hypothesis: Higher OCI is associated with 'doubt' in the internal model structure,
    leading to a reduction in w (less Model-Based control, more Model-Free).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_max: [0, 1] (Maximum possible MB weight when OCI is 0)
    w_decay: [0, 5] (Rate at which OCI reduces w)
    """
    learning_rate, beta, w_max, w_decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Effective w decreases as OCI increases.
    # Uses a sigmoid-like decay or simple linear clip to ensure bounds [0, 1]
    # Here using linear decay clipped at 0.
    w_effective = max(0.0, w_max - (w_decay * current_oci))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
             p_choice_1[trial] = 0.5
             p_choice_2[trial] = 0.5
             continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mixing MB and MF based on effective w
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```