Here are 3 cognitive models expressed as Python functions.

### Model 1: OCI-Modulated Action Trace Decay
This model hypothesizes that OCI scores modulate the persistence of habits. Instead of a simple "stickiness" to the last choice, the model maintains an accumulating "action trace" (habit strength) that decays over time. High OCI scores are hypothesized to reduce the decay rate (increase the decay factor), causing habits to persist longer and accumulate more strength, leading to the observed "blocky" repetition of choices.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Action Trace Decay Model.
    
    Hypothesis: High OCI scores lead to slower decay of action traces (habits),
    resulting in stronger and longer-lasting perseveration (stickiness).
    
    Parameters:
    - learning_rate: [0,1] Rate of value updating.
    - beta: [0,10] Inverse temperature (determinism).
    - w: [0,1] Weighting of Model-Based vs Model-Free (1 = pure MB).
    - stick_weight: [0,5] Scaling factor for the influence of the action trace.
    - decay_base: [0,1] Base decay factor for the action trace (0 = instant decay, 1 = no decay).
    - decay_oci_slope: [0,1] Effect of OCI on the decay factor (positive = slower decay/more habit).
    """
    learning_rate, beta, w, stick_weight, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay factor. 
    # Logic: High OCI -> Higher decay factor (closer to 1) -> Slower forgetting of habit.
    # We clamp max decay to 0.99 to prevent explosion.
    decay_mod = decay_base + (0.99 - decay_base) * decay_oci_slope * oci_score
    decay_factor = np.clip(decay_mod, 0.0, 0.99)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Trace vector for the two spaceships
    trace = np.zeros(2)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value combines MB, MF, and the accumulated Habit Trace
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_weight * trace
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            
            # Update Action Trace
            # Decay all traces, then increment the chosen one
            trace = trace * decay_factor
            trace[a1] += 1.0
        else:
            p_choice_1[trial] = 0.5
            trace = trace * decay_factor # Decay even if missed

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        if state_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                # MF Stage 1 Update (SARSA-style)
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += learning_rate * delta_stage1
                
                # MF Stage 2 Update
                delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
                q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            else:
                p_choice_2[trial] = 0.5
        else:
            p_choice_2[trial] = 0.5

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Model-Free Drive Strength
This model separates the "drive strength" (inverse temperature) for the Model-Based (MB) and Model-Free (MF) systems. It hypothesizes that OCI specifically amplifies the `beta_mf` parameter. A higher `beta_mf` makes the MF component more dominant and more deterministic (rigid), while the MB component remains independent. This explains the combination of rigid choices and insensitivity to rare transitions (characteristic of MF).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Model-Free Drive Model.
    
    Hypothesis: OCI scores increase the inverse temperature (drive strength) 
    specifically for the Model-Free system, making behavior more rigid and 
    habit-driven relative to the Model-Based system.
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta_mb: [0,10] Inverse temperature for Model-Based values.
    - beta_mf_base: [0,10] Base Inverse temperature for Model-Free values.
    - beta_mf_oci_slope: [0,10] Increase in MF Beta per unit of OCI.
    - stickiness: [0,5] Choice perseverance bonus (simple 1-step).
    """
    learning_rate, beta_mb, beta_mf_base, beta_mf_oci_slope, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective Model-Free Beta
    beta_mf = beta_mf_base + beta_mf_oci_slope * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF using their specific betas (no 'w' parameter needed)
        # This allows OCI to change both the balance and the total determinism.
        logits = beta_mb * q_stage1_mb + beta_mf * q_stage1_mf
        
        if prev_a1 != -1:
            logits[prev_a1] += stickiness
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1
        else:
            p_choice_1[trial] = 0.5
            prev_a1 = -1

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        if state_idx != -1:
            # Use sum of betas as proxy for total precision in Stage 2
            beta_2 = beta_mb + beta_mf
            
            exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += learning_rate * delta_stage1
                
                delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
                q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            else:
                p_choice_2[trial] = 0.5
        else:
            p_choice_2[trial] = 0.5

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Value Forgetting
This model incorporates a "forgetting" mechanism where the values of unchosen options decay over time. It hypothesizes that OCI modulates this forgetting rate. Specifically, high OCI might lead to *lower* forgetting (obsessive retention of old values), preventing the participant from adapting to the slowly changing probabilities of the aliens.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Value Forgetting Model.
    
    Hypothesis: OCI scores modulate the forgetting rate of unchosen options.
    High OCI is hypothesized to reduce forgetting (slower decay), causing 
    participants to hold onto outdated value estimates for longer.
    
    Parameters:
    - learning_rate: [0,1] Value update rate for chosen options.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF weighting.
    - stickiness: [0,5] Choice perseverance.
    - forget_base: [0,1] Base forgetting rate for unchosen options.
    - forget_oci_slope: [-1,1] Modulation of forgetting rate by OCI.
    """
    learning_rate, beta, w, stickiness, forget_base, forget_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective forgetting rate
    # Bounded between 0 and 1
    raw_forget = forget_base + forget_oci_slope * oci_score
    forget_rate = np.clip(raw_forget, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1
        else:
            p_choice_1[trial] = 0.5
            prev_a1 = -1

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        if state_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += learning_rate * delta_stage1
                
                delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
                q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
                
                # --- Forgetting ---
                # Decay the value of the unchosen alien on the current planet
                unchosen_a2 = 1 - a2
                q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - forget_rate)
                
                # Also decay unchosen spaceship value
                unchosen_a1 = 1 - a1
                q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)
                
            else:
                p_choice_2[trial] = 0.5
        else:
            p_choice_2[trial] = 0.5

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```