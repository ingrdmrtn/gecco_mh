Here are the 3 proposed cognitive models.

### Model 1: OCI-Modulated Rare Transition Learning
This model hypothesizes that OCI symptoms influence how participants learn from "Rare" transitions (unexpected state changes). High OCI might lead to either over-interpreting rare events (increasing learning rate, treating them as significant) or ignoring them (decreasing learning rate, treating them as noise), compared to common transitions.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Rare Transition Learning Model.
    
    Hypothesis: OCI symptoms modulate the learning rate specifically following 
    Rare transitions (Action 0 -> State 1, or Action 1 -> State 0). 
    This reflects an altered processing of 'unexpected' causal structure events.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for common transitions and stage 2.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - stickiness: [-5, 5] Choice stickiness (perseveration).
    - rare_mod_base: [0, 5] Base multiplier for learning rate on rare transitions.
    - rare_mod_oci: [-5, 5] OCI slope modulating the rare learning multiplier.
    """
    learning_rate, beta, w, stickiness, rare_mod_base, rare_mod_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness

        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        # Determine if transition was Rare (Action != State assuming 0->0, 1->1 common)
        is_rare = (a1 != s_idx)
        
        # Calculate effective learning rate for Stage 1
        lr_eff = learning_rate
        if is_rare:
            mod = rare_mod_base + (rare_mod_oci * oci_score)
            # Ensure multiplier is non-negative
            mod = max(0.0, mod) 
            lr_eff = learning_rate * mod

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_eff * delta_stage1

        # Stage 2 update uses base learning rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    
    return log_loss
```

### Model 2: OCI-Modulated Habit Trace
This model replaces simple one-step stickiness with a decaying "Habit Trace". This trace accumulates over repeated choices of the same action and decays when not chosen. OCI modulates the *weight* of this habit trace, hypothesizing that high OCI correlates with stronger, more persistent compulsive habits that are harder to break.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Habit Trace Model.
    
    Hypothesis: Perseveration is driven by an accumulated 'habit trace' that decays 
    over time, rather than just the previous trial's choice. OCI modulates the 
    strength (weight) of this habit influence on choice.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - decay: [0, 1] Decay rate of the habit trace (1 = no decay, 0 = instant decay).
    - habit_w_base: [-5, 5] Base weight of the habit trace.
    - habit_w_oci: [-5, 5] OCI slope modulating the habit weight.
    """
    learning_rate, beta, w, decay, habit_w_base, habit_w_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit trace initialized to 0
    habit_trace = np.zeros(2)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Calculate habit bonus
        habit_weight = habit_w_base + (habit_w_oci * oci_score)
        habit_bonus = habit_weight * habit_trace

        logits_1 = beta * q_net + habit_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Update Habit Trace
        # Decay all traces, then increment chosen action
        habit_trace = habit_trace * decay
        habit_trace[a1] += 1.0

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    
    return log_loss
```

### Model 3: OCI-Modulated Post-Loss Beta Asymmetry
This model proposes that OCI symptoms affect the exploration/exploitation balance (Beta) specifically after a negative outcome (loss). A high OCI score might lead to "panic" (low beta/randomness) or "rigidity" (high beta) after a loss, distinct from behavior after a win.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Post-Loss Beta Asymmetry Model.
    
    Hypothesis: OCI affects the consistency of choice (Inverse Temperature Beta) 
    specifically after receiving a Loss (0 coins). This captures potential 
    dysregulation (either rigidity or collapse of strategy) following failure.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta_win: [0, 10] Beta used after a Reward (Win).
    - w: [0, 1] Mixing weight.
    - stickiness: [-5, 5] Simple choice stickiness.
    - beta_loss_mod_base: [0, 5] Base multiplier for Beta after a Loss.
    - beta_loss_mod_oci: [-5, 5] OCI slope for the post-loss Beta multiplier.
    """
    learning_rate, beta_win, w, stickiness, beta_loss_mod_base, beta_loss_mod_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Determine effective Beta
        beta_eff = beta_win # Default to win beta (or initial)
        if prev_reward == 0:
            # Apply modulation if previous trial was a loss
            mod = beta_loss_mod_base + (beta_loss_mod_oci * oci_score)
            mod = max(0.0, mod) # Ensure non-negative
            beta_eff = beta_win * mod

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness

        logits_1 = beta_eff * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy (Use base/win beta for simplicity in stage 2)
        logits_2 = beta_win * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        prev_action_1 = a1
        prev_reward = r

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    
    return log_loss
```