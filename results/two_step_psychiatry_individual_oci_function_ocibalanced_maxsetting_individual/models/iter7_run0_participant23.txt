Here are the 3 proposed cognitive models.

### Model 1: Pessimistic Model-Based Valuation
This model hypothesizes that individuals with higher OCI scores exhibit "pessimistic planning" in the Model-Based system. Instead of valuing a future state based solely on the best possible outcome (max Q), they incorporate a fear of the worst outcome (min Q), modulated by their OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pessimistic Model-Based Valuation.
    
    Hypothesis: High OCI participants exhibit anxiety-driven pessimism in planning.
    When calculating Model-Based values for the first stage, they do not assume
    optimal action at the second stage (max Q). Instead, they value the second stage
    as a weighted average of the best and worst case scenarios (max Q and min Q).
    The weight on the worst case (pessimism) increases with OCI.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Weight for Model-Based system (0=Pure MF, 1=Pure MB).
    stickiness: [0, 5] Choice persistence bonus.
    pessimism_slope: [0, 5] Scales the OCI influence on pessimistic valuation.
                     Pessimism factor rho = (pessimism_slope * oci) clipped to [0, 1].
    """
    lr, beta, w, stickiness, pessimism_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate pessimism factor rho based on OCI
    rho = np.clip(pessimism_slope * oci_score, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Q-values initialization
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        # Handle missing data if any (though description implies complete data)
        if np.isnan(a1):
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Model-Based Valuation with Pessimism ---
        # Standard MB takes max(Q) of next stage.
        # Pessimistic MB takes (1-rho)*max(Q) + rho*min(Q).
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        min_q_stage2 = np.min(q_stage2_mf, axis=1)
        
        val_stage2_pessimistic = (1.0 - rho) * max_q_stage2 + rho * min_q_stage2
        
        q_stage1_mb = transition_matrix @ val_stage2_pessimistic
        
        # Net Q-values for Stage 1
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Choice Probability Stage 1
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2) # prevent div by zero
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # Choice Probability Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # --- Value Updates ---
        # Prediction error for Stage 2 (Alien choice)
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += lr * delta_stage2
        
        # Prediction error for Stage 1 (Spaceship choice)
        # Using pure MF update: TD(0)
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1

    return log_loss
```

### Model 2: Stage-Specific Beta Asymmetry
This model proposes that OCI creates a dissociation between the "rigidity" of high-level choices (Spaceships) and low-level choices (Aliens). High OCI leads to crystallized, rigid choices at Stage 1 (higher beta), while Stage 2 remains more stochastic or baseline.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Beta Asymmetry.
    
    Hypothesis: OCI modulates the exploration-exploitation balance differently 
    across hierarchical levels. High OCI participants are hypothesized to be 
    more rigid/deterministic (higher inverse temperature beta) at the first 
    stage (Spaceship selection) relative to the second stage (Alien selection).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_stage2: [0, 10] Inverse temperature for the second stage.
    w: [0, 1] Weight for Model-Based system.
    stickiness: [0, 5] Choice persistence.
    beta_s1_slope: [0, 5] Multiplier slope for Stage 1 beta relative to Stage 2.
                   beta_stage1 = beta_stage2 * (1 + beta_s1_slope * oci).
    """
    lr, beta_stage2, w, stickiness, beta_s1_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 Beta
    beta_stage1 = beta_stage2 * (1.0 + beta_s1_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if np.isnan(a1):
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Use beta_stage1
        exp_q1 = np.exp(beta_stage1 * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # Policy Stage 2
        # Use beta_stage2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # Updates
        q_stage2_mf[s, a2] += lr * (r - q_stage2_mf[s, a2])
        q_stage1_mf[a1] += lr * (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        
        last_action_1 = a1

    return log_loss
```

### Model 3: OCI-Modulated Second Stage Learning Rate
This model suggests that OCI affects the reaction to immediate feedback (Stage 2 outcomes) differently than the structural propagation of value (Stage 1 updates). High OCI participants may "over-learn" from the immediate coin outcomes due to heightened sensitivity or anxiety.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Second Stage Learning Rate.
    
    Hypothesis: OCI modulates the learning rate specifically for the second stage 
    (immediate reward outcomes), while the first stage learning rate (temporal 
    difference from state transitions) remains at baseline. High OCI leads to 
    hyper-correction or over-sensitivity to the fluctuating alien rewards.
    
    Parameters:
    lr_base: [0, 1] Baseline learning rate (used for Stage 1).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based system.
    stickiness: [0, 5] Choice persistence.
    lr_s2_slope: [0, 5] Slope for modulating Stage 2 learning rate by OCI.
                 lr_stage2 = lr_base * (1 + lr_s2_slope * oci).
                 (Clipped to max 1.0).
    """
    lr_base, beta, w, stickiness, lr_s2_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate separate learning rates
    lr_stage1 = lr_base
    lr_stage2 = np.clip(lr_base * (1.0 + lr_s2_slope * oci_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if np.isnan(a1):
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Policy Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # Policy Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # Update Stage 2 using OCI-modulated rate
        q_stage2_mf[s, a2] += lr_stage2 * (r - q_stage2_mf[s, a2])
        
        # Update Stage 1 using baseline rate
        q_stage1_mf[a1] += lr_stage1 * (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        
        last_action_1 = a1

    return log_loss
```