Here are the 3 cognitive models based on the provided participant data and OCI score.

### Model 1: "Lose-Stay" Perseveration Model
This model hypothesizes that OCI symptoms specifically modulate **perseveration after a loss** (unrewarded trial). While general perseveration (stickiness) is common, individuals with high OCI (compulsivity) may be more prone to repeating an action that just failed ("checking" behavior), whereas low OCI individuals might switch.
*   **Mechanism**: The stickiness bonus added to the previously chosen spaceship is `persev_base` generally, but an additional `losestay_oci_slope * OCI` is added if the previous outcome was 0 coins.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI modulates 'Lose-Stay' behavior.
    
    Hypothesis: High OCI participants exhibit compulsive repetition (stickiness) 
    specifically after unrewarded trials (losses), distinct from general perseveration.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Softmax inverse temperature.
    - w: [0, 1] Weighting of Model-Based values (1=MB, 0=MF).
    - persev_base: [0, 5] Baseline tendency to repeat the previous choice (general stickiness).
    - losestay_oci_slope: [-5, 5] Additional stickiness added ONLY after a loss (0 reward), scaled by OCI.
    """
    learning_rate, beta, w, persev_base, losestay_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Transition matrix: A(0)->X(0) is 0.7, A(0)->Y(1) is 0.3, etc.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)      
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_action_1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # Skip invalid data
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Net Value
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        # Perseveration Logic
        if prev_action_1 != -1:
            stickiness = persev_base
            # If previous trial was a loss, add OCI-modulated lose-stay bonus
            if prev_reward == 0:
                stickiness += (losestay_oci_slope * participant_oci)
            
            q_net_stage1[prev_action_1] += stickiness

        # Choice Probability
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF Update (TD-0)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        prev_action_1 = a1
        prev_reward = r

    eps = 1e-10
    log_lik = np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps)) + np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps))
    return -log_lik
```

### Model 2: Independent MB/MF Beta Model
This model removes the weighting parameter `w` and instead uses separate inverse temperatures (betas) for the Model-Based and Model-Free systems. It hypothesizes that OCI modulates the **strength of the Model-Free (habit) drive** specifically.
*   **Mechanism**: The decision logit is `beta_mb * Q_MB + beta_mf * Q_MF`. `beta_mf` is modulated by OCI. High OCI might lead to stronger habit intrusion (higher `beta_mf`) regardless of the MB system's confidence.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Independent Channels Model where OCI modulates Model-Free strength (Beta MF).
    
    Hypothesis: Instead of a trade-off (w), MB and MF systems compete with independent strengths.
    OCI modulates the 'compulsive' habit strength (Beta MF) directly.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta_mb: [0, 10] Inverse temperature for the Model-Based system.
    - beta_mf_base: [0, 10] Baseline inverse temperature for the Model-Free system.
    - beta_mf_oci_slope: [-5, 5] Effect of OCI on Model-Free beta.
    - persev: [0, 5] General choice stickiness.
    """
    learning_rate, beta_mb, beta_mf_base, beta_mf_oci_slope, persev = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate OCI-modulated MF Beta (clipped to be non-negative)
    beta_mf = np.maximum(0.0, beta_mf_base + (beta_mf_oci_slope * participant_oci))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Independent contribution of MB and MF values to logits
        # Note: No 'w' parameter. Betas act as weights.
        logits = (beta_mb * q_stage1_mb) + (beta_mf * q_stage1_mf)

        if prev_action_1 != -1:
            logits[prev_action_1] += persev

        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Using an average beta or just beta_mf for stage 2? 
        # Usually Stage 2 is purely MF. We use beta_mf here for consistency with habit strength.
        exp_q2 = np.exp(beta_mf * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        prev_action_1 = a1

    eps = 1e-10
    log_lik = np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps)) + np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps))
    return -log_lik
```

### Model 3: Rare-Transition Learning Rate Model
This model hypothesizes that OCI affects how participants learn from **structural surprise** (Rare Transitions).
*   **Mechanism**: The model uses two learning rates for the Stage 1 Model-Free update: `lr_common` (standard) and `lr_rare` (modulated by OCI). High OCI might lead to either discounting rare transitions (treating them as noise) or over-reacting to them (treating them as a change in rules).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model where OCI modulates Learning Rate for Rare Transitions.
    
    Hypothesis: OCI affects sensitivity to 'structural surprise'. The learning rate used 
    to update Stage 1 values after a Rare transition differs based on OCI.
    
    Parameters:
    - lr_common: [0, 1] Learning rate after Common transitions.
    - lr_rare_base: [0, 1] Baseline learning rate after Rare transitions.
    - lr_rare_oci_slope: [-2, 2] Effect of OCI on Rare transition learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting of Model-Based values.
    """
    lr_common, lr_rare_base, lr_rare_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate Rare LR and clip to [0, 1]
    lr_rare = np.clip(lr_rare_base + (lr_rare_oci_slope * participant_oci), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Determine if transition was Common or Rare
        # Common: (A->X) or (U->Y). i.e., action matches state index.
        # Rare: (A->Y) or (U->X). i.e., action != state index.
        # Note: action_1=0 is A, state=0 is X. 
        is_common = (a1 == s_idx) 
        
        current_lr = lr_common if is_common else lr_rare

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

        # Stage 2 update always uses common/base assumption or we could reuse lr_common
        # Typically Stage 2 is just direct reward learning. We'll use lr_common for simplicity.
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_common * delta_stage2

    eps = 1e-10
    log_lik = np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps)) + np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps))
    return -log_lik
```