Here are three new cognitive models exploring different mechanisms for how OCI scores might influence reinforcement learning in this two-step task.

### Model 1: Hybrid RL with OCI-Modulated Model-Based Weight
This model tests the hypothesis that higher OCI scores relate to a reliance on habit (Model-Free) over goal-directed planning (Model-Based). It uses a hybrid reinforcement learning framework where the weighting parameter `w` (which balances Model-Based and Model-Free values) is a function of the OCI score. Specifically, it tests if higher OCI leads to a *lower* `w` (more Model-Free/habitual).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the balance between Model-Based (MB) and Model-Free (MF) 
    control is determined by the OCI score.
    
    Hypothesis: Higher OCI scores lead to more habitual (MF) behavior, reducing 'w'.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for MF value updates.
    beta: [0, 10] - Inverse temperature for softmax action selection.
    w_intercept: [0, 1] - Baseline weight for MB control (at OCI=0).
    w_slope: [-1, 1] - How OCI score adjusts 'w'. Negative implies high OCI -> more habit.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w based on OCI, bounded between 0 and 1
    w = w_intercept + (w_slope * oci_score)
    w = np.clip(w, 0.0, 1.0)

    # Fixed transition matrix for the task structure (A->X, B->Y commonly)
    # Rows: Choice 0 (A), Choice 1 (B)
    # Cols: State 0 (X), State 1 (Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_mf_stage1 = np.zeros(2)      # Model-free values for stage 1 choices
    q_mf_stage2 = np.zeros((2, 2)) # Model-free values for stage 2 states (aliens)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based valuation: V_MB(s1) = T * max(Q_MF(s2))
        # We use the max Q-value of the next stage as the value of the state
        max_q_stage2 = np.max(q_mf_stage2, axis=1) # Max value for each planet
        q_mb_stage1 = transition_matrix @ max_q_stage2
        
        # Hybrid valuation: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_mb_stage1 + (1 - w) * q_mf_stage1
        
        # Softmax policy for stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0 # Handle missing data

        # --- Stage 2 Decision ---
        
        s_idx = int(state[trial]) # The planet arrived at
        
        if s_idx != -1:
            # Standard MF policy for stage 2 (aliens)
            exp_q2 = np.exp(beta * q_mf_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                r = reward[trial]
                
                # Update Stage 2 MF values (TD error)
                delta_stage2 = r - q_mf_stage2[s_idx, a2]
                q_mf_stage2[s_idx, a2] += learning_rate * delta_stage2
                
                # Update Stage 1 MF values (TD(1) / Monte Carlo update using actual reward)
                # Note: Often in hybrid models, stage 1 MF is updated via lambda-return or simple SARSA.
                # Here we use simple SARSA(0) logic: update towards value of state 2 + reward? 
                # Or standard formulation: update towards Q(s2) or just R? 
                # Given the prompt's template suggesting separate updates, we use direct reward update for simplicity 
                # or the value of the chosen second stage option.
                # Let's use the standard TD(1)-like update often used in Daw et al.: update Q1 based on R.
                
                if a1 != -1:
                    delta_stage1 = r - q_mf_stage1[a1]
                    q_mf_stage1[a1] += learning_rate * delta_stage1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI scores relate to an asymmetry in how participants learn from positive versus negative outcomes. It posits that OCI traits might amplify sensitivity to negative outcomes (or lack of reward), leading to a higher learning rate for prediction errors that are negative (`learning_rate_neg`).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with asymmetric learning rates for positive and negative prediction errors.
    The OCI score modulates the negative learning rate specifically.
    
    Hypothesis: Higher OCI leads to faster updating (higher alpha) in response to 
    disappointments (negative RPEs), reflecting hypersensitivity to error/punishment.
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature.
    oci_neg_scale: [0, 5] - Scaling factor: lr_neg = lr_neg_base + (oci * scale).
    """
    lr_pos, lr_neg_base, beta, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (oci_neg_scale * oci_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0) # Ensure bounds

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # Stage 2 Policy
        s_idx = int(state[trial])
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                r = reward[trial]
                
                # --- Update Stage 2 ---
                delta2 = r - q_stage2[s_idx, a2]
                alpha2 = lr_pos if delta2 > 0 else lr_neg
                q_stage2[s_idx, a2] += alpha2 * delta2
                
                # --- Update Stage 1 ---
                # Using TD(1) style update: Stage 1 Q-value updated by final reward
                if a1 != -1:
                    delta1 = r - q_stage1[a1]
                    alpha1 = lr_pos if delta1 > 0 else lr_neg
                    q_stage1[a1] += alpha1 * delta1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration Modulated by OCI
This model proposes that OCI is linked to intolerance of uncertainty. Instead of simple softmax exploration, this model includes an "uncertainty bonus" (or penalty) added to the Q-values. The magnitude of this bonus is controlled by the OCI score. If the parameter is positive, high OCI leads to exploring uncertain options; if negative, it leads to avoiding them (safety behavior). Uncertainty is approximated here by an inverse count of visits (less visited = more uncertain).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with an Uncertainty Bonus/Penalty modulated by OCI.
    
    Hypothesis: OCI scores influence how uncertainty (approximated by 1/visit_count)
    affects value estimation. High OCI might lead to avoiding uncertain options (safety)
    or checking them (compulsion).
    
    Parameters:
    learning_rate: [0, 1] - Standard learning rate.
    beta: [0, 10] - Inverse temperature.
    phi_base: [-2, 2] - Base weight for uncertainty (positive=exploration, negative=avoidance).
    oci_phi_mod: [-2, 2] - How OCI modulates the uncertainty weight.
    """
    learning_rate, beta, phi_base, oci_phi_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective uncertainty parameter phi
    phi = phi_base + (oci_phi_mod * oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track visits to calculate uncertainty proxy
    # Initialize with 1 to avoid division by zero
    counts_stage1 = np.ones(2) 
    counts_stage2 = np.ones((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 ---
        # Uncertainty bonus: phi * (1 / sqrt(count))
        uncertainty_1 = 1.0 / np.sqrt(counts_stage1)
        q_net_1 = q_stage1 + (phi * uncertainty_1)
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            counts_stage1[a1] += 1
        else:
            p_choice_1[trial] = 1.0

        # --- Stage 2 ---
        s_idx = int(state[trial])
        if s_idx != -1:
            uncertainty_2 = 1.0 / np.sqrt(counts_stage2[s_idx])
            q_net_2 = q_stage2[s_idx] + (phi * uncertainty_2)
            
            exp_q2 = np.exp(beta * q_net_2)
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                counts_stage2[s_idx, a2] += 1
                
                r = reward[trial]
                
                # Update Q-values (standard Q-learning)
                delta2 = r - q_stage2[s_idx, a2]
                q_stage2[s_idx, a2] += learning_rate * delta2
                
                if a1 != -1:
                    delta1 = r - q_stage1[a1]
                    q_stage1[a1] += learning_rate * delta1
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```