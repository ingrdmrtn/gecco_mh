Here are three new cognitive models for the two-step task, incorporating the OCI score into different mechanisms: Exploration/Novelty, State-Dependent Uncertainty (Rare Transitions), and Heuristic Mixture (WSLS).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Exploration Bonus (Novelty/Checking).
    
    Hypothesis: OCI scores relate to an intolerance of uncertainty or a compulsion to 'check' 
    neglected options. This model adds an exploration bonus to the Stage 1 Q-values based on 
    the log-time since an option was last chosen. High OCI may amplify (checking) or 
    diminish (avoidance) this bonus.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - phi_base: [-2, 2] Baseline exploration bonus weight. Positive = seek novelty; Negative = stick to familiar.
    - phi_oci: [-2, 2] Modulation of exploration bonus by OCI.
      Effective phi = phi_base + phi_oci * oci.
    """
    learning_rate, beta, w, phi_base, phi_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    phi = phi_base + phi_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track when each action was last chosen. Initialize to -1 (start of task).
    last_chosen_trial = np.array([-1, -1])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Calculate Exploration Bonus
        # Time elapsed since last choice. If never chosen, trial number + 1.
        # Use log scale to model diminishing returns of novelty.
        # +2 ensures log argument is >= 1, so bonus is >= 0 (if phi > 0).
        time_since = trial - last_chosen_trial
        bonus = phi * np.log(time_since + 2) 
        
        # Policy Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add bonus to the net Q-values
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + bonus
        
        logits_1 = beta * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update last chosen tracker
        last_chosen_trial[a1] = trial
        
        # Policy Stage 2
        s_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated "Rare Transition Confusion" (Stage 2 Beta Modulation).
    
    Hypothesis: High OCI participants may experience cognitive interference or "confusion" 
    when a rare transition occurs, leading to more random choice behavior (lower beta) 
    specifically in Stage 2 after a rare transition, compared to common transitions.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_common: [0, 10] Inverse temperature for Stage 1 and Stage 2 (Common transitions).
    - w: [0, 1] Weighting MB/MF.
    - rare_damp_base: [0, 1] Baseline factor for beta after rare transition (0=random, 1=same as common).
    - rare_damp_oci: [-1, 1] Modulation of the dampening factor by OCI.
      Effective factor = clip(rare_damp_base + rare_damp_oci * oci, 0, 1).
      Beta_Rare = Beta_Common * Effective_Factor.
    """
    learning_rate, beta_common, w, rare_damp_base, rare_damp_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    damp_factor = rare_damp_base + rare_damp_oci * current_oci
    damp_factor = np.clip(damp_factor, 0.0, 1.0)
    
    beta_rare = beta_common * damp_factor
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Policy Stage 1 (Uses beta_common)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_common * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Determine if transition was rare
        s_idx = int(state[trial])
        # Common transitions: Action 0 -> State 0, Action 1 -> State 1 (based on 0.7 diag)
        # Rare transitions: Action 0 -> State 1, Action 1 -> State 0
        is_rare = (a1 != s_idx)
        
        # Apply dampened beta if transition was rare
        current_beta_s2 = beta_rare if is_rare else beta_common
        
        # Policy Stage 2
        logits_2 = current_beta_s2 * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Win-Stay Lose-Shift (WSLS) Mixture.
    
    Hypothesis: OCI affects the reliance on a simple heuristic (WSLS) versus the complex hybrid RL strategy.
    The model assumes the final policy is a mixture of RL probabilities and a deterministic WSLS rule.
    High OCI might correlate with increased reliance on simple, rigid rules (WSLS) or decreased reliance.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - mix_base: [0, 1] Baseline mixture weight for WSLS (0 = Pure RL, 1 = Pure WSLS).
    - mix_oci: [-1, 1] Modulation of mixture weight by OCI.
      Effective mix = clip(mix_base + mix_oci * oci, 0, 1).
    """
    learning_rate, beta, w, mix_base, mix_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    mix_w = mix_base + mix_oci * current_oci
    mix_w = np.clip(mix_w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = -1
    
    for trial in range(n_trials):
        # RL Policy Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_rl = exp_q1 / np.sum(exp_q1)
        
        # WSLS Heuristic Mixture
        # Logic: If prev reward=1, prob=1 for prev_a1. If prev reward=0, prob=1 for other.
        # If trial 0 (no history), default to RL (mix logic handled by conditional).
        
        probs_final = np.zeros(2)
        if prev_a1 != -1:
            wsls_target = prev_a1 if prev_reward == 1 else (1 - prev_a1)
            # Mix RL probability with 1.0 mass on wsls_target
            probs_final[wsls_target] = (1 - mix_w) * probs_rl[wsls_target] + mix_w
            probs_final[1 - wsls_target] = (1 - mix_w) * probs_rl[1 - wsls_target]
        else:
            probs_final = probs_rl
            
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_final[a1]
        
        # Stage 2 (Standard RL)
        s_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```