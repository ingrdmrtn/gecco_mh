Here are three new cognitive models that incorporate the OCI score to explain the participant's behavior in the two-step task. These models introduce variations in learning rates, perseveration, and mixing weights based on the hypothesis that medium-to-high OCI scores relate to altered habit formation or rigidity.

### Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that individuals with higher OCI scores exhibit higher "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of the outcome. This reflects the repetitive behaviors characteristic of OCD. The model adds a perseveration bonus to the Q-values of the previously chosen action, scaled by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model (MF/MB) with OCI-modulated choice perseveration (stickiness).
    
    Hypothesis: Higher OCI scores lead to increased 'stickiness' or tendency to repeat 
    the previous action (perseveration), regardless of reward history.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for Model-Based (1) vs Model-Free (0) control.
    - stickiness_base: [0, 5] Base level of choice repetition bonus.
    """
    learning_rate, beta, w, stickiness_base = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate effective stickiness: increases with OCI score
    # We assume stickiness scales linearly with symptom severity
    effective_stickiness = stickiness_base * (1.0 + oci_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action for stickiness (initialize to -1 meaning none)
    prev_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue # Skip missing data

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        # Create a copy so we don't modify the Q-values permanently
        q_choice = q_integrated.copy()
        if prev_action_1 != -1:
            q_choice[prev_action_1] += effective_stickiness

        exp_q1 = np.exp(beta * q_choice)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update previous action
        prev_action_1 = a1
        
        s_idx = int(state[trial])
        if s_idx == -1: continue

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Learning Updates ---
        # Stage 2 Update (standard Q-learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1 Update (TD-learning)
        # Using the value of the state actually reached
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: OCI-Based Imbalance in Learning Rates (Positive vs. Negative)
This model tests the hypothesis that OCI symptoms are related to an asymmetry in processing positive versus negative feedback. Specifically, it posits that higher OCI scores might lead to "hyper-learning" from negative outcomes (or lack of reward), reflecting a fear of failure or error. The model splits the learning rate into positive (`alpha_pos`) and negative (`alpha_neg`) components, where the negative learning rate is scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated asymmetry in learning rates.
    
    Hypothesis: Higher OCI leads to stronger learning from negative prediction errors 
    (lack of reward), reflecting enhanced error monitoring or fear of failure.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    """
    lr_pos, lr_neg_base, beta, w = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Effective negative learning rate scales with OCI
    # We clip it to 1.0 to ensure stability
    lr_neg = min(1.0, lr_neg_base * (1.0 + oci_val))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        if s_idx == -1: continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Learning Updates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        effective_lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += effective_lr_2 * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        effective_lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += effective_lr_1 * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Model-Based Weight (W)
This model tests the hypothesis that OCI scores directly impact the balance between goal-directed (Model-Based) and habitual (Model-Free) control. The parameter `w` (the mixing weight) is defined as a logistic function of the OCI score. This allows the model to capture whether higher OCI is associated with more rigid, habitual behavior (lower `w`) or excessive deliberation/planning (higher `w`).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model where the balance between Model-Based and Model-Free control (w) 
    is a function of the OCI score.
    
    Hypothesis: OCI score predicts the reliance on goal-directed (MB) vs habitual (MF) systems.
    We model w using a sigmoid function centered on the OCI score to bound it between 0 and 1.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_slope: [-5, 5] Slope of the relationship between OCI and w. 
                 Positive slope -> Higher OCI = More Model-Based.
                 Negative slope -> Higher OCI = More Model-Free.
    - w_intercept: [-5, 5] Bias term for the sigmoid.
    """
    learning_rate, beta, w_slope, w_intercept = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate w based on OCI using a sigmoid function
    # w = 1 / (1 + exp(-(slope * oci + intercept)))
    # This dynamically determines the mixing weight for this participant
    w_logit = w_slope * oci_val + w_intercept
    w = 1.0 / (1.0 + np.exp(-w_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mixing MB and MF values
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        if s_idx == -1: continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        # --- Learning Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```