Here are three new cognitive models for the two-step task, incorporating the participant's OCI score to modulate specific learning and decision-making mechanisms.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Trace (Lambda).
    
    Hypothesis:
    OCI scores influence the 'eligibility trace' (lambda), which controls how much 
    the Stage 2 outcome (reward) directly updates the Stage 1 value. 
    A higher OCI score is hypothesized to lead to a stronger coupling between stages 
    (higher lambda), reflecting a tendency to reinforce the entire behavioral chain 
    based on the final outcome, potentially related to the persistence of compulsions.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - lambda_k: [0, 2] Scaling factor for OCI to determine lambda. lambda = clip(lambda_k * oci).
    """
    lr, beta, w, lambda_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lambda based on OCI
    # Lambda controls the mix between TD(0) (lambda=0) and Monte Carlo (lambda=1) updates for Stage 1
    lam = np.clip(lambda_k * oci_score, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Stage 1 prediction error (TD(0))
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        
        # Stage 2 prediction error
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        # Update Stage 1 with eligibility trace
        # If lambda > 0, the Stage 2 error (delta_stage2) propagates back to Stage 1 directly
        q_stage1_mf[a1] += lr * (delta_stage1 + lam * delta_stage2)
        
        # Update Stage 2
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Belief Distortion.
    
    Hypothesis:
    Participants with different OCI scores may hold distorted beliefs about the 
    reliability of the spaceship transitions. While the true common transition 
    probability is 0.7, OCI might correlate with believing the world is more 
    deterministic (higher prob) or more chaotic (lower prob). This distorts the 
    Model-Based value calculation, as the participant uses a subjective transition matrix.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - distort_k: [-0.5, 0.5] Distortion factor. P_belief = 0.7 + distort_k * (oci - 0.5).
    """
    lr, beta, w, distort_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Distort transition matrix based on OCI
    # Center OCI around 0.5 for stability. 
    # If distort_k is positive, high OCI -> belief in more deterministic transitions.
    p_common = 0.7 + distort_k * (oci_score - 0.5)
    p_common = np.clip(p_common, 0.0, 1.0)
    
    # Internal subjective model of transitions
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # MB values are calculated using the distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Temperature.
    
    Hypothesis:
    OCI scores may differentially affect the exploration/exploitation balance 
    at the second stage (immediate reward) compared to the first stage (strategic planning). 
    High OCI might be associated with higher anxiety or a need for precision at the 
    point of reward receipt, leading to a different inverse temperature (beta) 
    specifically for Stage 2 choices.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - w: [0, 1] Weighting between MB and MF.
    - beta_2_scale: [-5, 5] Scaling factor for Stage 2 beta. beta_2 = beta_1 * (1 + beta_2_scale * (oci - 0.5)).
    """
    lr, beta_1, w, beta_2_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 beta
    # beta_2 is derived from beta_1 but modulated by OCI
    beta_2 = beta_1 * (1.0 + beta_2_scale * (oci_score - 0.5))
    beta_2 = np.clip(beta_2, 0.0, 20.0) # Clip to prevent overflow
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy (uses beta_2) ---
        # The decision noise here is distinct from Stage 1 based on OCI
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood
```