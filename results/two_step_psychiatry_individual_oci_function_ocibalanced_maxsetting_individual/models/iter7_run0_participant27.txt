Here are the three cognitive models based on the provided participant data and task description.

### Model 1: OCI-Modulated Transition Belief (Intolerance of Uncertainty)
This model hypothesizes that OCI symptoms, often associated with an intolerance of uncertainty, lead participants to distort the probabilistic nature of the transition matrix during Model-Based planning. High OCI scores may cause the participant to treat the common transition (0.7) as more deterministic (closer to 1.0) or more chaotic, effectively altering their "trust" in the model-based structure.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI distorts the subjective transition probabilities 
    used for Model-Based planning.
    
    Hypothesis: OCI (Intolerance of Uncertainty) causes the agent to perceive 
    the transition structure as more deterministic or distorted compared to reality.
    
    Parameters:
    - lr: Learning rate for MF value updates [0, 1]
    - beta: Inverse temperature for softmax [0, 10]
    - w: Mixing weight between MB and MF systems (0=MF, 1=MB) [0, 1]
    - distort_base: Baseline distortion of the 0.7 probability [0, 0.3]
    - distort_oci: OCI-dependent modulation of the distortion [-1, 1]
    """
    lr, beta, w, distort_base, distort_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate subjective transition probability
    # True probability is 0.7. distortion shifts this.
    # We clip to ensure it stays a valid probability > 0.5 (maintaining structure)
    raw_distortion = distort_base + (distort_oci * current_oci)
    perceived_p = np.clip(0.7 + raw_distortion, 0.5, 0.99)
    
    # Subjective transition matrix used for planning
    transition_matrix = np.array([[perceived_p, 1 - perceived_p], 
                                  [1 - perceived_p, perceived_p]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Expected max value of next stage weighted by transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF Update (SARSA-like using Stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF Update (Prediction Error from Reward)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage 2 Learning Rate
This model proposes that OCI affects learning from immediate, concrete outcomes (Stage 2) differently than prospective, abstract updates (Stage 1). It introduces separate learning rates for the two stages, with the Stage 2 learning rate specifically modulated by the OCI score. This reflects the hypothesis that compulsive symptoms might be driven by hypersensitivity (or hyposensitivity) to direct feedback.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with separate learning rates for Stage 1 and Stage 2.
    The Stage 2 learning rate is modulated by OCI.
    
    Hypothesis: OCI affects the sensitivity to direct feedback (Stage 2 reward) 
    differently than the abstract value chaining in Stage 1.
    
    Parameters:
    - lr_s1: Learning rate for Stage 1 MF updates [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight [0, 1]
    - lr_s2_base: Base learning rate for Stage 2 [0, 1]
    - lr_s2_oci_slope: How OCI changes the Stage 2 learning rate [-1, 1]
    """
    lr_s1, beta, w, lr_s2_base, lr_s2_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate OCI-modulated Stage 2 learning rate
    lr_s2 = lr_s2_base + (lr_s2_oci_slope * current_oci)
    lr_s2 = np.clip(lr_s2, 0.0, 1.0)
    
    # Fixed transition matrix for MB calculation
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 uses lr_s1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Stage 2 uses OCI-modulated lr_s2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stickiness Decay
This model addresses the repetitive behavior (streaks) observed in the participant data. It implements a "stickiness" trace that biases the agent to repeat the previous choice. The key innovation is that the *decay rate* of this stickiness is modulated by OCI. Higher OCI leads to slower decay (longer-lasting stickiness), simulating the "stuck" nature of compulsive symptoms.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with an OCI-modulated choice stickiness decay.
    
    Hypothesis: OCI determines how quickly the urge to repeat a previous choice 
    decays over time. Higher OCI results in slower decay (more perseverance).
    
    Parameters:
    - lr: Learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight [0, 1]
    - stick_mag: Magnitude of stickiness added upon choice [0, 5]
    - decay_oci_coeff: Effect of OCI on the decay rate [-1, 1]
      (Decay = 0.5 - coeff * OCI). Positive coeff means High OCI -> Slower Decay.
    """
    lr, beta, w, stick_mag, decay_oci_coeff = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate Decay Rate
    # Base decay is 0.5. OCI modifies this.
    # If coeff is positive, higher OCI reduces decay (values stick around longer).
    decay_rate = 0.5 - (decay_oci_coeff * current_oci)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    stickiness_trace = np.zeros(2) # Trace for Stage 1 actions
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Stickiness
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + stickiness_trace
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Update Stickiness Trace
        # Decay existing trace
        stickiness_trace *= (1 - decay_rate)
        # Add stickiness to chosen action
        stickiness_trace[a1] += stick_mag

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```