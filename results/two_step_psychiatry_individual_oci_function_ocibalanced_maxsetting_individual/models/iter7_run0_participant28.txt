Here are three new cognitive models that incorporate the OCI score to explain participant behavior in the two-step task.

### Model 1: Subjective Transition Belief Distortion
This model hypothesizes that OCI symptoms distort the Model-Based system's belief about the transition probabilities. While the true common transition probability is 0.7, participants with high OCI might perceive this relationship as more deterministic (rigidity) or less reliable (doubt/uncertainty), affecting how they calculate Model-Based values.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI modulates the subjective 
    transition probability used by the Model-Based system.
    
    High OCI may lead to distorted beliefs about transition probabilities 
    (e.g., perceiving transitions as more deterministic or more random than reality).
    
    Parameters:
    lr: [0, 1] Learning rate for MF values.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    trans_base: [0, 1] Base subjective probability of common transition (mapped to [0.5, 1.0]).
    oci_trans_mod: [0, 1] Modulation of transition belief by OCI.
    """
    lr, beta, w, trans_base, oci_trans_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map parameters to subjective probability [0.5, 1.0]
    # Center modulation around 0 (-1 to +1 range)
    mod_val = (oci_trans_mod - 0.5) * 2.0 
    
    # Calculate base probability from 0.5 to 1.0
    base_prob = 0.5 + 0.5 * trans_base
    
    # Apply OCI modulation and clip
    p_common = np.clip(base_prob + mod_val * oci_score, 0.5, 1.0)
    
    # Construct Subjective Transition Matrix
    # T = [[p, 1-p], [1-p, p]]
    transition_matrix = np.array([[p_common, 1.0 - p_common], [1.0 - p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy (MB + MF)
        # MB Values calculated using subjective matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Stage 1 update (SARSA style)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Forgetting
This model tests whether OCI symptoms are related to the rate at which unchosen option values decay ("forgetting"). High OCI might be associated with "hyper-focus" (rapid forgetting of alternatives) or "rumination" (slower forgetting/maintenance of all values).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with OCI-modulated forgetting for unchosen options.
    
    This tests if OCI affects the decay rate of value estimates for actions not taken.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    forget_base: [0, 1] Base forgetting rate for unchosen options.
    oci_forget_mod: [0, 1] Modulation of forgetting rate by OCI.
    """
    lr, beta, w, forget_base, oci_forget_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate forgetting rate
    mod_val = (oci_forget_mod - 0.5) * 2.0
    forget_rate = np.clip(forget_base + mod_val * oci_score, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Stage 1 MF update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        # Forgetting for unchosen stage 1 action
        q_stage1_mf[1-a1] *= (1.0 - forget_rate)
        
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        # Forgetting for unchosen stage 2 action in current state
        q_stage2_mf[s_idx, 1-a2] *= (1.0 - forget_rate)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Compulsive "Lose-Stay" Perseveration
Compulsivity is often characterized by the repetition of behaviors despite negative outcomes. This model introduces a specific perseveration parameter that is applied only after a loss (reward = 0), with the strength of this "Lose-Stay" tendency modulated by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with OCI-modulated "Lose-Stay" perseveration.
    
    This model tests if OCI symptoms lead to compulsive repetition of choices specifically 
    after negative outcomes (rewards of 0).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    persev_loss_base: [0, 1] Base perseveration bonus after a loss (scaled).
    oci_persev_mod: [0, 1] Modulation of loss-perseveration by OCI.
    """
    lr, beta, w, persev_loss_base, oci_persev_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate perseveration strength for losses
    # Map [0,1] parameters to a reasonable logit range, e.g., [-2, 2]
    mod_val = (oci_persev_mod - 0.5) * 5.0 
    base_val = persev_loss_base * 4.0 - 2.0
    persev_strength = base_val + mod_val * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = -1.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to logits only if previous trial resulted in no reward
        logits = beta * q_net
        if prev_a1 != -1 and prev_reward == 0.0:
             logits[prev_a1] += persev_strength
        
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        prev_a1 = a1
        prev_reward = r
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```