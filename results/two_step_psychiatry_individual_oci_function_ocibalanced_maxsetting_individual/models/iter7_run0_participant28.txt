Here are the 3 new cognitive models as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Model.
    
    This model assumes that the participant's OCI score modulates their *belief* about 
    the transition structure of the task, rather than learning it or using the true one.
    High OCI might lead to rigid (deterministic) or uncertain (random) beliefs about 
    which spaceship goes to which planet, distorting the Model-Based value calculation.
    
    Parameters:
    learning_rate: [0,1] - Rate of updating Q-values.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Weighting between Model-Based and Model-Free values.
    p_stick: [0,1] - Choice stickiness (perseveration) on Stage 1 action.
    lambda_val: [0,1] - Eligibility trace for Stage 2 reward updating Stage 1.
    trans_base: [0,1] - Baseline subjective probability of the common transition.
    trans_oci: [-1,1] - Modulation of subjective transition probability by OCI.
    """
    learning_rate, beta, w, p_stick, lambda_val, trans_base, trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # We center OCI around typical values for stability
    subj_trans_prob = trans_base + trans_oci * (oci_score - 0.5)
    subj_trans_prob = np.clip(subj_trans_prob, 0.0, 1.0)
    
    # Subjective transition matrix: [[P(X|A), P(Y|A)], [P(X|U), P(Y|U)]]
    # Assuming symmetry: P(X|A) = P(Y|U) = subj_trans_prob
    transition_matrix = np.array([
        [subj_trans_prob, 1.0 - subj_trans_prob], 
        [1.0 - subj_trans_prob, subj_trans_prob]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(S1) = T * max(Q_S2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stick * 5.0  # Scale stickiness to be comparable to Q-values
            
        # Softmax
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 MF update (TD(lambda))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Modulation of Control and Stickiness.
    
    This model posits that OCI simultaneously affects the balance of Model-Based control (w)
    AND the degree of response stickiness (p). This tests the hypothesis that high OCI
    phenotype is characterized by a "double hit": reduced planning/flexibility (low w)
    and increased compulsive repetition (high p).
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1] - Baseline MB weight.
    w_oci: [-1,1] - OCI modulation of MB weight.
    p_base: [0,1] - Baseline stickiness (scaled by 5).
    p_oci: [-1,1] - OCI modulation of stickiness.
    lambda_val: [0,1]
    """
    learning_rate, beta, w_base, w_oci, p_base, p_oci, lambda_val = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate modulated parameters
    w = w_base + w_oci * (oci_score - 0.5)
    w = np.clip(w, 0.0, 1.0)
    
    p_stick = p_base + p_oci * (oci_score - 0.5)
    p_stick = np.clip(p_stick, 0.0, 1.0) # Stickiness parameter bounds
    
    # Fixed transition matrix for MB
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stick * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stimulus (Planet) Stickiness Model.
    
    This model introduces "Stimulus Stickiness" modulated by OCI. Unlike response stickiness 
    (repeating the motor action), stimulus stickiness is a drive to return to the 
    previously visited state (Planet). If the participant visited Planet X, they are biased 
    to choose the spaceship (A) that commonly leads to X, regardless of whether they chose A 
    or U last time (due to rare transitions).
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    p_resp: [0,1] - Standard response stickiness (repeat action).
    p_stim_base: [0,1] - Baseline stimulus stickiness (seek previous planet).
    p_stim_oci: [-1,1] - OCI modulation of stimulus stickiness.
    lambda_val: [0,1]
    """
    learning_rate, beta, w, p_resp, p_stim_base, p_stim_oci, lambda_val = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stimulus stickiness
    p_stim = p_stim_base + p_stim_oci * (oci_score - 0.5)
    p_stim = np.clip(p_stim, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_state = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Response Stickiness (Action Repetition)
        if last_action_1 != -1:
            logits_1[last_action_1] += p_resp * 5.0
            
        # Stimulus Stickiness (Planet Seeking)
        if last_state != -1:
            # If last state was 0 (Planet X), boost Action 0 (A -> X common)
            # If last state was 1 (Planet Y), boost Action 1 (U -> Y common)
            # This assumes the agent uses the 'Common' mapping to express state-stickiness.
            target_action = last_state 
            logits_1[target_action] += p_stim * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_state = s_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```