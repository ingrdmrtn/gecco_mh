Here are three cognitive models based on the participant's data and OCI score.

### Model 1: OCI-Modulated Spatial Bias
**Hypothesis:** The participant shows a strong preference for "Spaceship 0" (Action 0), choosing it in over 75% of trials. This behavior suggests a "default" or "safe" option bias. This model hypothesizes that OCI symptoms (associated with rigidity or avoidance) modulate a fixed bias towards Action 0, independent of the learned values. High OCI might increase the tendency to stick to the "default" choice.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Spatial Bias Model.
    
    This model incorporates a fixed bias towards Action 0 (Spaceship A) in the first stage.
    The magnitude of this bias is modulated by the participant's OCI score.
    
    Parameters:
    learning_rate: [0, 1] - Rate of updating Q-values.
    beta: [0, 10] - Inverse temperature for softmax (exploration/exploitation balance).
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0) control.
    stickiness: [0, 5] - Perseveration bonus for repeating the previous Stage 1 choice.
    bias_base: [-5, 5] - Baseline bias towards Action 0.
    bias_oci: [-5, 5] - Modulation of the bias by OCI score.
    """
    learning_rate, beta, w, stickiness, bias_base, bias_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective bias for Action 0
    # If positive, increases prob of choosing 0. If negative, decreases.
    action_bias = bias_base + bias_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of max stage 2 Q-values given transition probs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness (Perseveration)
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        # Add OCI-Modulated Bias to Action 0
        q_net[0] += action_bias
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # TD Update for Stage 1 (Model-Free)
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD Update for Stage 2
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage 2 Precision (Split Beta)
**Hypothesis:** The participant appears to have different levels of randomness in the spaceship choice (Stage 1) versus the alien choice (Stage 2). Specifically, they show sustained blocks of choosing the same alien (e.g., Alien 1 on Planet 1), suggesting high precision (exploitation) at Stage 2. This model splits the inverse temperature parameter ($\beta$) into $\beta_1$ and $\beta_2$, and hypothesizes that OCI specifically modulates $\beta_2$. High OCI might lead to more rigid/deterministic "checking" behavior once a state (planet) is reached.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Precision Model.
    
    This model separates the exploration/exploitation trade-off for Stage 1 and Stage 2.
    It hypothesizes that OCI affects the precision (beta) of the second-stage choice
    (choosing the alien), potentially reflecting compulsive checking behavior.
    
    Parameters:
    learning_rate: [0, 1]
    beta1: [0, 10] - Inverse temperature for Stage 1 (Spaceship choice).
    beta2_base: [0, 10] - Baseline inverse temperature for Stage 2.
    beta2_oci: [-5, 5] - Modulation of Stage 2 beta by OCI.
    w: [0, 1] - Weighting between Model-Based and Model-Free control.
    stickiness: [0, 5] - Perseveration bonus for Stage 1.
    """
    learning_rate, beta1, beta2_base, beta2_oci, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 Beta (bounded to be non-negative)
    # High beta -> more deterministic (exploitation/rigidity)
    beta2 = beta2_base + beta2_oci * oci_score
    if beta2 < 0:
        beta2 = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        # Use beta1 for Stage 1
        exp_q1 = np.exp(beta1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Use OCI-modulated beta2 for Stage 2
        exp_q2 = np.exp(beta2 * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Model-Based Risk Aversion
**Hypothesis:** In standard Model-Based control, the value of a future state is the *maximum* available Q-value in that state (assuming optimal future action). This model hypothesizes that OCI correlates with "pessimism" or "risk aversion" in planning. Instead of assuming the best outcome ($\max Q$), the participant might aggregate future possibilities using a weighted average of the best and worst outcomes ($\max Q$ and $\min Q$). High OCI increases the weight on the worst outcome (min Q), reflecting a fear of failure or avoidance motivation.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Model-Based Risk Aversion (Pessimism) Model.
    
    Modifies the Model-Based value calculation. Instead of V(s') = max(Q(s',:)),
    it uses a weighted mix: V(s') = rho * min(Q(s',:)) + (1-rho) * max(Q(s',:)).
    'rho' represents pessimism/risk-aversion and is modulated by OCI.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stickiness: [0, 5]
    rho_base: [-5, 5] - Base parameter for risk aversion (transformed via sigmoid).
    rho_oci: [-5, 5] - Modulation of risk aversion by OCI.
    """
    learning_rate, beta, w, stickiness, rho_base, rho_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Risk Aversion parameter rho via sigmoid to keep in [0, 1]
    # rho -> 1 implies pure pessimism (min Q), rho -> 0 implies pure optimism (max Q)
    rho_logit = rho_base + rho_oci * oci_score
    rho = 1.0 / (1.0 + np.exp(-rho_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Calculate Pessimistic Expected Value for future states
        max_q_vals = np.max(q_stage2_mf, axis=1)
        min_q_vals = np.min(q_stage2_mf, axis=1)
        
        # Weighted aggregation based on OCI-modulated risk aversion
        val_stage2 = rho * min_q_vals + (1 - rho) * max_q_vals
        
        q_stage1_mb = transition_matrix @ val_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```