Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might modulate decision-making in the two-step task.

### Model 1: OCI-Modulated Second-Stage "Stickiness" (Perseverance)
This model hypothesizes that OCI scores influence the tendency to repeat the previous second-stage action (stickiness), regardless of reward. High OCI might lead to habitual repetition of specific motor actions at the second stage (e.g., "I always press left on Planet X"), reflecting a compulsion-like rigidity.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Second-Stage Stickiness.
    
    This model posits that OCI affects choice perseverance (stickiness) specifically 
    at the second stage (alien choice). High OCI might lead to rigid repetition 
    of the previous action taken in that state, regardless of value.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between model-based and model-free values (0=MF, 1=MB).
    - stick_base: [0, 5] Baseline stickiness bonus added to the previously chosen action.
    - stick_oci_mod: [-5, 5] Modulation of stickiness by OCI score.
    """
    learning_rate, beta, w, stick_base, stick_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective stickiness based on OCI
    # Can be positive (perseveration) or negative (switching)
    stickiness = stick_base + (stick_oci_mod * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for each state (planet) to apply stickiness
    # Initialize with -1 (no previous action)
    last_action_stage2 = np.array([-1, -1]) 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Add stickiness bonus to the Q-values before softmax
        q_net_s2 = q_stage2_mf[s2].copy()
        
        if last_action_stage2[s2] != -1:
            q_net_s2[last_action_stage2[s2]] += stickiness

        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Update last action for this state
        last_action_stage2[s2] = a2

        # --- Value Updates ---
        # Stage 2 update (Model-Free)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (Model-Free TD)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Confirmation Bias in Learning
This model suggests that individuals with varying OCI levels might process positive and negative prediction errors differently. Specifically, it tests if OCI modulates the learning rate specifically for *positive* outcomes (confirmation bias), potentially reflecting a need for certainty or safety signals.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Confirmation Bias (Asymmetric Learning Rates).
    
    This model allows for different learning rates for positive vs. negative prediction errors.
    The learning rate for positive errors (alpha_pos) is modulated by OCI, while the 
    learning rate for negative errors (alpha_neg) is fixed. This captures potential 
    biases in how obsessionality relates to processing rewards (confirmation) vs. lack of rewards.
    
    Parameters:
    - alpha_neg: [0, 1] Learning rate for negative prediction errors.
    - alpha_pos_base: [0, 1] Base learning rate for positive prediction errors.
    - alpha_pos_oci: [-1, 1] Modulation of positive learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    """
    alpha_neg, alpha_pos_base, alpha_pos_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective positive learning rate, bounded [0, 1]
    alpha_pos = alpha_pos_base + (alpha_pos_oci * oci_score)
    alpha_pos = min(1.0, max(0.0, alpha_pos))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Updates ---
        # Stage 2
        delta_stage2 = r - q_stage2_mf[s2, a2]
        lr_s2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[s2, a2] += lr_s2 * delta_stage2
        
        # Stage 1
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        lr_s1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Model-Based Uncertainty (Beta Modulation)
This model proposes that high OCI scores might lead to a lack of confidence in the complex, model-based strategy. Instead of affecting the mixing weight ($w$) directly, OCI modulates the inverse temperature ($\beta$) specifically for the model-based component, making the model-based influence "noisier" or less decisive for individuals with higher symptoms.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Model-Based Precision.
    
    This model assumes separate precision (inverse temperature) parameters for the 
    Model-Free (MF) and Model-Based (MB) systems. The precision of the MB system 
    is modulated by OCI. High OCI might degrade the precision of the internal model 
    (lower beta_mb), making MB planning less effective, even if 'w' is high.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_mf: [0, 10] Inverse temperature for Model-Free values (and Stage 2).
    - beta_mb_base: [0, 10] Base inverse temperature for Model-Based values.
    - beta_mb_oci: [-5, 5] Modulation of MB beta by OCI.
    - w: [0, 1] Mixing weight.
    """
    learning_rate, beta_mf, beta_mb_base, beta_mb_oci, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective MB beta
    beta_mb = beta_mb_base + (beta_mb_oci * oci_score)
    beta_mb = max(0.0, beta_mb) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Instead of mixing Q-values, we mix the logits (weighted by their specific betas)
        # This treats MB and MF as separate voting systems with different confidence levels
        logit_mf = beta_mf * q_stage1_mf
        logit_mb = beta_mb * q_stage1_mb
        
        # Combined logit
        combined_logit = (w * logit_mb) + ((1 - w) * logit_mf)
        
        exp_q1 = np.exp(combined_logit)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Stage 2 is purely model-free, uses beta_mf
        exp_q2 = np.exp(beta_mf * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Value Updates ---
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```