Here are 3 new cognitive models that incorporate the OCI-R score to explain the participant's behavior.

### Cognitive Model 1: OCI-Modulated Eligibility Trace (Lambda)
This model hypothesizes that OCI modulates the strength of the "eligibility trace" ($\lambda$) in the reinforcement learning process. An eligibility trace determines how much the final outcome (reward at stage 2) directly reinforces the stage 1 choice, bypassing the model-based state structure. A high $\lambda$ indicates a stronger reliance on simple temporal contiguity (habitual chaining), effectively blurring the distinction between the two stages. High OCI might correlate with a higher tendency to form these direct, rigid associations between initial action and final outcome.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI modulates the eligibility trace parameter (lambda). 
    Higher lambda means the Stage 2 reward outcome directly updates the Stage 1 
    Q-value more strongly, representing a 'direct habit' or caching of the 
    outcome sequence, bypassing the step-by-step structure.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Weight for Model-Based control
    lambda_base: [0, 1] - Base eligibility trace
    lambda_oci_slope: [-1, 1] - Effect of OCI on lambda
    """
    learning_rate, beta, w, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate OCI-dependent lambda
    lambda_val = lambda_base + (lambda_oci_slope * oci_score)
    lambda_val = np.clip(lambda_val, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value mixes MB and MF
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        if r != -1: # Skip invalid trials
            # Stage 1 TD Error (based on Stage 2 value)
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            # Stage 2 TD Error (based on Reward)
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2
            
            # Eligibility Trace Update: Apply Stage 2 error to Stage 1 value
            # This reinforces the stage 1 choice based on the final reward directly
            q_stage1_mf[a1] += learning_rate * lambda_val * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Subjective Transition Belief
This model assumes that while the objective transition probabilities are 0.7/0.3, the participant constructs a *subjective* model of the world where the strength of this transition belief is modulated by OCI. High OCI participants might perceive the world as more deterministic (superstitious rigidity, belief near 1.0) or more chaotic (uncertainty, belief near 0.5), regardless of the actual statistics. This alters the Model-Based value calculation significantly.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Subjective Transition Belief.
    
    Hypothesis: OCI modulates the subjective probability assigned to the 'common' 
    transition in the Model-Based planner. High OCI might lead to rigid beliefs 
    (perceiving transitions as more deterministic than they are) or uncertainty 
    (perceiving them as more random).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    belief_base: [0.5, 1.0] - Base subjective probability of common transition
    belief_oci_slope: [-0.5, 0.5] - Effect of OCI on belief strength
    """
    learning_rate, beta, w, belief_base, belief_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability
    # We clip between 0.5 (random) and 1.0 (deterministic)
    subjective_p = belief_base + (belief_oci_slope * oci_score)
    subjective_p = np.clip(subjective_p, 0.5, 0.99)
    
    # Construct the subjective transition matrix
    # Row 0: Space A -> [Prob(X), Prob(Y)]
    # Row 1: Space B -> [Prob(X), Prob(Y)] (Inverse of A)
    # Assuming A->X is common and B->Y is common
    transition_matrix = np.array([
        [subjective_p, 1 - subjective_p], 
        [1 - subjective_p, subjective_p]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use subjective matrix for MB calculation
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        if r != -1:
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Loss Sensitivity
This model proposes that OCI affects how the absence of a reward (0 coins) is perceived. Instead of treating 0 coins as a neutral zero value, participants with high OCI might perceive it as a "loss" or punishment. This "disappointment" or loss aversion signal is then fed into the learning update.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Loss Sensitivity.
    
    Hypothesis: OCI modulates the perceived magnitude of a 'loss' (receiving 0 coins).
    Instead of 0, the effective reward becomes negative (-loss_magnitude). 
    High OCI may be associated with higher sensitivity to failure, driving 
    stronger avoidance learning when no gold is found.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    loss_base: [0, 2] - Base magnitude of negative reward for 0 coins
    loss_oci_slope: [-1, 1] - Effect of OCI on loss magnitude
    """
    learning_rate, beta, w, loss_base, loss_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective loss magnitude
    loss_mag = loss_base + (loss_oci_slope * oci_score)
    loss_mag = np.clip(loss_mag, 0.0, 5.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        if r != -1:
            # Transform reward: 0 becomes negative loss
            effective_r = r
            if r == 0:
                effective_r = -loss_mag
            
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            # Update stage 2 using effective reward (potentially negative)
            delta_stage2 = effective_r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```