Here are three new cognitive models designed to explore different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Eligibility Traces
This model tests the hypothesis that higher OCI scores relate to how much credit is assigned to the first-stage choice based on the second-stage outcome. Instead of a fixed mixture of model-based and model-free control, this model uses a reinforcement learning parameter called "eligibility trace" ($\lambda$). A higher $\lambda$ means the first-stage action is updated more strongly by the final reward (effectively linking the two stages more tightly in a model-free manner). We hypothesize OCI modulates this trace, perhaps reflecting a "mental stickiness" or perseveration on the chain of events.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Traces (Lambda).
    
    This model posits that OCI affects the 'eligibility trace' (lambda), which controls
    how much the second-stage prediction error updates the first-stage value directly.
    A high lambda makes the agent more 'model-free' in a temporal difference sense 
    (updating stage 1 based on stage 2's outcome directly).
    
    Lambda is calculated as: lambda = lambda_base + (lambda_oci * oci)
    We clip lambda to [0, 1].
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice.
    lambda_base: [0, 1] - Baseline eligibility trace.
    lambda_oci: [-1, 1] - Sensitivity of eligibility trace to OCI score.
    """
    learning_rate, beta, lambda_base, lambda_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated eligibility trace
    lam = lambda_base + (lambda_oci * current_oci)
    # Ensure lambda stays within [0, 1] bounds
    if lam < 0: lam = 0
    if lam > 1: lam = 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1 = np.zeros(2)      # TD values for stage 1
    q_stage2 = np.zeros((2, 2)) # TD values for stage 2

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Pure Model-Free choice based on TD values
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Prediction error at stage 2
        delta_stage2 = r - q_stage2[state_idx, a2]
        
        # Prediction error at stage 1
        # Note: In standard TD(lambda), the value of the state arrived at (q_stage2) 
        # is the target for stage 1.
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1
        # Stage 1 is updated by its own prediction error, PLUS a portion of the stage 2 error
        # governed by lambda. This allows the final reward to propagate back to stage 1.
        q_stage1[a1] += learning_rate * delta_stage1 + learning_rate * lam * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Second-Stage Learning Rate Asymmetry
This model investigates if OCI symptoms relate to an asymmetry in how positive vs. negative outcomes are learned, specifically at the second stage (the direct reward receipt). High OCI might be associated with an over-sensitivity to negative outcomes (failures to get gold) or a specific rigidity in updating values. We define a base learning rate and an OCI-dependent modifier that specifically scales learning from *negative* prediction errors (disappointment).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Asymmetric Learning Rates.
    
    This model allows for different learning rates for positive (better than expected)
    and negative (worse than expected) prediction errors. It hypothesizes that OCI
    specifically modulates the learning rate for negative prediction errors (lr_neg).
    
    lr_pos = lr_base
    lr_neg = lr_base * (1 + oci_neg_bias * oci)
    
    If oci_neg_bias is positive, high OCI subjects learn faster from disappointment.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate (used for positive PE).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting parameter (fixed, not OCI modulated here).
    oci_neg_bias: [-1, 5] - Scaling factor for negative learning rate based on OCI.
    """
    lr_base, beta, w, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    # Calculate asymmetric learning rates
    lr_pos = lr_base
    lr_neg = lr_base * (1.0 + oci_neg_bias * current_oci)
    
    # Bounds check for derived LR
    if lr_neg < 0: lr_neg = 0
    if lr_neg > 1: lr_neg = 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice (Hybrid MB/MF) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Learning ---
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        current_lr2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += current_lr2 * delta_stage2

        # Stage 1 update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        current_lr1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += current_lr1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Stickiness
Previous models explored modulating `w` or learning rates. This model focuses purely on "choice stickiness" (perseveration). It hypothesizes that the fundamental difference in high OCI participants is not their learning or planning (MB/MF), but a raw tendency to repeat the previous motor action regardless of reward history. We define a stickiness parameter that is a linear function of OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Choice Stickiness.
    
    This model adds a 'stickiness' bonus to the Q-values of the previously chosen action.
    The magnitude of this stickiness is determined by the OCI score.
    High OCI is hypothesized to lead to higher stickiness (compulsive repetition).
    
    stickiness = stick_base + stick_oci_slope * oci
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight (fixed).
    stick_base: [-5, 5] - Baseline stickiness intercept.
    stick_oci_slope: [-5, 5] - Slope of stickiness w.r.t OCI.
    """
    learning_rate, beta, w, stick_base, stick_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    stickiness = stick_base + (stick_oci_slope * current_oci)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the net Q-values before softmax
        # We create a temporary Q array for decision making so we don't corrupt the learned values
        q_decision = q_net.copy()
        if last_action != -1:
            q_decision[last_action] += stickiness
        
        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])
        
        last_action = a1 # Update for next trial

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```