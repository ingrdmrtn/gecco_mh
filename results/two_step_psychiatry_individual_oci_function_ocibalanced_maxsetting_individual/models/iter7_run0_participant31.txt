Here are three new cognitive models designed to capture the behavior of the participant, specifically focusing on how Obsessive-Compulsive Inventory (OCI) scores might modulate decision-making processes like sensitivity to structural surprise, goal-based perseveration, and counterfactual updating.

### Model 1: OCI-Modulated Surprise Sensitivity
This model hypothesizes that individuals with high OCI scores might react differently to "rare" or surprising transitions (e.g., Spaceship A going to Planet Y). While standard Model-Free learning updates values based on prediction error regardless of the transition type, this model modulates the learning rate for Stage 1 updates when a rare transition occurs. High OCI might lead to over-interpreting these rare events (high sensitivity) or ignoring them (low sensitivity/gating), distinct from common events.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Surprise Sensitivity Model.
    
    This model modulates the Model-Free learning rate in Stage 1 based on whether
    the observed transition was 'Rare' or 'Common'. The magnitude of this modulation
    is determined by the OCI score.
    
    Parameters:
    - learning_rate: [0,1] Base learning rate for common transitions.
    - beta: [0,10] Inverse temperature (exploration/exploitation).
    - w: [0,1] Weighting between Model-Based and Model-Free values.
    - surp_base: [0,1] Baseline multiplier for learning rate on rare transitions (mapped to [0, 3]).
    - surp_oci: [0,1] Effect of OCI on the surprise multiplier (mapped to [-2, 2]).
    """
    learning_rate, beta, w, surp_base, surp_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the surprise multiplier
    # surp_factor scales the learning rate when a transition is rare.
    # A factor > 1 implies over-learning from rare events; < 1 implies under-learning.
    base_factor = surp_base * 3.0 
    oci_effect = (surp_oci - 0.5) * 4.0 # Range [-2, 2]
    surp_factor = base_factor + oci_effect * oci_score
    surp_factor = max(0.0, surp_factor) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (State, Action)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s1 = int(state[trial]) # Planet reached: 0 or 1
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value (Hybrid MB/MF)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        logits = beta * (q_net - np.max(q_net))
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        qs_2 = q_stage2_mf[s1]
        logits_2 = beta * (qs_2 - np.max(qs_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Determine if transition was rare
        # Common: 0->0 (prob 0.7) or 1->1 (prob 0.7)
        # Rare: 0->1 (prob 0.3) or 1->0 (prob 0.3)
        is_common = (a1 == 0 and s1 == 0) or (a1 == 1 and s1 == 1)
        
        # Calculate Stage 1 Prediction Error
        delta_stage1 = q_stage2_mf[s1, a2] - q_stage1_mf[a1]
        
        # Apply modulated learning rate based on surprise
        current_lr = learning_rate
        if not is_common:
            current_lr *= surp_factor
            current_lr = min(1.0, current_lr) # Cap at 1.0
            
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Calculate Stage 2 Prediction Error and Update
        delta_stage2 = r - q_stage2_mf[s1, a2]
        q_stage2_mf[s1, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Planet Stickiness
This model proposes that perseveration (stickiness) is driven by the *outcome stimulus* (the Planet) rather than the motor action. A participant might try to return to the same Planet they just visited (or avoid it), regardless of which spaceship they chose previously. The model adds a bias to the Stage 1 Q-values proportional to the probability of reaching the previously visited planet, modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Planet Stickiness Model.
    
    This model includes a perseveration bias towards the *Planet* (stimulus outcome)
    visited on the previous trial, rather than the Action chosen.
    The strength of this bias is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0,1] Update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF mixing weight.
    - p_stick_base: [0,1] Baseline planet stickiness magnitude (mapped to [-5, 5]).
    - p_stick_oci: [0,1] OCI modulation of planet stickiness (mapped to [-5, 5]).
    """
    learning_rate, beta, w, p_stick_base, p_stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map parameters to stickiness magnitude
    # Positive magnitude = attraction to previous planet.
    # Negative magnitude = repulsion from previous planet.
    stick_mag = (p_stick_base - 0.5) * 10.0 + (p_stick_oci - 0.5) * 10.0 * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_planet = -1 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s1 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Planet Stickiness Bonus
        if last_planet != -1:
            # Calculate the probability of reaching the last visited planet for each action
            # transition_matrix columns correspond to planets.
            # col 0 = probs of reaching Planet 0 given Action 0 and 1.
            prob_reach_last = transition_matrix[:, last_planet] 
            bonus = prob_reach_last * stick_mag
            q_net += bonus

        logits = beta * (q_net - np.max(q_net))
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_planet = s1

        # --- Stage 2 Policy ---
        qs_2 = q_stage2_mf[s1]
        logits_2 = beta * (qs_2 - np.max(qs_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s1, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s1, a2]
        q_stage2_mf[s1, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Counterfactual Updating
This model incorporates "fictitious play" or counterfactual updating, where the participant updates the value of the *unchosen* spaceship in the opposite direction of the chosen spaceship's prediction error. This mechanism can generate alternation behavior (switching after losses, or even after wins if the counterfactual weight is negative). The OCI score modulates the strength of this counterfactual checking.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Counterfactual Updating Model.
    
    This model updates the unchosen action in Stage 1 in the opposite direction 
    of the chosen action's prediction error. The magnitude of this counterfactual
    update is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0,1] Base learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] MB/MF mixing weight.
    - cf_base: [0,1] Baseline counterfactual weight (mapped to [0, 1]).
    - cf_oci: [0,1] OCI modulation of counterfactual weight (mapped to [-1, 1]).
    """
    learning_rate, beta, w, cf_base, cf_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map parameters
    # cf_weight represents the fraction of the PE applied inversely to the unchosen option.
    # High positive weight implies strong "if I won with A, U must be worse" logic.
    cf_weight = cf_base + (cf_oci - 0.5) * 2.0 * oci_score
    cf_weight = np.clip(cf_weight, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s1 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * (q_net - np.max(q_net))
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        qs_2 = q_stage2_mf[s1]
        logits_2 = beta * (qs_2 - np.max(qs_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s1, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Counterfactual update for the unchosen action
        # If delta > 0 (outcome better than expected), unchosen action value decreases.
        # If delta < 0 (outcome worse than expected), unchosen action value increases.
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] -= learning_rate * delta_stage1 * cf_weight
        
        delta_stage2 = r - q_stage2_mf[s1, a2]
        q_stage2_mf[s1, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```