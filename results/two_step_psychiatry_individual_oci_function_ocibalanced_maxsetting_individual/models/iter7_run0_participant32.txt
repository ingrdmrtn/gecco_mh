Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task, specifically focusing on perseveration, model-based/model-free weighting dynamics, and reward sensitivity asymmetry.

### Cognitive Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that OCI scores relate to "cognitive inflexibility" or "stickiness." High OCI scores might lead to a higher tendency to repeat the previous choice regardless of reward (perseveration). The model implements a standard hybrid reinforcement learning agent where the "stickiness" parameter is a linear function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL with OCI-modulated choice stickiness (perseveration).
    High OCI is hypothesized to increase the tendency to repeat the previous Stage 1 action.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1] (Weight of Model-Based system)
    stick_base: [-5, 5] (Base stickiness parameter)
    stick_oci_slope: [-5, 5] (How OCI modulates stickiness)
    """
    learning_rate, beta, w, stick_base, stick_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stickiness based on OCI
    stickiness = stick_base + (stick_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free stage 2 (also used for MB)
    
    prev_action_1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the previous action
        logits = beta * q_net
        if prev_action_1 != -1:
            logits[int(prev_action_1)] += stickiness
            
        exp_q1 = np.exp(logits - np.max(logits)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx] - np.max(beta * q_stage2_mf[s_idx]))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (TD using Stage 2 value)
        # Note: Standard TD(0) update for MF Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Model-Based Weighting (Sigmoid)
This model tests the hypothesis that OCI affects the balance between Model-Based (goal-directed) and Model-Free (habitual) control. Instead of a linear relationship, it uses a sigmoidal transformation to map the mixing weight `w` directly to the OCI score, assuming that the balance might shift non-linearly or saturate.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL where the Model-Based weight (w) is a logistic function of OCI.
    This tests if OCI determines the balance between goal-directed and habitual control.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_logit_base: [-5, 5] (Base log-odds of being Model-Based)
    w_oci_coeff: [-5, 5] (Effect of OCI on the log-odds)
    """
    learning_rate, beta, w_logit_base, w_oci_coeff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Logistic function to keep w between 0 and 1
    # w = 1 / (1 + exp(-(base + slope * oci)))
    logit_w = w_logit_base + (w_oci_coeff * oci_score)
    w = 1.0 / (1.0 + np.exp(-logit_w))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted combination
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net - np.max(beta * q_net))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx] - np.max(beta * q_stage2_mf[s_idx]))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Learning Rates for Positive vs Negative Prediction Errors
This model investigates if OCI is associated with asymmetric learning from positive versus negative outcomes. People with high OCI might be hyper-sensitive to errors (negative prediction errors) or less sensitive to rewards. The model splits the learning rate into `alpha_pos` and `alpha_neg`, where `alpha_neg` is modulated by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with asymmetric learning rates for positive/negative prediction errors.
    The learning rate for negative prediction errors (alpha_neg) is modulated by OCI.
    This tests if high OCI leads to over-learning from failures/omissions.
    
    Bounds:
    alpha_pos: [0,1] (Learning rate for positive PE)
    alpha_neg_base: [0,1] (Base learning rate for negative PE)
    alpha_neg_oci_slope: [-1, 1] (Modulation of negative learning rate by OCI)
    beta: [0,10]
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate alpha_neg based on OCI
    alpha_neg = alpha_neg_base + (alpha_neg_oci_slope * oci_score)
    alpha_neg = np.clip(alpha_neg, 0.001, 0.999) # Ensure bounds

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice Probability
        exp_q1 = np.exp(beta * q_stage1 - np.max(beta * q_stage1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Choice Probability
        exp_q2 = np.exp(beta * q_stage2[s_idx] - np.max(beta * q_stage2[s_idx]))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        # Using max Q of stage 2 as the target (Q-learning style update for stage 1)
        # This is strictly model-free
        target_stage1 = np.max(q_stage2[s_idx])
        delta_stage1 = target_stage1 - q_stage1[a1]
        
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1[a1] += lr_1 * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```