Here are the three proposed cognitive models.

### Cognitive Model 1: OCI-Modulated Correlated Counterfactual Learning
This model hypothesizes that individuals with high OCI scores may engage in "black-and-white" thinking or counterfactual reasoning. Specifically, when they observe a reward (or lack thereof) for their chosen action, they may infer the opposite outcome for the unchosen action (e.g., "If this path was bad, the other must have been good"). This is implemented as an update to the unchosen Stage 1 option towards `1 - Reward`, with the learning rate modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Correlated Counterfactual Learning Model.
    
    Hypothesis: High OCI participants may update the value of the unchosen Stage 1 
    option based on the inverse of the received reward (Counterfactual updating).
    If Reward=0, they update unchosen towards 1. If Reward=1, update unchosen towards 0.
    This captures a "grass is greener" or binary thinking style.
    
    Parameters:
    - lr: [0,1] Learning rate for chosen options.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight (0=MF, 1=MB).
    - lambd: [0,1] Eligibility trace.
    - lr_corr_base: [0,1] Baseline learning rate for unchosen counterfactual update.
    - lr_corr_oci: [-1,1] Modulation of counterfactual learning rate by OCI.
    """
    lr, beta, w, lambd, lr_corr_base, lr_corr_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-specific counterfactual learning rate
    lr_corr = lr_corr_base + lr_corr_oci * oci_score
    lr_corr = np.clip(lr_corr, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Missing data check
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updates ---
        # Stage 1 MF Update (TD)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        # Stage 2 MF Update (TD)
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Eligibility Trace for Stage 1
        q_stage1_mf[a1] += lr * lambd * delta_2
        
        # --- Counterfactual Update for Unchosen Stage 1 Action ---
        # Update unchosen action towards (1 - r)
        unchosen_a1 = 1 - a1
        counterfactual_target = 1.0 - r
        q_stage1_mf[unchosen_a1] += lr_corr * (counterfactual_target - q_stage1_mf[unchosen_a1])

    return log_loss
```

### Cognitive Model 2: OCI-Modulated Stage 2 Learning Rate
This model proposes that OCI symptoms differentially affect learning from immediate, concrete outcomes (Stage 2) versus distal predictions (Stage 1). While the Stage 1 learning rate is fixed, the Stage 2 learning rate is modulated by the OCI score. This allows the model to capture if high-OCI individuals are more reactive (or rigid) regarding the immediate reward value of specific aliens, independent of their broader planning strategy.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Learning Rate Model.
    
    Hypothesis: OCI scores correlate with the speed of learning for immediate 
    outcomes (Stage 2), distinct from the general learning rate used for 
    Stage 1 predictions. High OCI may lead to volatility (high lr) or 
    rigidity (low lr) in valuing the aliens.
    
    Parameters:
    - lr_s1: [0,1] Learning rate for Stage 1 (Spaceships).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight.
    - lambd: [0,1] Eligibility trace.
    - lr_s2_base: [0,1] Baseline learning rate for Stage 2 (Aliens).
    - lr_s2_oci: [-1,1] Modulation of Stage 2 learning rate by OCI.
    """
    lr_s1, beta, w, lambd, lr_s2_base, lr_s2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-specific Stage 2 learning rate
    lr_s2 = lr_s2_base + lr_s2_oci * oci_score
    lr_s2 = np.clip(lr_s2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updates ---
        # Stage 1 Update (using lr_s1)
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_1
        
        # Stage 2 Update (using OCI-modulated lr_s2)
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_s2 * delta_2
        
        # Eligibility trace (scales with lr_s1 as it updates Stage 1 Q)
        q_stage1_mf[a1] += lr_s1 * lambd * delta_2

    return log_loss
```

### Cognitive Model 3: OCI-Modulated Structural Confidence
This model hypothesizes that OCI symptoms impact the "confidence" in the transition structure of the task. While the true transition probability is 0.7, high OCI might be associated with uncertainty (perceiving the structure as more random, closer to 0.5) or hyper-rigidity (perceiving it as deterministic). The model modulates the transition probability used in the Model-Based calculation based on the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Structural Confidence Model.
    
    Hypothesis: OCI scores affect the participant's internal model of the 
    task structure (transition probabilities). High OCI might degrade confidence 
    in the common/rare structure (driving p(trans) towards 0.5) or enhance it.
    This modulates the quality of the Model-Based value estimation.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight.
    - lambd: [0,1] Eligibility trace.
    - conf_base: [0,1] Baseline structural confidence (0=Random/0.5, 1=True/0.7).
    - conf_oci: [-1,1] Modulation of confidence by OCI.
    """
    lr, beta, w, lambd, conf_base, conf_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # conf=0 -> p=0.5 (Random structure)
    # conf=1 -> p=0.7 (True structure)
    conf = conf_base + conf_oci * oci_score
    conf = np.clip(conf, 0.0, 1.0)
    p_trans = 0.5 + 0.2 * conf
    
    # Subjective transition matrix used for MB calculation
    transition_matrix = np.array([[p_trans, 1-p_trans], [1-p_trans, p_trans]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Choice ---
        # MB calculation uses the OCI-modulated transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updates ---
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2

    return log_loss
```