Here are three new cognitive models tailored to the participant's data and OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Risk Attitude in Model-Based Planning.
    
    Hypothesis: High OCI individuals may exhibit altered risk sensitivity during the planning phase.
    Standard Model-Based control assumes the agent plans to maximize the expected value (using Max Q2).
    This model introduces an 'optimism' parameter modulated by OCI. 
    High optimism focuses on the best outcome (Max Q2), while low optimism (pessimism) 
    incorporates the worst outcome (Min Q2), reflecting a potential avoidance motivation or anxiety.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - stickiness: [0, 5] Perseveration bonus for repeated Stage 1 choices.
    - optimism_base: [0, 1] Baseline weight for Max(Q2). 1.0 recovers standard MB.
    - optimism_oci: [-1, 1] Modulation of optimism by OCI score.
    """
    learning_rate, beta, w, stickiness, optimism_base, optimism_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective optimism parameter bounded [0, 1]
    optimism = optimism_base + (optimism_oci * oci_score)
    if optimism < 0: optimism = 0.0
    if optimism > 1: optimism = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) # State x Alien
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # Calculate MB values with modulated risk attitude
        # Standard MB: val = max(Q2). Here: val = opt*max(Q2) + (1-opt)*min(Q2)
        max_q2 = np.max(q_stage2_mf, axis=1)
        min_q2 = np.min(q_stage2_mf, axis=1)
        q2_aggregated = (optimism * max_q2) + ((1 - optimism) * min_q2)
        
        q_stage1_mb = transition_matrix @ q2_aggregated

        # Net Q-value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        # Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # MF Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # MF Update Stage 1 (TD-0 using Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Forgetting of Unchosen Options.
    
    Hypothesis: OCI score relates to the persistence of memory for unchosen options.
    High OCI might be associated with obsessive retention (low forgetting) or 
    anxiety-driven instability (high forgetting). This model introduces a decay
    for the unchosen alien's Q-value in the visited state.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Perseveration bonus.
    - forget_base: [0, 1] Baseline forgetting rate for unchosen options.
    - forget_oci: [-1, 1] Modulation of forgetting rate by OCI.
    """
    learning_rate, beta, w, stickiness, forget_base, forget_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate forgetting rate bounded [0, 1]
    forget_rate = forget_base + (forget_oci * oci_score)
    if forget_rate < 0: forget_rate = 0.0
    if forget_rate > 1: forget_rate = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Update Chosen S2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Forget Unchosen S2 in the current state
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - forget_rate)

        # Update S1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Exploration After Non-Reward (Post-Loss Beta).
    
    Hypothesis: High OCI individuals may react to non-reward (loss of potential gain) 
    with altered exploration strategies. This model allows the inverse temperature (beta)
    to shift specifically after a trial with 0 coins, modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_win: [0, 10] Beta used after a reward (1).
    - beta_loss_base: [0, 10] Baseline beta used after a non-reward (0).
    - beta_loss_oci: [-10, 10] Modulation of post-loss beta by OCI.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Perseveration bonus.
    """
    learning_rate, beta_win, beta_loss_base, beta_loss_oci, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate beta for loss trials
    beta_loss = beta_loss_base + (beta_loss_oci * oci_score)
    if beta_loss < 0: beta_loss = 0.0
    if beta_loss > 10: beta_loss = 10.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 1.0 # Initialize assuming win or neutral state

    for trial in range(n_trials):
        
        # Determine current beta based on previous outcome
        if last_reward == 1.0:
            current_beta = beta_win
        else:
            current_beta = beta_loss

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```