Here are three new cognitive models that incorporate the OCI-R score to explain the participant's behavior in the two-step task. These models focus on mechanisms relevant to compulsivity, such as habit formation (model-free control), rigidity in switching (perseveration), and altered uncertainty processing.

### Model 1: Model-Based/Model-Free Hybrid with OCI-modulated Eligibility Traces
This model hypothesizes that high OCI scores lead to "mental stickiness" or ruminative processing of past actions. Instead of a standard TD(0) update or a fixed TD($\lambda$), the eligibility trace parameter ($\lambda$) is modulated by the OCI score. A higher OCI might increase the eligibility trace, meaning credit for the final reward is assigned more strongly to the first-stage action, blurring the distinction between the two stages and effectively making the Stage 1 update more "direct" (resembling Model-Free) but with a longer memory window.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: MB/MF Hybrid with OCI-modulated Eligibility Traces (Lambda).
    Hypothesis: High OCI increases the eligibility trace (lambda), causing credit 
    for the reward to be assigned more strongly to the first-stage action directly, 
    potentially bypassing the model-based transition structure.

    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - lambda_base: [0, 1] Base eligibility trace decay parameter.
    - oci_lambda_mod: [-1, 1] Modulation of lambda by OCI score.
    """
    learning_rate, beta, w, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate lambda based on OCI
    eligibility_lambda = np.clip(lambda_base + (oci_score * oci_lambda_mod), 0.0, 1.0)
    
    # Transition matrix (fixed for this task structure)
    # 0 -> 0 (0.7), 0 -> 1 (0.3)
    # 1 -> 0 (0.3), 1 -> 1 (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T * max(Q_s2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        r = reward[trial]

        # --- Updates ---
        # Stage 1 Prediction Error (TD error)
        # Using Q_stage2(s', a') as the target for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 MF values using eligibility trace
        # The update includes the immediate stage 1 error + decayed stage 2 error
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * eligibility_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Dual-Beta Model with OCI-modulated Stage 2 Exploration
This model posits that OCI affects the balance of exploration vs. exploitation differently at different stages. Specifically, high OCI might lead to "checking" behavior or rigidity at the second stage (the aliens), regardless of the first stage choice. We model this by having separate inverse temperature parameters ($\beta$) for Stage 1 and Stage 2, where the Stage 2 $\beta$ is modulated by the OCI score. A higher OCI might lead to higher $\beta_2$ (more deterministic/rigid choice at the alien level) or lower $\beta_2$ (more erratic/anxious sampling).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Dual-Beta Model with OCI-modulated Stage 2 Inverse Temperature.
    Hypothesis: OCI specifically impacts decision noise/rigidity at the second stage (alien choice).
    High OCI might lead to more rigid (high beta) behavior at the outcome stage.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta1: [0, 10] Inverse temperature for Stage 1.
    - beta2_base: [0, 10] Base inverse temperature for Stage 2.
    - oci_beta2_mod: [-1, 1] Modulation of Stage 2 beta by OCI.
    - w: [0, 1] Model-based weight.
    """
    learning_rate, beta1, beta2_base, oci_beta2_mod, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate Stage 2 Beta
    # We scale the modulation to ensure beta remains in a reasonable range, though bounds handle clipping generally.
    # Here we apply the modulation factor (often beta is log-space, but additive is fine for small ranges).
    beta2 = np.clip(beta2_base + (oci_score * oci_beta2_mod * 5.0), 0.0, 20.0) # *5 to give the modifier range

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Uses Beta 1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy (Uses Beta 2 modulated by OCI) ---
        exp_q2 = np.exp(beta2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pure Model-Based with OCI-Modulated Transition Learning Rate
This model assumes the participant is purely Model-Based (no 'w' parameter) but their belief about the transition structure ($P(State|Action)$) is not fixed at 0.7/0.3. Instead, they learn the transition matrix online. The hypothesis is that high OCI leads to "hyper-learning" or volatility in beliefs about the environment's structure. High OCI participants might over-update their transition probabilities after rare events, leading to unstable model-based values.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dynamic Transition Learning (Pure MB) with OCI-modulated Transition Learning Rate.
    Hypothesis: The participant learns the transition matrix online rather than assuming it fixed.
    OCI modulates how quickly they update their belief about spaceship->planet transitions.
    High OCI might correspond to over-sensitivity to rare transitions (high transition learning rate).

    Parameters:
    - lr_reward: [0, 1] Learning rate for reward values (Q-values).
    - lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    - oci_trans_gain: [-1, 1] Modulation of transition learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    - stickiness: [0, 5] Perseveration bonus.
    """
    lr_reward, lr_trans_base, oci_trans_gain, beta, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate transition learning rate
    lr_trans = np.clip(lr_trans_base + (oci_score * oci_trans_gain), 0.0, 1.0)

    # Initialize transition counts/probabilities
    # Start with uniform priors or slight bias towards the "true" structure if desired, 
    # but here we initialize to 0.5 to assume they learn from scratch, 
    # OR initialize to 0.7/0.3 to represent prior instruction.
    # Let's initialize to the instructed structure but allow it to drift.
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    q_stage2_mf = np.zeros((2, 2)) # Values of aliens
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Pure MB) ---
        # V_MB(s1) = Sum(P(s2|s1) * max(Q_s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Add stickiness
        if last_action_1 != -1:
            q_stage1_mb[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        a1 = action_1[trial]
        a2 = action_2[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        r = reward[trial]

        # --- Updates ---
        
        # 1. Update Reward Values (Standard Q-learning for Stage 2)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_reward * delta_stage2
        
        # 2. Update Transition Probabilities
        # We observed transition: a1 -> state_idx
        # State_idx is 0 or 1. If a1=0, observed 0. If a1=1, observed 1? 
        # The transition matrix row is a1.
        
        # Create a one-hot vector for the observed state
        observed_trans = np.zeros(2)
        observed_trans[state_idx] = 1.0
        
        # Update the row corresponding to the chosen action
        # P_new = P_old + lr * (Outcome - P_old)
        trans_probs[a1] += lr_trans * (observed_trans - trans_probs[a1])
        
        # Ensure probabilities sum to 1 (normalization usually handled by the delta rule but good to be safe)
        trans_probs[a1] /= np.sum(trans_probs[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```