Here are three cognitive models that incorporate the OCI score to explain participant behavior in the two-step task.

### Model 1: Habit Learning with OCI Modulation
This model hypothesizes that high OCI participants are prone to forming strong habits or compulsions. Unlike simple "stickiness" (repeating the very last choice), this model tracks a **habit strength** variable that accumulates over time based on choice history. The OCI score modulates the **weight** of this habit strength in the decision-making process, leading to the long streaks of repetitive behavior observed in the data.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Habit Learning.
    
    Hypothesis: High OCI participants rely on a slowly building 'habit' or compulsion.
    A habit trace accumulates for chosen actions and decays for unchosen ones.
    OCI modulates how strongly this habit trace influences the current choice,
    leading to 'sticky' behavior or inertia.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Mixing weight between Model-Based (1) and Model-Free (0).
    - lambda_coeff: [0, 1] Eligibility trace for Stage 1 Model-Free update.
    - habit_lr: [0, 1] Learning rate for the habit trace accumulation.
    - habit_oci_weight: [0, 5] Scaling factor for how much OCI amplifies the habit's influence.
    """
    learning_rate, beta, w, lambda_coeff, habit_lr, habit_oci_weight = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit values for the two spaceships (initialized to 0)
    habit_values = np.zeros(2)

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits: Base utility + Habit Bonus
        # The habit bonus is proportional to the habit strength * OCI * weight
        logits_1 = beta * q_net + (habit_oci_weight * oci_score * habit_values)
        
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])

        # Value Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 MF with eligibility trace
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        
        # Update Stage 2 MF
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Habit Trace
        # H(a) moves towards 1 if chosen, moves towards 0 if not chosen.
        # Vectorized update:
        one_hot_action = np.zeros(2)
        one_hot_action[a1] = 1.0
        habit_values += habit_lr * (one_hot_action - habit_values)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Distorted Transition Belief
This model hypothesizes that high OCI participants distrust the stability or commonality of the environment's structure. They perceive the "common" transitions (0.7) as less reliable and the "rare" transitions (0.3) as more likely, effectively flattening the transition matrix towards randomness (0.5/0.5). This distortion degrades the efficacy of the Model-Based controller.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Transition Uncertainty.
    
    Hypothesis: High OCI participants distrust the transition structure of the task.
    They perceive the transition probabilities as more entropic (closer to 0.5) 
    than they actually are. This distortion scales with OCI score.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_coeff: [0, 1]
    - distortion_scale: [0, 0.4] Amount of distortion per unit of OCI. 
      Distortion reduces p_common from 0.7 towards 0.5.
    """
    learning_rate, beta, w, lambda_coeff, distortion_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate distorted transition matrix based on OCI
    # Base p_common is 0.7. Distortion subtracts from this.
    # We clip distortion to ensure p_common doesn't drop below 0.5 (randomness).
    distortion = distortion_scale * oci_score
    if distortion > 0.2:
        distortion = 0.2
        
    p_common = 0.7 - distortion
    p_rare = 1.0 - p_common
    
    # The MB system uses this 'perceived' matrix, not the objective one.
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Post-Loss Rigidity
This model hypothesizes that for high OCI participants, negative outcomes (0 coins) trigger anxiety or a "need for control," resulting in cognitive rigidity. This is modeled as an increase in the inverse temperature parameter (`beta`) specifically after a loss. A higher beta makes the participant more likely to deterministically choose the option with the slightly higher value, potentially locking them into a suboptimal choice (perseveration) if values are close.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Post-Loss Rigidity.
    
    Hypothesis: High OCI participants respond to losses (0 coins) with increased 
    cognitive rigidity in the subsequent trial. This is modeled by increasing 
    the inverse temperature (beta) after a loss, reducing exploration.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1]
    - lambda_coeff: [0, 1]
    - psi_loss_rigidity: [0, 10] Scaling factor for beta increase after a loss.
    """
    learning_rate, beta_base, w, lambda_coeff, psi_loss_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_reward = 1.0 # Initialize neutral (as if win) so no rigidity on trial 1

    for trial in range(n_trials):
        # Calculate dynamic beta
        # If prev_reward was 0 (loss), beta increases.
        # Factor is (1 + psi * OCI). If prev_reward was 1 (win), factor is 1.
        beta_current = beta_base * (1.0 + psi_loss_rigidity * oci_score * (1.0 - prev_reward))
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_current * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Policy
        # Rigidity applies to the 'state of mind' for the whole trial
        logits_2 = beta_current * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])

        # Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```