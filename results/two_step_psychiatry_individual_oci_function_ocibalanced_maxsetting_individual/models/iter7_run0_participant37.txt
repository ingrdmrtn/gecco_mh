Here are three new cognitive models that explore different mechanisms by which OCI scores might influence decision-making in this task, specifically focusing on rigidity, perseveration, and altered learning rates.

### Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that individuals with higher OCI scores exhibit greater "stickiness" or perseveration in their choices. They are more likely to repeat their previous action regardless of the reward outcome. The OCI score scales a stickiness parameter added to the Q-values.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Perseveration.
    Hypothesis: High OCI leads to increased perseveration (stickiness) on the previous choice.
    The strength of this stickiness is modulated by the OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stickiness_strength: [0, 5] (Base stickiness parameter)
    """
    learning_rate, beta, w, stickiness_strength = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness is stronger for higher OCI
    current_stickiness = stickiness_strength * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        q_net_stick = q_net.copy()
        if last_action_1 != -1:
            q_net_stick[last_action_1] += current_stickiness
        
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Learning Rate Asymmetry
This model suggests that high OCI scores might lead to an asymmetry in how positive and negative prediction errors are processed. Specifically, it tests if high OCI makes participants more sensitive to punishments (or lack of reward) versus rewards, or vice versa, by splitting the learning rate based on the sign of the prediction error and scaling one side with OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Learning Rate Asymmetry.
    Hypothesis: OCI modulates the balance between learning from positive vs negative outcomes.
    We use a base learning rate and an OCI-scaled multiplier for negative prediction errors.
    
    Bounds:
    alpha_pos: [0, 1] (Learning rate for positive RPE)
    beta: [0, 10]
    w: [0, 1]
    neg_alpha_scale: [0, 2] (Multiplier for negative RPE learning rate based on OCI)
    """
    alpha_pos, beta, w, neg_alpha_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate based on OCI
    # If neg_alpha_scale * oci > 1, we cap it at 1 implicitly by logic or clamp it
    alpha_neg_raw = alpha_pos * (neg_alpha_scale * oci_score)
    alpha_neg = min(1.0, max(0.0, alpha_neg_raw))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2

        # Stage 1 Update
        # Note: SARSA-like update for stage 1 using stage 2 Q-value
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Model-Based Suppression
This model posits that high OCI interferes with the complex cognitive processing required for Model-Based (planning) control. Instead of a fixed mixing weight `w`, the effective weight given to the model-based system is reduced as the OCI score increases, reflecting a reliance on simpler, habitual (Model-Free) strategies under high symptom load.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Model-Based Suppression.
    Hypothesis: High OCI scores suppress the contribution of the Model-Based system.
    The parameter w represents the maximum possible MB weight, which is attenuated by OCI.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_max: [0, 1] (Maximum model-based weight for a theoretical 0 OCI)
    suppression_factor: [0, 1] (How strongly OCI reduces w)
    """
    learning_rate, beta, w_max, suppression_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective w is reduced by OCI
    # If suppression_factor is high, high OCI reduces w significantly
    w_effective = w_max * (1.0 - (suppression_factor * oci_score))
    w_effective = max(0.0, w_effective)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Use w_effective instead of a static w parameter
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```