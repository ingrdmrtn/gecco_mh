Here are the 3 proposed cognitive models.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Forgetting of Unchosen Options.
    
    This model combines Model-Based (MB) and Model-Free (MF) learning.
    It hypothesizes that high OCI scores drive a 'pruning' or active forgetting 
    of unchosen options in the Model-Free system, leading to rigid habits.
    
    The decay rate of unchosen Q-values is proportional to the OCI score.
    
    Parameters:
    - learning_rate: [0,1] Update rate for Q-values.
    - beta: [0,10] Inverse temperature (softmax) for choice stochasticity.
    - w: [0,1] Mixing weight. 1 = Pure MB, 0 = Pure MF.
    - lam: [0,1] Eligibility trace parameter (credit assignment to Stage 1).
    - stickiness: [0,10] Choice stickiness (perseveration) bonus for Stage 1.
    - forget_base: [0,1] Scaling factor for OCI-dependent decay rate.
    """
    learning_rate, beta, w, lam, stickiness, forget_base = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Transition matrix (fixed structure for the task)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_1 = np.zeros(2)      # Stage 1 MF Q-values
    q_mf_2 = np.zeros((2, 2)) # Stage 2 MF Q-values
    
    # Decay rate depends on OCI
    decay = forget_base * oci_score
    if decay > 1.0: decay = 1.0
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Choice ---
        # Model-Based Value calculation
        max_q2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q2
        
        # Net Value: Weighted sum of MB and MF
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Add Stickiness
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning Updates ---
        # Stage 1 MF Update (TD-0)
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        # Forgetting mechanism: Unchosen Stage 1 option decays
        q_mf_1[1 - a1] *= (1.0 - decay)
        
        # Stage 2 MF Update
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        # Forgetting mechanism: Unchosen Stage 2 option decays
        q_mf_2[s_idx, 1 - a2] *= (1.0 - decay)
        
        # Eligibility Trace (Lambda)
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_action_1 = a1
        
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-weighted Accumulated Habit.
    
    This model replaces static stickiness with a dynamic 'habit strength' trace
    that accumulates over time. The impact of this habit strength on decision-making
    is scaled by the participant's OCI score.
    
    Hypothesis: High OCI individuals rely more heavily on accumulated habit traces
    than on current value estimates.
    
    Parameters:
    - learning_rate: [0,1] Update rate for Q-values.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Mixing weight (MB vs MF).
    - lam: [0,1] Eligibility trace.
    - habit_lr: [0,1] Learning rate for the habit trace (how fast habits form/decay).
    - habit_oci_weight: [0,10] Weight scaling the influence of habit * OCI on choice.
    """
    learning_rate, beta, w, lam, habit_lr, habit_oci_weight = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    # Dynamic habit trace for Stage 1 actions (initialized to 0)
    habit_values = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 ---
        max_q2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q2
        
        # Habit Bonus depends on OCI and accumulated habit strength
        habit_bonus = habit_oci_weight * oci_score * habit_values
        
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1 + habit_bonus
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        # Update Habit Traces
        # Chosen action trace increases towards 1, unchosen decays towards 0
        habit_values[a1] += habit_lr * (1.0 - habit_values[a1])
        habit_values[1 - a1] += habit_lr * (0.0 - habit_values[1 - a1])
        
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-determined Mixing Weight (w).
    
    Hypothesis: The balance between Goal-Directed (MB) and Habitual (MF) control
    is directly determined by the OCI score via a logistic function.
    w = sigmoid(base + slope * OCI)
    
    This allows the model to learn if high OCI predicts lower MB usage (MF dominance).
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - lam: [0,1]
    - stickiness: [0,10]
    - w_base_norm: [0,1] Normalized base parameter for the logistic function (mapped to [-5, 5]).
    - w_slope_norm: [0,1] Normalized slope parameter for the logistic function (mapped to [-5, 5]).
    """
    learning_rate, beta, lam, stickiness, w_base_norm, w_slope_norm = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Map normalized parameters [0,1] to logit space [-5, 5]
    w_logit_base = (w_base_norm - 0.5) * 10.0
    w_logit_slope = (w_slope_norm - 0.5) * 10.0
    
    # Calculate w based on OCI
    logit = w_logit_base + w_logit_slope * oci_score
    w = 1.0 / (1.0 + np.exp(-logit))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # MB Value
        max_q2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q2
        
        # Net Value using OCI-derived w
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # Updates
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_action_1 = a1
        
    return log_loss
```