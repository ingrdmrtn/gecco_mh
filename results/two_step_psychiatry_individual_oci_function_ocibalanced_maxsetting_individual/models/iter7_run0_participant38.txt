Here are 3 new cognitive models for the two-step task, incorporating the OCI score to explain individual variability in decision-making strategies.

### Model 1: Asymmetric Learning Rate Model (OCI-Enhanced Negative Sensitivity)
This model hypothesizes that individuals with high OCI scores are hyper-sensitive to negative prediction errors (outcomes that are worse than expected). While they learn from positive outcomes at a baseline rate, they update their value estimates much more aggressively when they encounter a negative prediction error (e.g., receiving 0 coins when expecting some probability of reward). This reflects a "fear of failure" or avoidance-learning bias common in compulsive symptomatology.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rate Model (OCI-Enhanced Negative Sensitivity).
    
    Hypothesis: High OCI participants are hyper-sensitive to negative prediction errors.
    They update value estimates more aggressively when outcomes are disappointing 
    (delta < 0) compared to positive outcomes.
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate (applied when prediction error is positive).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-Based / Model-Free mixing weight.
    lambda_decay: [0, 1] - Eligibility trace decay parameter.
    stickiness: [0, 5] - General choice stickiness bonus.
    neg_alpha_oci: [0, 10] - Scaling factor for the learning rate when PE is negative, modulated by OCI.
                             alpha_neg = learning_rate * (1 + neg_alpha_oci * oci).
    """
    learning_rate, beta, w, lambda_decay, stickiness, neg_alpha_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate scalar
    # We cap the effective learning rate at 1.0 to ensure stability
    alpha_neg_scale = 1.0 + neg_alpha_oci * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        prev_a1 = a1

        # --- Stage 2 Policy ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 1 Prediction Error
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Determine LR for Stage 1 update
        # If delta is negative (disappointment), use boosted alpha
        lr_1 = learning_rate
        if delta_stage1 < 0:
            lr_1 = min(1.0, learning_rate * alpha_neg_scale)
            
        # Determine LR for Stage 2 update
        lr_2 = learning_rate
        if delta_stage2 < 0:
            lr_2 = min(1.0, learning_rate * alpha_neg_scale)

        q_stage1_mf[a1] += lr_1 * delta_stage1 + lr_1 * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += lr_2 * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: Sticky Beliefs Model (OCI-Modulated Value Decay)
This model posits that high OCI scores relate to "mental hoarding" or the rigidity of beliefs. In standard reinforcement learning, unchosen options usually retain their value (or decay to a baseline). Here, we introduce a passive decay for unchosen options, representing forgetting or flexibility. However, this decay is *suppressed* by OCI. High OCI participants hold onto old value estimates longer, making their behavior more rigid and less responsive to environmental changes.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Sticky Beliefs Model (OCI-Modulated Value Decay).
    
    Hypothesis: High OCI participants exhibit "mental hoarding" or rigid beliefs. 
    We model this as a suppression of the decay rate for unchosen options. 
    High OCI -> Lower decay -> Old values persist longer, reducing flexibility.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    lambda_decay: [0, 1]
    stickiness: [0, 5]
    decay_base: [0, 1] - Base decay rate for unchosen options (0 = no decay/perfect memory, 1 = instant forget).
    decay_oci_suppress: [0, 10] - OCI suppression of decay. 
                                  decay_eff = decay_base / (1 + decay_oci_suppress * oci).
    """
    learning_rate, beta, w, lambda_decay, stickiness, decay_base, decay_oci_suppress = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective decay rate calculation
    # High OCI -> Lower decay -> Values persist
    decay_eff = decay_base / (1.0 + decay_oci_suppress * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        prev_a1 = a1

        # --- Stage 2 Choice ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update chosen options
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen options (Sticky Beliefs mechanism)
        # Unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_eff)
        
        # Decay unchosen Stage 2 (for the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay_eff)

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: Post-Error Rigidity Model (OCI-Modulated Beta Dynamics)
This model suggests that OCI manifests as a reaction to failure. While standard models use a constant exploration parameter ($\beta$), this model proposes that after a loss (Reward = 0), high OCI participants become anxious or rigid, leading to a temporary increase in $\beta$ (decreased exploration/increased exploitation). This "freezing" or "clamping down" on the current policy prevents them from exploring potentially better options after a negative outcome.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Post-Error Rigidity Model (OCI-Modulated Beta Dynamics).
    
    Hypothesis: High OCI participants react to failure (Reward=0) by becoming more rigid 
    (higher inverse temperature beta), whereas low OCI participants might maintain 
    exploration. This models a "clamping down" on the perceived best option after a loss.
    
    Parameters:
    learning_rate: [0, 1]
    beta_base: [0, 10] - Baseline inverse temperature.
    w: [0, 1]
    lambda_decay: [0, 1]
    stickiness: [0, 5]
    beta_loss_oci: [0, 10] - Boost to beta after a loss, scaled by OCI.
                             beta_eff = beta_base + beta_loss_oci * oci * (1 - prev_reward).
    """
    learning_rate, beta_base, w, lambda_decay, stickiness, beta_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = 1 # Initialize as if we won, so no rigidity on trial 1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Dynamic Beta Calculation ---
        # If prev_reward was 0 (loss), add rigidity proportional to OCI
        rigidity_boost = beta_loss_oci * oci_score * (1.0 - prev_reward)
        beta_eff = beta_base + rigidity_boost

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        prev_a1 = a1
        prev_reward = 1 if r > 0 else 0

        # --- Stage 2 Choice ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta_eff * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```