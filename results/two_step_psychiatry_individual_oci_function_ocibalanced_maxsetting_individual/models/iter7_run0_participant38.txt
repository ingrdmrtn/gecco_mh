Here are 3 new cognitive models that explore different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in the two-step task, specifically focusing on perseveration, learning rate asymmetries, and model-based deficits.

### Model 1: Perseveration (Stickiness) Modulated by OCI
This model hypothesizes that high OCI scores are associated with increased "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of the outcome. This is a common finding in compulsion-related disorders. The model adds a choice autocorrelation term (stickiness) to the Q-values, where the strength of this stickiness is scaled by the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner with OCI-modulated Choice Stickiness.
    
    This model posits that individuals with higher OCI scores exhibit greater 
    perseveration (stickiness) in their first-stage choices. They are more likely 
    to repeat their previous action, regardless of reward history.
    
    Parameters:
    learning_rate: [0,1] - Value update rate.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting parameter (0=MF, 1=MB).
    stickiness_base: [0,5] - Baseline tendency to repeat choices.
    oci_stickiness_amp: [0,5] - Additional stickiness scaling with OCI score.
    """
    learning_rate, beta, w, stickiness_base, oci_stickiness_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness parameter is enhanced by OCI
    stickiness_weight = stickiness_base + (oci_stickiness_amp * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialized to 0/none)
    # We use a one-hot vector [prev_choice_0, prev_choice_1]
    last_action_1 = -1 

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-values before softmax
        q_hybrid_sticky = q_hybrid.copy()
        if last_action_1 != -1:
            q_hybrid_sticky[last_action_1] += stickiness_weight

        exp_q1 = np.exp(beta * q_hybrid_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update last action
        last_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning Updates ---
        # Stage 2 update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates (Pessimism) Modulated by OCI
This model explores the idea that high OCI (and anxiety-related traits) might lead to a bias towards negative information. Instead of a single learning rate, this model splits learning into positive (`alpha_pos`) and negative (`alpha_neg`) updates. The OCI score specifically amplifies the learning rate for negative prediction errors, making the agent "over-learn" from disappointments or punishments.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner with OCI-amplified Negative Learning Rate.
    
    This model tests if high OCI leads to hypersensitivity to negative outcomes.
    The learning rate for negative prediction errors is boosted by the OCI score,
    while the positive learning rate remains baseline.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0,1] - Baseline learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Model-based weight.
    oci_neg_boost: [0,1] - Factor increasing alpha_neg based on OCI.
    """
    alpha_pos, alpha_neg_base, beta, w, oci_neg_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate, capped at 1.0
    alpha_neg = min(1.0, alpha_neg_base + (oci_neg_boost * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 PE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg * delta_stage2
        
        # Stage 1 PE
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Eligibility Trace Decay (Lambda) Modulated by OCI
This model investigates how OCI affects the credit assignment process between stages. The parameter $\lambda$ (lambda) controls the "eligibility trace," determining how much the Stage 1 choice is reinforced by the Stage 2 outcome directly (Model-Free TD(lambda)). A higher $\lambda$ links the final reward more strongly to the first action. This model posits that high OCI might disrupt this temporal credit assignment, perhaps making it harder to link distant outcomes to initial choices (lower lambda) or conversely, creating rigid chains (higher lambda). Here, we model $\lambda$ as a function of OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner (TD-Lambda) with OCI-modulated Eligibility Trace.
    
    This model uses an eligibility trace parameter (lambda) to connect Stage 2 rewards
    back to Stage 1 choices. The strength of this connection is modulated by OCI.
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based control.
    lambda_base: [0,1] - Baseline eligibility trace decay.
    oci_lambda_slope: [-1,1] - How OCI changes lambda (can increase or decrease).
    """
    learning_rate, beta, w, lambda_base, oci_lambda_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lambda, bounded between 0 and 1
    # We allow slope to be negative or positive to find the direction of effect
    eff_lambda = lambda_base + (oci_lambda_slope * oci_score)
    eff_lambda = min(max(eff_lambda, 0.0), 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates using Eligibility Trace ---
        
        # 1. Update Stage 2 Q-value based on Reward
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        # 2. Update Stage 1 Q-value
        # In TD(lambda) for this task, Stage 1 is updated by its own immediate prediction error (usually 0 here)
        # PLUS the eligibility trace of the Stage 2 prediction error.
        # Immediate PE for Stage 1: (Q_stage2[chosen] - Q_stage1[chosen])
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1] # This is the standard SARSA/Q-learning term
        
        # The total update includes a fraction (lambda) of the Stage 2 error (delta_2)
        # This effectively allows the reward 'r' to update Q_stage1 directly.
        q_stage1_mf[a1] += learning_rate * (delta_1 + eff_lambda * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```