Here are three new cognitive models implemented as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Compulsive Momentum Model.
    
    Hypothesis: High OCI participants develop 'momentum' in their choices.
    Unlike standard stickiness which applies only to the immediately previous choice,
    momentum builds up over consecutive repetitions of the same choice.
    The stickiness bonus scales with the 'run length' of the current choice, modulated by OCI.
    This explains the extremely long runs of repetitive behavior (e.g., 110 trials) seen in the data.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based values (1=MB, 0=MF).
    - stick_base: [0, 5] Baseline stickiness to previous choice.
    - momentum_oci: [0, 1] Scaling of stickiness growth by OCI and run length.
    - lambda_eligibility: [0, 1] Eligibility trace for Stage 1 updates.
    """
    learning_rate, beta_1, beta_2, w, stick_base, momentum_oci, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_choice_1 = -1
    run_length = 0

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if last_choice_1 != -1:
            # Stickiness increases with run length, modulated by OCI
            # This creates a "trap" where staying makes it harder to switch
            stick_bonus = stick_base + momentum_oci * current_oci * run_length
            logits_1[last_choice_1] += stick_bonus
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update run length for next trial
        if last_choice_1 == -1:
            run_length = 1
        elif a1 == last_choice_1:
            run_length += 1
        else:
            run_length = 1
        last_choice_1 = a1

        # Stage 2 Policy
        state_idx = int(state[trial])
        logits_2 = beta_2 * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Learning
        r = reward[trial]
        
        # Prediction errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update values
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_eligibility * delta_stage2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Eligibility Trace Model.
    
    Hypothesis: OCI affects the credit assignment process (Lambda).
    High OCI might disconnect Stage 1 choices from Stage 2 rewards (low Lambda), 
    making Stage 1 behavior insensitive to outcomes and purely habitual/perseverative.
    This model allows OCI to modulate the eligibility trace parameter directly.
    It uses separate betas to maintain the stage-specific rigidity found in previous best models.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Weight for Model-Based values.
    - stickiness: [0, 5] Choice stickiness bonus.
    - lambda_base: [0, 1] Base eligibility trace.
    - lambda_oci_mod: [-1, 1] Modulation of lambda by OCI.
    """
    learning_rate, beta_1, beta_2, w, stickiness, lambda_base, lambda_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated lambda and clip to valid range
    lambda_eff = lambda_base + lambda_oci_mod * current_oci
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_choice_1 != -1:
            logits_1[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_choice_1 = a1

        # Stage 2
        state_idx = int(state[trial])
        logits_2 = beta_2 * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Updates
        r = reward[trial]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Use effective lambda for Stage 1 update
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_eff * delta_stage2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning with Dual Beta Model.
    
    Hypothesis: High OCI participants are insensitive to negative feedback (compulsive maintenance).
    They may ignore losses (Reward=0) while learning normally from wins.
    This model implements asymmetric learning rates for positive and negative reward prediction errors.
    The negative learning rate is explicitly dampened by OCI.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive RPEs.
    - lr_neg_base: [0, 1] Base learning rate for negative RPEs.
    - lr_neg_oci_damp: [0, 1] OCI-dependent dampening of negative learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2: [0, 10] Inverse temperature for Stage 2.
    - w: [0, 1] Model-based weight.
    - stickiness: [0, 5] Choice stickiness.
    - lambda_eligibility: [0, 1] Eligibility trace.
    """
    lr_pos, lr_neg_base, lr_neg_oci_damp, beta_1, beta_2, w, stickiness, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    # Dampening factor reduces lr_neg based on OCI intensity
    lr_neg = lr_neg_base * (1.0 - lr_neg_oci_damp * current_oci)
    lr_neg = max(0.0, lr_neg) # Ensure non-negative
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta_1 * q_net
        if prev_choice_1 != -1:
            logits_1[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_choice_1 = a1

        # Stage 2
        state_idx = int(state[trial])
        logits_2 = beta_2 * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Updates
        r = reward[trial]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Determine alpha for reward update based on sign of RPE (delta_stage2)
        if delta_stage2 >= 0:
            alpha = lr_pos
        else:
            alpha = lr_neg
            
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += alpha * delta_stage2
        
        # Update Stage 1
        # TD-error update uses standard positive rate (assumed baseline for consistency)
        # Eligibility trace update uses the asymmetric alpha (driven by reward)
        q_stage1_mf[a1] += lr_pos * delta_stage1 + lambda_eligibility * alpha * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```