Here are three new cognitive models that explore different mechanisms for how OCI symptoms might influence decision-making in this two-step task, specifically focusing on the balance between model-based/model-free control and learning rates.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight (`w`)
This model hypothesizes that individuals with higher OCI scores have a reduced reliance on model-based (goal-directed) planning. It uses a hybrid reinforcement learning framework where the final Q-value for the first stage is a weighted sum of model-free (MF) and model-based (MB) values. The weight `w` is determined by a baseline and an OCI-dependent reduction factor.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w' 
    is modulated by OCI score.
    
    Hypothesis: High OCI reduces the contribution of the Model-Based system (w),
    biasing the participant towards Model-Free (habitual) control.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_base: [0,1]
    oci_w_penalty: [0,1]
    """
    learning_rate, beta, w_base, oci_w_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    # Fixed transition matrix for the task (70% common, 30% rare)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)  # Model-free values for stage 1
    q_stage2 = np.zeros((2, 2)) # Values for stage 2 (shared by MB and MF)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Calculate w based on OCI. w represents the weight of the Model-Based system.
    # Higher OCI reduces w, making the agent more Model-Free.
    # We clip w to be between 0 and 1.
    w = np.clip(w_base - (oci_w_penalty * oci_score), 0.0, 1.0)

    for t in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value Calculation:
        # V(state) = max(Q_stage2(state, action))
        v_stage2 = np.max(q_stage2, axis=1) 
        # Q_MB = Transition_Matrix * V_stage2
        q_stage1_mb = transition_matrix @ v_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Decision ---
        s_idx = state[t]
        
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Update Stage 2 Q-values (TD(0))
        # Q2(s, a2) <- Q2(s, a2) + alpha * (r - Q2(s, a2))
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        # Update Stage 1 Model-Free Q-values (TD(1) / SARSA-like update using Q2)
        # We use the value of the state actually reached to update Q_MF(stage1)
        # This is the standard "Model-Free" update in Daw et al. 2011
        pe_1 = q_stage2[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1
        
        # Note: A full TD(lambda) model often includes an eligibility trace connecting
        # reward directly to stage 1. For simplicity in this hybrid structure, 
        # we stick to the standard update where Stage 1 tracks Stage 2 values.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Separate Learning Rates for Positive and Negative Prediction Errors (OCI-Modulated)
This model investigates if OCI affects how participants learn from positive versus negative outcomes. Specifically, it posits that high OCI might lead to hypersensitivity to negative outcomes (punishment/lack of reward) or deficits in learning from positive ones, leading to different learning rates (`alpha_pos` vs `alpha_neg`). The OCI score scales the negative learning rate.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates, where the negative
    learning rate is modulated by OCI.
    
    Hypothesis: High OCI leads to altered learning from negative prediction errors
    (e.g., failure to receive reward), potentially causing over-correction or rigidity.
    
    Bounds:
    alpha_pos: [0,1]
    alpha_neg_base: [0,1]
    beta: [0,10]
    oci_neg_scale: [0,5]
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)
    
    # Calculate the effective negative learning rate
    # We allow OCI to scale the base negative learning rate up or down
    alpha_neg = np.clip(alpha_neg_base * (1 + oci_neg_scale * oci_score), 0.0, 1.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        # --- Stage 2 Choice ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        if pe_2 >= 0:
            q_stage2[s_idx, a2] += alpha_pos * pe_2
        else:
            q_stage2[s_idx, a2] += alpha_neg * pe_2
            
        # Stage 1 Update (Model-Free TD)
        # Using the updated Q2 value as the target for Q1
        # This is effectively TD(0) for stage 1 looking at stage 2 value
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        if pe_1 >= 0:
            q_stage1[a1] += alpha_pos * pe_1
        else:
            q_stage1[a1] += alpha_neg * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Inverse Temperature (Beta) Modulation by OCI
This model proposes that the primary effect of OCI is on the exploration-exploitation trade-off. High OCI might be associated with reduced exploration (higher beta/lower temperature), leading to more deterministic choices based on current value estimates, even if those estimates are uncertain.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free learner where the inverse temperature (beta) is 
    modulated by OCI score.
    
    Hypothesis: High OCI leads to higher beta (lower temperature), resulting in
    more deterministic/rigid responding and less exploration.
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10]
    beta_oci_slope: [0,10]
    stickiness: [0,5]
    """
    learning_rate, beta_base, beta_oci_slope, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)
    
    # Calculate effective beta
    # beta = base + slope * OCI
    beta = beta_base + (beta_oci_slope * oci_score)
    # Ensure beta doesn't explode or go negative (though bounds usually handle this)
    beta = np.clip(beta, 0.0, 20.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for t in range(n_trials):
        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1.copy()
        
        # Add stickiness (choice autocorrelation)
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]
        
        last_action_1 = action_1[t]
        
        # --- Stage 2 Choice ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]
        
        # --- Learning ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Standard Q-learning updates
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        # Stage 1 update based on Stage 2 value
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```