Here are the three cognitive models as Python functions.

### Cognitive Model 1: Fictive Learning (Counterfactuals) Modulated by OCI
This model posits that individuals with high OCI symptoms may engage in increased counterfactual thinking ("rumination") about the unchosen option. When they receive a reward (or lack thereof), they update not only the chosen path but also the unchosen path based on a belief of anticorrelation (i.e., if A yielded a reward, U likely wouldn't have). The strength of this fictive update is modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Fictive Learning Model with OCI modulation.
    
    The agent updates the unchosen Stage 1 option based on a "grass is greener" 
    or anticorrelation assumption. If the chosen path yields a reward, the unchosen 
    path is assumed to be worse (target 0). If the chosen path yields no reward, 
    the unchosen path is assumed to be better (target 1).
    The rate of this fictive update is modulated by OCI.
    
    Parameters:
    learning_rate: [0,1] - Direct learning rate for chosen options.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0) control.
    alpha_fic_low: [0,1] - Fictive learning rate at OCI=0.
    alpha_fic_high: [0,1] - Fictive learning rate at OCI=1.
    """
    learning_rate, beta, w, alpha_fic_low, alpha_fic_high = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-dependent fictive learning rate
    alpha_fic = alpha_fic_low * (1 - oci_score) + alpha_fic_high * oci_score
    alpha_fic = np.clip(alpha_fic, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        if np.sum(exp_q1) == 0 or np.any(np.isinf(exp_q1)):
            probs_1 = np.ones(2) / 2
        else:
            probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        if a2 == -1:
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        if np.sum(exp_q2) == 0 or np.any(np.isinf(exp_q2)):
            probs_2 = np.ones(2) / 2
        else:
            probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 MF Update (TD from Stage 2 Q-value)
        target_s1 = q_stage2_mf[s_idx, a2]
        delta_stage1 = target_s1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Fictive Update for Unchosen Stage 1 Action
        a1_unchosen = 1 - a1
        # Assume anticorrelation: if r=1, unchosen target=0. If r=0, unchosen target=1.
        fictive_target = 1.0 - r
        delta_fictive = fictive_target - q_stage1_mf[a1_unchosen]
        q_stage1_mf[a1_unchosen] += alpha_fic * delta_fictive

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Additive Disappointment Penalty
This model incorporates a "fear of failure" mechanism. High OCI is associated with perfectionism and anxiety about errors. Instead of just learning from prediction errors, the agent applies an additional, fixed subtractive penalty (`rho`) to the value of an action if it results in no reward (0 coins). This penalty is distinct from learning rate changes and represents an emotional "cost" of failure.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Additive Disappointment Penalty Model.
    
    The agent incurs an additional subtractive penalty to the Q-value when a reward is not received (Reward=0).
    This penalty is fixed in magnitude (rho) and modulated by OCI, representing "fear of failure" or 
    obsessive avoidance of bad outcomes beyond standard error correction.
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF mixing weight.
    rho_low: [0,1] - Disappointment penalty magnitude at OCI=0.
    rho_high: [0,1] - Disappointment penalty magnitude at OCI=1.
    """
    learning_rate, beta, w, rho_low, rho_high = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    rho = rho_low * (1 - oci_score) + rho_high * oci_score
    rho = np.clip(rho, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        if np.sum(exp_q1) == 0: probs_1 = np.ones(2)/2
        else: probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        if a2 == -1:
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        if np.sum(exp_q2) == 0: probs_2 = np.ones(2)/2
        else: probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Apply Disappointment Penalty if outcome was bad (r=0)
        if r == 0.0:
            q_stage2_mf[s_idx, a2] -= rho
            
        # Stage 1 Update
        # Using the updated Q2 value as the target
        target_s1 = q_stage2_mf[s_idx, a2]
        delta_stage1 = target_s1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Stage-Specific Beta Modulation
This model hypothesizes that OCI affects decision-making noise (inverse temperature $\beta$) specifically at the high-level planning stage (Stage 1) while leaving the immediate response stage (Stage 2) relatively intact (or governed by a different parameter). High OCI might lead to "analysis paralysis" or, conversely, rigid habits at the spaceship choice level, distinct from how they choose aliens.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Beta Modulation Model.
    
    This model posits that OCI affects decision noise differently at the high-level planning stage (Stage 1)
    versus the immediate action stage (Stage 2). Specifically, Stage 1 beta is modulated by OCI.
    
    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta_1_low: [0,10] - Stage 1 inverse temperature at OCI=0.
    beta_1_high: [0,10] - Stage 1 inverse temperature at OCI=1.
    beta_2: [0,10] - Stage 2 inverse temperature (fixed across OCI).
    w: [0,1] - MB/MF mixing weight.
    """
    learning_rate, beta_1_low, beta_1_high, beta_2, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate Stage 1 Beta
    beta_1 = beta_1_low * (1 - oci_score) + beta_1_high * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_s1)
        if np.sum(exp_q1) == 0: probs_1 = np.ones(2)/2
        else: probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        if a2 == -1:
            p_choice_2[trial] = 1.0
            continue
            
        # --- Stage 2 Policy (Uses beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        if np.sum(exp_q2) == 0: probs_2 = np.ones(2)/2
        else: probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        target_s1 = q_stage2_mf[s_idx, a2]
        delta_stage1 = target_s1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```