Here are three cognitive models expressed as Python functions.

### Cognitive Model 1: OCI-Modulated Learning Rate Annealing
This model hypothesizes that OCI scores relate to **hyper-vigilance**, manifesting as a failure to "settle" or anneal the learning rate over time. While typical learners reduce their learning rate as they gain experience (annealing), high OCI participants may maintain a higher learning rate to constantly monitor for threats or environmental changes.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Learning Rate Annealing Model.
    
    Hypothesis: High OCI participants exhibit hyper-vigilance, preventing the 
    natural decay (annealing) of the learning rate over time. They treat the 
    environment as constantly volatile.
    
    Parameters:
    - lr_init: [0,1] Initial learning rate at trial 0.
    - decay_rate: [0,1] Base rate at which learning rate decays over trials.
    - oci_vigilance: [0,1] OCI modulation. Higher values reduce the decay effect.
                     If 1, decay is blocked (constant LR).
    - beta: [0,10] Inverse temperature (decision noise).
    - w: [0,1] Model-based weight.
    """
    lr_init, decay_rate, oci_vigilance, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective decay rate is dampened by OCI
    # If oci_vigilance is high, effective_decay approaches 0 -> LR stays high
    effective_decay = decay_rate * (1.0 - oci_vigilance * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Calculate dynamic learning rate for this trial
        current_lr = lr_init / (1.0 + effective_decay * trial)

        # Policy for Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Policy for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Value Updating
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        
        # Eligibility trace-like update for stage 1 based on stage 2 outcome
        q_stage1_mf[a1] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Loss-Perseveration
This model hypothesizes that OCI drives compulsive repetition specifically in the face of failure. While "Win-Stay" is a common strategy, "Lose-Stay" is maladaptive. This model separates stickiness (choice repetition bias) into a "Win" component (unaffected or base) and a "Loss" component that is increased by OCI scores.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Loss-Perseveration Model.
    
    Hypothesis: High OCI is associated with compulsive repetition of actions
    specifically after non-rewarded trials (Lose-Stay), whereas Win-Stay is 
    normal.
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - stick_win: [0,5] Stickiness bonus applied if the previous trial was rewarded.
    - stick_loss_base: [0,5] Base stickiness bonus if previous trial was unrewarded.
    - stick_loss_oci: [0,5] Additional stickiness after loss scaled by OCI.
    """
    learning_rate, beta, w, stick_win, stick_loss_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Store previous choice and reward for stickiness
    prev_a1 = -1
    prev_reward = 0

    for trial in range(n_trials):
        # Policy for Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness
        if prev_a1 != -1:
            if prev_reward == 1:
                stickiness = stick_win
            else:
                # OCI increases stickiness specifically after losses/no-reward
                stickiness = stick_loss_base + stick_loss_oci * oci_score
            
            q_net[prev_a1] += stickiness
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Policy for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Value Updating
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        # Update history
        prev_a1 = a1
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Transition Belief Distortion
This model posits that OCI affects the internal model of the environment's structure. High OCI participants may distrust the stability of the "common" transition (0.7), perceiving the world as more entropic (closer to 0.5) or conversely more deterministic. This model allows OCI to distort the `p_common` probability used in the Model-Based calculation.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Transition Belief Distortion Model.
    
    Hypothesis: OCI scores correlate with a distorted internal model of the 
    transition probabilities. High OCI participants may underestimate the 
    reliability of the common transition (doubt/uncertainty) or overestimate it.
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - p_common_base: [0.5, 1.0] Baseline belief in the common transition prob.
    - p_oci_shift: [-0.5, 0.5] Distortion applied by OCI. 
                   p_subjective = p_base + p_oci_shift * oci.
    """
    learning_rate, beta, w, p_common_base, p_oci_shift = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # We clip to ensure it stays within valid probability bounds [0, 1]
    p_common_subj = p_common_base + p_oci_shift * oci_score
    if p_common_subj > 1.0: p_common_subj = 1.0
    if p_common_subj < 0.0: p_common_subj = 0.0
    
    p_rare_subj = 1.0 - p_common_subj
    
    # Subjective transition matrix used for MB planning
    subj_transition_matrix = np.array([
        [p_common_subj, p_rare_subj], 
        [p_rare_subj, p_common_subj]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use subjective matrix for MB calculation
        q_stage1_mb = subj_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Policy for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Value Updating
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```