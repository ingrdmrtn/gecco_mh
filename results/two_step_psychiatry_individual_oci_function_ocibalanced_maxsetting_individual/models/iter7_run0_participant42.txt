Here are 3 new cognitive models based on the participant's high OCI score and the provided data patterns.

### Cognitive Model 1: OCI-Modulated "Safe-Bet" Bias
This model hypothesizes that high OCI participants exhibit a strong bias towards the option that has historically been the "safest" or most frequent choice, regardless of recent reward fluctuations. This manifests as a persistent bias towards the most commonly chosen spaceship (often Spaceship 1 in the later trials of the data) or a general resistance to switching away from a favored option, modeled here as a "safety bias" parameter scaled by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated "Safe-Bet" Bias.
    Hypothesis: High OCI participants develop a "safety bias" towards one option (arbitrarily initialized or learned)
    and are resistant to deviation. Here, we model it as a bias towards the option that currently has the
    highest Model-Free value, amplified by OCI. It acts as a "confirming" bias, pushing the value of the 
    leader even higher, making the policy more deterministic (rigid) for the leading option.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - safety_bias: [0, 5] Strength of the bias towards the current max-value option, scaled by OCI.
    """
    learning_rate, beta, w, safety_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # OCI-driven Safety Bias: Add a bonus to the option that is currently winning in MF valuation
        # This simulates a compulsion to stick with the "best" perceived option rigidly.
        current_leader = np.argmax(q_stage1_mf)
        bias_vector = np.zeros(2)
        bias_vector[current_leader] = safety_bias * oci_score
        
        q_net_biased = q_net + bias_vector

        exp_q1 = np.exp(beta * q_net_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Q-values
        # Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Stage 1 update
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Dependent Learning Rate Asymmetry
This model proposes that high OCI is associated with an over-sensitivity to negative feedback (punishment/lack of reward) or, conversely, a difficulty in "unlearning" established habits. Given the OCI literature often links compulsivity to habit formation, this model tests if OCI scales the *difference* between learning from positive vs. negative prediction errors. Specifically, it allows the learning rate for negative prediction errors (disappointment) to be modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Learning Rate Asymmetry.
    Hypothesis: OCI scores modulate how participants process positive versus negative prediction errors.
    High OCI might lead to "sticky" values where negative outcomes don't reduce value as much as they should
    (perseverance), or conversely, hypersensitivity to failure.
    
    Parameters:
    - alpha_base: [0, 1] Base learning rate for positive prediction errors (RPE > 0).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - neg_alpha_mod: [0, 1] Modifier for negative RPE learning rate, scaled by OCI. 
      alpha_neg = alpha_base * (1 - neg_alpha_mod * oci).
      If neg_alpha_mod is high, high OCI reduces learning from negative outcomes (perseverance).
    """
    alpha_base, beta, w, neg_alpha_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate the effective learning rate for negative errors based on OCI
    # We clip to ensure it stays valid [0, 1]
    alpha_neg = alpha_base * (1.0 - (neg_alpha_mod * oci_score))
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Q-values with asymmetric learning rates
        # Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        lr_s2 = alpha_base if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_s2 * delta_stage2
        
        # Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        lr_s1 = alpha_base if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[int(action_1[trial])] += lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Driven Decay of Unchosen Options
This model posits that high OCI participants have a "tunnel vision" or obsessive focus on the chosen option, causing the value of unchosen options to decay differently. While standard models might not update unchosen options, or decay them at a fixed rate, this model assumes that high OCI accelerates the forgetting (decay) of alternative paths, reinforcing the compulsion to stick with the current path because alternatives become undervalued or uncertain (effectively 0).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Decay of Unchosen Options.
    Hypothesis: High OCI leads to a neglect of counterfactuals or unchosen options.
    The value of the unchosen spaceship decays towards 0 at a rate determined by OCI.
    This reinforces the current choice by making the alternative appear less valuable over time.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - decay_rate_base: [0, 1] Base decay rate for unchosen options, amplified by OCI.
      decay = decay_rate_base * oci.
    """
    learning_rate, beta, w, decay_rate_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate decay rate specific to this participant
    decay = decay_rate_base * oci_score
    decay = np.clip(decay, 0.0, 1.0)

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Chosen Option - Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Update Chosen Option - Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Decay Unchosen Option - Stage 1
        unchosen_act1 = 1 - int(action_1[trial])
        q_stage1_mf[unchosen_act1] *= (1.0 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```