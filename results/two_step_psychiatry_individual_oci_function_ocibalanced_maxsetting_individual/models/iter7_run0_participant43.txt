Here are three cognitive models formulated as Python functions. Each model incorporates the OCI score into a different component of the reinforcement learning process: decision noise (rigidity), habit formation (choice history), and credit assignment (eligibility traces).

```python
def cognitive_model1_oci_rigidity(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Decision Rigidity (Beta Scaling).
    
    Hypothesis: High OCI scores are associated with behavioral rigidity and a reduced tendency 
    to explore. This model posits that the inverse temperature parameter (beta), which regulates 
    the exploration-exploitation trade-off, is linearly increased by the OCI score. 
    Higher OCI leads to more deterministic choices (sharper softmax) based on current value estimates.
    
    beta_effective = beta_base + (beta_oci_slope * OCI)
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta_base: [0, 10] Baseline inverse temperature (for OCI=0).
    - beta_oci_slope: [0, 10] Slope representing the increase in beta per unit of OCI.
    - w: [0, 1] Weight of Model-Based control (1 = Pure MB, 0 = Pure MF).
    """
    learning_rate, beta_base, beta_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta based on OCI
    beta = beta_base + (beta_oci_slope * oci_score)
    # Ensure beta stays non-negative
    if beta < 0: beta = 0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2_oci_choice_kernel(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Weighted Choice Kernel (Habitual History).
    
    Hypothesis: OCI is linked to compulsive repetition (habit) independent of reward. 
    This model includes a 'Choice Kernel' that tracks the exponentially weighted history 
    of past choices. The influence of this history on current choice is scaled by the OCI score.
    Unlike simple perseveration (which only looks at the previous trial), this kernel 
    captures longer-term behavioral trends (streaks).
    
    Logits = beta * Q_net + (kernel_weight_oci * OCI * ChoiceKernel)
    
    Parameters:
    - learning_rate: [0, 1] Update rate for MF values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - kernel_lr: [0, 1] Learning rate of the choice kernel (controls decay/memory length).
    - kernel_weight_oci: [0, 10] Strength of the kernel's influence scaled by OCI.
    """
    learning_rate, beta, w, kernel_lr, kernel_weight_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2) # Tracks choice frequency/history
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add habitual bias from Choice Kernel
        habit_bias = kernel_weight_oci * oci_score * choice_kernel
        logits_1 = beta * q_net + habit_bias
        
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update Choice Kernel (Exponential Moving Average of choices)
        # CK[a] = (1 - alpha_k) * CK[a] + alpha_k * I(choice == a)
        chosen = action_1[trial]
        choice_kernel[chosen] = (1.0 - kernel_lr) * choice_kernel[chosen] + kernel_lr * 1.0
        choice_kernel[1 - chosen] = (1.0 - kernel_lr) * choice_kernel[1 - chosen] + kernel_lr * 0.0

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3_oci_eligibility(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace (Direct Reinforcement).
    
    Hypothesis: High OCI scores are associated with a reliance on direct reinforcement from outcomes,
    bypassing the step-by-step TD structure. This model modulates the eligibility trace parameter (lambda)
    by the OCI score. A higher OCI leads to a higher lambda, meaning the Stage 1 choice is 
    updated more strongly by the final reward (Stage 2 outcome) directly, characteristic of 
    habitual/Monte-Carlo learning rather than temporal difference learning.
    
    lambda_effective = lambda_scale * OCI
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - lambda_scale: [0, 1] Scaling factor for the eligibility trace based on OCI. 
      (Resulting lambda is clipped to [0, 1]).
    """
    learning_rate, beta, w, lambda_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective lambda
    lambda_param = lambda_scale * oci_score
    if lambda_param > 1.0: lambda_param = 1.0
    if lambda_param < 0.0: lambda_param = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Stage 1 Prediction Error (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1: Includes direct eligibility trace from Stage 2 RPE
        # Q1 = Q1 + alpha * delta1 + alpha * lambda * delta2
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1 + learning_rate * lambda_param * delta_stage2
        
        # Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```