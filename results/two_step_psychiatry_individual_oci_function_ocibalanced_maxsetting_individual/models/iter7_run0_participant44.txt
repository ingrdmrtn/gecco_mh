Here are three cognitive models expressed as Python functions that incorporate the OCI score into the decision-making process.

### Model 1: Conditional Win-Stay (Reward-Dependent Habit)
This model hypothesizes that high OCI scores lead to a specific form of "stickiness" that is triggered only by success. Unlike general perseveration (repeating any choice), this mechanism represents a "successful ritual" bias: if an action yields a reward, the urge to repeat it is amplified by the OCI score, creating strong "lucky streaks" or superstitious adherence to a winning option.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Conditional Win-Stay Model.
    
    Hypothesis: High OCI participants exhibit enhanced "Win-Stay" behavior. 
    While standard stickiness applies to the previous action regardless of outcome,
    this model adds a choice bonus to the previous action ONLY if it was rewarded.
    The magnitude of this bonus is scaled by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - lambd: [0, 1] Eligibility trace decay (credit assignment to Stage 1).
    - winstay_gain: [0, 5] Scaling factor for the win-stay bonus based on OCI.
    """
    learning_rate, beta, w, lambd, winstay_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Bonus vector for win-stay
    win_stay_bonus = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Win-Stay bonus (conditional on previous reward)
        # The bonus is applied to the logits
        logits = beta * q_net + win_stay_bonus
        
        exp_q1 = np.exp(logits - np.max(logits)) # sub max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2 - np.max(beta * qs2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Update Win-Stay Bonus for next trial
        win_stay_bonus[:] = 0.0 # Reset
        if r > 0: # If rewarded
            # The urge to repeat the successful action is scaled by OCI
            win_stay_bonus[a1] = winstay_gain * oci_score

        # Standard SARSA/Q-learning updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Disappointment Hypersensitivity (Asymmetric Distortion)
This model suggests that high OCI leads to an asymmetric reaction to prediction errors. Specifically, "disappointment" (negative prediction error) is amplified by the OCI score. This reflects a mechanism where the participant is overly sensitive to failure or lack of reward, causing them to devalue unrewarded options more aggressively than a standard learner (rigidity in avoidance).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Enhanced Disappointment Model.
    
    Hypothesis: High OCI participants are hypersensitive to negative outcomes (disappointment).
    When the Prediction Error (PE) is negative (outcome worse than expected), 
    the effective PE is amplified by the OCI score. This causes rapid devaluation 
    of options that fail to pay out, leading to faster switching after losses compared to gains.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lambd: [0, 1] Eligibility trace.
    - disappoint_factor: [0, 5] Factor by which OCI amplifies negative prediction errors.
    """
    learning_rate, beta, w, lambd, disappoint_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net - np.max(beta * q_net))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2 - np.max(beta * qs2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates with Disappointment Distortion ---
        
        # Stage 2 PE
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # If the outcome was worse than expected (negative PE), amplify it based on OCI
        if delta_stage2 < 0:
            effective_delta2 = delta_stage2 * (1.0 + disappoint_factor * oci_score)
        else:
            effective_delta2 = delta_stage2

        q_stage2_mf[state_idx, a2] += learning_rate * effective_delta2
        
        # Stage 1 PE
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Note: We apply the distortion primarily to the reward reception (Stage 2), 
        # but the eligibility trace carries this distorted signal back to Stage 1.
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage1_mf[a1] += learning_rate * lambd * effective_delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage 2 Hyper-Plasticity (Immediate Outcome Focus)
This model proposes that high OCI is associated with an intense focus on immediate outcomes (the aliens/coins) rather than the abstract structure of the task. Mechanistically, this is implemented as a learning rate boost specifically for the Stage 2 values. High OCI participants update their beliefs about the aliens much faster than low OCI participants, making their model-based system (which relies on these values) more volatile.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Driven Stage 2 Hyper-Plasticity Model.
    
    Hypothesis: High OCI participants exhibit "hyper-plasticity" regarding immediate outcomes.
    They update the values of the specific terminal states (Aliens) more aggressively than 
    the transition states. This results in a volatile value function where recent wins/losses 
    at the second stage have a disproportionately large effect on subsequent behavior.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate (for Stage 1 and baseline Stage 2).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lambd: [0, 1] Eligibility trace.
    - s2_gain: [0, 5] Multiplier for OCI to boost the Stage 2 learning rate.
    """
    learning_rate, beta, w, lambd, s2_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate the effective learning rate for Stage 2 based on OCI
    # It is the base rate scaled up by (1 + gain * OCI)
    # We clip to 1.0 to ensure stability
    lr_stage2 = learning_rate * (1.0 + s2_gain * oci_score)
    lr_stage2 = min(lr_stage2, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net - np.max(beta * q_net))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2 - np.max(beta * qs2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        
        # Use the OCI-boosted learning rate for Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        
        # Use the base learning rate for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Eligibility trace uses the base learning rate scale for consistency with Stage 1 MF
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```