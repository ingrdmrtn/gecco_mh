Here are 3 new cognitive models based on the two-step task and the participant's low OCI score.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight (`w`)
This model assumes agents use a hybrid of Model-Based (MB) and Model-Free (MF) strategies. The parameter `w` controls the balance (w=1 is pure MB, w=0 is pure MF). Since OCI is linked to rigid, habitual behavior (often associated with MF) or sometimes over-deliberation (MB), we test if the weighting `w` is a function of the OCI score. Specifically, we use a logistic function to bound `w` between 0 and 1, where the OCI score shifts the midpoint.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is modulated by OCI.
    
    The mixing weight w is determined by a logistic function involving the OCI score.
    w = 1 / (1 + exp(-(w_logit_base + w_oci_coeff * oci)))
    
    Parameters:
    learning_rate: [0, 1] Learning rate for MF updates.
    beta: [0, 10] Inverse temperature.
    w_logit_base: [-5, 5] Base logit for the weighting parameter.
    w_oci_coeff: [-5, 5] Coefficient for how OCI affects the weighting logit.
    """
    learning_rate, beta, w_logit_base, w_oci_coeff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w using logistic function
    w_logit = w_logit_base + w_oci_coeff * oci_score
    w = 1.0 / (1.0 + np.exp(-w_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for stage 2 (aliens)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_stage1_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 1 MF update (TD(1) style for simplicity or SARSA)
        # Here we use the standard hybrid update:
        # Q_MF(s1, a1) += lr * (Q_MF(s2, a2) - Q_MF(s1, a1)) + lr * lambda * delta2
        # But for standard hybrid often just simple TD:
        # We'll use simple TD(0) for stage 1 based on stage 2 value
        
        # Prediction error stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Prediction error stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Note: In full hybrid models, eligibility traces connect stage 2 error to stage 1.
        # Simplified here to just direct MF updates for clarity in the parameters requested.
        # To strictly follow "TD(1)" logic often used:
        q_stage1_mf[a1] += learning_rate * delta_stage2 

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Model-Based with OCI-Dependent Forgetting
This model posits that while the participant is Model-Based, their memory for the values of the second-stage options (the aliens) decays over time. The rate of this decay (forgetting) is hypothesized to be related to OCI. High OCI might correlate with "checking" behaviors or uncertainty, potentially modeled as a higher forgetting rate (uncertainty about past values), or conversely, rigid retention.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with value decay (forgetting) modulated by OCI.
    
    The Q-values for the second stage decay towards 0.5 (uncertainty/prior) on every trial.
    The decay rate is: decay = decay_base + decay_oci_slope * oci
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature.
    decay_base: [0, 1] Base decay rate.
    decay_oci_slope: [-1, 1] How OCI modifies the decay rate.
    """
    learning_rate, beta, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate, bounded [0, 1]
    raw_decay = decay_base + decay_oci_slope * oci_score
    decay = np.clip(raw_decay, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initial values at 0.5 (ambivalence)
    q_stage2_mf = np.ones((2, 2)) * 0.5

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update chosen option
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # --- Forgetting ---
        # Decay all values towards 0.5
        # Q(t+1) = (1-decay)*Q(t) + decay*0.5
        q_stage2_mf = (1 - decay) * q_stage2_mf + decay * 0.5

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Success-Predictor Learner (PrModel) with OCI Modulation
This model uses a "Success-Predictor" strategy (often called PrModel in literature). Instead of calculating values, the agent learns a "probability of success" for each first-stage action directly, updated only when a reward occurs. This is a heuristic strategy. We modulate the learning rate for this success predictor based on OCI, hypothesizing that OCI traits might affect how strongly success reinforces the top-level choice (habit formation).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Success-Predictor (Pr) model where the learning rate for the first stage
    is modulated by OCI.
    
    The agent maintains a 'propensity' for stage 1 actions based on direct reinforcement.
    Stage 2 is learned via standard Q-learning.
    
    lr_pr = lr_base * (1 + lr_oci_scale * oci)
    
    Parameters:
    lr_stage2: [0, 1] Learning rate for stage 2 (aliens).
    beta: [0, 10] Inverse temperature.
    lr_base: [0, 1] Base learning rate for stage 1 success predictor.
    lr_oci_scale: [-5, 5] Scaling factor for OCI on stage 1 learning rate.
    """
    lr_stage2, beta, lr_base, lr_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective learning rate for stage 1
    # Bounding it between 0 and 1
    eff_lr_pr = lr_base * (1.0 + lr_oci_scale * oci_score)
    eff_lr_pr = np.clip(eff_lr_pr, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Success predictor values for Stage 1 (initialized to 0)
    pr_stage1 = np.zeros(2) 
    
    # Q-values for Stage 2
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Based on Success Predictor) ---
        exp_q1 = np.exp(beta * pr_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (Based on Q-values) ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        
        # Stage 2 Update (Standard Q-learning)
        delta2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_stage2 * delta2
        
        # Stage 1 Update (Success Predictor)
        # Update rule: Pr(a1) <- Pr(a1) + lr * (Reward - Pr(a1))
        # This heuristic ignores the transition structure entirely.
        delta_pr = r - pr_stage1[a1]
        pr_stage1[a1] += eff_lr_pr * delta_pr

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```