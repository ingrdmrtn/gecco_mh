Here are three new cognitive models based on the participant data and OCI score.

### Model 1: OCI-Modulated Asymmetric Learning Rate
This model hypothesizes that OCI severity specifically modulates how participants learn from negative prediction errors (disappointments). High OCI might be associated with a "negativity bias" or rigidity, leading to different updating speeds after losses compared to wins.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Asymmetric Learning Rate Model.
    
    Hypothesis: OCI score modulates the learning rate specifically for negative prediction errors 
    (disappointments). Positive learning rate is constant, but negative learning rate varies 
    linearly with OCI, reflecting potential negativity bias or rigidity.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - lr_neg_oci: [-1, 1] Slope of OCI modulation on negative learning rate.
    - beta: [0, 10] Inverse temperature (softness of choice).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Calculate modulated negative learning rate
    lr_neg = lr_neg_base + lr_neg_oci * oci_val
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: 
            prev_a1 = -1
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        stick_bonus = np.zeros(2)
        if prev_a1 != -1:
            stick_bonus[prev_a1] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        # ACTION VALUE UPDATING FOR CHOICE 1 (MF)
        alpha_1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        # ACTION VALUE UPDATING FOR CHOICE 2 (MF)
        alpha_2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2
        
        # Eligibility trace: update Stage 1 based on Stage 2 outcome
        q_stage1_mf[a1] += alpha_2 * delta_stage2
        
        prev_a1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Exponential Model-Based Impairment
This model assumes that the capacity for Model-Based (goal-directed) control decays exponentially as OCI symptoms increase. This reflects a rapid functional impairment in planning associated with compulsivity, distinct from a linear decline.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Dependent Exponential Model-Based Impairment.
    
    Hypothesis: The weight of Model-Based control (w) decays exponentially with increasing OCI scores.
    This models a rapid loss of goal-directed control as compulsive symptoms increase.
    w = w_max * exp(-oci_decay * oci)
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_max: [0, 1] Maximum model-based weight (at OCI=0).
    - oci_decay: [0, 10] Rate at which w decays with OCI.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    learning_rate, beta, w_max, oci_decay, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    # Exponential decay of MB weight
    w = w_max * np.exp(-oci_decay * oci_val)
    w = np.clip(w, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            prev_a1 = -1
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_a1 != -1:
            stick_bonus[prev_a1] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        # ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        prev_a1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Reward Valuation
This model posits that OCI scores modulate the subjective utility of the reward itself. High OCI might lead to devaluation (apathy) or hypersensitivity to rewards, scaling the effective reward signal `R` used in TD updates.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Reward Valuation Model.
    
    Hypothesis: OCI scores modulate the subjective utility (valuation) of the reward. 
    The effective reward used for updates is R * (1 + reward_oci_mod * OCI).
    This captures potential anhedonia or reward hypersensitivity associated with symptoms.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stickiness: [0, 5]
    - reward_oci_mod: [-1, 1] Modulation factor for reward value (negative=devaluation).
    """
    learning_rate, beta, w, stickiness, reward_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            prev_a1 = -1
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Modulate reward value
        r_effective = r * (1.0 + reward_oci_mod * oci_val)

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_a1 != -1:
            stick_bonus[prev_a1] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        # ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r_effective - q_stage2_mf[s_idx, a2]
        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        prev_a1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```