Here are three cognitive models designed to explain the participant's behavior, incorporating the OCI score to modulate specific decision-making mechanisms.

### Model 1: Coupled OCI Strategy-Perseveration Model
This model tests the hypothesis that OCD symptoms (OCI) simultaneously degrade goal-directed (model-based) control and enhance habitual perseveration. Instead of treating these as independent effects, a single `coupling` parameter links OCI to both a decrease in the model-based weight `w` and an increase in `stickiness`.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Coupled OCI Strategy-Perseveration Model.
    
    Hypothesis: OCI acts as a 'compulsivity' factor that simultaneously reduces model-based control (w)
    and increases perseveration (stickiness). A single coupling parameter controls this trade-off.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature (decision precision).
    - w_base: [0, 1] Base model-based weight (for OCI=0).
    - stick_base: [0, 5] Base stickiness (for OCI=0).
    - coupling: [0, 5] Strength of OCI effect (w decreases, stickiness increases).
        w = w_base - coupling * oci
        stickiness = stick_base + coupling * oci
    """
    lr, beta, w_base, stick_base, coupling = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate coupled parameters based on OCI
    w = w_base - (coupling * current_oci)
    w = np.clip(w, 0.0, 1.0)
    
    stickiness = stick_base + (coupling * current_oci)
    stickiness = np.clip(stickiness, 0.0, 10.0) # Clip to reasonable upper bound
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Prediction errors
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Q-values (TD(1) for Stage 1 MF)
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Loss Aversion Model
This model hypothesizes that OCI relates to an increased sensitivity to negative outcomes (losses). Given the presence of negative rewards (-1.0) in the data, this model scales the magnitude of negative rewards based on the OCI score. Higher OCI leads to stronger "loss aversion" or impact of punishment on value updates.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Loss Aversion Model.
    
    Hypothesis: OCI modulates the subjective weight of negative rewards (losses). 
    Participants with higher OCI weigh losses more heavily in their value updates.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stickiness: [0, 5] Choice perseveration.
    - loss_sens_base: [0, 5] Base sensitivity to losses.
    - loss_sens_oci_mod: [0, 10] OCI modulation of loss sensitivity.
        loss_weight = loss_sens_base + (loss_sens_oci_mod - 5.0) * oci
    """
    lr, beta, w, stickiness, loss_sens_base, loss_sens_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate loss weight
    # loss_sens_oci_mod centered at 5.0 allows for positive or negative modulation
    mod_val = loss_sens_oci_mod - 5.0
    loss_weight = loss_sens_base + (mod_val * current_oci)
    loss_weight = np.maximum(loss_weight, 0.0) # Ensure non-negative
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Apply loss aversion: scale reward if it is a loss (negative)
        if r < 0:
            eff_r = r * loss_weight
        else:
            eff_r = r
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = eff_r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 1 Precision Model
This model tests whether OCI specifically affects the decision noise (inverse temperature $\beta$) during the planning/choice phase (Stage 1), as opposed to the harvesting phase (Stage 2). It allows for separate precision parameters for Stage 1 and Stage 2, with OCI modulating only the Stage 1 precision.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 1 Precision Model.
    
    Hypothesis: OCI specifically affects the decision noise in Stage 1 (planning/choice),
    while Stage 2 (harvesting) remains distinct. High OCI might lead to either
    more rigid (high beta) or more chaotic (low beta) initial choices.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_2: [0, 10] Inverse temperature for Stage 2 (fixed).
    - beta_1_base: [0, 10] Base inverse temperature for Stage 1.
    - beta_1_oci_mod: [0, 10] OCI modulation for Stage 1 beta.
        beta_1 = beta_1_base + (beta_1_oci_mod - 5.0) * oci
    - w: [0, 1] Model-based weight.
    - stickiness: [0, 5] Choice perseveration.
    """
    lr, beta_2, beta_1_base, beta_1_oci_mod, w, stickiness = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 1 Beta based on OCI
    mod_val = beta_1_oci_mod - 5.0
    beta_1 = beta_1_base + (mod_val * current_oci)
    beta_1 = np.clip(beta_1, 0.0, 10.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Use OCI-modulated beta_1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Use fixed beta_2
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += lr * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```