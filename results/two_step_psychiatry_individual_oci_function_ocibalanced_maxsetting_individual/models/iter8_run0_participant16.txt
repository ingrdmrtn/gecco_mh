Here are three new cognitive models designed to explain the participant's behavior, incorporating the OCI score to modulate specific cognitive mechanisms.

### Model 1: Confidence-Modulated Beta Model
This model hypothesizes that the participant's exploration-exploitation balance (inverse temperature $\beta$) is not static but fluctuates based on recent success ("confidence"). A reward increases confidence, leading to more exploitation (higher $\beta$), while a lack of reward decreases confidence, leading to more exploration (lower $\beta$). The OCI score modulates the sensitivity of this confidence mechanism, testing the hypothesis that OCI severity correlates with how strongly outcomes impact decision consistency.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Confidence-Modulated Beta Model.
    
    Hypothesis: The balance between exploration and exploitation (Beta) is modulated 
    by recent success (Confidence). A win increases Beta (exploit), a loss decreases 
    Beta (explore). OCI severity modulates the magnitude of this confidence effect.
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - stickiness: [0, 5] Bonus added to the previously chosen option (perseveration).
    - conf_sens_low: [0, 5] Sensitivity of beta to reward history for Low OCI.
    - conf_sens_high: [0, 5] Sensitivity of beta to reward history for High OCI.
    """
    lr, beta_base, w, stickiness, conf_sens_low, conf_sens_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate sensitivity based on OCI score
    conf_sens = conf_sens_low * (1 - current_oci) + conf_sens_high * current_oci
    
    # Initialize Q-values and Transition Matrix
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action_1 = -1
    last_reward = 0.5 # Initialize neutral confidence
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Skip invalid trials
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
        
        # Calculate Effective Beta based on previous outcome
        # (last_reward - 0.5) ranges from -0.5 (loss) to 0.5 (win)
        # If conf_sens > 0: Wins increase beta (exploit), Losses decrease beta (explore)
        beta_eff = beta_base * (1 + conf_sens * (last_reward - 0.5))
        beta_eff = max(0.0, beta_eff) # Ensure beta is non-negative

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1
        last_reward = r

    return -log_likelihood
```

### Model 2: Transition Belief Rigidity Model
This model focuses on the Model-Based (MB) component. Standard MB learning assumes the agent learns transition probabilities or uses a fixed model. This model proposes that the agent uses a mixture of a fixed prior (the instructions: 70% common) and empirically learned transition counts. The OCI score modulates the weight of the fixed prior. High OCI is hypothesized to correlate with "rigidity," leading the agent to rely more on the fixed instruction-based prior and less on the observed transition statistics of the specific block.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Transition Belief Rigidity Model.
    
    Hypothesis: The Model-Based system relies on a transition matrix. This matrix is a mixture 
    of a fixed prior (instruction-based) and learned counts. OCI modulates the weight of the 
    fixed prior. High OCI leads to rigidity (ignoring observed transition statistics), while 
    Low OCI allows for adaptive learning of transitions.
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - stickiness: [0, 5] Choice stickiness.
    - prior_w_low: [0, 1] Weight of the fixed prior (vs learned counts) for Low OCI.
    - prior_w_high: [0, 1] Weight of the fixed prior (vs learned counts) for High OCI.
    """
    lr, beta, w, stickiness, prior_w_low, prior_w_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Determine the weight of the fixed prior based on OCI
    prior_weight = prior_w_low * (1 - current_oci) + prior_w_high * current_oci
    
    # Value initialization
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Transition Learning initialization
    # Start with counts of 1 (Laplace smoothing)
    trans_counts = np.ones((2, 2)) 
    fixed_prior = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # --- Compute Transition Matrix ---
        # Normalize learned counts to get probabilities
        row_sums = trans_counts.sum(axis=1, keepdims=True)
        learned_trans = trans_counts / row_sums
        
        # Mix the fixed prior with the learned matrix
        mixed_trans = prior_weight * fixed_prior + (1 - prior_weight) * learned_trans
        
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = mixed_trans @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Update Values
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Update Transition Counts (Adaptive Learning)
        trans_counts[a1, s_idx] += 1
        
        last_action_1 = a1

    return -log_likelihood
```

### Model 3: OCI-Modulated Q-Value Forgetting Model
This model introduces a memory decay mechanism where the values of unchosen options "forget" or decay towards zero over time. The rate of this forgetting is modulated by the OCI score. This tests the hypothesis that OCI symptoms relate to the persistence of value representations; high OCI might be associated with "obsessive" maintenance of values (low forgetting) or, conversely, a difficulty in maintaining value representations without reinforcement (high forgetting).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Q-Value Forgetting Model.
    
    Hypothesis: Participants may decay the value of unchosen options over time (forgetting). 
    OCI severity modulates this decay rate. This allows distinguishing between 'sticky' 
    value retention (low forgetting) and rapid value degradation (high forgetting) 
    as a function of compulsivity.
    
    Parameters:
    - lr: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - stickiness: [0, 5] Choice stickiness.
    - forget_low: [0, 1] Decay rate for unchosen options for Low OCI.
    - forget_high: [0, 1] Decay rate for unchosen options for High OCI.
    """
    lr, beta, w, stickiness, forget_low, forget_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Determine forgetting rate based on OCI
    forget_rate = forget_low * (1 - current_oci) + forget_high * current_oci
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action_1 = -1
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Stage 2 Update (Chosen)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 2 Forgetting (Unchosen)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1 - forget_rate)

        # Stage 1 Update (Chosen)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 1 Forgetting (Unchosen)
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1 - forget_rate)
        
        last_action_1 = a1

    return -log_likelihood
```