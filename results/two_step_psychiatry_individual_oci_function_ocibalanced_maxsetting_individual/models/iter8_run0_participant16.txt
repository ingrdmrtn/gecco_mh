Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: OCI-Modulated Model-Based Weighting with Stickiness
This model hypothesizes that individuals with higher OCI scores might rely more on habitual (model-free) control and exhibit repetitive behaviors (stickiness), potentially at the expense of flexible, goal-directed (model-based) planning. It introduces a choice stickiness parameter and allows the weighting between model-based and model-free systems (`w`) to be modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner with choice stickiness.
    OCI modulates the balance between Model-Based and Model-Free control (w).
    High OCI is hypothesized to reduce Model-Based control (lower w).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness: [0, 5]
    w_base: [0, 1]
    w_oci_mod: [-1, 1]
    """
    learning_rate, beta, stickiness, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective w based on OCI
    w = w_base + (w_oci_mod * current_oci)
    if w > 1.0: w = 1.0
    if w < 0.0: w = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous choice for stickiness
    prev_action_1 = -1

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        q_net_1_sticky = q_net_1.copy()
        if prev_action_1 != -1:
            q_net_1_sticky[int(prev_action_1)] += stickiness

        exp_q1 = np.exp(beta * q_net_1_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
            prev_action_1 = a1 # Update previous action
        else:
            p_choice_1[trial] = 1.0
            prev_action_1 = -1
            continue
            
        state_idx = int(state[trial])

        # Policy for the second choice (Standard Softmax)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]

            # Update Stage 2 values (Model-Free)
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            
            # Update Stage 1 values (Model-Free using TD-error)
            # Note: In standard hybrid models, MF update for stage 1 uses the value of the state reached
            # or the prediction error from stage 2. Here we use a standard TD(1) style update or 
            # simple Q-learning update. Let's use the max of next stage as target for MF update.
            # However, the template suggests updating based on stage 2 value.
            
            # Use the value of the chosen stage 2 action to update stage 1
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage 2 Learning Rate Asymmetry
This model suggests that OCI relates to how individuals process positive versus negative outcomes. Specifically, it tests if high OCI leads to a learning bias where negative outcomes (receiving 0 coins) are learned from differently than positive ones, potentially reflecting altered sensitivity to punishment or lack of reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free learner where OCI modulates the learning rate for negative outcomes (0 reward).
    This tests if OCI is related to hypersensitivity or hyposensitivity to 'failure' (no reward).
    
    Bounds:
    lr_pos: [0, 1]
    lr_neg_base: [0, 1]
    lr_neg_oci_mod: [-1, 1]
    beta: [0, 10]
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (lr_neg_oci_mod * current_oci)
    if lr_neg > 1.0: lr_neg = 1.0
    if lr_neg < 0.0: lr_neg = 0.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice (Pure MF)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue
            
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]
            
            # Determine which learning rate to use
            current_lr = lr_pos if r > 0 else lr_neg

            # Update Stage 2
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
            
            # Update Stage 1
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += current_lr * delta_stage1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Transition Learning (State Prediction Error)
This model posits that while the transition probabilities (0.7/0.3) are usually fixed in standard analyses, participants might actually learn them over time. High OCI might be associated with a more rigid belief system or faster updating of structural knowledge. Here, OCI modulates the learning rate specifically for the state transition probabilities, affecting the Model-Based component.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner that dynamically learns the transition matrix.
    OCI modulates the learning rate for the state transitions (lr_trans).
    High OCI might lead to over-updating or rigidity in structural learning.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    lr_trans_base: [0, 1]
    lr_trans_oci_mod: [-1, 1]
    """
    learning_rate, beta, w, lr_trans_base, lr_trans_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective transition learning rate
    lr_trans = lr_trans_base + (lr_trans_oci_mod * current_oci)
    if lr_trans > 1.0: lr_trans = 1.0
    if lr_trans < 0.0: lr_trans = 0.0
    
    # Initialize transition matrix (start with uniform or prior belief)
    # Rows: Action (Spaceship A/U), Cols: State (Planet X/Y)
    # Initializing near 0.5 implies uncertainty.
    est_transition_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB value calculation using dynamic transition matrix
        q_stage1_mb = est_transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue
            
        state_idx = int(state[trial])

        # Update Transition Matrix based on observed state
        # Create a one-hot vector for the observed state
        state_one_hot = np.zeros(2)
        state_one_hot[state_idx] = 1.0
        
        # Delta rule update for transitions
        # Prediction error = Observed State - Expected State Probabilities
        trans_pe = state_one_hot - est_transition_matrix[a1]
        est_transition_matrix[a1] += lr_trans * trans_pe
        
        # Ensure rows sum to 1 (normalization)
        est_transition_matrix[a1] /= np.sum(est_transition_matrix[a1])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]

            # Update Stage 2
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            
            # Update Stage 1
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```