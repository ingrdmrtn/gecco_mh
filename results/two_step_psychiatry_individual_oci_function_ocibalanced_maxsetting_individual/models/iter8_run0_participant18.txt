Here are three new cognitive models that explore different mechanisms by which OCI scores might influence decision-making in the two-step task, specifically focusing on how obsessive-compulsive traits might alter learning rates, exploration, or the balance between model-based and model-free control.

### Cognitive Model 1: OCI-Modulated Eligibility Traces
This model hypothesizes that high OCI scores lead to "over-thinking" or stronger retrospective attribution of credit. In reinforcement learning terms, this is modeled as an eligibility trace ($\lambda$) that is modulated by the OCI score. A higher OCI score increases $\lambda$, meaning the agent connects the reward at the second stage more strongly back to the first-stage choice, effectively blurring the distinction between the two stages.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Traces.
    
    This model assumes a hybrid learner where the eligibility trace (lambda) 
    is a function of the OCI score. 
    High OCI -> Higher lambda (stronger credit assignment from stage 2 reward to stage 1 choice).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - lambda_base: [0, 1] Base eligibility trace parameter.
    - lambda_oci_slope: [0, 1] Sensitivity of lambda to OCI score.
    """
    learning_rate, beta, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective lambda, bounded between 0 and 1
    eff_lambda = lambda_base + (lambda_oci_slope * oci_score)
    eff_lambda = np.clip(eff_lambda, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1 = np.zeros(2)      # MF values for stage 1
    q_stage2 = np.zeros((2, 2)) # MF values for stage 2

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1: continue

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Prediction error at stage 2 (reward - Q2)
        delta_stage2 = r - q_stage2[s2, a2]
        
        # Prediction error at stage 1 (Q2 - Q1)
        delta_stage1 = q_stage2[s2, a2] - q_stage1[a1]

        # Update Stage 2
        q_stage2[s2, a2] += learning_rate * delta_stage2
        
        # Update Stage 1:
        # Standard TD(0) part: learning_rate * delta_stage1
        # Eligibility trace part: learning_rate * lambda * delta_stage2
        q_stage1[a1] += learning_rate * delta_stage1 + (learning_rate * eff_lambda * delta_stage2)

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Exploration Asymmetry
This model suggests that OCI traits influence how "sticky" or persistent a participant is, but specifically by modulating the inverse temperature ($\beta$). This reflects the hypothesis that high OCI individuals might be more rigid (higher $\beta$, more deterministic) or more uncertain/anxious (lower $\beta$, more random). Here, we model $\beta$ as a linear function of the OCI score, allowing the data to determine if OCI increases or decreases exploration noise.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Exploration (Beta Modulation).
    
    This model posits that the OCI score directly affects the exploration-exploitation 
    trade-off (inverse temperature beta).
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - beta_intercept: [0, 10] Baseline inverse temperature.
    - beta_oci_slope: [-5, 5] Effect of OCI on beta (can be positive or negative).
      (Note: we map the [0,1] param input to a range that allows negative slope).
    """
    learning_rate, w, beta_intercept, beta_oci_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Map beta_oci_param from [0,1] to [-5, 5] to allow OCI to increase OR decrease beta
    beta_oci_slope = (beta_oci_param * 10.0) - 5.0
    
    # Calculate effective beta, ensure it stays non-negative
    beta_eff = beta_intercept + (beta_oci_slope * oci_score)
    beta_eff = max(0.0, beta_eff)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1: continue

        # --- Stage 1 Policy (Hybrid) ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Specific Punishment Sensitivity
This model investigates if OCI scores correlate with an asymmetry in learning from positive vs. negative outcomes (rewards vs. omissions). It proposes that high OCI individuals might be hypersensitive to "failure" (0 coins), leading to a different learning rate for negative prediction errors compared to positive ones.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Specific Punishment Sensitivity.
    
    This model separates learning rates for positive prediction errors (doing better than expected)
    and negative prediction errors (doing worse than expected). The negative learning rate is 
    modulated by OCI, testing if high OCI leads to over-reaction to omitted rewards.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (delta > 0).
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors (delta < 0).
    - lr_neg_oci_slope: [0, 1] How much OCI increases sensitivity to negative errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    """
    lr_pos, lr_neg_base, lr_neg_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    lr_neg_eff = lr_neg_base + (lr_neg_oci_slope * oci_score)
    lr_neg_eff = np.clip(lr_neg_eff, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        if a1 == -1: continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        alpha_2 = lr_pos if delta_stage2 >= 0 else lr_neg_eff
        q_stage2_mf[s2, a2] += alpha_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 >= 0 else lr_neg_eff
        q_stage1_mf[a1] += alpha_1 * delta_stage1

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```