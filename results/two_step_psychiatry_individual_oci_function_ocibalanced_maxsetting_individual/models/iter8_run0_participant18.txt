Here are the three proposed cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Deterministic Belief (Black-and-White Thinking).
    Hypothesis: Participants with high OCI scores exhibit 'black-and-white' thinking, perceiving the probabilistic 
    transitions as more deterministic than they are. They mix the true transition model with a deterministic 
    model (where Action A -> Planet X with p=1), effectively ignoring the risk of rare transitions.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Choice perseveration [0,5]
    - oci_det_bias: Degree of deterministic bias scaled by OCI [0,1]
    """
    lr, beta, w, lambda_val, stickiness, oci_det_bias = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # True transition matrix
    # A0 -> P0 (0.7), P1 (0.3)
    # A1 -> P0 (0.3), P1 (0.7)
    trans_true = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Deterministic matrix (Black-and-white view)
    trans_det = np.array([[1.0, 0.0], [0.0, 1.0]])
    
    # Effective transition matrix based on OCI
    # bias = oci_det_bias * oci_score (clamped to 0-1)
    bias = min(max(oci_det_bias * oci_score, 0.0), 1.0)
    trans_eff = (1.0 - bias) * trans_true + bias * trans_det
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2)) # State (Planet) x Action (Alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 0.5
            p_choice_2[t] = 0.5
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t]) # Planet
        a2 = int(action_2[t])
        r = reward[t]
        
        # Stage 1 Choice
        # MB Value calculation using distorted transition matrix
        max_q_s2 = np.max(q_mf_s2, axis=1) # Max value of aliens on each planet
        q_mb_s1 = trans_eff @ max_q_s2
        
        # Net Value
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        # Logits
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        # Softmax
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # Stage 2 Choice
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # Learning
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        # Eligibility Trace (Lambda) update for Stage 1
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1
        
    # Log Loss
    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Surprise Learning (Rare Transition Reactivity).
    Hypothesis: High OCI is associated with hyper-vigilance to error or surprise. When a 'Rare' transition occurs 
    (violating the common structure), OCI participants increase their learning rate to update their internal models 
    more aggressively than for common transitions.
    
    Parameters:
    - lr: Base learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Choice perseveration [0,5]
    - oci_surprise: Boost to LR on rare transitions, scaled by OCI [0,5]
    """
    lr, beta, w, lambda_val, stickiness, oci_surprise = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 0.5
            p_choice_2[t] = 0.5
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Determine if transition was common or rare
        # A0 -> S0 (Common), S1 (Rare)
        # A1 -> S0 (Rare), S1 (Common)
        is_rare = False
        if a1 == 0 and s2 == 1:
            is_rare = True
        elif a1 == 1 and s2 == 0:
            is_rare = True
            
        # Adjust Learning Rate
        lr_eff = lr
        if is_rare:
            lr_eff = lr * (1.0 + oci_surprise * oci_score)
            # Clip to 1.0 to avoid instability
            lr_eff = min(lr_eff, 1.0)
            
        # Stage 1 Policy
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = trans_matrix @ max_q_s2
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # Updates
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr_eff * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr_eff * delta_2
        
        q_mf_s1[a1] += lr_eff * lambda_val * delta_2
        
        prev_a1 = a1
        
    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Win-Stickiness (Reinforcement Rigidity).
    Hypothesis: OCI participants exhibit excessive repetition of actions that have just been rewarded. 
    While general stickiness applies regardless of outcome, this model posits an additional 'Win-Stay' 
    tendency that is exacerbated by OCI severity.
    
    Parameters:
    - lr: Learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stick_base: General choice perseveration [0,5]
    - oci_win_stick: Additional stickiness after a win, scaled by OCI [0,5]
    """
    lr, beta, w, lambda_val, stick_base, oci_win_stick = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 0.5
            p_choice_2[t] = 0.5
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Stage 1 Policy
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = trans_matrix @ max_q_s2
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        
        logits_1 = beta * q_net_s1
        
        # Apply Stickiness
        if prev_a1 != -1:
            # Base stickiness
            stick_val = stick_base
            # OCI-Modulated Win Stickiness
            if prev_reward == 1:
                stick_val += oci_win_stick * oci_score
            
            logits_1[prev_a1] += stick_val
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # Updates
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        q_mf_s1[a1] += lr * lambda_val * delta_2
        
        prev_a1 = a1
        prev_reward = r
        
    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```