Here are three new cognitive models for the two-step task, incorporating the OCI score to explain individual variability in decision-making strategies.

### Cognitive Model 1: OCI-Modulated Goal Stickiness
This model hypothesizes that high OCI scores are associated with a "Model-Based Win-Stay" mechanism. Unlike standard stickiness (repeating an action), this mechanism implies a desire to return to the *state* (Planet) that was previously rewarding. If a participant receives a reward on a planet, they temporarily boost the value of that planet in their Model-Based planning. Crucially, if the previous reward was obtained after a rare transition, this model predicts a switch to the spaceship that *reliably* leads to the rewarding planet (Win-Shift behavior), distinguishing it from simple motor perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Goal Stickiness (Model-Based Win-Stay).
    
    Hypothesis: High OCI participants exhibit stickiness to the *outcome state* (Planet)
    rather than the action itself. If a planet yielded a reward, its value is temporarily
    boosted in the Model-Based calculation for the next trial. This leads to 
    'Win-Stay' for common transitions but 'Win-Shift' for rare transitions (switching 
    to the spaceship that reliably goes to the winning planet).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for value estimation.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - goal_stick: [0, 5] Bonus added to the previous winning planet's value, scaled by OCI.
    """
    learning_rate, beta, w, goal_stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    stickiness_mag = goal_stick * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_planet = -1
    last_reward = 0.0
    
    for trial in range(n_trials):
        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Value of each planet
        
        # Apply Goal Stickiness Bonus to MB calculation
        # We modify the values fed into the transition matrix for the current decision
        current_planet_values = max_q_stage2.copy()
        if last_planet != -1 and last_reward == 1.0:
            current_planet_values[last_planet] += stickiness_mag
            
        q_stage1_mb = transition_matrix @ current_planet_values
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2 Choice
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_planet = s_idx
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Luck Discounting
This model proposes that high OCI is associated with an intolerance of uncertainty or a distrust of "unreliable" events. When a participant experiences a rare transition (e.g., Spaceship A going to Planet Y), they view the outcome of that trial as "lucky" or aberrant. Consequently, they discount the prediction error, learning less from these trials compared to trials with common transitions.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Luck Discounting.
    
    Hypothesis: High OCI participants distrust outcomes resulting from rare (unexpected) 
    transitions. They discount the learning signal (prediction error) when the transition 
    was rare, treating it as "luck" or "noise" rather than a reliable signal to update 
    their internal models.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - luck_discount: [0, 1] Factor reducing learning rate on rare transitions, scaled by OCI.
      Effective discount = luck_discount * OCI.
    """
    learning_rate, beta, w, luck_discount = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    discount_factor = luck_discount * oci_score
    # Clamp discount to [0, 1] to prevent negative learning rate
    discount_factor = min(max(discount_factor, 0.0), 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2 Policy
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Determine if transition was rare
        # Common: 0->0, 1->1. Rare: 0->1, 1->0.
        is_rare = (a1 != s_idx)
        
        current_lr = learning_rate
        if is_rare:
            current_lr = learning_rate * (1.0 - discount_factor)
            
        # Updates
        # Stage 1 MF update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Accelerated Habit Formation
This model posits that compulsivity is linked to an accelerated formation of habits (Model-Free associations). While the participant learns the value of stimuli (aliens) at a normal rate, the mapping from Stage 1 actions to values (the habit) is updated more aggressively in high OCI individuals. This leads to a faster "caching" of recent outcomes into the habitual controller, making them more reactive to recent history in their high-level choices.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Accelerated Habit Formation (Differential MF Learning).
    
    Hypothesis: Compulsivity is associated with an over-reliance on or faster acquisition 
    of Model-Free (habitual) associations. This model posits that OCI specifically 
    accelerates the learning rate for the Stage 1 Model-Free system, while the 
    Stage 2 (stimulus-outcome) learning rate remains baseline. This creates a 
    stronger habit bias in action selection over time.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate (for Stage 2).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - mf_boost: [0, 5] Proportional increase in Stage 1 MF learning rate, scaled by OCI.
      lr_mf = lr * (1 + mf_boost * OCI).
    """
    learning_rate, beta, w, mf_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate boosted MF learning rate
    lr_mf = learning_rate * (1.0 + mf_boost * oci_score)
    # Clamp to reasonable bounds [0, 1]
    lr_mf = min(lr_mf, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2 Policy
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Stage 1 MF uses boosted learning rate
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_mf * delta_stage1
        
        # Stage 2 uses base learning rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```