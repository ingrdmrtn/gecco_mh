Here are three new cognitive models designed to capture the behavior of the participant, specifically focusing on how Obsessive-Compulsive Inventory (OCI) scores might modulate reinforcement learning processes in the two-step task.

### Model 1: OCI-Modulated Eligibility Traces (Eligibility Trace Decay)
This model hypothesizes that individuals with different OCI scores process the credit assignment between the second-stage outcome and the first-stage choice differently. Specifically, it tests if OCI modulates the eligibility trace parameter ($\lambda$), which controls how much the Stage 2 reward prediction error "drifts back" to update Stage 1 values directly (Model-Free). A higher OCI might lead to stronger rigid connections between distal outcomes and initial choices (higher $\lambda$), or conversely, a more fragmented credit assignment.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Traces.
    
    Hypothesis: The strength of the eligibility trace (lambda), which connects 
    the second-stage reward error back to the first-stage choice in the model-free 
    system, is modulated by the OCI score. This reflects how strongly distal 
    outcomes reinforce proximal choices.

    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Mixing weight for Model-Based (1) vs Model-Free (0).
    lambda_base: [0,1] - Baseline eligibility trace parameter.
    lambda_oci_slope: [-1,1] - How OCI modifies the eligibility trace.
    
    Lambda calculation:
    lambda_param = lambda_base + lambda_oci_slope * oci
    (Bounded between 0 and 1)
    """
    learning_rate, beta, w, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate lambda based on OCI, clamping between 0 and 1
    lambda_param = lambda_base + lambda_oci_slope * participant_oci
    if lambda_param < 0: lambda_param = 0
    if lambda_param > 1: lambda_param = 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update (TD error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1 Update
        # Standard TD(0) update: delta = Q(s2, a2) - Q(s1, a1)
        # Eligibility trace update adds lambda * delta_stage2
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Combined update: Regular TD(0) part + Eligibility Trace part
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model proposes that OCI symptoms relate to a bias in how positive versus negative prediction errors are learned. Individuals with higher compulsivity might be more sensitive to punishments (lack of reward) or less sensitive to rewards, leading to asymmetric updates. We model separate learning rates for positive and negative prediction errors at the second stage, where the ratio or balance is shifted by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Asymmetric Learning Rates Modulated by OCI.
    
    Hypothesis: OCI affects the balance between learning from positive outcomes (rewards)
    vs negative outcomes (omissions). High OCI might lead to hyper-sensitivity to
    negative prediction errors (avoidance learning) or blunted reward learning.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors (RPE > 0).
    lr_neg_base: [0,1] - Base learning rate for negative prediction errors (RPE < 0).
    lr_neg_oci_slope: [-1,1] - How OCI scales the negative learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weighting.
    
    The effective negative learning rate is:
    lr_neg = lr_neg_base + lr_neg_oci_slope * oci
    """
    lr_pos, lr_neg_base, lr_neg_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    lr_neg = lr_neg_base + lr_neg_oci_slope * participant_oci
    if lr_neg < 0: lr_neg = 0
    if lr_neg > 1: lr_neg = 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        current_lr = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        # Use the same LR logic for stage 1 MF update for consistency
        current_lr_s1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += current_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Transition Learning Rate
The standard two-step model assumes the transition matrix (0.7/0.3) is fixed and known. However, participants often learn or update their belief about these transitions online. This model posits that OCI affects "uncertainty intolerance" or checking behavior, manifesting as a dynamic updating of the transition matrix where the learning rate for transitions is modulated by OCI. High OCI might lead to over-updating beliefs about transitions based on rare events.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Transition Learning Rate.
    
    Hypothesis: While the task has stable transitions (70/30), participants may 
    continuously update their internal model of the spaceship-planet transitions.
    High OCI might correlate with over-sensitivity to "rare" transitions, 
    causing faster (or slower) updating of the internal transition matrix.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weight.
    trans_lr_base: [0,1] - Base learning rate for the transition matrix.
    trans_lr_oci_slope: [-1,1] - How OCI modulates transition learning.
    
    The transition learning rate is:
    lr_trans = trans_lr_base + trans_lr_oci_slope * oci
    """
    learning_rate, beta, w, trans_lr_base, trans_lr_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    lr_trans = trans_lr_base + trans_lr_oci_slope * participant_oci
    if lr_trans < 0: lr_trans = 0
    if lr_trans > 1: lr_trans = 1

    # Initialize internal transition model (start with uniform or slight prior)
    # Rows: Action 1 (0 or 1), Cols: State (Planet 0 or 1)
    # We initialize to 0.5 to represent initial uncertainty, or 0.7 if we assume instruction knowledge
    # Let's assume they start with the instructed knowledge but update it.
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy (Model-Based uses dynamic trans_probs)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Value Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Transition Matrix Update
        # We observed transition from a1 to s_idx.
        # Create a 1-hot vector for the observed state
        observed_trans = np.zeros(2)
        observed_trans[s_idx] = 1.0
        
        # Update the row corresponding to the chosen action a1
        # trans_probs[a1] = (1 - lr_trans) * trans_probs[a1] + lr_trans * observed_trans
        trans_probs[a1] += lr_trans * (observed_trans - trans_probs[a1])
        
        # Ensure the other row maintains symmetry if we assume coupled learning, 
        # or just leave it. The standard assumption is often coupled, 
        # but here we update only the experienced transition row for simplicity of the "state prediction error".
        # However, to keep it a valid probability distribution, the row must sum to 1, which the formula above ensures.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```