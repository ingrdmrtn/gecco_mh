Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in this task, specifically focusing on stickiness, model-based/model-free arbitration, and learning rate asymmetries.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model assumes that individuals with higher OCI scores might rely more (or less) on rigid, habit-like (Model-Free) strategies versus flexible, planning (Model-Based) strategies. Here, the arbitration parameter `w` (weight of Model-Based control) is a logistic function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w'
    is determined by the OCI score via a logistic transformation.
    
    Higher OCI could potentially drive behavior more towards habits (w -> 0)
    or goal-directed planning (w -> 1), depending on the fitted parameters.

    Parameters:
    learning_rate : [0, 1] Learning rate for value updates.
    beta : [0, 10] Inverse temperature for softmax choice.
    w_intercept : [-5, 5] Intercept for the logistic function determining w.
    w_slope : [-5, 5] Slope for the logistic function determining w based on OCI.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate model-based weight w using logistic function of OCI
    # w will be bounded between 0 and 1
    w = 1.0 / (1.0 + np.exp(-(w_intercept + w_slope * current_oci)))
    
    # Transition matrix (fixed for this task structure)
    # A -> X (0.7), A -> Y (0.3); U -> X (0.3), U -> Y (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2 = np.zeros((2, 2)) # Values for stage 2 (states X, Y)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based value calculation: V_MB(s1) = T * max(Q(s2))
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Choice ---
        # Standard model-free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update (TD learning)
        delta_stage2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Model-Free update (TD learning)
        # Using the value of the chosen state in stage 2 as the target
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Perseveration (Stickiness) with Decay
This model posits that OCI affects "cognitive stickiness"â€”the tendency to repeat the previous choice regardless of reward. Unlike a simple bonus, this stickiness decays over time if not reinforced, representing a lingering obsessive thought or urge to repeat that fades slowly. The decay rate is modulated by OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner where OCI modulates the decay rate of a 'perseveration trace'.
    High OCI might lead to slower decay of the urge to repeat the last action.
    
    Parameters:
    learning_rate : [0, 1] Standard learning rate.
    beta : [0, 10] Inverse temperature.
    stickiness_weight : [0, 5] How strongly the perseveration trace affects choice.
    decay_base : [0, 1] Base decay rate of the perseveration trace.
    decay_oci_scale : [-1, 1] How OCI modifies the decay rate.
    """
    learning_rate, beta, stickiness_weight, decay_base, decay_oci_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate decay rate: decay = base + scale * OCI
    # Constrain decay to be between 0 and 1
    decay_rate = decay_base + decay_oci_scale * current_oci
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Perseveration trace for action 0 and action 1
    persev_trace = np.zeros(2) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Q_net = Q_MF + stickiness * trace
        q_net = q_stage1 + stickiness_weight * persev_trace
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update perseveration trace
        # The chosen action gets a boost of 1, then the whole trace decays
        # If decay_rate is high (close to 1), memory is short. If low, memory persists.
        # Here we implement it as: Trace(t+1) = Trace(t) * (1-decay) + Indicator(a)
        persev_trace *= (1.0 - decay_rate)
        persev_trace[a1] += 1.0

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update
        delta_stage2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD-0)
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI is associated with an imbalance in how positive vs. negative prediction errors are processed. Specifically, it tests if higher OCI leads to hyper-sensitivity to negative outcomes (punishment/loss of reward) or hypo-sensitivity to positive ones, affecting the learning rate for negative prediction errors.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with separate learning rates for positive and negative
    prediction errors. The negative learning rate is modulated by OCI.
    
    This tests the hypothesis that OCI is related to altered sensitivity to 
    'disappointment' or lack of reward (negative RPEs).

    Parameters:
    lr_pos : [0, 1] Learning rate for positive prediction errors (RPE > 0).
    lr_neg_base : [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    lr_neg_oci_slope : [-1, 1] Slope determining how OCI affects lr_neg.
    beta : [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-dependent negative learning rate
    lr_neg = lr_neg_base + lr_neg_oci_slope * current_oci
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update
        delta_stage2 = r - q_stage2[state_idx, a2]
        eff_lr2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2[state_idx, a2] += eff_lr2 * delta_stage2
        
        # Stage 1 update
        # Using the value of the chosen state in stage 2 (SARSA-like connection)
        delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
        eff_lr1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1[a1] += eff_lr1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```