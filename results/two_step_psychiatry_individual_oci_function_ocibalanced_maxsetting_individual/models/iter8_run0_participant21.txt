Here are three cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

### Model 1: Subjective Zero Model
This model hypothesizes that the participant's OCI score modulates their subjective valuation of the "no reward" outcome (receiving 0 coins). While the objective reward is 0, high-OCI individuals might perceive this differentlyâ€”either as a neutral event (value closer to 0 or even positive, potentially explaining "lose-stay" behavior due to weak negative reinforcement) or as a painful loss (negative value).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Zero Model.
    
    Hypothesis: The participant assigns a subjective value to the '0 coins' outcome,
    which deviates from the objective 0. This subjective value is modulated by OCI.
    If the subjective value of 0 is higher (less negative prediction error), 
    it explains the observed 'Lose-Stay' persistence (stickiness) after unrewarded trials.
    
    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    pers: [-2, 2] General stickiness to the previously chosen spaceship.
    zero_val_base: [-1, 1] Base subjective value for receiving 0 coins.
    zero_val_oci: [-2, 2] Slope of OCI modulation on the subjective zero value.
    """
    lr, beta, w, pers, zero_val_base, zero_val_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate subjective value for 0 reward
    # High OCI might lead to a different perception of failure (0 coins)
    subjective_zero = zero_val_base + zero_val_oci * current_oci
    
    # Initialize Q-values
    # Stage 1: 2 spaceships
    q_mf = np.zeros(2) 
    # Stage 2: 2 planets x 2 aliens
    q2 = np.zeros((2, 2)) 
    
    # Fixed transition matrix for MB calculation (0.7 probability to common planet)
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    log_lik = 0.0
    last_action = -1
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice ---
        # Model-Based Value: Expected max value of next stage
        max_q2 = np.max(q2, axis=1)
        q_mb = tm @ max_q2
        
        # Net Value
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Apply Perseveration
        if last_action != -1:
            q_net[last_action] += pers
            
        # Softmax Probability
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        
        # Accumulate Log Likelihood
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # --- Updates ---
        # Determine effective reward
        if r == 1:
            r_eff = 1.0
        else:
            r_eff = subjective_zero
            
        # Stage 2 Update (TD Error using effective reward)
        pe2 = r_eff - q2[s, a2]
        q2[s, a2] += lr * pe2
        
        # Stage 1 Update (TD Error using updated Stage 2 value)
        # Note: Standard hybrid models often update MF Q1 towards Q2
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        last_action = a1

    return -log_lik
```

### Model 2: Dynamic Transition Learning Model
This model hypothesizes that the participant does not trust the fixed transition probabilities (70/30) but continually updates their internal model of the spaceship-planet transitions. OCI modulates the *learning rate* of these transitions. High-OCI individuals might be hyper-sensitive to rare transitions (updating the belief rapidly), leading to instability in Model-Based planning, or conversely, very rigid (low learning rate).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Transition Learning Model.
    
    Hypothesis: The participant learns the transition matrix between spaceships and planets
    rather than assuming it is fixed. The rate at which they update this belief is
    modulated by OCI. This captures variability in how participants react to 
    rare transitions (e.g., ignoring them vs. restructuring their world model).
    
    Parameters:
    lr: [0, 1] Learning rate for reward value updates.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between Model-Based and Model-Free.
    pers: [-2, 2] Stickiness parameter.
    lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    lr_trans_oci: [-1, 1] OCI modulation of transition learning rate.
    """
    lr, beta, w, pers, lr_trans_base, lr_trans_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate transition learning rate (bounded 0-1)
    lr_trans = lr_trans_base + lr_trans_oci * current_oci
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize dynamic transition matrix
    # Start with assumption of 0.7 common, 0.3 rare as per instructions
    # Row 0: Spaceship A -> [Planet X, Planet Y]
    # Row 1: Spaceship U -> [Planet X, Planet Y]
    current_tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    log_lik = 0.0
    last_action = -1
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice ---
        # Use CURRENT dynamic transition matrix for MB calculation
        max_q2 = np.max(q2, axis=1)
        q_mb = current_tm @ max_q2
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action != -1:
            q_net[last_action] += pers
            
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # --- Value Updates ---
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        # --- Transition Matrix Update ---
        # Update P(s|a1) towards 1.0
        # P(other|a1) becomes 1 - P(s|a1)
        current_prob = current_tm[a1, s]
        new_prob = current_prob + lr_trans * (1.0 - current_prob)
        
        current_tm[a1, s] = new_prob
        current_tm[a1, 1-s] = 1.0 - new_prob
        
        last_action = a1

    return -log_lik
```

### Model 3: Outcome-Dependent Stickiness Model
This model hypothesizes that stickiness (perseveration) is not uniform but depends on the previous outcome (Win vs. Loss). OCI modulates the stickiness specifically after a Loss (0 coins). This directly addresses the "Lose-Stay" behavior observed in the data, positing that high-OCI participants may have a compulsive urge to repeat an action even after it fails.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Outcome-Dependent Stickiness Model.
    
    Hypothesis: Perseveration strength differs depending on whether the previous 
    trial was rewarded (Win) or unrewarded (Loss). OCI modulates the stickiness 
    specifically after a Loss.
    High OCI may lead to high 'Loss-Stickiness' (compulsive repetition despite failure),
    explaining blocks of repeated choices even with 0 rewards.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF weight.
    pers_win: [-2, 2] Stickiness after a reward (Win-Stay).
    pers_loss_base: [-2, 2] Base stickiness after a loss.
    pers_loss_oci: [-2, 2] OCI modulation of loss-stickiness.
    """
    lr, beta, w, pers_win, pers_loss_base, pers_loss_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Loss-Stickiness
    pers_loss = pers_loss_base + pers_loss_oci * current_oci
    
    q_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    log_lik = 0.0
    last_action = -1
    last_reward = -1 # Indicator for no previous reward
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice ---
        max_q2 = np.max(q2, axis=1)
        q_mb = tm @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Apply Outcome-Dependent Stickiness
        if last_action != -1:
            if last_reward == 1:
                q_net[last_action] += pers_win
            else:
                # Includes case where last_reward == 0
                q_net[last_action] += pers_loss
            
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # --- Updates ---
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        last_action = a1
        last_reward = r

    return -log_lik
```