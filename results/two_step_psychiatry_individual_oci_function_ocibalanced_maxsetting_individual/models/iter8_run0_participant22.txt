Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Stage 1 Learning Rate
This model hypothesizes that OCI scores specifically affect the learning rate for the first-stage choices (spaceships), creating a dissociation between high-level policy updating and low-level value learning. High OCI might lead to rigidity (low learning rate) or volatility (high learning rate) in spaceship preferences, independent of how they learn from the aliens.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 1 Learning Rate Model.

    Hypothesis: OCI scores modulate the learning rate specifically for the first-stage 
    choices (spaceship selection). High OCI may lead to rigid policy updating 
    (low stage 1 LR) or volatility, distinct from stage 2 learning.

    Parameters:
    - lr_s1_base: [0,1] Base learning rate for stage 1.
    - lr_s1_oci_slope: [-1,1] Modulation of stage 1 LR by OCI.
    - lr_s2: [0,1] Fixed learning rate for stage 2 (aliens).
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - stickiness: [0,5] Choice perseverance.
    """
    lr_s1_base, lr_s1_oci_slope, lr_s2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate modulated learning rate for stage 1
    raw_lr1 = lr_s1_base + lr_s1_oci_slope * oci_score
    lr_1 = np.clip(raw_lr1, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the previous choice
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1
        else:
            p_choice_1[trial] = 0.5
            prev_a1 = -1
            
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        if state_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Updates ---
                # Stage 1 Update (using modulated lr_1)
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += lr_1 * delta_stage1
                
                # Stage 2 Update (using fixed lr_s2)
                delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
                q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2
            else:
                p_choice_2[trial] = 0.5
        else:
            p_choice_2[trial] = 0.5

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Structural Learning
This model assumes that participants do not assume the transition probabilities are fixed, but learn them online. It hypothesizes that OCI modulates the rate of this structural learning. High OCI participants might be more rigid in their beliefs about the environment's structure (low structural learning rate) or conversely, hyper-sensitive to transition surprises.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Structural Learning Model.

    Hypothesis: Participants learn the transition matrix online. OCI scores modulate 
    the learning rate for this structural learning. High OCI might be associated 
    with rigid beliefs (low structural learning) or hyper-sensitivity to 
    transition surprises.

    Parameters:
    - lr_reward: [0,1] Learning rate for value updates (MF).
    - lr_trans_base: [0,1] Base learning rate for transition matrix updates.
    - lr_trans_oci_slope: [-1,1] Modulation of transition LR by OCI.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - stickiness: [0,5] Choice perseverance.
    """
    lr_reward, lr_trans_base, lr_trans_oci_slope, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate modulated transition learning rate
    raw_lr_trans = lr_trans_base + lr_trans_oci_slope * oci_score
    lr_trans = np.clip(raw_lr_trans, 0.0, 1.0)
    
    # Initialize transition matrix (start with standard assumption)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1
        else:
            p_choice_1[trial] = 0.5
            prev_a1 = -1
            
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        if state_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Updates ---
                # Value updates
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                q_stage1_mf[a1] += lr_reward * delta_stage1
                
                delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
                q_stage2_mf[state_idx, a2] += lr_reward * delta_stage2
                
                # Structural Update (Transition Matrix)
                # Update row a1. Target is 1 at state_idx, 0 elsewhere.
                target_dist = np.zeros(2)
                target_dist[state_idx] = 1.0
                transition_matrix[a1] += lr_trans * (target_dist - transition_matrix[a1])
                
            else:
                p_choice_2[trial] = 0.5
        else:
            p_choice_2[trial] = 0.5

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Asymmetric Learning
This model hypothesizes that OCI modulates sensitivity to negative prediction errors (punishments or worse-than-expected outcomes). High OCI participants might learn more effectively from failure (fear of mistakes) or less effectively (avoidance), creating an asymmetry in how positive and negative outcomes drive learning.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Asymmetric Learning Model.

    Hypothesis: OCI scores modulate the sensitivity to negative prediction errors.
    High OCI participants may learn differently from negative outcomes compared 
    to positive ones (valence-dependent learning).
    
    Parameters:
    - lr_pos: [0,1] Learning rate for positive prediction errors.
    - lr_neg_base: [0,1] Base learning rate for negative prediction errors.
    - lr_neg_oci_slope: [-1,1] Modulation of negative LR by OCI.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - stickiness: [0,5] Choice perseverance.
    """
    lr_pos, lr_neg_base, lr_neg_oci_slope, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate modulated negative learning rate
    raw_lr_neg = lr_neg_base + lr_neg_oci_slope * oci_score
    lr_neg = np.clip(raw_lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
            prev_a1 = a1
        else:
            p_choice_1[trial] = 0.5
            prev_a1 = -1
            
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        if state_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Updates ---
                # Stage 1
                delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
                if delta_stage1 >= 0:
                    q_stage1_mf[a1] += lr_pos * delta_stage1
                else:
                    q_stage1_mf[a1] += lr_neg * delta_stage1
                
                # Stage 2
                delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
                if delta_stage2 >= 0:
                    q_stage2_mf[state_idx, a2] += lr_pos * delta_stage2
                else:
                    q_stage2_mf[state_idx, a2] += lr_neg * delta_stage2
                
            else:
                p_choice_2[trial] = 0.5
        else:
            p_choice_2[trial] = 0.5

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```