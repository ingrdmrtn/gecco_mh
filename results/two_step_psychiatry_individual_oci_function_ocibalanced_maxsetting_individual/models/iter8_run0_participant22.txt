Here are three cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: Stickiness with OCI-Mediated Decay
This model hypothesizes that obsessive-compulsive symptoms relate to an inability to "let go" of a choice history. While standard models might include a perseveration bonus (stickiness) that decays rapidly or is constant, this model proposes that the **rate of decay** for the stickiness trace is inversely related to the OCI score. High OCI leads to higher retention of the perseveration trace, causing urges to repeat choices to linger longer, independent of reward.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Perseveration Decay.
    
    Hypothesis: OCI affects the persistence of 'stickiness' to a choice.
    High OCI leads to higher retention (slower decay) of the perseveration trace,
    causing the urge to repeat a choice to linger longer.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    pers_mag: [0, 5] (Magnitude of the stickiness bonus)
    retention_oci_sens: [0, 1] (Sensitivity of trace retention to OCI)
    """
    learning_rate, beta, w, pers_mag, retention_oci_sens = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate the retention rate for the perseveration trace.
    # Base retention is low (0.0), but increases with OCI.
    # If OCI is high, retention approaches 0.9 (slow decay).
    # If OCI is low, retention is near 0 (instant decay).
    retention = retention_oci_sens * current_oci
    retention = np.clip(retention, 0.0, 0.95)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Perseveration trace vector
    pers_trace = np.zeros(2)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        
        # Skip missing data
        if a1 < 0 or s < 0 or a2 < 0:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Perseveration Trace
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + pers_trace
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update Perseveration Trace
        # Decay existing trace
        pers_trace *= retention
        # Add to the chosen action
        pers_trace[a1] += pers_mag

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        
        # TD Update Stage 2
        delta_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_2
        
        # TD Update Stage 1
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1
            
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Stage-Specific Rigidity
This model hypothesizes that high OCI participants exhibit "ritualistic" or rigid behavior specifically in the first stage of the task (choosing the spaceship), while their response to immediate rewards (choosing the alien) remains standard. This is modeled by having the OCI score boost the inverse temperature (`beta`) specifically for Stage 1 choices, making them more deterministic/compulsive.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model where OCI selectively increases Beta (inverse temp) 
    for the first stage choice only.
    
    Hypothesis: High OCI participants are more rigid/deterministic in their 
    higher-level planning (Stage 1) but retain standard exploration in 
    local harvesting (Stage 2).
    
    Bounds:
    learning_rate: [0, 1]
    beta_base: [0, 10]
    beta_oci_boost: [0, 10] (Additional rigidity per unit of OCI for Stage 1)
    w: [0, 1]
    """
    learning_rate, beta_base, beta_oci_boost, w = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Beta is boosted by OCI only for Stage 1
    beta_stage1 = beta_base + beta_oci_boost * current_oci
    beta_stage2 = beta_base

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])

        if a1 < 0 or s < 0 or a2 < 0:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Policy (Uses beta_stage1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy (Uses beta_stage2)
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        
        delta_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_2
        
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Subjective Transition Distortion
This model hypothesizes that OCI relates to an intolerance of uncertainty or a "need for control," leading participants to perceive the environment as more deterministic than it actually is. The model scales the perceived probability of the common transition based on the OCI score. A high OCI participant assumes the "common" transition is nearly guaranteed, distorting their Model-Based value calculations.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-distorted Transition Probabilities.
    
    Hypothesis: OCI affects the Model-Based system's belief about transition reliability.
    High OCI leads to an overestimation of the common transition probability 
    (determinism bias), distorting Model-Based values.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    trans_distortion_mag: [0, 1] (Magnitude of OCI-driven probability distortion)
    """
    learning_rate, beta, w, trans_distortion_mag = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate subjective transition probability
    # Base is 0.7. Distortion increases this towards 1.0 based on OCI.
    # Max possible p is approx 0.99.
    # Logic: perceived_p = 0.7 + (distortion_magnitude * scaling * oci)
    
    perceived_p = 0.7 + trans_distortion_mag * 0.29 * current_oci
    perceived_p = np.clip(perceived_p, 0.01, 0.99)
    
    # Subjective transition matrix used for MB steps
    subjective_trans_matrix = np.array([[perceived_p, 1-perceived_p], 
                                        [1-perceived_p, perceived_p]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])

        if a1 < 0 or s < 0 or a2 < 0:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # Stage 1 Policy (Uses Subjective Matrix)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_trans_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        r = reward[trial]
        
        delta_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_2
        
        delta_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```