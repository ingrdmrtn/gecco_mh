Here are three new cognitive models that incorporate OCI scores into the two-step decision-making process.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Asymmetric Learning Rates (Valence Bias).
    
    Hypothesis: High OCI scores correlate with a bias in how positive versus 
    negative prediction errors are learned. Specifically, this model tests if OCI 
    modulates the learning rate for negative outcomes (disappointment/avoidance) 
    relative to a base learning rate for positive outcomes.
    
    Parameters:
    lr_base: [0, 1] Base learning rate (applied to positive PEs).
    beta: [0, 10] Inverse temperature (softmax randomness).
    w: [0, 1] Model-based weight.
    stickiness: [0, 5] Perseverance bonus for repeated Stage 1 choices.
    neg_lr_slope: [0, 5] OCI scaling factor for negative prediction error learning rate.
                  lr_neg = lr_base * (1 + neg_lr_slope * oci).
    """
    lr_base, beta, w, stickiness, neg_lr_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate negative learning rate based on OCI
    lr_neg = lr_base * (1.0 + neg_lr_slope * oci_score)
    # Clamp to reasonable bounds [0, 1]
    lr_neg = min(max(lr_neg, 0.0), 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        # Handle missing/invalid data
        if np.isnan(a1) or a1 < 0:
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value (Hybrid)
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness

        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # --- Stage 2 Policy ---
        # Simple Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # --- Updates ---
        # Stage 2 Update (Prediction Error 2)
        pe2 = r - q_stage2_mf[s, a2]
        # Apply asymmetric learning rate
        eff_lr2 = lr_base if pe2 >= 0 else lr_neg
        q_stage2_mf[s, a2] += eff_lr2 * pe2
        
        # Stage 1 Update (Prediction Error 1)
        # Using the value of the chosen state-action pair in stage 2 as the target
        pe1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # Apply asymmetric learning rate
        eff_lr1 = lr_base if pe1 >= 0 else lr_neg
        q_stage1_mf[a1] += eff_lr1 * pe1
        
        last_action_1 = a1

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Subjective Transition Beliefs.
    
    Hypothesis: High OCI scores are associated with reduced confidence in 
    probabilistic structures (intolerance of uncertainty). This model hypothesizes 
    that higher OCI leads to a "flattening" of the Model-Based transition matrix, 
    effectively reducing the distinctiveness of the two spaceships' destinations.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based weight.
    stickiness: [0, 5] Choice persistence.
    trans_distortion: [0, 1] OCI-dependent distortion of the transition matrix.
                      p_common = 0.7 - (trans_distortion * oci).
    """
    lr, beta, w, stickiness, trans_distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective common transition probability
    # Base is 0.7. Distortion reduces this towards 0.5 (randomness).
    # We clip it so it doesn't flip (doesn't go below 0.5).
    p_common = 0.7 - (trans_distortion * oci_score)
    p_common = max(p_common, 0.5)
    p_rare = 1.0 - p_common
    
    # Subjective Transition Matrix
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if np.isnan(a1) or a1 < 0:
            last_action_1 = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use the subjective matrix for MB calculation
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # Updates (Standard)
        q_stage2_mf[s, a2] += lr * (r - q_stage2_mf[s, a2])
        q_stage1_mf[a1] += lr * (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        
        last_action_1 = a1

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Post-Rare Transition Beta Shock.
    
    Hypothesis: Rare transitions act as "cognitive shocks." High OCI participants, 
    who may be more sensitive to prediction error or volatility, experience a 
    greater disruption in decision consistency (beta) immediately following a 
    rare transition.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature.
    w: [0, 1] Model-based weight.
    stickiness: [0, 5] Choice persistence.
    rare_shock_slope: [0, 5] OCI scaling factor for beta reduction after rare transition.
                      beta_trial = beta_base * (1 - rare_shock_slope * oci) if prev was rare.
    """
    lr, beta_base, w, stickiness, rare_shock_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.ones(2) * 0.5
    q_stage2_mf = np.ones((2, 2)) * 0.5
    
    last_action_1 = -1
    last_state = -1
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if np.isnan(a1) or a1 < 0:
            last_action_1 = -1
            last_state = -1
            continue
            
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Determine effective beta
        beta_eff = beta_base
        if last_action_1 != -1 and last_state != -1:
            # Check if previous transition was rare
            # Common: (0->0) or (1->1). Rare: (0->1) or (1->0).
            # If a1 == s, it's common. If a1 != s, it's rare.
            is_rare = (last_action_1 != last_state)
            
            if is_rare:
                # Apply shock: reduce beta (increase randomness)
                modifier = 1.0 - (rare_shock_slope * oci_score)
                modifier = max(modifier, 0.0) # Prevent negative beta
                beta_eff = beta_base * modifier

        # Stage 1 Choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness

        exp_q1 = np.exp(beta_eff * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # Stage 2 Choice (Using effective beta to represent state of shock/anxiety)
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + 1e-10)

        # Updates
        q_stage2_mf[s, a2] += lr * (r - q_stage2_mf[s, a2])
        q_stage1_mf[a1] += lr * (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        
        last_action_1 = a1
        last_state = s

    return log_loss
```