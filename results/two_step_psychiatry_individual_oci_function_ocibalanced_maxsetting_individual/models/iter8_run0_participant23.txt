Here are three cognitive models that incorporate the OCI-R score to explain individual variability in the two-step task.

### Model 1: OCI-Modulated Stress-Induced Control Shift
This model hypothesizes that OCI symptoms regulate the stability of goal-directed (Model-Based) control in the face of negative feedback. Specifically, it proposes that "losses" (0 reward) act as stressors that shift control towards the Model-Free (Habitual) system, and the magnitude of this shift is modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stress-Induced Control Shift.
    
    Hypothesis: OCI scores correlate with a stronger shift from Model-Based (Goal-Directed) 
    to Model-Free (Habitual) control following a negative outcome (stressor).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - stickiness: [0, 5] Perseveration bonus for the previous action.
    - w_base: [0, 1] Baseline Model-Based weight.
    - w_loss_delta_base: [-1, 1] Baseline change in w after a loss (reward=0).
    - w_loss_delta_oci: [-1, 1] Modulation of the w-change by OCI score.
    """
    learning_rate, beta, stickiness, w_base, w_loss_delta_base, w_loss_delta_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Alien
    
    prev_action_1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Dynamic w calculation
        w_current = w_base
        if prev_reward == 0:
            # Apply shift if previous trial was a loss
            shift = w_loss_delta_base + w_loss_delta_oci * oci_score
            w_current += shift
        
        # Clip w to [0, 1]
        w_current = np.clip(w_current, 0.0, 1.0)

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_current * q_stage1_mb + (1 - w_current) * q_stage1_mf
        
        # Stickiness
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_action_1 = a1
        prev_reward = r

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Dual Learning Rates
This model proposes that OCI symptoms differentially affect the learning of "Outcomes" (Stage 2 reward probabilities) versus "Actions" (Stage 1 values). It introduces separate learning rates for the two stages, with the Stage 2 learning rate modulated by the OCI score. This could capture deficits in updating internal models of the environment (outcomes) while habit formation (action learning) remains intact.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Dual Learning Rates.
    
    Hypothesis: OCI affects the learning rate for outcomes (Stage 2) differently 
    than the learning rate for actions (Stage 1). High OCI may correlate with 
    rigid or hyper-flexible updating of goal values.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for Stage 1 (Actions).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Perseveration bonus.
    - lr_stage2_base: [0, 1] Baseline learning rate for Stage 2 (Outcomes).
    - lr_stage2_oci: [-1, 1] Modulation of Stage 2 learning rate by OCI.
    """
    lr_stage1, beta, w, stickiness, lr_stage2_base, lr_stage2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate modulated learning rate
    lr_stage2 = lr_stage2_base + lr_stage2_oci * oci_score
    lr_stage2 = np.clip(lr_stage2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        # Stage 1 uses fixed lr_stage1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Stage 2 uses modulated lr_stage2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2
        
        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Outcome Value Decay
This model hypothesizes that OCI symptoms relate to the handling of uncertainty regarding unobserved options. It introduces a decay parameter for unvisited Stage 2 states (aliens not chosen). High OCI might lead to faster decay (forgetting/pessimism about unverified outcomes) or slower decay (rigidity).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Outcome Value Decay.
    
    Hypothesis: OCI modulates the rate at which unvisited outcome values (Stage 2) 
    decay. High OCI might be associated with rapid 'forgetting' or devaluation 
    of unverified options (intolerance of uncertainty), or conversely, rigid maintenance.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stickiness: [0, 5] Perseveration bonus.
    - decay_base: [0, 1] Baseline decay rate for unchosen Stage 2 values.
    - decay_oci: [-1, 1] Modulation of decay rate by OCI.
    """
    learning_rate, beta, w, stickiness, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate decay rate
    decay = decay_base + decay_oci * oci_score
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_action_1 != -1:
            stick_bonus[prev_action_1] = stickiness
            
        logits_1 = beta * q_net + stick_bonus
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Chosen Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Decay Unchosen Stage 2 values
        # We decay all values that were NOT (s_idx, a2)
        for s in range(2):
            for a in range(2):
                if not (s == s_idx and a == a2):
                    q_stage2_mf[s, a] *= (1.0 - decay)
        
        prev_action_1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```