Here are three new cognitive models that incorporate the OCI score to explain individual variability in the two-step task.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Outcome-Dependent Arbitration Model.
    
    Hypothesis: The balance between Model-Based (planning) and Model-Free (habit) control 
    changes depending on the previous trial's outcome. High OCI participants might 
    shift their strategy differently after a loss (e.g., retreating to habits due to stress 
    or attempting to over-control via planning).
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Softmax inverse temperature.
    - w_win: [0, 1] Weight of Model-Based system after a rewarded trial (Win).
    - w_loss_base: [0, 1] Baseline weight of Model-Based system after an unrewarded trial (Loss).
    - w_loss_oci_slope: [-5, 5] Effect of OCI on the MB weight after a loss.
    """
    learning_rate, beta, w_win, w_loss_base, w_loss_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate w_loss for this participant
    w_loss = w_loss_base + (w_loss_oci_slope * participant_oci)
    w_loss = np.clip(w_loss, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_reward = 1 # Assume start with "win" state (or neutral)
    
    for trial in range(n_trials):
        # Skip missing data
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Determine w based on previous outcome
        if prev_reward == 1:
            w = w_win
        else:
            w = w_loss
            
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_reward = r

    eps = 1e-10
    # Sum logs only for valid trials (where p > 0)
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Choice Kernel Learning Rate Modulation Model.
    
    Hypothesis: OCI affects the speed at which habits (choice repetitions) are formed and forgotten.
    High OCI might lead to faster habit formation (high learning rate for kernel) or stickier habits 
    (equivalent to faster updating towards '1' for chosen options).
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Softmax inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - ck_weight: [0, 5] Weight of the Choice Kernel (habit strength) in the decision.
    - ck_lr_base: [0, 1] Baseline learning rate for the Choice Kernel.
    - ck_lr_oci_slope: [-1, 1] Effect of OCI on the Choice Kernel learning rate.
    """
    learning_rate, beta, w, ck_weight, ck_lr_base, ck_lr_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate Choice Kernel Learning Rate
    ck_lr = ck_lr_base + (ck_lr_oci_slope * participant_oci)
    ck_lr = np.clip(ck_lr, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, MF, and Choice Kernel
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf) + (ck_weight * choice_kernel)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Choice Kernel
        # Chosen action approaches 1, unchosen approaches 0
        choice_kernel[a1] += ck_lr * (1 - choice_kernel[a1])
        choice_kernel[1-a1] += ck_lr * (0 - choice_kernel[1-a1])

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Uncertainty Exploration Modulation Model.
    
    Hypothesis: OCI modulates the tendency to explore uncertain options. 
    An exploration bonus is added to Stage 1 options based on the inverse square root 
    of visit counts. High OCI might suppress this bonus (anxiety/avoidance of unknown) 
    or enhance it (compulsive checking).
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Softmax inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - phi_base: [0, 5] Baseline exploration bonus parameter.
    - phi_oci_slope: [-5, 5] Effect of OCI on the exploration bonus.
    """
    learning_rate, beta, w, phi_base, phi_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate exploration bonus weight
    phi = phi_base + (phi_oci_slope * participant_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Counts for Stage 1 actions (initialize with 0)
    counts_stage1 = np.zeros(2) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Uncertainty Bonus
        # Using 1/sqrt(N+1) as a proxy for uncertainty
        uncertainty = 1.0 / np.sqrt(counts_stage1 + 1)
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf) + (phi * uncertainty)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update counts
        counts_stage1[a1] += 1

    eps = 1e-10
    valid_mask = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```