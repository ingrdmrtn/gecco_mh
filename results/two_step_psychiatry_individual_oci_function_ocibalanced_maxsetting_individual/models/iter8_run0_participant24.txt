Here are three new cognitive models that incorporate the OCI score into the decision-making process, focusing on mechanisms like habit formation, punishment sensitivity, and uncertainty avoidance, which are often relevant to obsessive-compulsive traits.

### Cognitive Model 1: Hybrid RL with OCI-modulated Learning Rate for Punishment
This model posits that individuals with higher OCI scores might react more strongly to negative outcomes (punishment or lack of reward) compared to positive outcomes. This "negative bias" in learning is modulated by their OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-modulated Learning Rate for Punishment.
    
    This model assumes that the learning rate for negative prediction errors (worse than expected)
    is modulated by the OCI score, reflecting a potential hypersensitivity to failure or 
    lack of reward common in OC traits.
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting parameter for model-based vs model-free control (0=MF, 1=MB).
    oci_neg_scale: [0, 5] - Scaling factor for how OCI increases the negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective negative learning rate based on OCI
    # We clamp it to [0, 1] to ensure stability
    lr_neg = min(1.0, lr_neg_base + (oci_neg_scale * oci_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5 # Initialize with neutral values
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Skip invalid trials
        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha_2 * delta_stage2
        
        # Stage 1 Update (TD-learning)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Model-Based/Model-Free Hybrid with OCI-driven Uncertainty Avoidance
This model explores the idea that high OCI might correlate with "uncertainty avoidance." Instead of just maximizing reward, the agent adds a penalty to options where the outcome variance (uncertainty) is high, and this penalty is scaled by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-driven Uncertainty Avoidance.
    
    The agent tracks the variance/uncertainty of rewards for second-stage options.
    High OCI leads to a stronger penalty for options with high uncertainty.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight (0=MF, 1=MB).
    uncertainty_lr: [0, 1] - Learning rate for tracking reward variance.
    oci_uncert_penalty: [0, 5] - Penalty weight for uncertainty, scaled by OCI.
    """
    learning_rate, beta, w, uncertainty_lr, oci_uncert_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    penalty_weight = oci_uncert_penalty * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track expected reward squared to calculate variance: Var = E[X^2] - (E[X])^2
    # We approximate uncertainty tracking simply as a running average of squared prediction errors
    # or just tracking the variance directly. Let's track a simple uncertainty metric.
    uncertainty_stage2 = np.zeros((2, 2)) # Initialize with 0 uncertainty

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        # Incorporate uncertainty into Stage 2 values before MB projection
        # Effective Q2 = Q2 - (penalty * uncertainty)
        eff_q_stage2 = q_stage2_mf - (penalty_weight * uncertainty_stage2)
        
        max_eff_q2 = np.max(eff_q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_eff_q2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        # Use effective Q-values (penalized by uncertainty) for selection
        eff_q2_curr = eff_q_stage2[s_idx]
        exp_q2 = np.exp(beta * eff_q2_curr)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Update Q-values
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Uncertainty (approximate as squared prediction error)
        # Higher prediction error -> higher uncertainty
        # Smooth update: U_new = U_old + lr_u * (delta^2 - U_old)
        u_error = (delta_stage2**2) - uncertainty_stage2[s_idx, a2]
        uncertainty_stage2[s_idx, a2] += uncertainty_lr * u_error

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-modulated Transition Learning (State Inference Rigidity)
This model hypothesizes that high OCI individuals might have a more rigid belief about the world structure (transitions) or, conversely, might update their belief about transition probabilities differently. Here we assume the agent learns the transition matrix online, but the rate at which they update this matrix (flexibility) is inversely related to OCI (higher OCI -> more rigid/slower updating).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based RL with OCI-modulated Transition Learning Rate.
    
    The agent learns the transition probabilities between Stage 1 and Stage 2 dynamically.
    The learning rate for these transitions is modulated by OCI. 
    High OCI might lead to rigidity (lower learning rate) or hyper-updating.
    Here we model it as a base rate modified by OCI.
    
    Parameters:
    learning_rate_reward: [0, 1] - Learning rate for reward values (Q-values).
    beta: [0, 10] - Inverse temperature.
    lr_trans_base: [0, 1] - Base learning rate for transition matrix updating.
    oci_trans_mod: [0, 5] - Modifier for transition learning rate based on OCI.
                            (Can decrease it to model rigidity).
    w: [0, 1] - MB/MF weight.
    """
    learning_rate_reward, beta, lr_trans_base, oci_trans_mod, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective transition learning rate. 
    # We model rigidity: Higher OCI reduces the learning rate from the base.
    # We use a decay formulation: lr = base / (1 + oci_mod * oci)
    lr_trans = lr_trans_base / (1.0 + (oci_trans_mod * oci_score))

    # Initialize transition matrix beliefs (start with uniform/flat prior or true structure)
    # Let's start with a weak prior favoring the common transitions slightly but allowing learning
    trans_probs = np.array([[0.6, 0.4], [0.4, 0.6]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 == -1 or s_idx == -1 or a2 == -1:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Choice ---
        # MB Calculation using dynamic transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # 1. Update Transition Matrix Beliefs
        # If we chose a1=0 and went to s_idx=0, we reinforce trans_probs[0,0]
        # State estimation error: 1 if observed, 0 otherwise
        target_state = np.zeros(2)
        target_state[s_idx] = 1.0
        
        # Update row corresponding to action taken
        trans_probs[a1] += lr_trans * (target_state - trans_probs[a1])
        # Normalize to ensure valid probabilities
        trans_probs[a1] /= np.sum(trans_probs[a1])

        # 2. Update Reward Values
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate_reward * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate_reward * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```