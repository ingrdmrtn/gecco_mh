Here are the 3 proposed cognitive models.

### Model 1: Pure Model-Based with OCI-Modulated Stickiness
This model hypothesizes that the participant relies entirely on a Model-Based (planning) strategy, using the known transition structure to calculate values. However, their choices are biased by a "stickiness" (perseveration) factor that is modulated by their OCI score. This contrasts with the previous best model (Pure Model-Free) by assuming the underlying learning mechanism is goal-directed, but the *expression* of that learning is hindered by compulsive repetition.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner with choice perseveration (stickiness) modulated by OCI.
    Assumes w=1 (fully Model-Based) and no Model-Free caching for Stage 1.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Stage 2 values.
    - beta: [0, 10] Inverse temperature.
    - stick_base: [-5, 5] Base level of choice perseveration.
    - oci_stick_mod: [-5, 5] Modulation of stickiness by OCI score.
    """
    learning_rate, beta, stick_base, oci_stick_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective stickiness
    stickiness = stick_base + (oci_score * oci_stick_mod)

    # Fixed transition matrix as defined in the task
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    # Stage 1 is calculated on the fly (Model-Based)
    q_stage2 = np.zeros((2, 2)) # State x Action

    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Value of Stage 1 actions is the expected value of the best Stage 2 option
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Add stickiness to the previously chosen action
        q_stage1_effective = q_stage1_mb.copy()
        if prev_a1 != -1:
            q_stage1_effective[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_a1 = a1

        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Only update Stage 2 values (Stage 1 is derived)
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MF/MB with OCI-Modulated Transition Belief
This model assumes the participant uses a hybrid strategy (mixing Model-Free and Model-Based), but their internal model of the task structure (the transition probabilities) is distorted by their OCI symptoms. High OCI might lead to "doubting" the common transition (0.7), effectively flattening the transition matrix towards randomness (0.5), which degrades the quality of Model-Based planning.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MF/MB learner where OCI modulates the belief in the transition probability.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for MF updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0=Pure MF, 1=Pure MB).
    - trans_prob_base: [0, 1] Base belief in the 'common' transition probability.
    - oci_trans_mod: [-1, 1] Modulation of transition belief by OCI.
    """
    learning_rate, beta, w, trans_prob_base, oci_trans_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective transition probability
    # We clip to ensure it remains a valid probability [0, 1]
    subjective_p = trans_prob_base + (oci_score * oci_trans_mod)
    subjective_p = np.clip(subjective_p, 0.0, 1.0)
    
    # Construct subjective transition matrix
    # Row 0: spaceship A -> [p, 1-p] to planets X, Y
    # Row 1: spaceship U -> [1-p, p] to planets X, Y
    transition_matrix = np.array([[subjective_p, 1.0 - subjective_p], 
                                  [1.0 - subjective_p, subjective_p]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Hybrid) ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]

        # Stage 2 Update (MF)
        delta_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_2
        
        # Stage 1 Update (MF - TD(0))
        # Update stage 1 MF value using the value of the state actually reached
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid MF/MB with OCI-Modulated Negative Learning Rate
This model posits that while the participant uses a hybrid strategy, their learning from negative outcomes (disappointments) is distinct from positive outcomes, and this specific sensitivity is driven by their OCI score. This captures the potential for OCD-related traits to manifest as hyper-sensitivity to failure or errors (or conversely, rigidity/insensitivity to errors).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MF/MB learner with asymmetric learning rates, where the learning rate
    for negative prediction errors is modulated by OCI.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors.
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - oci_neg_mod: [-1, 1] Modulation of negative learning rate by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0=Pure MF, 1=Pure MB).
    """
    alpha_pos, alpha_neg_base, oci_neg_mod, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    alpha_neg = alpha_neg_base + (oci_score * oci_neg_mod)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Hybrid) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]

        # Stage 2 Update
        delta_2 = r - q_stage2_mf[s_idx, a2]
        lr_2 = alpha_pos if delta_2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr_2 * delta_2
        
        # Stage 1 Update
        delta_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr_1 = alpha_pos if delta_1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr_1 * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```