Here are the three proposed cognitive models.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Pessimism in Model-Based Valuation.
    
    Hypothesis: OCI score modulates the degree of pessimism when evaluating 
    Stage 2 states for the Model-Based system. High OCI individuals might 
    weight the worst-case scenario (minimum Q-value of a state) more heavily 
    than the best-case (maximum Q-value), reflecting anxiety-driven risk aversion.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weight for Model-Based control (1=MB, 0=MF).
    - omega_base: [0, 1] Baseline pessimism weight (0 = pure max/optimism, 1 = pure min/pessimism).
    - omega_oci: [-1, 1] Modulation of pessimism by OCI.
      Effective omega = omega_base + omega_oci * oci.
    """
    learning_rate, beta, w, omega_base, omega_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate pessimism weight and clip to [0, 1]
    omega = omega_base + (omega_oci * current_oci)
    omega = np.clip(omega, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Estimation with Pessimism
        # Standard MB uses max(Q), here we use a weighted average of max(Q) and min(Q)
        max_q2 = np.max(q_stage2_mf, axis=1)
        min_q2 = np.min(q_stage2_mf, axis=1)
        
        # Weighted valuation of states
        val_stage2 = (1 - omega) * max_q2 + omega * min_q2
        
        q_stage1_mb = transition_matrix @ val_stage2
        
        # Integrated Q-values
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax for Choice 1
        logits_1 = beta * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Delta 1: Prediction error from S1 to S2
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Delta 2: Prediction error from S2 to Reward
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update for Stage 1 (propagate Delta 2 back)
        q_stage1_mf[a1] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Choice Trace Decay (Habit Formation Speed).
    
    Hypothesis: OCI scores relate to the speed at which motor habits (choice perseveration)
    are formed and decayed. While 'phi' controls the strength of the habit influence,
    'alpha_k' controls the learning rate (memory horizon) of past choices. High OCI 
    might lead to stickier habits (slow decay/fast accumulation).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - phi: [0, 5] Perseveration weight (bonus magnitude).
    - alpha_k_base: [0, 1] Baseline decay/update rate for choice trace.
    - alpha_k_oci: [-1, 1] Modulation of trace update rate by OCI.
    """
    learning_rate, beta, w, phi, alpha_k_base, alpha_k_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate kernel learning rate and clip
    alpha_k = alpha_k_base + (alpha_k_oci * current_oci)
    alpha_k = np.clip(alpha_k, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_trace = np.zeros(2) # Tracks frequency of choosing A vs U
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus (Choice Kernel)
        logits_1 = beta * q_net_1 + phi * choice_trace
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Q-learning updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        # Choice Trace Update
        # Decays unchosen, increases chosen towards 1
        choice_trace[a1] += alpha_k * (1.0 - choice_trace[a1])
        choice_trace[1-a1] += alpha_k * (0.0 - choice_trace[1-a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Counterfactual Updating.
    
    Hypothesis: OCI affects the tendency to engage in counterfactual updating 
    (updating the unchosen option based on the assumption of anticorrelated rewards).
    High OCI might drive compulsive switching or sticking based on imagined outcomes 
    of the forgone path (e.g., "The other one must have been a winner").
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - lr_cf_base: [0, 1] Baseline counterfactual learning rate.
    - lr_cf_oci: [-1, 1] Modulation of counterfactual rate by OCI.
    """
    learning_rate, beta, w, lr_cf_base, lr_cf_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate counterfactual learning rate
    lr_cf = lr_cf_base + (lr_cf_oci * current_oci)
    lr_cf = np.clip(lr_cf, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_1
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Standard Q-learning for chosen path
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2
        
        # Counterfactual Update for Unchosen Stage 1 Action
        # Assumption: Rewards are anticorrelated (if obtained R, unchosen would have yielded 1-R)
        r_cf = 1.0 - r
        # We update the MF value of the unchosen spaceship towards this imagined outcome
        q_stage1_mf[1-a1] += lr_cf * (r_cf - q_stage1_mf[1-a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```