Here are the three cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: OCI-Modulated Win-Stay Lose-Shift (WSLS)
**Hypothesis:** Participants with higher OCI scores may rely more heavily on a heuristic "Win-Stay, Lose-Shift" strategy, independent of the reinforcement learning value updates. This model adds a heuristic bonus to the action values based on the previous outcome, scaled by the OCI score.
**Parameters:**
*   `learning_rate`: Rate of updating Q-values [0, 1].
*   `beta`: Inverse temperature for the softmax function [0, 10].
*   `w`: Weighting parameter for Model-Based vs Model-Free control [0, 1].
*   `wsls_scale`: Scaling factor for the OCI-dependent WSLS bonus [0, 5].

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with an OCI-modulated Win-Stay Lose-Shift (WSLS) heuristic bonus.
    
    The OCI score scales a bonus added to the Q-values:
    - If previous trial was a Win, the previous action gets a bonus (Stay).
    - If previous trial was a Loss, the *other* action gets a bonus (Shift).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - wsls_scale: [0, 5]
    """
    learning_rate, beta, w, wsls_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate the bonus magnitude based on OCI
    wsls_bonus = current_oci * wsls_scale
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply WSLS Bonus
        if prev_a1 != -1 and prev_reward != -1:
            if prev_reward == 1:
                # Win-Stay: Bonus to previous action
                q_hybrid[prev_a1] += wsls_bonus
            else:
                # Lose-Shift: Bonus to the other action (assuming binary choice 0/1)
                other_action = 1 - prev_a1
                q_hybrid[other_action] += wsls_bonus

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: 
            prev_a1 = -1
            prev_reward = -1
            continue
            
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        if s_idx == -1: continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        # Update Stage 2 MF
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        # Update Stage 1 MF (TD(0))
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1
        
        # Store for next trial
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Value Decay
**Hypothesis:** High OCI scores may be associated with an inability to maintain stable value representations for unchosen options, leading to faster "forgetting" or decay of values. This model introduces a decay rate for unchosen actions that is proportional to the OCI score.
**Parameters:**
*   `learning_rate`: Rate of updating chosen Q-values [0, 1].
*   `beta`: Inverse temperature [0, 10].
*   `w`: Mixing weight [0, 1].
*   `decay_scale`: Scaling factor for the OCI-dependent decay rate [0, 1].

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model where unchosen Q-values decay at a rate determined by OCI.
    
    Hypothesis: Higher OCI leads to faster forgetting of option values that are not 
    currently being sampled.
    
    Decay rate = oci * decay_scale.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - decay_scale: [0, 1]
    """
    learning_rate, beta, w, decay_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate decay rate based on OCI
    decay_rate = current_oci * decay_scale
    # Clip to ensure stability
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: continue
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        if s_idx == -1: continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        
        # Stage 2: Update chosen, decay unchosen
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay_rate)
        
        # Stage 1: Update chosen, decay unchosen
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1
        
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Stage 2 Precision
**Hypothesis:** OCI may differentially affect the decision noise in the planning phase (Stage 1) versus the exploitation phase (Stage 2). This model proposes that high OCI leads to more rigid, exploitative behavior specifically at the second stage (choosing the alien), while the first stage beta remains baseline.
**Parameters:**
*   `learning_rate`: [0, 1]
*   `beta_base`: Baseline inverse temperature for Stage 1 [0, 10].
*   `w`: Mixing weight [0, 1].
*   `beta2_scale`: OCI scaling factor for Stage 2 beta [0, 10]. (Stage 2 Beta = `beta_base * (1 + oci * beta2_scale)`).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with separate Beta (inverse temperature) parameters for Stage 1 and Stage 2.
    The Stage 2 Beta is modulated by OCI.
    
    Hypothesis: OCI affects local exploitation (Stage 2) intensity more than global planning.
    High OCI -> Higher Beta2 (more deterministic choice of aliens).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_base: [0, 10] (Used for Stage 1)
    - w: [0, 1]
    - beta2_scale: [0, 10] (Scales beta_base for Stage 2 based on OCI)
    """
    learning_rate, beta_base, w, beta2_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Stage 2 Beta
    beta_stage2 = beta_base * (1.0 + current_oci * beta2_scale)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy (Uses beta_base) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_base * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: continue
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        if s_idx == -1: continue

        # --- Stage 2 Policy (Uses beta_stage2) ---
        # More rigid choice here for high OCI
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: continue
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1

    eps = 1e-10
    valid_trials = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```