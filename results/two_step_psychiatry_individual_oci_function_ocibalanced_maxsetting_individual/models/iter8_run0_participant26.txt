Here are three new cognitive models for the two-step task, incorporating the OCI score to explain individual variability in decision-making.

### Model 1: OCI-Modulated Counterfactual Updating
**Hypothesis:** Participants with higher OCI scores may exhibit "counterfactual thinking" or "regret," obsessively simulating what would have happened if they had chosen differently. This model posits that OCI modulates the learning rate for the *unchosen* alien at Stage 2. If the chosen alien yields a reward (1), the participant infers the unchosen one would have yielded 0 (and vice versa), updating its value accordingly. The strength of this update is scaled by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Counterfactual Updating (Stage 2).
    
    Hypothesis:
    High OCI participants engage in counterfactual updating for the unchosen option 
    at Stage 2. If they receive a reward from one alien, they infer the other 
    would not have yielded a reward (and vice versa), updating the unchosen option's 
    value. The strength of this counterfactual update is proportional to OCI.
    
    Parameters:
    - lr: [0, 1] Base learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF (0=MF, 1=MB).
    - cf_k: [0, 5] Scaling factor for counterfactual learning rate based on OCI.
      lr_cf = lr * cf_k * OCI.
    """
    lr, beta, w, cf_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Counterfactual learning rate scaled by OCI
    # We clip to ensure stability, though cf_k * OCI (0.55) * lr should be reasonable.
    lr_cf = lr * cf_k * oci_score
    lr_cf = np.clip(lr_cf, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 Update (Chosen)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Stage 2 Update (Unchosen - Counterfactual)
        # Assume anticorrelation: if R=1, unchosen would be 0. If R=0, unchosen would be 1.
        unchosen_a2 = 1 - a2
        counterfactual_outcome = 1.0 - r
        delta_cf = counterfactual_outcome - q_stage2_mf[s2, unchosen_a2]
        q_stage2_mf[s2, unchosen_a2] += lr_cf * delta_cf

    return -log_likelihood
```

### Model 2: OCI-Modulated Surprise Sensitivity
**Hypothesis:** This model posits that OCI relates to "intolerance of uncertainty" or hyper-sensitivity to prediction errors regarding the environment structure. Specifically, when a **Rare** transition occurs (a "surprise"), participants with high OCI boost their Model-Free learning rate for that trial, reacting more strongly to the unexpected state transition than low OCI participants.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Surprise Sensitivity (Rare Transition Boosting).
    
    Hypothesis:
    High OCI participants are hyper-sensitive to "surprising" (Rare) transitions.
    When a Rare transition occurs (Action 0 -> State 1 or Action 1 -> State 0),
    the learning rate for the Stage 1 Model-Free update is boosted by a factor
    proportional to their OCI score.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate for common transitions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - surprise_k: [0, 10] Scaling factor for learning rate boost on rare trials.
      lr_rare = lr_base * (1 + surprise_k * OCI).
    """
    lr_base, beta, w, surprise_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        
        # Determine if transition was Common or Rare
        # Common: 0->0 or 1->1 (based on 0.7 diag in transition matrix)
        is_common = (a1 == s2)
        
        if is_common:
            current_lr = lr_base
        else:
            # Boost learning rate for rare transitions based on OCI
            current_lr = lr_base * (1.0 + surprise_k * oci_score)
            current_lr = np.clip(current_lr, 0.0, 1.0)

        # Stage 1 MF Update uses the context-dependent learning rate
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Stage 2 Update (Standard)
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr_base * delta_stage2

    return -log_likelihood
```

### Model 3: OCI-Modulated Outcome-Dependent Stickiness
**Hypothesis:** Compulsivity is often characterized by rigid repetition of behavior (stickiness) or avoidance of failure. This model introduces a "Win-Stay, Lose-Shift" bias that is explicitly scaled by the OCI score. Unlike simple stickiness, this bias is positive if the previous trial was rewarded (reinforcing repetition) and negative if it was unrewarded (driving switching), with the magnitude determined by the participant's OCI severity.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Outcome-Dependent Stickiness (Win-Stay/Lose-Shift).
    
    Hypothesis:
    OCI scores drive a rigid response to outcomes. High OCI participants feel 
    a stronger compulsion to repeat a successful action (Win-Stay) and avoid 
    a failed action (Lose-Shift) at Stage 1. This is modeled as a bonus added 
    to the Q-value of the previously chosen action, where the sign depends on 
    the previous reward and the magnitude depends on OCI.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - stick_outcome_k: [0, 5] Scaling factor for outcome-dependent stickiness.
      Bonus = stick_outcome_k * OCI * (1 if PrevReward=1 else -1).
    """
    lr, beta, w, stick_outcome_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = 0
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add OCI-modulated Outcome Stickiness
        if prev_a1 != -1:
            # If prev reward was 1, outcome_sign is +1 (encourage stay)
            # If prev reward was 0, outcome_sign is -1 (encourage switch)
            outcome_sign = 1.0 if prev_reward == 1.0 else -1.0
            stickiness_bonus = stick_outcome_k * oci_score * outcome_sign
            
            # Add bonus to the Q-value of the previous action
            # We add it to q_net effectively for the choice probability
            # We copy q_net to avoid modifying the learned values permanently
            q_choice = q_net.copy()
            q_choice[prev_a1] += stickiness_bonus
        else:
            q_choice = q_net

        exp_q1 = np.exp(beta * q_choice)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Update history
        prev_a1 = a1
        prev_reward = r

    return -log_likelihood
```