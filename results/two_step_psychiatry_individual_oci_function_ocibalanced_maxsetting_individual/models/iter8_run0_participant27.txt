Here are three new cognitive models for the two-step task, incorporating OCI-R scores to explain individual variability.

### Model 1: Hybrid Model with OCI-Modulated Memory Decay
This model hypothesizes that OCI symptoms affect the rate at which participants "forget" or decay the values of unchosen options. High OCI might be associated with "stickier" memory (lower decay) or, conversely, rapid overwriting of value representations (high decay).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated memory decay for unchosen options.
    
    Hypothesis: OCI scores influence the passive forgetting rate of unchosen options.
    
    Parameters:
    - lr: Learning rate [0, 1]
    - beta: Inverse temperature (softmax sensitivity) [0, 10]
    - w: Mixing weight (0=MF, 1=MB) [0, 1]
    - decay_base: Base decay rate for unchosen actions [0, 1]
    - decay_oci_slope: Modulation of decay rate by OCI [-1, 1]
    """
    lr, beta, w, decay_base, decay_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-specific decay rate
    decay = decay_base + (decay_oci_slope * current_oci)
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize values (0.5 is neutral/initial expectation)
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5 # 2 planets (states), 2 aliens (actions)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value (Hybrid)
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net_s1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue
            
        s_idx = int(state[trial])
        if s_idx == -1: 
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 != -1:
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]
            
            # --- Updates ---
            
            # Stage 2 Update (MF)
            pe2 = r - q_stage2_mf[s_idx, a2]
            q_stage2_mf[s_idx, a2] += lr * pe2
            
            # Decay Stage 2 unchosen option
            unchosen_a2 = 1 - a2
            q_stage2_mf[s_idx, unchosen_a2] *= (1 - decay)
            
            # Stage 1 Update (MF) via TD(0)
            pe1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += lr * pe1
            
            # Decay Stage 1 unchosen option
            unchosen_a1 = 1 - a1
            q_stage1_mf[unchosen_a1] *= (1 - decay)
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-Modulated Rare Transition Sensitivity
This model tests if OCI symptoms alter how participants learn from "Rare" transitions (e.g., choosing Spaceship A but ending up at Planet Y). High OCI participants might view these as unlucky anomalies and under-weight them (dampening), or conversely, over-react to the uncertainty.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated sensitivity to Rare Transitions.
    
    Hypothesis: OCI affects the magnitude of the Model-Free update for Stage 1 
    specifically when a Rare transition occurs.
    
    Parameters:
    - lr: Base Learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight [0, 1]
    - rare_mod_base: Base multiplier for learning rate on rare transitions [0, 2]
    - rare_mod_oci: Effect of OCI on this multiplier [-1, 1]
    """
    lr, beta, w, rare_mod_base, rare_mod_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate multiplier for rare transitions
    rare_multiplier = rare_mod_base + (rare_mod_oci * current_oci)
    rare_multiplier = np.clip(rare_multiplier, 0.0, 5.0) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 == -1: 
            p_choice_1[trial] = 1.0; p_choice_2[trial] = 1.0; continue
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        if s_idx == -1: p_choice_2[trial] = 1.0; continue

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        if a2 == -1: p_choice_2[trial] = 1.0; continue
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Updates
        # Stage 2 Update (Standard)
        pe2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * pe2
        
        # Stage 1 Update (Modulated by Transition Type)
        # Determine if transition was common or rare
        # A(0) -> X(0) Common; A(0) -> Y(1) Rare
        # U(1) -> Y(1) Common; U(1) -> X(0) Rare
        is_rare = (a1 == 0 and s_idx == 1) or (a1 == 1 and s_idx == 0)
        
        effective_lr = lr * rare_multiplier if is_rare else lr
        effective_lr = np.clip(effective_lr, 0, 1)
        
        pe1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += effective_lr * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Modulated "Lapse" (Epsilon-Greedy Mixture)
This model posits that OCI introduces a "checking" behavior or random uncertainty, modeled as a mixture between a rational Softmax policy and a random uniform policy (epsilon-greedy). The weight of the random component (`epsilon`) is modulated by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated 'Lapse' (Epsilon-Greedy mixture).
    
    Hypothesis: OCI introduces 'checking' noise, modeled as a random lapse probability 
    mixed with the softmax policy.
    
    Parameters:
    - lr: Learning rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Mixing weight [0, 1]
    - eps_base: Base lapse rate (random exploration) [0, 1]
    - eps_oci_slope: Effect of OCI on lapse rate [-1, 1]
    """
    lr, beta, w, eps_base, eps_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate epsilon (lapse rate)
    epsilon = eps_base + (eps_oci_slope * current_oci)
    epsilon = np.clip(epsilon, 0.001, 0.99) # Keep within bounds
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # Stage 1 Policy (Mixture of Softmax and Random)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        softmax_probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Mixture: (1-eps) * Softmax + eps * (0.5 for each option)
        probs_1 = (1 - epsilon) * softmax_probs_1 + epsilon * 0.5
        
        a1 = int(action_1[trial])
        if a1 == -1: 
            p_choice_1[trial] = 1.0; p_choice_2[trial] = 1.0; continue
        p_choice_1[trial] = probs_1[a1]
        
        s_idx = int(state[trial])
        if s_idx == -1: p_choice_2[trial] = 1.0; continue

        # Stage 2 Policy (Mixture)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        softmax_probs_2 = exp_q2 / np.sum(exp_q2)
        
        probs_2 = (1 - epsilon) * softmax_probs_2 + epsilon * 0.5
        
        a2 = int(action_2[trial])
        if a2 == -1: p_choice_2[trial] = 1.0; continue
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Updates
        pe2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * pe2
        
        pe1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```