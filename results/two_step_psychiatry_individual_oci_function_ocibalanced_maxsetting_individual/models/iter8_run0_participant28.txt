Here are three new cognitive models for the two-step task, incorporating the OCI score to explain individual variability.

### Model 1: Planet Perseveration (Model-Based Stickiness)
This model hypothesizes that OCD symptoms (OCI) modulate a "compulsion" to return to the previously visited state (Planet), rather than just repeating the previous motor action. This is a form of model-based stickiness where the agent biases their choice towards the spaceship that is most likely to lead back to the last encountered planet.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Planet Perseveration Model.
    
    Incorporates a 'Planet Stickiness' bonus: a tendency to choose the action 
    that leads to the previously visited planet. This is distinct from simple 
    action repetition. OCI modulates the strength of this planet-based perseveration.
    
    Parameters:
    learning_rate: [0,1] - Value updating rate.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0).
    p_stick: [0,1] - Simple motor stickiness (scaled by 5).
    p_planet_base: [0,1] - Base stickiness to the previous planet (scaled by 5).
    p_planet_oci_mod: [0,1] - Modulation of planet stickiness by OCI.
    lambda_val: [0,1] - Eligibility trace for Stage 1 update.
    """
    learning_rate, beta, w, p_stick, p_planet_base, p_planet_oci_mod, lambda_val = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulation: Center the modifier around 0
    p_planet = p_planet_base + (p_planet_oci_mod - 0.5) * oci_score
    # Clip to ensure valid range (though scaling happens later)
    p_planet = np.clip(p_planet, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_state = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Apply Stickiness
        if last_action_1 != -1:
            # 1. Motor Stickiness (Action Repetition)
            logits_1[last_action_1] += p_stick * 5.0
            
            # 2. Planet Perseveration (State Repetition)
            # Add bonus to actions proportional to their probability of reaching the last_state
            # If last_state was 0, Action 0 gets 0.7*bonus, Action 1 gets 0.3*bonus.
            planet_bonus = p_planet * 5.0
            logits_1[0] += planet_bonus * transition_matrix[0, last_state]
            logits_1[1] += planet_bonus * transition_matrix[1, last_state]
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_state = s_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Eligibility Traces (Loss-Sensitive)
This model proposes that OCI affects how credit is assigned specifically during failure (non-rewarded) trials. While `lambda_rew` controls credit assignment for rewards, `lambda_loss` controls how much a lack of reward propagates back to the initial choice. High OCI might lead to excessive "blame" assignment to the Stage 1 choice when the outcome is negative.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Lambda Model.
    
    Differentiates the eligibility trace (lambda) for rewarded vs. unrewarded trials.
    OCI modulates the lambda specifically for unrewarded (loss) trials, testing if
    symptoms relate to how failures are attributed to the initial choice.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    p_stick: [0,1] - General stickiness (scaled by 5).
    lambda_rew: [0,1] - Eligibility trace for rewarded trials.
    lambda_loss_base: [0,1] - Base eligibility trace for unrewarded trials.
    lambda_loss_oci_mod: [0,1] - Modulation of loss lambda by OCI.
    """
    learning_rate, beta, w, p_stick, lambda_rew, lambda_loss_base, lambda_loss_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate lambda for loss trials
    lambda_loss = lambda_loss_base + (lambda_loss_oci_mod - 0.5) * oci_score
    lambda_loss = np.clip(lambda_loss, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stick * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Select lambda based on outcome
        if r == 1.0:
            current_lambda = lambda_rew
        else:
            current_lambda = lambda_loss
            
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * current_lambda * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Surprise-Modulated Learning Rate
This model posits that learning is dynamic and driven by "surprise" (the magnitude of the prediction error). OCI modulates the sensitivity to this surprise. High OCI individuals might be hyper-vigilant, increasing their learning rate significantly when outcomes (transitions) violate their expectations (`delta_stage1`), while low OCI individuals might have a more stable learning rate.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Surprise-Modulated Learning Rate Model.
    
    The learning rate is not fixed but dynamic, increasing with the absolute 
    magnitude of the Stage 1 prediction error (surprise). OCI modulates the 
    strength of this dynamic adjustment (k), reflecting sensitivity to prediction errors.
    
    Parameters:
    lr_base: [0,1] - Baseline learning rate.
    beta: [0,10]
    w: [0,1]
    p_stick: [0,1] - General stickiness (scaled by 5).
    lambda_val: [0,1]
    k_base: [0,1] - Base scaling factor for the dynamic learning rate component.
    k_oci_mod: [0,1] - Modulation of the surprise sensitivity by OCI.
    """
    lr_base, beta, w, p_stick, lambda_val, k_base, k_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate sensitivity to surprise
    k_val = k_base + (k_oci_mod - 0.5) * oci_score
    k_val = np.clip(k_val, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stick * 5.0
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Dynamic Learning Rate based on surprise (abs(delta_stage1))
        # delta_stage1 represents the discrepancy between expected val of choice 1 and val of state 2
        surprise = np.abs(delta_stage1)
        current_lr = lr_base + k_val * surprise
        current_lr = np.clip(current_lr, 0.0, 1.0)
        
        q_stage1_mf[a1] += current_lr * delta_stage1 + current_lr * lambda_val * delta_stage2
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```