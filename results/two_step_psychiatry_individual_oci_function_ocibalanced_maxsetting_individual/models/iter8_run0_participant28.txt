Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in this two-step task, specifically focusing on mixing model-based/model-free strategies, punishment sensitivity, and second-stage stickiness.

### Cognitive Model 1: OCI-Modulated Balance between Model-Based and Model-Free Control
This model hypothesizes that OCI symptoms correlate with a deficit in Model-Based (goal-directed) control, leading to a greater reliance on Model-Free (habitual) control. Instead of a fixed mixing weight `w`, the weight is a logistic function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where the mixing weight 'w' 
    is a logistic function of the OCI score.
    
    Hypothesis: Higher OCI scores might lead to reduced model-based control (lower w).
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_intercept: [0, 1] Intercept for the logistic function determining w.
    w_slope: [-5, 5] Slope for the logistic function determining w based on OCI.
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w based on OCI using a logistic function to keep it in [0, 1]
    # We map the bounded parameters to a logit space for the linear combination
    # w = 1 / (1 + exp(-(intercept + slope * oci)))
    # To make the parameters intuitive, we treat w_intercept as the 'base' logit
    logit_w = np.log(w_intercept / (1 - w_intercept + 1e-10)) + w_slope * oci_score
    w = 1.0 / (1.0 + np.exp(-logit_w))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value: Q_MB(s1, a1) = Sum(P(s2|s1,a1) * max_a2 Q_MF(s2, a2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 update (Model-Free)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (Model-Free TD(1) style for simplicity in hybrid models)
        # Using the actual reward experienced at stage 2 to update stage 1 MF value
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1] # TD(0)
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage1_mf[a1] += learning_rate * delta_stage2 # Eligibility trace lambda=1 effectively

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Punishment Sensitivity (Loss Aversion)
This model investigates if OCI scores relate to how strongly participants react to the *absence* of reward (0 coins). It proposes that higher OCI might lead to a stronger "punishment" signal when receiving 0 coins, effectively treating 0 as a loss rather than just a neutral outcome.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated punishment sensitivity.
    
    Hypothesis: Participants with higher OCI might perceive the lack of reward (0)
    as more aversive (a loss) than those with lower scores.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    lambda_param: [0, 1] Eligibility trace parameter.
    oci_punish_sens: [0, 5] Scaling factor for how much OCI increases the negativity of 0 rewards.
                            Effective reward = reward - (oci_punish_sens * oci * (1-reward))
    """
    learning_rate, beta, lambda_param, oci_punish_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # If reward is 0, the effective value becomes negative based on OCI.
    # If reward is 1, it stays 1.
    # punishment_magnitude = oci_punish_sens * oci_score

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Transform reward
        # If r=1, term is 0. If r=0, term is 1.
        effective_r = r - (oci_punish_sens * oci_score * (1 - r))

        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = effective_r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1
        
        # Eligibility trace for Stage 1
        q_stage1[a1] += learning_rate * lambda_param * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Second-Stage Stickiness
This model posits that OCI symptoms manifest as a repetitive behavior or "stickiness" specifically at the second stage (choosing the alien). It tests if the tendency to repeat the previous choice of alien (regardless of reward) is modulated by the OCI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where OCI modulates 'stickiness' (choice perseveration) 
    specifically at the second stage (alien choice).
    
    Hypothesis: High OCI leads to repetitive checking/choice behavior at the terminal stage.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-based weighting.
    stick_base: [0, 5] Base level of choice stickiness (bonus added to Q-value of previous choice).
    oci_stick_slope: [-5, 5] How OCI modulates this stickiness.
    """
    learning_rate, beta, w, stick_base, oci_stick_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness parameter
    # We allow it to be negative (alternation) or positive (perseveration)
    # but base is usually positive.
    stickiness = stick_base + (oci_stick_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous action for stage 2
    # Initialize with -1 (no previous action)
    prev_a2 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        # Add stickiness bonus to the Q-values before softmax
        q_stage2_modified = q_stage2_mf[s_idx].copy()
        if prev_a2 != -1:
            # Note: The stickiness applies to the specific motor action (alien index)
            # regardless of which planet we are on, assuming buttons are consistent.
            # However, typically in this task, aliens are specific to planets. 
            # If we assume stickiness is about the 'slot' (left/right), we apply to index.
            q_stage2_modified[prev_a2] += stickiness
            
        exp_q2 = np.exp(beta * q_stage2_modified)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        q_stage1_mf[a1] += learning_rate * delta_stage2 # lambda=1 implicit

        prev_a2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```