Here are the 3 new cognitive models based on the task description and participant data.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Distorted Probability Model.
    
    Hypothesis: OCI symptoms correlate with a distortion in the perception of 
    transition probabilities used in Model-Based planning. High OCI might lead to 
    perceiving the transitions as more uncertain (entropy maximization) or more 
    deterministic, altering the MB value calculation.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weighting between Model-Based and Model-Free values.
    stickiness: [0, 5] - Choice perseverance bonus.
    gamma_base: [0, 5] - Base probability distortion exponent.
    gamma_oci: [-2, 2] - Modulation of distortion by OCI.
    """
    learning_rate, beta, w, stickiness, gamma_base, gamma_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate distorted transition probability
    # p_perceived = p^gamma / sum(p^gamma)
    gamma = gamma_base + gamma_oci * oci_score
    gamma = max(0.01, gamma) # Ensure positive
    
    # Base common probability
    p0 = 0.7
    # Distort
    p_common = (p0**gamma) / (p0**gamma + (1-p0)**gamma)
    p_rare = 1.0 - p_common
    
    # Transition matrix: T[action, state]
    # Action 0 (A) -> Common 0 (X), Rare 1 (Y)
    # Action 1 (U) -> Common 1 (Y), Rare 0 (X)
    transition_matrix = np.array([[p_common, p_rare], 
                                  [p_rare, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state, alien
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Counterfactual Update Model.
    
    Hypothesis: OCI symptoms correlate with 'fictitious play' or counterfactual 
    updating in the second stage. Participants may update the value of the 
    unchosen alien based on the outcome of the chosen one (assuming anticorrelation), 
    with the strength of this effect modulated by OCI.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    stickiness: [0, 5] - Choice stickiness.
    cf_base: [0, 1] - Base counterfactual update rate.
    cf_oci: [-1, 1] - Modulation of counterfactual rate by OCI.
    """
    learning_rate, beta, w, stickiness, cf_base, cf_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate counterfactual learning rate modifier
    cf_rate = cf_base + cf_oci * oci_score
    cf_rate = np.clip(cf_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2
        # Update chosen
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Update unchosen (Counterfactual)
        # Assume anticorrelation: if chosen got r, unchosen would get 1-r
        # We scale the update by cf_rate * learning_rate
        unchosen_a2 = 1 - a2
        assumed_r = 1.0 - r
        delta_cf = assumed_r - q_stage2_mf[s, unchosen_a2]
        q_stage2_mf[s, unchosen_a2] += learning_rate * cf_rate * delta_cf
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Rare Transition Learning Model.
    
    Hypothesis: OCI symptoms affect the sensitivity to prediction errors generated 
    after rare transitions. High OCI might lead to over-learning or under-learning 
    from these 'surprising' events in the Model-Free system.
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weight.
    stickiness: [0, 5] - Choice stickiness.
    rare_mod_base: [0, 5] - Base multiplier for learning rate on rare transitions.
    rare_mod_oci: [-2, 2] - Modulation of rare transition multiplier by OCI.
    """
    learning_rate, beta, w, stickiness, rare_mod_base, rare_mod_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate rare transition learning rate modifier
    rare_mod = rare_mod_base + rare_mod_oci * oci_score
    rare_mod = max(0.0, rare_mod)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Determine if transition was common or rare
        # A(0)->X(0) Common, A(0)->Y(1) Rare
        # U(1)->Y(1) Common, U(1)->X(0) Rare
        is_common = (a1 == s)
        
        # Select learning rate
        if is_common:
            current_lr = learning_rate
        else:
            current_lr = learning_rate * rare_mod
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Stage 2 Update (Standard LR)
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```