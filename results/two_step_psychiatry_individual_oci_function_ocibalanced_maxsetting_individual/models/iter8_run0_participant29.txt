Here are three new cognitive models that introduce different mechanisms for how OCI scores might modulate learning and decision-making in the two-step task.

### Model 1: OCI-Modulated Second-Stage "Anxiety" (Loss Sensitivity)
This model hypothesizes that individuals with higher OCI scores might be more sensitive to the *absence* of reward (0 coins) in the second stage. High OCI is often associated with anxiety and error monitoring. In this model, the learning rate for negative prediction errors (when reward is 0) is scaled by the OCI score. This creates an asymmetry where "bad" outcomes are learned differently depending on the participant's symptom severity.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Loss Sensitivity.
    
    This model posits that OCI modulates the learning rate specifically for 
    negative outcomes (reward = 0). High OCI might lead to hypersensitivity 
    or hyposensitivity to 'losses' (missing out on coins).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - lr_neg_oci_scale: [0, 1] Scaling factor for OCI on negative learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based (1) vs Model-free (0) weighting.
    """
    lr_pos, lr_neg_base, lr_neg_oci_scale, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the specific learning rate for negative errors based on OCI
    # We clip to ensure it stays valid [0, 1]
    lr_neg = min(1.0, max(0.0, lr_neg_base + (lr_neg_oci_scale * oci_score)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        
        # Select learning rate based on sign of prediction error
        current_lr = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s2, a2] += current_lr * delta_stage2

        # Stage 1 update (TD-0)
        # Using the same current_lr for consistency in this trial's update logic
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Learning (Uncertainty)
Standard models assume the transition matrix is fixed (70/30). However, anxious or obsessive individuals might constantly re-evaluate the structure of the world, doubting the stability of the transitions. This model allows the participant to "learn" the transition matrix online, with the learning rate for these transitions modulated by OCI. A high OCI score might lead to 'over-updating' the transition probabilities based on recent rare events, reflecting a belief that the world structure is volatile.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Learning.
    
    This model assumes the participant does not trust the fixed 70/30 transitions 
    and updates their internal model of state transitions. The rate at which they 
    update this belief is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - trans_lr_base: [0, 1] Base learning rate for transition probabilities.
    - trans_lr_oci: [0, 1] Modulation of transition learning rate by OCI.
    """
    learning_rate, beta, w, trans_lr_base, trans_lr_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition learning rate modulated by OCI
    trans_lr = min(1.0, max(0.0, trans_lr_base + (trans_lr_oci * oci_score)))

    # Initialize subjective transition matrix (starts at true values or neutral)
    # Rows: Action 1 (0 or 1), Cols: State 2 (0 or 1)
    # We initialize at 0.5 to represent initial uncertainty or 0.7 for prior knowledge.
    # Let's assume they start with the instruction knowledge:
    # Action 0 -> State 0 is common (0.7)
    # Action 1 -> State 1 is common (0.7)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]]) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Calculate MB values using the DYNAMIC transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB calculation: P(S2|A1) * max Q(S2)
        q_stage1_mb = np.zeros(2)
        q_stage1_mb[0] = trans_probs[0, 0] * max_q_stage2[0] + trans_probs[0, 1] * max_q_stage2[1]
        q_stage1_mb[1] = trans_probs[1, 0] * max_q_stage2[0] + trans_probs[1, 1] * max_q_stage2[1]

        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating Values ---
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # --- Updating Transitions ---
        # Update probability of reaching state s2 given action a1
        # The row trans_probs[a1] must sum to 1.
        # We increase prob of the observed transition and decrease the other.
        trans_probs[a1, s2] += trans_lr * (1 - trans_probs[a1, s2])
        trans_probs[a1, 1 - s2] = 1.0 - trans_probs[a1, s2]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated "Perseveration" (Choice Stickiness)
This model tests the hypothesis that OCI is related to behavioral rigidity or perseveration. Instead of modifying how values are learned, OCI modifies a "stickiness" parameter. This parameter adds a bonus to the action chosen in the previous trial, regardless of reward. A higher OCI score might lead to higher stickiness (repeating choices compulsively) or potentially lower stickiness (switching frequently due to doubt), captured by the linear modulation.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Choice Stickiness.
    
    This model adds a 'stickiness' bonus to the previously chosen action in Stage 1.
    The magnitude of this stickiness is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick_base: [0, 5] Base stickiness bonus (can be large to force repetition).
    - stick_oci_slope: [-5, 5] How OCI changes stickiness (can increase or decrease it).
    """
    learning_rate, beta, w, stick_base, stick_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate stickiness parameter
    stickiness = stick_base + (stick_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice (initialize to -1 or handle first trial separately)
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness to the Q-values before softmax
        # We create a temporary Q vector for decision making
        q_decision = q_net.copy()
        if last_action_1 != -1:
            q_decision[last_action_1] += stickiness
        
        exp_q1 = np.exp(beta * q_decision)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update last action
        last_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```