Here are three new cognitive models that explore different mechanisms for how OCI might influence decision-making in this two-step task, specifically focusing on perseveration, learning rates for rare transitions, and second-stage temperature modulation.

### Cognitive Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that high OCI scores lead to increased "stickiness" or perseveration—a tendency to repeat the previous choice regardless of reward history. This reflects the repetitive behaviors often characteristic of OCD.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Perseveration (Stickiness).
    
    Hypothesis: Participants with higher OCI scores exhibit greater 'stickiness' 
    (perseveration) in their first-stage choices, repeating the previous action 
    regardless of the outcome.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Weighting parameter (0=MF, 1=MB)
    stickiness_base: [-5, 5] - Base tendency to repeat choice
    stickiness_oci_slope: [-5, 5] - How much OCI increases stickiness
    """
    learning_rate, beta, w, stickiness_base, stickiness_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    stickiness = stickiness_base + (stickiness_oci_slope * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness
    prev_a1 = -1 

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        q_net_stick = q_net.copy()
        if prev_a1 != -1:
            q_net_stick[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1: # Valid trial
            # Update Stage 2 MF values (TD learning)
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2
            
            # Update Stage 1 MF values (TD learning)
            # Using the stage 2 value of the chosen state/action as the target
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Second-Stage Temperature
This model suggests that OCI affects the exploration-exploitation balance specifically in the second stage. High OCI might correlate with "anxiety" or uncertainty about the specific reward probabilities of the aliens, leading to more random (higher temperature/lower beta) or more deterministic (lower temperature/higher beta) behavior once a planet is reached.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Second-Stage Temperature.
    
    Hypothesis: OCI modulates the inverse temperature (beta) specifically at the 
    second stage. This reflects how symptoms might alter decision consistency 
    when facing immediate reward outcomes (aliens) vs abstract planning (spaceships).
    
    Bounds:
    learning_rate: [0, 1]
    beta_1: [0, 10] - Inverse temperature for stage 1
    beta_2_base: [0, 10] - Base inverse temperature for stage 2
    beta_2_oci_slope: [-5, 5] - Effect of OCI on stage 2 beta
    w: [0, 1]
    """
    learning_rate, beta_1, beta_2_base, beta_2_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stage 2 beta based on OCI
    # We use exp/softplus or simple clipping to keep beta positive if needed, 
    # but simple addition with clipping is robust enough for this context.
    beta_2 = beta_2_base + (beta_2_oci_slope * oci_score)
    beta_2 = np.maximum(0.0, beta_2) # Ensure non-negative
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Use the OCI-modulated beta here
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # Update Stage 2
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2
            
            # Update Stage 1
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Dependent Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) that determines how much the second-stage reward prediction error updates the first-stage values directly. This effectively blends Model-Free TD(0) and TD(1). The hypothesis is that OCI affects how credit is assigned to past actions—high OCI might link outcomes more strongly to the distal cause (the spaceship choice), effectively increasing $\lambda$.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Eligibility Trace (Lambda).
    
    Hypothesis: OCI modulates the eligibility trace parameter (lambda). 
    A higher lambda means the stage 1 value is updated more strongly by the 
    final reward outcome (TD(1)-like), rather than just the stage 2 value (TD(0)-like).
    High OCI might reflect a stronger drive to attribute outcomes to initial choices.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    lambda_base: [0, 1] - Base eligibility trace
    lambda_oci_slope: [-1, 1] - Effect of OCI on lambda
    """
    learning_rate, beta, w, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate lambda based on OCI
    lambda_param = lambda_base + (lambda_oci_slope * oci_score)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        if r != -1:
            # Stage 2 RPE
            delta_stage2 = r - q_stage2_mf[s, a2]
            
            # Stage 1 RPE (standard TD(0) part)
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            
            # Update Stage 2
            q_stage2_mf[s, a2] += learning_rate * delta_stage2
            
            # Update Stage 1 with eligibility trace
            # The update combines the immediate transition error (delta_stage1)
            # and the stage 2 error weighted by lambda (lambda * delta_stage2)
            q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_param * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```