Here are the three proposed cognitive models.

### Model 1: Rare Transition Aversion (OCI-Modulated Penalty)
This model hypothesizes that individuals with high OCI symptoms experience a sense of "loss of control" or "error" when a rare transition occurs, leading them to penalize the chosen spaceship regardless of the actual reward obtained. This "transition aversion" is modeled as a penalty term subtracted from the Stage 1 update target, proportional to the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Rare Transition Aversion Model.
    
    Hypothesis: High OCI individuals penalize the Stage 1 choice if it results in a 
    Rare transition, treating the unexpected state transition as a negative outcome 
    (loss of control), distinct from the monetary reward.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - rare_penalty_scale: [0, 5] Magnitude of the penalty applied to Q-values 
      after a rare transition, scaled by OCI.
    """
    learning_rate, beta, w, rare_penalty_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    # Q-values initialization
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Determine if transition was rare
        # Common: 0->0, 1->1. Rare: 0->1, 1->0.
        is_rare = (a1 != s_idx)
        
        # Calculate penalty based on OCI
        penalty = 0.0
        if is_rare:
            penalty = rare_penalty_scale * oci_score

        # Stage 2 Update (Standard)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (With Rare Transition Penalty)
        # The target is Q_stage2 - penalty
        delta_stage1 = (q_stage2_mf[s_idx, a2] - penalty) - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    return log_loss
```

### Model 2: OCI-Modulated Counterfactual Updating
This model hypothesizes that high OCI is associated with "worrying" or "checking" the unchosen path. Participants update not only the chosen spaceship's value but also the *unchosen* spaceship's value based on the best possible outcome of the *other* planet (counterfactual simulation). The rate of this counterfactual update is modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Counterfactual Updating Model.
    
    Hypothesis: High OCI individuals engage in counterfactual updating ('fictitious play'),
    updating the value of the unchosen Stage 1 option towards the value of the 
    alternative state (assuming common transitions). The strength of this 
    update is proportional to OCI severity.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - cf_update_scale: [0, 1] Scaling factor for the counterfactual learning rate.
      Alpha_CF = learning_rate * cf_update_scale * OCI.
    """
    learning_rate, beta, w, cf_update_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (Chosen)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 1 Update (Counterfactual / Unchosen)
        # Assume unchosen action would have led to the OTHER planet (common transition logic)
        a_unchosen = 1 - a1
        s_counterfactual = 1 - s_idx # Approximate counterfactual state
        
        # Value of the counterfactual state (max Q)
        v_counterfactual = np.max(q_stage2_mf[s_counterfactual])
        
        # Counterfactual Learning Rate
        alpha_cf = learning_rate * cf_update_scale * oci_score
        # Clamp to [0, 1] for stability
        if alpha_cf > 1.0: alpha_cf = 1.0
        
        delta_cf = v_counterfactual - q_stage1_mf[a_unchosen]
        q_stage1_mf[a_unchosen] += alpha_cf * delta_cf

    return log_loss
```

### Model 3: OCI-Enhanced Stage 2 Precision
This model proposes that OCI symptoms manifest as "local perfectionism" or rigidity specifically at the second stage (the bandit task). While Stage 1 involves complex planning (MB/MF), Stage 2 is a direct choice. High OCI leads to a significantly higher inverse temperature (beta) at Stage 2 compared to Stage 1, reflecting a compulsive drive to maximize immediate reward or intolerance of exploration at the local level.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Enhanced Stage 2 Precision Model.
    
    Hypothesis: High OCI individuals exhibit differential rigidity between stages.
    Specifically, OCI increases the inverse temperature (beta) at Stage 2 relative 
    to Stage 1, reflecting a compulsive maximization strategy ('greediness') 
    once the state is determined.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_stage1: [0, 10] Inverse temperature for Stage 1 choice.
    - w: [0, 1] Mixing weight.
    - beta2_boost_oci: [0, 5] Factor increasing Stage 2 beta based on OCI.
      Beta_Stage2 = Beta_Stage1 * (1 + beta2_boost_oci * OCI).
    """
    learning_rate, beta_stage1, w, beta2_boost_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10
    
    # Calculate Stage 2 Beta
    beta_stage2 = beta_stage1 * (1.0 + beta2_boost_oci * oci_score)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Uses beta_stage1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # --- Stage 2 Policy (Uses beta_stage2) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    return log_loss
```