Here are three new cognitive models, expressed as Python functions, that incorporate the OCI score into different mechanisms of the two-step task.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Stage 1 Learning Rate.
    
    Hypothesis: OCI affects the learning rate of the first-stage (abstract) choices differently
    than the second-stage (concrete) choices. High OCI might lead to rigidity (low LR) or 
    volatility (high LR) specifically in the planning stage, distinct from the immediate
    sensory association learning in stage 2.
    
    lr_stage1 = lr_s1_base + lr_s1_oci * oci
    
    Parameters:
    beta: [0, 10] - Inverse temperature (shared across stages).
    w: [0, 1] - Model-based / Model-free weight.
    lr_stage2: [0, 1] - Learning rate for stage 2 (fixed).
    lr_s1_base: [0, 1] - Base learning rate for stage 1.
    lr_s1_oci: [-1, 1] - Modulation of stage 1 learning rate by OCI.
    """
    beta, w, lr_stage2, lr_s1_base, lr_s1_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated Stage 1 Learning Rate
    lr_stage1 = lr_s1_base + (lr_s1_oci * current_oci)
    lr_stage1 = np.clip(lr_stage1, 0.0, 1.0) # Ensure bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        # Stage 1 prediction error (TD(0) style, using Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1 # Use OCI-modulated LR
        
        # Stage 2 prediction error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2 # Use fixed LR

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Stage 1 Inverse Temperature (Exploration).
    
    Hypothesis: OCI affects the exploration/exploitation trade-off specifically at the 
    first stage (strategic choice). High OCI might lead to more deterministic choices 
    (high beta) or more random anxiety-driven choices (low beta) at the top level, 
    while the second stage (easy discrimination) remains constant.
    
    beta_stage1 = beta_s1_base + beta_s1_oci * oci
    
    Parameters:
    learning_rate: [0, 1] - Learning rate (shared).
    w: [0, 1] - Model-based / Model-free weight.
    beta_stage2: [0, 10] - Inverse temperature for stage 2 (fixed).
    beta_s1_base: [0, 10] - Base inverse temperature for stage 1.
    beta_s1_oci: [-5, 5] - Modulation of stage 1 beta by OCI.
    """
    learning_rate, w, beta_stage2, beta_s1_base, beta_s1_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated Stage 1 Beta
    beta_stage1 = beta_s1_base + (beta_s1_oci * current_oci)
    beta_stage1 = np.maximum(beta_stage1, 0.0) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use OCI-modulated Beta for Stage 1
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Use fixed Beta for Stage 2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Memory Decay.
    
    Hypothesis: OCI is related to the persistence of value representations. 
    A decay parameter 'forgets' Q-values over time (on every trial, values decay towards 0). 
    The rate of this decay is modulated by OCI.
    
    decay = decay_base + decay_oci * oci
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-based / Model-free weight.
    decay_base: [0, 1] - Base decay rate (0 = no decay, 1 = instant forgetting).
    decay_oci: [-1, 1] - Modulation of decay by OCI.
    """
    learning_rate, beta, w, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate OCI-modulated Decay
    decay = decay_base + (decay_oci * current_oci)
    decay = np.clip(decay, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Apply Decay ---
        # Values decay towards 0 before decision/update on every trial
        q_stage1_mf *= (1.0 - decay)
        q_stage2_mf *= (1.0 - decay)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```