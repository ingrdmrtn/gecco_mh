Here are three new cognitive models exploring different mechanisms of how OCI scores might influence decision-making in this two-step task, specifically focusing on the balance between model-based/model-free control, habit formation, and learning rates.

### Model 1: OCI-Modulated Mixing Weight (w) with Sigmoid Transformation
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is determined by the OCI score. Instead of a linear relationship, which can be unbounded, this model uses a sigmoid function to map the OCI score to the mixing weight $w$, allowing for a non-linear transition between strategies based on symptom severity. High OCI might push participants towards extreme model-based (over-planning) or model-free (habitual) extremes.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL where the mixing weight 'w' is a sigmoid function of OCI.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_scale: [0, 10]  # Steepness of the sigmoid curve
    w_shift: [-1, 1]  # Shift of the sigmoid midpoint relative to OCI
    """
    learning_rate, beta, w_scale, w_shift = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Sigmoid mapping for w to keep it strictly in [0, 1]
    # w = 1 / (1 + exp(-k * (oci - x0)))
    # High w -> Model-Based, Low w -> Model-Free
    # w_scale determines how sensitive w is to OCI differences
    # w_shift adjusts the inflection point
    w = 1.0 / (1.0 + np.exp(-w_scale * (oci_score - (0.5 + w_shift))))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: Q_MB(s1, a1) = Sum(T(s1, a1, s2) * max(Q_MF(s2, :)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updating ---
        # Stage 2 TD Error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 TD Error (SARSA-like for MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Second-Stage Learning Rate Asymmetry
This model posits that OCI affects how participants learn from positive versus negative outcomes specifically in the second stage (the direct reward stage). Individuals with high OCI might be hyper-sensitive to errors (negative prediction errors) or rewards. This model introduces separate learning rates for positive and negative prediction errors in the second stage, where the ratio or balance is modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with asymmetric learning rates for positive/negative RPEs in Stage 2.
    The asymmetry is modulated by OCI.
    
    Bounds:
    alpha_base: [0,1]
    alpha_asym_oci: [-1, 1]  # Modulation of asymmetry by OCI
    beta: [0,10]
    w: [0, 1] # Fixed mixing weight
    """
    alpha_base, alpha_asym_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Define alpha_pos and alpha_neg centered around alpha_base
    # If alpha_asym_oci is positive, high OCI increases learning from positive RPEs
    # relative to negative RPEs, or vice versa.
    # We use a tanh to bound the modulation factor.
    mod = np.tanh(alpha_asym_oci * oci_score) 
    
    # We ensure alphas stay in [0,1]
    # If mod > 0: pos boosted, neg suppressed. If mod < 0: pos suppressed, neg boosted.
    alpha_pos = np.clip(alpha_base * (1 + mod), 0.001, 0.999)
    alpha_neg = np.clip(alpha_base * (1 - mod), 0.001, 0.999)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy (Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updating Stage 2 with asymmetric alphas
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, a2] += alpha_neg * delta_stage2
        
        # Updating Stage 1 (Standard MF update using alpha_base for simplicity/stability)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_base * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Decay Rate (Forgetting)
This model investigates if OCI relates to the persistence of value representations. It adds a decay (forgetting) parameter to unchosen options. The hypothesis is that high OCI might be associated with "stickier" memories (low decay) or conversely, a constant need to refresh information (high decay/volatility estimation). The decay rate is a linear function of the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the decay rate (forgetting) of unchosen options depends on OCI.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    decay_base: [0,1]
    decay_oci_slope: [-1, 1]
    w: [0,1]
    """
    learning_rate, beta, decay_base, decay_oci_slope, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate specific to this participant
    # Decay = 0 means perfect memory (values stay), Decay = 1 means instant forgetting (values -> 0)
    decay = decay_base + (decay_oci_slope * oci_score)
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Update Chosen Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay Unchosen Stage 2
        # For the current state, decay the unchosen action
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1 - decay)
        
        # Note: We are not decaying the other state's values here to keep it simple,
        # but one could decay all unvisited states. Here we focus on the active state options.

        # Update Chosen Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay Unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1 - decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```