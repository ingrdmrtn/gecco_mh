Here are the 3 proposed cognitive models.

### Model 1: OCI-Modulated Spatial Bias
This model hypothesizes that OCI scores correlate with a fixed, intrinsic preference (bias) for one of the choice options (Spaceship 0 vs Spaceship 1), independent of the reward history. This "spatial bias" or "side bias" captures the participant's tendency to default to a specific action (e.g., Spaceship 0.0) which is evident in the prolonged streaks in the data.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Spatial Bias Model.
    
    Hypothesis: High OCI scores are associated with a rigid, baseline preference 
    for one spaceship over the other (Spatial Bias), regardless of value.
    This bias is added to the Q-value of Action 0.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - lambd: [0,1] Eligibility trace.
    - bias_base: [-5, 5] Baseline bias towards Action 0.
    - bias_oci: [-5, 5] Modulation of bias by OCI score.
    """
    lr, beta, w, lambd, bias_base, bias_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate bias towards Action 0
    # A positive bias increases prob of choosing Action 0
    spatial_bias = bias_base + bias_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Skip invalid data
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Spatial Bias to Action 0
        q_net_biased = q_net.copy()
        q_net_biased[0] += spatial_bias
        
        exp_q1 = np.exp(beta * q_net_biased)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # Updates
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[a1] += lr * lambd * delta_2
        
    return log_loss
```

### Model 2: OCI-Modulated Subjective Transition Belief
This model suggests that high OCI participants may harbor distorted beliefs about the transition probabilities of the task (the "model" in Model-Based control). Instead of using the true 0.7/0.3 probabilities, they act according to a subjective probability that is modulated by their OCI score. This could reflect intolerance of uncertainty (believing transitions are more deterministic) or confusion (believing them to be more random).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Subjective Transition Belief Model.
    
    Hypothesis: OCI scores modulate the subjective probability 'T' used in the 
    Model-Based planning step. High OCI might lead to over-estimating the 
    reliability of transitions (rigidity) or under-estimating it (uncertainty),
    distinct from the true 0.7 probability.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - lambd: [0,1] Eligibility trace.
    - trans_logit_base: [-5, 5] Base logit for subjective transition probability.
    - trans_logit_oci: [-5, 5] Modulation of logit by OCI.
    """
    lr, beta, w, lambd, trans_logit_base, trans_logit_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability T
    logit = trans_logit_base + trans_logit_oci * oci_score
    # Sigmoid function to bound T between 0 and 1
    T = 1.0 / (1.0 + np.exp(-logit))
    
    # Subjective transition matrix
    # Assumes symmetry: P(X|A) = T, P(Y|A) = 1-T, etc.
    subj_transition_matrix = np.array([[T, 1.0 - T], [1.0 - T, T]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use subjective matrix for planning
        q_stage1_mb = subj_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # Updates
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2
        
    return log_loss
```

### Model 3: OCI-Modulated Habit Trace Decay
This model proposes that high OCI participants exhibit a different time-constant for habit formation. Instead of a simple 1-step stickiness, choices leave a "trace" that accumulates and decays over time. The rate of this decay is modulated by OCI. A slower decay (high persistence) in high OCI participants would explain the observed "streaks" of identical choices, as the habit strength builds up and persists longer than in low OCI participants.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Habit Trace Decay Model.
    
    Hypothesis: Choices leave a 'habit trace' that decays over time. 
    OCI modulates the decay rate of this trace. High OCI participants may have 
    slower decay (higher persistence), leading to stronger streaks of repetitive behavior.
    
    Parameters:
    - lr: [0,1] Learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-based weight.
    - lambd: [0,1] Eligibility trace.
    - habit_w: [0, 5] Weight of the habit trace in decision.
    - decay_base: [0,1] Base decay rate for habit trace.
    - decay_oci: [-1, 1] Modulation of decay rate by OCI.
    """
    lr, beta, w, lambd, habit_w, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate bounded [0, 1]
    decay = decay_base + decay_oci * oci_score
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit trace for Stage 1 actions
    habit_trace = np.zeros(2)
    
    log_loss = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s2 < 0 or a2 < 0:
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add habit trace influence
        q_net_habit = q_net + habit_w * habit_trace
        
        exp_q1 = np.exp(beta * q_net_habit)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-10)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s2])
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-10)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # Updates
        delta_1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_1
        
        delta_2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_2
        
        q_stage1_mf[a1] += lr * lambd * delta_2
        
        # Update Habit Trace
        # Decay existing traces
        habit_trace *= decay
        # Reinforce chosen action
        habit_trace[a1] += 1.0
        
    return log_loss
```