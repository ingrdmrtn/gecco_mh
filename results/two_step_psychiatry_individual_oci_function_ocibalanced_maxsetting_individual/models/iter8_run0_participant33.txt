def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Choice Kernel with OCI-Modulated Decay.
    
    Hypothesis: High OCI scores lead to more persistent habits. This is modeled 
    by a 'Choice Kernel' (CK) that tracks choice history. The decay rate of this 
    kernel is modulated by OCI: higher OCI results in slower decay (values stay 
    closer to 1), causing past choices to influence current decisions for longer.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    w_habit: [0, 5] - Weight of the choice kernel in the decision.
    decay_min: [0, 1] - The baseline decay rate when OCI is 0. 
                        Effective decay = decay_min + (1 - decay_min) * OCI.
    """
    learning_rate, beta, w_habit, decay_min = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective decay rate scales with OCI. 
    # High OCI -> Decay approaches 1.0 (very slow forgetting of habits).
    decay_rate = decay_min + (1.0 - decay_min) * oci_score

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    choice_kernel = np.zeros(2)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Decision ---
        # Net value is Model-Free Q + Weighted Habit (Choice Kernel)
        q_net = q_stage1 + w_habit * choice_kernel
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning & Habit Update ---
        # Update Choice Kernel
        choice_kernel *= decay_rate
        choice_kernel[a1] += 1.0
        
        # Standard Q-Learning
        delta_stage2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    TD(Lambda) with OCI-Modulated Eligibility Trace.
    
    Hypothesis: High OCI impairs the ability to perform long-range credit assignment.
    This is modeled using the eligibility trace parameter (lambda) in a TD algorithm.
    Higher OCI scores reduce lambda, making the agent more 'myopic' (relying only 
    on the immediate successor state value) rather than connecting the Stage 1 choice 
    directly to the final Reward.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature.
    lambda_max: [0, 1] - The maximum eligibility trace value (when OCI is 0).
                         Effective lambda = lambda_max * (1 - OCI).
    """
    learning_rate, beta, lambda_max = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Effective lambda decreases with OCI.
    # High OCI -> lambda near 0 (Pure TD, Step-by-Step).
    # Low OCI -> lambda near lambda_max (More Monte-Carlo-like integration).
    lambda_val = lambda_max * (1.0 - oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Decision ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning with Eligibility Trace ---
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2[s, a2]
        
        # Stage 1 Prediction Error
        delta_stage1 = q_stage2[s, a2] - q_stage1[a1]
        
        # Update Stage 2
        q_stage2[s, a2] += learning_rate * delta_stage2
        
        # Update Stage 1:
        # 1. Standard TD update (from Stage 2 value)
        # 2. Eligibility trace update (direct from Reward error)
        q_stage1[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Asymmetric Learning (Failure Ignoring).
    
    Hypothesis: High OCI individuals exhibit 'perseveration' by failing to unlearn 
    options that yield no reward. They learn normally from positive rewards (1), 
    but the learning rate for negative outcomes (0) is suppressed by their OCI score.
    This causes them to stick with a choice even after repeated failures.
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate for positive outcomes.
    beta: [0, 10] - Inverse temperature.
    w_mix: [0, 1] - Weight for the Model-Based component (0=MF, 1=MB).
    """
    learning_rate, beta, w_mix = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Asymmetric Learning Rates
    lr_pos = learning_rate
    # Negative learning rate is suppressed by OCI. 
    # If OCI is high, lr_neg becomes very small (ignoring failures).
    lr_neg = learning_rate * (1.0 - oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Decision (Hybrid) ---
        # Model-Based Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF
        q_net = w_mix * q_stage1_mb + (1.0 - w_mix) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Asymmetric Learning Update ---
        # Determine which learning rate to use based on the outcome
        current_lr = lr_pos if r > 0 else lr_neg
        
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += current_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss