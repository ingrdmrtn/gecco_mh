Here are three new cognitive models exploring different mechanisms by which OCI scores might interact with reinforcement learning in this two-step task.

### Model 1: Hybrid RL with OCI-Modulated Model-Based Weighting
This model tests the hypothesis that high OCI scores are associated with a deficit in goal-directed (model-based) control, leading to a reliance on habitual (model-free) strategies. Instead of a fixed weighting parameter $w$, the weight given to the model-based system is dynamically reduced as OCI increases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-Modulated Model-Based Weighting.

    Hypothesis: Higher OCI scores correlate with reduced goal-directed (model-based) control.
    The parameter `w_max` defines the maximum possible reliance on model-based planning,
    which is then dampened by the OCI score. A high OCI participant will have a lower `w`
    and rely more on model-free TD learning.

    Parameters:
    learning_rate: [0, 1] Update rate for Q-values (alpha).
    beta: [0, 10] Inverse temperature for softmax.
    w_max: [0, 1] Maximum weight for model-based values (when OCI is 0).
    oci_dampening: [0, 1] Strength of OCI's negative effect on model-based weight.
    """
    learning_rate, beta, w_max, oci_dampening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the effective mixing weight w
    # w represents the proportion of model-based control.
    # As OCI increases, w decreases.
    w = w_max * (1.0 - (oci_score * oci_dampening))
    # Ensure w stays within [0, 1]
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)  # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Calculation: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Stage 2 is purely model-free in this task structure
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # 1. Update Stage 2 MF values (TD error)
        # Q2(s, a2) <- Q2(s, a2) + alpha * (r - Q2(s, a2))
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # 2. Update Stage 1 MF values (TD(1) logic usually, or TD(0) to best stage 2)
        # Here we use the standard TD update using the value of the state reached
        # Q1(a1) <- Q1(a1) + alpha * (Q2(s, a2) - Q1(a1))
        # Note: Using the updated Q2 value for the TD target
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates with OCI-Driven Punishment Sensitivity
This model hypothesizes that OCI is related to an altered sensitivity to negative outcomes (punishment or lack of reward). Specifically, individuals with high OCI might update their beliefs more drastically after a loss (0 reward) compared to a win, or vice-versa. This model splits the learning rate into positive (`alpha_pos`) and negative components, where the negative learning rate is modulated by OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rates with OCI-Driven Punishment Sensitivity.

    Hypothesis: OCI modulates how strongly participants learn from unrewarded trials (punishment).
    High OCI might lead to "hyper-correction" or increased sensitivity to failure.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for rewarded trials (reward = 1).
    alpha_neg_base: [0, 1] Base learning rate for unrewarded trials (reward = 0).
    oci_neg_boost: [0, 1] Additional boost to negative learning rate scaled by OCI.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_neg_boost, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # High OCI -> Higher learning rate from zero-reward outcomes
    alpha_neg = alpha_neg_base + (oci_score * oci_neg_boost)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Pure Model-Free for simplicity in this mechanism focus) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Select learning rate based on outcome
        current_alpha = alpha_pos if r > 0 else alpha_neg

        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_alpha * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Free RL with OCI-Scaled Uncertainty Aversion
This model posits that OCI relates to intolerance of uncertainty. In this framework, participants track not just the value of actions, but also their uncertainty (variance). High OCI participants might penalize options that have high uncertainty, preferring "safer" or more familiar options even if their expected value is slightly lower.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with OCI-Scaled Uncertainty Aversion.

    Hypothesis: OCI correlates with Intolerance of Uncertainty.
    The model tracks the variance of rewards for each action.
    The choice policy subtracts a penalty proportional to the standard deviation (uncertainty)
    of the Q-value, scaled by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature.
    unc_penalty_base: [0, 5] Base penalty for uncertainty.
    oci_unc_sens: [0, 5] Multiplier for OCI's contribution to uncertainty aversion.
    """
    learning_rate, beta, unc_penalty_base, oci_unc_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective penalty weight for uncertainty
    w_uncertainty = unc_penalty_base + (oci_score * oci_unc_sens)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Uncertainty tracking (approximated as variance of prediction error)
    # Initialize with some uncertainty
    var_stage1 = np.ones(2) * 0.25 
    var_stage2 = np.ones((2, 2)) * 0.25

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Utility = Q - (w * sqrt(Variance))
        # We use sqrt to get standard deviation which is on the same scale as Q
        utility_1 = q_stage1_mf - (w_uncertainty * np.sqrt(var_stage1))
        
        exp_u1 = np.exp(beta * utility_1)
        probs_1 = exp_u1 / np.sum(exp_u1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        utility_2 = q_stage2_mf[s_idx] - (w_uncertainty * np.sqrt(var_stage2[s_idx]))
        
        exp_u2 = np.exp(beta * utility_2)
        probs_2 = exp_u2 / np.sum(exp_u2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Variance estimate for Stage 2 (simple moving average of squared error)
        # Var <- Var + alpha * (delta^2 - Var)
        var_stage2[s_idx, a2] += learning_rate * ((delta_stage2**2) - var_stage2[s_idx, a2])
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Variance estimate for Stage 1
        var_stage1[a1] += learning_rate * ((delta_stage1**2) - var_stage1[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```