Here are three new cognitive models based on the participant's data and OCI profile.

### Cognitive Model 1: Asymmetric Learning Rates (Pessimism Dampening)
This model posits that high OCI participants exhibit a specific rigidity in updating beliefs after negative outcomes. While they learn from rewards (positive prediction errors) normally, their OCI score dampens the learning rate for non-rewards (negative prediction errors). This "denial" of failure prevents the devaluation of the chosen option, leading to the observed "sticky" behavior where the participant continues to choose a spaceship despite repeated lack of rewards.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rates Model.
    
    Hypothesis:
    High OCI participants exhibit 'learning rigidity' specifically regarding negative feedback.
    They update expectations normally after rewards (positive prediction errors) but 
    suppress updates after lack of reward (negative prediction errors).
    This asymmetry causes them to stick with a choice even when it stops yielding rewards.

    Parameters:
    learning_rate: [0,1] - Base learning rate (used for positive prediction errors).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0) control.
    lambda_elig: [0,1] - Eligibility trace parameter.
    oci_neg_damp: [0,1] - Scaling factor for OCI-based dampening of negative learning rate.
                          lr_neg = learning_rate * (1 - oci * oci_neg_damp).
    """
    learning_rate, beta, w, lambda_elig, oci_neg_damp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate the dampened learning rate for negative prediction errors
    # If OCI is high, lr_neg becomes very small, ignoring failures.
    lr_neg = learning_rate * (1.0 - current_oci * oci_neg_damp)
    # Ensure non-negative
    lr_neg = max(0.0, lr_neg)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5  # Initialize at neutral
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Calculate Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Select Learning Rate based on sign of PE
        # Stage 1 MF Update
        lr_s1 = learning_rate if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += lr_s1 * (delta_stage1 + lambda_elig * delta_stage2)
        
        # Stage 2 MF Update
        lr_s2 = learning_rate if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Unchosen Option Devaluation (Tunnel Vision)
This model proposes that high OCI induces a "tunnel vision" effect where options not currently selected are actively devalued or forgotten. On every trial, the Q-value of the unchosen spaceship decays. This creates a self-reinforcing loop (entrenchment): the longer the participant ignores the alternative, the less valuable it appears, making a switch increasingly unlikely even if the current option performs poorly. This explains the long blocks of perseveration seen in the data.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Unchosen Option Devaluation Model (Tunnel Vision).
    
    Hypothesis:
    High OCI acts as a focusing lens that degrades the representation of unchosen alternatives.
    On each trial, the Q-value of the unchosen spaceship decays, representing 
    a loss of confidence or attention to options not actively engaged with.
    This leads to entrenchment: the longer an option is ignored, the less attractive it becomes.

    Parameters:
    learning_rate: [0,1] - Standard learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weight.
    lambda_elig: [0,1] - Eligibility trace.
    oci_decay: [0,1] - Rate at which unchosen Q-values decay, scaled by OCI.
                       decay_factor = 1 - (oci * oci_decay).
    """
    learning_rate, beta, w, lambda_elig, oci_decay = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    decay_rate = current_oci * oci_decay

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Standard Update for Chosen
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Devaluation of Unchosen Stage 1 Option
        # The unchosen option fades towards 0
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Conflict-Driven Rigidity
This model hypothesizes that OCI-related compulsivity is driven by an intolerance of uncertainty or decision conflict. When the Q-values of the two spaceships are similar (high conflict), a healthy participant might explore or dither (low beta). However, a high OCI participant reacts to this anxiety by becoming *more* rigid and deterministic (boosting beta), "locking in" to the slightly better option to resolve the conflict. This prevents switching when the current option's value drops to near-equal with the alternative.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Conflict-Driven Rigidity Model.
    
    Hypothesis:
    High OCI participants become more rigid (higher inverse temperature) specifically 
    when decision conflict is high (Q-values are similar). 
    Instead of dithering when unsure, they 'lock in' to the slightly better option 
    to resolve the anxiety of ambiguity.
    
    Parameters:
    learning_rate: [0,1] - Standard learning rate.
    beta: [0,10] - Base inverse temperature.
    w: [0,1] - MB/MF weight.
    lambda_elig: [0,1] - Eligibility trace.
    oci_conflict_boost: [0,10] - Increase in beta per unit of conflict, scaled by OCI.
                                 beta_eff = beta * (1 + oci * boost * conflict).
    """
    learning_rate, beta, w, lambda_elig, oci_conflict_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Conflict (Similarity between Q values)
        # Conflict is 1.0 when values are identical, approaches 0 as they diverge.
        q_diff = np.abs(q_net_stage1[0] - q_net_stage1[1])
        conflict = np.exp(-q_diff * 2.0) # Scaling factor for sensitivity
        
        # Modulate Beta based on Conflict and OCI
        # High Conflict + High OCI -> High Beta (Deterministic choice of the max)
        beta_effective = beta * (1.0 + current_oci * oci_conflict_boost * conflict)
        
        exp_q1 = np.exp(beta_effective * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        # No conflict modulation for stage 2 (aliens) assumed
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```