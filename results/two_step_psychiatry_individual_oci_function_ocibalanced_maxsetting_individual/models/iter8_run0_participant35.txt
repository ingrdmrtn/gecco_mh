Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Counterfactual Learning (Stage 2)
This model hypothesizes that individuals with varying OCI scores may differ in how they learn from the outcomes of options they *did not* choose (counterfactuals). Specifically, high OCI might be associated with increased "what if" thinking, updating the value of the unchosen alien in the second stage assuming an anti-correlated reward structure (if the chosen alien gave 0 coins, the unchosen one likely would have given 1).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Counterfactual Learning in Stage 2.
    
    Hypothesis: High OCI individuals may engage in stronger counterfactual updating 
    at the second stage (aliens). If they choose an alien and receive a reward (or not),
    they update the unchosen alien's value assuming an inverse outcome, modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based / Model-Free weight.
    - stickiness: [0, 5] Choice perseveration bonus for Stage 1.
    - cf_base: [0, 1] Baseline counterfactual update weight.
    - cf_oci: [-1, 1] Modulation of counterfactual weight by OCI.
    """
    learning_rate, beta, w, stickiness, cf_base, cf_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective counterfactual weight
    cf_weight = cf_base + (cf_oci * oci_score)
    if cf_weight < 0: cf_weight = 0.0
    if cf_weight > 1: cf_weight = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for aliens (State x Alien)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update Stage 2 (Chosen Alien)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 2 (Unchosen Alien - Counterfactual)
        # Assume unchosen would have yielded (1-r)
        unchosen_a2 = 1 - a2
        delta_cf = (1 - r) - q_stage2_mf[state_idx, unchosen_a2]
        q_stage2_mf[state_idx, unchosen_a2] += learning_rate * cf_weight * delta_cf

        # Update Stage 1 (Spaceship)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Action Generalization (Coupling)
This model proposes that high OCI scores might correlate with how much a participant generalizes feedback from the chosen spaceship to the unchosen spaceship. A positive coupling implies "global" generalization (e.g., "the environment is bad today"), while negative coupling implies a competitive belief structure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Action Generalization (Coupling) in Stage 1.
    
    Hypothesis: OCI modulates the extent to which prediction errors for the chosen
    spaceship generalize to the unchosen spaceship. High OCI might lead to 
    over-generalization (positive coupling) or strong differentiation (negative coupling).
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based / Model-Free weight.
    - stickiness: [0, 5] Perseveration bonus.
    - coupling_base: [-1, 1] Baseline coupling coefficient.
    - coupling_oci: [-1, 1] Modulation of coupling by OCI.
    """
    learning_rate, beta, w, stickiness, coupling_base, coupling_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective coupling
    coupling = coupling_base + (coupling_oci * oci_score)
    if coupling < -1.0: coupling = -1.0
    if coupling > 1.0: coupling = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 (Chosen)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 1 (Unchosen - Generalization)
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] += learning_rate * coupling * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Novelty Bonus (Checking Behavior)
This model captures "checking" behavior often associated with OCD. It adds an exploration bonus to spaceships based on how long it has been since they were last chosen. OCI modulates the strength of this novelty seeking (or avoidance).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Novelty Bonus (Checking Behavior).
    
    Hypothesis: High OCI individuals may exhibit specific "checking" behaviors,
    modeled here as a bonus added to the utility of an action proportional to 
    the time since it was last selected.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based / Model-Free weight.
    - stickiness: [0, 5] Perseveration bonus.
    - nov_base: [-1, 1] Baseline novelty/checking weight.
    - nov_oci: [-1, 1] Modulation of novelty weight by OCI.
    """
    learning_rate, beta, w, stickiness, nov_base, nov_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate novelty weight
    nov_weight = nov_base + (nov_oci * oci_score)
    # Clamp to reasonable range to prevent overflow/dominance
    if nov_weight < -2.0: nov_weight = -2.0
    if nov_weight > 2.0: nov_weight = 2.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    # Track time since last choice for Stage 1 actions (initialized to 0)
    time_since_choice = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Apply Novelty Bonus (Checking)
        # Add bonus to actions based on how long ago they were chosen
        q_net += nov_weight * time_since_choice
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Learning ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1
        
        # Update time counters
        time_since_choice += 1.0
        time_since_choice[a1] = 0.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```