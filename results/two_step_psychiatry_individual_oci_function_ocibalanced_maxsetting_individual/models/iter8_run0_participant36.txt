Here are the three proposed cognitive models.

### Model 1: Mixture Habit Model
This model hypothesizes that high OCI participants suffer from "compulsive lapses" where they repeat their previous choice regardless of the learned value. Instead of adding a bonus to the Q-values (which merely biases the softmax), this model implements a **mixture of policies**: a goal-directed/habitual RL policy and a deterministic "Repeat" policy. The weight of the "Repeat" policy is modulated by OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Mixture Habit Model.
    
    Hypothesis: OCI modulates the probability of 'lapsing' into a purely repetitive 
    habit. The final choice probability is a mixture of the RL-derived probability 
    and a deterministic 'stay' probability. High OCI increases the mixture weight 
    of the 'stay' component.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature for the RL component.
    - w: [0, 1] Weight for Model-Based vs Model-Free values in the RL component.
    - lambda_coeff: [0, 1] Eligibility trace for Stage 1 updates.
    - mix_base: [0, 1] Base mixture weight for the 'Repeat' policy.
    - mix_oci_slope: [0, 1] How strongly OCI increases the 'Repeat' weight.
    """
    learning_rate, beta, w, lambda_coeff, mix_base, mix_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixture weight for the "Repeat" policy, bounded [0, 1]
    omega = mix_base + (mix_oci_slope * oci_score)
    omega = np.clip(omega, 0.0, 0.99) # Cap to avoid log(0) issues

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # RL Component Probabilities
        logits = beta * q_net
        logits = logits - np.max(logits)
        probs_rl = np.exp(logits) / np.sum(np.exp(logits))
        
        # Mixture with "Repeat" Policy
        if prev_action_1 != -1:
            # Create deterministic stay probabilities [1, 0] or [0, 1]
            probs_repeat = np.zeros(2)
            probs_repeat[int(prev_action_1)] = 1.0
            
            # Mix
            probs_final = (1 - omega) * probs_rl + omega * probs_repeat
        else:
            # No history, use pure RL
            probs_final = probs_rl
            
        p_choice_1[trial] = probs_final[int(action_1[trial])]
        
        # Record action for next trial
        prev_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        # Stage 1 RPE (TD error)
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        # Stage 2 RPE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]

        # Update
        # Stage 1 gets contribution from Stage 1 error and Stage 2 error (via eligibility trace lambda)
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Subjective Transition Sharpness
This model hypothesizes that high OCI participants have a distorted, rigid belief about the task structure. While the true transition probability is 0.7, high OCI participants may perceive this as more deterministic (e.g., 0.9 or 1.0), causing them to ignore "rare" transitions as anomalies rather than informative outcomes. This "sharpness" prevents them from devaluing a spaceship after a bad outcome on a rare planet, leading to persistence.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Sharpness Model.
    
    Hypothesis: OCI modulates the perceived 'sharpness' or determinism of the 
    transition matrix used in Model-Based planning. High OCI leads to a belief 
    that transitions are nearly deterministic (ignoring rare transitions), 
    while low OCI might track the true stochasticity.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_coeff: [0, 1]
    - sharp_base: [-0.5, 0.5] Base distortion of the 0.7 probability.
    - sharp_oci: [0, 1] How much OCI increases the perceived determinism.
    """
    learning_rate, beta, w, lambda_coeff, sharp_base, sharp_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate subjective common transition probability
    # True is 0.7. Subjective can range from 0.5 (random) to 1.0 (deterministic)
    # sharp_base allows for under-confidence, sharp_oci adds over-confidence/rigidity
    p_common = 0.7 + sharp_base + (sharp_oci * oci_score)
    p_common = np.clip(p_common, 0.0, 1.0) # Bound between 0 and 1
    
    # Construct subjective transition matrix
    # If p_common > 0.5, diagonal is dominant.
    transition_matrix_mb = np.array([[p_common, 1 - p_common], [1 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Use the SUBJECTIVE transition matrix for MB calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix_mb @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        logits = beta * q_net
        logits = logits - np.max(logits)
        probs_1 = np.exp(logits) / np.sum(np.exp(logits))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]

        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Choice Confirmation
This model proposes that for high OCI participants, the act of choosing an option itself reinforces that option's value, independent of the outcome. This creates a self-reinforcing loop (compulsion) where choices are repeated because they were chosen before, accumulating "value" that is hard to unlearn. This is distinct from a transient bonus; it modifies the learned Q-values directly.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Choice Confirmation Model.
    
    Hypothesis: OCI modulates a 'choice confirmation' bias where the value of a 
    chosen action is updated positively just because it was chosen (Self-Reinforcement).
    This leads to runaway values and strong perseveration blocks.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - lambda_coeff: [0, 1]
    - conf_base: [0, 1] Base choice reinforcement rate.
    - conf_oci: [0, 1] OCI-dependent increase in choice reinforcement.
    """
    learning_rate, beta, w, lambda_coeff, conf_base, conf_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate confirmation learning rate/magnitude
    alpha_conf = conf_base + (conf_oci * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        logits = beta * q_net
        logits = logits - np.max(logits)
        probs_1 = np.exp(logits) / np.sum(np.exp(logits))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]

        # Standard Update
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1 + learning_rate * lambda_coeff * delta_stage2
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Choice Confirmation Update (Self-Reinforcement)
        # We update the chosen Stage 1 action towards a 'virtual reward' of 1 (or just add value)
        # Here we implement it as adding a fraction of the distance to 1.0 (saturation at 1)
        # Q = Q + alpha_conf * (1 - Q)
        val = q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += alpha_conf * (1.0 - val)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```