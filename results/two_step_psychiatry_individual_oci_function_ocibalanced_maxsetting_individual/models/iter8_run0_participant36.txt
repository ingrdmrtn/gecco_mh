def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Context-Dependent MB/MF Weighting (Rare Transition Sensitivity).
    Hypothesis: The balance between Model-Based and Model-Free control (w) depends on the 
    previous trial's transition type (Common vs Rare). High OCI participants may show 
    a specific deficit in maintaining MB control after Rare transitions, reverting to 
    habitual control (low w) when the internal model's predictions are violated.

    Parameters:
    - lr: [0, 1] Learning rate for Stage 1 and Stage 2 values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_common: [0, 1] MB weight applied after a Common transition.
    - w_rare_base: [0, 1] Base MB weight applied after a Rare transition.
    - oci_w_rare_mod: [-1, 1] Modulation of w_rare by OCI score.
    """
    lr, beta, w_common, w_rare_base, oci_w_rare_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate modulated weight for rare transitions
    w_rare = np.clip(w_rare_base + (oci_score * oci_w_rare_mod), 0.0, 1.0)

    # Transition matrix: A(0)->X(0) 0.7, A(0)->Y(1) 0.3; U(1)->X(0) 0.3, U(1)->Y(1) 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous transition type. True=Common, False=Rare. Start with Common assumption.
    last_transition_common = True 
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Determine weight w based on previous transition
        if last_transition_common:
            w = w_common
        else:
            w = w_rare
            
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Determine if this transition was common
        # Action 0 -> State 0 (Common), Action 1 -> State 1 (Common)
        is_common = (a1 == state_idx)
        last_transition_common = is_common
        
        # TD Updates
        # Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Stage 1 (MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Pure Model-Free with Outcome-Dependent Stickiness (Lose-Stay Bias).
    Hypothesis: OCI is associated with compulsive repetition of actions even after failure 
    (Lose-Stay). This model splits stickiness into 'Win-Stickiness' and 'Lose-Stickiness', 
    with OCI modulating the tendency to stick to the previous choice specifically after 
    a loss (reward=0).

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stick_win: [0, 5] Perseveration bonus after a reward (Win-Stay).
    - stick_lose_base: [0, 5] Base perseveration bonus after no reward (Lose-Stay).
    - oci_stick_lose_mod: [-1, 1] Modulation of Lose-Stay stickiness by OCI.
    """
    lr, beta, stick_win, stick_lose_base, oci_stick_lose_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    stick_lose = np.clip(stick_lose_base + (oci_score * oci_stick_lose_mod), 0.0, 5.0)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_reward = 0 # Assume neutral/loss start
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        q_net = q_stage1_mf.copy()
        
        # Apply outcome-dependent stickiness
        if last_action_1 != -1:
            if last_reward == 1.0:
                q_net[last_action_1] += stick_win
            else:
                q_net[last_action_1] += stick_lose
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        last_reward = r
        
        # Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Stage 1 (MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Reward-Dependent Precision (Beta Modulation).
    Hypothesis: Participants adjust their exploration/exploitation balance (beta) based on 
    the previous outcome. High OCI participants may exhibit rigidity (high beta) specifically 
    after losses, failing to explore alternative options when things go wrong, whereas 
    low OCI participants might increase exploration (low beta) after losses.

    Parameters:
    - lr: [0, 1] Learning rate.
    - w: [0, 1] Model-Based weight.
    - beta_win: [0, 10] Inverse temperature after a reward.
    - beta_lose_base: [0, 10] Base inverse temperature after no reward.
    - oci_beta_lose_mod: [-1, 1] Modulation of beta_lose by OCI.
    """
    lr, w, beta_win, beta_lose_base, oci_beta_lose_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    beta_lose = np.clip(beta_lose_base + (oci_score * oci_beta_lose_mod), 0.0, 10.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_reward = 0 
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Select beta based on previous reward
        if last_reward == 1.0:
            current_beta = beta_win
        else:
            current_beta = beta_lose
            
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Decision ---
        state_idx = int(state[trial])
        # Using the same current_beta for stage 2 to reflect general state of the organism.
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        last_reward = r
        
        # Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        
        # Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss