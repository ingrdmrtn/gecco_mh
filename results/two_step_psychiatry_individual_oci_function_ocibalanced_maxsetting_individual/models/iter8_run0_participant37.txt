Here are three new cognitive models grounded in the observation of the participant's high OCI score and their rigid adherence to specific choices (Spaceship 1.0). These models explore different mechanisms for how obsessive-compulsive traits might influence reinforcement learning, specifically focusing on perseveration, uncertainty avoidance, and altered learning rates.

### Cognitive Model 1: OCI-Modulated Perseveration (Stickiness)
**Hypothesis:** High OCI scores are associated with "stuckness" or behavioral rigidity. This model posits that the OCI score directly scales a "stickiness" parameter, which adds a bonus to the previously chosen action, making the agent more likely to repeat the same choice regardless of the reward outcome. This explains the participant's strong tendency to keep choosing Spaceship 1.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Perseveration (Stickiness).
    Hypothesis: High OCI leads to behavioral rigidity (stickiness). The model adds a 
    'stickiness' bonus to the Q-value of the previously chosen action at Stage 1.
    The magnitude of this bonus is scaled by the participant's OCI score.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stickiness_factor: [0, 5] (Scales the impact of OCI on repetition bias)
    """
    learning_rate, beta, w, stickiness_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate the effective stickiness bonus based on OCI
    # Higher OCI -> higher bonus for repeating the previous action
    stick_bonus = stickiness_factor * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Handle -1 in data (initial trials or errors)
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Model-Free and Model-Based values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Bonus
        q_net_stick = q_net.copy()
        if last_action_1 != -1:
            q_net_stick[last_action_1] += stick_bonus
            
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        # Stage 2 Update (TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        # Stage 1 Update (TD)
        # Note: Stickiness is a choice bias, usually not learned in Q-values directly
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Driven Model-Based Suppression
**Hypothesis:** Individuals with high compulsivity might rely less on flexible, goal-directed (model-based) planning and more on habitual (model-free) responses. This model proposes that as OCI increases, the weight `w` placed on the model-based component decreases. Instead of a fixed `w`, the model learns a `w_max` which is then penalized by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Model-Based Suppression.
    Hypothesis: High OCI reduces the reliance on the model-based (planning) system.
    The effective mixing weight 'w' is calculated as a base maximum capability 
    minus a penalty proportional to the OCI score. High OCI -> Lower w (more MF).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_max: [0, 1] (Maximum theoretical model-based weight)
    oci_penalty_factor: [0, 1] (How strongly OCI reduces w)
    """
    learning_rate, beta, w_max, oci_penalty_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w
    # If OCI is high, w decreases. We clip it to be non-negative.
    w_effective = w_max - (oci_penalty_factor * oci_score)
    w_effective = max(0.0, min(1.0, w_effective))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Use the OCI-adjusted w_effective
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Biased Learning Rates (Asymmetric Updating)
**Hypothesis:** Compulsivity might be driven by an over-sensitivity to negative outcomes or a failure to update beliefs positively. This model splits the learning rate into two: one for positive prediction errors (learning from success) and one for negative prediction errors (learning from disappointment). The OCI score shifts the balance between these two, potentially making the agent learn less from positive feedback (rigidly holding onto old beliefs) or more from negative feedback.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Biased Learning Rates (Asymmetric Updating).
    Hypothesis: OCI modulates the balance between learning from positive vs negative 
    prediction errors. High OCI might dampen learning from positive outcomes 
    (making behavior harder to change) or amplify negative learning.
    Here, OCI scales the learning rate for positive prediction errors specifically.
    
    Bounds:
    alpha_base: [0, 1] (Base learning rate)
    beta: [0, 10]
    w: [0, 1]
    oci_alpha_mod: [0, 1] (Modulation factor: 1.0 means OCI can fully suppress positive learning)
    """
    alpha_base, beta, w, oci_alpha_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate asymmetric learning rates
    # alpha_pos is reduced by OCI score. High OCI -> slower learning from "better than expected"
    alpha_pos = alpha_base * (1.0 - (oci_alpha_mod * oci_score))
    alpha_pos = max(0.01, alpha_pos) # Ensure minimal learning
    
    # alpha_neg remains the base rate (or could be modeled to increase)
    alpha_neg = alpha_base 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updating Stage 2 ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        if delta_stage2 > 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg * delta_stage2
            
        # --- Updating Stage 1 ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 > 0:
             q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
             q_stage1_mf[a1] += alpha_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```