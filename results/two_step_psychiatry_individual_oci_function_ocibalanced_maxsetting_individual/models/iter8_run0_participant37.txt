Here are the three proposed cognitive models.

### Model 1: Hybrid MB/MF with OCI-modulated Forgetting
This model extends the "best model" (which was pure Model-Free) to a **Hybrid Model-Based/Model-Free** architecture. It posits that high OCI scores drive a "pruning" or forgetting process of unchosen options, which reinforces habits by suppressing alternatives. This forgetting applies to both the Stage 1 and Stage 2 value estimates, affecting both the Goal-Directed (MB) and Habitual (MF) systems.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Forgetting of Unchosen Options.
    
    Hypothesis: High OCI leads to faster decay (pruning) of the value of unchosen actions,
    effectively isolating the chosen path and reinforcing repetitive behavior (compulsivity).
    This decay applies to both Stage 1 and Stage 2 values.
    
    decay = forget_base + forget_oci_slope * OCI
    
    Parameters:
    - learning_rate: [0,1] Value update rate.
    - beta: [0,10] Inverse temperature (softmax randomness).
    - w: [0,1] Weight of Model-Based system (0=MF, 1=MB).
    - lam: [0,1] Eligibility trace for Stage 1 MF update.
    - stickiness: [0,10] Basic choice perseveration bonus.
    - forget_base: [0,1] Baseline forgetting rate for unchosen options.
    - forget_oci_slope: [0,1] Sensitivity of forgetting rate to OCI score.
    """
    learning_rate, beta, w, lam, stickiness, forget_base, forget_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate based on OCI
    decay = forget_base + forget_oci_slope * oci_score
    if decay > 1.0: decay = 1.0
    if decay < 0.0: decay = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Skip missing trials
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
        
        # --- Stage 1 Policy ---
        # Model-Based Value: Expected value of Stage 2 states
        max_q2 = np.max(q_mf_2, axis=1) # V(S2)
        q_mb_1 = transition_matrix @ max_q2
        
        # Net Value: Weighted sum of MB and MF
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Apply Stickiness
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        # Stage 1 MF update (TD prediction error)
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        # Stage 2 MF update (Reward prediction error)
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        # Eligibility Trace: Update Stage 1 based on Stage 2 outcome
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        # --- OCI-Driven Forgetting ---
        # Decay unchosen Stage 1 option
        q_mf_1[1 - a1] *= (1.0 - decay)
        # Decay unchosen Stage 2 option (in the current state)
        q_mf_2[s_idx, 1 - a2] *= (1.0 - decay)
        
        prev_a1 = a1

    return log_loss
```

### Model 2: Hybrid MB/MF with OCI-modulated Stage 2 Temperature
This model hypothesizes that OCI specifically affects the decision noise (exploration/exploitation) at the second stage (the "harvesting" stage on the planet). High OCI might drive more deterministic, greedy behavior at the point of reward, while low OCI allows for more exploration.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Stage 2 Temperature.
    
    Hypothesis: OCI affects the exploration-exploitation balance specifically at the 
    reward-collection stage (Stage 2). High OCI leads to higher inverse temperature (beta),
    resulting in more deterministic (greedy) behavior at the final step.
    
    beta_2 = beta_2_base * (1 + beta_2_oci * OCI)
    
    Parameters:
    - learning_rate: [0,1]
    - beta_1: [0,10] Inverse temperature for Stage 1.
    - beta_2_base: [0,10] Base inverse temperature for Stage 2.
    - beta_2_oci: [0,5] Scaling factor for OCI impact on beta_2.
    - w: [0,1] MB weight.
    - lam: [0,1] Eligibility trace.
    - stickiness: [0,10] Choice stickiness.
    """
    learning_rate, beta_1, beta_2_base, beta_2_oci, w, lam, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate Stage 2 beta by OCI
    beta_2 = beta_2_base * (1.0 + beta_2_oci * oci_score)
    if beta_2 > 20.0: beta_2 = 20.0 # Cap to prevent overflow

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_a1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
        
        # --- Stage 1 ---
        max_q2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q2
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta_1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 (OCI Modulated Beta) ---
        exp_q2 = np.exp(beta_2 * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_a1 = a1
        
    return log_loss
```

### Model 3: Hybrid MB/MF with OCI-modulated Compulsive Persistence
This model introduces a specific "Lose-Stay" mechanism modulated by OCI. While standard stickiness promotes repeating any previous choice, "Compulsive Persistence" specifically adds extra stickiness when the previous trial was a **loss** (no reward), capturing the compulsive tendency to persist despite negative feedback.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Compulsive Persistence (Lose-Stay).
    
    Hypothesis: High OCI leads to 'compulsive' stickiness, where the participant 
    repeats the previous choice even after a lack of reward (loss), resisting the 
    learning signal that would normally encourage shifting.
    
    Stickiness Bonus = stick_base + stick_loss_oci * OCI * (1 - prev_reward)
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w: [0,1]
    - lam: [0,1]
    - stick_base: [0,10] Baseline stickiness (applied on all trials).
    - stick_loss_oci: [0,10] Additional stickiness applied only after a loss (0 reward), scaled by OCI.
    """
    learning_rate, beta, w, lam, stick_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_a1 = -1
    prev_reward = 0.0 # Default for first trial
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
        
        # --- Stage 1 ---
        max_q2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q2
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        # Apply Stickiness
        if prev_a1 != -1:
            eff_stick = stick_base
            # If previous trial was a loss (reward == 0), add OCI-driven compulsive stickiness
            if prev_reward == 0.0:
                eff_stick += stick_loss_oci * oci_score
            
            q_net_1[prev_a1] += eff_stick
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Updates ---
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_a1 = a1
        prev_reward = r
        
    return log_loss
```