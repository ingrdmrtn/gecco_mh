To explain the decision-making behavior of the participant with high OCI symptoms, I propose three cognitive models. The participant's data exhibits strong stickiness (repetition of choices), particularly for Spaceship A (Action 1) and specific aliens, and suggests an insensitivity to certain outcomes or transitions.

### Model 1: Rare Transition Discounting (Surprise Freeze)
**Hypothesis:** High OCI scores correlate with cognitive rigidity and an inability to incorporate "surprising" or noisy information into habit formation. In this model, OCI modulates the learning rate specifically for **rare transitions**. When a transition is rare (unexpected), the Model-Free system "freezes" or discounts the update, treating the event as an outlier rather than a learning opportunity. This prevents the Model-Free values from being corrupted by rare events (which is actually adaptive) but also reflects a refusal to adapt to structural noise.

### Model 2: Stage-Specific Precision (Dual Beta)
**Hypothesis:** OCI symptoms may differentially affect decision noise in the planning phase (Stage 1) versus the proximal action phase (Stage 2). This model proposes that OCI scales the inverse temperature ($\beta$) for the **second stage** relative to the first. A higher OCI might lead to hyper-rigid selection of aliens (Stage 2) even while Stage 1 remains exploratory, or vice versa. This captures the observed high consistency in alien selection (e.g., choosing Alien 1 repeatedly).

### Model 3: OCI-Mediated Fixed Bias
**Hypothesis:** Participants with high OCI may exhibit a compulsive baseline preference for a specific option (e.g., Spaceship A) that is independent of reward history. This model adds a fixed **bias term** to the net Q-values of Action 1, with the magnitude of this bias scaled by the OCI score. This explains the observed asymmetry in choice frequencies (preference for Action 1) as a symptom-related fixed compulsion rather than a learned value.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Rare Transition Discounting Model.
    
    Hypothesis: High OCI participants discount Model-Free updates when the transition 
    was 'Rare' (unexpected), treating such events as outliers.
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    lambda_decay: [0, 1] - Eligibility trace decay.
    stickiness: [0, 5] - Choice repetition bonus.
    rare_discount_oci: [0, 1] - Scaling factor for OCI to reduce learning on rare transitions.
                                Effective LR_rare = LR * (1 - rare_discount_oci * OCI).
    """
    learning_rate, beta, w, lambda_decay, stickiness, rare_discount_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the discount factor for rare transitions based on OCI
    # If OCI is high, rare_discount is high, so learning on rare trials is low.
    discount_factor = rare_discount_oci * oci_score
    # Clip to ensure learning rate doesn't go negative
    if discount_factor > 1.0:
        discount_factor = 1.0
        
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Stickiness bonus
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        prev_a1 = a1

        # --- Stage 2 Policy ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Determine if transition was Common or Rare
        # Common: (A1=0 -> S=0) or (A1=1 -> S=1). Rare: (A1=0 -> S=1) or (A1=1 -> S=0).
        # Note: In description, A(1)->X(1) is common. U(0)->Y(0) is common. (Assuming mapping 0->Y, 1->X based on data)
        # Actually, let's infer from transition matrix: T[0,0]=0.7 (Common), T[1,1]=0.7 (Common).
        # So if a1 == s_idx (assuming 0->0, 1->1 mapping roughly), it's Common.
        # Wait, usually indices align. Let's assume T[a, s] > 0.5 is common.
        is_common = transition_matrix[a1, s_idx] > 0.5
        
        current_lr = learning_rate
        if not is_common:
            current_lr = learning_rate * (1.0 - discount_factor)

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 MF with potentially discounted learning rate
        q_stage1_mf[a1] += current_lr * delta_stage1 + current_lr * lambda_decay * delta_stage2
        
        # Stage 2 update (usually standard LR, as it's direct reward learning)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage-Specific Precision Model (Dual Beta OCI).
    
    Hypothesis: OCI differentially affects the precision (randomness) of choices in the second stage 
    compared to the first. High OCI leads to hyper-rigidity (or noise) in selecting aliens.
    
    Parameters:
    learning_rate: [0, 1]
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceship).
    w: [0, 1]
    lambda_decay: [0, 1]
    stickiness: [0, 5]
    beta_2_oci_scale: [0, 5] - Scaling factor for Stage 2 beta based on OCI.
                               Beta_2 = Beta_1 * (1 + beta_2_oci_scale * OCI).
    """
    learning_rate, beta_1, w, lambda_decay, stickiness, beta_2_oci_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 Beta
    # If scale is positive, OCI makes Stage 2 more deterministic (rigid).
    beta_2 = beta_1 * (1.0 + beta_2_oci_scale * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Uses Beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        prev_a1 = a1

        # --- Stage 2 Policy (Uses Beta_2) ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta_2 * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Mediated Fixed Bias Model.
    
    Hypothesis: High OCI participants exhibit a fixed, compulsive bias towards a specific 
    spaceship (Action 1), independent of learned values. This bias is additive to the Q-values.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    lambda_decay: [0, 1]
    stickiness: [0, 5]
    bias_oci: [0, 5] - Magnitude of fixed bias towards Action 1, scaled by OCI.
                       Bias added to Q(Action 1).
    """
    learning_rate, beta, w, lambda_decay, stickiness, bias_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Bias
    # Bias is added to Action 1. 
    bias_val = bias_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        # Add fixed bias to Action 1 (index 1)
        bias_vec = np.array([0.0, bias_val])
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec + bias_vec
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        prev_a1 = a1

        # --- Stage 2 Policy ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```