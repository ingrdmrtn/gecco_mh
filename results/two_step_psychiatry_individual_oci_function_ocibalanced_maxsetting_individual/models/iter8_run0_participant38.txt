Here are three new cognitive models that introduce distinct mechanisms for how high OCI scores might influence decision-making in the two-step task, specifically focusing on perseveration, model-based deficits, and learning rate asymmetries.

### Model 1: Hybrid MB/MF with OCI-modulated Perseveration
This model hypothesizes that high OCI scores are associated with "stuckness" or perseveration (repeating the previous choice regardless of reward). Instead of just modifying the learning process or the balance between model-based and model-free control, this model adds a direct "stickiness" bonus to the choice probability that scales with the OCI score. High OCI participants will find it harder to switch away from their previous action at the first stage.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner with OCI-modulated Perseveration (Stickiness).
    
    This model posits that high OCI leads to repetitive behavior (perseveration).
    A 'stickiness' bonus is added to the value of the previously chosen action 
    at Stage 1. The magnitude of this bonus is determined by the OCI score.
    
    Parameters:
    learning_rate: [0,1] - Value update rate.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weight of Model-Based control (0=pure MF, 1=pure MB).
    stickiness_oci_slope: [0,5] - How much OCI increases the tendency to repeat the last choice.
    """
    learning_rate, beta, w, stickiness_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective stickiness weight based on OCI
    stickiness_weight = stickiness_oci_slope * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness
    last_action_1 = -1

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-value of the previous action
        q_hybrid_sticky = q_hybrid.copy()
        if last_action_1 != -1:
            q_hybrid_sticky[last_action_1] += stickiness_weight

        exp_q1 = np.exp(beta * q_hybrid_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning (TD updates)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid MB/MF with OCI-modulated Model-Based Weight (Sigmoid)
Previous models might have assumed a linear relationship between OCI and the model-based weight `w`. This model proposes a non-linear interaction. It suggests that as OCI increases, the ability to use the model-based system (planning) degrades, but potentially in a threshold-like manner. We use a sigmoid-like transformation to model how `w` might be suppressed by high OCI scores, controlled by a sensitivity parameter.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner where OCI suppresses Model-Based control via a scaling factor.
    
    This model tests the hypothesis that high OCI directly impairs model-based planning
    capacity. The mixing weight 'w' is dynamically reduced based on the OCI score.
    Instead of a linear shift, we use a multiplicative scaling where high OCI
    dampens the baseline 'w'.
    
    Parameters:
    learning_rate: [0,1] - Value update rate.
    beta: [0,10] - Inverse temperature.
    w_base: [0,1] - Baseline weight of Model-Based control for a low-OCI individual.
    oci_impairment: [0,1] - Strength of impairment. 1 = OCI fully eliminates MB control.
    """
    learning_rate, beta, w_base, oci_impairment = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w: higher OCI reduces w towards 0 based on impairment factor
    # If oci_impairment is 0, w is constant. If 1, high OCI significantly reduces w.
    w_effective = w_base * (1.0 - (oci_impairment * oci_score))
    # Clip to ensure valid range [0, 1]
    w_effective = np.clip(w_effective, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Use the OCI-modulated effective weight
        q_hybrid = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates with OCI-driven Negative Bias
This model investigates if OCI is linked to an asymmetry in how positive vs. negative prediction errors are processed. Specifically, it tests if high OCI scores lead to "over-learning" from negative outcomes (or lack of reward). It separates the learning rate into two components: a base rate and a negative-outcome amplifier that scales with OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF learner with OCI-amplified learning from negative prediction errors.
    
    This model posits that high OCI individuals are hypersensitive to 'bad news' 
    (negative prediction errors). While there is a base learning rate, 
    prediction errors < 0 are multiplied by an additional factor derived from OCI.
    
    Parameters:
    lr_base: [0,1] - Base learning rate for positive prediction errors.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Model-based weight.
    neg_lr_amp_oci: [0,5] - Multiplier for negative learning rate based on OCI.
    """
    lr_base, beta, w, neg_lr_amp_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate learning rate for negative PE: base * (1 + OCI_factor)
    # This implies high OCI learns faster/more drastically from disappointments.
    lr_neg_multiplier = 1.0 + (neg_lr_amp_oci * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Learning Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        # Determine LR based on sign of PE
        if delta_stage2 < 0:
            eff_lr_2 = lr_base * lr_neg_multiplier
        else:
            eff_lr_2 = lr_base
        # Clip effective LR to prevent instability (max 1.0)
        eff_lr_2 = min(eff_lr_2, 1.0)
        
        q_stage2_mf[s_idx, a2] += eff_lr_2 * delta_stage2
        
        # Learning Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 < 0:
            eff_lr_1 = lr_base * lr_neg_multiplier
        else:
            eff_lr_1 = lr_base
        eff_lr_1 = min(eff_lr_1, 1.0)
            
        q_stage1_mf[a1] += eff_lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```