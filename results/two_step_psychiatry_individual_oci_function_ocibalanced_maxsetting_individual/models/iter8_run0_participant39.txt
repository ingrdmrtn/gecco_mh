Here are three new cognitive models that explore different mechanisms for how OCI scores might influence decision-making in this two-step task.

### Model 1: OCI-Modulated Transition Learning (Model-Based Update)
This model tests the hypothesis that high OCI individuals might be more rigid in their internal model of the world (the transition probabilities), or conversely, they might over-update their transition model in response to rare events (hyper-sensitivity to prediction errors in the structure of the task). Here, we model the transition probability learning rate as a function of OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with dynamic transition learning where the learning rate for the 
    transition matrix is modulated by OCI.
    
    Hypothesis: High OCI participants may update their internal model of 
    state transitions (spaceship -> planet) differently. They might be 
    hyper-vigilant to structural changes (high transition learning rate) 
    or rigid (low transition learning rate). This model allows the data 
    to determine if OCI scales the transition learning rate up or down.

    Parameters:
    - learning_rate: [0, 1] Value learning rate for TD updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weight of Model-Based control.
    - lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    - lr_trans_oci_sens: [-1, 1] Modulation of transition learning rate by OCI. 
      (Implemented as a scaling factor: effective_lr = base + sens * oci, clipped 0-1).
    """
    learning_rate, beta, w, lr_trans_base, lr_trans_oci_sens = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Calculate effective transition learning rate, ensuring it stays in [0, 1]
    # We use a slightly different parameterization to allow negative impact
    lr_trans = lr_trans_base + (participant_oci * lr_trans_oci_sens)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)

    # Initialize transition matrix (rows: action 0/1, cols: state 0/1)
    # Start with the true probabilities or a uniform prior? 
    # Usually participants learn this. Let's start uniform/weakly informative 
    # or initialized to the common structure but allowed to drift.
    # Here we initialize to the "common" belief but allow updating.
    # Row 0: Action 0 (A) -> usually State 0 (X)
    # Row 1: Action 1 (U) -> usually State 1 (Y)
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]]) 

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T(s1, s2) * max(Q_MF(s2, a))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # 1. Update Transition Matrix (State Prediction Error)
        # One-hot encoding of the state we actually arrived at
        state_onehot = np.zeros(2)
        state_onehot[s_idx] = 1.0
        
        # Prediction error vector for the transition
        trans_pe = state_onehot - trans_probs[a1]
        trans_probs[a1] += lr_trans * trans_pe
        # Normalize to ensure probabilities sum to 1
        trans_probs[a1] /= np.sum(trans_probs[a1])

        # 2. Update Stage 2 Values (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # 3. Update Stage 1 MF Values (TD(0))
        # Note: Standard TD uses value of next state.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven "Relief" Learning (Asymmetric Learning Rates)
This model posits that OCI symptoms, often driven by anxiety relief or harm avoidance, lead to asymmetric learning from positive versus negative outcomes. Specifically, it tests if higher OCI leads to faster learning from "missed" rewards (or punishments, effectively 0 reward here) compared to obtained rewards, conceptualized here as a sensitivity to negative prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model with asymmetric learning rates for positive and negative prediction errors,
    where the negative learning rate is modulated by OCI.
    
    Hypothesis: Individuals with high OCI may be more sensitive to negative feedback 
    (lack of reward) as a signal of failure or "something wrong", leading to 
    faster updating when outcomes are worse than expected.

    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    - alpha_neg_oci_sens: [0, 1] Additional sensitivity to negative RPEs scaled by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_sens, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Effective negative learning rate
    alpha_neg = alpha_neg_base + (participant_oci * alpha_neg_oci_sens)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0) # Ensure bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 2 Update
        delta2 = r - q_stage2_mf[s_idx, a2]
        lr2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr2 * delta2

        # Stage 1 MF Update
        # Using the updated Q2 value for the TD target is a variation (SARSA vs Q-learning)
        # Here we stick to the standard logic: Q(s1,a1) updates towards Q(s2,a2)
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1_mf[a1] += lr1 * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Choice Stickiness (Perseveration)
This model investigates if OCI is associated with behavioral repetition (compulsivity) independent of value learning. It adds a "stickiness" parameter that biases the subject to repeat the previous choice, where the strength of this stickiness is a function of the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model incorporating choice stickiness (perseveration) in Stage 1, 
    modulated by OCI.
    
    Hypothesis: High OCI is associated with compulsive repetition of actions. 
    This model adds a 'stickiness' bonus to the previously chosen spaceship, 
    where the magnitude of this bonus scales with the OCI score.

    Parameters:
    - learning_rate: [0, 1] Value learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stick_base: [0, 5] Base stickiness parameter (bonus added to logits).
    - stick_oci_sens: [0, 5] Additional stickiness per unit of OCI.
    """
    learning_rate, beta, w, stick_base, stick_oci_sens = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Effective stickiness
    stickiness = stick_base + (participant_oci * stick_oci_sens)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous choice for stickiness (initialize to -1 or random)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Calculate logits (values * beta)
        logits_1 = beta * q_net
        
        # Add stickiness bonus to the previously selected action
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        # Update last action
        last_action_1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        delta2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta2
        
        delta1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```