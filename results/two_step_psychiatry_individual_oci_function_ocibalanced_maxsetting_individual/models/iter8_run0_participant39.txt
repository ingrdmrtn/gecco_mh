Here are three new cognitive models for the two-step decision task, incorporating OCI scores to explain individual variability in decision-making.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Subjective Transition Belief.
    
    Hypothesis:
    OCI scores influence the participant's internal model of the spaceship-planet transition probabilities.
    While the true common transition probability is 0.7, participants with varying OCI levels might 
    overestimate (perceive as deterministic) or underestimate (perceive as random) this reliability.
    This distortion affects the Goal-Directed (Model-Based) value calculation.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax (global).
    w: [0, 1] - Weighting between Model-Based and Model-Free values (0=MF, 1=MB).
    stickiness: [-5, 5] - Choice perseveration bonus.
    p_trans_base: [0, 1] - Base subjective probability that Spaceship A goes to Planet X (and U to Y).
    p_trans_oci: [-1, 1] - Modulation of subjective probability by OCI.
    """
    learning_rate, beta, w, stickiness, p_trans_base, p_trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    p_trans = p_trans_base + p_trans_oci * oci_score
    p_trans = np.clip(p_trans, 0.0, 1.0)
    
    # Subjective transition matrix: [[P(X|A), P(Y|A)], [P(X|U), P(Y|U)]]
    # Assuming symmetry: A->X is p, A->Y is 1-p; U->Y is p, U->X is 1-p
    transition_matrix = np.array([[p_trans, 1.0 - p_trans], [1.0 - p_trans, p_trans]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Model-Based Value: T * Max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        if a2 == -1: 
             p_choice_2[trial] = 1.0
             continue

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 1 MF Update (using lambda=1 equivalent for efficiency)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Beta.
    
    Hypothesis:
    OCI influences the exploration/exploitation balance specifically in the second stage (concrete alien choice),
    distinct from the abstract planning in Stage 1. High OCI might lead to more rigid (high beta) or 
    more anxious/noisy (low beta) behavior when facing the immediate reward source.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    w: [0, 1] - MB/MF balance.
    stickiness: [-5, 5] - Choice perseveration.
    beta_stage1: [0, 20] - Inverse temperature for Stage 1 choice.
    beta_stage2_base: [0, 20] - Base inverse temperature for Stage 2.
    beta_stage2_oci: [-10, 10] - Modulation of Stage 2 beta by OCI.
    """
    learning_rate, w, stickiness, beta_stage1, beta_stage2_base, beta_stage2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    beta_stage2 = beta_stage2_base + beta_stage2_oci * oci_score
    beta_stage2 = np.clip(beta_stage2, 0.0, 30.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        if a2 == -1:
             p_choice_2[trial] = 1.0
             continue

        # Stage 2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Model-Based Beta (Independent MB/MF Strengths).
    
    Hypothesis:
    Instead of a trade-off parameter 'w' (where increasing MB decreases MF), the Model-Based and Model-Free 
    systems have independent strengths (betas). OCI specifically modulates the strength (inverse temperature) 
    of the Model-Based system. This allows OCI to degrade MB control without necessarily increasing MF control 
    proportionally, or to affect the total decision confidence.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    stickiness: [-5, 5] - Choice perseveration.
    beta_stage2: [0, 20] - Inverse temperature for Stage 2.
    beta_mf: [0, 20] - Strength of Model-Free influence in Stage 1.
    beta_mb_base: [0, 20] - Base strength of Model-Based influence in Stage 1.
    beta_mb_oci: [-10, 10] - Modulation of MB strength by OCI.
    """
    learning_rate, stickiness, beta_stage2, beta_mf, beta_mb_base, beta_mb_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    beta_mb = beta_mb_base + beta_mb_oci * oci_score
    beta_mb = np.clip(beta_mb, 0.0, 30.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Independent contribution of MB and MF values to logits
        # Logits = beta_mb * Q_mb + beta_mf * Q_mf + stickiness
        logits = beta_mb * q_stage1_mb + beta_mf * q_stage1_mf
        
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        # Softmax
        # Stabilize for numerical safety
        logits = logits - np.max(logits)
        exp_logits = np.exp(logits)
        probs_1 = exp_logits / np.sum(exp_logits)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        if a2 == -1:
             p_choice_2[trial] = 1.0
             continue

        # Stage 2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```