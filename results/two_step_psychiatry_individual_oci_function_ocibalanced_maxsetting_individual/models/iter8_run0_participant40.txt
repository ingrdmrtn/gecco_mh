Here are 3 new cognitive models based on the participant's data and OCI score.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model tests the hypothesis that higher OCI scores lead to a deficit in model-based control (goal-directed behavior), pushing the agent towards more model-free (habitual) behavior. The mixing parameter `w` determines the balance between model-based and model-free values, and this balance is directly modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w'
    is modulated by the OCI score.
    
    Hypothesis: High OCI reduces model-based control (lower w), leading to more
    habitual (model-free) choices.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w_max: [0,1] - Maximum possible model-based weight (for OCI=0).
    oci_penalty: [0,1] - How much OCI reduces the model-based weight.
    """
    learning_rate, beta, w_max, oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI
    # If OCI is high, w decreases (more model-free).
    # w is clamped between 0 and 1.
    w = w_max - (oci_penalty * oci_score)
    if w < 0: w = 0
    if w > 1: w = 1

    # Transition matrix (fixed for this task structure)
    # A -> X (0.7), U -> Y (0.7) usually
    # Indices: [A, U] -> [X, Y]
    # transitions[0,0] = P(X|A) = 0.7
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage 1 Policy: Hybrid
        # Model-Based Value calculation
        # V_MB(s1) = sum(P(s2|s1, a1) * max_a2(Q(s2, a2)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value for each state (X, Y)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[int(action_1[t])]
        
        # Stage 2 Policy: Pure Model-Free
        s_idx = int(state[t])
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[int(action_2[t])]
        
        # Learning
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Update Stage 2 Q-values (SARSA/Q-learning equivalent here since terminal)
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * pe_2
        
        # Update Stage 1 MF Q-values (TD(0))
        # Note: In standard hybrid models, MF update often uses the stage 2 value
        # But simpler MF often just updates from reward or stage 2 value. 
        # Here we use the standard TD(1) style update often used in 2-step tasks:
        # Update Q_MF(s1,a1) based on reward directly (ignoring stage 2 state value for MF)
        # or via TD-error from stage 2. Let's use direct reward update (TD(1)-like) 
        # which is common in "pure MF" components of these models.
        pe_1 = r - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free with OCI-based Learning Rate Asymmetry
This model explores the idea that high OCI might be associated with an over-sensitivity to negative outcomes (or lack of reward) compared to positive ones. It implements separate learning rates for positive and negative prediction errors, where the balance is shifted by the OCI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates modulated by OCI.
    
    Hypothesis: High OCI participants might learn differently from failures (0 reward)
    vs successes (1 reward). This model scales the learning rate for negative
    prediction errors based on OCI.
    
    Bounds:
    alpha_base: [0,1] - Base learning rate.
    beta: [0,10] - Inverse temperature.
    oci_neg_bias: [0,5] - Multiplier for learning from negative PEs based on OCI.
    """
    alpha_base, beta, oci_neg_bias = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate specific learning rates
    # Alpha for positive PEs is just base
    alpha_pos = alpha_base
    
    # Alpha for negative PEs is modified by OCI
    # If OCI is high, they might be more sensitive to errors (learn faster from them)
    # or more rigid. Let's assume hypersensitivity to negative feedback.
    # We clip to [0, 1] to ensure stability.
    alpha_neg = alpha_base * (1 + oci_neg_bias * oci_score)
    if alpha_neg > 1.0: alpha_neg = 1.0

    for t in range(n_trials):
        # Stage 1 Choice
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[int(action_1[t])]
        
        # Stage 2 Choice
        s_idx = int(state[t])
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[int(action_2[t])]
        
        # Learning
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if pe_2 >= 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * pe_2
        
        # Stage 1 Update (TD(1) / Direct Reward)
        pe_1 = r - q_stage1[a1]
        lr_1 = alpha_pos if pe_1 >= 0 else alpha_neg
        q_stage1[a1] += lr_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based with OCI-driven Exploration Suppression
This model posits that high OCI leads to rigid, low-entropy behavior (reduced exploration). Instead of affecting the learning or the model-based weight directly, OCI here increases the inverse temperature (`beta`), making choices more deterministic and less exploratory as OCI increases.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner where OCI modulates the inverse temperature (beta).
    
    Hypothesis: High OCI is associated with rigidity and reduced exploration.
    This is modeled by increasing beta (making the softmax steeper) as OCI increases.
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10] - Baseline inverse temperature.
    beta_oci_slope: [0,10] - How much OCI increases beta (rigidity).
    """
    learning_rate, beta_base, beta_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Beta increases with OCI -> Higher OCI = More deterministic (less random)
    beta = beta_base + (beta_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage2 = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage 1: Pure Model-Based
        # Calculate expected values of Stage 2 states
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        logits_1 = beta * q_stage1_mb
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[int(action_1[t])]
        
        # Stage 2 Choice
        s_idx = int(state[t])
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[int(action_2[t])]
        
        # Learning (Stage 2 values only needed for MB calculation)
        a2 = int(action_2[t])
        r = reward[t]
        
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```