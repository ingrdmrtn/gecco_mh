Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Neophobia (Exploration Penalty)
This model hypothesizes that high OCI participants exhibit "neophobia"â€”a fear of options that have not been chosen recently. Instead of standard stickiness (which rewards the *last* choice), this model applies a cumulative penalty to options the longer they go unchosen. This penalty is modulated by the OCI score, potentially explaining the participant's extreme perseveration (avoidance of the alternative spaceship).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Neophobia Model.
    
    Hypothesis: High OCI participants experience increasing aversion to options 
    that have not been chosen recently (neophobia). This creates a 'lock-in' effect 
    where the participant sticks to a safe/known option to avoid the anxiety of 
    the 'unknown' alternative.
    
    The value of an option is penalized by the time elapsed since it was last chosen:
    Q_net(a) = Q_RL(a) - rho * log(time_since_last_choice(a))
    
    The penalty weight (rho) is a function of OCI.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stickiness: [0, 5] Standard 1-step choice stickiness.
    - lambda_eligibility: [0, 1] Eligibility trace for Stage 1 update.
    - neo_base: [0, 5] Base neophobia penalty weight.
    - neo_oci_slope: [0, 5] Additional neophobia weight per unit of OCI.
    """
    learning_rate, beta, w, stickiness, lambda_eligibility, neo_base, neo_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Neophobia weight increases with OCI
    rho = neo_base + neo_oci_slope * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Track when each option was last chosen (initialize to -1)
    last_chosen_trial = np.array([-1.0, -1.0])
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Neophobia Penalty
        # Time since last choice. Add 1 to avoid log(0). 
        # If never chosen, trial number serves as proxy.
        time_since = trial - last_chosen_trial
        # Penalty grows logarithmically with time
        neophobia_penalty = rho * np.log(time_since + 2.0) 
        
        logits_1 = beta * (q_net - neophobia_penalty)
        
        # Standard stickiness
        if prev_choice_1 != -1:
            logits_1[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update history
        prev_choice_1 = a1
        last_chosen_trial[a1] = trial

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]

        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + lambda_eligibility * learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Risk Aversion
This model assumes that high OCI participants are highly sensitive to outcome uncertainty. The model tracks not just the expected value ($Q$) but also the variability/risk ($V$) of the model-free values. High OCI leads to a stronger penalty for options with high prediction error volatility, encouraging the participant to stick with options where the outcome is predictable (even if the reward is low).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Risk Aversion Model.
    
    Hypothesis: High OCI participants are risk-averse and avoid options with high 
    uncertainty or volatility. The model tracks a 'Risk' value (V) for each Stage 1 
    option, which accumulates the magnitude of prediction errors.
    
    Q_net(a) = w * Q_MB + (1-w) * (Q_MF(a) - omega * Risk(a))
    
    The risk penalty weight (omega) scales with OCI. This explains sticking to 
    'known' bad options over 'unknown' potentially good ones.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q and Risk.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stickiness: [0, 5] Choice stickiness.
    - lambda_eligibility: [0, 1] Eligibility trace.
    - risk_base: [0, 5] Base risk aversion weight.
    - risk_oci_slope: [0, 5] Additional risk aversion per unit of OCI.
    """
    learning_rate, beta, w, stickiness, lambda_eligibility, risk_base, risk_oci_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Risk aversion weight depends on OCI
    omega = risk_base + risk_oci_slope * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Track Risk (volatility of prediction errors) for Stage 1 options
    risk_stage1 = np.zeros(2) # Initialize risk as 0 (or low)
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Apply risk penalty to MF values
        q_mf_risk_adjusted = q_stage1_mf - omega * risk_stage1
        
        q_net = w * q_stage1_mb + (1 - w) * q_mf_risk_adjusted
        
        logits_1 = beta * q_net
        if prev_choice_1 != -1:
            logits_1[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_choice_1 = a1

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Calculate Stage 1 RPE (TD error)
        # Note: Standard TD error is used to update Value, magnitude used for Risk
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Update Risk: Moving average of absolute prediction error
        risk_stage1[a1] += learning_rate * (np.abs(delta_stage1) - risk_stage1[a1])

        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + lambda_eligibility * learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Pessimistic Evaluation
This model suggests that high OCI participants have a pessimistic bias in their model-based planning. When evaluating the future value of a state (Planet X or Y), instead of assuming they will choose the best alien (Max Q), they weight the worst-case scenario (Min Q) more heavily. This "Pessimism" reduces the incentive to explore the alternative spaceship if its outcomes are perceived as potentially negative, reinforcing the status quo.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Pessimistic Evaluation Model.
    
    Hypothesis: High OCI participants exhibit pessimistic model-based planning. 
    Standard MB planning assumes the agent will choose the best option at Stage 2 
    (V(s) = max Q(s, a)). This model introduces an 'optimism' parameter modulated 
    by OCI. High OCI reduces optimism, causing the agent to consider the minimum 
    possible reward (worst case) at the next stage.
    
    V_stage2(s) = optimism * max(Q(s,:)) + (1 - optimism) * min(Q(s,:))
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stickiness: [0, 5] Choice stickiness.
    - lambda_eligibility: [0, 1] Eligibility trace.
    - opt_base: [0, 1] Base optimism level (1.0 = standard Max operator).
    - opt_oci_damp: [0, 1] Reduction in optimism scaled by OCI.
    """
    learning_rate, beta, w, stickiness, lambda_eligibility, opt_base, opt_oci_damp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Optimism decreases with OCI. Clamped to [0, 1].
    optimism = opt_base * (1.0 - opt_oci_damp * current_oci)
    optimism = max(0.0, min(1.0, optimism))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # Pessimistic Valuation of Stage 2 States
        max_q = np.max(q_stage2_mf, axis=1)
        min_q = np.min(q_stage2_mf, axis=1)
        
        # Weighted average of Best Case and Worst Case
        val_stage2 = optimism * max_q + (1.0 - optimism) * min_q
        
        q_stage1_mb = transition_matrix @ val_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        if prev_choice_1 != -1:
            logits_1[prev_choice_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_choice_1 = a1

        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        logits_2 = beta * q_stage2_mf[state_idx, :]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]

        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + lambda_eligibility * learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```