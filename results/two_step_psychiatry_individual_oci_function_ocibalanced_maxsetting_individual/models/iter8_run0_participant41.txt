Here are 3 new cognitive models based on the participant data and the two-step task structure.

### Cognitive Model 1: Model-Based/Model-Free Hybrid with OCI-Modulated Mixing Weight
This model hypothesizes that the balance between Model-Based (planning) and Model-Free (habitual) control is disrupted by obsessive-compulsive symptoms. Specifically, high OCI scores might lead to a rigid reliance on habits (Model-Free) at the expense of flexible planning (Model-Based), or conversely, an over-thinking planning mode. Here, we model the mixing weight `w` as a function of OCI, allowing the data to determine if high OCI pushes the participant towards pure Model-Free (w -> 0) or pure Model-Based (w -> 1).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based / Model-Free learner where the mixing weight 'w'
    is modulated by the OCI score.
    
    Hypothesis: OCI symptoms alter the balance between goal-directed (MB) and
    habitual (MF) control. A high OCI score shifts 'w' away from a baseline.

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Baseline mixing weight (0=MF, 1=MB) for low OCI.
    oci_shift: [0, 1] Magnitude of shift in 'w' induced by OCI score.
    """
    learning_rate, beta, w_base, oci_shift = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w based on OCI
    # We constrain w to be between 0 and 1.
    # If oci_shift is positive, high OCI increases w (more MB).
    # However, we allow the optimizer to effectively subtract if it finds w_base high.
    # To make it flexible: w = w_base + (oci_shift * oci_score) if we assume shift is positive,
    # or we can use a bounded sigmoid.
    # Simple linear shift clipped to [0,1]:
    # We assume the parameter 'oci_shift' represents a reduction in MB control 
    # (habitual tendency in OCD), so w = w_base - (oci_shift * oci_score).
    # But to be agnostic, let's try: w = w_base * (1 - oci_score * oci_shift)
    # This implies high OCI reduces w towards 0 (Model Free).
    
    w = w_base * (1.0 - (oci_score * oci_shift))
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (also used for MB calculation)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        # Max Q value for each state in stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Bellman equation using transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        
        logits_2 = beta * q_stage2_mf[s_curr]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        # Stage 2 update (TD learning)
        delta_stage2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += learning_rate * delta_stage2
        
        # Stage 1 MF update (TD learning)
        # Using the value of the state actually reached (SARSA-like for Stage 1 MF)
        # or Q(s2, a2) as the target.
        delta_stage1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Asymmetric Learning Rates Modulated by OCI
This model focuses on how participants update their value estimates after positive versus negative outcomes. High OCI might be associated with a "negative bias" or hypersensitivity to failure (punishment), leading to faster learning from zero rewards (losses/omissions) compared to positive rewards. The OCI score scales the learning rate specifically for negative prediction errors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for positive and negative
    prediction errors. The negative learning rate is scaled by the OCI score.
    
    Hypothesis: High OCI participants are hypersensitive to negative outcomes 
    (or lack of reward), learning faster from '0' reward trials.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    oci_sens: [0, 5] Sensitivity factor scaling the negative learning rate by OCI.
    """
    lr_pos, lr_neg_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # lr_neg increases with OCI score
    lr_neg = lr_neg_base * (1.0 + oci_score * oci_sens)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        
        logits_2 = beta * q_stage2[s_curr]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        # Stage 2 Prediction Error
        pe_2 = r - q_stage2[s_curr, a2]
        
        # Apply asymmetric learning rate
        if pe_2 >= 0:
            q_stage2[s_curr, a2] += lr_pos * pe_2
        else:
            q_stage2[s_curr, a2] += lr_neg * pe_2
            
        # Stage 1 Prediction Error
        # Using the updated Q-value of stage 2 as target
        pe_1 = q_stage2[s_curr, a2] - q_stage1[a1]
        
        if pe_1 >= 0:
            q_stage1[a1] += lr_pos * pe_1
        else:
            q_stage1[a1] += lr_neg * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Decay-Based Perseveration with OCI Scaling
Instead of a simple "stickiness" to the immediately previous choice, this model implements a "decaying trace" of past choices. High OCI participants might exhibit stronger habits that form over a longer history of choices, rather than just repeating the last one. The decay rate of this choice trace is modulated by the OCI score, testing if high OCI leads to more persistent choice traces (slower decay).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with a decaying choice trace (perseveration) added to Q-values.
    The decay rate of the choice trace is modulated by OCI.
    
    Hypothesis: High OCI leads to 'stickier' habits where past choices influence
    current decisions for longer (slower decay of choice trace).
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature.
    persev_weight: [0, 5] Weight of the perseveration trace in the decision.
    decay_base: [0, 1] Base decay rate for the choice trace (0=instant decay, 1=no decay).
    oci_stiff: [0, 1] Factor reducing decay based on OCI (stiffness).
    """
    learning_rate, beta, persev_weight, decay_base, oci_stiff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate
    # Higher OCI -> Higher effective decay parameter (closer to 1.0) -> Slower forgetting of choice
    # decay_eff = decay_base + (1 - decay_base) * oci_score * oci_stiff
    decay_eff = decay_base + (oci_score * oci_stiff)
    decay_eff = np.clip(decay_eff, 0.0, 0.99) # Cap at 0.99 to prevent explosion
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Choice trace for Stage 1 options
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add choice trace to Q-values for decision making
        net_val_1 = q_stage1 + (persev_weight * choice_trace)
        
        logits_1 = beta * net_val_1
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update choice trace
        # Decay all traces
        choice_trace *= decay_eff
        # Increment trace for chosen action
        choice_trace[a1] += 1.0
        
        # --- Stage 2 Choice ---
        s_curr = int(state[trial])
        
        logits_2 = beta * q_stage2[s_curr]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2[s_curr, a2]
        q_stage2[s_curr, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2[s_curr, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```