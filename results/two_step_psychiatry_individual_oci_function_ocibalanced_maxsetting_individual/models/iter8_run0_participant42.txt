def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Lose-Stay Bias.
    Hypothesis: High OCI participants exhibit a pathological "Lose-Stay" behavior. 
    While standard reinforcement learning predicts shifting after a loss (Lose-Shift), 
    obsessive-compulsive traits may lead to rigidity or doubt-induced repetition 
    specifically after negative outcomes.
    
    This model adds a "stay" bias to the previously chosen spaceship, but this bias 
    is significantly amplified by the OCI score specifically when the previous outcome 
    was a loss (0 coins).
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for value updates.
    - beta: [0, 10] Inverse temperature (randomness).
    - w: [0, 1] Weight for Model-Based values (0=MF, 1=MB).
    - stick_base: [0, 5] General stickiness independent of outcome/OCI.
    - lose_stay_oci: [0, 5] Additional stickiness applied ONLY after a loss, scaled by OCI.
    """
    learning_rate, beta, w, stick_base, lose_stay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (planets), 2 actions (aliens)

    prev_act1 = -1
    prev_reward = 0.0

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if prev_act1 != -1:
            stick_bias = np.zeros(2)
            # Base stickiness applied always
            stick_bias[prev_act1] += stick_base
            
            # OCI-Modulated Lose-Stay: Extra stickiness if previous reward was 0
            if prev_reward == 0.0:
                stick_bias[prev_act1] += lose_stay_oci * oci_score
            
            q_net += stick_bias

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

        prev_act1 = act1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Habit Trace.
    Hypothesis: High OCI participants form stronger, longer-lasting habits.
    Instead of simple 1-back stickiness, this model maintains an exponential 
    moving average 'trace' of past choices. The influence of this habit trace 
    on the current decision is scaled by the OCI score.
    
    High OCI leads to a stronger weight on the history of choices, explaining 
    long streaks of behavior (perseveration) that are resistant to immediate value changes.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB weight.
    - trace_decay: [0, 1] Decay rate of the choice trace (0 = instant forget, 1 = perfect memory).
    - trace_weight_oci: [0, 5] How strongly the trace biases the decision, scaled by OCI.
    """
    learning_rate, beta, w, trace_decay, trace_weight_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for the two spaceships
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Habit Trace Bias modulated by OCI
        bias = trace_weight_oci * oci_score * choice_trace
        q_net += bias

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Update Choice Trace
        # Decay existing trace
        choice_trace *= trace_decay
        # Reinforce current choice
        choice_trace[act1] += 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Asymmetric Learning (Failure Suppression).
    Hypothesis: High OCI participants exhibit "learning rigidity" specifically 
    in response to negative outcomes (0 coins). They may fail to downgrade the value 
    of an action after a loss, leading to persistence (streaks) despite non-reward.
    
    This model uses separate learning rates for positive (Reward=1) and negative (Reward=0) 
    outcomes. The negative learning rate is suppressed by the OCI score.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for rewarded trials (1 coin).
    - lr_neg_base: [0, 1] Base learning rate for unrewarded trials (0 coins).
    - neg_damp_oci: [0, 1] OCI-dependent dampening of the negative learning rate.
      (Effective lr_neg = lr_neg_base * (1 - neg_damp_oci * OCI)).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB weight.
    """
    lr_pos, lr_neg_base, neg_damp_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # --- Determine Learning Rate ---
        if r == 1.0:
            current_lr = lr_pos
        else:
            # Dampen learning from failure based on OCI
            # If OCI is high and damp is high, lr_neg approaches 0 (ignore failure)
            damp_factor = np.clip(1.0 - (neg_damp_oci * oci_score), 0.0, 1.0)
            current_lr = lr_neg_base * damp_factor

        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += current_lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss