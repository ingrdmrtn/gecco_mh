Here are the three proposed cognitive models.

### Model 1: OCI-Modulated Counterfactual Regret
This model hypothesizes that high OCI participants engage in counterfactual thinking ("regret") after a decision. If they receive no reward (0 coins), they simulate that the *unchosen* option would have yielded a reward (1 coin), and update their value estimates accordingly. This "grass is greener" bias is scaled by their OCI score, leading to potentially excessive switching or instability in value estimation for the second stage.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Counterfactual Regret Model.
    
    Hypothesis: High OCI participants engage in counterfactual updating. 
    When an outcome is observed, they also update the unchosen option in Stage 2. 
    Specifically, if Reward=0, they assume the unchosen option would have had Reward=1, 
    driven by a 'regret' parameter scaled by OCI.
    
    Parameters:
    lr: [0, 1] Learning rate for chosen options.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stickiness: [0, 5] Choice stickiness for Stage 1.
    oci_regret: [0, 1] Weight of counterfactual updating for the unchosen option, scaled by OCI.
    """
    lr, beta, w, stickiness, oci_regret = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Scale regret by OCI
    regret_weight = participant_oci * oci_regret

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # 1. Update Chosen Stage 2 Option
        delta_s2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * delta_s2
        
        # 2. Counterfactual Update of Unchosen Stage 2 Option
        # If I got 0, assume other was 1. If I got 1, assume other was 0.
        # This simulates "If I had chosen the other, I would have gotten the opposite."
        # The strength of this belief is controlled by oci_regret.
        unchosen_a2 = 1 - a2
        counterfactual_reward = 1.0 - r
        delta_cf = counterfactual_reward - q_stage2_mf[s_curr, unchosen_a2]
        q_stage2_mf[s_curr, unchosen_a2] += lr * regret_weight * delta_cf
        
        # 3. Update Stage 1 MF
        # Using TD(1)-like update directly from reward as seen in successful models for this data
        q_stage1_mf[a1] += lr * (r - q_stage1_mf[a1])
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage 2 Precision
This model proposes that OCI symptoms manifest differently during the planning phase (Stage 1) versus the execution phase (Stage 2). High OCI is modeled as inducing rigidity or hyper-focus in the immediate choice of the alien (Stage 2), resulting in a higher inverse temperature (`beta`) for that stage specifically. This effectively makes the participant more deterministic and less exploratory once they have reached a planet.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Precision Model.
    
    Hypothesis: High OCI participants exhibit differential decision noise between stages.
    While Stage 1 involves complex planning (MB/MF trade-off), Stage 2 is a direct value choice.
    This model posits that OCI increases the 'beta' (inverse temperature) specifically for 
    Stage 2, reflecting compulsive adherence to perceived value or reduced exploration 
    once a state is reached.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature (used for Stage 1).
    w: [0, 1] Model-Based weight.
    stickiness: [0, 5] Choice stickiness for Stage 1.
    oci_s2_scale: [0, 5] Scaling factor for Stage 2 beta based on OCI.
                     beta_stage2 = beta_base * (1 + oci * oci_s2_scale)
    """
    lr, beta_base, w, stickiness, oci_s2_scale = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate separate betas
    beta_s1 = beta_base
    beta_s2 = beta_base * (1.0 + participant_oci * oci_s2_scale)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Decision (uses beta_s1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta_s1 * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision (uses beta_s2) ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta_s2 * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2
        delta_s2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * delta_s2
        
        # Update Stage 1 MF
        q_stage1_mf[a1] += lr * (r - q_stage1_mf[a1])
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Win-Stay Stickiness
This model investigates the "stickiness" phenomenon more deeply. While standard stickiness is constant, this model proposes that high OCI participants are particularly prone to "Win-Stay" behavior. If the previous action resulted in a reward, the urge to repeat that action is amplified by their OCI score, reflecting a compulsive desire to maintain safety/success and an aversion to switching away from a winning option.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Win-Stay Stickiness Model.
    
    Hypothesis: OCI is associated with repetitive behaviors and safety seeking. 
    This model posits that stickiness is not constant but outcome-dependent. 
    Specifically, high OCI participants experience a 'super-stickiness' 
    after a rewarded trial (Win-Stay), making them extremely unlikely to switch 
    after a success.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stickiness_base: [0, 5] Baseline choice stickiness (applied always).
    oci_win_stick: [0, 5] Additional stickiness applied ONLY if previous reward was 1, scaled by OCI.
    """
    lr, beta, w, stickiness_base, oci_win_stick = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 0
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if prev_a1 != -1:
            # Base stickiness
            total_stick = stickiness_base
            # Add OCI-modulated Win-Stay bonus
            if prev_reward == 1:
                total_stick += participant_oci * oci_win_stick
                
            q_net_1[prev_a1] += total_stick
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2
        delta_s2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * delta_s2
        
        # Update Stage 1 MF
        q_stage1_mf[a1] += lr * (r - q_stage1_mf[a1])
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```