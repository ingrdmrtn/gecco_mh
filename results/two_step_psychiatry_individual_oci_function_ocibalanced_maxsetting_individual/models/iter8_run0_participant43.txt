Here are the three proposed cognitive models.

```python
import numpy as np

def cognitive_model1_oci_subjective_loss(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Mediated Subjective Loss.
    
    Hypothesis: Individuals with high OCI scores exhibit increased sensitivity to failure (perfectionism/loss aversion). 
    In this model, obtaining 0 coins is not merely a lack of reward, but is perceived as a subjective loss 
    proportional to the OCI score. This negative reinforcement drives avoidance behavior more strongly than 
    a neutral outcome would.
    
    Effective Reward = Reward - (loss_theta * OCI * (1 - Reward))
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weight for Model-Based control (1=MB, 0=MF).
    - loss_theta: [0, 5] Scaling factor for subjective loss when reward is 0.
    """
    learning_rate, beta, w, loss_theta = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Transition matrix: A(0)->X(0) 0.7, A(0)->Y(1) 0.3; U(1)->Y(1) 0.7, U(1)->X(0) 0.3
    # Organized as [P(State0|Action0), P(State1|Action0)], [P(State0|Action1), P(State1|Action1)]
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Alien
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Hybrid) ---
        # Model-Based Value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Choice 1
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy (Model-Free) ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates with Subjective Loss ---
        # Calculate Effective Reward
        r_observed = reward[trial]
        # If r=1, term is 0. If r=0, term is -loss_theta * OCI.
        r_effective = r_observed - (loss_theta * oci_score * (1 - r_observed))
        
        # Update Stage 1 MF
        # TD(1)-like update: Stage 1 learns from Stage 2 prediction error? 
        # Or simple SARSA/Q-learning. Here we use standard hybrid update structure.
        # Update S1 based on S2 value (TD error at transition)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF using Effective Reward
        delta_stage2 = r_effective - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2_oci_asymmetric_learning(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Learner with OCI-Modulated Asymmetric Learning Rates.
    
    Hypothesis: High OCI contributes to rigidity and a failure to "unlearn" established habits when faced with 
    negative outcomes. This model implements asymmetric learning rates where the learning rate for 
    negative prediction errors (disappointment) is suppressed by the OCI score.
    
    LR_negative = LR_base * (1 - suppression * OCI)
    LR_positive = LR_base
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - neg_lr_suppress: [0, 1] Factor by which OCI suppresses learning from negative errors.
    """
    learning_rate, beta, w, neg_lr_suppress = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates with Asymmetry ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Determine effective learning rate
        if delta_stage1 < 0:
            eff_lr_1 = learning_rate * (1.0 - neg_lr_suppress * oci_score)
            eff_lr_1 = max(0.0, eff_lr_1) # Ensure non-negative
        else:
            eff_lr_1 = learning_rate
        q_stage1_mf[action_1[trial]] += eff_lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 < 0:
            eff_lr_2 = learning_rate * (1.0 - neg_lr_suppress * oci_score)
            eff_lr_2 = max(0.0, eff_lr_2)
        else:
            eff_lr_2 = learning_rate
        q_stage2_mf[state_idx, action_2[trial]] += eff_lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3_oci_compulsive_accumulation(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Learner with OCI-Driven Compulsive Accumulation.
    
    Hypothesis: Obsessive-compulsive behavior is characterized by repetitive actions where the urge to repeat 
    grows with each repetition (compulsion). This model includes a 'streak counter' for the chosen spaceship. 
    The probability of repeating the action is boosted by a term proportional to the streak length and OCI score.
    This explains long runs of identical choices (perseveration) seen in the data.
    
    Logit_Bonus = compulsion_rate * OCI * current_streak_length
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - compulsion_rate: [0, 2] Scaling factor for the streak-based bonus.
    """
    learning_rate, beta, w, compulsion_rate = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Track the streak of choosing the same spaceship
    current_streak = 0
    last_choice = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        
        # Apply Compulsive Bonus if we have a previous choice
        if last_choice != -1:
            # Add bonus to the last chosen action
            logits_1[last_choice] += compulsion_rate * oci_score * current_streak
        
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update streak
        if action_1[trial] == last_choice:
            current_streak += 1
        else:
            current_streak = 1 # Reset to 1 (current choice starts new streak)
        last_choice = action_1[trial]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```