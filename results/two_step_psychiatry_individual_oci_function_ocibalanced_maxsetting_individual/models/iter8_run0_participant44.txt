Here are the 3 new cognitive models based on the participant data and OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Unrewarded Stickiness.
    
    Hypothesis: High OCI leads to compulsive repetition of actions specifically 
    when they were NOT rewarded on the previous trial. 
    Standard stickiness applies always, but an extra "compulsive" stickiness 
    is added if the previous outcome was a loss.

    Bounds:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF mixing weight.
    stick_base: [0, 5] Base stickiness for any choice.
    oci_punish_stick: [0, 5] Additional stickiness scaled by OCI after unrewarded trials.
    """
    lr, beta, w, stick_base, oci_punish_stick = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Track the previous choice to apply stickiness
    last_choice = -1
    last_reward = 1 # Initialize as if rewarded so no punishment stickiness on trial 1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Stickiness
        stickiness_bonus = np.zeros(2)
        if last_choice != -1:
            # Base stickiness
            stickiness_bonus[last_choice] += stick_base
            # OCI-driven punishment stickiness
            if last_reward == 0:
                stickiness_bonus[last_choice] += oci_punish_stick * oci_score

        q_net_sticky = q_net + stickiness_bonus
        
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # TD(0) updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Update history
        last_choice = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Post-Loss Rigidity (Beta).
    
    Hypothesis: High OCI participants exhibit "freezing" or rigidity after a loss.
    This is modeled as an increase in the inverse temperature (beta) specifically 
    following unrewarded trials, making choices more deterministic (less exploratory).

    Bounds:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature.
    w: [0, 1] MB/MF mixing weight.
    stick: [0, 5] General stickiness parameter.
    oci_loss_beta_scale: [0, 5] Scaling factor for Beta increase after loss, modulated by OCI.
    """
    lr, beta_base, w, stick, oci_loss_beta_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_choice = -1
    last_reward = 1

    for trial in range(n_trials):
        # --- Determine Beta ---
        current_beta = beta_base
        if last_reward == 0:
            # Increase beta (reduce exploration) if previous trial was a loss
            current_beta = beta_base * (1.0 + oci_score * oci_loss_beta_scale)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Stickiness
        if last_choice != -1:
            q_net[last_choice] += stick
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_choice = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Differential Learning Rates.
    
    Hypothesis: High OCI participants may over-learn or under-learn the immediate 
    outcomes (Stage 2) compared to the abstract transitions (Stage 1).
    This model splits the learning rate: `lr` is used for Stage 1, while 
    Stage 2 uses `lr_stage2` which is scaled by OCI.

    Bounds:
    lr: [0, 1] Base learning rate (applied to Stage 1).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF mixing weight.
    stick: [0, 5] General stickiness.
    oci_s2_lr_scale: [0, 5] Scaling factor for Stage 2 learning rate based on OCI.
                            (lr_s2 = lr * (1 + oci * scale)).
    """
    lr, beta, w, stick, oci_s2_lr_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    last_choice = -1

    # Calculate the specific learning rate for Stage 2
    # We clip it to [0, 1] to ensure stability
    lr_s2_raw = lr * (1.0 + oci_score * oci_s2_lr_scale)
    lr_s2 = np.clip(lr_s2_raw, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_choice != -1:
            q_net[last_choice] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Update Stage 2 with OCI-modulated rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2
        
        # Update Stage 1 with base rate
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_choice = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```