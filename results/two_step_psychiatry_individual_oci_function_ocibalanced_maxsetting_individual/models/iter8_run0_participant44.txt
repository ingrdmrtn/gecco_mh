Here are the three cognitive models as requested.

### Model 1: Compulsive Relief Model
This model hypothesizes that for high-OCI individuals, repeating an action generates an intrinsic "relief" reward, regardless of the external outcome. This internal reward reinforces the chosen action, driving the Q-values higher and causing perseveration (stickiness) even in the absence of external gold coins.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Compulsive Relief Model.
    
    Hypothesis: High OCI participants derive intrinsic reinforcement ("relief") from 
    repeating the same action (compulsion). This internal reward is added to the 
    external reward signal, inflating the value of the habitual action and causing 
    perseveration despite lack of external success.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - lambd: [0, 1] Eligibility trace decay.
    - relief_amp: [0, 2] Magnitude of the intrinsic relief reward, scaled by OCI.
    """
    learning_rate, beta, w, lambd, relief_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    prev_a1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Relief Calculation ---
        # If the participant repeats the Stage 1 choice, they get a bonus "relief" reward.
        relief_bonus = 0.0
        if prev_a1 != -1 and a1 == prev_a1:
            relief_bonus = relief_amp * oci_score
        
        effective_reward = r + relief_bonus
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace carries the effective reward (including relief) back to Stage 1
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Conflict-Driven Habit Model
This model posits that high OCI leads to anxiety when the Model-Based (goal-directed) and Model-Free (habitual) systems conflict. When the values predicted by these two systems diverge (high conflict), the participant reduces reliance on the Model-Based system (lowers `w`), falling back to safe, habitual Model-Free behaviors.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Conflict-Driven Habit Model.
    
    Hypothesis: High OCI participants experience "arbitration anxiety" when the 
    Model-Based and Model-Free systems disagree (conflict). In high-conflict states, 
    they suppress the Model-Based controller (reducing w) and revert to Model-Free 
    habits to resolve the uncertainty.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_base: [0, 1] Baseline weight for Model-Based system.
    - lambd: [0, 1]
    - conflict_sens: [0, 10] Sensitivity to MB-MF conflict. Higher values cause stronger fallback to MF.
    """
    learning_rate, beta, w_base, lambd, conflict_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Calculate Conflict: Euclidean distance between the value vectors of the two systems
        conflict = np.linalg.norm(q_stage1_mb - q_stage1_mf)
        
        # Dynamic Arbitration: Reduce w based on conflict and OCI
        w_eff = w_base / (1.0 + conflict_sens * oci_score * conflict)
        
        q_net = w_eff * q_stage1_mb + (1 - w_eff) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Compulsive Entrenchment Model
This model suggests that high OCI leads to behavioral rigidity that deepens with repetition. As a participant continuously chooses the same option (forms a streak), their decision noise decreases (effective `beta` increases), making them "entrenched" in their current course of action and highly resistant to exploration.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Compulsive Entrenchment Model.
    
    Hypothesis: High OCI leads to progressively increasing rigidity (entrenchment) 
    as an action is repeated. The inverse temperature (beta) scales with the length 
    of the current choice streak, making the participant deterministic and 
    unlikely to switch away from the repeated option.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1]
    - lambd: [0, 1]
    - entrench_gain: [0, 5] Rate at which beta increases per streak count, scaled by OCI.
    """
    learning_rate, beta_base, w, lambd, entrench_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    streak_count = 0
    prev_a1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate effective beta based on current streak length
        # Cap the streak multiplier to prevent numerical overflow in exp()
        eff_streak = min(streak_count, 20)
        beta_eff = beta_base * (1.0 + entrench_gain * oci_score * eff_streak)
        # Cap beta_eff to a safe upper bound
        beta_eff = min(beta_eff, 50.0)
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # Update streak counter
        if a1 == prev_a1:
            streak_count += 1
        else:
            streak_count = 1 # Reset streak to 1 for the new action
        prev_a1 = a1

        # --- Stage 2 Policy ---
        # Use base beta for stage 2 (asking aliens) as compulsion is primarily on spaceship choice
        qs2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_base * qs2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```