Here are three new cognitive models exploring different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might modulate learning and decision-making in a two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight
This model assumes agents use a mixture of Model-Based (MB) and Model-Free (MF) strategies. The core hypothesis here is that OCI symptoms might correlate with a rigidity or over-reliance on habitual (MF) responding, or conversely, an obsessive tracking of transition structures (MB). Given the low OCI score (0.3) of the participant, this model tests if the weighting parameter `w` (mixing MB and MF) is a linear function of the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight 'w' is modulated by OCI.
    
    The agent calculates both Model-Based (transition-matrix dependent) and 
    Model-Free (TD learning) values for the first stage. The final Q-value is a weighted sum:
    Q_net = w * Q_MB + (1-w) * Q_MF.
    
    The weight 'w' is determined by a base level and an OCI-dependent slope.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for MF updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_base: [0, 1] Base weight for Model-Based control (at OCI=0).
    w_oci_slope: [-1, 1] How OCI score changes the MB weight.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w and clip to [0, 1]
    w = w_base + w_oci_slope * oci_score
    w = np.clip(w, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # MF values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for aliens (Stage 2)

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial]) # 0 or 1
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_stage1_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_stage1_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 MF update (TD(0))
        # Note: In pure hybrid models, Stage 1 MF is often updated by Q(s', a') 
        # but here we use the standard TD error form often used in 2-step tasks:
        # delta = Q_stage2(chosen) - Q_stage1(chosen)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Reward Sensitivity (Outcome Valuation)
This model posits that OCI traits might influence how strongly an agent reacts to rewards versus non-rewards. Specifically, high OCI might be associated with heightened error signals or reward sensitivity. Instead of modifying the learning rate or strategy weight, we modify the effective reward magnitude. The "subjective reward" is scaled by OCI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner where the subjective valuation of reward is modulated by OCI.
    
    The hypothesis is that individuals with different OCI scores might perceive 
    outcomes differently (e.g., hyper-valuation of success or failure).
    Effective Reward = Reward * (reward_sensitivity_base + sensitivity_oci_slope * OCI)
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    sens_base: [0, 5] Base reward sensitivity.
    sens_oci_slope: [-2, 2] OCI modulation of sensitivity.
    """
    learning_rate, beta, sens_base, sens_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective sensitivity
    sensitivity = sens_base + sens_oci_slope * oci_score
    # Ensure sensitivity is non-negative to avoid inverting reward meaning
    sensitivity = max(0.0, sensitivity)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # We use a pure MB framework for Stage 1 choice to isolate valuation effects,
    # but store Q-values for stage 2 to learn reward probabilities.
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r_raw = reward[trial]
        
        # Apply subjective scaling
        r_effective = r_raw * sensitivity

        # --- Stage 1 Policy (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Update ---
        # Update stage 2 values based on effective reward
        delta_stage2 = r_effective - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Separate Learning Rates for Positive and Negative Prediction Errors (OCI-Modulated)
This model investigates if OCI scores relate to an asymmetry in learning from positive vs. negative outcomes. For instance, obsessive-compulsive traits might relate to a "negativity bias" where learning from disappointment (0 coins) is faster or slower than learning from success. Here, we fix the positive learning rate but allow the negative learning rate to vary as a function of OCI.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with asymmetric learning rates for positive and negative prediction errors.
    The learning rate for negative prediction errors (alpha_neg) is modulated by OCI.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    alpha_neg_oci_slope: [-1, 1] How OCI modifies the negative learning rate.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Determine alpha_neg based on OCI
    alpha_neg = alpha_neg_base + alpha_neg_oci_slope * oci_score
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Model-Based) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Update ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Apply asymmetric learning rates
        if delta_stage2 >= 0:
            eff_lr = alpha_pos
        else:
            eff_lr = alpha_neg
            
        q_stage2_mf[s_idx, a2] += eff_lr * delta_stage2

    eps = 1e-10
    valid_trials = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```