Here are three new cognitive models based on the provided template and participant data.

### Model 1: OCI-Modulated Stage-Specific Learning Rates
This model hypothesizes that OCI symptoms differentially affect the learning rate for abstract choices (Stage 1, selecting a spaceship) versus concrete outcomes (Stage 2, receiving coins from aliens). High OCI might be associated with faster habit formation (higher Stage 1 learning rate) or rigidity, independent of how they value the immediate reward in Stage 2.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage-Specific Learning Rates.
    
    Hypothesis: OCI score modulates the learning rate for the first stage (abstract choice)
    differently from the second stage (concrete reward). This separates the speed of 
    habit formation (Stage 1) from direct reward valuation (Stage 2).
    
    Parameters:
    - lr_s1_base: [0, 1] Base learning rate for Stage 1.
    - lr_s1_oci: [-1, 1] Effect of OCI on Stage 1 learning rate.
    - lr_s2: [0, 1] Learning rate for Stage 2 (fixed).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based/Model-free weight.
    - stickiness: [0, 5] Choice perseveration bonus.
    """
    lr_s1_base, lr_s1_oci, lr_s2, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate Stage 1 Learning Rate based on OCI
    lr_s1 = lr_s1_base + lr_s1_oci * oci_val
    lr_s1 = np.clip(lr_s1, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            prev_a1 = -1
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        stick_bonus = np.zeros(2)
        if prev_a1 != -1:
            stick_bonus[prev_a1] = stickiness

        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Stage 1 Update (TD(0)) using OCI-modulated rate
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Stage 2 Update using fixed rate
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2
        
        # Eligibility trace update for Stage 1 (TD(1)) using Stage 1 rate
        q_stage1_mf[a1] += lr_s1 * delta_stage2
        
        prev_a1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Active Forgetting
This model incorporates a decay parameter for unchosen options. The hypothesis is that OCI score relates to the persistence of value representations. A high OCI might be associated with low forgetting (rumination/obsessiveness) or high forgetting (anxiety/interference), causing unchosen Q-values to decay toward zero at a rate dependent on the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Active Forgetting (Decay).
    
    Hypothesis: Unchosen action values decay over time (forgetting). The rate of this 
    decay is modulated by OCI, representing either obsessive retention (low decay) 
    or anxious interference (high decay) of unvisited states/actions.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Choice perseveration.
    - decay_base: [0, 1] Base decay rate for unchosen options.
    - decay_oci: [-1, 1] Modulation of decay rate by OCI.
    """
    lr, beta, w, stickiness, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate decay rate
    decay = decay_base + decay_oci * oci_val
    decay = np.clip(decay, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            prev_a1 = -1
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_a1 != -1:
            stick_bonus[prev_a1] = stickiness
            
        exp_q1 = np.exp(beta * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * delta_stage2
        
        # --- Forgetting / Decay ---
        # Decay unchosen Stage 1 value
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay)
        
        # Decay unchosen Stage 2 value (in the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - decay)
        
        prev_a1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Second Stage Precision
This model proposes that OCI affects the exploration/exploitation balance (inverse temperature $\beta$) specifically in the second stage. While Stage 1 involves planning and uncertainty about transitions, Stage 2 is a direct choice. High OCI might correlate with a higher need for control or determinism in the direct reward stage (higher $\beta_2$), or conversely, higher anxiety/noise (lower $\beta_2$).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Second Stage Precision (Beta).
    
    Hypothesis: OCI modulates the inverse temperature (beta) specifically for the 
    second stage choice. While Stage 1 beta is fixed, Stage 2 beta varies with OCI, 
    reflecting how obsessive symptoms impact decision noise in the immediate 
    proximity of reward/punishment.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1.
    - beta_2_base: [0, 10] Base inverse temperature for Stage 2.
    - beta_2_oci: [-5, 5] Modulation of Stage 2 beta by OCI.
    - w: [0, 1] MB/MF weight.
    - stickiness: [0, 5] Choice perseveration.
    """
    lr, beta_1, beta_2_base, beta_2_oci, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]

    # Calculate Stage 2 Beta
    beta_2 = beta_2_base + beta_2_oci * oci_val
    beta_2 = np.clip(beta_2, 0.0, 10.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1:
            prev_a1 = -1
            continue
            
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Policy Stage 1 (uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_bonus = np.zeros(2)
        if prev_a1 != -1:
            stick_bonus[prev_a1] = stickiness
            
        exp_q1 = np.exp(beta_1 * (q_net + stick_bonus))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy Stage 2 (uses OCI-modulated beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * delta_stage2
        
        prev_a1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```