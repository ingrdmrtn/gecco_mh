Here are the three proposed cognitive models.

### Model 1: Subjective Transition Belief Model
This model hypothesizes that OCI symptoms correlate with distorted beliefs about the causal structure of the environment. While the true transition probability from Spaceship A to Planet X is 0.7, the participant's Model-Based system may operate with a subjective probability (e.g., believing transitions are more deterministic or more random) that is modulated by their OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Subjective Transition Belief Model.
    
    Hypothesis: The Model-Based system uses a subjective transition probability 
    that differs from the true probability (0.7). This subjective belief is 
    modulated by OCI. High OCI may lead to beliefs in higher determinism 
    (or rigidity) in the transition structure.
    
    Parameters:
    - lr: [0, 1] Learning rate for Model-Free values.
    - beta: [0, 10] Inverse temperature (softmax precision).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stickiness: [0, 5] Choice perseveration on Stage 1.
    - p_subj_base: [0, 1] Base subjective probability of common transition (mapped to 0.5-1.0).
    - p_subj_oci_mod: [0, 10] OCI modulation of subjective probability.
        Slope = p_subj_oci_mod - 5.0.
    """
    lr, beta, w, stickiness, p_subj_base, p_subj_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Map p_subj parameters to a probability in [0.5, 1.0]
    # We use a sigmoid-like transformation or simple clamping centered on base
    # Transformation: base + slope * oci, passed through sigmoid, scaled to [0.5, 1.0]
    slope = p_subj_oci_mod - 5.0
    
    # Calculate subjective probability of common transition
    # We treat p_subj_base as the logit intercept
    logit = (p_subj_base - 0.5) * 10.0 + (slope * current_oci) 
    p_common = 0.5 + 0.5 * (1.0 / (1.0 + np.exp(-logit)))
    
    # Subjective Transition Matrix
    # [[P(X|A), P(Y|A)], [P(X|U), P(Y|U)]]
    # A->X is common, U->Y is common
    subj_trans_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 planets, 2 aliens
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Skip invalid/missing trials
        if a1 not in [0, 1] or s_idx not in [0, 1] or a2 not in [0, 1]:
            continue

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subj_trans_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        # Stage 1 Update (MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 Update (MF)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Eligibility trace (lambda=1 implied by standard update of S1 from reward)
        q_stage1_mf[a1] += lr * delta_stage2
        
        last_action_1 = a1

    eps = 1e-10
    # Filter out 0 probabilities from skipped trials
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 2: Outcome-Directed Stickiness Model
This model introduces a stickiness bias towards the *outcome* (Planet) experienced on the previous trial, rather than just the *action* (Spaceship). OCI modulates the strength of this "state perseveration." High OCI participants may feel a compulsion to return to the same state (Planet X or Y), prompting them to choose the spaceship most likely to lead there, effectively blending a habit-like repetition with a model-based understanding of transitions.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Outcome-Directed Stickiness Model.
    
    Hypothesis: Participants exhibit stickiness not just to the motor action (Spaceship),
    but to the outcome state (Planet). If they visited Planet X, they are biased 
    to choose the spaceship that leads to Planet X on the next trial.
    OCI modulates this 'State Perseveration'.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick_action: [0, 5] Standard motor stickiness (repeat same spaceship).
    - stick_outcome_base: [0, 5] Base stickiness to the previous planet.
    - stick_outcome_oci_mod: [0, 10] OCI modulation of outcome stickiness.
        Slope = stick_outcome_oci_mod - 5.0.
    """
    lr, beta, w, stick_action, stick_outcome_base, stick_outcome_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate outcome stickiness magnitude
    slope = stick_outcome_oci_mod - 5.0
    stick_outcome = stick_outcome_base + (slope * current_oci)
    # Ensure stickiness is non-negative
    stick_outcome = np.maximum(stick_outcome, 0.0)
    
    # True transition matrix for calculating outcome stickiness bonus
    # [[P(X|A), P(Y|A)], [P(X|U), P(Y|U)]]
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_state = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 not in [0, 1] or s_idx not in [0, 1] or a2 not in [0, 1]:
            continue

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Action Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stick_action
            
        # Outcome Stickiness
        # If we visited a planet last time, add bonus to actions leading there
        if last_state != -1:
            # Bonus for action 'a' is P(last_state | a) * stick_outcome
            outcome_bonus = trans_matrix[:, last_state] * stick_outcome
            q_net += outcome_bonus
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * delta_stage2
        
        last_action_1 = a1
        last_state = s_idx

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```

### Model 3: Reward-Dependent Arbitration Model
This model proposes that the balance between Model-Based (goal-directed) and Model-Free (habitual) control is dynamic and depends on the previous trial's outcome. Specifically, OCI modulates the reliance on Model-Based control after a **Loss**. High OCI participants may default to reduced Model-Based control (lower $w$) under stress or failure (Loss), exhibiting increased habit reliance compared to when they Win.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Reward-Dependent Arbitration Model.
    
    Hypothesis: The mixing weight 'w' (MB vs MF) changes based on the previous outcome.
    OCI modulates the 'w' used specifically after a Loss (Reward = 0).
    High OCI might lead to a fallback to MF (habit) strategies after negative feedback.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - stickiness: [0, 5] Choice perseveration.
    - w_win: [0, 1] Mixing weight after a Win (Reward = 1).
    - w_loss_base: [0, 1] Base mixing weight after a Loss (Reward = 0).
    - w_loss_oci_mod: [0, 10] OCI modulation of w_loss.
        Slope = w_loss_oci_mod - 5.0.
    """
    lr, beta, stickiness, w_win, w_loss_base, w_loss_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate w_loss modulated by OCI
    slope = w_loss_oci_mod - 5.0
    # Apply logistic transform to keep w_loss in [0, 1]
    # We treat w_loss_base as a logit-like seed
    logit_loss = (w_loss_base - 0.5) * 10.0 + (slope * current_oci)
    w_loss = 1.0 / (1.0 + np.exp(-logit_loss))
    
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0 # Default initialization
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 not in [0, 1] or s_idx not in [0, 1] or a2 not in [0, 1]:
            continue

        # Determine current w based on last reward
        # Assuming r is 0 or 1. If r was -1 (miss), we treat as loss or keep previous?
        # We'll treat < 0.5 as Loss, >= 0.5 as Win.
        if last_reward >= 0.5:
            current_w = w_win
        else:
            current_w = w_loss
            
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_matrix @ max_q_stage2
        q_net = current_w * q_stage1_mb + (1 - current_w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    valid_trials = (p_choice_1 > 0) & (p_choice_2 > 0)
    log_loss = -(np.sum(np.log(p_choice_1[valid_trials] + eps)) + 
                 np.sum(np.log(p_choice_2[valid_trials] + eps)))
    return log_loss
```