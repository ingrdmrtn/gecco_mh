Here are three new cognitive models exploring different mechanisms for how OCI might influence decision-making in the two-step task.

### Model 1: OCI-Modulated Model-Based Weighting (Hybrid Learner)
This model hypothesizes that OCI symptoms affect the balance between Model-Based (planning) and Model-Free (habitual) control. Specifically, higher OCI scores might lead to a reliance on habitual, repetitive behaviors (low `w`) or, conversely, hyper-planning (high `w`). This model allows the weighting parameter `w` to be a linear function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where the balance between Model-Based and Model-Free control (w)
    is modulated by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Baseline weight for model-based values.
    w_oci_mod: [-1, 1] - How strongly OCI shifts w from the baseline.
    
    The effective w is clamped between 0 and 1.
    """
    learning_rate, beta, w_base, w_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective w based on OCI
    w = w_base + (w_oci_mod * current_oci)
    if w > 1.0: w = 1.0
    if w < 0.0: w = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value Calculation
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue 

        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]

            # --- Updates ---
            # Update Stage 2 MF values (TD(0))
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            
            # Update Stage 1 MF values (TD(1) / SARSA-like backup)
            # Note: In standard hybrid models, stage 1 MF is often updated by the stage 2 value
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Stickiness (Perseveration)
This model tests the hypothesis that OCI is related to behavioral rigidity or "stickiness" (perseveration). Instead of affecting learning rates or planning weights, OCI here modulates an additional "stickiness" parameter added to the Q-values of the previously chosen action. High OCI might lead to higher stickiness (repeating the same choice regardless of reward).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with choice stickiness (perseveration) modulated by OCI.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    stickiness_base: [-5, 5] - Baseline tendency to repeat the previous choice.
    stickiness_oci_mod: [-5, 5] - How OCI alters the stickiness.
    
    """
    learning_rate, beta, stickiness_base, stickiness_oci_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective stickiness
    stickiness = stickiness_base + (stickiness_oci_mod * current_oci)

    # Pure Model-Free setup for simplicity to isolate stickiness effect, 
    # but structure follows the two-step logic.
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy with Stickiness ---
        q_stage1_effective = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_effective[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_stage1_effective)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
            last_action_1 = a1
        else:
            p_choice_1[trial] = 1.0
            last_action_1 = -1
            continue

        state_idx = int(state[trial])

        # --- Stage 2 Policy (Standard Softmax) ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]

            # --- Updates ---
            delta_stage2 = r - q_stage2_mf[state_idx, a2]
            q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
            
            delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry (Pos/Neg Bias)
This model investigates if OCI is associated with a bias in learning from positive versus negative prediction errors. For example, individuals with high OCI might be more sensitive to punishments (negative prediction errors) or less sensitive to rewards. The OCI score modulates the learning rate specifically for negative prediction errors (`alpha_neg`), creating an asymmetry relative to `alpha_pos`.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for positive and negative 
    prediction errors. OCI modulates the negative learning rate.
    
    Parameters:
    lr_pos: [0, 1] - Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] - Baseline learning rate for negative prediction errors.
    lr_neg_oci_mod: [-1, 1] - How OCI modulates sensitivity to negative outcomes.
    beta: [0, 10]
    
    The effective lr_neg is clamped between 0 and 1.
    """
    lr_pos, lr_neg_base, lr_neg_oci_mod, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    lr_neg = lr_neg_base + (lr_neg_oci_mod * current_oci)
    if lr_neg > 1.0: lr_neg = 1.0
    if lr_neg < 0.0: lr_neg = 0.0

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            a1 = int(action_1[trial])
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            continue

        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            a2 = int(action_2[trial])
            p_choice_2[trial] = probs_2[a2]
            
            r = reward[trial]

            # --- Updates with Asymmetric Learning Rates ---
            
            # Stage 2 Update
            pe2 = r - q_stage2_mf[state_idx, a2]
            alpha2 = lr_pos if pe2 >= 0 else lr_neg
            q_stage2_mf[state_idx, a2] += alpha2 * pe2
            
            # Stage 1 Update
            # Using the updated Stage 2 value as the target (SARSA-like)
            pe1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
            alpha1 = lr_pos if pe1 >= 0 else lr_neg
            q_stage1_mf[a1] += alpha1 * pe1
            
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```