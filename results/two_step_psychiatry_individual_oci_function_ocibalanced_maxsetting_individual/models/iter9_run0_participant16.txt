Here are three new cognitive models for the two-step task.

### Cognitive Model 1: Hybrid RL-WSLS Model
This model hypothesizes that the participant arbitrates between a computationally expensive Reinforcement Learning (RL) strategy (mixture of Model-Based and Model-Free) and a low-cost, heuristic "Win-Stay, Lose-Shift" (WSLS) strategy. The reliance on the rigid WSLS heuristic is modulated by the OCI score, testing if higher compulsivity leads to greater reliance on simple, repetitive rules over probabilistic value estimation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL-WSLS Model.
    
    The agent computes choice probabilities as a weighted mixture of a standard 
    RL policy (MB/MF blend) and a heuristic Win-Stay, Lose-Shift (WSLS) policy.
    The weight of the WSLS strategy is modulated by OCI severity.
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for the RL component.
    - w: [0, 1] Weighting between Model-Based and Model-Free values (1 = pure MB).
    - stickiness: [0, 5] Perseveration bonus for the RL component.
    - wsls_low: [0, 1] Weight of the WSLS heuristic for Low OCI.
    - wsls_high: [0, 1] Weight of the WSLS heuristic for High OCI.
    """
    lr, beta, w, stickiness, wsls_low, wsls_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate WSLS weight based on OCI
    w_wsls = wsls_low * (1 - current_oci) + wsls_high * current_oci
    w_rl = 1.0 - w_wsls
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action_1 = -1
    last_reward = -1
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Skip invalid trials
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # --- Stage 1 Policy (RL Component) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_rl = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to RL values
        if last_action_1 != -1:
            q_net_rl[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_rl)
        probs_rl = exp_q1 / np.sum(exp_q1)
        
        # --- Stage 1 Policy (WSLS Component) ---
        probs_wsls = np.array([0.5, 0.5]) # Default uniform if no history
        if last_action_1 != -1:
            if last_reward == 1:
                # Win-Stay
                probs_wsls = np.zeros(2)
                probs_wsls[last_action_1] = 1.0
            else:
                # Lose-Shift
                probs_wsls = np.zeros(2)
                probs_wsls[1 - last_action_1] = 1.0
        
        # --- Mixture Policy ---
        probs_final = w_rl * probs_rl + w_wsls * probs_wsls
        
        # Safety clip
        probs_final = np.maximum(probs_final, eps)
        probs_final = probs_final / np.sum(probs_final)
        
        log_likelihood += np.log(probs_final[a1])

        # --- Stage 2 Policy (Standard Softmax) ---
        # WSLS is assumed to apply primarily to the high-level spaceship choice.
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Stage 2 Update (MF)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 1 Update (MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_action_1 = a1
        last_reward = r

    return -log_likelihood
```

### Cognitive Model 2: Choice Kernel Memory Model
This model replaces the simple "one-trial back" stickiness with a decaying "Choice Kernel" that integrates a history of past actions. This captures stronger, more persistent habits (perseveration) or avoidance behaviors. The strength of this habit influence is modulated by OCI, testing if high OCI leads to more entrenched motor habits (stronger influence of the choice kernel).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Choice Kernel Memory Model.
    
    Instead of simple stickiness (repeating the very last action), this model 
    maintains a 'Choice Kernel' that tracks the frequency/recency of past choices 
    with a decay rate. OCI modulates the weight of this kernel in the decision process.
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for value-based decisions.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - ck_decay: [0, 1] Decay rate of the choice kernel (0 = instant forgetting, 1 = perfect memory).
    - ck_weight_low: [0, 5] Weight of the choice kernel for Low OCI.
    - ck_weight_high: [0, 5] Weight of the choice kernel for High OCI.
    """
    lr, beta, w, ck_decay, ck_weight_low, ck_weight_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]

    ck_weight = ck_weight_low * (1 - current_oci) + ck_weight_high * current_oci

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    choice_kernel = np.zeros(2) # Tracks history of stage 1 choices
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB/MF values
        q_val = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Choice Kernel influence (Habit)
        # Note: beta applies to values; ck_weight applies to the kernel directly 
        # (or can be inside beta, but usually distinct in literature).
        # Here we add it to the net utility before softmax.
        logits = beta * q_val + ck_weight * choice_kernel
        
        exp_logits = np.exp(logits - np.max(logits)) # Subtract max for stability
        probs_1 = exp_logits / np.sum(exp_logits)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Value Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Choice Kernel Update
        # Decay existing trace
        choice_kernel *= ck_decay
        # Reinforce chosen action
        choice_kernel[a1] += 1.0

    return -log_likelihood
```

### Cognitive Model 3: Volatility-Adjusted Learning Rate (Pearce-Hall)
This model implements a dynamic learning rate inspired by the Pearce-Hall theory. The learning rate for the next trial is determined by the "surprise" (absolute prediction error) of the current trial. OCI modulates the sensitivity to this surprise (`gamma`). High OCI participants may be hypersensitive to prediction errors (high `gamma`), causing their learning rate to fluctuate wildly and preventing stable convergence, or conversely, they may be insensitive to surprise (low `gamma`).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Volatility-Adjusted Learning Rate Model (Pearce-Hall style).
    
    The learning rate is not static but evolves based on the magnitude of 
    prediction errors (surprise). 
    lr(t+1) = gamma * |delta(t)| + (1 - gamma) * lr(t)
    
    OCI modulates 'gamma', the sensitivity of the learning rate to recent surprise.
    
    Parameters:
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - stickiness: [0, 5] Perseveration bonus.
    - lr_init: [0, 1] Initial learning rate.
    - gamma_low: [0, 1] Sensitivity to surprise for Low OCI.
    - gamma_high: [0, 1] Sensitivity to surprise for High OCI.
    """
    beta, w, stickiness, lr_init, gamma_low, gamma_high = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Interpolate gamma based on OCI
    gamma = gamma_low * (1 - current_oci) + gamma_high * current_oci
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    current_lr = lr_init
    last_action_1 = -1
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Updates ---
        # Stage 2 Prediction Error
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 2 Q-values using dynamic LR
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2

        # Stage 1 Prediction Error
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 1 Q-values using dynamic LR
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # --- Update Learning Rate for Next Trial ---
        # Based on the absolute surprise in Stage 1 (which reflects the total path surprise)
        # We use delta_stage1 as the primary signal of value-surprise.
        abs_delta = np.abs(delta_stage1)
        current_lr = gamma * abs_delta + (1 - gamma) * current_lr
        
        # Clamp LR to stay in [0, 1] just in case
        current_lr = min(max(current_lr, 0.0), 1.0)
        
        last_action_1 = a1

    return -log_likelihood
```