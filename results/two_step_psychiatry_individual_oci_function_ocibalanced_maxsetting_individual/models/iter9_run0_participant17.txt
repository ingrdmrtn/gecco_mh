Here are the 3 proposed cognitive models.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Surprise-Modulated Exploration (Beta) by OCI.
    
    Hypothesis: Rare transitions (unexpected outcomes) induce a change in exploration/exploitation 
    state (beta) for the subsequent trial. OCI modulates the magnitude and direction of this 
    reaction to surprise. High OCI might lead to rigid behavior (increased beta) or 
    disengagement (decreased beta) specifically after rare events.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-value updates.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0) values.
    stickiness: [-5, 5] - Choice perseveration bonus.
    beta_base: [0, 10] - Baseline inverse temperature.
    rare_mod_base: [-1, 5] - Base multiplier adjustment to beta after a rare transition.
                             (0 means no change, >0 means increase, <0 means decrease).
    rare_mod_oci: [-2, 2] - Modulation of the rare transition effect by OCI score.
    """
    learning_rate, w, stickiness, beta_base, rare_mod_base, rare_mod_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_transition_rare = False 

    for trial in range(n_trials):
        # Calculate current beta based on previous transition type
        beta_current = beta_base
        if last_transition_rare:
            mod_factor = rare_mod_base + (rare_mod_oci * oci_val)
            # Ensure multiplier doesn't make beta negative
            multiplier = 1.0 + mod_factor
            if multiplier < 0: multiplier = 0
            beta_current *= multiplier

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_current * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s1_outcome = int(state[trial])
        
        # Determine if transition was rare (0->0 and 1->1 are Common; 0->1 and 1->0 are Rare)
        is_rare = (a1 != s1_outcome)
        last_transition_rare = is_rare
        last_action_1 = a1
        
        # Stage 2 Policy
        logits_2 = beta_current * q_stage2_mf[s1_outcome]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # Updates (TD(1))
        delta_stage1 = q_stage2_mf[s1_outcome, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s1_outcome, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s1_outcome, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Outcome-Dependent Model-Based Weighting.
    
    Hypothesis: The balance between Model-Based (planning) and Model-Free (habit) control 
    depends on the previous trial's outcome. OCI specifically modulates the Model-Based 
    weight after a loss (unrewarded trial), reflecting potential deficits in maintaining 
    goal-directedness after failure.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    stickiness: [-5, 5] - Choice perseveration bonus.
    w_win: [0, 1] - Model-Based weight after a rewarded trial.
    w_loss_base: [0, 1] - Base Model-Based weight after an unrewarded trial.
    w_loss_oci: [-1, 1] - Modulation of w_loss by OCI.
    """
    learning_rate, beta, stickiness, w_win, w_loss_base, w_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 1.0 # Assume win for initial state

    for trial in range(n_trials):
        
        # Determine w based on last reward
        if last_reward == 1.0:
            w_curr = w_win
        else:
            w_curr = w_loss_base + (w_loss_oci * oci_val)
        
        w_curr = np.clip(w_curr, 0.0, 1.0)

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_curr * q_stage1_mb + (1 - w_curr) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        last_action_1 = a1
        s1_outcome = int(state[trial])
        
        logits_2 = beta * q_stage2_mf[s1_outcome]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        last_reward = r
        
        delta_stage1 = q_stage2_mf[s1_outcome, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s1_outcome, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s1_outcome, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Planet Stickiness (Outcome-State Perseveration).
    
    Hypothesis: Participants may persevere not just in the motor action (choosing spaceship A/B)
    but in the desire to return to the same planet (Outcome State). High OCI might be 
    associated with a stronger drive to repeat the previous outcome state. This model 
    adds a bonus to actions based on their probability of reaching the previous planet.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-Based weight.
    stick_action: [-5, 5] - Standard stickiness (repeat same spaceship).
    stick_planet_base: [-5, 5] - Base stickiness to the previous planet.
    stick_planet_oci: [-5, 5] - OCI modulation of planet stickiness.
    """
    learning_rate, beta, w, stick_action, stick_planet_base, stick_planet_oci = model_parameters
    n_trials = len(action_1)
    oci_val = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_planet = -1

    for trial in range(n_trials):
        
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Apply standard action stickiness
        if last_action_1 != -1:
            logits_1[last_action_1] += stick_action
            
        # Apply planet stickiness
        if last_planet != -1:
            stick_planet = stick_planet_base + (stick_planet_oci * oci_val)
            # Add bonus to actions proportional to probability of reaching last_planet
            logits_1[0] += stick_planet * transition_matrix[0, last_planet]
            logits_1[1] += stick_planet * transition_matrix[1, last_planet]
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        last_action_1 = a1
        s1_outcome = int(state[trial])
        last_planet = s1_outcome
        
        logits_2 = beta * q_stage2_mf[s1_outcome]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[s1_outcome, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s1_outcome, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + delta_stage2)
        q_stage2_mf[s1_outcome, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```