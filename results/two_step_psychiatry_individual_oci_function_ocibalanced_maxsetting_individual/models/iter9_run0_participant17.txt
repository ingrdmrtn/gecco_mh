Here are three new cognitive models that incorporate the OCI score into the decision-making process in novel ways, respecting the provided template and constraints.

### Model 1: OCI-modulated Learning Rate Asymmetry
This model hypothesizes that individuals with higher OCI scores might learn differently from positive versus negative prediction errors. Specifically, it tests if higher OCI leads to an over-weighting of negative outcomes (avoidance learning) or positive outcomes (compulsive checking), modulating the learning rate `alpha`.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-modulated Learning Rate Asymmetry.
    
    This model posits that the learning rate is split into positive and negative 
    components, and the balance between them is shifted by the OCI score.
    Higher OCI might amplify learning from negative prediction errors (punishment sensitivity).
    
    Parameters:
    alpha_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    oci_asymmetry: [0, 5] - How much OCI scales the learning rate for negative PE.
    """
    alpha_base, beta, w, oci_asymmetry = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update logic
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Calculate dynamic alpha based on sign of PE and OCI
        if delta_stage2 < 0:
            # If outcome is worse than expected, OCI boosts learning (hyper-sensitivity to failure)
            current_alpha = min(1.0, alpha_base * (1.0 + oci_score * oci_asymmetry))
        else:
            current_alpha = alpha_base
            
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + current_alpha * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Apply same asymmetry logic to Stage 1 PE
        if delta_stage1 < 0:
            current_alpha_s1 = min(1.0, alpha_base * (1.0 + oci_score * oci_asymmetry))
        else:
            current_alpha_s1 = alpha_base
            
        q_stage1_mf[a1] = q_stage1_mf[a1] + current_alpha_s1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Exploration Suppression (Inverse Temperature Modulation)
This model suggests that OCI relates to rigidity or a reduction in stochastic exploration. Instead of affecting *what* is learned, OCI affects *how* decisions are made by modulating the inverse temperature parameter ($\beta$). A higher OCI score leads to a higher effective $\beta$, making choices more deterministic and "stuck" on the current best option, reducing random exploration.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Driven Exploration Suppression.
    
    This model assumes that higher OCI scores lead to more rigid, deterministic 
    behavior by increasing the inverse temperature (beta). High OCI participants 
    may be less willing to explore probabilistically.
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta_base: [0, 10] - Baseline inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    oci_rigidity: [0, 5] - Multiplier for OCI's effect on increasing beta.
    """
    learning_rate, beta_base, w, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective beta once
    beta_eff = beta_base * (1.0 + oci_score * oci_rigidity)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use effective beta
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update logic
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Decay)
This model introduces an eligibility trace parameter ($\lambda$) for the Model-Free system, where the OCI score modulates how fast the trace decays. A higher OCI might be associated with "ruminating" on past states or connecting distant causes and effects more strongly (or conversely, getting stuck on immediate details). Here, we model OCI as increasing the eligibility trace $\lambda$, allowing the Stage 2 outcome to influence Stage 1 values more directly (TD(lambda)).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Eligibility Trace (TD-Lambda).
    
    This model implements a TD(lambda) update where the eligibility trace parameter 
    (lambda) is modulated by OCI. This represents how much credit for the second-stage 
    reward is assigned directly to the first-stage choice.
    
    Parameters:
    learning_rate: [0, 1] - Update rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    lambda_base: [0, 1] - Base eligibility trace decay parameter.
    oci_lambda_scale: [0, 1] - Scaling factor for OCI influence on lambda.
    """
    learning_rate, beta, w, lambda_base, oci_lambda_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective lambda, bounded [0, 1]
    # We assume OCI increases the trace (linking stages more strongly)
    lambda_eff = min(1.0, lambda_base + (oci_score * oci_lambda_scale))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Standard TD updates with eligibility trace
        
        # 1. Stage 1 prediction error (TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * delta_stage1
        
        # 2. Stage 2 prediction error
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] = q_stage2_mf[state_idx, a2] + learning_rate * delta_stage2
        
        # 3. Eligibility trace update: Pass Stage 2 PE back to Stage 1 choice
        # The strength of this backward pass is determined by lambda_eff
        q_stage1_mf[a1] = q_stage1_mf[a1] + learning_rate * lambda_eff * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```