Here are three new cognitive models for the two-step task, incorporating the OCI score to modulate specific learning and decision-making mechanisms.

### Model 1: OCI-Modulated Eligibility Trace
This model hypothesizes that OCI symptoms affect "credit assignment"—how much the final outcome (reward) reinforces the initial choice (spaceship). High OCI might lead to over-attribution (obsessive linking of outcomes to distant causes) or under-attribution (focus on immediate steps), modeled by modulating the eligibility trace parameter $\lambda$.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Trace.
    Hypothesis: OCI scores correlate with altered credit assignment for past actions.
    The eligibility trace (lambda) determines how much the Stage 2 reward updates 
    the Stage 1 value. This model modulates lambda based on OCI.
    
    Parameters:
    - lr: Base learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - stickiness: Choice perseveration [0,5]
    - lambda_base: Base eligibility trace [0,1]
    - lambda_mod_param: OCI sensitivity for lambda [0,1].
      Mapped to range [-1, 1]. lambda_eff = lambda_base + oci * mapped_mod.
    """
    lr, beta, w, stickiness, lambda_base, lambda_mod_param = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Map lambda_mod_param from [0,1] to [-1, 1] to allow both increase and decrease
    lambda_sensitivity = (lambda_mod_param - 0.5) * 2.0
    lambda_eff = lambda_base + oci_score * lambda_sensitivity
    # Ensure lambda remains valid
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Stage 1 Policy
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # Learning
        
        # Stage 1 Prediction Error (TD0)
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr * delta_1
        
        # Stage 2 Prediction Error
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr * delta_2
        
        # Eligibility Trace Update for Stage 1 using OCI-modulated lambda
        q_mf_s1[a1] += lr * lambda_eff * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Relief Learning
This model posits that individuals with higher OCI scores are driven by "relief"—the successful avoidance of negative outcomes or the receipt of safety signals. Here, the learning rate is selectively boosted when a reward (1.0) is received, representing a stronger update from success ("relief") than from failure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Relief Learning.
    Hypothesis: OCI is associated with relief-seeking. Participants may learn more
    aggressively from positive outcomes (relief) than neutral/negative ones.
    The learning rate is boosted by OCI specifically when a reward is received.
    
    Parameters:
    - lr: Base learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Choice perseveration [0,5]
    - relief_factor: Multiplier for LR on reward [0,5]
      lr_eff = lr * (1 + oci * relief_factor) if Reward=1 else lr
    """
    lr, beta, w, lambda_val, stickiness, relief_factor = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Determine effective learning rate based on reward (Relief hypothesis)
        if r == 1.0:
            lr_eff = lr * (1.0 + oci_score * relief_factor)
        else:
            lr_eff = lr
        
        # Cap lr at 1.0 for stability
        lr_eff = min(lr_eff, 1.0)

        # Stage 1 Policy
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # Learning
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr_eff * delta_1
        
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr_eff * delta_2
        
        q_mf_s1[a1] += lr_eff * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 1 Learning Boost
This model tests the hypothesis that OCI symptoms lead to differential learning rates for the two stages. High OCI might be associated with anxiety about the initial transition (Spaceship -> Planet), leading to faster, more reactive updating of Stage 1 values compared to the standard learning rate used for Stage 2.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 1 Learning Boost.
    Hypothesis: OCI participants may exhibit differential learning speeds for the 
    initial choice (Stage 1) versus the final outcome (Stage 2). High OCI might
    drive faster updating of the spaceship choice due to anxiety about the transition.
    
    Parameters:
    - lr: Base learning rate [0,1]
    - beta: Inverse temperature [0,10]
    - w: Model-based weight [0,1]
    - lambda_val: Eligibility trace [0,1]
    - stickiness: Choice perseveration [0,5]
    - s1_boost: Multiplier for Stage 1 LR based on OCI [0,5]
      lr_s1 = lr * (1 + oci * s1_boost)
      lr_s2 = lr
    """
    lr, beta, w, lambda_val, stickiness, s1_boost = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_s1 = np.zeros(2)
    q_mf_s2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    # Calculate separate learning rates
    lr_s1 = lr * (1.0 + oci_score * s1_boost)
    lr_s1 = min(lr_s1, 1.0) # Cap at 1.0
    lr_s2 = lr
    
    for t in range(n_trials):
        if action_1[t] == -1:
            p_choice_1[t] = 1.0
            p_choice_2[t] = 1.0
            continue
            
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # Stage 1 Policy
        max_q_s2 = np.max(q_mf_s2, axis=1)
        q_mb_s1 = transition_matrix @ max_q_s2
        
        q_net_s1 = w * q_mb_s1 + (1 - w) * q_mf_s1
        logits_1 = beta * q_net_s1
        if prev_a1 != -1:
            logits_1[prev_a1] += stickiness
            
        exp_1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_1 / np.sum(exp_1)
        p_choice_1[t] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_mf_s2[s2]
        exp_2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_2 / np.sum(exp_2)
        p_choice_2[t] = probs_2[a2]
        
        # Learning
        
        # Update Stage 1 Value (using boosted lr_s1)
        delta_1 = q_mf_s2[s2, a2] - q_mf_s1[a1]
        q_mf_s1[a1] += lr_s1 * delta_1
        
        # Update Stage 2 Value (using standard lr_s2)
        delta_2 = r - q_mf_s2[s2, a2]
        q_mf_s2[s2, a2] += lr_s2 * delta_2
        
        # Eligibility trace update for Stage 1 (using lr_s1)
        q_mf_s1[a1] += lr_s1 * lambda_val * delta_2
        
        prev_a1 = a1

    eps = 1e-10
    valid_mask = (action_1 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```