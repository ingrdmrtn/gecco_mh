Here are three new cognitive models implemented as Python functions.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Trace Perseveration Stickiness.
    
    Hypothesis: High OCI participants exhibit more rigid, historically-driven perseveration.
    Instead of just repeating the last choice (simple stickiness), they repeat based on a 
    longer history (exponential choice trace). The decay rate of this trace is modulated by OCI.
    High OCI -> Slower decay -> Stickiness influenced by longer history.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - stick_mag: [0, 5] Magnitude of stickiness bias applied to the trace.
    - tau_base: [0, 1] Base decay rate for the choice trace (0 = only last choice).
    - tau_oci: [0, 1] Modulation of decay rate by OCI. 
      tau = tau_base + tau_oci * oci (clipped to 0.95 to prevent instability).
    """
    lr, beta, w, stick_mag, tau_base, tau_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate tau (decay rate)
    tau = tau_base + tau_oci * oci_score
    if tau > 0.95: tau = 0.95
    if tau < 0.0: tau = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    # Choice trace for actions 0 and 1
    trace = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add trace stickiness (instead of simple last-choice stickiness)
        logits_1 = beta * q_net + stick_mag * trace
        
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 Policy
        logits_2 = beta * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        # Trace update: decay existing, add current choice
        # trace[a] = tau * trace[a] + (1-tau) * 1 (if chosen)
        trace *= tau
        trace[a1] += (1.0 - tau)
        
        # Q-learning
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Conflict-Gated Decision Noise.
    
    Hypothesis: High OCI participants experience anxiety or indecision when the 
    Model-Based (goal-directed) and Model-Free (habitual) systems suggest different actions.
    This is modeled as a reduction in Beta (increased randomness/hesitation) specifically 
    on trials where MB and MF systems favor different Stage 1 choices.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_base: [0, 10] Base inverse temperature (used when no conflict).
    - w: [0, 1] MB/MF mixing weight.
    - stick: [0, 5] Choice stickiness.
    - conflict_damp_oci: [0, 1] Factor reducing beta based on OCI during conflict.
      beta_eff = beta_base * (1 - conflict_damp_oci * oci) if MB and MF disagree.
    """
    lr, beta_base, w, stick, conflict_damp_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Stage 1 Valuations
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Determine Conflict
        # Conflict exists if the preferred action of MB differs from MF
        # We check argmax. If values are identical, defaults to 0 (no conflict assumed).
        pref_mb = np.argmax(q_stage1_mb)
        pref_mf = np.argmax(q_stage1_mf)
        is_conflict = (pref_mb != pref_mf)
        
        # Adjust Beta based on Conflict and OCI
        beta_eff = beta_base
        if is_conflict:
            factor = conflict_damp_oci * oci_score
            if factor > 0.9: factor = 0.9 # Cap dampening to avoid beta < 0 or too low
            beta_eff = beta_base * (1.0 - factor)
            
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_eff * q_net
        
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 (Standard Beta, no conflict concept here)
        logits_2 = beta_base * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Precision.
    
    Hypothesis: High OCI participants show differential exploitation in the two stages.
    While Stage 1 involves complex planning (MB/MF integration) and uncertainty, 
    Stage 2 is a simple bandit task. High OCI (compulsivity) may manifest as "greedy" 
    or high-precision behavior specifically in the immediate reward stage (Stage 2), 
    increasing beta_2 relative to beta_1.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Strategic choice).
    - w: [0, 1] MB/MF mixing weight.
    - stick: [0, 5] Stickiness.
    - beta_2_slope_oci: [0, 5] Increase in Stage 2 beta per unit OCI.
      beta_2 = beta_1 * (1 + beta_2_slope_oci * oci).
    """
    lr, beta_1, w, stick, beta_2_slope_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Beta 2 is coupled to Beta 1 but amplified by OCI
    beta_2 = beta_1 * (1.0 + beta_2_slope_oci * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_choice_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])
        
        # Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta_1 * q_net
        
        if last_choice_1 != -1:
            logits_1[last_choice_1] += stick
            
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Stage 2 (Using Beta 2)
        logits_2 = beta_2 * q_stage2_mf[s2]
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Updates
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        last_choice_1 = a1

    mask = (action_1 != -1)
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```