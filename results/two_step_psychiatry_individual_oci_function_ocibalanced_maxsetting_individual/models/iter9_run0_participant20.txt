Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in this two-step task, focusing on mechanisms like reward sensitivity, punishment sensitivity, and second-stage specific perseveration.

### Cognitive Model 1: OCI-Modulated Reward vs. Punishment Sensitivity
This model hypothesizes that individuals with higher OCI scores may exhibit differential learning rates for positive outcomes (rewards) versus negative outcomes (omissions). Specifically, it tests if OCI amplifies the learning rate for punishment (zero coins), reflecting a hyper-sensitivity to failure or "not winning," which is common in compulsivity.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Reward vs. Punishment Sensitivity.
    
    Hypothesis: OCI scores modulate how strongly participants learn from negative outcomes 
    (punishment/omission) compared to positive outcomes. High OCI might lead to 
    over-correction after failure.
    
    Parameters:
    alpha_pos: [0,1] - Learning rate for rewarded trials (outcome = 1).
    alpha_neg_base: [0,1] - Base learning rate for unrewarded trials (outcome = 0).
    alpha_neg_oci_scale: [0,1] - Scaling factor for OCI on negative learning rate.
    beta: [0,10] - Inverse temperature (softmax sensitivity).
    w: [0,1] - Mixing weight for Model-Based (1) vs Model-Free (0).
    """
    alpha_pos, alpha_neg_base, alpha_neg_oci_scale, beta, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate the effective negative learning rate based on OCI
    # Bounded to [0, 1] to ensure stability
    alpha_neg = alpha_neg_base + (alpha_neg_oci_scale * participant_oci)
    if alpha_neg > 1.0: alpha_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Select learning rate based on outcome
        current_alpha = alpha_pos if r > 0 else alpha_neg
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_alpha * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Dependent Second-Stage Stickiness
This model investigates if OCI specifically influences "local" perseveration at the second stage (the aliens). While general perseveration is often studied, this model posits that compulsive symptoms might manifest as getting "stuck" on a specific terminal option (an alien) regardless of the spaceship choice, modeled as a stickiness bonus applied to the second-stage choice that scales with OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Dependent Second-Stage Stickiness.
    
    Hypothesis: OCI relates to repetitive behaviors. This model tests if OCI predicts 
    perseveration specifically at the second stage (choosing the same alien), 
    independent of reward history.
    
    Parameters:
    learning_rate: [0,1] - Standard learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weighting.
    stickiness_s2_base: [0,5] - Base stickiness for repeating the stage 2 choice.
    stickiness_s2_oci_slope: [0,5] - How much OCI increases stage 2 stickiness.
    """
    learning_rate, beta, w, stickiness_s2_base, stickiness_s2_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate stickiness magnitude
    stickiness_mag = stickiness_s2_base + (stickiness_s2_oci_slope * participant_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_2_by_state = np.full(2, -1) # Track last action taken in each state [state0_last_act, state1_last_act]

    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        # Add stickiness bonus to Q-values temporarily for decision
        current_q_s2 = q_stage2_mf[s_idx].copy()
        
        # If we have visited this state before and made a choice
        if last_action_2_by_state[s_idx] != -1:
            prev_a2 = int(last_action_2_by_state[s_idx])
            current_q_s2[prev_a2] += stickiness_mag
            
        exp_q2 = np.exp(beta * current_q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Update history
        last_action_2_by_state[s_idx] = a2
        
        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Temperature (Exploration/Exploitation Balance)
This model posits that OCI affects the overall randomness of choice (exploration vs. exploitation). Higher OCI might be associated with more rigid, deterministic behavior (higher beta), or conversely, more anxiety-driven random checking (lower beta). This model allows the inverse temperature parameter `beta` to vary linearly with the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Temperature (Beta).
    
    Hypothesis: OCI affects the exploration-exploitation trade-off. 
    High OCI might lead to 'rigid' behavior (high beta, very deterministic) 
    or 'noisy' behavior (low beta). This model makes beta a function of OCI.
    
    Parameters:
    learning_rate: [0,1] - Standard learning rate.
    beta_base: [0,10] - Baseline inverse temperature.
    beta_oci_slope: [-5,5] - How OCI modifies beta. Can be positive (rigid) or negative (noisy).
                            (Note: effectively bounded by optimization, here we assume input is suitable).
    w: [0,1] - MB/MF weighting.
    """
    learning_rate, beta_base, beta_oci_slope, w = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective beta
    # We use absolute value or bounding to ensure beta doesn't become negative and invert preferences
    # effectively turning maximization into minimization
    beta_eff = beta_base + (beta_oci_slope * participant_oci)
    if beta_eff < 0: beta_eff = 0.0
    if beta_eff > 20: beta_eff = 20.0 # Upper bound for numerical stability
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```