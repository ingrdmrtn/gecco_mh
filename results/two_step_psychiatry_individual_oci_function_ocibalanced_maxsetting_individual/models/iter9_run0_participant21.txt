Here are 3 new cognitive models that explore different mechanisms for how Obsessive-Compulsive Inventory (OCI) scores might influence learning and decision-making in a two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores correlate with a deficit in goal-directed (model-based) control, leading to a stronger reliance on habit (model-free) systems. Instead of a linear relationship, it uses a sigmoid scaling factor to modulate the mixing weight `w` between model-based and model-free values based on the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner where the balance between Model-Based (MB) and Model-Free (MF) 
    control is modulated by OCI. Higher OCI scores might reduce the contribution 
    of the MB system (reduced 'w').
    
    Parameters:
    learning_rate : [0, 1] Learning rate for value updates.
    beta : [0, 10] Inverse temperature for softmax choice.
    w_max : [0, 1] Maximum possible weight for model-based control (at OCI=0).
    oci_sensitivity : [0, 10] How sharply OCI reduces the model-based weight.
    """
    learning_rate, beta, w_max, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Transition matrix (fixed structure of the task)
    # 0 -> 0 (70%), 0 -> 1 (30%)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Calculate the mixing weight w based on OCI
    # We model this as w decreasing as OCI increases.
    # w = w_max * exp(-sensitivity * OCI)
    w = w_max * np.exp(-oci_sensitivity * current_oci)
    
    # Initialize values
    q_stage1_mf = np.zeros(2)      # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for stage 2 (states x actions)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: Bellman equation using known transition matrix
        # V_MB(s1) = sum(P(s2|s1) * max(Q_stage2(s2, a)))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Policy Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        # Standard Model-Free Q-learning for the second stage
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Update Stage 2 Q-values
        # RPE stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 Model-Free Q-values (TD(0) style)
        # Often in hybrid models, Stage 1 MF values are updated using the Q-value of the chosen stage 2 action
        # or the max of the next stage. Here we use the value of the state reached.
        # Standard TD(1) or similar often used, here using simple TD(0) to state value
        v_stage2_state = q_stage2_mf[s_idx, a2] # Value of the action taken
        delta_stage1 = v_stage2_state - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates with OCI-Driven Punishment Sensitivity
This model investigates whether OCI is associated with an increased sensitivity to negative outcomes (omission of reward). It separates learning from positive feedback (reward = 1) and negative feedback (reward = 0), hypothesizing that higher OCI leads to faster learning (or overreaction) from failures.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with Asymmetric Learning Rates.
    The learning rate for negative prediction errors (punishment/omission) 
    is scaled by the OCI score.
    
    Parameters:
    lr_pos : [0, 1] Learning rate for positive prediction errors (reward).
    lr_neg_base : [0, 1] Base learning rate for negative prediction errors.
    lr_neg_oci_slope : [0, 1] Increase in negative learning rate per unit of OCI.
    beta : [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_oci_slope, beta = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective negative learning rate
    # Bounded at 1.0 to prevent instability
    lr_neg = min(1.0, lr_neg_base + lr_neg_oci_slope * current_oci)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 Update
        prediction_error_2 = r - q_stage2[s_idx, a2]
        
        if prediction_error_2 >= 0:
            eff_lr = lr_pos
        else:
            eff_lr = lr_neg
            
        q_stage2[s_idx, a2] += eff_lr * prediction_error_2
        
        # Stage 1 Update (TD(0) to best next stage value)
        # Using max Q of the state arrived at as the target
        target_val = np.max(q_stage2[s_idx])
        prediction_error_1 = target_val - q_stage1[a1]
        
        # Apply the same asymmetry logic to stage 1 updates
        if prediction_error_1 >= 0:
            eff_lr_1 = lr_pos
        else:
            eff_lr_1 = lr_neg
            
        q_stage1[a1] += eff_lr_1 * prediction_error_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Uncertainty Avoidance
This model posits that individuals with higher OCI scores are averse to uncertainty. It adds an "uncertainty penalty" to the value of actions based on how much their estimates fluctuate or how rarely they are visited. Here, we approximate uncertainty using a decay counter: actions not chosen recently become "uncertain". High OCI participants might penalize these uncertain options, preferring familiar, safe routines.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free Learner with Uncertainty Avoidance.
    Tracks 'time since last chosen' for Stage 1 actions. 
    High OCI introduces a penalty for actions that haven't been chosen recently 
    (uncertainty aversion/neophobia).
    
    Parameters:
    learning_rate : [0, 1] Learning rate.
    beta : [0, 10] Inverse temperature.
    penalty_base : [0, 2] Base penalty for uncertainty.
    penalty_oci_scale : [0, 5] Scaling of uncertainty penalty by OCI.
    """
    learning_rate, beta, penalty_base, penalty_oci_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    # Track how many trials since an action was last chosen
    # Initialize with 0
    days_since_choice = np.zeros(2) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Choice ---
        # Calculate uncertainty penalty
        # Penalty increases with days_since_choice
        # Effective penalty = (Base + OCI * Scale) * log(1 + days_since)
        # We use log to dampen the effect of very long unchosen streaks
        uncertainty_factor = np.log(1 + days_since_choice)
        penalty = (penalty_base + penalty_oci_scale * current_oci) * uncertainty_factor
        
        # Net Q value is estimated value minus uncertainty penalty
        q_net_stage1 = q_stage1 - penalty
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update counters
        days_since_choice += 1
        days_since_choice[a1] = 0 # Reset counter for chosen action
        
        # --- Stage 2 Choice ---
        # Standard choice for stage 2
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Standard SARSA/Q-learning updates
        
        # Stage 2
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Stage 1
        # Update using the value of the state reached (max Q of next stage)
        target_val = np.max(q_stage2[s_idx])
        delta_1 = target_val - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```