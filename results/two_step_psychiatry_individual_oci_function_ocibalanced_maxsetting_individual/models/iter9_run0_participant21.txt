Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

### Model 1: Dual Learning Rate and OCI-Modulated Perseveration
This model hypothesizes that the participant processes positive and negative outcomes with different learning rates (asymmetric updating) and that their tendency to stick with a choice (perseveration) is modulated by their OCI score. This structure allows the model to capture "loss chasing" or "ignoring failure" via the learning rates, while the OCI parameter scales the overall rigidity of behavior.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dual Learning Rate and OCI-Modulated Perseveration Model.
    
    Hypothesis: The participant separates learning from positive vs negative outcomes (Dual LRs).
    Additionally, their stickiness (perseveration) is modulated by their OCI score.
    High OCI might lead to higher perseveration regardless of the learning rates.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (wins).
    lr_neg: [0, 1] Learning rate for negative prediction errors (losses).
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    pers_base: [-2, 2] Base perseveration bonus.
    pers_oci: [-2, 2] Effect of OCI on perseveration.
    """
    lr_pos, lr_neg, beta, w, pers_base, pers_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate effective perseveration based on OCI
    pers = pers_base + pers_oci * current_oci
    
    # Initialize values
    q_mf = np.zeros(2) + 0.5
    q2 = np.zeros((2, 2)) + 0.5
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action = -1
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Choice ---
        # Model-Based Value
        max_q2 = np.max(q2, axis=1)
        q_mb = tm @ max_q2
        
        # Net Value (Hybrid)
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Add Perseveration
        if last_action != -1:
            q_net[last_action] += pers
            
        # Softmax Probability
        exp_q1 = np.exp(beta * q_net)
        probs1 = exp_q1 / np.sum(exp_q1)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # --- Updates ---
        # Stage 1 MF Update
        pe1 = q2[s, a2] - q_mf[a1]
        lr1 = lr_pos if pe1 >= 0 else lr_neg
        q_mf[a1] += lr1 * pe1
        
        # Stage 2 Update
        pe2 = r - q2[s, a2]
        lr2 = lr_pos if pe2 >= 0 else lr_neg
        q2[s, a2] += lr2 * pe2
        
        last_action = a1
        
    return -log_lik
```

### Model 2: Post-Loss Rigidity Model
This model tests the hypothesis that OCI symptoms modulate the reaction to negative feedback. Specifically, it proposes that high OCI leads to increased "rigidity" (higher inverse temperature `beta`) immediately following a loss (0 coins). This reflects a "freezing" or compulsive adherence to the current strategy under stress, making the participant less likely to explore stochastically after failure.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Post-Loss Rigidity Model.
    
    Hypothesis: OCI symptoms modulate the reaction to negative feedback.
    Specifically, high OCI leads to increased rigidity (higher beta) after a loss (0 reward),
    reflecting "freezing" or compulsive adherence to a strategy under stress.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature.
    w: [0, 1] Model-Based weight.
    pers: [-2, 2] Perseveration bonus.
    stiff_oci: [-5, 5] Modulation of beta after a loss by OCI. 
               Positive value -> Higher beta (rigidity) after loss for high OCI.
    """
    lr, beta_base, w, pers, stiff_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    q_mf = np.zeros(2) + 0.5
    q2 = np.zeros((2, 2)) + 0.5
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action = -1
    last_reward = 1 # Assume start with neutral/win state to avoid initial rigidity
    
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Dynamic Beta Calculation ---
        # If last reward was 0 (loss), modify beta based on OCI
        if last_reward == 0:
            beta_eff = beta_base * (1 + stiff_oci * current_oci)
            # Ensure beta doesn't go negative
            beta_eff = np.maximum(0.0, beta_eff)
        else:
            beta_eff = beta_base
            
        # --- Stage 1 Choice ---
        max_q2 = np.max(q2, axis=1)
        q_mb = tm @ max_q2
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action != -1:
            q_net[last_action] += pers
            
        exp_q1 = np.exp(beta_eff * q_net)
        probs1 = exp_q1 / np.sum(exp_q1)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        # Applying effective beta here as well to reflect general state of mind
        exp_q2 = np.exp(beta_eff * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # --- Updates ---
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
        last_action = a1
        last_reward = r
        
    return -log_lik
```

### Model 3: Conflict-Modulated Uncertainty Model
This model hypothesizes that high OCI individuals experience hesitation (uncertainty) when the Model-Based (goal-directed) and Model-Free (habitual) systems provide conflicting values. When the difference in preference between the two systems is high, the model reduces the inverse temperature (`beta`), making choices more stochastic (hesitant). The degree of this reduction is scaled by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Conflict-Modulated Uncertainty Model.
    
    Hypothesis: High OCI individuals experience hesitation (uncertainty) when
    Model-Based (goal-directed) and Model-Free (habitual) systems provide conflicting values.
    This conflict reduces the inverse temperature (beta), making choices more stochastic.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature.
    w: [0, 1] Model-Based weight.
    pers: [-2, 2] Perseveration bonus.
    conflict_oci: [0, 10] Scaling of conflict effect by OCI.
                  Higher value -> More reduction in beta when conflict is high.
    """
    lr, beta_base, w, pers, conflict_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    q_mf = np.zeros(2) + 0.5
    q2 = np.zeros((2, 2)) + 0.5
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    last_action = -1
    log_lik = 0.0
    
    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]
        
        # --- Stage 1 Values ---
        max_q2 = np.max(q2, axis=1)
        q_mb = tm @ max_q2
        
        # --- Calculate Conflict ---
        # Conflict is defined as the absolute difference in preference magnitude between MB and MF
        diff_mb = q_mb[0] - q_mb[1]
        diff_mf = q_mf[0] - q_mf[1]
        
        # We use the absolute difference of the differences to represent disagreement
        conflict = np.abs(diff_mb - diff_mf)
        
        # --- Modulate Beta ---
        # Beta is reduced as conflict increases, scaled by OCI
        beta_eff = beta_base / (1.0 + conflict_oci * current_oci * conflict)
        
        q_net = w * q_mb + (1 - w) * q_mf
        
        if last_action != -1:
            q_net[last_action] += pers
            
        exp_q1 = np.exp(beta_eff * q_net)
        probs1 = exp_q1 / np.sum(exp_q1)
        log_lik += np.log(probs1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        # Stage 2 uses base beta as conflict is primarily a planning vs habit phenomenon
        exp_q2 = np.exp(beta_base * q2[s])
        probs2 = exp_q2 / np.sum(exp_q2)
        log_lik += np.log(probs2[a2] + 1e-10)
        
        # --- Updates ---
        pe1 = q2[s, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        
        last_action = a1
        
    return -log_lik
```