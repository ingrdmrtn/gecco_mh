Here are three new cognitive models based on the provided data and task description. These models explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might modulate decision-making, specifically focusing on perseveration, learning rates from failure, and model-based weighting.

### Model 1: OCI-Modulated Perseveration (Stickiness)
This model hypothesizes that OCI symptoms are linked to "stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of reward. Higher OCI scores increase this stickiness parameter, making behavior more rigid.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Perseveration (Stickiness).
    
    Hypothesis: Higher OCI scores lead to increased 'stickiness' (perseveration)
    in the first-stage choice. This reflects a compulsion to repeat actions 
    regardless of outcomes.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature (softmax sensitivity).
    w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    stick_base: [0, 5] Baseline choice persistence.
    stick_oci_slope: [0, 5] Additional stickiness scaled by OCI score.
                     Total stickiness = stick_base + (stick_oci_slope * oci).
    """
    learning_rate, beta, w, stick_base, stick_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    stickiness = stick_base + (stick_oci_slope * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states, 2 actions)
    
    last_action_1 = -1
    log_loss = 0.0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB(s1) = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value mixing MB and MF
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        # Softmax choice probability for Stage 1
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2) # Safety for overflow/underflow
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Accumulate Log Loss for Action 1
        # Add epsilon to prevent log(0)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Policy ---
        # Simple Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        # Accumulate Log Loss for Action 2
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Learning Updates ---
        # Prediction error for Stage 2 (TD error)
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        # Prediction error for Stage 1 (TD error using Stage 2 value as target)
        # Note: We use the value of the state actually reached (q_stage2_mf[s, a2])
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    return log_loss
```

### Model 2: OCI-Specific Punishment Sensitivity
This model tests the idea that OCI is associated with an over-sensitivity to negative outcomes (omission of reward). It introduces a separate learning rate for unrewarded trials (failures), which is modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Specific Punishment Sensitivity (Dual Learning Rates).
    
    Hypothesis: High OCI participants learn differently from failure (0 reward) 
    compared to success. Specifically, OCI modulates the learning rate for 
    negative outcomes (lr_neg), potentially reflecting hyper-correction or 
    rigidity after errors.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for rewarded trials (reward=1).
    lr_neg_base: [0, 1] Base learning rate for unrewarded trials (reward=0).
    lr_neg_oci_slope: [0, 5] Modulation of negative learning rate by OCI.
                      lr_neg = lr_neg_base * (1 + lr_neg_oci_slope * oci).
                      (The result is clipped to [0, 1]).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    """
    lr_pos, lr_neg_base, lr_neg_oci_slope, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-modulated negative learning rate
    lr_neg = lr_neg_base * (1.0 + lr_neg_oci_slope * oci_score)
    lr_neg = min(max(lr_neg, 0.0), 1.0) # Clip to valid range
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_loss = 0.0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Determine current learning rate based on outcome
        current_lr = lr_pos if r > 0 else lr_neg

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + 1e-10)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updating ---
        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += current_lr * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    return log_loss
```

### Model 3: OCI-Driven Model-Based Suppression
This model posits that high OCI scores interfere with the computationally expensive Model-Based (planning) system. Instead of simply changing the weight `w`, this model assumes OCI acts as a "tax" or suppression factor on the `w` parameter, reducing reliance on the transition structure of the task.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Driven Model-Based Suppression.
    
    Hypothesis: OCI symptoms consume cognitive resources or induce rigid habits,
    suppressing the Model-Based (goal-directed) system. 
    The effective weight `w` is reduced as OCI increases.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w_max: [0, 1] Maximum possible Model-Based weight (for OCI=0).
    w_oci_decay: [0, 5] Rate at which OCI suppresses the Model-Based weight.
                 w_effective = w_max * exp(-w_oci_decay * oci).
    stickiness: [0, 5] General choice persistence.
    """
    learning_rate, beta, w_max, w_oci_decay, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective Model-Based weight: decays exponentially with OCI
    w_effective = w_max * np.exp(-w_oci_decay * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    log_loss = 0.0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine using OCI-suppressed weight
        q_net_1 = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_1)
        if np.sum(exp_q1) == 0: exp_q1 = np.ones(2)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + 1e-10)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        if np.sum(exp_q2) == 0: exp_q2 = np.ones(2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + 1e-10)
        
        # --- Updating ---
        delta_stage2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        last_action_1 = a1

    return log_loss
```