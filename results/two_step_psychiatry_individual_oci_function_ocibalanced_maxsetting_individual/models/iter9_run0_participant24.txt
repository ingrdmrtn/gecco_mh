Here are three new cognitive models that incorporate the OCI score into the decision-making process in different ways, specifically focusing on the balance between Model-Based (MB) and Model-Free (MF) control, as well as learning rate dynamics.

### Cognitive Model 1: OCI-Modulated Model-Based Weighting
This model tests the hypothesis that higher OCI scores correlate with a stronger reliance on habitual (Model-Free) control over goal-directed (Model-Based) control. It implements a standard hybrid learner where the weighting parameter `w` (mixing MB and MF values) is a logistic function of the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free RL where the mixing weight 'w' is determined by OCI.
    
    Hypothesis: Higher OCI scores might lead to more rigid, habitual (Model-Free) behavior,
    implying a lower 'w' (where w=1 is pure MB, w=0 is pure MF).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax.
    w_intercept: [0, 1] - Base MB weight when OCI is 0.
    w_slope: [-5, 5] - How OCI affects the MB weight (logistic transform).
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate mixing weight w using a logistic function to keep it bounded [0,1]
    # We use a standard logistic formulation: 1 / (1 + exp(-(intercept + slope*oci)))
    # However, to map parameters intuitively:
    # Let's define the logit directly.
    w_logit = w_intercept + w_slope * oci_score
    w = 1.0 / (1.0 + np.exp(-w_logit))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: V_MB(s1) = T * max(Q_MF(s2))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                r = reward[trial]
                
                # TD Error Stage 2
                delta_stage2 = r - q_stage2_mf[s_idx, a2]
                q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
                
                # TD Error Stage 1 (Model-Free update)
                # Note: In standard hybrid models, Q_MF(s1) is updated via SARSA or Q-learning
                if a1 != -1:
                    delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
                    q_stage1_mf[a1] += learning_rate * delta_stage1
                    # Eligibility trace for stage 1 update from stage 2 reward (lambda=1 assumed)
                    q_stage1_mf[a1] += learning_rate * delta_stage2
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Dependent Learning Rate Asymmetry
This model investigates if OCI symptoms relate to how participants learn from positive versus negative prediction errors. It posits that individuals with higher OCI might be more sensitive to "missed" rewards (negative prediction errors) or vice versa, effectively having different learning rates for positive and negative outcomes modulated by their symptom score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free RL with Asymmetric Learning Rates modulated by OCI.
    
    Hypothesis: OCI affects the ratio between learning from positive prediction errors (alpha_pos)
    and negative prediction errors (alpha_neg).
    
    Parameters:
    lr_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    bias_param: [0, 1] - Base bias towards positive (if >0.5) or negative (if <0.5) learning.
    oci_mod: [-1, 1] - How OCI shifts the bias. Positive means OCI increases learning from positive PEs.
    """
    lr_base, beta, bias_param, oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective bias
    # bias_eff determines the split between alpha_pos and alpha_neg
    # We clamp it to [0.01, 0.99] to ensure non-zero learning rates
    bias_eff = bias_param + (oci_mod * oci_score)
    bias_eff = np.clip(bias_eff, 0.01, 0.99)
    
    # We define alpha_pos and alpha_neg such that their average is roughly lr_base
    # This keeps the magnitude of learning controlled by lr_base, while bias_eff controls the ratio.
    # If bias_eff is 0.5, both are lr_base.
    # If bias_eff > 0.5, alpha_pos > alpha_neg.
    alpha_pos = lr_base * (2 * bias_eff)
    alpha_neg = lr_base * (2 * (1 - bias_eff))
    
    # Clip to valid range [0, 1]
    alpha_pos = np.clip(alpha_pos, 0, 1)
    alpha_neg = np.clip(alpha_neg, 0, 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0

        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        if s_idx != -1:
            exp_q2 = np.exp(beta * q_stage2[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                r = reward[trial]
                
                # Stage 2 Update
                delta2 = r - q_stage2[s_idx, a2]
                lr2 = alpha_pos if delta2 > 0 else alpha_neg
                q_stage2[s_idx, a2] += lr2 * delta2
                
                # Stage 1 Update
                if a1 != -1:
                    # TD(1) style update: propagate reward back to stage 1
                    # Note: We use the stored Q-value of the chosen stage 2 state/action
                    # as the target for stage 1, plus the prediction error from stage 2.
                    # Commonly simplified to just updating Q(s1,a1) with the total reward prediction error.
                    
                    # Using a simple SARSA-like update for stage 1 based on Q(s2,a2)
                    delta1 = q_stage2[s_idx, a2] - q_stage1[a1]
                    lr1 = alpha_pos if delta1 > 0 else alpha_neg # Use PE sign for learning rate choice
                    q_stage1[a1] += lr1 * delta1
                    
                    # Second step of TD(1): drive Q(s1) with the stage 2 PE
                    # Effectively: Q(s1) += alpha * (r - Q(s1))
                    # But we apply the stage 2 PE with the stage 2 learning rate logic
                    q_stage1[a1] += lr2 * delta2 

            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Inverse Temperature (Exploration/Exploitation)
This model tests if OCI scores influence the exploration-exploitation trade-off. It posits that higher OCI scores might lead to more deterministic (less random) choice behavior, represented by a higher inverse temperature (`beta`).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Inverse Temperature (Beta).
    
    Hypothesis: OCI affects decision noise/exploration. Higher OCI might lead to 
    more rigid adherence to values (higher beta) or more anxiety-driven noise (lower beta).
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    w: [0, 1] - Weight for Model-Based control (fixed, not OCI dependent here).
    beta_base: [0, 10] - Base inverse temperature.
    oci_beta_factor: [-5, 5] - Scaling factor for OCI's effect on beta.
    """
    learning_rate, w, beta_base, oci_beta_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    # We use an exponential formulation to ensure beta stays positive
    # beta_eff = beta_base * exp(factor * oci)
    # This allows OCI to scale beta up or down multiplicatively.
    beta_eff = beta_base * np.exp(oci_beta_factor * oci_score)
    
    # Cap beta to prevent overflow/numerical instability, though exp(10) is huge anyway.
    beta_eff = np.clip(beta_eff, 0, 20) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        if a1 != -1:
            p_choice_1[trial] = probs_1[a1]
        else:
            p_choice_1[trial] = 1.0
            
        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        if s_idx != -1:
            exp_q2 = np.exp(beta_eff * q_stage2_mf[s_idx])
            probs_2 = exp_q2 / np.sum(exp_q2)
            
            a2 = int(action_2[trial])
            if a2 != -1:
                p_choice_2[trial] = probs_2[a2]
                
                # --- Learning ---
                r = reward[trial]
                
                delta_stage2 = r - q_stage2_mf[s_idx, a2]
                q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
                
                if a1 != -1:
                    delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
                    q_stage1_mf[a1] += learning_rate * delta_stage1
                    q_stage1_mf[a1] += learning_rate * delta_stage2
            else:
                p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```