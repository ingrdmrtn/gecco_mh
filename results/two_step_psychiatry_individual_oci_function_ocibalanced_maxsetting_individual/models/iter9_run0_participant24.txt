def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Goal Stickiness Model (Model-Based Perseveration).
    
    Hypothesis: High OCI participants exhibit a compulsive drive to return to the previously 
    visited goal state (Planet), utilizing their knowledge of the transition structure 
    (Model-Based) to select the action most likely to lead there, regardless of whether 
    the previous outcome was a reward. This differs from simple motor perseveration.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Softmax inverse temperature.
    - w: [0, 1] Weighting of Model-Based values (1=MB, 0=MF).
    - goal_stick_base: [0, 5] Baseline tendency to choose the action leading to the previous planet.
    - goal_stick_oci_slope: [-5, 5] Modulation of goal stickiness by OCI score.
    """
    learning_rate, beta, w, goal_stick_base, goal_stick_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # Transition matrix: Row 0 -> Action 0 (A) probs [X, Y]; Row 1 -> Action 1 (U) probs [X, Y]
    # A -> X (0.7), U -> Y (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track the previous state (planet) visited
    prev_state = -1

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Goal Stickiness
        # If we visited a planet last time, boost the action commonly leading to it.
        # Given T=[[0.7, 0.3], [0.3, 0.7]], Action 0 leads to State 0, Action 1 leads to State 1.
        # So we simply boost the index corresponding to prev_state.
        if prev_state != -1:
            stickiness = goal_stick_base + (goal_stick_oci_slope * participant_oci)
            # Ensure stickiness doesn't invert preferences illogically (optional, but standard is additive)
            q_net_stage1[prev_state] += stickiness

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update (SARSA-like for MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_state = s_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps)) + np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Stage 2 Beta Modulation Model.
    
    Hypothesis: OCI scores correlate with the exploration/exploitation balance specifically 
    at the second stage (the 'bandit' task of choosing aliens), distinct from the planning 
    stage. High OCI may lead to more rigid (high beta) or more random (low beta) behavior 
    at the concrete outcome stage.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - w: [0, 1] Weighting of Model-Based values.
    - beta_stage1: [0, 10] Fixed inverse temperature for Stage 1 (Spaceships).
    - beta_stage2_base: [0, 10] Baseline inverse temperature for Stage 2 (Aliens).
    - beta_stage2_oci_slope: [-10, 10] Modulation of Stage 2 beta by OCI.
    """
    learning_rate, w, beta_stage1, beta_stage2_base, beta_stage2_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate OCI-dependent Stage 2 Beta
    beta_stage2 = beta_stage2_base + (beta_stage2_oci_slope * participant_oci)
    # Ensure beta stays non-negative
    beta_stage2 = max(0.0, beta_stage2)

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy (Fixed Beta) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta_stage1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy (OCI-Modulated Beta) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps)) + np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Positive Learning Rate Modulation Model.
    
    Hypothesis: OCI modulates the learning rate specifically for rewarded trials (Positive Feedback),
    while the learning rate for unrewarded trials (Negative Feedback) remains fixed. This tests
    if OCI relates to altered sensitivity to gains/success.
    
    Parameters:
    - beta: [0, 10] Softmax inverse temperature.
    - w: [0, 1] Weighting of Model-Based values.
    - lr_neg: [0, 1] Fixed learning rate for unrewarded trials.
    - lr_pos_base: [0, 1] Baseline learning rate for rewarded trials.
    - lr_pos_oci_slope: [-1, 1] Modulation of positive learning rate by OCI.
    """
    beta, w, lr_neg, lr_pos_base, lr_pos_oci_slope = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Calculate OCI-dependent Positive Learning Rate
    lr_pos = lr_pos_base + (lr_pos_oci_slope * participant_oci)
    # Clip to valid range [0, 1]
    lr_pos = np.clip(lr_pos, 0.0, 1.0)

    for trial in range(n_trials):
        if action_1[trial] == -1 or action_2[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Determine Learning Rate for this trial based on outcome
        if r > 0:
            current_lr = lr_pos
        else:
            current_lr = lr_neg

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Note: We use the outcome-dependent LR for both stages in this trial
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1[p_choice_1 > 0] + eps)) + np.sum(np.log(p_choice_2[p_choice_2 > 0] + eps)))
    return log_loss