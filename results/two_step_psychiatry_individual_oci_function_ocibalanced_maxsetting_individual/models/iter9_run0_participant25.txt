Here are three new cognitive models exploring different mechanisms for how OCI might influence decision-making in this task.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight (`w`)
This model assumes agents use a mixture of model-free (MF) and model-based (MB) strategies. The parameter `w` controls the balance (0 = pure MF, 1 = pure MB). Here, we hypothesize that OCI symptoms might relate to a tendency to over-rely on habitual (MF) or goal-directed (MB) control. We model `w` as a logistic function of OCI to keep it bounded between 0 and 1.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter (w)
    is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_base: [-5, 5] Base logit for the mixing weight w.
    - oci_w_mod: [-5, 5] Effect of OCI on the mixing weight logit.
    """
    learning_rate, beta, w_base, oci_w_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate mixing weight w using a sigmoid function to keep it in [0, 1]
    w_logit = w_base + (oci_score * oci_w_mod)
    w = 1.0 / (1.0 + np.exp(-w_logit))
    
    # Transition matrix (fixed for this task structure)
    # A -> 70% X (state 0), 30% Y (state 1)
    # U -> 30% X (state 0), 70% Y (state 1)
    # Rows: Actions (0, 1), Cols: States (0, 1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free Q-values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V_MB(s1, a) = Sum P(s'|s1,a) * max_a' Q(s', a')
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 Q-values (TD prediction error)
        # delta2 = r - Q(s2, a2)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values (Model-Free TD)
        # delta1 = Q(s2, a2) - Q(s1, a1)
        # Note: In standard TD(0), we update towards the value of the next state.
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with OCI-Modulated Learning Rate Asymmetry
This model investigates if OCI scores relate to a bias in learning from positive versus negative outcomes. High OCI might be associated with heightened sensitivity to punishment (or lack of reward) or reassurance seeking (reward). We split the learning rate into positive (`alpha_pos`) and negative (`alpha_neg`) components, where `alpha_neg` is modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with separate learning rates for positive and negative prediction errors.
    The negative learning rate is modulated by the OCI score.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    - oci_neg_mod: [-1, 1] Modulation of OCI on the negative learning rate.
    - beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_neg_mod, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate, bounded [0, 1]
    alpha_neg = alpha_neg_base + (oci_score * oci_neg_mod)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]

        # Stage 2 Update
        delta_2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if delta_2 > 0 else alpha_neg
        q_stage2[s_idx, a2] += lr_2 * delta_2

        # Stage 1 Update
        # Using the Q-value of the chosen second-stage action as the target (SARSA-like logic often used here)
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1] 
        lr_1 = alpha_pos if delta_1 > 0 else alpha_neg
        q_stage1[a1] += lr_1 * delta_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based Learner with OCI-Modulated Transition Learning Rate
Standard Model-Based RL assumes the transition matrix is fixed or perfectly known. However, participants often learn transition probabilities online. This model assumes the participant updates their internal model of the spaceship-planet transitions. We hypothesize that OCI might correlate with "hyper-vigilance" or rigidity in updating structural knowledge. OCI modulates the learning rate for the transition matrix (`lr_trans`).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner that learns the transition matrix online.
    The learning rate for updating the transition matrix is modulated by OCI.
    
    Parameters:
    - lr_reward: [0, 1] Learning rate for reward values (stage 2).
    - beta: [0, 10] Inverse temperature.
    - lr_trans_base: [0, 1] Base learning rate for transition probabilities.
    - oci_trans_mod: [-1, 1] Modulation of OCI on the transition learning rate.
    """
    lr_reward, beta, lr_trans_base, oci_trans_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective transition learning rate
    lr_trans = lr_trans_base + (oci_score * oci_trans_mod)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition matrix: Start with uniform prior or slight bias?
    # Let's assume they start knowing nothing (0.5/0.5) to maximize the effect of learning.
    # Dimensions: [Action, Next State] -> [2, 2]
    # transitions[a, s'] = P(s'|a)
    transitions = np.ones((2, 2)) * 0.5 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2 = np.zeros((2, 2)) # Q-values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Policy (Model-Based) ---
        # Calculate expected value of each stage 1 action based on current transition model
        # V(s') = max_a' Q(s', a')
        values_stage2 = np.max(q_stage2, axis=1) # Shape (2,)
        
        # Q_MB(a1) = sum_s' P(s'|a1) * V(s')
        q_stage1_mb = transitions @ values_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # 1. Update Reward Expectations (Stage 2)
        delta_r = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += lr_reward * delta_r
        
        # 2. Update Transition Model (Stage 1 to Stage 2)
        # We observed transition a1 -> s_idx
        # Update row a1: increase prob of s_idx, decrease prob of other
        # One-hot encoding of the observed state
        observed_trans = np.zeros(2)
        observed_trans[s_idx] = 1.0
        
        # Delta rule for probability estimation
        # P_new(s'|a) = P_old(s'|a) + lr_trans * (Indicator(s') - P_old(s'|a))
        trans_error = observed_trans - transitions[a1]
        transitions[a1] += lr_trans * trans_error
        
        # Ensure probabilities sum to 1 (numerical stability)
        transitions[a1] /= np.sum(transitions[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```