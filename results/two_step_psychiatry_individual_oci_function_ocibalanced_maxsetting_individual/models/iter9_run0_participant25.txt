Here are three cognitive models that incorporate the OCI score to explain individual variability in the two-step task.

### Model 1: OCI-Modulated Transition Belief (Subjective Probability)
This model hypothesizes that OCI symptoms affect the participant's internal model of the environment's structure. Specifically, OCI modulates the perceived probability of "common" transitions. High OCI might lead to rigidity (believing transitions are deterministic, $P \approx 1.0$) or doubt (believing transitions are random, $P \approx 0.5$). This "belief" parameter replaces the fixed 0.7 probability in the Model-Based planning step.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Transition Belief.
    
    Hypothesis: OCI scores modulate the strength of the participant's belief in the 
    transition structure (transition matrix). High OCI may lead to overly rigid beliefs 
    (perceiving common transitions as deterministic) or uncertainty/doubt (perceiving 
    them as more random).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for value estimation.
    - beta: [0, 10] Inverse temperature (exploration/exploitation balance).
    - w: [0, 1] Weighting of Model-Based vs Model-Free values.
    - p_base: [0, 1] Baseline perceived probability of common transitions.
    - p_oci: [-1, 1] Modulation of perceived probability by OCI score.
    """
    learning_rate, beta, w, p_base, p_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Perceived Transition Probability
    # We clip to [0, 1] to ensure valid probabilities.
    # p_common is the perceived probability of Action 0 -> State 0 (and 1 -> 1)
    p_common = np.clip(p_base + p_oci * current_oci, 0.0, 1.0)
    
    # Dynamic transition matrix based on OCI
    transition_matrix = np.array([[p_common, 1.0 - p_common], 
                                  [1.0 - p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Alien

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Policy for Choice 1 ---
        # Model-Based Value: Expected value of next states given transition belief
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        logits_1 = beta * q_net_1
        # Numerical stability shift
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for Choice 2 ---
        # Standard Model-Free Softmax on Aliens
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2 MF
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # TD(1) update for Stage 1 MF (Eligibility trace)
        q_stage1_mf[a1] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Side Bias (Fixed Preference)
The participant data shows a strong preference for Spaceship 0 (A). This model tests if OCI is associated with a static bias towards a specific "safe" or "habitual" spaceship, independent of reward history. This captures the tendency for compulsive behavior to manifest as rigid preferences or avoidance.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Side Bias.
    
    Hypothesis: OCI relates to a static bias or preference for one specific spaceship 
    (Spaceship 0), potentially viewed as a "safe" or default option. This bias acts 
    as a constant offset in the decision logits, distorting choice probabilities 
    regardless of learned values.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - bias_base: [-5, 5] Baseline bias towards Spaceship 0.
    - bias_oci: [-5, 5] Modulation of this bias by OCI score.
      (Positive total bias increases prob of choosing Spaceship 0).
    """
    learning_rate, beta, w, bias_base, bias_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Bias
    side_bias = bias_base + bias_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net_1
        
        # Apply Side Bias to Action 0
        logits_1[0] += side_bias
        
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for Choice 2 ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Directed Exploration (Recency Bonus)
This model proposes that OCI affects how participants value information or novelty. It adds an "exploration bonus" to actions based on how long ago they were last chosen. A positive weight implies directed exploration (checking neglected options), while a negative weight implies familiarity seeking (avoiding change). OCI modulates this weight.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Directed Exploration (Time-Since-Last-Chosen).
    
    Hypothesis: OCI affects the drive to explore or "check" alternative options.
    We add a bonus to the value of each spaceship proportional to the number of trials 
    since it was last chosen. High OCI might correlate with increased checking (positive weight)
    or avoidance of the unknown (negative weight).
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - expl_base: [-1, 1] Baseline exploration weight per trial lag.
    - expl_oci: [-1, 1] Modulation of exploration weight by OCI.
    """
    learning_rate, beta, w, expl_base, expl_oci = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Calculate Exploration Weight
    expl_weight = expl_base + expl_oci * current_oci
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last trial each action was chosen (init with -1)
    last_chosen = np.array([-1.0, -1.0])

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = int(reward[trial])

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate Exploration Bonus
        # Bonus = weight * (current_trial - last_time_chosen)
        # For the very first trial, lag is small or 0.
        lags = trial - last_chosen
        # Avoid exploding bonus on first few trials if initialized to -1
        # We treat never-chosen as a lag of 'trial'.
        
        exploration_bonus = expl_weight * lags
        
        logits_1 = beta * (q_net_1 + exploration_bonus)
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update last chosen tracker
        last_chosen[a1] = trial

        # --- Policy for Choice 2 ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        q_stage1_mf[a1] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```