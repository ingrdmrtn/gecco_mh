To explain the decision-making behavior of the participant with a medium OCI score, I propose three cognitive models. These models incorporate the OCI score to modulate specific mechanisms such as exploration noise, transition learning, and response to failure.

### Model 1: OCI-Modulated Stage 2 Precision
This model hypothesizes that obsessive-compulsive traits differentially affect the precision of choices at the planning stage (Stage 1) versus the outcome stage (Stage 2). Specifically, it posits that while the baseline inverse temperature ($\beta$) governs Stage 1, the Stage 2 precision is scaled by the OCI score. A high OCI might lead to more rigid (deterministic) behavior at the immediate reward stage.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Model with OCI-Modulated Stage 2 Precision.
    
    Hypothesis:
    OCI scores correlate with a difference in exploration/exploitation balance 
    between the two stages. This model uses a baseline beta for Stage 1, 
    but scales the beta for Stage 2 based on the OCI score. 
    High OCI may lead to hyper-precision (rigidity) in the final choice.
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta_base: [0, 10] Baseline inverse temperature (used for Stage 1).
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - beta_2_oci_k: [0, 10] Scaling factor for OCI's effect on Stage 2 beta.
      beta_stage2 = beta_base * (1 + beta_2_oci_k * oci)
    """
    lr, beta_base, w, beta_2_oci_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Define betas
    beta_1 = beta_base
    beta_2 = beta_base * (1.0 + beta_2_oci_k * oci_score)
    
    # Fixed transition matrix as per template assumption
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based value: Expected value of next stage's best option
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax with beta_1
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        # Softmax with beta_2 (OCI modulated)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        # Stage 1 MF update (SARSA-like using Stage 2 value)
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2

    return -log_likelihood
```

### Model 2: OCI-Modulated Transition Learning
This model hypothesizes that OCI scores influence the rigidity of the participant's internal model of the environment. Instead of a fixed transition matrix, the agent learns the transition probabilities over time. The rate of this learning is modulated by OCI: higher OCI scores result in a lower learning rate for transitions, reflecting a resistance to updating beliefs about environmental structure (rigidity).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Model with OCI-Modulated Transition Learning.
    
    Hypothesis:
    Participants learn the transition matrix over time, but the flexibility 
    of this belief update is inversely related to OCI symptoms. 
    High OCI leads to rigid beliefs (lower transition learning rate).
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - lr_trans_base: [0, 1] Base learning rate for transition matrix updates.
      lr_trans = lr_trans_base * (1.0 - oci)
    """
    lr, beta, w, lr_trans_base = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition learning rate decreases with OCI (rigidity)
    lr_trans = lr_trans_base * (1.0 - oci_score)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize transition matrix (start with prior or uniform, here using template prior)
    # T[a, s'] = P(s'|a)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Use current estimate of transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning Q-values ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # --- Learning Transitions ---
        # Update row a1: increase prob of s2, decrease prob of other
        # T[a1, s2] <- T[a1, s2] + alpha_t * (1 - T[a1, s2])
        transition_matrix[a1, s2] += lr_trans * (1.0 - transition_matrix[a1, s2])
        # The other state probability must be 1 - T[a1, s2] to sum to 1
        transition_matrix[a1, 1-s2] = 1.0 - transition_matrix[a1, s2]

    return -log_likelihood
```

### Model 3: OCI-Modulated "Loss-Chasing" Stickiness
This model investigates the relationship between OCI and compulsive repetition after failure. Unlike standard stickiness which applies to all repeated choices, this model adds a "stickiness" bonus to the previous action only if the previous trial resulted in no reward (0 coins). The magnitude of this effect is scaled by the OCI score, hypothesizing that higher OCI leads to a stronger urge to "correct" or repeat failed actions (chasing losses).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Model with OCI-Modulated "Loss-Chasing" Stickiness.
    
    Hypothesis:
    OCI is associated with compulsive repetition, specifically after negative outcomes 
    (chasing losses). This model adds a stickiness bonus to the previously chosen 
    Stage 1 action ONLY if the previous reward was 0. The magnitude of this 
    bonus is scaled by the OCI score.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - loss_stick_k: [0, 5] Scaling factor for stickiness after loss.
      stickiness = loss_stick_k * oci (Applied only if prev_reward == 0).
    """
    lr, beta, w, loss_stick_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    log_likelihood = 0.0
    eps = 1e-10
    
    prev_a1 = -1
    prev_reward = -1.0

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s2 = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply OCI-modulated stickiness only if previous trial was unrewarded
        stickiness_bonus = np.zeros(2)
        if prev_a1 != -1 and prev_reward == 0.0:
            stick_val = loss_stick_k * oci_score
            stickiness_bonus[prev_a1] = stick_val
        
        # Add bonus to Q-values before softmax
        q_net_stick = q_net + stickiness_bonus
        
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_likelihood += np.log(probs_1[a1] + eps)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s2, :])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_likelihood += np.log(probs_2[a2] + eps)

        # --- Learning ---
        delta_stage1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += lr * delta_stage2
        
        # Update history
        prev_a1 = a1
        prev_reward = r

    return -log_likelihood
```