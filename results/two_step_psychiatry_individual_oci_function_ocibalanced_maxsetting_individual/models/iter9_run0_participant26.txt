Here are three new cognitive models, expressed as Python functions, that incorporate the OCI score into the decision-making process in distinct, theoretically grounded ways.

### Model 1: OCI-Driven Stickiness Model
This model hypothesizes that obsessive-compulsive traits drive "stickiness" or choice perseverance (the tendency to repeat the previous action) regardless of reward history. The degree of stickiness is directly modulated by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MF/MB model with OCI-modulated Stickiness (Perseverance).
    
    Hypothesis: Participants with higher OCI scores exhibit higher choice perseverance 
    (stickiness) at the first stage, repeating choices regardless of reward.
    
    Parameters:
    - learning_rate: [0, 1] rate of Q-value updating.
    - beta: [0, 10] inverse temperature for softmax.
    - w: [0, 1] mixing weight (0=MF, 1=MB).
    - stick_sens: [0, 5] sensitivity of stickiness to OCI score. 
                  Stickiness = stick_sens * OCI.
    """
    learning_rate, beta, w, stick_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness proportional to OCI
    stickiness = stick_sens * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for stickiness
    prev_a1 = -1
    
    for trial in range(n_trials):
        # Handle missing data
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the integrated value of the previously chosen action
        q_final = q_integrated.copy()
        if prev_a1 != -1:
            q_final[prev_a1] += stickiness
            
        # Softmax
        exp_q1 = np.exp(beta * q_final)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        prev_a1 = a1

    eps = 1e-10
    # Filter valid trials for likelihood
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage 2 Rigidity
This model suggests that OCI is associated with behavioral rigidity (lower exploration) specifically in the second stage (execution/habitual level), while the first stage (planning level) remains more flexible.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MF/MB model with OCI-modulated Stage 2 Rigidity.
    
    Hypothesis: OCI is associated with rigidity (lower exploration/higher beta) 
    specifically in the second stage (closer to reward), while stage 1 planning 
    remains more flexible.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_stage1: [0, 10] inverse temperature for stage 1.
    - beta_stage2_gain: [0, 10] factor increasing stage 2 beta based on OCI.
                        beta_stage2 = beta_stage1 * (1 + beta_stage2_gain * OCI).
    - w: [0, 1] mixing weight.
    """
    learning_rate, beta_stage1, beta_stage2_gain, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate stage 2 beta: higher OCI -> higher beta2 (more deterministic)
    beta_stage2 = beta_stage1 * (1.0 + beta_stage2_gain * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        # --- Stage 1 Policy (beta_stage1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_stage1 * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy (beta_stage2) ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```

### Model 3: MB-Deficit with Eligibility Trace
This model posits that higher OCI is linked to a deficit in Model-Based control (reduced `w`), which is compensated for by a Model-Free system that uses eligibility traces (`lambda`) to assign credit to Stage 1 choices.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MF/MB model where OCI determines the Model-Based Deficit,
    compensated by Model-Free Eligibility Traces.
    
    Hypothesis: Higher OCI reduces the weight of Model-Based control (w),
    shifting behavior towards Model-Free control which utilizes eligibility 
    traces (lambda) to connect outcomes to stage 1 choices.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_max: [0, 1] Maximum model-based weight (when OCI=0).
                    w = w_max * (1 - OCI).
    - lambda_elig: [0, 1] Eligibility trace parameter for MF updates.
    """
    learning_rate, beta, w_max, lambda_elig = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # OCI reduces MB weight
    w = w_max * (1.0 - oci_score)
    # Clip w to be safe
    w = max(0.0, min(1.0, w))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        if action_1[trial] == -1 or state[trial] == -1 or action_2[trial] == -1:
            continue
            
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_integrated = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_integrated)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates with Eligibility Trace ---
        r = reward[trial]
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # MF Stage 1 Update: Includes both direct transition error and traced reward error
        # Update using eligibility trace (lambda)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)

    eps = 1e-10
    valid_mask = (action_1 != -1) & (state != -1) & (action_2 != -1)
    log_loss = -(np.sum(np.log(p_choice_1[valid_mask] + eps)) + np.sum(np.log(p_choice_2[valid_mask] + eps)))
    return log_loss
```