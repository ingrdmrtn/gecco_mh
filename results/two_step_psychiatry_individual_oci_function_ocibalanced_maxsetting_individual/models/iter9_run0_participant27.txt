Here are three new cognitive models based on the participant's data and OCI score.

### Model 1: Cumulative Habit Strength
This model hypothesizes that the participant's high "stickiness" (repeating choices) is not constant but accumulates over consecutive repetitions. The OCI score modulates how quickly this habit strength builds up. This captures the "deepening" nature of compulsive habits observed in the long blocks of repeated choices in the data.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Cumulative Habit Strength Model.
    
    Hypothesis: Stickiness is not static but grows with consecutive repetitions of the same choice.
    The rate of this habit growth is modulated by the OCI score.
    
    Parameters:
    lr: [0,1] - Learning rate for MF values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    stick_base: [0,5] - Base stickiness bonus.
    habit_step: [0,2] - Increment of stickiness per consecutive repeat.
    habit_oci: [-2,2] - OCI scaling factor for the habit growth rate.
    """
    lr, beta, w, stick_base, habit_step, habit_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective habit growth rate
    effective_habit_step = habit_step * (1.0 + oci_score * habit_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    last_action_1 = -1
    streak_count = 0

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Handle missing data
        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply cumulative stickiness
        if last_action_1 != -1:
            current_stickiness = stick_base + streak_count * effective_habit_step
            q_net_1[last_action_1] += current_stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update streak
        if a1 == last_action_1:
            streak_count += 1
        else:
            streak_count = 0
        last_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 MF update (TD(0))
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 1 MF update from Stage 2 RPE (Eligibility Trace / TD(1) proxy)
        # Using fixed lambda=1.0 for simplicity in this mechanism-focused model
        q_stage1_mf[a1] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Choice Trace Decay
This model implements stickiness as a decaying "choice trace" rather than a simple 1-trial memory. The OCI score modulates the *decay rate* of this trace. High OCI leads to slower decay, meaning past choices influence behavior for longer, explaining the persistence of the participant's choices.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Choice Trace Decay Model.
    
    Hypothesis: 'Stickiness' is a memory trace of past choices that decays over time.
    OCI modulates the decay rate: higher OCI leads to slower decay (longer-lasting habits).
    
    Parameters:
    lr: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF mixing weight.
    stick_amp: [0,5] - Amplitude of the choice trace influence.
    decay_base: [0,1] - Base decay rate (0=instant forget, 1=no decay).
    decay_oci: [-1,1] - OCI scaling for decay rate.
    """
    lr, beta, w, stick_amp, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate, clipped to [0, 0.99]
    effective_decay = decay_base * (1.0 + oci_score * decay_oci)
    effective_decay = np.clip(effective_decay, 0.0, 0.99)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Choice trace for the two spaceships
    choice_trace = np.zeros(2)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add choice trace bonus
        q_net_1 += stick_amp * choice_trace
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update choice trace
        choice_trace *= effective_decay
        choice_trace[a1] += 1.0

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        q_stage1_mf[a1] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Asymmetric Learning
This model posits that OCI affects how the participant learns from negative outcomes (avoidance) versus positive outcomes. It uses separate learning rates for positive and negative prediction errors, with the negative learning rate specifically scaled by the OCI score. This reflects the potential for compulsive individuals to be hypersensitive to failure or lack of reward.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Asymmetric Learning Model.
    
    Hypothesis: OCI scores relate to a bias in learning from negative prediction errors (avoidance).
    The model uses separate learning rates for positive and negative errors, with the 
    negative rate scaled by OCI.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg_base: [0,1] - Base learning rate for negative prediction errors.
    lr_neg_oci: [-2,2] - OCI scaling factor for the negative learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF mixing weight.
    stickiness: [0,5] - Basic choice stickiness.
    lam: [0,1] - Eligibility trace parameter.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w, stickiness, lam = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base * (1.0 + oci_score * lr_neg_oci)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 == -1 or s_idx == -1 or a2 == -1 or r == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if last_action_1 != -1:
            q_net_1[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        alpha1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha1 * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        alpha2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += alpha2 * delta_stage2
        
        # Stage 1 Eligibility Update (Stage 2 RPE propagating back)
        alpha1_elig = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha1_elig * lam * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```