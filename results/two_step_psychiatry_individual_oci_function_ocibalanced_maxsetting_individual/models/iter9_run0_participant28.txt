Here are three new cognitive models for the two-step task, incorporating the OCI score into different mechanisms: counterfactual learning, surprise-dependent model-based weighting, and uncertainty-based exploration.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Two-step model with Counterfactual Learning at Stage 2 modulated by OCI.
    
    This model assumes that participants may update the value of the unchosen alien 
    at the second stage based on a counterfactual outcome (e.g., if chosen got 0, unchosen would get 1).
    The learning rate for this counterfactual update is modulated by the OCI score.
    
    Parameters:
    learning_rate: [0,1] - Standard learning rate for chosen options
    beta: [0,10] - Inverse temperature for softmax
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0)
    p_stick: [0,5] - Choice stickiness for the first stage
    lambda_val: [0,1] - Eligibility trace for Stage 1 update
    cf_lr_base: [0,1] - Base learning rate for counterfactual updates
    cf_lr_oci: [0,1] - OCI modulation of counterfactual learning rate
    """
    learning_rate, beta, w, p_stick, lambda_val, cf_lr_base, cf_lr_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Modulate counterfactual learning rate by OCI
    # (cf_lr_oci - 0.5) allows for positive or negative correlation
    cf_lr = cf_lr_base + (cf_lr_oci - 0.5) * oci_score
    cf_lr = np.clip(cf_lr, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Shape: (State, Alien)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 MF update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * (r - q_stage2_mf[s_idx, a2])
        
        # Stage 2 Chosen update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 2 Counterfactual (Unchosen) update
        # Assume binary rewards: if r=1, counterfactual r=0; if r=0, counterfactual r=1
        unchosen_a2 = 1 - a2
        r_cf = 1.0 - r
        delta_cf = r_cf - q_stage2_mf[s_idx, unchosen_a2]
        q_stage2_mf[s_idx, unchosen_a2] += cf_lr * delta_cf
        
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Two-step model with Surprise-Dependent Model-Based Weighting modulated by OCI.
    
    The weighting parameter 'w' adapts based on whether the previous trial involved a 
    common or rare transition. OCI modulates the weight used after a rare transition,
    reflecting how 'surprise' or 'cognitive disruption' affects reliance on the model.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    p_stick: [0,5]
    lambda_val: [0,1]
    w_common: [0,1] - MB weight after a common transition
    w_rare_base: [0,1] - Base MB weight after a rare transition
    w_rare_oci: [0,1] - OCI modulation of the rare-transition weight
    """
    learning_rate, beta, p_stick, lambda_val, w_common, w_rare_base, w_rare_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w_rare modulated by OCI
    w_rare = w_rare_base + (w_rare_oci - 0.5) * oci_score
    w_rare = np.clip(w_rare, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    # Initialize previous transition as Common (to avoid bias on trial 1)
    prev_common = True 
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Determine current w based on previous transition type
        current_w = w_common if prev_common else w_rare
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = current_w * q_stage1_mb + (1 - current_w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * (r - q_stage2_mf[s_idx, a2])
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        
        # Update transition history
        # Common: (A->X or B->Y) => (0->0 or 1->1)
        # Rare: (A->Y or B->X) => (0->1 or 1->0)
        if a1 == s_idx:
            prev_common = True
        else:
            prev_common = False

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Two-step model with Uncertainty-Based Exploration (UCB) modulated by OCI.
    
    This model adds an exploration bonus to the Stage 1 choice values based on the 
    inverse square root of the visit counts for each spaceship. The magnitude of this 
    bonus (phi) is modulated by OCI, capturing intolerance of uncertainty or 
    exploratory drive.
    
    Parameters:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    p_stick: [0,5]
    lambda_val: [0,1]
    phi_base: [0,5] - Base exploration bonus weight
    phi_oci: [0,1] - OCI modulation of exploration bonus
    """
    learning_rate, beta, w, p_stick, lambda_val, phi_base, phi_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate exploration bonus weight
    # Scale modulation by 5.0 to match the scale of phi_base
    phi = phi_base + (phi_oci - 0.5) * oci_score * 5.0
    phi = np.clip(phi, -5.0, 5.0) # Allow for uncertainty avoidance (negative phi)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    counts_stage1 = np.zeros(2)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Uncertainty Bonus (UCB-like)
        # Add 1 to denominator to avoid division by zero
        uncertainty = 1.0 / np.sqrt(counts_stage1 + 1.0)
        
        logits_1 = beta * (q_net + phi * uncertainty)
        
        if last_action_1 != -1:
            logits_1[last_action_1] += p_stick
            
        exp_q1 = np.exp(logits_1 - np.max(logits_1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_val * (r - q_stage2_mf[s_idx, a2])
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        counts_stage1[a1] += 1
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```