Here are 3 new cognitive models exploring different mechanisms for how OCI might influence decision-making in this task.

### Model 1: Hybrid Learner with OCI-modulated "Stickiness" (Perseveration)
This model hypothesizes that OCI symptoms manifest as a tendency to repeat previous choices (perseveration or "stickiness"), regardless of the reward outcome. This is a common feature in compulsivity. The stickiness parameter is modulated by the participant's OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where choice perseveration (stickiness)
    is modulated by the OCI score.
    
    Hypothesis: High OCI leads to higher behavioral rigidity (stickiness), making 
    participants more likely to repeat the previous action regardless of value.

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    stick_base: [0, 5] Base level of choice stickiness.
    oci_stick_mod: [-5, 5] Modulation of stickiness by OCI score.
    """
    learning_rate, beta, w, stick_base, oci_stick_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective stickiness based on OCI
    # We use an exponential or linear mapping, here simple linear addition
    # Stickiness adds a bonus to the previously chosen action in the softmax
    stickiness = stick_base + (oci_stick_mod * oci_score)
    
    # Initialize Q-values
    # Stage 1: 2 actions
    q_mf = np.zeros(2)      # Model-free Q-values for stage 1
    q_mb = np.zeros(2)      # Model-based Q-values for stage 1
    
    # Stage 2: 2 states, 2 actions
    q_stage2 = np.zeros((2, 2)) 
    
    # Transition matrix (fixed for this task structure, usually learned but kept fixed here for simplicity)
    # A -> X (0.7), A -> Y (0.3); U -> Y (0.7), U -> X (0.3)
    # We map A=0, U=1; X=0, Y=1.
    # T[action, next_state]
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_action_1 = -1 # No previous action for first trial

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value Calculation
        # V_MB(a1) = sum(P(s2|a1) * max(Q_stage2(s2, :)))
        max_q2 = np.max(q_stage2, axis=1) # Max value of each stage 2 state
        q_mb = trans_probs @ max_q2 # Matrix multiplication
        
        # Integrated Value
        q_net = w * q_mb + (1 - w) * q_mf
        
        # Add stickiness bonus to the net value before softmax
        logits = beta * q_net
        if prev_action_1 != -1:
            logits[prev_action_1] += stickiness
            
        exp_q = np.exp(logits - np.max(logits)) # Subtract max for numerical stability
        probs_1 = exp_q / np.sum(exp_q)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        
        # Simple Model-Free choice at stage 2
        logits_2 = beta * q_stage2[s_idx]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (TD(0))
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        # Stage 1 Update (TD(1) / SARSA-like)
        # In standard 2-step models, MF values at stage 1 are updated by the stage 2 prediction error
        # and the stage 1 prediction error (which is usually 0 if gamma=1 and no intermediate reward).
        # Often simplified to: Q_MF(a1) += lr * (Q_stage2(s2, a2) - Q_MF(a1)) + lr * lambda * pe_2
        # Here we use a standard simple update towards the outcome.
        
        # Update MF stage 1 based on the value of the state reached (TD(0))
        # V(s2) is approximated by Q(s2, a2)
        pe_1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * pe_1
        
        # Eligibility trace: Stage 1 choice also gets credit for the final reward prediction error
        q_mf[a1] += learning_rate * pe_2 # Lambda=1 implicitly here for simplicity
        
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Mixing Weight ($w$) with Transition Learning
This model hypothesizes that OCI affects the balance between Model-Based (goal-directed) and Model-Free (habitual) control. Specifically, high OCI might be associated with a deficit in Model-Based control (lower $w$), relying more on habits. Alternatively, they might over-think (higher $w$). We model $w$ as a logistic function of OCI to keep it bound between 0 and 1.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where the balance between Model-Based and Model-Free control (w)
    is modulated by the OCI score using a logistic function.
    
    Hypothesis: OCI score predicts the reliance on model-based planning.
    
    Parameters:
    learning_rate: [0, 1] Learning rate for Q-values.
    beta: [0, 10] Inverse temperature.
    w_intercept: [-5, 5] Intercept for the logistic function determining w.
    w_slope: [-5, 5] Slope for the logistic function determining w (sensitivity to OCI).
    lambda_param: [0, 1] Eligibility trace parameter for MF updates.
    """
    learning_rate, beta, w_intercept, w_slope, lambda_param = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate w based on OCI using logistic function to ensure it stays in [0, 1]
    # w = 1 / (1 + exp(-(intercept + slope * oci)))
    logit_w = w_intercept + (w_slope * oci_score)
    w = 1.0 / (1.0 + np.exp(-logit_w))
    
    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 ---
        # Model-Based
        max_q2 = np.max(q_stage2, axis=1)
        q_mb = trans_probs @ max_q2
        
        # Combined
        q_net = w * q_mb + (1 - w) * q_mf
        
        exp_q = np.exp(beta * q_net)
        probs_1 = exp_q / np.sum(exp_q)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 RPE
        delta_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * delta_2
        
        # Stage 1 MF update
        # TD(0) error at stage 1: Value of chosen stage 2 state - Value of chosen stage 1 action
        delta_1 = q_stage2[s_idx, a2] - q_mf[a1]
        q_mf[a1] += learning_rate * delta_1
        
        # Eligibility trace passing stage 2 outcome back to stage 1
        q_mf[a1] += learning_rate * lambda_param * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Asymmetry (Loss Aversion)
This model tests if OCI is linked to altered sensitivity to negative outcomes (losses/omission of reward) vs positive outcomes. It implements a split learning rate where `lr_pos` and `lr_neg` are derived from a base rate and a bias parameter. The *bias* itself is modulated by OCI.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive vs negative prediction errors.
    The asymmetry (bias) is modulated by OCI.
    
    Hypothesis: High OCI individuals might be hyper-sensitive to "failure" (0 reward),
    learning more from negative RPEs than positive ones.
    
    Parameters:
    lr_base: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    bias_base: [0, 1] Base bias towards positive (if >0.5) or negative (if <0.5) learning.
    oci_bias_sens: [-1, 1] How strongly OCI shifts the bias.
    w: [0, 1] Model-based weight (fixed effect, not modulated).
    """
    lr_base, beta, bias_base, oci_bias_sens, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective bias
    # We center OCI roughly around 0.5 for stability or just use raw score
    # bias = bias_base + sensitivity * oci
    # We clip bias to [0.01, 0.99] to ensure valid learning rates
    raw_bias = bias_base + (oci_bias_sens * oci_score)
    bias = np.clip(raw_bias, 0.01, 0.99)
    
    # Define lr_pos and lr_neg based on bias
    # If bias is high, lr_pos is high relative to lr_neg
    # We scale them so they center around lr_base roughly
    lr_pos = lr_base * 2 * bias
    lr_neg = lr_base * 2 * (1 - bias)
    
    # Cap at 1.0
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    q_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 ---
        # MB
        max_q2 = np.max(q_stage2, axis=1)
        q_mb = trans_probs @ max_q2
        
        # Net
        q_net = w * q_mb + (1 - w) * q_mf
        
        exp_q = np.exp(beta * q_net)
        probs_1 = exp_q / np.sum(exp_q)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 PE
        delta_2 = r - q_stage2[s_idx, a2]
        
        # Asymmetric Update Stage 2
        lr_2 = lr_pos if delta_2 >= 0 else lr_neg
        q_stage2[s_idx, a2] += lr_2 * delta_2
        
        # Stage 1 PE
        delta_1 = q_stage2[s_idx, a2] - q_mf[a1]
        
        # Asymmetric Update Stage 1 (TD(0))
        lr_1 = lr_pos if delta_1 >= 0 else lr_neg
        q_mf[a1] += lr_1 * delta_1
        
        # Eligibility trace (simplified, using stage 2 lr)
        q_mf[a1] += lr_2 * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```