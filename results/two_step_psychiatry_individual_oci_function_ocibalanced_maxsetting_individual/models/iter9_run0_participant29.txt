Here are three new cognitive models that incorporate the OCI score into the decision-making process, specifically exploring mechanisms related to perseveration, uncertainty modulation, and separate learning rates for positive/negative outcomes which might interact with obsessive-compulsive traits.

### Model 1: OCI-Modulated Perseveration (Sticky Choice)
This model hypothesizes that OCI symptoms relate to "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of the outcome or value. High OCI might lead to higher stickiness (compulsive repetition). We modulate a stickiness bonus added to the Q-values based on the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Perseveration.
    
    This model adds a 'stickiness' parameter that biases the agent to repeat 
    their previous choice. The magnitude of this stickiness is modulated by the 
    participant's OCI score, testing the hypothesis that OCI relates to 
    habitual or compulsive repetition.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based (1) vs Model-free (0) weight.
    - stick_base: [0, 5] Base stickiness bonus.
    - stick_oci_sens: [0, 5] Sensitivity of stickiness to OCI score.
    """
    learning_rate, beta, w, stick_base, stick_oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective stickiness based on OCI
    stickiness = stick_base + (stick_oci_sens * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness (initialize with -1 for no previous choice)
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the net Q-values
        q_net_stick = q_net.copy()
        if prev_action_1 != -1:
            q_net_stick[int(prev_action_1)] += stickiness
            
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Update Stage 1 MF
        # Using the standard SARSA-like update for stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Store action for next trial's stickiness
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Model-Based/Model-Free Trade-off (W-parameter)
This model tests if the balance between goal-directed (Model-Based) and habitual (Model-Free) control is shifted by OCI symptoms. Specifically, it posits that `w` is a linear function of the OCI score. This is a classic hypothesis in computational psychiatry where compulsivity is linked to deficits in model-based control or over-reliance on habits.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated MB/MF Balance.
    
    This model assumes the mixing weight 'w' (determining the balance between 
    Model-Based and Model-Free control) is directly modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_intercept: [0, 1] Baseline mixing weight.
    - w_oci_slope: [-1, 1] How OCI changes w. (Negative = OCI reduces MB control).
    - eligibility: [0, 1] Eligibility trace for stage 1 update.
    """
    learning_rate, beta, w_intercept, w_oci_slope, eligibility = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate w and constrain to [0, 1]
    w = w_intercept + (w_oci_slope * oci_score)
    w = min(1.0, max(0.0, w))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 MF update using eligibility trace
        # Instead of just Q(S2), we use the RPE from stage 2 to update stage 1
        # Q_MF(S1) = Q_MF(S1) + lr * (Q(S2) - Q(S1)) + lr * lambda * delta_stage2
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1] # TD error at stage 1
        
        # Standard TD update for stage 1
        q_stage1_mf[a1] += learning_rate * delta_stage1 
        
        # Eligibility trace update: Stage 1 choice also learns from Stage 2 reward outcome
        q_stage1_mf[a1] += learning_rate * eligibility * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Asymmetric Learning Rates (Pos/Neg)
This model investigates if OCI affects sensitivity to positive versus negative prediction errors differently. It proposes that the learning rate for negative prediction errors (disappointments) is modulated by OCI, while the positive learning rate is fixed or separate. This captures the idea that individuals with higher OCI might be hyper-sensitive to errors or failures.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Asymmetric Learning Rates.
    
    This model separates learning rates for positive (better than expected) and 
    negative (worse than expected) prediction errors. The negative learning rate 
    is modulated by OCI, testing if OCI is associated with over-learning from 
    negative outcomes (e.g., punishment sensitivity or error monitoring).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - lr_neg_oci: [-1, 1] Modulation of negative LR by OCI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    """
    lr_pos, lr_neg_base, lr_neg_oci, beta, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    lr_neg = lr_neg_base + (lr_neg_oci * oci_score)
    lr_neg = min(1.0, max(0.0, lr_neg))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Apply asymmetric learning rates for Stage 2
        eff_lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += eff_lr_s2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Apply asymmetric learning rates for Stage 1
        eff_lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += eff_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```