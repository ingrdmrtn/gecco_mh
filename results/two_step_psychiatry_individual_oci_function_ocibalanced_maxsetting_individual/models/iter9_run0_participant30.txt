Here are 3 new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: OCI-Modulated Eligibility Traces
This model hypothesizes that high OCI scores relate to a stronger "memory" or eligibility trace for the first-stage choice when updating based on the second-stage outcome. People with high compulsivity might over-attribute the final outcome to the initial choice, effectively blurring the distinction between the two stages (a higher eligibility trace lambda).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Eligibility Traces.
    
    Hypothesis: The eligibility trace (lambda), which controls how much the second-stage 
    prediction error updates the first-stage value, is modulated by OCI. 
    High OCI might lead to higher lambda (attributing outcome more to the first choice).
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Weighting between model-based and model-free
    lambda_base: [0, 1] - Base eligibility trace
    lambda_oci_slope: [-1, 1] - Modulation of lambda by OCI
    """
    learning_rate, beta, w, lambda_base, lambda_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-specific lambda
    lambda_param = lambda_base + (lambda_oci_slope * oci_score)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        if r != -1:
            # Stage 2 Prediction Error
            delta_stage2 = r - q_stage2_mf[s, a2]
            
            # Stage 1 Prediction Error (TD(0))
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            
            # Update Stage 2 values
            q_stage2_mf[s, a2] += learning_rate * delta_stage2
            
            # Update Stage 1 values using eligibility trace (TD(lambda))
            # The update combines the immediate Stage 1 error and the propagated Stage 2 error scaled by lambda
            q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_param * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Biased Transition Learning
This model proposes that high OCI participants might have a distorted view of the transition probabilities (the model of the world). Instead of using the objective fixed matrix (0.7/0.3), they might learn the transitions but with a bias, or perhaps their OCI score makes them more or less sensitive to rare transitions when building their model-based values. Here, we model a dynamic transition matrix update where the learning rate for the transition matrix is modulated by OCI.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Biased Transition Learning Rate.
    
    Hypothesis: High OCI participants might be more rigid or conversely more hypersensitive 
    to transition structure changes. This model allows the transition matrix to be learned 
    (instead of fixed), with the learning rate for these transitions modulated by OCI.
    
    Bounds:
    lr_reward: [0, 1] - Learning rate for reward values (MF)
    beta: [0, 10]
    w: [0, 1]
    lr_trans_base: [0, 1] - Base learning rate for transition matrix
    lr_trans_oci_slope: [-1, 1] - OCI modulation of transition learning
    """
    lr_reward, beta, w, lr_trans_base, lr_trans_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate transition learning rate
    lr_trans = lr_trans_base + (lr_trans_oci_slope * oci_score)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)

    # Initialize transition counts or probabilities. 
    # Starting with a uniform prior or the true prior? Let's start with true prior but allow drift.
    # We represent the probability of transitioning to state 0 given action 0, etc.
    # T[action, next_state]
    # Actually, the task defines: A -> X (common), U -> Y (common).
    # Let's track p(State=0 | Action=0) and p(State=1 | Action=1).
    # Initial belief is 0.7 for common transitions.
    prob_a0_s0 = 0.7
    prob_a1_s1 = 0.7
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Construct current transition matrix based on beliefs
        # T[0] is probs for Action 0 -> [State 0, State 1]
        # T[1] is probs for Action 1 -> [State 0, State 1]
        T = np.zeros((2, 2))
        T[0, 0] = prob_a0_s0
        T[0, 1] = 1 - prob_a0_s0
        T[1, 1] = prob_a1_s1
        T[1, 0] = 1 - prob_a1_s1

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB value calculation using learned T
        q_stage1_mb = np.zeros(2)
        q_stage1_mb[0] = T[0] @ max_q_stage2
        q_stage1_mb[1] = T[1] @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        if r != -1:
            # Value Updating (MF)
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += lr_reward * delta_stage2
            
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += lr_reward * delta_stage1

            # Transition Updating (State Prediction Error)
            # If a1=0, did we go to s=0? 1 if yes, 0 if no.
            if a1 == 0:
                outcome = 1 if s == 0 else 0
                prob_a0_s0 += lr_trans * (outcome - prob_a0_s0)
            elif a1 == 1:
                outcome = 1 if s == 1 else 0
                prob_a1_s1 += lr_trans * (outcome - prob_a1_s1)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Dependent Pruning
This model hypothesizes that high OCI leads to a form of "safety behavior" or pruning where low-value options are disregarded more aggressively. In this model, the inverse temperature parameter (beta), which governs the exploration-exploitation trade-off, is modulated by OCI. Specifically, high OCI might be associated with a higher beta (lower temperature), implying very deterministic choices and an aversion to exploring the seemingly suboptimal path, even if the environment is stochastic.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Pruning (Exploration Suppression).
    
    Hypothesis: OCI modulates the inverse temperature (beta). High OCI participants 
    may be more averse to exploration (higher beta), sticking rigidly to the 
    perceived best option and exploring less.
    
    Bounds:
    learning_rate: [0, 1]
    w: [0, 1]
    beta_base: [0, 10]
    beta_oci_slope: [-5, 5] - How much OCI increases/decreases beta (determinism)
    stickiness: [0, 5] - Choice stickiness (repetition bias)
    """
    learning_rate, w, beta_base, beta_oci_slope, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective beta
    beta_eff = beta_base + (beta_oci_slope * oci_score)
    # Ensure beta stays within reasonable positive bounds (0 to 20 for stability)
    beta_eff = np.clip(beta_eff, 0.0, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness
    last_action_1 = -1

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        logits_1 = beta_eff * q_net_s1
        if last_action_1 != -1:
            logits_1[last_action_1] += stickiness
            
        # Softmax with numerical stability
        max_logit = np.max(logits_1)
        exp_q1 = np.exp(logits_1 - max_logit)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        # Note: Beta is applied here too. If OCI increases rigidity, it likely applies to both stages.
        logits_2 = beta_eff * q_stage2_mf[s]
        max_logit_2 = np.max(logits_2)
        exp_q2 = np.exp(logits_2 - max_logit_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        if r != -1:
            delta_stage2 = r - q_stage2_mf[s, a2]
            q_stage2_mf[s, a2] += learning_rate * delta_stage2
            
            delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```