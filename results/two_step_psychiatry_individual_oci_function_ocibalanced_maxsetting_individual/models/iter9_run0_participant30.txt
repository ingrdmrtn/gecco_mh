Here are three new cognitive models for the two-step task, incorporating OCI scores into distinct mechanisms: **Model-Based Pessimism**, **Dynamic Transition Learning**, and **Stage 2 Stickiness**.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    MB/MF Model with OCI-Modulated Pessimism (Risk Aversion).
    
    Hypothesis: High OCI individuals exhibit 'pessimism' or risk aversion in their 
    Model-Based planning. Instead of assuming the best outcome from the second stage 
    (Max Q), they weight the worst outcome (Min Q) proportionally to their OCI severity.
    This reflects a "worry" mechanism where potential failures loom larger than successes.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - pessimism_scale: [0, 1] Scaling factor for pessimism.
      The weight on Min_Q is calculated as: tau = pessimism_scale * OCI.
    """
    learning_rate, beta, w, pessimism_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Pessimism factor constrained to [0, 1]
    tau = min(max(pessimism_scale * oci_score, 0.0), 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10
    
    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy (Model-Based Step)
        # Standard MB: V(s') = max(Q(s', :))
        # Pessimistic MB: V(s') = (1-tau)*max(Q) + tau*min(Q)
        
        max_q2 = np.max(q_stage2_mf, axis=1)
        min_q2 = np.min(q_stage2_mf, axis=1)
        
        # Weighted average of best and worst case scenarios for the next stage
        v_stage2 = (1.0 - tau) * max_q2 + tau * min_q2
        
        q_stage1_mb = transition_matrix @ v_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Dynamic Transition Learning Model with OCI-Modulated Update Rate.
    
    Hypothesis: High OCI individuals do not rely on fixed transition probabilities 
    (0.7/0.3) but continuously update their internal model of the spaceship-planet 
    transitions. The rate of this update is driven by OCI, reflecting uncertainty 
    intolerance or hyper-responsiveness to recent transition outcomes (instability).
    
    Parameters:
    - learning_rate: [0, 1] Q-value learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - trans_lr_scale: [0, 1] Scaling for transition learning rate.
      The transition learning rate is: alpha_t = trans_lr_scale * OCI.
    """
    learning_rate, beta, w, trans_lr_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition learning rate specific to the internal model
    alpha_t = min(max(trans_lr_scale * oci_score, 0.0), 1.0)

    # Initialize subjective transition matrix (rows=actions, cols=states)
    # Start with the true priors, but allow them to drift based on alpha_t
    subj_trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy (MB uses subjective dynamic matrix)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subj_trans_matrix @ max_q_stage2

        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Update Q-values
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Subjective Transition Matrix
        # T(a1, :) moves towards the observed state (one-hot vector)
        target = np.zeros(2)
        target[s_idx] = 1.0
        subj_trans_matrix[a1] = subj_trans_matrix[a1] + alpha_t * (target - subj_trans_matrix[a1])

    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-Modulated Stage 2 Stickiness.
    
    Hypothesis: OCI-related compulsivity manifests as perseveration (stickiness) 
    specifically at the second stage (choice of alien), distinct from Stage 1 planning.
    High OCI individuals are more likely to repeat their previous alien choice 
    at a given planet, regardless of reward history.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - s2_stick_scale: [0, 5] Scaling for Stage 2 stickiness bonus.
      Bonus = s2_stick_scale * OCI.
    """
    learning_rate, beta, w, s2_stick_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    stick_bonus = s2_stick_scale * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Track last choice for each state (Planet 0, Planet 1)
    # Initialize with -1 (no previous choice)
    last_a2 = np.array([-1, -1]) 
    
    log_loss = 0.0
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)

        # Stage 2 Policy with Stickiness
        q2_vals = q_stage2_mf[s_idx].copy()
        
        # Add stickiness bonus if we have a previous choice for this specific planet
        if last_a2[s_idx] != -1:
            q2_vals[last_a2[s_idx]] += stick_bonus
            
        exp_q2 = np.exp(beta * q2_vals)
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)

        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update history
        last_a2[s_idx] = a2

    return log_loss
```