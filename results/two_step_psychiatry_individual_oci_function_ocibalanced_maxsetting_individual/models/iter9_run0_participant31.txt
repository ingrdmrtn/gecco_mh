Here are three new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1: OCI-Modulated Epsilon-Greedy Mixture (Lapse Rate)
This model hypothesizes that high OCI scores modulate the "lapse rate" or "checking behavior" (random exploration). While the agent primarily uses a Model-Based/Model-Free mixture (softmax), they occasionally choose randomly with probability $\epsilon$. The OCI score determines whether this random exploration is more or less frequent (e.g., high OCI might lead to rigid adherence to values, i.e., low epsilon, or anxious checking, i.e., high epsilon).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Epsilon-Greedy Mixture.
    
    The policy is a mixture of a Softmax distribution (derived from MB/MF values)
    and a Uniform distribution (random lapse). The weight of the random component (epsilon)
    is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0,1] Learning rate for Q-value updates.
    - beta: [0,10] Inverse temperature for the Softmax component.
    - w: [0,1] Weighting between Model-Based (1) and Model-Free (0) control.
    - eps_base: [0,1] Baseline epsilon (lapse rate).
    - eps_oci: [0,1] Modulation of epsilon by OCI.
                     Mapped such that 0.5 is neutral, >0.5 increases eps, <0.5 decreases eps.
    """
    learning_rate, beta, w, eps_base, eps_oci = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
    
    # Map eps_oci to range [-0.5, 0.5] to allow positive or negative modulation
    # Epsilon is clamped between 0 and 1
    epsilon = eps_base + (eps_oci - 0.5) * oci
    epsilon = np.clip(epsilon, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Probability
        logits = beta * (q_net - np.max(q_net)) # Subtract max for stability
        exp_q = np.exp(logits)
        prob_softmax = exp_q / np.sum(exp_q)
        
        # Mixture with Uniform (Lapse)
        # P(a) = (1 - eps) * P_softmax(a) + eps * 0.5
        prob_final = (1 - epsilon) * prob_softmax + epsilon * 0.5
        
        p_choice_1[trial] = prob_final[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        q_vals_2 = q_stage2_mf[s_idx]
        
        logits_2 = beta * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        prob_softmax_2 = exp_q2 / np.sum(exp_q2)
        
        # Apply same epsilon to stage 2
        prob_final_2 = (1 - epsilon) * prob_softmax_2 + epsilon * 0.5
        p_choice_2[trial] = prob_final_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Update (SARSA-like for MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps_safe = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps_safe)) + np.sum(np.log(p_choice_2 + eps_safe)))
    return log_loss
```

### Cognitive Model 2: OCI-Modulated Off-Policy Weight (Kappa)
This model hypothesizes that OCI affects the rigidity of the learning update rule. Specifically, it modulates the parameter $\kappa$ (kappa) in the Stage 1 Model-Free update. A $\kappa$ of 1 corresponds to Q-learning (updating based on the *best* possible Stage 2 value), while a $\kappa$ of 0 corresponds to SARSA (updating based on the *actually chosen* Stage 2 value). High OCI might correlate with SARSA-like behavior (learning from habit/path taken) or Q-learning (learning optimal structure).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Off-Policy Weight (Kappa).
    
    Modulates the mixture between Q-learning (max over next state actions) and 
    SARSA (value of chosen next action) for the Stage 1 Model-Free update.
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w: [0,1]
    - kappa_base: [0,1] Baseline weight for Q-learning (1=Q-learning, 0=SARSA).
    - kappa_oci: [0,1] Modulation of kappa by OCI.
                       Mapped such that 0.5 is neutral.
    """
    learning_rate, beta, w, kappa_base, kappa_oci = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
    
    # Map kappa_oci to modulation range
    kappa = kappa_base + (kappa_oci - 0.5) * 2.0 * oci
    kappa = np.clip(kappa, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * (q_net - np.max(q_net))
        exp_q = np.exp(logits)
        probs_1 = exp_q / np.sum(exp_q)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate update target based on Kappa
        # Target = (1-kappa)*Q(s', a') + kappa*max(Q(s', :))
        value_chosen = q_stage2_mf[s_idx, a2]
        value_best = np.max(q_stage2_mf[s_idx])
        
        target_stage1 = (1.0 - kappa) * value_chosen + kappa * value_best
        
        delta_stage1 = target_stage1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: OCI-Modulated Initial Optimism (Q0)
This model hypothesizes that OCI levels correlate with the participant's initial expectations (Optimism vs Pessimism). Instead of starting all Q-values at 0, the initial values $Q_0$ are modulated by OCI. High initial values encourage early exploration (as observed rewards of 0 are "disappointing" relative to expectation), while low initial values discourage exploration.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Initial Q-Values (Optimism/Pessimism).
    
    Sets the initial Q-values for all states and actions based on the OCI score.
    High Q0 implies optimism (encouraging exploration), low Q0 implies pessimism.
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w: [0,1]
    - q0_base: [0,1] Baseline initial value.
    - q0_oci: [0,1] Modulation of initial value by OCI.
                    Mapped such that 0.5 is neutral.
    """
    learning_rate, beta, w, q0_base, q0_oci = model_parameters
    n_trials = len(action_1)
    oci = oci[0]
    
    # Calculate Initial Q-value (Q0)
    # Map q0_oci to modulation effect
    q0 = q0_base + (q0_oci - 0.5) * 2.0 * oci
    q0 = np.clip(q0, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values with Q0 instead of 0
    q_stage1_mf = np.full(2, q0)
    q_stage2_mf = np.full((2, 2), q0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * (q_net - np.max(q_net))
        exp_q = np.exp(logits)
        probs_1 = exp_q / np.sum(exp_q)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        q_vals_2 = q_stage2_mf[s_idx]
        logits_2 = beta * (q_vals_2 - np.max(q_vals_2))
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```