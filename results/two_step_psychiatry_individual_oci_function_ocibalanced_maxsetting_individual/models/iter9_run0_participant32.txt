To explain the behavior of the participant with high OCI symptoms, I propose three cognitive models. These models investigate different mechanisms—compulsive repetition after failure, asymmetric learning from negative outcomes, and stage-specific decision precision—to capture how obsessive-compulsive traits might alter decision-making dynamics in this two-step task.

### Model 1: OCI-Modulated Loss-Stay Stickiness
This model tests the hypothesis that high OCI scores are associated with **compulsive repetition of actions that led to failure** (loss-stay), distinguishing this behavior from the adaptive "win-stay" strategy. In this model, if the previous trial resulted in no reward (0 coins), a "stickiness" bonus is added to the value of the previously chosen spaceship for the current trial. The magnitude of this bonus is scaled by the participant's OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Loss-Stay Stickiness Model.
    
    Hypothesis: High OCI participants exhibit 'compulsive' stickiness, where they are more likely 
    to repeat a choice specifically after receiving no reward (a loss), contrary to standard 
    reinforcement learning which suggests switching.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature parameter for softmax choice rule.
    - w: [0, 1] Weighting parameter between Model-Based (1) and Model-Free (0) systems.
    - loss_stick_oci: [0, 5] Stickiness bonus applied to the previous action if the result was a loss, scaled by OCI.
    """
    learning_rate, beta, w, loss_stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Loss-Stay Stickiness
        stickiness = np.zeros(2)
        if last_action_1 != -1 and last_reward == 0.0:
            stickiness[int(last_action_1)] = loss_stick_oci * oci_score
        
        exp_q1 = np.exp(beta * (q_net + stickiness))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Stage 1 Update (SARSA-style)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Zero-Outcome Learning Amplification
This model posits that high OCI leads to an **over-reaction to negative outcomes** (zero rewards). Participants with high OCI scores may perceive the lack of gold as a salient error signal, prompting them to update their value estimates more aggressively than when they receive a reward. This is modeled by amplifying the learning rate specifically when the reward is 0.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Zero-Outcome Learning Amplification.
    
    Hypothesis: High OCI participants exhibit a 'negativity bias' in learning, where the learning rate 
    is amplified when the outcome is a loss (0 coins). This leads to faster value changes after 
    failures compared to successes.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate (applied when reward is 1).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based system.
    - zero_lr_amp_oci: [0, 10] Multiplier for the learning rate when reward is 0, scaled by OCI.
    """
    learning_rate, beta, w, zero_lr_amp_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Determine effective learning rate for this trial
        current_lr = learning_rate
        if r == 0.0:
            # Amplify learning rate for zero outcomes based on OCI
            current_lr = learning_rate * (1.0 + zero_lr_amp_oci * oci_score)
            # Clip to ensure stability
            if current_lr > 1.0:
                current_lr = 1.0
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 2 Precision
This model suggests that high OCI participants exhibit **hyper-precision or rigidity specifically in the second stage** of the task (choosing the alien). While the planning stage (choosing the spaceship) might be subject to broader exploration or model-based weighting, the proximal choice of the alien is executed with higher determinism (higher beta) as OCI increases, reflecting a "narrowing of focus" or perfectionism at the point of reward consumption.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Precision Model.
    
    Hypothesis: High OCI participants show increased decision precision (higher inverse temperature) 
    specifically in the second stage of the task. This reflects a rigid, exploitative focus when 
    selecting the final reward source, distinct from the planning occurring in the first stage.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Base inverse temperature (used for Stage 1).
    - w: [0, 1] Weight of Model-Based system.
    - beta_s2_scale_oci: [0, 5] Scaling factor for Stage 2 beta. Stage 2 beta = beta * (1 + scale * OCI).
    """
    learning_rate, beta, w, beta_s2_scale_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate Stage 2 specific beta
    beta_stage2 = beta * (1.0 + beta_s2_scale_oci * oci_score)
    
    for trial in range(n_trials):
        # --- Stage 1 Policy (uses base beta) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy (uses OCI-scaled beta) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```