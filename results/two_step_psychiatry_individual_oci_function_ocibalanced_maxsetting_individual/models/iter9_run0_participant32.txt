Here are three new cognitive models based on the participant data and OCI scores.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Dynamic Transition Learning.
    Unlike standard models that assume fixed transition probabilities (e.g., 0.7/0.3),
    this model learns the transition structure online. OCI modulates the learning rate
    for this transition matrix update, testing if high OCI leads to rigid structural 
    beliefs (low learning rate) or hyper-sensitivity to recent transitions (high learning rate).
    
    Bounds:
    lr_q: [0,1]
    beta: [0,10]
    w: [0,1]
    lr_trans_base: [0,1]
    lr_trans_oci_coeff: [-1, 1]
    """
    lr_q, beta, w, lr_trans_base, lr_trans_oci_coeff = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate transition learning rate
    alpha_T = lr_trans_base + (lr_trans_oci_coeff * oci_score)
    alpha_T = np.clip(alpha_T, 0.0, 1.0)
    
    # Initialize transition probability (probability of common transition)
    # Starts flat (0.5) representing no initial knowledge
    p_common = 0.5
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # Construct current transition matrix based on learned p_common
        # Row 0 (Act 0): [p_common, 1-p_common] (Common is 0->0)
        # Row 1 (Act 1): [1-p_common, p_common] (Common is 1->1)
        T = np.array([[p_common, 1-p_common], 
                      [1-p_common, p_common]])
        
        # Model-Based Value Calculation using dynamic T
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T @ max_q_stage2
        
        # Integrated Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage 1 Choice Probability
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # Stage 2 Choice Probability
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update Q-values (SARSA/TD)
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_q * delta_stage2
        
        # Stage 1 update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_q * delta_stage1
        
        # Update Transition Probability
        # If transition was common (0->0 or 1->1), target is 1. Else 0.
        is_common = (a1 == s_idx)
        target = 1.0 if is_common else 0.0
        p_common += alpha_T * (target - p_common)
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with Stage-Specific Inverse Temperatures.
    This model posits that OCI affects decision noise (beta) differently at the planning stage (Stage 1)
    versus the simple bandit stage (Stage 2). High OCI might induce specific rigidity or noise 
    during the complex model-based planning phase.
    
    Bounds:
    lr: [0,1]
    w: [0,1]
    beta_stage2: [0,10]
    beta_stage1_base: [0,10]
    beta_stage1_oci_mod: [-5, 5]
    """
    lr, w, beta_stage2, beta_stage1_base, beta_stage1_oci_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 1 Beta
    beta_stage1 = beta_stage1_base + (beta_stage1_oci_mod * oci_score)
    beta_stage1 = np.clip(beta_stage1, 0.0, 10.0)
    
    # Fixed transition matrix for standard Hybrid model
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # MB Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage 1 Choice (using beta_stage1)
        exp_q1 = np.exp(beta_stage1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # Stage 2 Choice (using beta_stage2)
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with "Pain of Omission" (Negative Reinforcement).
    This model hypothesizes that high OCI participants perceive the absence of reward (0 coins)
    not as a neutral event, but as a loss (negative utility). This aligns with clinical 
    observations of avoidance and fear of failure in OC symptoms.
    
    Bounds:
    lr: [0,1]
    beta: [0,10]
    w: [0,1]
    loss_magnitude_base: [0, 5]
    loss_magnitude_oci_slope: [0, 5]
    """
    lr, beta, w, loss_magnitude_base, loss_magnitude_oci_slope = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective loss magnitude for 0 reward
    loss_mag = loss_magnitude_base + (loss_magnitude_oci_slope * oci_score)
    loss_mag = np.max([0.0, loss_mag]) # Ensure magnitude is positive (so outcome is negative)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # MB Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage 1 Choice
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # Stage 2 Choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Subjective Reward Transformation
        # If reward is 0, it is perceived as -loss_mag
        if r == 1:
            r_eff = 1.0
        else:
            r_eff = -1.0 * loss_mag
            
        # Updates using effective reward
        delta_stage2 = r_eff - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```