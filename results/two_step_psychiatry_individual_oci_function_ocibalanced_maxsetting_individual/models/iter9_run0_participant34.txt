Here are the 3 proposed cognitive models.

### Model 1: OCI-Modulated Model-Based Pessimism
This model tests the hypothesis that high OCI scores are associated with anxiety-driven pessimism during model-based planning. While a standard agent calculates the value of a spaceship based on the probability-weighted value of the aliens on the destination planet (or the max value), a pessimistic agent disproportionately weights the worst-case scenario (the lower-value alien) when estimating the value of the planet.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Model-Based Pessimism.
    
    Hypothesis:
    High OCI individuals exhibit risk aversion/pessimism in planning. 
    When calculating the Model-Based value of a spaceship (Stage 1), they 
    underweight the best outcome (max Q of aliens) and overweight the 
    worst outcome (min Q of aliens) on the prospective planet.
    
    Parameters:
    learning_rate: [0,1] - Value updating rate.
    beta: [0,10] - Inverse temperature (softmax sensitivity).
    w: [0,1] - MB/MF mixing weight.
    lambda_elig: [0,1] - Eligibility trace for Stage 1 update.
    oci_pessimism: [0,1] - Degree of pessimism scaled by OCI. 
                           0 = Standard Max/Avg; 1 = Min value dominance.
    
    Total parameters: 5
    """
    learning_rate, beta, w, lambda_elig, oci_pessimism = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    # Pessimism weight calculation (clamped [0,1])
    pess_w = oci_pessimism * current_oci
    if pess_w > 1.0: pess_w = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        
        # Model-Based Valuation with Pessimism
        # Standard MB uses max(Q_stage2) for the value of the state.
        # Pessimistic MB interpolates between max and min.
        max_q2 = np.max(q_stage2_mf, axis=1)
        min_q2 = np.min(q_stage2_mf, axis=1)
        
        # Value of the state is a mixture of best and worst case
        mixed_q2 = (1 - pess_w) * max_q2 + pess_w * min_q2
        
        q_stage1_mb = transition_matrix @ mixed_q2
        
        # Net Stage 1 Values
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage 1 Choice Probability
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Stage 2 Choice
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Learning
        # Stage 1 PE
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Stage 2 PE
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Update
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Fictive Learning (Counterfactuals)
This model proposes that high OCI scores lead to obsessive "what if" thinking (counterfactual processing). When the participant receives a reward (or lack thereof), they not only update the chosen option but also update the *unchosen* option based on the inverse outcome. This simulates the belief that "if I had chosen the other spaceship, I would have gotten the opposite result."

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Fictive Learning (Counterfactual Updating).
    
    Hypothesis:
    High OCI is linked to obsessive doubt and counterfactual thinking.
    This model updates the unchosen Stage 1 action based on the fictive 
    outcome (1 - reward). If the chosen action yielded 0, the agent 
    assumes the unchosen one would have yielded 1, and vice versa.
    
    Parameters:
    learning_rate: [0,1] - Standard learning rate for chosen action.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF mixing weight.
    lambda_elig: [0,1] - Eligibility trace.
    oci_fictive_weight: [0,1] - Weight of the fictive update scaled by OCI.
    
    Total parameters: 5
    """
    learning_rate, beta, w, lambda_elig, oci_fictive_weight = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # Policy Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Policy Stage 2
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Standard Update (Chosen)
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Fictive Update (Unchosen Stage 1)
        # Assume the unchosen path would have yielded the opposite reward (1-R).
        # We approximate the PE for the unchosen as: (1 - Reward) - Q_unchosen
        # This is a simplified fictive update at the outcome level.
        unchosen_a1 = 1 - a1
        fictive_outcome = 1.0 - reward[trial]
        delta_fictive = fictive_outcome - q_stage1_mf[unchosen_a1]
        
        fictive_lr = learning_rate * oci_fictive_weight * current_oci
        q_stage1_mf[unchosen_a1] += fictive_lr * delta_fictive

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Ritual Bias (Accumulating Win-Stay)
This model extends the "Win-Stay" concept to a continuous, accumulating "ritual" bias. Instead of just checking the previous trial, this model maintains a separate "ritual strength" trace that increases with every reward and decays slowly over time. High OCI leads to stronger accumulation or persistence of this ritualistic preference, explaining long streaks of behavior that are resistant to occasional losses.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Ritual Bias (Accumulating Win-Stay).
    
    Hypothesis:
    High OCI individuals form ritualistic habits where rewarded actions 
    build up a 'ritual value' that decays slowly. Unlike simple Win-Stay 
    (which resets every trial), this bias accumulates, making the agent 
    resistant to switching even after a loss (perseveration), as observed 
    in the participant's data (streaks).
    
    Parameters:
    learning_rate: [0,1] - Value updating rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF mixing weight.
    lambda_elig: [0,1] - Eligibility trace.
    ritual_decay: [0,1] - Decay rate of the ritual bias (persistence).
    oci_ritual_amp: [0,5] - Magnitude of the ritual boost per reward, scaled by OCI.
    
    Total parameters: 6
    """
    learning_rate, beta, w, lambda_elig, ritual_decay, oci_ritual_amp = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Ritual trace vector (one for each Stage 1 action)
    ritual_trace = np.zeros(2)

    for trial in range(n_trials):
        
        # Policy Stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q combines MB, MF, and the Ritual Bias
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + ritual_trace
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Policy Stage 2
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # Standard RL Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_elig * delta_stage2)
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Ritual Trace Update
        # Decay the trace for both actions
        ritual_trace *= ritual_decay
        
        # If rewarded, boost the trace for the chosen action
        if reward[trial] == 1.0:
            ritual_trace[a1] += oci_ritual_amp * current_oci

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```