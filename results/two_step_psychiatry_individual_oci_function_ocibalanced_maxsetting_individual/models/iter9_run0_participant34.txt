Here are three new cognitive models that incorporate the OCI score to explain the participant's behavior in the two-step task.

### Model 1: Hybrid RL with OCI-Modulated Model-Based Weight (`cognitive_model1`)
This model assumes that individuals with high OCI scores might rely more on habitual (model-free) control rather than goal-directed (model-based) planning, or vice versa. In this specific formulation, we test the hypothesis that high OCI leads to a *deficit* in model-based control (a common finding in compulsivity research), meaning their `w` parameter (mixing weight) is suppressed by their OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL with OCI-Modulated Model-Based Weight.
    
    Hypothesis: High OCI scores are associated with reduced model-based control 
    (goal-directed planning) and increased reliance on model-free (habitual) control.
    The mixing weight 'w' is dynamically scaled down by the OCI score.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature for softmax.
    w_max: [0, 1] The maximum possible weight for model-based control (at OCI=0).
    oci_w_suppression: [0, 1] How strongly OCI reduces the model-based weight w.
    """
    learning_rate, beta, w_max, oci_w_suppression = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective w: higher OCI reduces w towards 0 (pure model-free)
    # If oci_w_suppression is 0, w is constant at w_max.
    # If oci_w_suppression is 1 and OCI is 1, w becomes 0.
    w = w_max * (1.0 - (oci_score * oci_w_suppression))
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Update Stage 2 Q-values (Model-Free)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 Q-values (Model-Free via TD-error from stage 2 value)
        # Note: In standard hybrid models, MF stage 1 updates from the *value* of the state reached.
        # Often implemented as Q(s1,a1) <- Q(s1,a1) + alpha * (Q(s2, a2) - Q(s1,a1)) + alpha * lambda * delta2
        # Here we use a simplified TD(0) update common in these simplified template tasks:
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates with OCI-Driven Punishment Sensitivity (`cognitive_model2`)
This model posits that OCI relates to an altered sensitivity to negative outcomes (missing rewards). Instead of a single learning rate, the model splits learning into positive (`alpha_pos`) and negative (`alpha_neg`) updates. The OCI score specifically amplifies the learning rate for negative prediction errors, reflecting a "fear of failure" or hyper-responsiveness to errors often seen in anxiety and OCD.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Asymmetric Learning Rates with OCI-Driven Punishment Sensitivity.
    
    Hypothesis: High OCI individuals are hyper-sensitive to errors (missing rewards).
    They learn more drastically from 0 outcomes than from 1 outcomes.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    alpha_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    oci_neg_amplification: [0, 5] Multiplier for how much OCI boosts the negative learning rate.
    beta: [0, 10] Inverse temperature.
    """
    alpha_pos, alpha_neg_base, oci_neg_amplification, beta = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Effective negative learning rate scales with OCI
    alpha_neg_effective = alpha_neg_base * (1.0 + oci_score * oci_neg_amplification)
    alpha_neg_effective = np.clip(alpha_neg_effective, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Pure Model-Free for this mechanism to isolate learning rate effects
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += alpha_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += alpha_neg_effective * delta_stage2
            
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += alpha_pos * delta_stage1
        else:
            q_stage1_mf[a1] += alpha_neg_effective * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Avoidance with OCI-Scaling (`cognitive_model3`)
This model introduces an "uncertainty penalty" to the value of actions. Compulsivity is often linked to intolerance of uncertainty. Here, uncertainty is modeled simply as the variance or inverse of the visit count (or unreliability). For simplicity in this template, we track how often an action has been taken recently (using a decay trace). Actions taken less frequently are "uncertain" and penalized. The OCI score determines the magnitude of this penalty.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Uncertainty-Avoidance with OCI-Scaling.
    
    Hypothesis: High OCI individuals are intolerant of uncertainty.
    They penalize actions that they haven't visited recently (high uncertainty).
    This promotes repetitive, "safe" behavior.
    
    Parameters:
    learning_rate: [0, 1] Update rate for Q-values.
    beta: [0, 10] Inverse temperature.
    trace_decay: [0, 1] How fast the 'familiarity' trace decays (1=keeps memory, 0=instantly forgets).
    oci_uncertainty_penalty: [0, 5] Penalty subtracted from Q-values for low familiarity, scaled by OCI.
    """
    learning_rate, beta, trace_decay, oci_uncertainty_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Familiarity traces: Start at 1 (assume initial familiarity) or 0.
    # Higher value = more familiar = less uncertainty penalty.
    familiarity_s1 = np.ones(2) 
    familiarity_s2 = np.ones((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Calculate uncertainty penalty: (1 - familiarity) * penalty_factor
        # If familiarity is 1, penalty is 0. If familiarity is 0, penalty is max.
        unc_pen_1 = (1.0 - familiarity_s1) * (oci_score * oci_uncertainty_penalty)
        
        q_net_1 = q_stage1_mf - unc_pen_1
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        unc_pen_2 = (1.0 - familiarity_s2[s_idx]) * (oci_score * oci_uncertainty_penalty)
        
        q_net_2 = q_stage2_mf[s_idx] - unc_pen_2
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning & Trace Update ---
        # Update familiarity traces
        # Decay all
        familiarity_s1 *= trace_decay
        familiarity_s2 *= trace_decay
        # Boost chosen
        familiarity_s1[a1] += (1.0 - trace_decay) # Simple update towards 1
        familiarity_s2[s_idx, a2] += (1.0 - trace_decay)
        
        # Clip to [0, 1] just in case
        familiarity_s1 = np.clip(familiarity_s1, 0, 1)
        familiarity_s2 = np.clip(familiarity_s2, 0, 1)

        # Standard Q-learning
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```