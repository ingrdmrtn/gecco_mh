Here are three new cognitive models for the two-step task, incorporating the OCI score to explain individual variability in decision-making.

### Model 1: Distorted Transition Beliefs (The "Doubting" Model)
This model hypothesizes that high OCI participants doubt the stability or reliability of the transition structure. While the true common transition probability is 0.7, high OCI individuals perceive this probability as lower (closer to chance, 0.5), effectively dampening the Model-Based system's ability to plan accurately.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where OCI distorts the Model-Based transition probabilities.
    High OCI participants may doubt the stability of the transition structure,
    effectively flattening the transition matrix used for MB planning.

    Parameters:
    - learning_rate: [0, 1] for MF value updates
    - beta: [0, 10] inverse temperature
    - w: [0, 1] weight of Model-Based system (0=MF, 1=MB)
    - doubt_k: [0, 0.5] how much OCI reduces the perceived common transition probability.
               p_common = 0.7 - (doubt_k * oci). Lower bound 0.5.
    """
    learning_rate, beta, w, doubt_k = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Distort transition matrix based on OCI
    # Base is 0.7. If doubt_k is high and OCI is high, p approaches 0.5 (random)
    p_common = 0.7 - (doubt_k * oci_score)
    if p_common < 0.5: 
        p_common = 0.5
    
    transition_matrix = np.array([[p_common, 1-p_common], [1-p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
        else:
            p_choice_1[trial] = 0.5 

        # Stage 2 Policy
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_mf2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[int(action_2[trial])]
        else:
            p_choice_2[trial] = 0.5

        # Updates
        if action_1[trial] != -1 and action_2[trial] != -1:
            a1 = int(action_1[trial])
            a2 = int(action_2[trial])
            r = reward[trial]
            
            # SARSA/Q-learning at Stage 2
            delta_2 = r - q_mf2[s_curr, a2]
            q_mf2[s_curr, a2] += learning_rate * delta_2
            
            # TD(1) update for Stage 1 MF values
            delta_1 = q_mf2[s_curr, a2] - q_mf1[a1]
            q_mf1[a1] += learning_rate * delta_1
            q_mf1[a1] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Rigidity (The "Proximal Anxiety" Model)
This model hypothesizes that OCI-related anxiety manifests as increased rigidity (higher inverse temperature `beta`) specifically at the second stage, when the reward is proximal. The participant may be flexible in planning (Stage 1) but becomes compulsively exploitative or rigid once on the planet (Stage 2).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model with OCI-modulated Stage 2 Rigidity.
    Hypothesis: High OCI participants exhibit higher inverse temperature (rigidity/exploitation)
    specifically at the second stage (proximal to reward), while stage 1 remains flexible.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_1: [0, 10] Stage 1 inverse temperature
    - w: [0, 1] Model-based weight
    - rigidity_k: [0, 10] OCI scaling factor for Stage 2 beta boost.
    - stickiness: [0, 5] Choice perseveration bonus for Stage 1.
    """
    learning_rate, beta_1, w, rigidity_k, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stage 2 beta is boosted by OCI
    beta_2 = beta_1 + (rigidity_k * oci_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # Stage 1
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Add stickiness to the net values
        q_net_stick = q_net.copy()
        if last_action_1 != -1:
            q_net_stick[last_action_1] += stickiness
        
        exp_q1 = np.exp(beta_1 * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
            last_action_1 = int(action_1[trial])
        else:
            p_choice_1[trial] = 0.5
            last_action_1 = -1

        # Stage 2 - Uses beta_2
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta_2 * q_mf2[s_curr]) 
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[int(action_2[trial])]
        else:
            p_choice_2[trial] = 0.5

        # Updates
        if action_1[trial] != -1 and action_2[trial] != -1:
            a1 = int(action_1[trial])
            a2 = int(action_2[trial])
            r = reward[trial]
            
            delta_2 = r - q_mf2[s_curr, a2]
            q_mf2[s_curr, a2] += learning_rate * delta_2
            
            delta_1 = q_mf2[s_curr, a2] - q_mf1[a1]
            q_mf1[a1] += learning_rate * delta_1
            q_mf1[a1] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Static Ritual Bias (The "Safe Option" Model)
This model hypothesizes that compulsivity (OCI) creates a static, intrinsic preference (a "ritual") for one specific option (here, Spaceship A/Action 0) that is independent of reward history. This bias is added to the computed Q-values, skewing choices even when value learning suggests otherwise.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model with OCI-driven Static Ritual Bias.
    Hypothesis: High OCI participants have a fixed, intrinsic preference (ritual)
    for one of the Stage 1 options (e.g., Spaceship A/0), acting as a static bias
    regardless of reward history.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - bias_mag: [0, 5] Magnitude of bias towards Action 0 scaled by OCI. 
    """
    learning_rate, beta, w, bias_mag = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_mf1 = np.zeros(2)
    q_mf2 = np.zeros((2, 2))

    # Calculate static bias
    # bias applied to action 0 (Spaceship A)
    ritual_bias = bias_mag * oci_score

    for trial in range(n_trials):
        # Stage 1
        max_q_mf2 = np.max(q_mf2, axis=1)
        q_mb1 = transition_matrix @ max_q_mf2
        
        q_net = w * q_mb1 + (1 - w) * q_mf1
        
        # Apply bias to Action 0
        q_net[0] += ritual_bias
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        if action_1[trial] != -1:
            p_choice_1[trial] = probs_1[int(action_1[trial])]
        else:
            p_choice_1[trial] = 0.5

        # Stage 2
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_mf2[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        if action_2[trial] != -1:
            p_choice_2[trial] = probs_2[int(action_2[trial])]
        else:
            p_choice_2[trial] = 0.5

        # Updates
        if action_1[trial] != -1 and action_2[trial] != -1:
            a1 = int(action_1[trial])
            a2 = int(action_2[trial])
            r = reward[trial]
            
            delta_2 = r - q_mf2[s_curr, a2]
            q_mf2[s_curr, a2] += learning_rate * delta_2
            
            delta_1 = q_mf2[s_curr, a2] - q_mf1[a1]
            q_mf1[a1] += learning_rate * delta_1
            q_mf1[a1] += learning_rate * delta_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```