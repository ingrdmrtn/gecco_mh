Here are three new cognitive models implemented as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Fixed Option Bias Model.
    
    Hypothesis: High OCI participants exhibit a static compulsion or preference for 
    a specific spaceship (Spaceship 0), independent of reward history. This is modeled 
    as a static bias added to the logit of Spaceship 0. This explains the strong 
    tendency to choose spaceship 0 regardless of outcomes.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - stick: [0, 5] Perseveration bonus.
    - bias_0_base: [-2, 2] Base bias for Spaceship 0.
    - bias_0_oci: [-2, 2] OCI scaling for the bias.
    """
    learning_rate, beta, w, stick, bias_0_base, bias_0_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Static bias for Action 0
    bias_0 = bias_0_base + (bias_0_oci * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        # Add Stickiness
        if prev_a1 != -1:
            logits_1[int(prev_a1)] += stick
            
        # Add Option Bias to Spaceship 0
        logits_1[0] += bias_0
        
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Planning Pessimism Model.
    
    Hypothesis: High OCI participants engage in 'worst-case' planning due to anxiety. 
    Instead of calculating the Model-Based value using the maximum Q-value of the 
    next state (assuming optimal future choice), they incorporate the minimum Q-value 
    (pessimism). The weight of the minimum value increases with OCI.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stick: [0, 5]
    - pessimism_base: [0, 1] Base weight for min(Q) in MB calculation.
    - pessimism_oci: [-1, 1] OCI scaling for pessimism.
    """
    learning_rate, beta, w, stick, pessimism_base, pessimism_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate pessimism weight rho
    rho = pessimism_base + (pessimism_oci * oci_score)
    rho = np.clip(rho, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Pessimistic Valuation: V(s') = (1-rho)*max(Q) + rho*min(Q)
        max_q = np.max(q_stage2_mf, axis=1)
        min_q = np.min(q_stage2_mf, axis=1)
        val_stage2 = (1 - rho) * max_q + rho * min_q
        
        q_stage1_mb = transition_matrix @ val_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if prev_a1 != -1:
            logits_1[int(prev_a1)] += stick
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
        q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Repetition-Induced Learning Freeze Model.
    
    Hypothesis: High OCI participants become rigid when repeating choices. 
    As they repeat the same spaceship choice, their learning rate for that choice 
    decays (freezes), preventing them from updating values based on new (negative) 
    outcomes. This leads to sticking with an option even when rewards stop.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10]
    - w: [0, 1]
    - stick: [0, 5]
    - freeze_base: [0, 1] Base freeze factor per repetition.
    - freeze_oci: [0, 1] OCI scaling for freeze factor.
    """
    learning_rate, beta, w, stick, freeze_base, freeze_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    freeze_factor = freeze_base + (freeze_oci * oci_score)
    freeze_factor = np.clip(freeze_factor, 0.0, 0.9) # Cap to avoid excessively fast total freeze
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    streak_count = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits_1 = beta * q_net
        
        if prev_a1 != -1:
            logits_1[int(prev_a1)] += stick
            
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        # Update streak
        if a1 == prev_a1:
            streak_count += 1
        else:
            streak_count = 0
            
        # Calculate effective learning rate for Stage 1
        # Decay lr based on streak length
        lr_eff = learning_rate * ((1.0 - freeze_factor) ** streak_count)
        
        q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
        q_stage1_mf[a1] += lr_eff * (r - q_stage1_mf[a1])
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```