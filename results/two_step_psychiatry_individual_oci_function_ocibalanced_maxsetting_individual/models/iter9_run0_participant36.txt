Here are 3 new cognitive models based on the participant's data and OCI score.

### Cognitive Model 1
**Hypothesis:** This model tests the "habitual rigidity" hypothesis often associated with OCD. It proposes that individuals with high OCI scores rely more heavily on Model-Free (habitual) learning specifically when updating Stage 1 values, effectively decoupling the second-stage outcome from the first-stage choice more than low OCI individuals. Instead of a standard `w` parameter mixing MB and MF values during *choice*, this model implements an OCI-modulated eligibility trace (`lambda`) in a TD(lambda) framework. A higher OCI score might lead to a lower `lambda`, meaning the Stage 1 value update is less sensitive to the specific Stage 2 reward prediction error (RPE), making behavior more "habitual" (driven by Stage 1 RPEs) rather than goal-directed (linking Stage 2 outcomes back to Stage 1).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: TD(lambda) with OCI-modulated Eligibility Trace.
    Hypothesis: High OCI reduces the eligibility trace parameter (lambda), making the 
    agent less likely to propagate Stage 2 Reward Prediction Errors back to Stage 1 
    choices. This simulates a disconnection between outcomes and initial choices 
    (habitual behavior).

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature (softmax).
    - lambda_base: [0, 1] Base eligibility trace parameter.
    - oci_lambda_damp: [0, 1] Amount by which OCI score reduces lambda.
    """
    lr, beta, lambda_base, oci_lambda_damp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # OCI reduces the eligibility trace, making updates more purely MF (Step 1 driven)
    # lambda_eff = lambda_base * (1 - OCI * damp)
    lambda_eff = np.clip(lambda_base * (1.0 - (oci_score * oci_lambda_damp)), 0.0, 1.0)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State (Planet) x Action (Alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        # Stage 1 Prediction Error (TD error at transition)
        # Note: In standard TD(lambda) for this task, V(s2) is usually max(Q(s2)) or Q(s2, a2)
        # Here we use Q(s2, a2) as the estimate of the value of the next state
        delta_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        
        # Stage 2 Prediction Error (TD error at outcome)
        delta_2 = r - q_stage2[s_idx, a2]

        # Update Stage 2 Q-values
        q_stage2[s_idx, a2] += lr * delta_2
        
        # Update Stage 1 Q-values
        # Q1 is updated by its own error (delta_1) AND a fraction of the stage 2 error (lambda * delta_2)
        q_stage1[a1] += lr * (delta_1 + lambda_eff * delta_2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2
**Hypothesis:** This model investigates "Compulsive Checking" behavior. It posits that high OCI individuals have a specific bias towards the "rare" transition path if they haven't visited it recently, mimicking a checking compulsion (e.g., "I must verify the other state"). This is modeled as a "curiosity" or "uncertainty" bonus added to the Model-Based calculation. The `oci_check_bonus` parameter scales with the OCI score, increasing the value of actions leading to the state that was *not* visited in the previous trial, conflicting with the standard maximization of reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Model with OCI-modulated 'Checking' Bonus.
    Hypothesis: High OCI leads to 'checking' behavior. The agent receives an artificial 
    value bonus for the spaceship that leads to the planet NOT visited on the previous 
    trial. This competes with standard reward maximization.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - check_bonus_base: [0, 2] Base bonus for switching states.
    - oci_check_gain: [0, 2] Scaling of checking bonus by OCI score.
    """
    lr, beta, w, check_bonus_base, oci_check_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    check_bonus = check_bonus_base + (oci_score * oci_check_gain)
    
    # Transition matrix (fixed for MB)
    # 0->0 (Common), 0->1 (Rare); 1->1 (Common), 1->0 (Rare)
    # Row 0: Spaceship A -> [Planet X, Planet Y]
    # Row 1: Spaceship B -> [Planet X, Planet Y]
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_state = -1 # No previous state on trial 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Max value of each planet
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Checking Bonus: Add value to the action most likely to take us to the OTHER state
        bonus_vec = np.zeros(2)
        if last_state != -1:
            target_state = 1 - last_state # The state we didn't visit
            # Which action leads to target_state?
            # Action 0 goes to 0 (0.7) and 1 (0.3). Action 1 goes to 0 (0.3) and 1 (0.7).
            if target_state == 0:
                # Action 0 is the "highway" to state 0
                bonus_vec[0] = check_bonus
            else:
                # Action 1 is the "highway" to state 1
                bonus_vec[1] = check_bonus

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + bonus_vec
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Update Stage 2 (MF)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Update Stage 1 (MF)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        last_state = s_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3
**Hypothesis:** This model explores "Asymmetric Learning from Rare Events." Standard MB learning uses the transition matrix to invert values. However, OCI might be linked to a distrust of "luck" or rare outcomes. This model modifies the Model-Based update. If a Rare transition occurs, the agent with high OCI might "discount" the information gained from that trial when calculating future MB values, treating it as noise rather than signal. We implement this by modulating the mixing weight `w` dynamically: `w` is temporarily suppressed on the trial following a rare transition, scaled by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Dynamic Model-Based Weighting (Rare Transition Suppression).
    Hypothesis: High OCI individuals may distrust rare transitions, viewing them as 
    unreliable 'noise'. This model reduces the Model-Based weight (w) specifically 
    on the trial *after* a rare transition occurred, reverting to habitual (MF) control.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_base: [0, 1] Baseline mixing weight for Common transitions.
    - rare_suppression_base: [0, 1] Base reduction of w after rare transition.
    - oci_suppression_gain: [0, 1] Additional reduction of w by OCI after rare.
    """
    lr, beta, w_base, rare_suppression_base, oci_suppression_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    was_rare = False

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        # Calculate current trial's w
        current_w = w_base
        if was_rare:
            suppression = rare_suppression_base + (oci_score * oci_suppression_gain)
            suppression = np.clip(suppression, 0.0, 1.0)
            current_w = w_base * (1.0 - suppression)
        
        q_net = current_w * q_stage1_mb + (1 - current_w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Determine if transition was rare ---
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # Common: (A0 -> S0) or (A1 -> S1). Rare: (A0 -> S1) or (A1 -> S0).
        if (a1 == 0 and s_idx == 1) or (a1 == 1 and s_idx == 0):
            was_rare = True
        else:
            was_rare = False

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        a2 = int(action_2[trial])
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```