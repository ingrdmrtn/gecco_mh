Here are three cognitive models formulated as Python functions. Each model incorporates the OCI score to explain the participant's decision-making process, specifically focusing on mechanisms related to habit formation, persistence, and learning asymmetry.

### Model 1: Hybrid Model with OCI-Modulated Forgetting
This model posits that high OCI scores are associated with a "pruning" mechanism where unchosen options are forgotten more rapidly. This creates a feedback loop where the chosen option's relative value increases simply because alternatives degrade, leading to rigid, habitual behavior.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Forgetting of Unchosen Options.
    
    Hypothesis: High OCI individuals prune (forget) the value of unchosen options 
    more aggressively. This decay isolates the chosen habit, making it dominant 
    over time regardless of small fluctuations in reward.
    
    Decay rate = forget_scale * OCI.
    
    Parameters:
    - learning_rate: [0,1] Learning rate for Q-value updates.
    - beta: [0,10] Inverse temperature for softmax choice.
    - w: [0,1] Weight of Model-Based system (0=MF, 1=MB).
    - lam: [0,1] Eligibility trace parameter.
    - stickiness: [0,10] Choice stickiness bonus (last trial).
    - forget_scale: [0,1] Sensitivity of forgetting rate to OCI score.
    """
    learning_rate, beta, w, lam, stickiness, forget_scale = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    # Fixed transition matrix for the task
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    # Calculate decay rate based on OCI
    decay = forget_scale * oci_score
    if decay > 1.0: decay = 1.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Choice ---
        max_q_mf_2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q_mf_2
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        # Stage 1 Update
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        # Forgetting Unchosen Stage 1
        unchosen_a1 = 1 - a1
        q_mf_1[unchosen_a1] *= (1.0 - decay)
        
        # Stage 2 Update
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        # Forgetting Unchosen Stage 2
        unchosen_a2 = 1 - a2
        q_mf_2[s_idx, unchosen_a2] *= (1.0 - decay)
        
        # Eligibility Trace
        q_mf_1[a1] += learning_rate * lam * delta_2
        
        prev_action_1 = a1
        
    return log_loss
```

### Model 2: Hybrid Model with OCI-Modulated Resistance to Extinction
This model suggests that high OCI is linked to a resistance to extinction. Specifically, when the participant experiences a negative prediction error (e.g., getting 0 coins when expecting something), they dampen the learning rate. This prevents the value of the chosen option from dropping, maintaining the behavior despite lack of reward.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Resistance to Extinction.
    
    Hypothesis: High OCI individuals dampen the learning from negative prediction 
    errors (outcomes worse than expected). This leads to persistence (compulsivity) 
    even when the action yields no reward.
    
    Effective learning rate is reduced by (1 - damp * OCI) when delta < 0.
    
    Parameters:
    - learning_rate: [0,1] Base learning rate.
    - beta: [0,10] Inverse temperature.
    - w: [0,1] Model-Based weight.
    - lam: [0,1] Eligibility trace.
    - stickiness: [0,10] Choice stickiness.
    - neg_damp_oci: [0,1] Factor dampening learning from negative PEs based on OCI.
    """
    learning_rate, beta, w, lam, stickiness, neg_damp_oci = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    log_loss = 0.0
    eps = 1e-10
    prev_action_1 = -1
    
    damp_factor = neg_damp_oci * oci_score
    if damp_factor > 1.0: damp_factor = 1.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Choice ---
        max_q_mf_2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q_mf_2
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1
        
        if prev_action_1 != -1:
            q_net_1[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning with Asymmetry ---
        
        # Stage 1 PE
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        lr_1 = learning_rate
        if delta_1 < 0:
            lr_1 *= (1.0 - damp_factor)
        q_mf_1[a1] += lr_1 * delta_1
        
        # Stage 2 PE
        delta_2 = r - q_mf_2[s_idx, a2]
        lr_2 = learning_rate
        if delta_2 < 0:
            lr_2 *= (1.0 - damp_factor)
        q_mf_2[s_idx, a2] += lr_2 * delta_2
        
        # Eligibility Trace
        # Updates Stage 1 based on Stage 2 outcome (delta_2)
        q_mf_1[a1] += lr_2 * lam * delta_2
        
        prev_action_1 = a1
        
    return log_loss
```

### Model 3: Hybrid Model with OCI-Modulated Cumulative Stickiness
This model expands the concept of stickiness from a simple "repeat last choice" heuristic to a cumulative "habit trace". The trace accumulates with choices and decays over time. High OCI slows down this decay, representing deeper, more persistent habits that are harder to break.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model with OCI-modulated Cumulative Stickiness (Habit Trace).
    
    Hypothesis: High OCI leads to deeper habits. Stickiness is modeled as an 
    accumulating trace. OCI modulates the decay rate (gamma) of this trace:
    higher OCI = slower decay = longer-lasting habit influence.
    
    Trace_{t+1} = Trace_t * gamma + Choice_t
    gamma = decay_base + decay_oci_slope * OCI
    
    Parameters:
    - learning_rate: [0,1]
    - beta: [0,10]
    - w: [0,1]
    - lam: [0,1]
    - stick_weight: [0,10] Weight of the cumulative habit trace in decision.
    - decay_base: [0,1] Base decay rate of stickiness trace.
    - decay_oci_slope: [0,1] Effect of OCI on preserving the habit trace.
    """
    learning_rate, beta, w, lam, stick_weight, decay_base, decay_oci_slope = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_mf_1 = np.zeros(2)
    q_mf_2 = np.zeros((2, 2))
    
    # Cumulative stickiness trace for Stage 1 actions
    stick_trace = np.zeros(2)
    
    log_loss = 0.0
    eps = 1e-10
    
    # Calculate effective decay (gamma)
    # Higher gamma = slower decay (more memory/persistence)
    gamma = decay_base + decay_oci_slope * oci_score
    if gamma > 1.0: gamma = 1.0
    if gamma < 0.0: gamma = 0.0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
        
        if a1 < 0 or s_idx < 0 or a2 < 0:
            continue
            
        # --- Stage 1 Choice ---
        max_q_mf_2 = np.max(q_mf_2, axis=1)
        q_mb_1 = transition_matrix @ max_q_mf_2
        
        # Net value includes cumulative stickiness trace
        q_net_1 = w * q_mb_1 + (1 - w) * q_mf_1 + stick_weight * stick_trace
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        log_loss -= np.log(probs_1[a1] + eps)
        
        # Update Stickiness Trace
        stick_trace *= gamma
        stick_trace[a1] += 1.0
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_mf_2[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        log_loss -= np.log(probs_2[a2] + eps)
        
        # --- Learning ---
        delta_1 = q_mf_2[s_idx, a2] - q_mf_1[a1]
        q_mf_1[a1] += learning_rate * delta_1
        
        delta_2 = r - q_mf_2[s_idx, a2]
        q_mf_2[s_idx, a2] += learning_rate * delta_2
        
        q_mf_1[a1] += learning_rate * lam * delta_2
        
    return log_loss
```