Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in this task. These models focus on the interplay between model-based/model-free control, perseveration, and learning rates.

### Model 1: OCI-Modulated Model-Based Suppression
This model hypothesizes that high OCI scores lead to a suppression of model-based (goal-directed) control in favor of model-free (habitual) control. High compulsivity is often associated with rigid, habitual behaviors. Here, the weighting parameter `w` (which balances model-based vs. model-free) is dynamically reduced based on the participant's OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Model-Based Suppression.
    Hypothesis: High OCI scores suppress the contribution of the model-based system (w),
    biasing the agent towards model-free (habitual) control. The parameter `w_max`
    represents the maximum possible model-based weight, which is reduced by the OCI score
    scaled by `suppression_strength`.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_max: [0, 1]
    suppression_strength: [0, 1]
    """
    learning_rate, beta, w_max, suppression_strength = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w based on OCI
    # If OCI is high, w is reduced from w_max.
    # We ensure w stays between 0 and 1.
    w = w_max * (1.0 - (suppression_strength * oci_score))
    w = max(0.0, min(1.0, w))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Handle -1 inputs for initial trials if present
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined value: mixture of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Action value updating
        # Stage 2 update (Standard Q-learning)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD-learning using stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration (Choice Stickiness)
This model posits that OCI relates to "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward. The participant data shows a strong tendency to repeat spaceship choice 1.0. This model adds a `stickiness` bonus to the Q-values of the previously chosen action, where the magnitude of this bonus is scaled by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Perseveration.
    Hypothesis: High OCI scores increase choice stickiness (perseveration).
    A bonus is added to the value of the previously chosen action at stage 1.
    The magnitude of this stickiness is determined by `stick_base` + `oci_stick_scale` * OCI.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    oci_stick_scale: [0, 5] (Scaling factor for OCI impact on stickiness)
    """
    learning_rate, beta, w, oci_stick_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus
        # If it's not the first trial, add bonus to the previously chosen action
        q_net_stick = q_net.copy()
        if last_action_1 != -1:
            stickiness_bonus = oci_stick_scale * oci_score
            q_net_stick[last_action_1] += stickiness_bonus
        
        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # Update last action
        last_action_1 = a1

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Action value updating
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Asymmetric Learning Rates
This model suggests that OCI affects how individuals learn from positive versus negative prediction errors. Specifically, high OCI might lead to hypersensitivity to negative outcomes (avoidance) or rigidity in updating beliefs. Here, we model separate learning rates for positive (`alpha_pos`) and negative (`alpha_neg`) prediction errors, where the negative learning rate is modulated by the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Dependent Asymmetric Learning Rates.
    Hypothesis: OCI scores influence the learning rate for negative prediction errors.
    A base negative learning rate is multiplied by an OCI-dependent factor.
    This captures potential biases in how high-OCI individuals process "disappointments"
    or failures to receive expected rewards.
    
    Bounds:
    alpha_pos: [0, 1]
    alpha_neg_base: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    oci_neg_mod: [0, 2] (Multiplier for negative learning rate based on OCI)
    """
    alpha_pos, alpha_neg_base, beta, w, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate negative learning rate specific to this participant
    # We allow it to be scaled up or down by OCI, but capped at 1.0
    alpha_neg = alpha_neg_base * (1.0 + (oci_neg_mod * oci_score))
    alpha_neg = min(1.0, max(0.0, alpha_neg))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        if a1 == -1: a1 = 0
        s_idx = int(state[trial])
        if s_idx == -1: s_idx = 0
        a2 = int(action_2[trial])
        if a2 == -1: a2 = 0
        r = reward[trial]
        if r == -1: r = 0

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # Action value updating with asymmetric learning rates
        
        # Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        lr2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += lr2 * delta_stage2
        
        # Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        lr1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[a1] += lr1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```