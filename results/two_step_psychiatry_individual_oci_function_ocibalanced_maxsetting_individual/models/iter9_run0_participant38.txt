Here are three cognitive models based on the participant's data and OCI profile.

### Cognitive Model 1: OCI-Distorted Transition Beliefs (Black-and-White Thinking)
**Hypothesis:** High OCI is often associated with "black-and-white" or dichotomous thinking and an intolerance of uncertainty. In the context of Model-Based (MB) control, this may manifest as a distortion of the internal model of the environment. While the true transition probability is 0.7, a high-OCI participant may conceptually round this up towards 1.0 (certainty), believing that Spaceship A *always* goes to Planet X. This rigidity makes their Model-Based value calculations over-confident and potentially erroneous when rare transitions occur.

**Parameters (6):**
*   `learning_rate`: [0, 1] - Learning rate for Model-Free updates.
*   `beta`: [0, 10] - Inverse temperature for softmax choice.
*   `w`: [0, 1] - Weighting between Model-Based and Model-Free systems.
*   `lambda_decay`: [0, 1] - Eligibility trace decay (credit assignment to Stage 1).
*   `stickiness`: [0, 5] - General tendency to repeat the previous Stage 1 choice.
*   `distortion_oci`: [0, 1] - Scales how much OCI distorts the transition matrix probability from 0.7 towards 1.0.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Distorted Transition Beliefs Model.
    
    Hypothesis: High OCI participants distort the transition matrix probabilities 
    towards certainty (Black-and-White thinking), treating 0.7 as closer to 1.0.
    """
    learning_rate, beta, w, lambda_decay, stickiness, distortion_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Distort the model-based transition matrix based on OCI
    # Base is 0.7. Max distortion approaches 1.0.
    # p_common = 0.7 + (1.0 - 0.7) * distortion_oci * oci_score
    # We clip to ensure it stays valid prob [0, 1] though logic implies > 0.7
    p_common = 0.7 + (0.3 * distortion_oci * oci_score)
    if p_common > 0.99: p_common = 0.99
    
    transition_matrix = np.array([[p_common, 1 - p_common], 
                                  [1 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)     # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (States x Actions)
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue
            
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation using Distorted Matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Stickiness (Choice Autocorrelation)
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        # Integrated Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 prediction error (using best next Q)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        # Stage 2 prediction error (Reward - Est)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 MF (TD-Lambda)
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * lambda_decay * delta_stage2
        
        # Update Stage 2 MF
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 2: OCI-Suppressed Negative Learning (Perseveration)
**Hypothesis:** OCI is characterized by perseverationâ€”the repetition of actions despite the absence of reward. Standard Reinforcement Learning reduces the value of an action when a negative prediction error occurs (Reward < Expectation). This model proposes that high OCI scores correlate with a *suppression* of learning from negative prediction errors. Effectively, the participant "ignores" the failure to get gold, preventing the Q-value from dropping, which leads to compulsive repetition (Lose-Stay) driven by the failure to unlearn.

**Parameters (6):**
*   `learning_rate`: [0, 1] - Base learning rate for positive prediction errors.
*   `beta`: [0, 10] - Inverse temperature.
*   `w`: [0, 1] - MB/MF weight.
*   `lambda_decay`: [0, 1] - Eligibility trace.
*   `stickiness`: [0, 5] - Choice repetition bonus.
*   `neg_lr_suppress_oci`: [0, 1] - Scaling factor for OCI that reduces the learning rate when prediction error is negative.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Suppressed Negative Learning Model.
    
    Hypothesis: High OCI leads to perseveration by suppressing the learning rate 
    specifically when prediction errors are negative (ignoring losses).
    """
    learning_rate, beta, w, lambda_decay, stickiness, neg_lr_suppress_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Calculate Prediction Errors
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Determine Effective Learning Rate for Negative PEs
        # If delta is negative, OCI suppresses the update (Perseveration)
        
        # We apply this logic to the Stage 2 update (the primary source of value change)
        current_lr = learning_rate
        if delta_stage2 < 0:
            # Suppression: LR becomes smaller as OCI increases
            suppression = neg_lr_suppress_oci * oci_score
            # Clamp suppression to max 1.0
            if suppression > 1.0: suppression = 1.0
            current_lr = learning_rate * (1.0 - suppression)
            
        # Update Stage 1 MF
        # Note: We use the effective LR for the propagated error as well
        q_stage1_mf[a1] += current_lr * delta_stage1 + current_lr * lambda_decay * delta_stage2
        
        # Update Stage 2 MF
        q_stage2_mf[s_idx, a2] += current_lr * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```

### Cognitive Model 3: Immediate Reward Hypersensitivity (Stage 2 Boost)
**Hypothesis:** OCI may involve a hyper-focus on immediate outcomes (the alien's coin) at the expense of integrating the broader task structure. This model posits that OCI participants have a significantly higher learning rate for Stage 2 (the immediate reward state) compared to Stage 1 (the spaceship choice). They learn very quickly which alien pays out, but this "greedy" learning does not necessarily propagate efficiently to the first stage due to the structural disconnect. This is modeled by boosting the Stage 2 learning rate by OCI.

**Parameters (6):**
*   `lr_base`: [0, 1] - Baseline learning rate (Stage 1).
*   `beta`: [0, 10] - Inverse temperature.
*   `w`: [0, 1] - MB/MF weight.
*   `lambda_decay`: [0, 1] - Eligibility trace.
*   `stickiness`: [0, 5] - Choice repetition bonus.
*   `lr_s2_boost_oci`: [0, 5] - Multiplier for OCI that increases the learning rate specifically for Stage 2.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Immediate Reward Hypersensitivity Model.
    
    Hypothesis: OCI participants are hypersensitive to the immediate outcome (Stage 2),
    resulting in a boosted learning rate for Stage 2 relative to Stage 1.
    """
    lr_base, beta, w, lambda_decay, stickiness, lr_s2_boost_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Calculate Stage 2 Learning Rate
    # It is boosted by OCI. We clip at 1.0.
    lr_stage2 = lr_base * (1.0 + lr_s2_boost_oci * oci_score)
    if lr_stage2 > 1.0: lr_stage2 = 1.0
    
    # Stage 1 uses base LR
    lr_stage1 = lr_base

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):
        if action_1[trial] == -1:
            continue

        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        stick_vec = np.zeros(2)
        if prev_a1 != -1:
            stick_vec[prev_a1] = stickiness
            
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf + stick_vec
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        qs_2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        
        # Update Stage 1 MF (using base LR)
        # Note: Lambda usually propagates the Stage 2 error. 
        # We use lr_stage1 for the Stage 1 update steps.
        q_stage1_mf[a1] += lr_stage1 * delta_stage1 + lr_stage1 * lambda_decay * delta_stage2
        
        # Update Stage 2 MF (using boosted LR)
        q_stage2_mf[s_idx, a2] += lr_stage2 * delta_stage2
        
        prev_a1 = a1

    eps = 1e-10
    mask = action_1 != -1
    log_loss = -(np.sum(np.log(p_choice_1[mask] + eps)) + np.sum(np.log(p_choice_2[mask] + eps)))
    return log_loss
```