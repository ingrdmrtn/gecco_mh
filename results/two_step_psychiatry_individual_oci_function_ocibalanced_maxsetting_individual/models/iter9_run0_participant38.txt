Here are three new cognitive models that explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence decision-making in the two-step task, specifically focusing on how high OCI (like this participant's 0.78) might alter learning rates, habitual control, or sensitivity to rare transitions.

### Model 1: OCI-Modulated Transition Learning (Checking Behavior)
This model hypothesizes that individuals with high OCI scores might not trust the stable transition structure (70/30) and instead constantly update their internal model of the transition matrix, effectively "checking" if the world has changed. High OCI leads to a higher learning rate for the transition matrix itself, making the model-based system more volatile.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Dynamic Transition Learning with OCI-modulated Volatility.
    
    Hypothesis: High OCI participants constantly 'check' or update their belief 
    about the spaceship-planet transitions rather than relying on the fixed 70/30 prior.
    This is implemented as a transition learning rate that scales with OCI.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate for rewards.
    beta: [0,10] - Inverse temperature for softmax.
    w: [0,1] - Weight of Model-Based control.
    trans_lr_base: [0,1] - Baseline learning rate for the transition matrix.
    oci_trans_boost: [0,1] - Additional transition learning rate proportional to OCI.
    """
    learning_rate, beta, w, trans_lr_base, oci_trans_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Transition learning rate increases with OCI (hyper-vigilance to structure change)
    lr_transition = trans_lr_base + (oci_trans_boost * oci_score)
    # Cap at 1.0
    if lr_transition > 1.0: lr_transition = 1.0
    
    # Initialize transition matrix (start with uniform or 0.5 belief if fully learning, 
    # but here we start with the true structure and allow drift)
    # Rows: Action (0 or 1), Cols: State (0 or 1)
    # Initial belief: slightly biased towards truth but mutable
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        # Model-Based Value: Uses current dynamic transition belief
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Calculate expected value based on current transition matrix belief
        # trans_probs[action, state]
        q_stage1_mb = np.zeros(2)
        q_stage1_mb[0] = trans_probs[0, 0] * max_q_stage2[0] + trans_probs[0, 1] * max_q_stage2[1]
        q_stage1_mb[1] = trans_probs[1, 0] * max_q_stage2[0] + trans_probs[1, 1] * max_q_stage2[1]
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # 1. Update Value Estimates (Standard TD)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # 2. Update Transition Matrix (State Prediction Error)
        # Observed transition: a1 -> s_idx. 
        # Create a one-hot vector for the observed state
        observed_state_vec = np.zeros(2)
        observed_state_vec[s_idx] = 1.0
        
        # Update row corresponding to a1
        # Prediction error vector = Actual - Predicted
        trans_error = observed_state_vec - trans_probs[a1]
        trans_probs[a1] += lr_transition * trans_error
        
        # Normalize to ensure valid probabilities
        trans_probs[a1] = trans_probs[a1] / np.sum(trans_probs[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Habitual Stickiness
This model proposes that high OCI is associated with repetitive, compulsive behaviors, manifested here as "choice stickiness." Unlike standard stickiness which is a constant bias towards the previous action, here the *strength* of this stickiness is directly modulated by the OCI score. Higher OCI leads to a stronger urge to repeat the last Stage 1 action, regardless of reward history.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Driven Habitual Stickiness.
    
    Hypothesis: High OCI scores correlate with compulsive repetition. This is modeled
    as a choice autocorrelation (stickiness) parameter that scales with the OCI score.
    Participants with high OCI will be more likely to repeat their previous Stage 1 choice.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based control.
    stickiness_base: [0,5] - Baseline tendency to repeat the previous choice.
    oci_stickiness_gain: [0,5] - Additional stickiness multiplier for OCI score.
    """
    learning_rate, beta, w, stickiness_base, oci_stickiness_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Stickiness bonus: baseline + OCI-dependent component
    stickiness_param = stickiness_base + (oci_stickiness_gain * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action for trial 0

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-values before softmax
        q_hybrid_sticky = q_hybrid.copy()
        if last_action_1 != -1:
            q_hybrid_sticky[last_action_1] += stickiness_param

        exp_q1 = np.exp(beta * q_hybrid_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1 # Update for next trial

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Pruning (Negative Learning Suppression)
This model suggests that high OCI individuals are hypersensitive to punishment or lack of reward (loss aversion/anxiety) but in a specific way: they may fail to "unlearn" bad options effectively, or conversely, overreact to them. Here, we test the hypothesis that OCI modulates the learning rate specifically for *negative* prediction errors (when outcomes are worse than expected).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Asymmetric Learning with OCI-modulated Negative Updating.
    
    Hypothesis: OCI is related to anxiety and error monitoring. This model proposes
    that OCI specifically modulates how much weight is given to negative prediction errors
    (disappointments). High OCI might lead to 'over-learning' from failure (catastrophizing)
    or avoidance.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors (better than expected).
    lr_neg_base: [0,1] - Baseline learning rate for negative prediction errors.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight of Model-Based control.
    oci_neg_scale: [0,5] - Multiplier for OCI to scale the negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_neg_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective negative learning rate
    # If oci_neg_scale > 0, high OCI increases sensitivity to negative outcomes.
    lr_neg = lr_neg_base * (1.0 + oci_neg_scale * oci_score)
    if lr_neg > 1.0: lr_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        s_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        if delta_stage2 >= 0:
            q_stage2_mf[s_idx, a2] += lr_pos * delta_stage2
        else:
            q_stage2_mf[s_idx, a2] += lr_neg * delta_stage2
        
        # Stage 1 Update
        # Note: We apply the same asymmetric logic to the MF Stage 1 update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        if delta_stage1 >= 0:
            q_stage1_mf[a1] += lr_pos * delta_stage1
        else:
            q_stage1_mf[a1] += lr_neg * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```