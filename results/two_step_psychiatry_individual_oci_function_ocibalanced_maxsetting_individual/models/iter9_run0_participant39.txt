Here are the 3 proposed cognitive models.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Loss-Stickiness Model.
    
    Hypothesis:
    Compulsivity (OCI) is characterized by a tendency to repeat behaviors even when they are not 
    rewarding (perseveration after loss/failure). This model separates choice stickiness into 
    'Win-Stickiness' (repeat after reward) and 'Loss-Stickiness' (repeat after no reward).
    OCI specifically modulates the 'Loss-Stickiness' parameter.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Q-values (MF and Stage 2).
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    w: [0, 1] - Weighting parameter (0 = Pure MF, 1 = Pure MB).
    stick_win: [0, 5] - Tendency to repeat choice after receiving a reward.
    stick_loss_base: [-5, 5] - Base tendency to repeat choice after receiving NO reward.
    stick_loss_oci: [-5, 5] - Modulation of loss-stickiness by OCI score.
    """
    learning_rate, beta, w, stick_win, stick_loss_base, stick_loss_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate OCI-dependent loss stickiness
    stick_loss = stick_loss_base + stick_loss_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Model-Free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Values for stage 2 (Aliens)
    
    last_action_1 = -1
    last_reward = 0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        
        # Apply Stickiness based on previous reward
        if last_action_1 != -1:
            if last_reward == 1.0:
                logits[last_action_1] += stick_win
            else:
                logits[last_action_1] += stick_loss

        # Softmax Choice 1
        logits = logits - np.max(logits) # Stability
        exp_logits = np.exp(logits)
        probs_1 = exp_logits / np.sum(exp_logits)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        if a2 != -1:
            logits_2 = beta * q_stage2_mf[s_idx, :]
            logits_2 = logits_2 - np.max(logits_2)
            exp_logits_2 = np.exp(logits_2)
            probs_2 = exp_logits_2 / np.sum(exp_logits_2)
            p_choice_2[trial] = probs_2[a2]
            
            # --- Updates ---
            # Update Stage 2 Values (TD)
            # Q2(s, a2) += alpha * (r - Q2(s, a2))
            q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
            
            # Update Stage 1 MF Values (Simple TD(1) / Outcome based for parsimony)
            # Q1(a1) += alpha * (r - Q1(a1))
            q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])
        else:
            p_choice_2[trial] = 1.0

        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Heuristic (WSLS) vs Planning Model.
    
    Hypothesis:
    Instead of a standard Model-Free Q-learning system, the 'habitual' component is modeled as a 
    simple Win-Stay-Lose-Shift (WSLS) heuristic. The decision is a weighted combination of 
    Model-Based planning and this WSLS heuristic. OCI modulates the reliance on the WSLS strategy.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Stage 2 Q-values (used for MB planning).
    beta_mb: [0, 10] - Strength of Model-Based influence.
    beta_wsls_base: [0, 10] - Base strength of WSLS heuristic influence.
    beta_wsls_oci: [-5, 5] - Modulation of WSLS strength by OCI.
    stickiness: [-5, 5] - General choice perseveration (inertia).
    """
    learning_rate, beta_mb, beta_wsls_base, beta_wsls_oci, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    beta_wsls = beta_wsls_base + beta_wsls_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage2_mf = np.zeros((2, 2))
    wsls_vec = np.zeros(2) # Heuristic value for [Action 0, Action 1]
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB, WSLS, and Stickiness
        logits = (beta_mb * q_stage1_mb) + (beta_wsls * wsls_vec)
        
        if last_action_1 != -1:
            logits[last_action_1] += stickiness
            
        logits = logits - np.max(logits)
        exp_logits = np.exp(logits)
        probs_1 = exp_logits / np.sum(exp_logits)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        if a2 != -1:
            # Use a fixed beta for stage 2 or reuse beta_mb? 
            # To be parsimonious and ensure stage 2 values are learned reasonably,
            # we imply a standard softmax with beta_mb (or 1.0 scaled into learning).
            # Let's use beta_mb for consistency in temperature.
            logits_2 = beta_mb * q_stage2_mf[s_idx, :]
            logits_2 = logits_2 - np.max(logits_2)
            exp_logits_2 = np.exp(logits_2)
            probs_2 = exp_logits_2 / np.sum(exp_logits_2)
            p_choice_2[trial] = probs_2[a2]
            
            # --- Updates ---
            # Update Stage 2
            q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
            
            # Update WSLS Vector for next trial
            # If Win: value 1 on chosen, 0 on other.
            # If Lose: value 0 on chosen, 1 on other (Shift).
            wsls_vec = np.zeros(2)
            if r == 1.0:
                wsls_vec[a1] = 1.0
            else:
                wsls_vec[1 - a1] = 1.0
        else:
             p_choice_2[trial] = 1.0

        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Stickiness Model.
    
    Hypothesis:
    Compulsivity might manifest as repetition of choices not just at the spaceship level (Stage 1),
    but also at the alien selection level (Stage 2). This model introduces a stickiness parameter
    specifically for the second stage, modulated by OCI.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature for both stages.
    w: [0, 1] - Weighting between MB and MF in Stage 1.
    stick_s1: [-5, 5] - Stickiness for Stage 1 (Spaceship choice).
    stick_s2_base: [-5, 5] - Base stickiness for Stage 2 (Alien choice).
    stick_s2_oci: [-5, 5] - Modulation of Stage 2 stickiness by OCI.
    """
    learning_rate, beta, w, stick_s1, stick_s2_base, stick_s2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    stick_s2 = stick_s2_base + stick_s2_oci * oci_score
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_action_2_per_state = [-1, -1] # Track last choice for each planet (state)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        logits = beta * q_net
        
        if last_action_1 != -1:
            logits[last_action_1] += stick_s1
            
        logits = logits - np.max(logits)
        exp_logits = np.exp(logits)
        probs_1 = exp_logits / np.sum(exp_logits)
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1
        
        # --- Stage 2 Policy ---
        if a2 != -1:
            logits_2 = beta * q_stage2_mf[s_idx, :]
            
            # Apply State-Dependent Stickiness for Stage 2
            prev_a2 = last_action_2_per_state[s_idx]
            if prev_a2 != -1:
                logits_2[prev_a2] += stick_s2
            
            logits_2 = logits_2 - np.max(logits_2)
            exp_logits_2 = np.exp(logits_2)
            probs_2 = exp_logits_2 / np.sum(exp_logits_2)
            p_choice_2[trial] = probs_2[a2]
            
            last_action_2_per_state[s_idx] = a2
            
            # --- Updates ---
            # Stage 2 Update
            q_stage2_mf[s_idx, a2] += learning_rate * (r - q_stage2_mf[s_idx, a2])
            
            # Stage 1 MF Update (Simple Outcome-based)
            q_stage1_mf[a1] += learning_rate * (r - q_stage1_mf[a1])
        else:
            p_choice_2[trial] = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```