Here are three new cognitive models exploring different mechanisms by which Obsessive-Compulsive symptoms (OCI) might influence decision-making in this task.

### Model 1: Hybrid Learner with OCI-Driven Model-Based Suppression
This model tests the hypothesis that high OCI scores are associated with a deficit in model-based (goal-directed) control. Instead of just scaling the mixing weight `w`, this model posits that OCI acts as a suppressor on the model-based contribution, effectively forcing the agent to rely more on model-free (habitual) values as symptoms increase.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where OCI score suppresses the Model-Based contribution.
    
    Hypothesis: High OCI participants have reduced goal-directed (Model-Based) control.
    The mixing weight 'w' is dynamically reduced by the OCI score.
    w_effective = w_base * (1 - oci_suppression * oci_score)

    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_base: [0, 1]
    oci_suppression: [0, 1]
    """
    learning_rate, beta, w_base, oci_suppression = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    # Transition matrix (fixed structure of task)
    # 0 -> 0 (common), 0 -> 1 (rare)
    # 1 -> 1 (common), 1 -> 0 (rare)
    # But usually defined as: P(State|Action). 
    # Let's assume standard task structure: A(0)->X(0) (0.7), U(1)->Y(1) (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q_stage1_mf = np.zeros(2) # Model-free stage 1 values
    q_stage2_mf = np.zeros((2, 2)) # Model-free stage 2 values (also used for MB calc)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Calculate effective mixing weight. 
    # If oci_suppression is high and oci is high, w becomes small (Model-Free dominance).
    w_effective = w_base * (1.0 - (oci_suppression * oci_score))
    # Clip to ensure it stays valid [0, 1]
    w_effective = np.clip(w_effective, 0.0, 1.0)

    for t in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation: V(S') = max(Q(S', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        logits_1 = beta * q_net
        # Numerical stability
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]

        # --- Stage 2 Policy ---
        s_idx = state[t]
        logits_2 = beta * q_stage2_mf[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]

        # --- Learning ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 2 Update (TD)
        pe_2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * pe_2

        # Stage 1 Update (TD)
        # Using the value of the state actually reached (SARSA-like for stage 1 value)
        # Standard Q-learning at stage 1 usually uses Q(s2, a2)
        pe_1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated "Winning-Stay" Heuristic
This model ignores the complex model-based/model-free trade-off and posits a simpler heuristic strategy often observed in compulsive phenotypes: a rigid adherence to previous success ("Win-Stay") but potentially erratic behavior after losses. Here, OCI modulates how strongly the agent clings to a previously rewarded choice (Win-Stay bias) versus exploring after a loss.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Heuristic learner with OCI-modulated Win-Stay/Lose-Shift logic.
    
    Hypothesis: High OCI participants exhibit rigid 'Win-Stay' behavior.
    They are very unlikely to switch after a reward.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    win_stay_base: [0, 5]
    oci_rigidity: [0, 5]
    """
    learning_rate, beta, win_stay_base, oci_rigidity = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    # Simple Q-learning for the base values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_reward = 0
    prev_action_1 = -1
    
    # Calculate the strength of the Win-Stay bias
    # Higher OCI -> Stronger tendency to repeat rewarded actions
    win_stay_bonus = win_stay_base + (oci_rigidity * oci_score)

    for t in range(n_trials):
        # --- Stage 1 Policy ---
        logits_1 = beta * q_stage1.copy()
        
        # Apply Win-Stay Bonus
        if t > 0 and prev_reward == 1 and prev_action_1 != -1:
            logits_1[prev_action_1] += win_stay_bonus
        
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]

        # --- Stage 2 Policy ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]

        # --- Learning ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Standard TD updates
        pe_2 = r - q_stage2[s_idx, a2]
        q_stage2[s_idx, a2] += learning_rate * pe_2
        
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * pe_1
        
        prev_reward = r
        prev_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Asymmetric Learning Rates Modulated by OCI
This model investigates if OCI affects how participants learn from positive versus negative prediction errors. Clinical literature suggests anxiety and compulsivity can lead to hypersensitivity to negative outcomes (or punishment) or, conversely, an inability to "unlearn" habits when rewards cease. Here, we model separate learning rates for positive and negative prediction errors, where the negative learning rate is scaled by OCI.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates, where OCI modulates
    learning from negative prediction errors (forgetting/unlearning).
    
    Hypothesis: High OCI participants may have difficulty updating values 
    downward after non-rewards (lower alpha_neg), leading to persistence.
    
    Bounds:
    alpha_pos: [0, 1]
    alpha_neg_base: [0, 1]
    beta: [0, 10]
    oci_neg_dampening: [0, 1]
    """
    alpha_pos, alpha_neg_base, beta, oci_neg_dampening = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    reward = reward.astype(int)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Calculate effective negative learning rate
    # If oci_neg_dampening is high, alpha_neg becomes very small for high OCI.
    # This means they don't lower the value of an action even if it yields no reward.
    alpha_neg_effective = alpha_neg_base * (1.0 - (oci_neg_dampening * oci_score))
    alpha_neg_effective = np.clip(alpha_neg_effective, 0.001, 1.0) # Avoid 0

    for t in range(n_trials):
        # --- Stage 1 Policy ---
        logits_1 = beta * q_stage1
        logits_1 = logits_1 - np.max(logits_1)
        probs_1 = np.exp(logits_1) / np.sum(np.exp(logits_1))
        p_choice_1[t] = probs_1[action_1[t]]

        # --- Stage 2 Policy ---
        s_idx = state[t]
        logits_2 = beta * q_stage2[s_idx]
        logits_2 = logits_2 - np.max(logits_2)
        probs_2 = np.exp(logits_2) / np.sum(np.exp(logits_2))
        p_choice_2[t] = probs_2[action_2[t]]

        # --- Learning ---
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 2 Update
        pe_2 = r - q_stage2[s_idx, a2]
        lr_2 = alpha_pos if pe_2 >= 0 else alpha_neg_effective
        q_stage2[s_idx, a2] += lr_2 * pe_2

        # Stage 1 Update
        # Note: Using Q(s2, a2) as the target for Stage 1
        pe_1 = q_stage2[s_idx, a2] - q_stage1[a1]
        lr_1 = alpha_pos if pe_1 >= 0 else alpha_neg_effective
        q_stage1[a1] += lr_1 * pe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```