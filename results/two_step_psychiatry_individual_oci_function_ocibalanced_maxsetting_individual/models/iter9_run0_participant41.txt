Here are three new cognitive models for the two-step task, incorporating OCI-modulated mechanisms.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Persistent Stickiness.
    
    Standard stickiness repeats the last action. Persistent stickiness accumulates
    a trace of past choices, creating longer streaks of repetition.
    OCI modulates the decay rate of this trace.
    High OCI may lead to slower decay (higher persistence), resulting in longer choice streaks.
    
    Parameters:
    learning_rate: [0,1] - Q-value update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight for Model-Based control (1=MB, 0=MF).
    stick_weight: [0,5] - Weight of the stickiness trace in choice logits.
    decay_base: [0,1] - Base decay rate of the stickiness trace.
    decay_oci: [-1,1] - Modulation of decay rate by OCI.
    """
    learning_rate, beta, w, stick_weight, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate modulated by OCI
    decay = decay_base + decay_oci * oci_score
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Persistent stickiness trace
    choice_trace = np.zeros(2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        # Handle missing data
        if a2 == -1 or r == -1:
            p_choice_1[trial] = 0.5 
            p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        # MB Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Values (Hybrid)
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Stickiness Trace to logits
        logits = beta * q_net + stick_weight * choice_trace
        
        # Softmax
        exp_q = np.exp(logits - np.max(logits))
        probs_1 = exp_q / np.sum(exp_q)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # SARSA for Stage 1 MF
        dt_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * dt_1
        
        # RPE for Stage 2
        dt_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * dt_2
        
        # Direct reinforcement of Stage 1 choice (TD(1)-like)
        q_stage1_mf[a1] += learning_rate * dt_2
        
        # Update Stickiness Trace
        choice_trace *= decay     # Decay old traces
        choice_trace[a1] += 1.0   # Increment current choice
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Mixture of Hybrid MB/MF and Win-Stay Lose-Shift (WSLS) Strategy.
    
    The agent employs a simple heuristic WSLS strategy alongside the standard hybrid mechanism.
    The weight of the WSLS strategy is modulated by OCI.
    High OCI may relate to increased reliance on simple heuristics like WSLS over complex value integration.
    
    Parameters:
    learning_rate: [0,1] - Q-value update rate.
    beta: [0,10] - Inverse temperature for Hybrid model.
    w_mb: [0,1] - Weight of Model-Based control within the Hybrid component.
    stickiness: [0,5] - Standard stickiness (last choice repetition) for Hybrid component.
    w_wsls_base: [0,1] - Base weight of the WSLS strategy.
    w_wsls_oci: [-1,1] - OCI modulation of the WSLS weight.
    """
    learning_rate, beta, w_mb, stickiness, w_wsls_base, w_wsls_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate WSLS mixture weight
    w_wsls = w_wsls_base + w_wsls_oci * oci_score
    w_wsls = np.clip(w_wsls, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_reward = 0
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        if a2 == -1 or r == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            last_action_1 = -1
            continue
            
        # --- Hybrid Policy Component ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w_mb * q_stage1_mb + (1 - w_mb) * q_stage1_mf
        
        # Standard Stickiness
        stick_vec = np.zeros(2)
        if last_action_1 != -1:
            stick_vec[last_action_1] = stickiness
            
        logits = beta * q_net + stick_vec
        exp_q = np.exp(logits - np.max(logits))
        probs_hybrid = exp_q / np.sum(exp_q)
        
        # --- WSLS Policy Component ---
        probs_wsls = np.ones(2) * 0.5
        if last_action_1 != -1:
            if last_reward > 0: # Win -> Stay
                probs_wsls = np.zeros(2)
                probs_wsls[last_action_1] = 1.0
            else: # Lose -> Shift
                probs_wsls = np.zeros(2)
                probs_wsls[1 - last_action_1] = 1.0
        
        # --- Mixture ---
        probs_final = (1 - w_wsls) * probs_hybrid + w_wsls * probs_wsls
        p_choice_1[trial] = probs_final[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        dt_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * dt_1
        
        dt_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * dt_2
        
        q_stage1_mf[a1] += learning_rate * dt_2
        
        last_action_1 = a1
        last_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid MB/MF model with OCI-modulated Stage 2 Learning Rate.
    
    This model posits that OCI specifically affects the learning of the reward model (Stage 2),
    while the decision process (Stage 1) uses a base learning rate.
    High OCI might lead to faster (hyper-responsive) or slower (rigid) updating of alien reward probabilities.
    
    Parameters:
    lr_1: [0,1] - Learning rate for Stage 1 (MF values).
    lr_2_base: [0,1] - Base learning rate for Stage 2 (Reward model).
    lr_2_oci: [-1,1] - Modulation of Stage 2 learning rate by OCI.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Weight for Model-Based control.
    stickiness: [0,5] - Choice stickiness.
    """
    lr_1, lr_2_base, lr_2_oci, beta, w, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate modulated Stage 2 learning rate
    lr_2 = lr_2_base + lr_2_oci * oci_score
    lr_2 = np.clip(lr_2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        a1 = int(action_1[trial])
        s = int(state[trial])
        a2 = int(action_2[trial])
        r = float(reward[trial])
        
        if a2 == -1 or r == -1:
            p_choice_1[trial] = 0.5
            p_choice_2[trial] = 0.5
            last_action_1 = -1
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        stick_vec = np.zeros(2)
        if last_action_1 != -1:
            stick_vec[last_action_1] = stickiness
            
        logits = beta * q_net + stick_vec
        exp_q = np.exp(logits - np.max(logits))
        probs_1 = exp_q / np.sum(exp_q)
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        logits_2 = beta * q_stage2_mf[s]
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        # Stage 1 update uses lr_1 (Fixed base)
        dt_1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_1 * dt_1
        
        # Stage 2 update uses lr_2 (OCI modulated)
        dt_2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += lr_2 * dt_2
        
        # TD(1) update to Stage 1 also uses lr_1
        q_stage1_mf[a1] += lr_1 * dt_2
        
        last_action_1 = a1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```