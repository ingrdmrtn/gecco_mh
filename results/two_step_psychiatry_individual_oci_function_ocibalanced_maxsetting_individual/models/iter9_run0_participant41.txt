Here are three new cognitive models that explore different mechanisms for how Obsessive-Compulsive symptoms (OCI) might influence decision-making in this task.

### Model 1: Hybrid Learner with OCI-modulated Mixing Weight ($w$)
This model hypothesizes that high OCI scores lead to a rigid reliance on Model-Free (habitual) learning over Model-Based (goal-directed) planning. Here, the mixing parameter $w$ (which balances MB and MF control) is directly reduced by the OCI score, making high-OCI participants more habitual.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where OCI modulates the balance (w).
    High OCI is hypothesized to reduce Model-Based control (lower w), leading to 
    more habitual behavior.
    
    Parameters:
    learning_rate: [0,1] Learning rate for value updates.
    beta: [0,10] Inverse temperature for softmax.
    w_max: [0,1] Maximum possible weight for Model-Based control (when OCI=0).
    oci_penalty: [0,1] How much OCI reduces the Model-Based weight.
    """
    learning_rate, beta, w_max, oci_penalty = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the mixing weight w based on OCI
    # If OCI is high (1.0), w is reduced by oci_penalty.
    # We clip to ensure w stays in [0,1]
    w = w_max * (1.0 - (oci_score * oci_penalty))
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value: V_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Stage 2 Policy ---
        # Standard Model-Free choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 RPE (TD(0))
        # Note: In a hybrid model, the MF component is usually updated via SARSA or Q-learning
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Asymmetric Learning Rates Modulated by OCI
This model hypothesizes that OCI is associated with an increased sensitivity to negative outcomes (punishment/loss aversion). High OCI participants might learn faster from lack of reward (0 coins) than from reward (1 coin).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with asymmetric learning rates for positive and negative prediction errors.
    The learning rate for negative prediction errors (LR_neg) is scaled up by the OCI score.
    
    Hypothesis: High OCI participants are hyper-sensitive to "failure" (0 reward), 
    updating their values more drastically when expectations are not met.
    
    Parameters:
    lr_pos: [0,1] Learning rate for positive prediction errors (RPE > 0).
    lr_neg_base: [0,1] Base learning rate for negative prediction errors (RPE < 0).
    beta: [0,10] Inverse temperature.
    oci_sens: [0,5] Scaling factor for OCI impact on negative learning rate.
    """
    lr_pos, lr_neg_base, beta, oci_sens = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate
    # We allow it to go up to 1.0 but not exceed it.
    lr_neg = lr_neg_base * (1.0 + oci_score * oci_sens)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning Stage 2 ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Apply asymmetric learning rates
        eff_lr2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += eff_lr2 * delta_stage2
        
        # --- Learning Stage 1 ---
        # Using the value of the chosen stage 2 state-action as the target
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        eff_lr1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += eff_lr1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Uncertainty-Driven Exploration Modulated by OCI
This model suggests that OCI relates to intolerance of uncertainty. Instead of simple random exploration (softmax temperature), this model tracks the uncertainty (variance) of value estimates. High OCI participants are hypothesized to avoid uncertain options (uncertainty penalty) rather than explore them (uncertainty bonus).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner that tracks uncertainty (variance) of Q-values.
    The OCI score determines an 'uncertainty penalty' parameter (phi).
    High OCI leads to a stronger avoidance of options with high variance 
    (intolerance of uncertainty).
    
    Parameters:
    learning_rate: [0,1] Learning rate for Q-values.
    beta: [0,10] Inverse temperature.
    phi_base: [0,5] Base weight for uncertainty penalty.
    decay: [0,1] Decay rate for uncertainty (variance) tracking.
    """
    learning_rate, beta, phi_base, decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Uncertainty penalty increases with OCI
    phi = phi_base * (1.0 + oci_score)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    var_stage1 = np.ones(2) # Initialize variance/uncertainty as high
    
    q_stage2 = np.zeros((2, 2))
    # We only model uncertainty influence at Stage 1 for simplicity of the decision architecture
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Value is Q - phi * sqrt(Variance)
        # We subtract because we hypothesize avoidance of uncertainty for high OCI
        adjusted_q1 = q_stage1 - phi * np.sqrt(var_stage1)
        
        exp_q1 = np.exp(beta * adjusted_q1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        state_idx = int(state[trial])
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Learning ---
        # Update Stage 2
        delta_2 = r - q_stage2[state_idx, a2]
        q_stage2[state_idx, a2] += learning_rate * delta_2
        
        # Update Stage 1
        delta_1 = q_stage2[state_idx, a2] - q_stage1[a1]
        q_stage1[a1] += learning_rate * delta_1
        
        # Update Variance/Uncertainty for chosen option 1
        # Variance approximates the squared prediction error
        # Var[t+1] = (1-decay)*Var[t] + decay*PE^2
        var_stage1[a1] = (1 - decay) * var_stage1[a1] + decay * (delta_1**2)
        
        # Unchosen options might increase in uncertainty (simple counter logic not used here, 
        # just decay based tracking of error magnitude)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```