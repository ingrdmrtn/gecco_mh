Here are three new cognitive models designed to capture the behavior of a participant with high OCI symptoms in a two-step task. These models focus on potential mechanisms like altered reward sensitivity, perseveration (stickiness), and deficits in model-based control integration.

### Model 1: OCI-Modulated Reward Sensitivity Asymmetry
This model hypothesizes that high OCI participants might process positive and negative feedback differently, specifically showing heightened sensitivity to punishment (or lack of reward) which drives avoidance. Instead of just separate learning rates, this model scales the *magnitude* of the reward signal itself based on OCI, making losses (0 coins) feel more significant than gains (1 coin) or vice versa.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Reward Sensitivity Asymmetry.
    Hypothesis: High OCI participants may exhibit altered sensitivity to outcomes.
    This model introduces an asymmetry where the effective reward value of a loss (0)
    is pushed lower (more negative) proportional to OCI, effectively increasing 
    punishment sensitivity.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    - loss_amp: [0, 5] Amplification factor for zero-reward outcomes based on OCI.
    """
    learning_rate, beta, w, loss_amp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Calculate effective reward
        # If reward is 0 (loss), we treat it as negative value scaled by OCI
        # Standard RL assumes 0, but anxiety/compulsivity might treat 0 as -X
        effective_r = r
        if r == 0:
            effective_r = -1.0 * loss_amp * oci_score

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # Value Updating
        # Update Stage 2 values using effective reward
        delta_stage2 = effective_r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

        # Update Stage 1 MF values
        # Note: standard TD uses the max of next stage, here we simplify to the value of chosen
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Dependent Choice Perseveration (Stickiness)
This model tests the hypothesis that high OCI leads to repetitive behavior ("stickiness") regardless of reward outcomes. Unlike the "Win-Stay" model which requires a reward to trigger repetition, this model assumes a baseline urge to repeat the last action taken at Stage 1, and the strength of this urge is linearly modulated by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Dependent Choice Perseveration.
    Hypothesis: OCI is associated with behavioral rigidity. This model adds a 
    'stickiness' bonus to the previously chosen Stage 1 action, where the 
    magnitude of this stickiness is determined by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting parameter (0 = pure MF, 1 = pure MB).
    - stick_factor: [0, 5] Multiplier for OCI to determine stickiness strength.
    """
    learning_rate, beta, w, stick_factor = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_act1 = -1

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus based on OCI
        if prev_act1 != -1:
            stickiness_bonus = stick_factor * oci_score
            q_net[prev_act1] += stickiness_bonus

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # Value Updating
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        prev_act1 = act1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Driven Model-Based Suppression
This model posits that high OCI symptoms actively interfere with the recruitment of model-based (goal-directed) strategies. Instead of `w` being a free parameter directly, `w` is modeled as a base capability reduced by OCI. The higher the OCI, the more the participant is forced into a Model-Free strategy, reflecting a shift from goal-directed to habitual control.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Driven Model-Based Suppression.
    Hypothesis: High OCI impairs model-based control. Here, the mixing weight 'w' 
    is not static but is a function of a 'base_w' capacity minus an OCI-dependent 
    penalty. High OCI forces w closer to 0 (Model-Free).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_capacity: [0, 1] Maximum potential model-based weight (when OCI is 0).
    - oci_impairment: [0, 1] How strongly OCI reduces w_capacity.
    """
    learning_rate, beta, w_capacity, oci_impairment = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective w
    # w = w_capacity - (impairment * oci)
    # Clipped at 0 to prevent negative weight
    w_effective = w_capacity - (oci_impairment * oci_score)
    if w_effective < 0:
        w_effective = 0.0

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Use the calculated effective w
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]

        # Value Updating
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```