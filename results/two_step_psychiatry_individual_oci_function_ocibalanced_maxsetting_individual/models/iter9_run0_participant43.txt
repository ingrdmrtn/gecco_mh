Here are three new cognitive models based on the participant's high OCI score and the provided data, exploring mechanisms of uncertainty intolerance, compulsive perseveration, and post-error rigidity.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Transition Flattening Model.
    
    Hypothesis: High OCI participants may exhibit intolerance of uncertainty or distrust 
    the probabilistic structure of the task. This model proposes that OCI modulates 
    the transition matrix used in the Model-Based calculation, 'flattening' it towards 
    a uniform distribution (50/50). This represents a subjective belief that the 
    transitions are less reliable than they objectively are.
    
    Parameters:
    lr: [0, 1] Learning rate for Q-value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    stickiness: [0, 5] Choice stickiness for Stage 1.
    oci_flatness: [0, 1] Degree to which OCI flattens the transition matrix (0=True Matrix, 1=Flat).
    """
    lr, beta, w, stickiness, oci_flatness = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]

    # True transition matrix: A(0)->X(0) common, U(1)->Y(1) common
    T_true = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Flat transition matrix (maximum entropy / uniform uncertainty)
    T_flat = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    # Calculate effective transition matrix based on OCI
    # mixing_rate bounded by [0,1]
    mixing_rate = participant_oci * oci_flatness
    if mixing_rate > 1.0: mixing_rate = 1.0
        
    # Linearly interpolate between the true structure and a flat structure
    T_used = T_true * (1 - mixing_rate) + T_flat * mixing_rate

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value calculation using the OCI-flattened matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T_used @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to Stage 1
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update (TD)
        delta_s2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * delta_s2
        
        # Stage 1 Update (TD)
        delta_s1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_s1
        
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Stage 2 Perseveration Model.
    
    Hypothesis: High OCI leads to compulsive repetition (stickiness) not just in the 
    planning stage (Stage 1), but specifically in the execution stage (Stage 2). 
    This model separates stickiness into a base Stage 1 stickiness and an OCI-driven 
    Stage 2 stickiness (response repetition of the alien choice), representing 
    motor/response compulsivity.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stickiness_s1: [0, 5] Stickiness for Stage 1 choice (Spaceship).
    oci_stickiness_s2: [0, 5] OCI-scaled stickiness for Stage 2 choice (Alien).
    """
    lr, beta, w, stickiness_s1, oci_stickiness_s2 = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    # Calculate effective Stage 2 stickiness based on OCI
    eff_stick_s2 = participant_oci * oci_stickiness_s2

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_a2 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness_s1
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_curr = int(state[trial])
        q_net_2 = q_stage2_mf[s_curr].copy()
        
        # Apply response stickiness to Stage 2 (Alien choice)
        # This implies repeating the same motor action (Alien 0 or Alien 1)
        if prev_a2 != -1:
            q_net_2[prev_a2] += eff_stick_s2
            
        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_s2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * delta_s2
        
        # Stage 1 Update
        delta_s1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_s1
        
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI-Modulated Post-Error Rigidity Model.
    
    Hypothesis: High OCI participants exhibit "post-error rigidity". After a trial 
    with no reward (a loss or error), they become more deterministic (higher beta) 
    in the subsequent trial, attempting to compulsively correct the error or exploit 
    perceived patterns. Standard exploration (base beta) resumes after a reward.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature (used after a Reward=1 trial).
    w: [0, 1] Model-Based weight.
    stickiness: [0, 5] Choice stickiness for Stage 1.
    oci_post_error_beta: [0, 5] Increase in Beta after a 0-reward trial, scaled by OCI.
    """
    lr, beta_base, w, stickiness, oci_post_error_beta = model_parameters
    n_trials = len(action_1)
    participant_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    prev_a1 = -1
    prev_reward = 1.0 # Initialize as if previous trial was successful (standard beta start)
    
    for trial in range(n_trials):
        
        # Determine current beta based on previous reward
        if prev_reward == 0.0:
            # Post-error: Increase beta based on OCI (becoming more rigid/deterministic)
            current_beta = beta_base + (participant_oci * oci_post_error_beta)
        else:
            # Post-success: Use base beta
            current_beta = beta_base
            
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        if prev_a1 != -1:
            q_net_1[prev_a1] += stickiness
            
        exp_q1 = np.exp(current_beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 ---
        s_curr = int(state[trial])
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_curr])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_s2 = r - q_stage2_mf[s_curr, a2]
        q_stage2_mf[s_curr, a2] += lr * delta_s2
        
        delta_s1 = q_stage2_mf[s_curr, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_s1
        
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```