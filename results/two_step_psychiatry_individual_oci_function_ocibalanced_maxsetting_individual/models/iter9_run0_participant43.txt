Here are three new cognitive models that incorporate the OCI score to explain participant behavior in the two-step task.

### Model 1: Hybrid Learner with OCI-Modulated Habit Rigidity
This model hypothesizes that high OCI scores are associated with rigid habit formation. Specifically, it proposes that the learning rate for the Model-Free Stage 1 values (habits) is suppressed by the OCI score. A suppressed learning rate means that once habits are formed, they are updated very slowly, making the participant "stick" to their spaceship choice even when values change.

```python
def cognitive_model1_oci_habit_rigidity(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: Hybrid Learner with OCI-Modulated Habit Rigidity.
    
    Hypothesis: High OCI scores are associated with rigid habit formation. 
    This model posits that the learning rate for the Model-Free Stage 1 values (habits) 
    is suppressed by the OCI score, making habits harder to unlearn/change.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - habit_rigidity: [0, 10] Factor by which OCI reduces the learning rate for Stage 1 MF values.
      alpha_mf1 = learning_rate / (1 + habit_rigidity * OCI)
    """
    learning_rate, beta, w, habit_rigidity = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # OCI-modulated learning rate for Stage 1 MF
    # High OCI -> Lower effective learning rate for habits -> Rigidity
    lr_mf1 = learning_rate / (1.0 + habit_rigidity * oci_score)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1) # Numerical stability
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Stage 1 MF using the rigid (suppressed) learning rate
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_mf1 * delta_stage1
        
        # Update Stage 2 MF using the base learning rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Learner with OCI-Modulated Negative Learning Suppression
This model hypothesizes that high OCI individuals are insensitive to negative feedback (lack of reward), leading to persistence in behavior despite failure ("compulsive persistence"). The learning rate for negative prediction errors (Reward=0) is suppressed by the OCI score, meaning they update their values less when they don't get a coin compared to when they do.

```python
def cognitive_model2_oci_neg_suppression(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: Hybrid Learner with OCI-Modulated Negative Learning Suppression.
    
    Hypothesis: High OCI individuals may be insensitive to negative feedback (lack of reward),
    leading to persistence in behavior despite lack of reinforcement.
    The learning rate for negative prediction errors (Reward=0) is suppressed by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight of Model-Based control.
    - neg_suppress: [0, 10] Scaling factor for OCI-based suppression of negative learning.
      If delta < 0: alpha = learning_rate / (1 + neg_suppress * OCI)
    """
    learning_rate, beta, w, neg_suppress = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits_1 = beta * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # OCI-modulated learning rate for Stage 2
        # If outcome is worse than expected (delta < 0), suppress learning based on OCI
        current_lr = learning_rate
        if delta_stage2 < 0: 
             current_lr = learning_rate / (1.0 + neg_suppress * oci_score)
             
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Learner with OCI-Modulated Stage 1 Rigidity
This model suggests that OCI is associated with rigid, deterministic choice behavior specifically at the higher-level planning stage (Spaceship choice), while the lower-level choice (Alien choice) remains standard. This is modeled by scaling the inverse temperature (`beta`) for Stage 1 by the OCI score, making the first choice much sharper/stickier than the second.

```python
def cognitive_model3_oci_stage1_rigidity(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: Hybrid Learner with OCI-Modulated Stage 1 Rigidity.
    
    Hypothesis: OCI is associated with rigid, deterministic choice behavior specifically 
    at the higher-level planning stage (Spaceship choice), while the lower-level choice 
    remains flexible. This is modeled by scaling the inverse temperature (beta) for 
    Stage 1 by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Base inverse temperature (used for Stage 2).
    - w: [0, 1] Weight of Model-Based control.
    - rigidity_s1: [0, 10] Factor boosting beta for Stage 1 based on OCI.
      beta_1 = beta * (1 + rigidity_s1 * OCI)
    """
    learning_rate, beta, w, rigidity_s1 = model_parameters
    oci_score = oci[0]
    n_trials = len(action_1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    # Calculate Stage 1 beta once: Boosted by OCI
    beta_1 = beta * (1.0 + rigidity_s1 * oci_score)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_1 (rigid) for Stage 1
        logits_1 = beta_1 * q_net
        logits_1 = logits_1 - np.max(logits_1)
        exp_q1 = np.exp(logits_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        # Use standard beta for Stage 2
        logits_2 = beta * q_stage2_mf[state_idx]
        logits_2 = logits_2 - np.max(logits_2)
        exp_q2 = np.exp(logits_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```