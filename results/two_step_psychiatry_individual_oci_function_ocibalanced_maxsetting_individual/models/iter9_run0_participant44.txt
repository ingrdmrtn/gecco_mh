Here are 3 new cognitive models based on the participant's data and OCI score.

### Analysis of Participant Data
The participant has a high OCI score (0.7). Looking at the trial data:
- They show strong perseveration (stickiness) on Spaceship 1 (A), often choosing it many times in a row even after losses (e.g., trials 36-40).
- They seem to exhibit "habitual" behavior, where choices are repeated regardless of the specific outcome or transition, suggesting a reliance on model-free (habit) systems or a specific stickiness mechanism.
- The high OCI score suggests rigidity. In cognitive modeling, this often maps to:
    1.  **Imbalance in Model-Based/Model-Free weighting**: High OCI might lead to reliance on habits (Model-Free) over goal-directed planning (Model-Based), or vice versa depending on the theory (some theories suggest OCD is a failure of goal-directed control, others suggest over-active model-based checking). Given the repetition, a bias toward Model-Free or pure stickiness is likely.
    2.  **Learning Rate Asymmetry**: High OCI might make someone hyper-sensitive to losses (negative prediction errors) or insensitive to them (rigidly repeating regardless of feedback).
    3.  **Stickiness**: A direct tendency to repeat the previous action.

The previous best model used OCI-driven stickiness. I will explore variations on how OCI interacts with learning rates and the balance between systems.

### Model 1: OCI-Modulated Learning Rate Asymmetry
This model hypothesizes that high OCI individuals process positive and negative feedback differently. Specifically, they might be "stuck" in their ways because they learn less from negative outcomes (ignoring failures) or learn too much from positive outcomes (over-reinforcing). This model scales the learning rate for negative prediction errors based on the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Learning Rate Asymmetry.
    
    Hypothesis: High OCI scores lead to asymmetric learning from prediction errors.
    This model splits the learning rate into positive and negative domains.
    The negative learning rate is modulated by the OCI score, testing if
    obsessive symptoms lead to ignoring negative feedback (lowering lr_neg)
    or over-reacting to it.

    Bounds:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (0=MF, 1=MB).
    oci_neg_mod: [0, 5] Multiplier for OCI's effect on negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, oci_neg_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective negative learning rate based on OCI
    # We constrain it to be between 0 and 1
    lr_neg = lr_neg_base * (1 + oci_neg_mod * oci_score)
    if lr_neg > 1.0: lr_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) + 0.5  # Initialize neutral
    q_stage2_mf = np.zeros((2, 2)) + 0.5

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based value calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Decision ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        current_lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[s_idx, a2] += current_lr_2 * delta_stage2
        
        # Stage 1 RPE (using TD-learning from Stage 2 value)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        current_lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += current_lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven "Checking" (Uncertainty Penalty)
This model posits that OCI relates to "checking" behavior driven by uncertainty. Instead of just reward maximization, the agent is penalized for uncertainty. However, given the participant's repetitive behavior, this might manifest as an aversion to switching (which entails uncertainty) or a rigidity where the mixing weight `w` (MB vs MF) is shifted by OCI. Here, we test if OCI shifts the balance `w` towards Model-Free (habitual) control, making behavior more repetitive and less sensitive to transition structure.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Shifted Model-Based/Model-Free Balance.
    
    Hypothesis: High OCI scores shift the controller balance. 
    While standard models fit 'w', this model assumes 'w' is a function of OCI.
    Specifically, high OCI might degrade Model-Based control (planning) 
    in favor of rigid Model-Free habits.
    
    Bounds:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w_base: [0, 1] Baseline mixing weight for a person with 0 OCI.
    oci_w_shift: [0, 1] How much OCI reduces the Model-Based weight.
    stickiness: [0, 5] General choice perseveration bonus.
    """
    lr, beta, w_base, oci_w_shift, stickiness = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Calculate effective w. 
    # If oci_w_shift is high, w becomes smaller (more Model-Free/Habitual) as OCI increases.
    w = w_base * (1.0 - (oci_w_shift * oci_score))
    if w < 0: w = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action = -1

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add simple stickiness to previous choice
        if prev_action != -1:
            q_net[prev_action] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        prev_action = a1
        
        # --- Stage 2 ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Update ---
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        
        q_stage1_mf[a1] += lr * delta_stage1
        q_stage2_mf[s_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter $\lambda$ (lambda), common in reinforcement learning (TD($\lambda$)). $\lambda$ controls how much the Stage 1 value is updated by the Stage 2 outcome directly. If $\lambda=1$, it's like Monte Carlo learning (outcome directly updates stage 1). If $\lambda=0$, it's pure TD (stage 1 updates from stage 2 value).
Hypothesis: High OCI might correlate with a specific credit assignment styleâ€”perhaps "over-connecting" distal outcomes to choices (High Lambda).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI affects credit assignment depth.
    Standard Q-learning updates Stage 1 based on Stage 2's value (TD(0)).
    This model implements TD(lambda), where lambda allows the reward at the
    end of the trial to directly impact the Stage 1 choice.
    The OCI score scales lambda.
    
    Bounds:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    lambda_base: [0, 1] Base eligibility trace parameter.
    oci_lambda_mod: [0, 1] How much OCI increases/decreases lambda.
    """
    lr, beta, w, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]

    # Modulate lambda by OCI. 
    # We assume OCI increases the 'connection' or 'responsibility' felt for outcomes.
    lam = lambda_base + (oci_lambda_mod * oci_score)
    if lam > 1.0: lam = 1.0
    if lam < 0.0: lam = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Choice 2 ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        r = reward[trial]
        
        # Standard Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr * delta_stage2
        
        # Stage 1 update with Eligibility Trace
        # In TD(1) or Monte Carlo, Stage 1 is updated by the final Reward.
        # In TD(0), Stage 1 is updated by Stage 2 Value.
        # TD(lambda) blends these.
        # Effective target for Stage 1 = (1-lambda)*Q_s2 + lambda*Reward
        
        # Note: A simplified implementation of eligibility traces for 2-step task:
        # Update = lr * [ (1-lambda)*(Q_s2 - Q_s1) + lambda*(Reward - Q_s1) ]
        # Which simplifies to: lr * [ (1-lambda)*delta_stage1_TD0 + lambda*delta_stage1_MonteCarlo ]
        
        delta_TD0 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1] # Prediction error based on value
        delta_MC = r - q_stage1_mf[a1]                       # Prediction error based on outcome
        
        combined_delta = (1 - lam) * delta_TD0 + lam * delta_MC
        q_stage1_mf[a1] += lr * combined_delta

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```