Here are three new cognitive models that incorporate the OCI score to explain the participant's decision-making behavior.

### Model 1: Cognitive Stiffness (OCI-Suppressed Learning)
**Hypothesis:** High OCI scores are associated with cognitive rigidity and "stiffness." In this model, OCI acts as a suppressor on the learning rate. Participants with high OCI symptoms update their value estimates more slowly in response to prediction errors, making them less sensitive to environmental volatility and more prone to sticking with established priors or early experiences.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Cognitive Stiffness Model.
    
    Hypothesis: High OCI leads to reduced learning rates (cognitive stiffness).
    The effective learning rate is scaled down by the OCI score, making the 
    participant less responsive to new reward information (both positive and negative).
    
    Parameters:
    - lr_base: [0, 1] Base learning rate.
    - stiffness: [0, 10] Factor by which OCI suppresses the learning rate.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - lambd: [0, 1] Eligibility trace for Stage 1 updates.
    """
    lr_base, stiffness, beta, w, lambd = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # OCI modulation: Higher OCI reduces the effective learning rate
    # Using a divisor ensures the rate scales down smoothly.
    learning_rate = lr_base / (1.0 + stiffness * oci_score)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        q_net_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_net_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Eligibility trace update for Stage 1
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Loss Anxiety (OCI-Amplified Punishment Sensitivity)
**Hypothesis:** High OCI is often correlated with anxiety and fear of failure. This model hypothesizes that high OCI participants are hyper-sensitive to "losses" (receiving 0 coins). While they learn from rewards normally, the prediction error generated by a lack of reward is amplified by their OCI score. This causes rapid devaluation of options that fail to pay out, potentially leading to "switch" behavior after losses even in high-probability states.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Loss Anxiety Model.
    
    Hypothesis: High OCI leads to hyper-sensitivity to negative outcomes (0 reward).
    When a reward of 0 is received, the prediction error is amplified by the OCI score,
    causing a stronger negative update (avoidance learning) compared to positive updates.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for rewards.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lambd: [0, 1] Eligibility trace.
    - anxiety_gain: [0, 5] Multiplier for prediction errors when reward is 0, scaled by OCI.
    """
    learning_rate, beta, w, lambd, anxiety_gain = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        q_net_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_net_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
  
        # Calculate effective prediction error scaling
        # If reward is 0 (loss), amplify the error based on OCI (Anxiety)
        scale_factor = 1.0
        if r == 0.0:
            scale_factor = 1.0 + (anxiety_gain * oci_score)

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Note: Stage 1 update relies on value difference, not direct reward, so we don't scale here
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Apply anxiety scaling to the reward prediction error
        effective_delta2 = delta_stage2 * scale_factor
        
        q_stage2_mf[state_idx, a2] += learning_rate * effective_delta2
        q_stage1_mf[a1] += learning_rate * lambd * effective_delta2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Tunnel Vision (OCI-Modulated Q-Value Decay)
**Hypothesis:** High OCI is associated with an intense focus on the current task or compulsion, leading to a neglect of alternative options. In this model, OCI drives the decay of *unchosen* Q-values. While the chosen action is updated via RL, the unchosen action's value decays toward 0. Higher OCI accelerates this decay, creating a "tunnel vision" effect where the participant rapidly forgets the value of alternatives, effectively locking them into their current preference unless a strong negative update occurs.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Tunnel Vision Model (Q-Value Decay).
    
    Hypothesis: High OCI causes 'tunnel vision' where values of unchosen options 
    decay/fade rapidly. The participant focuses solely on the active path.
    Unchosen Q-values decay towards 0 at a rate determined by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lambd: [0, 1] Eligibility trace.
    - tunnel_rate: [0, 1] Decay rate for unchosen options, scaled by OCI.
    """
    learning_rate, beta, w, lambd, tunnel_rate = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate decay factor based on OCI
    # If OCI is high, decay is stronger (factor is smaller)
    decay_factor = 1.0 - (tunnel_rate * oci_score)
    # Ensure bounds
    if decay_factor < 0: decay_factor = 0.0

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # Decay the unchosen Stage 1 MF value
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= decay_factor

        # policy for the second choice
        q_net_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * q_net_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Decay the unchosen Stage 2 MF value (for the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= decay_factor
  
        # Standard RL Updates
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        q_stage1_mf[a1] += learning_rate * lambd * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```