```python
import numpy as np

def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4,
    oci):
    """
    Simulates the Dynamic Perseveration Model.
    Perseveration strength accumulates with consecutive choices of the same option.
    OCI modulates the rate at which this stickiness builds up.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list): [lr, beta, w, pers_static, pers_accum_base, pers_accum_oci]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities.
        oci (float, list, or None): Obsessive-Compulsive Inventoryâ€“Revised score.

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    
    # Unpack parameters
    lr, beta, w, pers_static, pers_accum_base, pers_accum_oci = parameters
    
    # Handle OCI input (can be scalar, list/array, or None)
    if isinstance(oci, (list, np.ndarray)):
        current_oci = oci[0]
    else:
        current_oci = oci if oci is not None else 0.0

    # Calculate the effective accumulation rate based on OCI
    accum_rate = pers_accum_base + pers_accum_oci * current_oci

    # Initialize Q-values
    # q_mf: Stage 1 model-free values (2 options)
    q_mf = np.zeros(2)
    # q2: Stage 2 values (2 states x 2 options)
    q2 = np.zeros((2, 2))
    
    # Transition matrix: tm[action] -> [prob_state0, prob_state1]
    # Action 0 -> 0.7 to State 0, 0.3 to State 1
    # Action 1 -> 0.3 to State 0, 0.7 to State 1
    tm = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Perseveration tracking variables
    consecutive_counts = np.zeros(2)
    last_action = -1

    # Output arrays
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # --- Stage 1 Decision ---
        
        # Model-Based Value: Transition * Max(Stage 2 Values)
        q_mb = tm @ np.max(q2, axis=1)
        
        # Net Value (Hybrid MB/MF)
        q_net = w * q_mb + (1 - w) * q_mf

        # Static Perseveration (1-back stickiness)
        if last_action != -1:
            q_net[last_action] += pers_static

        # Dynamic Perseveration (Accumulation based on consecutive history)
        q_net += consecutive_counts * accum_rate

        # Action Selection (Softmax)
        exp_q = np.exp(beta * q_net)
        probs1 = exp_q / np.sum(exp_q)
        a1 = np.random.choice([0, 1], p=probs1)

        # Update Consecutive Counts (Logic from fitting code)
        if a1 == last_action:
            consecutive_counts[a1] += 1
        else:
            consecutive_counts[a1] = 1 # Reset to 1 (current choice becomes the new streak)
            consecutive_counts[1-a1] = 0 # Reset other option
        last_action = a1

        # --- Transition ---
        # Determine State 2 based on transition matrix probabilities
        transition_probs = tm[a1]
        s2 = np.random.choice([0, 1], p=transition_probs)

        # --- Stage 2 Decision ---
        
        # Action Selection (Softmax)
        exp_q2 = np.exp(beta * q2[s2])
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = np.random.choice([0, 1], p=probs2)

        # --- Reward Outcome ---
        # Map state/action to specific drift probability
        if s2 == 0:
            prob_r = drift1[t] if a2 == 0 else drift2[t]
        else: # s2 == 1
            prob_r = drift3[t] if a2 == 0 else drift4[t]
        
        r = 1 if np.random.random() < prob_r else 0

        # --- Learning Updates ---
        
        # Stage 1 Update: Model-Free TD(0) using Stage 2 Q-value of chosen option
        pe1 = q2[s2, a2] - q_mf[a1]
        q_mf[a1] += lr * pe1
        
        # Stage 2 Update: TD(0) using obtained Reward
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += lr * pe2

        # Record data for this trial
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```