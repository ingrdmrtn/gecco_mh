class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants exhibit a tendency to
    perseverate on their previous first-stage choice (a 'stay' bias). However,
    this perseveration is sensitive to the outcome of the previous trial.
    Specifically, low anxiety (lower STAI) is hypothesized to make individuals
    more adaptive: they are less likely to perseverate on a choice that led
    to a zero reward (negative outcome) on the previous trial, allowing them
    to switch away from unrewarding options more readily. This suggests low
    anxiety facilitates flexible switching when performance is poor.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax action selection.
    perseveration_base: [0, 1] - Base strength of the perseveration bias,
                                 applied when the previous outcome was rewarding.
    stai_adaptiveness: [0, 1] - How much low STAI reduces perseveration
                                after a zero reward. A value of 0 means
                                STAI has no effect on this reduction.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.perseveration_base, self.stai_adaptiveness = model_parameters

    def init_model(self) -> None:
        # last_action1 and last_reward are initialized to None in CognitiveModelBase.
        # They will be used from the second trial onwards.
        pass

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities, incorporating an outcome-sensitive
        and STAI-modulated perseveration bias.
        """
        q_biased_stage1 = np.copy(self.q_stage1)

        if self.last_action1 is not None and self.last_reward is not None:
            current_perseveration_strength = self.perseveration_base

            if self.last_reward == 0:
                # If the last reward was 0, reduce the perseveration strength.
                # Lower STAI (e.g., 0.2875) means a higher (1 - self.stai),
                # which leads to a stronger reduction in perseveration, scaled by stai_adaptiveness.
                reduction_factor = (1 - (1 - self.stai) * self.stai_adaptiveness)
                # Ensure the reduction factor does not make perseveration negative
                reduction_factor = np.clip(reduction_factor, 0, 1)
                current_perseveration_strength *= reduction_factor
            
            q_biased_stage1[self.last_action1] += current_perseveration_strength

        return self.softmax(q_biased_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values using standard TD learning.
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)