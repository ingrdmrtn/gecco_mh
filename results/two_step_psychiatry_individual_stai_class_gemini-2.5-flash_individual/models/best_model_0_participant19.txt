class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants exhibit a "recency bias" or "forgetting"
    mechanism, where older value estimates decay over time, making recent experiences more influential.
    This decay is applied to both first and second-stage Q-values before each trial's update.
    The participant's anxiety level (STAI score) is hypothesized to increase this forgetting rate,
    meaning higher anxiety leads to a stronger emphasis on recent outcomes and a faster decay
    of past experiences, potentially due to reduced attentional resources or increased focus on
    immediate, salient events, or simply a reduced ability to maintain long-term value representations.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate for Q-values)
    beta: [0, 10] (Softmax inverse temperature)
    forget_rate_base: [0, 1] (Base rate at which Q-values decay per trial; 0=no decay, 1=complete decay)
    k_stai_forget: [0, 1] (Sensitivity of forgetting rate to STAI score; adds to base)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.forget_rate_base, self.k_stai_forget = model_parameters
        # Calculate the effective forgetting rate, modulated by STAI, ensuring it's within [0, 1)
        # We cap at 0.99 to prevent total forgetting in a single step, which would make learning impossible.
        self.forget_rate_effective = min(0.99, self.forget_rate_base + self.stai * self.k_stai_forget)

    def pre_trial(self) -> None:
        """
        Apply forgetting to Q-values before the current trial's update.
        """
        decay_factor = 1.0 - self.forget_rate_effective
        self.q_stage1 *= decay_factor
        self.q_stage2 *= decay_factor
        
        # Ensure Q-values don't decay below a reasonable minimum (e.g., min possible reward -1)
        # This prevents values from becoming excessively negative solely due to forgetting.
        self.q_stage1 = np.maximum(self.q_stage1, -1.0)
        self.q_stage2 = np.maximum(self.q_stage2, -1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values using standard TD learning after the forgetting step.
        """
        # Standard Q-learning updates
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)