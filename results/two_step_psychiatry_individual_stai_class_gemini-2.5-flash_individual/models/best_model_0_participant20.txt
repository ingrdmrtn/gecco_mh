class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that the participant uses a hybrid model-free/model-based strategy
    to evaluate first-stage actions. However, their high anxiety enhances a 'stickiness' bias, making
    them more likely to repeat their previous first-stage choice regardless of its current value.
    This perseveration might reflect a reduced ability to flexibly adapt choices under stress or
    uncertainty, leading to habitual repetition of past actions. The STAI score directly modulates
    the strength of this stickiness bonus.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for both stages.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Weight for model-based control (0=pure MF, 1=pure MB).
    stickiness_base: [0, 2] - Base magnitude of the stickiness bonus.
    stai_effect_on_stickiness: [0, 1] - How much STAI score boosts stickiness.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w, self.stickiness_base, self.stai_effect_on_stickiness = model_parameters
        
    def init_model(self) -> None:
        # Initialize last_action1 to a value that won't trigger stickiness on the first trial
        self.last_action1 = -1 

    def policy_stage1(self) -> np.ndarray:
        # Calculate Model-Based (MB) Q-values
        q_mb = np.zeros(self.n_choices)
        for s1_action in range(self.n_choices):
            # Q_MB(s1_a) = sum_over_states( P(next_state|s1_a) * max(Q_MF(next_state, s2_a)) )
            for next_state in range(self.n_states):
                # self.T[s1_action, next_state] is P(next_state | s1_action)
                q_mb[s1_action] += self.T[s1_action, next_state] * np.max(self.q_stage2[next_state, :])
        
        # Combine Model-Free and Model-Based Q-values
        hybrid_q_stage1 = (1 - self.w) * self.q_stage1 + self.w * q_mb

        # Add stickiness bonus, modulated by anxiety, to the last chosen first-stage action
        if self.last_action1 != -1: # Only apply if a previous action exists
            stickiness_actual = self.stickiness_base + self.stai * self.stai_effect_on_stickiness
            hybrid_q_stage1[self.last_action1] += stickiness_actual
        
        return self.softmax(hybrid_q_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning for both stages
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)