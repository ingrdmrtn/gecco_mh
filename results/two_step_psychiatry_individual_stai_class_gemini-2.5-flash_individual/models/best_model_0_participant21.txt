class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model posits that participants exhibit choice perseveration,
    meaning they have a tendency to repeat their previous first-stage action.
    This 'stickiness' adds a bonus to the value of the action chosen on the
    immediately preceding trial. The strength of this stickiness is modulated
    by the participant's anxiety level (STAI score). Higher anxiety is
    hypothesized to increase stickiness, leading to more rigid choices as a
    strategy to reduce cognitive load or avoid uncertainty.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       (Learning rate)
    beta: [0, 10]       (Softmax inverse temperature)
    stick_base: [0, 1]  (Base stickiness bonus)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        self.alpha, self.beta, self.stick_base = model_parameters

    def init_model(self) -> None:
        """
        Initialize model state. The effective stickiness parameter is
        calculated based on the base stickiness and the STAI score.
        """
        # Modulate stickiness based on STAI score. Higher STAI means higher effective stickiness.
        # If STAI is 0, stickiness = stick_base. If STAI is 1, stickiness = 2*stick_base.
        self.stickiness = self.stick_base * (1 + self.stai)
        # Ensure stickiness is non-negative. There's no inherent upper bound,
        # but values typically don't exceed 1-2 in practice for a bonus.
        self.stickiness = np.clip(self.stickiness, 0.0, None) 

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities. This method applies a stickiness
        bonus to the value of the last chosen first-stage action before
        applying the softmax function.
        """
        # Create a copy of the current Q-values to apply the bonus
        q_values_adjusted = np.copy(self.q_stage1)
        
        # Apply the stickiness bonus if a previous action has been recorded
        # (i.e., not the very first trial).
        if self.last_action1 is not None:
            q_values_adjusted[self.last_action1] += self.stickiness
        
        return self.softmax(q_values_adjusted, self.beta)
    
    # The value_update and policy_stage2 methods remain as in the base class.

# Create the callable cognitive model function for ParticipantModel3

cognitive_model3 = make_cognitive_model(ParticipantModel3)