class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants' stage-1 choices are influenced not only by
    learned values but also by a "stickiness" or perseveration bias towards the previously chosen
    spaceship. This stickiness is hypothesized to be modulated by anxiety, where a higher STAI
    score leads to a stronger bias to repeat the last action, potentially reflecting reduced
    exploration or increased reliance on familiar choices under stress.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate
    beta: [0, 10] - Inverse temperature
    stickiness_base: [0, 2] - Base stickiness bonus for repeating the last stage-1 action
    stickiness_stai_sens: [-2, 2] - Sensitivity of stickiness to STAI score
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.stickiness_stai_sens = model_parameters

    def init_model(self) -> None:
        """Initialize model-specific parameters."""
        # Calculate effective stickiness based on STAI score
        self.stickiness_effective = self.stickiness_base + self.stickiness_stai_sens * self.stai
        
        # The base class uses self.alpha for value_update and self.beta for softmax,
        # which are correctly unpacked.

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities, incorporating a stickiness bias."""
        # Start with current Q-values
        modified_q_stage1 = np.copy(self.q_stage1)
        
        # Apply stickiness bonus if a last action exists and it's not the very first trial
        if self.last_action1 is not None and self.trial > 0:
            modified_q_stage1[self.last_action1] += self.stickiness_effective
        
        return self.softmax(modified_q_stage1, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)