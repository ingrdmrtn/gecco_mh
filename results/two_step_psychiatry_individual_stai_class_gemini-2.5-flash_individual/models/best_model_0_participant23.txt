class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: The participant's first-stage choices are driven by learned Q-values,
    but also by a tendency to perseverate on the previously chosen spaceship.
    This perseveration bonus is dynamically adjusted based on the outcome of the
    previous trial and is modulated by the participant's anxiety level (STAI score).
    Specifically, medium anxiety might lead to a reduced perseveration (or increased
    switching) after an unrewarded trial.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-value updates for both stages.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    perseveration_base: [0, 1] - Base bonus added to the Q-value of the
                                 previously chosen stage-1 action.
    anxiety_switch_factor: [0, 1] - Factor by which anxiety reduces the perseveration
                                    bonus after a non-rewarded trial (reward = 0).
                                    Higher values mean anxiety makes them more likely to switch.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.perseveration_base, self.anxiety_switch_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_modified_stage1 = np.copy(self.q_stage1)
        
        # Apply perseveration bonus if a last action exists
        if self.last_action1 is not None:
            # Determine the effective perseveration bonus for the current trial
            effective_bonus = self.perseveration_base
            # If the last reward was 0, anxiety reduces the bonus (or makes it a penalty effectively)
            if self.last_reward == 0:
                # The reduction is scaled by STAI and anxiety_switch_factor
                effective_bonus -= (self.stai * self.anxiety_switch_factor)
            
            q_modified_stage1[self.last_action1] += effective_bonus
            
        return self.softmax(q_modified_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for Q-values
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    # The base class's post_trial method already stores last_action1 and last_reward
    # which are used by policy_stage1 in the subsequent trial.

cognitive_model3 = make_cognitive_model(ParticipantModel3)