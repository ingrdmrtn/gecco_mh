class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that the participant's exploration tendency
    in the second stage is not fixed but dynamically adjusts based on the
    perceived uncertainty or ambiguity of the options in that specific planet state.
    Anxiety (STAI score) amplifies this state-dependent exploration. When the
    Q-values for aliens on a planet are very similar (high ambiguity), the
    participant explores more (lower beta), and this tendency is stronger with higher anxiety.
    Stage 1 choices maintain a fixed exploration level.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta_stage1: [0, 10] - Inverse temperature for softmax choice in Stage 1.
    beta_base_stage2: [0, 10] - Base inverse temperature for softmax choice in Stage 2.
    stai_exploration_factor: [0, 5] - Factor by which STAI score increases exploration
                                       (reduces beta) in Stage 2 when Q-values are similar.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_stage1, self.beta_base_stage2, self.stai_exploration_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Stage 1 uses a fixed inverse temperature (beta_stage1)
        return self.softmax(self.q_stage1, self.beta_stage1)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Calculate the difference between the max and min Q-values for the current state
        # A smaller difference indicates higher uncertainty/ambiguity
        q_diff = np.max(self.q_stage2[state]) - np.min(self.q_stage2[state])
        
        # Adjust beta for stage 2:
        # Smaller q_diff (more uncertainty) leads to a lower effective beta (more exploration).
        # STAI score amplifies this reduction.
        # Ensure beta remains positive and within reasonable bounds (e.g., at least 0.1 to avoid division by zero)
        adjusted_beta_stage2 = np.clip(
            self.beta_base_stage2 - self.stai_exploration_factor * self.stai * (1 - q_diff),
            0.1, # Minimum beta to allow some exploitation
            10.0 # Maximum beta
        )
        
        return self.softmax(self.q_stage2[state], adjusted_beta_stage2)

    # The default value_update and post_trial methods are suitable.

# Create the cognitive_model2 function

cognitive_model2 = make_cognitive_model(ParticipantModel2)