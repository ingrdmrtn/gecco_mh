class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Habit Formation.
    This model proposes that the participant develops strong habits for specific actions,
    especially for those that have been historically rewarding. Medium anxiety (STAI score)
    amplifies the rate at which these habits form, making the chosen actions more preferred
    over time, independent of their current learned Q-values. This could explain the
    participant's strong and consistent preference for spaceship 1 and alien 0.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    habit_alpha_base: [0, 1] - Base learning rate for updating habit strength.
    stai_habit_gain_factor: [0, 1] - Factor by which STAI score linearly increases the habit learning rate.
    habit_decay: [0, 1] - Decay rate for habit strength on unchosen actions (0=no decay, 1=full decay).
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.habit_alpha_base, self.stai_habit_gain_factor, self.habit_decay = model_parameters

    def init_model(self) -> None:
        # Effective habit learning rate based on base rate and STAI score
        self.effective_habit_alpha = self.habit_alpha_base + self.stai_habit_gain_factor * self.stai
        self.effective_habit_alpha = np.clip(self.effective_habit_alpha, 0, 1) # Clamp to [0, 1]

        # Initialize habit strength for first-stage actions
        self.habit_strength_stage1 = np.zeros(self.n_choices)
        # Initialize habit strength for second-stage actions (per state)
        self.habit_strength_stage2 = np.zeros((self.n_states, self.n_choices))

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-value updates
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Q-value update towards the updated Q-value of the chosen stage 2 action
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

        # Update habit strength for chosen actions (increases towards 1)
        self.habit_strength_stage1[action_1] += self.effective_habit_alpha * (1 - self.habit_strength_stage1[action_1])
        self.habit_strength_stage2[state, action_2] += self.effective_habit_alpha * (1 - self.habit_strength_stage2[state, action_2])

        # Decay habit strength for unchosen actions (decreases towards 0)
        unchosen_a1 = 1 - action_1
        self.habit_strength_stage1[unchosen_a1] *= (1 - self.habit_decay)
        
        unchosen_a2 = 1 - action_2
        self.habit_strength_stage2[state, unchosen_a2] *= (1 - self.habit_decay)

    def policy_stage1(self) -> np.ndarray:
        # Combine Q-values with habit strength for choice. Habit strength acts as an additive bias.
        combined_values = self.q_stage1 + self.habit_strength_stage1
        return self.softmax(combined_values, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Combine Q-values with habit strength for choice.
        combined_values = self.q_stage2[state] + self.habit_strength_stage2[state]
        return self.softmax(combined_values, self.beta)

# Make the model callable

cognitive_model2 = make_cognitive_model(ParticipantModel2)