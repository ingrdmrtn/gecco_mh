class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-modulated General Perseveration.
    High anxiety leads to a general tendency to perseverate on the last chosen Stage 1 action,
    regardless of the outcome (rewarded or not). This represents a form of behavioral rigidity
    or reduced exploration, with the STAI score additively increasing this perseveration bias.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-value updates.
    beta: [0, 10] - Inverse temperature for softmax action selection.
    perseveration_bonus_base: [0, 5] - Base value for the perseveration bonus.
    stai_perseveration_scale: [0, 5] - Scaling factor for STAI's contribution to the perseveration bonus.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.perseveration_bonus_base, self.stai_perseveration_scale = model_parameters

    def init_model(self) -> None:
        super().init_model()
        # Initialize last_action1 to a sentinel value to prevent bonus on trial 0
        self.last_action1 = -1

    def policy_stage1(self) -> np.ndarray:
        # Calculate the effective perseveration bonus for the current trial
        effective_perseveration_bonus = 0.0
        if self.last_action1 != -1: # Only apply if a previous action was taken
            effective_perseveration_bonus = self.perseveration_bonus_base + self.stai_perseveration_scale * self.stai
            # Clip the bonus to a reasonable range
            effective_perseveration_bonus = np.clip(effective_perseveration_bonus, 0, 10)

        # Apply perseveration bonus to the Q-value of the last chosen action
        q_stage1_biased = np.copy(self.q_stage1)
        if self.last_action1 != -1:
            q_stage1_biased[self.last_action1] += effective_perseveration_bonus

        return self.softmax(q_stage1_biased, self.beta)

    # The value_update method remains the standard TD learning from the base class,
    # as this model's hypothesis is about action selection bias, not learning rules.
    # def value_update(...) is inherited from CognitiveModelBase

cognitive_model3 = make_cognitive_model(ParticipantModel3)