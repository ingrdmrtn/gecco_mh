class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants exhibit a tendency to repeat their previous
    first-stage action (perseveration). This perseveration is modeled as a bonus added to the
    Q-value of the last chosen action. Furthermore, anxiety (STAI score) modulates the
    exploration-exploitation trade-off, with higher anxiety potentially leading to more
    deterministic choices (higher beta), reducing exploration.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta_base: [0, 10] - Base softmax inverse temperature.
    perseveration_bonus: [0, 2] - Bonus added to the Q-value of the previous stage-1 action.
    stai_beta_mod: [-5, 5] - How STAI score modulates the beta parameter (positive values mean higher anxiety leads to higher beta).
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.perseveration_bonus, self.stai_beta_mod = model_parameters

    def init_model(self) -> None:
        # Calculate the actual beta based on STAI score
        self.beta = self.beta_base + self.stai_beta_mod * self.stai
        self.beta = np.clip(self.beta, 0.1, 10) # Ensure beta stays within a reasonable range (avoid beta=0 for softmax)

    def policy_stage1(self) -> np.ndarray:
        # Apply perseveration bonus to the Q-values if a previous action exists
        q_biased_stage1 = np.copy(self.q_stage1)
        if self.last_action1 is not None:
            q_biased_stage1[self.last_action1] += self.perseveration_bonus
        
        return self.softmax(q_biased_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning rule
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)