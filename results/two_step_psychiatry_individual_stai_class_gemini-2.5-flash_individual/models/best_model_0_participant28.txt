class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Model-Free RL with Anxiety-Modulated Choice Stickiness.
    This model assumes standard Q-learning for value updates. However,
    the participant's stage-1 choices are also influenced by a "stickiness"
    bias, making them more likely to repeat their previous first-stage action.
    High anxiety (STAI score) is hypothesized to increase this choice stickiness,
    leading to less flexible decision-making or increased reliance on simple heuristics
    under stress.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    stick_base: [0, 5] - Base tendency to stick to the previous choice.
    stick_stai_mod: [0, 5] - Modulatory parameter for STAI on choice stickiness.
                              A positive value means higher anxiety increases stickiness.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stick_stai_mod = model_parameters

    def init_model(self) -> None:
        # Initialize choice stickiness bonus.
        # It will be added to the Q-value of the last chosen action.
        self.stickiness_bonus = np.zeros(self.n_choices)
        # Calculate the effective stickiness, modulated by STAI score.
        # Clip to ensure it's within a reasonable non-negative range.
        self.eff_stickiness = np.clip(self.stick_base + self.stick_stai_mod * self.stai, 0, 5)

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities, including choice stickiness.
        """
        # Apply stickiness bonus to the Q-values
        q_stage1_biased = self.q_stage1 + self.stickiness_bonus
        return self.softmax(q_stage1_biased, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update stickiness bonus after each trial.
        """
        # Call base class post_trial to update last_action1 etc.
        super().post_trial(action_1, state, action_2, reward) 

        # Reset stickiness bonus for the next trial, then apply to the chosen action
        self.stickiness_bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None: # Only apply after the first trial
            self.stickiness_bonus[self.last_action1] = self.eff_stickiness

# The default value_update method from CognitiveModelBase is used, which correctly utilizes self.alpha.

cognitive_model2 = make_cognitive_model(ParticipantModel2)