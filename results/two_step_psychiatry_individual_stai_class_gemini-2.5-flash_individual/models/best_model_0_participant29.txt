class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases "choice stickiness" or a repetition bias
    for the first-stage action. This means participants are more likely to repeat
    the spaceship choice they made on the previous trial, with the strength of
    this bias increasing with their STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta: [0, 10] (Inverse temperature for softmax choice)
    stickiness_stai_factor: [0, 5] (Factor by which STAI score influences stickiness bonus.
                                    A positive value means higher STAI increases stickiness.)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_stai_factor = model_parameters
        
    def init_model(self) -> None:
        # Initialize last_action1 to a value that signifies no previous action (e.g., -1)
        # This prevents stickiness from being applied on the very first trial.
        self.last_action1 = -1 

    def policy_stage1(self) -> np.ndarray:
        # Create a copy of the current Q-values for Stage 1 to apply the stickiness bonus
        q_prime_stage1 = np.copy(self.q_stage1)
        
        # Apply stickiness bonus only if a previous Stage 1 action exists
        if self.last_action1 != -1: 
            # The stickiness bonus is scaled by the STAI score and the stickiness factor.
            # This makes the previous choice more attractive.
            stickiness_bonus = self.stickiness_stai_factor * self.stai
            q_prime_stage1[self.last_action1] += stickiness_bonus
            
        # Compute action probabilities using softmax on the modified Q-values
        return self.softmax(q_prime_stage1, self.beta)

    # The value_update and post_trial methods use the default implementations
    # from CognitiveModelBase, where post_trial correctly updates self.last_action1.

cognitive_model3 = make_cognitive_model(ParticipantModel3)