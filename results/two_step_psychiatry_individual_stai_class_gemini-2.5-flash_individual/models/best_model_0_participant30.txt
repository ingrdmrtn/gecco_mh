class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases choice perseveration (stickiness) at the
    first stage. Participants are more likely to repeat their previous first-stage
    choice, with this tendency amplified by anxiety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    stickiness_base: [-2, 2] - Base bonus added to the Q-value of the previous first-stage choice.
                               Can be negative (switching bias) or positive (perseveration bias).
    stickiness_stai_impact: [0, 2] - How much STAI score increases the stickiness.
                                      A positive value implies higher anxiety leads to more perseveration.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.stickiness_stai_impact = model_parameters
        # Calculate the effective stickiness bonus, amplified by STAI score
        # and clipped to a reasonable range.
        self.stickiness_effective = np.clip(self.stickiness_base + self.stai * self.stickiness_stai_impact, -5.0, 5.0)

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities, adding a stickiness bonus
        to the Q-value of the last chosen action.
        """
        q_biased = np.copy(self.q_stage1)
        # Apply stickiness only after the first trial, if a previous action exists
        if self.last_action1 is not None and self.trial > 0:
            q_biased[int(self.last_action1)] += self.stickiness_effective
        
        return self.softmax(q_biased, self.beta)

    # The default value_update method (TD learning) is used, as the stickiness
    # only affects action selection at stage 1. It will use self.alpha.

cognitive_model3 = make_cognitive_model(ParticipantModel3)