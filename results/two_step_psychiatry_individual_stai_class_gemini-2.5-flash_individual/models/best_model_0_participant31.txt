class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to perseverative behavior, making participants
    more likely to repeat their previous stage-1 choice. This model introduces a
    'stickiness' bonus to the Q-value of the last chosen stage-1 action.
    This bonus is amplified by the participant's STAI score, reflecting a tendency
    for anxious individuals to stick to familiar choices, potentially due to
    uncertainty avoidance or reduced cognitive flexibility.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax.
    stickiness_base: [0, 5] - Base bonus added to the last chosen stage-1 action.
    stai_perseverance_gain: [0, 5] - Factor by which STAI score amplifies the stickiness bonus.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.stai_perseverance_gain = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Start with current Q-values
        q_values_biased = np.copy(self.q_stage1)

        # Apply stickiness bias if a previous action exists (not on the very first trial)
        if self.last_action1 is not None and self.trial > 0:
            # Calculate the anxiety-modulated stickiness bonus
            perseverance_bonus = self.stickiness_base * (1 + self.stai * self.stai_perseverance_gain)
            q_values_biased[int(self.last_action1)] += perseverance_bonus
        
        return self.softmax(q_values_biased, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Use the single alpha for value updates, as defined in base class.
        # This model focuses on choice policy, not learning rate.
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)