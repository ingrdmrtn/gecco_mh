class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants exhibit a tendency to repeat their
    previous Stage 1 choice (perseveration), and the strength of this perseveration is
    modulated by their anxiety level (stai). Higher anxiety is hypothesized to increase
    this perseveration, making participants more likely to stick with a previously chosen
    option, potentially as a coping mechanism for uncertainty or to reduce cognitive load.
    Given the participant's medium anxiety, their perseveration will be moderately increased.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate.
    beta: [0, 10] - Softmax inverse temperature.
    pers_strength_base: [0, 2] - Base strength of the perseveration bonus.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_strength_base = model_parameters

    def init_model(self) -> None:
        # Initialize last_action1 to a value that won't trigger perseveration on the first trial.
        self.last_action1 = -1 
        # Calculate effective perseveration strength based on STAI score.
        # (1 + self.stai) ensures that higher anxiety (higher stai) leads to a stronger perseveration.
        self.pers_strength_effective = self.pers_strength_base * (1 + self.stai)
        # Clip to a reasonable maximum for the bonus.
        self.pers_strength_effective = np.clip(self.pers_strength_effective, 0, 5) 

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities, adding a perseveration bonus to the last chosen action."""
        q_values_with_bias = np.copy(self.q_stage1)
        
        if self.last_action1 != -1: # Apply perseveration bias if a previous action exists
            q_values_with_bias[self.last_action1] += self.pers_strength_effective
            
        return self.softmax(q_values_with_bias, self.beta)

    # The value_update method uses the standard TD learning from the base class,
    # with the 'self.alpha' parameter unpacked directly.
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome using the model's alpha."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)