class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety (STAI) increases action perseveration (stickiness).
    Participants with high anxiety are more likely to repeat their last
    stage-1 action, reflecting a tendency to stick to familiar choices.
    The STAI score modulates the strength of this stickiness bonus.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta: [0, 10] (Inverse temperature for softmax)
    stickiness_base: [0, 1] (Base value for repeating the last action)
    stai_stickiness_effect: [0, 1] (How much STAI amplifies stickiness)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.stai_stickiness_effect = model_parameters

    def init_model(self) -> None:
        # Initialize stickiness values for stage 1 actions
        self.stickiness_values = np.zeros(self.n_choices)

    def pre_trial(self) -> None:
        # Reset stickiness values for the current trial
        self.stickiness_values[:] = 0
        
        # If it's not the first trial, apply stickiness to the last chosen action
        if self.trial > 0 and self.last_action1 is not None:
            # Stickiness is base + STAI effect
            stickiness_strength = self.stickiness_base + self.stai_stickiness_effect * self.stai
            self.stickiness_values[self.last_action1] = stickiness_strength

    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bonus to Q-values before softmax
        return self.softmax(self.q_stage1 + self.stickiness_values, self.beta)

    # value_update uses the default TD learning from CognitiveModelBase

cognitive_model2 = make_cognitive_model(ParticipantModel2)