class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Reinforcement Learning with Anxiety-Modulated Reward-Driven Perseveration.
    This model suggests that participants, especially those with high anxiety, exhibit
    an increased tendency to repeat a first-stage action if it led to a reward on the
    immediately preceding trial. This perseveration acts as a bonus added to the Q-value
    of the previously rewarded action, making it more likely to be chosen again.
    The magnitude of this perseveration bonus is amplified by higher anxiety, potentially
    due to a desire to stick with "safe" or "known good" options to reduce uncertainty.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values (both stages).
    beta: [0, 10] - Inverse temperature for softmax action selection.
    pers_base: [0, 1] - Base perseveration bonus for repeating a rewarded action.
    stai_factor: [0, 1] - How much the STAI score linearly increases the perseveration bonus,
                           reflecting anxiety's impact on habitual responding.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_base, self.stai_factor = model_parameters
        
        # Calculate the current perseveration bonus, modulated by STAI.
        # Ensure the bonus does not exceed 1.0.
        self.current_pers_bonus = min(1.0, self.pers_base + self.stai_factor * self.stai)

    def pre_trial(self) -> None:
        """
        Before each trial, apply a temporary perseveration bonus to the Q-value of the last
        chosen first-stage action if it was rewarded. This bonus influences the current
        choice but does not permanently alter the learned Q-values.
        """
        # Create a temporary copy of q_stage1 to apply the bias for the current trial
        self.q_stage1_biased = np.copy(self.q_stage1)

        # Apply perseveration bonus if a previous action was taken and resulted in a reward
        if self.trial > 0 and self.last_action1 is not None and self.last_reward == 1:
            # Add the bonus to the Q-value of the action that was chosen on the previous trial
            # and led to a reward.
            self.q_stage1_biased[self.last_action1] += self.current_pers_bonus

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities using the potentially biased Q-values
        (including the perseveration bonus).
        """
        return self.softmax(self.q_stage1_biased, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Standard TD learning for both stages, updating the true underlying Q-values.
        The perseveration bonus is only applied during action selection, not learning.
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)