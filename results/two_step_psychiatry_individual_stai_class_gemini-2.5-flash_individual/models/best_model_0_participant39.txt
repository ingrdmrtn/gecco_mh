class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases the learning rate specifically for outcomes observed after rare transitions.
    This model assumes separate base learning rates for common and rare transitions, and the rare learning rate
    is further boosted by the participant's STAI score, reflecting increased attention or sensitivity to unexpected events.
    This could lead anxious individuals to over-update their expectations following surprising outcomes.

    Parameter Bounds:
    -----------------
    alpha_common: [0, 1] - Learning rate for value updates following common transitions.
    alpha_rare_base: [0, 1] - Base learning rate for value updates following rare transitions.
    beta: [0, 10] - Softmax inverse temperature.
    stai_rare_learning_boost: [0, 5] - Factor by which STAI amplifies the rare transition learning rate.
                                       A higher value means more anxious individuals learn more from rare events.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_common, self.alpha_rare_base, self.beta, self.stai_rare_learning_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine if the transition was rare.
        # The transition matrix T is structured such that T[0,0] and T[1,1] are common (0.7),
        # while T[0,1] and T[1,0] are rare (0.3).
        is_rare_transition = (self.T[action_1, state] < 0.5)

        if is_rare_transition:
            # Anxiety boosts the rare learning rate.
            # (1 + stai_rare_learning_boost * self.stai) ensures the boost is proportional to anxiety.
            effective_alpha = self.alpha_rare_base * (1 + self.stai_rare_learning_boost * self.stai)
            # Cap effective_alpha at 1.0 to prevent it from exceeding the theoretical maximum.
            effective_alpha = min(effective_alpha, 1.0)
        else:
            effective_alpha = self.alpha_common

        # Stage 2 value update using the effective alpha
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += effective_alpha * delta_2
        
        # Stage 1 value update also uses the same effective alpha,
        # as it learns from the updated Q2 value which was influenced by the transition type.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += effective_alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)