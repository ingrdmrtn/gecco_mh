class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants have a 'stay-switch' bias, where they are more likely
    to repeat their previous first-stage choice, especially after a positive outcome. This bias is modulated
    by anxiety: low anxiety (low STAI score) individuals are less likely to 'stay' after a non-positive outcome,
    promoting more adaptive switching behavior.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       - Learning rate for Q-values.
    beta: [0, 10]       - Inverse temperature for softmax action selection.
    stay_bonus_base: [0, 5] - Base value added to the Q-value of the previously chosen first-stage action.
    stai_switch_factor: [0, 1] - Factor determining how strongly low STAI reduces the stay bonus
                                  after non-positive rewards, encouraging switching.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stay_bonus_base, self.stai_switch_factor = model_parameters

    def init_model(self) -> None:
        # Initialize last_action1 and last_reward to None to indicate no previous trial
        # These are already handled by the base class, so no explicit init needed here.
        pass

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities, incorporating a stay bonus
        modulated by previous reward and STAI.
        """
        q_biased = np.copy(self.q_stage1)
        
        # Apply stay bonus only if there's a previous action and reward
        if self.last_action1 is not None and self.last_reward is not None:
            stay_bonus_effective = self.stay_bonus_base
            
            # If the last reward was not positive (0 or -1 coins),
            # low STAI reduces the stay bonus, making them more likely to switch.
            if self.last_reward <= 0:
                # (1 - self.stai * self.stai_switch_factor) will be higher for lower STAI,
                # meaning the reduction to the stay bonus is less pronounced, but it's
                # the *multiplication* by this factor that makes it smaller.
                # Example: STAI=0.26, stai_switch_factor=1 -> (1 - 0.26) = 0.74, so stay_bonus_base * 0.74
                # This means lower anxiety leads to a *smaller* stay bonus after a loss, promoting switching.
                stay_bonus_effective = self.stay_bonus_base * (1 - self.stai * self.stai_switch_factor)
                stay_bonus_effective = np.clip(stay_bonus_effective, 0, self.stay_bonus_base) # Ensure it's non-negative and doesn't exceed base
            
            q_biased[int(self.last_action1)] += stay_bonus_effective
            
        return self.softmax(q_biased, self.beta)
    
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values using standard TD learning."""
        # The base class's value_update is suitable for this model.
        # No modification needed beyond what CognitiveModelBase provides.
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)