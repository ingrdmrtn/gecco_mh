class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety might lead to increased perseveration or "stickiness" to the previously
    chosen stage 1 action. This could be due to reduced cognitive flexibility or a desire to avoid
    re-evaluating options. This model proposes that participants with higher STAI scores will
    experience an amplified bonus added to the value of the stage 1 action chosen on the previous trial,
    making them more likely to repeat it.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values in both stages.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    stick_stai_multiplier: [0, 5] - Parameter controlling how much STAI amplifies the stickiness bonus.
                                     A value of 0 means no stickiness due to anxiety.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_stai_multiplier = model_parameters

    def init_model(self) -> None:
        # Initialize q_stage1_biased for the first trial, so policy_stage1 doesn't fail
        self.q_stage1_biased = np.copy(self.q_stage1)

    def pre_trial(self) -> None:
        # Create a temporary Q-value array for stage 1 to apply stickiness
        self.q_stage1_biased = np.copy(self.q_stage1)
        
        if self.last_action1 is not None:
            # Calculate stickiness bonus, modulated by STAI
            # The bonus is proportional to STAI and the stick_stai_multiplier
            stick_bonus = self.stick_stai_multiplier * self.stai
            # Clamp stick_bonus to a reasonable range to prevent extreme values
            stick_bonus = np.clip(stick_bonus, 0.0, 5.0) 
            self.q_stage1_biased[self.last_action1] += stick_bonus

    def policy_stage1(self) -> np.ndarray:
        # Use the biased Q-values (with stickiness bonus) for action selection
        # These are computed in pre_trial for the current trial
        return self.softmax(self.q_stage1_biased, self.beta)

    # Override the default value_update to use our alpha
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)