class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model assumes participants use a hybrid model-free and model-based reinforcement
    learning strategy. The first-stage value is a weighted average of a model-free Q-value and a
    model-based Q-value. Medium anxiety (stai) is hypothesized to increase the reliance on model-based
    planning, potentially reflecting an increased effort to predict outcomes in an uncertain environment.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate for model-free Q-values)
    beta: [0, 10] (Inverse temperature for softmax choice)
    w_base: [0, 1] (Base weight for model-based control, 0=pure MF, 1=pure MB)
    w_stai_mult: [0, 1] (Multiplier for how STAI affects the model-based weight; 0 means no effect)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_stai_mult = model_parameters
        
        # Calculate the actual model-based weight based on STAI score
        # Higher STAI increases w, reflecting greater reliance on planning.
        self.w = self.w_base + (self.stai * self.w_stai_mult)
        self.w = np.clip(self.w, 0, 1) # Ensure w stays within [0, 1]

    def init_model(self) -> None:
        # Initialize model-free Q-values for stage 1 and stage 2.
        # self.q_stage1 will hold the blended values for policy.
        self.q_stage1_mf = np.zeros(self.n_choices) # Separate model-free Q1 values
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices)) # Stage 2 values are model-free

        # Transition probabilities (self.T) are already provided by the base class.

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 (model-free) update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 (model-free) update
        # The reward for Stage 1 is the updated value of the chosen Stage 2 action.
        delta_1_mf = self.q_stage2[state, action_2] - self.q_stage1_mf[action_1]
        self.q_stage1_mf[action_1] += self.alpha * delta_1_mf

    def pre_trial(self) -> None:
        # Before each trial, compute the blended q_stage1 values for the policy.
        
        # Compute model-based Q-values for Stage 1
        q_stage1_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            # The model-based value for a first-stage action is the expected value
            # of the best possible second-stage action across all possible next states.
            expected_next_value = 0
            for next_s in range(self.n_states):
                # Maximize over second-stage actions given the next state
                max_q_s2_for_next_s = np.max(self.q_stage2[next_s, :])
                # Add the probability of transitioning to this state times its max value
                expected_next_value += self.T[a1, next_s] * max_q_s2_for_next_s
            q_stage1_mb[a1] = expected_next_value
        
        # Blend model-free and model-based Q-values for Stage 1 policy
        self.q_stage1 = (1 - self.w) * self.q_stage1_mf + self.w * q_stage1_mb

cognitive_model2 = make_cognitive_model(ParticipantModel2)