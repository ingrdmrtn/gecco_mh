Here are three cognitive models proposing different hypotheses for how this participant makes decisions, incorporating their low anxiety (STAI score 0.2875).

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that the participant learns through standard
    reinforcement learning (TD learning) but exhibits a strong, anxiety-modulated
    bias towards the first option at Stage 1 (spaceship 0.0). Lower anxiety
    (lower STAI score) leads to a stronger, less inhibited bias, making the
    participant more likely to stick with their preferred spaceship, even if
    rewards are inconsistent.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax action selection.
    raw_bias_strength: [0, 5] - Base strength of the bias towards spaceship 0.0.
                                This parameter is scaled by (1 - stai) to reflect
                                that lower anxiety leads to a stronger bias.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.raw_bias_strength = model_parameters

    def init_model(self) -> None:
        # Scale bias_strength by (1 - stai):
        # If stai is low (e.g., 0.2875), (1 - stai) is high (e.g., 0.7125),
        # leading to a stronger effective bias.
        self.effective_bias = self.raw_bias_strength * (1 - self.stai)

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities, applying an anxiety-modulated bias
        to the value of spaceship 0.0 before softmax.
        """
        biased_q_stage1 = np.copy(self.q_stage1)
        biased_q_stage1[0] += self.effective_bias  # Add bias to spaceship 0.0
        return self.softmax(biased_q_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values after observing outcome using standard TD learning.
        """
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update: target is the updated Q-value of the chosen stage 2 action
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

# This creates the function 'cognitive_model1' that can be used for fitting
cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that the participant combines model-free (MF)
    and model-based (MB) control, with the weight given to the model-based system
    being modulated by anxiety. Lower anxiety (lower STAI score) leads to a
    stronger reliance on model-based planning, allowing the participant to better
    anticipate future rewards by considering transition probabilities.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values (both MF and for stage 2 values).
    beta: [0, 10] - Inverse temperature for softmax action selection.
    raw_lambda: [0, 1] - Base weight for the model-based component.
                         This is scaled by (1 - stai) to reflect that lower
                         anxiety leads to a stronger model-based contribution.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.raw_lambda = model_parameters

    def init_model(self) -> None:
        # Scale lambda: lower stai -> higher lambda (more MB control)
        self.mb_lambda = self.raw_lambda * (1 - self.stai)
        # self.T (transition probabilities) are already initialized in the base class

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities by combining model-free (MF)
        and model-based (MB) value estimates, weighted by mb_lambda.
        """
        # 1. Compute Model-Based (MB) values for Stage 1 actions
        q_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            # The MB value of a stage-1 action is the expected value of the best
            # stage-2 action across possible next states, weighted by transition probability.
            # T[a1, s] is P(s | a1)
            q_mb[a1] = np.sum([self.T[a1, s] * np.max(self.q_stage2[s]) for s in range(self.n_states)])

        # 2. Combine MF and MB values for total Stage 1 Q-values
        q_total_stage1 = (1 - self.mb_lambda) * self.q_stage1 + self.mb_lambda * q_mb
        
        return self.softmax(q_total_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values after observing outcome using standard TD learning
        for both stages (this represents the model-free learning component).
        """
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

# This creates the function 'cognitive_model2' that can be used for fitting
cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that the participant learns with separate
    learning rates for positive and negative prediction errors. Low anxiety
    (lower STAI score) makes the participant more sensitive to rewards
    (higher alpha_pos) and less sensitive to punishments (lower alpha_neg),
    leading to a more reward-seeking learning strategy. This could explain
    stickiness to options that provide occasional rewards, despite losses.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature for softmax action selection.
    alpha_asymmetry: [0, 1] - Parameter controlling the degree of asymmetry
                               between alpha_pos and alpha_neg. Lower anxiety
                               amplifies this asymmetry towards reward sensitivity.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.alpha_asymmetry = model_parameters

    def init_model(self) -> None:
        # Modulate asymmetry by (1 - stai): lower stai means more asymmetry
        # This makes alpha_pos higher and alpha_neg lower for low anxiety.
        stai_effect = (1 - self.stai) * self.alpha_asymmetry
        
        self.alpha_pos = self.alpha_base * (1 + stai_effect)
        self.alpha_neg = self.alpha_base * (1 - stai_effect)
        
        # Ensure learning rates stay within valid bounds [0, 1]
        self.alpha_pos = np.clip(self.alpha_pos, 0, 1)
        self.alpha_neg = np.clip(self.alpha_neg, 0, 1)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values after observing outcome, using separate learning rates
        for positive (reward) and negative (punishment) prediction errors.
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += self.alpha_neg * delta_2
        
        # Stage 1 update (target is the updated Q-value of the chosen stage 2 action)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += self.alpha_neg * delta_1

# This creates the function 'cognitive_model3' that can be used for fitting
cognitive_model3 = make_cognitive_model(ParticipantModel3)
```