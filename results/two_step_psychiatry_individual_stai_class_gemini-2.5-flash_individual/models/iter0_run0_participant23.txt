Here are three cognitive models proposing different mechanisms for how the participant makes decisions in the two-step task, with each model incorporating the participant's medium anxiety score (STAI = 0.325) in a distinct way.

### Model 1: Asymmetric Learning with Anxiety-Enhanced Negative Updates

**Hypothesis:** This model proposes that the participant learns from rewards and punishments using separate learning rates. Participants with higher anxiety are hypothesized to be more sensitive to negative outcomes, leading to an increased learning rate specifically for negative prediction errors. This allows for faster adaptation when things go wrong, which could be a coping mechanism for anxiety.

```python
import numpy as np # Ensure np is available for softmax and other operations

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: The participant uses a standard temporal difference (TD) learning rule,
    but their learning rate is asymmetric for positive and negative prediction errors.
    Specifically, their medium anxiety level (STAI score) enhances the learning rate
    for negative prediction errors, making them more sensitive to unexpected losses
    and promoting faster updates when outcomes are worse than expected.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1] - Base learning rate for both positive and negative prediction errors.
    alpha_anxiety_boost: [0, 1] - A parameter that scales how much anxiety increases the
                                  learning rate for negative prediction errors.
    beta: [0, 10] - Softmax inverse temperature for action selection, controlling exploration vs exploitation.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.alpha_anxiety_boost, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 learning
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Modulate learning rate for negative prediction errors based on STAI
        current_alpha_2 = self.alpha_base
        if delta_2 < 0:
            # If negative prediction error, increase learning rate by an anxiety-scaled boost
            current_alpha_2 = min(1.0, self.alpha_base + self.stai * self.alpha_anxiety_boost)
        self.q_stage2[state, action_2] += current_alpha_2 * delta_2
        
        # Stage 1 learning
        # The Q-value for stage 1 action is updated towards the Q-value of the chosen stage 2 action
        # in the *actual* state reached. This update is also sensitive to the sign of the prediction error.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        current_alpha_1 = self.alpha_base
        if delta_1 < 0:
            # If negative prediction error, increase learning rate by an anxiety-scaled boost
            current_alpha_1 = min(1.0, self.alpha_base + self.stai * self.alpha_anxiety_boost)
        self.q_stage1[action_1] += current_alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Hybrid Model-Based/Model-Free Control with Anxiety-Modulated Planning

**Hypothesis:** This model suggests that the participant employs a sophisticated strategy combining both model-free (habitual) learning and model-based (goal-directed) planning for the first stage. Anxiety, particularly at a medium level, is hypothesized to shift the balance between these two systems. Higher anxiety might reduce the influence of model-based planning, leading to a greater reliance on simpler, model-free habitual responses, possibly due to cognitive load or a preference for less uncertain strategies.

```python
import numpy as np # Ensure np is available for softmax and other operations

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: The participant uses a hybrid model-free and model-based control strategy
    for their first-stage decisions. The weight given to the model-based system is
    modulated by their anxiety level (STAI score), with higher anxiety potentially
    reducing the influence of model-based planning and increasing reliance on
    model-free, habitual responses.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for model-free value updates for both stages.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    w_base: [0, 1] - Base weight for the model-based component in stage 1.
    stai_w_factor: [0, 1] - Factor by which anxiety reduces the model-based weight.
                            A higher value means anxiety leads to more model-free control.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.stai_w_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate model-based Q-values for stage 1
        q_model_based = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            # Expected value of choosing a1, then acting optimally in stage 2
            # The optimal action in stage 2 is chosen greedily based on current q_stage2 values
            expected_next_state_value = sum(self.T[a1, s] * np.max(self.q_stage2[s, :]) for s in range(self.n_states))
            q_model_based[a1] = expected_next_state_value
        
        # Calculate the hybrid Q-value for stage 1
        # Anxiety modulates the weight 'w' for model-based control.
        # Higher STAI means lower 'w' (more model-free).
        # Clamp w between 0 and 1.
        w_effective = max(0.0, min(1.0, self.w_base - self.stai * self.stai_w_factor))
        
        q_hybrid_stage1 = w_effective * q_model_based + (1 - w_effective) * self.q_stage1
        
        return self.softmax(q_hybrid_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for stage 2 (model-free)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Standard TD update for stage 1 (model-free component)
        # This updates self.q_stage1, which represents the model-free Q-values
        delta_1_mf = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1_mf

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Stay/Switch with Anxiety-Modulated Perseveration

**Hypothesis:** This model suggests that beyond learning state-action values, the participant exhibits a tendency to repeat their previous first-stage action (perseveration). This perseveration is not constant but is dynamically modulated by the outcome of the previous trial and their anxiety level. Specifically, for a participant with medium anxiety, an unrewarded trial might lead to a significant reduction in the perseveration bonus (or even induce a switching tendency), reflecting an anxious response to negative feedback.

```python
import numpy as np # Ensure np is available for softmax and other operations

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: The participant's first-stage choices are driven by learned Q-values,
    but also by a tendency to perseverate on the previously chosen spaceship.
    This perseveration bonus is dynamically adjusted based on the outcome of the
    previous trial and is modulated by the participant's anxiety level (STAI score).
    Specifically, medium anxiety might lead to a reduced perseveration (or increased
    switching) after an unrewarded trial.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-value updates for both stages.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    perseveration_base: [0, 1] - Base bonus added to the Q-value of the
                                 previously chosen stage-1 action.
    anxiety_switch_factor: [0, 1] - Factor by which anxiety reduces the perseveration
                                    bonus after a non-rewarded trial (reward = 0).
                                    Higher values mean anxiety makes them more likely to switch.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.perseveration_base, self.anxiety_switch_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_modified_stage1 = np.copy(self.q_stage1)
        
        # Apply perseveration bonus if a last action exists
        if self.last_action1 is not None:
            # Determine the effective perseveration bonus for the current trial
            effective_bonus = self.perseveration_base
            # If the last reward was 0, anxiety reduces the bonus (or makes it a penalty effectively)
            if self.last_reward == 0:
                # The reduction is scaled by STAI and anxiety_switch_factor
                effective_bonus -= (self.stai * self.anxiety_switch_factor)
            
            q_modified_stage1[self.last_action1] += effective_bonus
            
        return self.softmax(q_modified_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for Q-values
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    # The base class's post_trial method already stores last_action1 and last_reward
    # which are used by policy_stage1 in the subsequent trial.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```