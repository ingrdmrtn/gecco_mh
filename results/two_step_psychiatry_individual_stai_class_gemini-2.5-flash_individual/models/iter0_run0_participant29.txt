Here are three cognitive models, each proposing a different hypothesis about how the participant's high anxiety (STAI = 0.575) might influence their decision-making in the two-step task.

---

### Model 1: Anxious Determinism (STAI modulates inverse temperature)

This model posits that participants with higher anxiety tend to make more deterministic choices, meaning they are less likely to explore suboptimal options. This is implemented by allowing the STAI score to increase the `beta` (inverse temperature) parameter in the softmax choice rule. A higher `beta` makes the agent choose the option with the highest value more often.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to more deterministic choices, reducing exploration.
    This is modeled by increasing the inverse temperature (beta) parameter in the
    softmax choice rule as STAI score increases. The learning process remains
    standard TD learning.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta_base: [0, 10] (Base inverse temperature)
    beta_stai_slope: [0, 10] (Slope for STAI modulation on beta)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.beta_stai_slope = model_parameters

    def init_model(self) -> None:
        """Initialize model state, specifically the beta parameter based on STAI."""
        # Beta increases linearly with STAI score, ensuring beta >= 0
        self.beta = max(0, self.beta_base + self.beta_stai_slope * self.stai)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

---

### Model 2: Anxious Sensitivity to Negative Outcomes (STAI modulates negative learning rate)

This model proposes that high anxiety makes participants more sensitive to negative outcomes (not receiving coins). This translates to an asymmetric learning rate, where the learning rate for negative prediction errors (`alpha_neg`) is higher than for positive prediction errors (`alpha_pos`), and this `alpha_neg` is further amplified by the STAI score.

```python
import numpy as np

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases sensitivity to negative outcomes.
    This is captured by introducing separate learning rates for positive (alpha_pos)
    and negative (alpha_neg) prediction errors. The negative learning rate is
    further increased by the participant's STAI score.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1] (Learning rate for positive prediction errors)
    alpha_neg_base: [0, 1] (Base learning rate for negative prediction errors)
    alpha_neg_stai_slope: [0, 1] (Slope for STAI modulation on alpha_neg)
    beta: [0, 10] (Inverse temperature)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.alpha_neg_stai_slope, self.beta = model_parameters

    def init_model(self) -> None:
        """Initialize the effective negative learning rate based on STAI."""
        # alpha_neg increases linearly with STAI, clipped to [0, 1]
        self.alpha_neg = np.clip(self.alpha_neg_base + self.alpha_neg_stai_slope * self.stai, 0, 1)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values using asymmetric learning rates for positive and negative
        prediction errors, with alpha_neg modulated by STAI.
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        current_alpha_2 = self.alpha_pos if delta_2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += current_alpha_2 * delta_2

        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        current_alpha_1 = self.alpha_pos if delta_1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += current_alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

---

### Model 3: Anxious Shift to Habitual Control (STAI modulates model-free weight)

This model proposes that high anxiety shifts the balance between goal-directed (model-based) and habitual (model-free) control towards the more habitual, model-free system. This is implemented by using a hybrid RL approach where the first-stage Q-values are a weighted sum of model-free and model-based values. The weight given to the model-free component (`w`) is increased with higher STAI scores.

```python
import numpy as np

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases reliance on habitual (model-free) control.
    This model implements a hybrid reinforcement learning agent where stage-1
    choices are based on a weighted sum of model-free and model-based Q-values.
    The weight (w) given to the model-free component increases with the STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate for model-free values)
    beta: [0, 10] (Inverse temperature)
    w_base: [0, 1] (Base weight for model-free control)
    w_stai_slope: [-1, 1] (Slope for STAI modulation on w, allowing increase or decrease)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_stai_slope = model_parameters

    def init_model(self) -> None:
        """Initialize the model-free weight (w) based on STAI."""
        # w is clipped to ensure it stays within [0, 1]
        self.w = np.clip(self.w_base + self.w_stai_slope * self.stai, 0, 1)
        # q_stage1 will be used to store model-free Q-values
        # q_stage2 is used for both model-free and model-based calculations

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities using a hybrid (model-free + model-based)
        value for decision-making.
        """
        # Calculate model-based Q-values for stage 1
        q_mb_values = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            expected_future_value = 0
            for s_prime in range(self.n_states):
                p_trans = self.T[a1, s_prime] # Transition probability from a1 to s_prime
                max_q_s2_s_prime = np.max(self.q_stage2[s_prime]) # Optimal value at stage 2 for s_prime
                expected_future_value += p_trans * max_q_s2_s_prime
            q_mb_values[a1] = expected_future_value
        
        # Combine model-free (self.q_stage1) and model-based (q_mb_values) values
        q_hybrid = self.w * self.q_stage1 + (1 - self.w) * q_mb_values
        
        return self.softmax(q_hybrid, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update model-free Q-values for stage 1 and standard Q-values for stage 2.
        Model-based values are recomputed for policy and not directly updated here.
        """
        # Stage 2 update (standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 model-free update (standard TD)
        # The target for MF learning is the Q-value of the state reached at stage 2
        delta_1_mf = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1_mf

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```