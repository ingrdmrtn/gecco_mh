Here are three cognitive models, each proposing a distinct hypothesis about how the highly anxious participant (STAI score 0.5375) makes decisions in the two-step task. Each model incorporates the STAI score to modulate a specific aspect of behavior, reflecting potential cognitive biases or strategies associated with anxiety.

### Model 1: Anxiety-Modulated Asymmetric Learning Rates

**Hypothesis**: High anxiety leads to asymmetric learning, where negative prediction errors (when outcomes are worse than expected) are learned from more rapidly than positive prediction errors (when outcomes are better than expected). This reflects a heightened sensitivity to negative feedback, a common characteristic in anxious individuals. The STAI score directly amplifies the learning rate for negative outcomes.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning rates asymmetrically. High anxiety
    leads to an increased learning rate for negative prediction errors compared
    to positive ones, reflecting heightened sensitivity to negative outcomes.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    alpha_neg_stai_impact: [0, 1] - How much STAI score increases alpha_neg.
                                    A positive value implies higher anxiety leads to faster learning from negative outcomes.
    beta: [0, 10] - Inverse temperature for softmax choice, controlling exploration/exploitation.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.alpha_neg_stai_impact, self.beta = model_parameters
        # Calculate the effective negative learning rate, amplified by STAI score
        # and clipped to stay within valid [0, 1] range.
        self.alpha_neg_effective = np.clip(self.alpha_neg_base + self.stai * self.alpha_neg_stai_impact, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values using separate learning rates for positive and negative prediction errors.
        """
        # Stage 2 learning
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += self.alpha_neg_effective * delta_2
        
        # Stage 1 learning: use the updated Q-value of the chosen second-stage action as the outcome
        # for the first-stage choice.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += self.alpha_neg_effective * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Hybrid Control (Model-Free vs. Model-Based)

**Hypothesis**: High anxiety reduces the reliance on complex model-based planning and shifts control towards simpler, habitual (model-free) decision strategies. This means the participant gives more weight to the immediate, learned values (model-free) and less to the planned future values (model-based) when making first-stage choices, with this shift amplified by anxiety.

```python
import numpy as np

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety shifts the balance towards model-free control in
    the two-step task, reducing the reliance on model-based planning.
    This means the participant gives more weight to the immediate, learned
    values (model-free) and less to the planned future values (model-based).

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_mf_base: [0, 1] - Base weight given to model-free values (0 = purely MB, 1 = purely MF).
    w_stai_impact: [0, 1] - How much STAI score increases the model-free weight.
                            A positive value implies higher anxiety leads to more model-free control.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_mf_base, self.w_stai_impact = model_parameters
        # Calculate the effective model-free weight, amplified by STAI score
        # and clipped to stay within valid [0, 1] range.
        self.w_mf_effective = np.clip(self.w_mf_base + self.stai * self.w_stai_impact, 0.0, 1.0)
    
    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities using a hybrid model-free/model-based approach.
        The weight for model-free control is modulated by anxiety.
        """
        # Model-free Q-values are self.q_stage1
        
        # Model-based Q-values: Expected value of going to each planet, then choosing the best alien.
        # Q_MB(action_1) = sum_{state} P(state | action_1) * max_{action_2} Q_MF(state, action_2)
        q_mb = self.T @ np.max(self.q_stage2, axis=1)

        # Combine model-free and model-based Q-values based on the effective weight.
        # If w_mf_effective is 1, purely model-free. If 0, purely model-based.
        q_hybrid = self.w_mf_effective * self.q_stage1 + (1 - self.w_mf_effective) * q_mb
        
        return self.softmax(q_hybrid, self.beta)

    # The default value_update method (TD learning) is used, as the hybrid nature
    # only affects action selection at stage 1. It will use self.alpha.

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Choice Stickiness (Perseveration)

**Hypothesis**: High anxiety increases the tendency to repeat the previous first-stage choice, regardless of its outcome. This "stickiness" or perseveration bias is amplified by the participant's anxiety level, suggesting a reduced flexibility in initial decision-making.

```python
import numpy as np

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases choice perseveration (stickiness) at the
    first stage. Participants are more likely to repeat their previous first-stage
    choice, with this tendency amplified by anxiety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    stickiness_base: [-2, 2] - Base bonus added to the Q-value of the previous first-stage choice.
                               Can be negative (switching bias) or positive (perseveration bias).
    stickiness_stai_impact: [0, 2] - How much STAI score increases the stickiness.
                                      A positive value implies higher anxiety leads to more perseveration.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.stickiness_stai_impact = model_parameters
        # Calculate the effective stickiness bonus, amplified by STAI score
        # and clipped to a reasonable range.
        self.stickiness_effective = np.clip(self.stickiness_base + self.stai * self.stickiness_stai_impact, -5.0, 5.0)

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities, adding a stickiness bonus
        to the Q-value of the last chosen action.
        """
        q_biased = np.copy(self.q_stage1)
        # Apply stickiness only after the first trial, if a previous action exists
        if self.last_action1 is not None and self.trial > 0:
            q_biased[int(self.last_action1)] += self.stickiness_effective
        
        return self.softmax(q_biased, self.beta)

    # The default value_update method (TD learning) is used, as the stickiness
    # only affects action selection at stage 1. It will use self.alpha.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```