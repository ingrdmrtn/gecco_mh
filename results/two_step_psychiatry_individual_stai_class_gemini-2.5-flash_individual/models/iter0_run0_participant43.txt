Here are three cognitive models, each proposing a distinct hypothesis about how the participant makes decisions, especially considering their medium anxiety level (STAI score of 0.325).

### Model 1: Asymmetric Learning Rates & Stickiness with Anxiety-Modulated Punishment Sensitivity

This model proposes that participants learn differently from positive and negative prediction errors. Individuals with higher anxiety are hypothesized to be more sensitive to negative outcomes, leading to a faster update of values when receiving no reward or experiencing an unexpected negative outcome. Additionally, a "stickiness" parameter accounts for the tendency to repeat the previous first-stage action, regardless of its immediate value, which is a common behavioral bias.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants learn with separate learning rates for positive
    and negative prediction errors. Higher anxiety (stai) is hypothesized to increase the learning rate
    for negative outcomes, making the participant more sensitive to punishments or unrewarded actions.
    Additionally, a 'stickiness' parameter captures the tendency to repeat the previous first-stage choice.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1] (Learning rate for positive prediction errors)
    alpha_neg_base: [0, 1] (Base learning rate for negative prediction errors)
    alpha_neg_stai_mult: [0, 2] (Multiplier for how STAI affects alpha_neg; 0 means no effect)
    beta: [0, 10] (Inverse temperature for softmax choice)
    stickiness_bias: [-2, 2] (Bias to repeat the previous first-stage action)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.alpha_neg_stai_mult, self.beta, self.stickiness_bias = model_parameters
        
        # Calculate the actual alpha_neg based on STAI score
        # Higher STAI increases alpha_neg, reflecting increased sensitivity to negative outcomes.
        self.alpha_neg = self.alpha_neg_base + (self.stai * self.alpha_neg_stai_mult)
        self.alpha_neg = np.clip(self.alpha_neg, 0, 1) # Ensure alpha_neg stays within [0, 1]

    def init_model(self) -> None:
        # Initialize Q-values. Base class already initializes q_stage1 to zeros and q_stage2 to 0.5.
        # We need to track the last action for stickiness. The base class does this via self.last_action1.
        pass

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update with asymmetric learning rates
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 > 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += self.alpha_neg * delta_2
        
        # Stage 1 update with asymmetric learning rates
        # The reward for Stage 1 is the updated value of the chosen Stage 2 action.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 > 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += self.alpha_neg * delta_1

    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bias to Q-values for stage 1 before softmax
        q_s1_biased = np.copy(self.q_stage1)
        if self.last_action1 is not None:
            # Add bias to the Q-value of the last chosen action
            q_s1_biased[self.last_action1] += self.stickiness_bias
        
        return self.softmax(q_s1_biased, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Hybrid Model-Based/Model-Free with Anxiety-Modulated Planning Reliance

This model posits that decisions are made by blending model-free (habitual, based on direct experience) and model-based (goal-directed, based on planning with knowledge of transitions) control. For this participant, medium anxiety is hypothesized to increase their reliance on model-based planning, possibly as a strategy to exert more control or reduce uncertainty in a complex environment.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model assumes participants use a hybrid model-free and model-based reinforcement
    learning strategy. The first-stage value is a weighted average of a model-free Q-value and a
    model-based Q-value. Medium anxiety (stai) is hypothesized to increase the reliance on model-based
    planning, potentially reflecting an increased effort to predict outcomes in an uncertain environment.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate for model-free Q-values)
    beta: [0, 10] (Inverse temperature for softmax choice)
    w_base: [0, 1] (Base weight for model-based control, 0=pure MF, 1=pure MB)
    w_stai_mult: [0, 1] (Multiplier for how STAI affects the model-based weight; 0 means no effect)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_stai_mult = model_parameters
        
        # Calculate the actual model-based weight based on STAI score
        # Higher STAI increases w, reflecting greater reliance on planning.
        self.w = self.w_base + (self.stai * self.w_stai_mult)
        self.w = np.clip(self.w, 0, 1) # Ensure w stays within [0, 1]

    def init_model(self) -> None:
        # Initialize model-free Q-values for stage 1 and stage 2.
        # self.q_stage1 will hold the blended values for policy.
        self.q_stage1_mf = np.zeros(self.n_choices) # Separate model-free Q1 values
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices)) # Stage 2 values are model-free

        # Transition probabilities (self.T) are already provided by the base class.

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 (model-free) update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 (model-free) update
        # The reward for Stage 1 is the updated value of the chosen Stage 2 action.
        delta_1_mf = self.q_stage2[state, action_2] - self.q_stage1_mf[action_1]
        self.q_stage1_mf[action_1] += self.alpha * delta_1_mf

    def pre_trial(self) -> None:
        # Before each trial, compute the blended q_stage1 values for the policy.
        
        # Compute model-based Q-values for Stage 1
        q_stage1_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            # The model-based value for a first-stage action is the expected value
            # of the best possible second-stage action across all possible next states.
            expected_next_value = 0
            for next_s in range(self.n_states):
                # Maximize over second-stage actions given the next state
                max_q_s2_for_next_s = np.max(self.q_stage2[next_s, :])
                # Add the probability of transitioning to this state times its max value
                expected_next_value += self.T[a1, next_s] * max_q_s2_for_next_s
            q_stage1_mb[a1] = expected_next_value
        
        # Blend model-free and model-based Q-values for Stage 1 policy
        self.q_stage1 = (1 - self.w) * self.q_stage1_mf + self.w * q_stage1_mb

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Dual Learning Rates & Anxiety-Modulated Exploration-Exploitation

This model allows for different learning rates at each stage of the task, reflecting potentially distinct learning processes for first-stage choices (spaceships) and second-stage choices (aliens). Crucially, the participant's anxiety level (STAI score) is hypothesized to modulate their exploration-exploitation trade-off. Specifically, medium anxiety might lead to more deterministic, exploitative choices (higher inverse temperature, beta), as the participant attempts to reduce uncertainty and secure rewards more directly.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants use distinct learning rates for the first and
    second stages of the task. Furthermore, the decision-making temperature (beta) for action
    selection is modulated by anxiety (stai). Specifically, higher anxiety is hypothesized to lead
    to more deterministic, exploitative choices (higher beta), possibly reflecting a reduced
    tolerance for uncertainty or a stronger drive to secure rewards.

    Parameter Bounds:
    -----------------
    alpha1: [0, 1] (Learning rate for Stage 1 Q-values)
    alpha2: [0, 1] (Learning rate for Stage 2 Q-values)
    beta_base: [0, 10] (Base inverse temperature for softmax choice)
    beta_stai_mult: [0, 5] (Multiplier for how STAI affects beta; 0 means no effect)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha1, self.alpha2, self.beta_base, self.beta_stai_mult = model_parameters
        
        # Calculate the actual beta based on STAI score
        # Higher STAI increases beta, leading to more deterministic choices.
        self.beta = self.beta_base + (self.stai * self.beta_stai_mult)
        # Ensure beta stays within a reasonable range, avoiding very low values that lead to highly random choices.
        self.beta = np.clip(self.beta, 0.1, 10) 

    def init_model(self) -> None:
        # Initialize Q-values. Base class already initializes q_stage1 to zeros and q_stage2 to 0.5.
        pass

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update with alpha2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha2 * delta_2
        
        # Stage 1 update with alpha1
        # The reward for Stage 1 is the updated value of the chosen Stage 2 action.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha1 * delta_1

    # The policy_stage1 and policy_stage2 methods from the base class will automatically
    # use the self.beta calculated in unpack_parameters, as they call self.softmax(..., self.beta).

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```