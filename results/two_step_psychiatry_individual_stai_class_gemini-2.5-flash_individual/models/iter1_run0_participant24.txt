Here are three new cognitive models, each proposing a distinct hypothesis about how the participant makes decisions, with `self.stai` modulating a key aspect of their behavior.

### Participant Data Analysis & Context
The participant has a STAI score of 0.35, placing them in the "medium anxiety" category.
Looking at the data:
*   The participant shows strong initial perseveration, choosing spaceship 0 for the first 22 trials.
*   They also largely stuck with alien 0 for a significant portion of the early trials.
*   Spaceship 0 commonly travels to planet X (0), but the participant frequently traveled to planet Y (1) when choosing spaceship 0, which is a rare transition. Despite this, they stuck with spaceship 0.
*   Rewards are mixed, suggesting a dynamic environment.

The previous "best model" introduced a "stickiness" or "perseveration" bonus for stage 1 choices, modulated by anxiety. My new models will explore different mechanisms.

---

### Proposed Cognitive Models

```python
import numpy as np
from abc import ABC, abstractmethod

# Base Class (DO NOT MODIFY) - This block is provided for context and is assumed to be present.
# It is not part of the solution to be written, but for completeness of the thought process.
# class CognitiveModelBase(ABC):
#     """
#     Base class for cognitive models in a two-step task.
#
#     Override methods to implement participant-specific cognitive strategies.
#     """
#
#     def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
#         # Task structure
#         self.n_trials = n_trials
#         self.n_choices = 2
#         self.n_states = 2
#         self.stai = stai
#
#         # Transition probabilities
#         self.trans_counts = np.array([[35, 15], [15, 35]]) # Prior counts for transitions, if needed for learning
#         self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True) # Transition probabilities, if needed for direct use
#
#         # Choice probability sequences
#         self.p_choice_1 = np.zeros(n_trials)
#         self.p_choice_2 = np.zeros(n_trials)
#
#         # Value representations
#         self.q_stage1 = np.zeros(self.n_choices)
#         self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
#
#         # Trial tracking
#         self.trial = 0
#         self.last_action1 = None
#         self.last_action2 = None
#         self.last_state = None
#         self.last_reward = None
#
#         # Initialize
#         self.unpack_parameters(model_parameters)
#         self.init_model()
#
#     @abstractmethod
#     def unpack_parameters(self, model_parameters: tuple) -> None:
#         """Unpack model_parameters into named attributes."""
#         pass
#
#     def init_model(self) -> None:
#         """Initialize model state. Override to set up additional variables."""
#         pass
#
#     def policy_stage1(self) -> np.ndarray:
#         """Compute stage-1 action probabilities. Override to customize."""
#         return self.softmax(self.q_stage1, self.beta)
#
#     def policy_stage2(self, state: int) -> np.ndarray:
#         """Compute stage-2 action probabilities. Override to customize."""
#         return self.softmax(self.q_stage2[state], self.beta)
#
#     def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
#         """Update values after observing outcome. Override to customize."""
#         delta_2 = reward - self.q_stage2[state, action_2]
#         self.q_stage2[state, action_2] += self.alpha * delta_2
#
#         delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
#         self.q_stage1[action_1] += self.alpha * delta_1
#
#     def pre_trial(self) -> None:
#         """Called before each trial. Override to add computations."""
#         pass
#
#     def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
#         """Called after each trial. Override to add computations."""
#         self.last_action1 = action_1
#         self.last_action2 = action_2
#         self.last_state = state
#         self.last_reward = reward
#
#     def run_model(self, action_1, state, action_2, reward) -> float:
#         """Run model over all trials. Usually don't override."""
#         for self.trial in range(self.n_trials):
#             a1, s = int(action_1[self.trial]), int(state[self.trial])
#             a2, r = int(action_2[self.trial]), float(reward[self.trial])
#
#             self.pre_trial()
#             self.p_choice_1[self.trial] = self.policy_stage1()[a1]
#             self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
#             self.value_update(a1, s, a2, r)
#             self.post_trial(a1, s, a2, r)
#
#         return self.compute_nll()
#
#     def compute_nll(self) -> float:
#         """Compute negative log-likelihood."""
#         eps = 1e-12
#         return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
#
#     def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
#         """Softmax action selection."""
#         centered = values - np.max(values)
#         exp_vals = np.exp(beta * centered)
#         return exp_vals / np.sum(exp_vals)
#
# def make_cognitive_model(ModelClass):
#     """Create function interface from model class."""
#     def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
#         n_trials = len(action_1)
#         stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
#         model = ModelClass(n_trials, stai_val, model_parameters)
#         return model.run_model(action_1, state, action_2, reward)
#     return cognitive_model

```

### Model 1: Asymmetric Learning Rates (Reward vs No Reward) Modulated by Anxiety

**Hypothesis:** This model proposes that participants learn differently from positive (reward = 1) and negative (reward = 0) outcomes. Specifically, it introduces separate learning rates for rewards and non-rewards. Anxiety (STAI score) is hypothesized to modulate the learning rate for non-rewards, potentially making participants more sensitive to negative outcomes as anxiety increases. For a participant with medium anxiety, this could mean an increased focus on avoiding non-rewards by learning faster from them, or a blunted response to non-rewards depending on the modulation direction.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants learn differently from positive
    (reward = 1) and negative (reward = 0) outcomes. Specifically, it introduces
    separate learning rates for rewards and non-rewards. Anxiety (STAI score)
    is hypothesized to modulate the learning rate for non-rewards. For a participant
    with medium anxiety, this might manifest as an increased or decreased sensitivity
    to outcomes where no gold coins were received.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1] - Base learning rate for both positive and negative outcomes.
    beta: [0, 10] - Inverse temperature for softmax choice.
    stai_mod_factor_alpha_noreward: [-2, 2] - Factor by which STAI score additively
                                              modulates the learning rate for non-rewards.
                                              A positive value means higher anxiety
                                              increases learning from non-rewards,
                                              a negative value means higher anxiety decreases it.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.stai_mod_factor_alpha_noreward = model_parameters
        
        # Calculate the effective learning rate for non-rewards
        # The modifier is added to a base learning rate, then clamped to [0, 1]
        self.alpha_reward = self.alpha_base
        self.alpha_noreward = np.clip(self.alpha_base + self.stai_mod_factor_alpha_noreward * self.stai, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine which alpha to use based on reward
        current_alpha = self.alpha_reward if reward == 1 else self.alpha_noreward

        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += current_alpha * delta_2
        
        # Stage 1 update: The value propagated to stage 1 is the updated Q-value of the chosen stage 2 action.
        # Thus, the learning rate for stage 1 also depends on the reward received.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += current_alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Hybrid Model-Free/Model-Based Control with Anxiety Modulation

**Hypothesis:** This model proposes that participants integrate both model-free (habitual Q-learning) and model-based (goal-directed planning using transition knowledge) control to make first-stage decisions. Anxiety (STAI score) modulates the weighting (`w`) between these two systems. For a participant with medium anxiety, this might mean a shift towards either more habitual (lower `w`) or more deliberative (higher `w`) control, potentially as a coping mechanism or due to altered cognitive resources. The participant data shows some initial perseveration, which could be indicative of a stronger model-free component or a cautious model-based approach.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants integrate both model-free
    (habitual Q-learning) and model-based (goal-directed planning using transition
    knowledge) control to make first-stage decisions. Anxiety (STAI score)
    modulates the weighting (w) between these two systems. For a participant
    with medium anxiety, this might mean a shift towards either more habitual
    (lower w) or more deliberative (higher w) control, potentially as a coping
    mechanism or due to altered cognitive resources.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_base: [0, 1] - Base weight for the model-based component (0=pure model-free, 1=pure model-based).
    stai_w_factor: [-1, 1] - Factor by which STAI score modulates the model-based weight 'w'.
                             A positive value means higher anxiety increases model-based control,
                             a negative value means higher anxiety increases model-free control.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.stai_w_factor = model_parameters
        
        # Calculate the effective model-based weight 'w'
        # Ensure 'w' stays within [0, 1]
        self.w = np.clip(self.w_base + self.stai_w_factor * self.stai, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Model-free Q-values are already in self.q_stage1
        q_mf = self.q_stage1

        # Calculate model-based Q-values
        q_mb = np.zeros(self.n_choices)
        for s1_action in range(self.n_choices):
            expected_q_s2 = 0
            # Iterate over possible second-stage states (planets)
            for s2_state in range(self.n_states):
                # Probability of transitioning to s2_state given s1_action
                transition_prob = self.T[s1_action, s2_state]
                # Maximize over second-stage actions (aliens) for the current s2_state
                max_q_s2_action = np.max(self.q_stage2[s2_state, :])
                expected_q_s2 += transition_prob * max_q_s2_action
            q_mb[s1_action] = expected_q_s2
        
        # Combine model-free and model-based values
        q_combined = self.w * q_mb + (1 - self.w) * q_mf
        
        return self.softmax(q_combined, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning updates for both stages (model-free component)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Rare Transition Aversion Modulated by Anxiety

**Hypothesis:** This model proposes that participants develop an aversion to rare transitions between the first and second stage, and this aversion is modulated by their anxiety level (STAI score). Specifically, first-stage choices that have a higher probability of leading to a rare transition incur an anticipatory penalty, making them less attractive. Higher anxiety is hypothesized to increase this penalty, reflecting a preference for predictable, common paths. For a participant with medium anxiety, this could manifest as a cautious strategy, favoring choices that are more likely to lead to expected outcomes.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that participants develop an aversion to
    rare transitions between the first and second stage, and this aversion is
    modulated by their anxiety level (STAI score). Choices that have a higher
    probability of leading to a rare transition incur an anticipatory penalty,
    making them less attractive. Higher anxiety is hypothesized to increase this
    penalty, reflecting a preference for predictable, common paths.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Inverse temperature for softmax choice.
    penalty_base: [0, 5] - Base penalty applied to Q-values for choices
                           that can lead to rare transitions.
    stai_penalty_factor: [-2, 2] - Factor by which STAI score modulates the
                                   rare transition penalty. A positive value
                                   means higher anxiety increases the penalty.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.penalty_base, self.stai_penalty_factor = model_parameters
        
        # Calculate the effective rare transition penalty
        # Ensure penalty is non-negative
        self.rare_trans_penalty = np.clip(self.penalty_base + self.stai_penalty_factor * self.stai, 0.0, 5.0)

    def policy_stage1(self) -> np.ndarray:
        # Create a copy of Q-values to apply the penalty
        q_values_with_penalty = np.copy(self.q_stage1)

        # Apply penalty based on the probability of a rare transition for each spaceship.
        # Spaceship 0 (A) has a rare transition to Planet 1 (Y) with probability self.T[0,1].
        # Spaceship 1 (U) has a rare transition to Planet 0 (X) with probability self.T[1,0].
        
        # Penalize spaceship 0's value by its probability of leading to a rare transition
        q_values_with_penalty[0] -= self.T[0,1] * self.rare_trans_penalty
        
        # Penalize spaceship 1's value by its probability of leading to a rare transition
        q_values_with_penalty[1] -= self.T[1,0] * self.rare_trans_penalty
        
        return self.softmax(q_values_with_penalty, self.beta)

    # The default value_update and post_trial methods are suitable.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```