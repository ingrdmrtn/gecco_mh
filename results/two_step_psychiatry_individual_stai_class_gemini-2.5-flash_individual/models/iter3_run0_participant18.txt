```python
import numpy as np

# Base class is provided and should not be modified.
# It is included here for context but will not be part of the final output.
# from abc import ABC, abstractmethod
# class CognitiveModelBase(ABC):
#     """
#     Base class for cognitive models in a two-step task.
#     
#     Override methods to implement participant-specific cognitive strategies.
#     """
#
#     def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
#         # Task structure
#         self.n_trials = n_trials
#         self.n_choices = 2
#         self.n_states = 2
#         self.stai = stai
#         
#         # Transition probabilities
#         self.trans_counts = np.array([[35, 15], [15, 35]]) # Prior counts for transitions, if needed for learning
#         self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True) # Transition probabilities, if needed for direct use
#         
#         # Choice probability sequences
#         self.p_choice_1 = np.zeros(n_trials)
#         self.p_choice_2 = np.zeros(n_trials)
#         
#         # Value representations
#         self.q_stage1 = np.zeros(self.n_choices)
#         self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
#         
#         # Trial tracking
#         self.trial = 0
#         self.last_action1 = None
#         self.last_action2 = None
#         self.last_state = None
#         self.last_reward = None
#         
#         # Initialize
#         self.unpack_parameters(model_parameters)
#         self.init_model()
#
#     @abstractmethod
#     def unpack_parameters(self, model_parameters: tuple) -> None:
#         """Unpack model_parameters into named attributes."""
#         pass
#
#     def init_model(self) -> None:
#         """Initialize model state. Override to set up additional variables."""
#         pass
#
#     def policy_stage1(self) -> np.ndarray:
#         """Compute stage-1 action probabilities. Override to customize."""
#         return self.softmax(self.q_stage1, self.beta)
#
#     def policy_stage2(self, state: int) -> np.ndarray:
#         """Compute stage-2 action probabilities. Override to customize."""
#         return self.softmax(self.q_stage2[state], self.beta)
#
#     def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
#         """Update values after observing outcome. Override to customize."""
#         delta_2 = reward - self.q_stage2[state, action_2]
#         self.q_stage2[state, action_2] += self.alpha * delta_2
#         
#         delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
#         self.q_stage1[action_1] += self.alpha * delta_1
#
#     def pre_trial(self) -> None:
#         """Called before each trial. Override to add computations."""
#         pass
#
#     def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
#         """Called after each trial. Override to add computations."""
#         self.last_action1 = action_1
#         self.last_action2 = action_2
#         self.last_state = state
#         self.last_reward = reward
#
#     def run_model(self, action_1, state, action_2, reward) -> float:
#         """Run model over all trials. Usually don't override."""
#         for self.trial in range(self.n_trials):
#             a1, s = int(action_1[self.trial]), int(state[self.trial])
#             a2, r = int(action_2[self.trial]), float(reward[self.trial])
#             
#             self.pre_trial()
#             self.p_choice_1[self.trial] = self.policy_stage1()[a1]
#             self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
#             self.value_update(a1, s, a2, r)
#             self.post_trial(a1, s, a2, r)
#         
#         return self.compute_nll()
#     
#     def compute_nll(self) -> float:
#         """Compute negative log-likelihood."""
#         eps = 1e-12
#         return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
#     
#     def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
#         """Softmax action selection."""
#         centered = values - np.max(values)
#         exp_vals = np.exp(beta * centered)
#         return exp_vals / np.sum(exp_vals)
#
# def make_cognitive_model(ModelClass):
#     """Create function interface from model class."""
#     def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
#         n_trials = len(action_1)
#         stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
#         model = ModelClass(n_trials, stai_val, model_parameters)
#         return model.run_model(action_1, state, action_2, reward)
#     return cognitive_model


class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes asymmetric learning rates for positive and
    negative prediction errors. Participants update their values differently
    when outcomes are better than expected (positive prediction error) versus
    worse than expected (negative prediction error). For low anxiety individuals
    (low STAI score), it is hypothesized that they are less reactive to negative
    outcomes, meaning their learning rate for negative prediction errors is reduced.
    This could lead to more persistence on options that occasionally yield zero
    rewards, as these failures do not strongly devalue the option.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
                             This rate is modulated by the STAI score.
    beta: [0, 10] - Inverse temperature for softmax action selection.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta = model_parameters
        # Modulate alpha_neg based on STAI: lower STAI -> lower effective alpha_neg
        self.alpha_neg_effective = self.alpha_neg_base * self.stai

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values using asymmetric learning rates based on prediction error sign.
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += self.alpha_neg_effective * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += self.alpha_neg_effective * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model posits that decisions are made by combining model-free
    (habitual) and model-based (goal-directed) control. The weight given to
    model-based control is modulated by the participant's anxiety level. Low
    anxiety individuals (low STAI score) are hypothesized to exhibit greater
    cognitive flexibility and planning, leading to a stronger reliance on
    model-based strategies.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for model-free Q-values.
    beta: [0, 10] - Inverse temperature for softmax action selection.
    w_base: [0, 1] - Base weight for model-based control. The actual weight `w`
                     is derived from this base and the STAI score.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters
        # Modulate 'w' (weight for MB control) by STAI.
        # Lower STAI (e.g., 0.2875) leads to higher (1 - STAI), increasing 'w'.
        self.w_effective = self.w_base + (1 - self.w_base) * (1 - self.stai)
        # Ensure w_effective stays within [0, 1]
        self.w_effective = np.clip(self.w_effective, 0, 1)

    def init_model(self) -> None:
        # Initialize model-free Q-values (self.q_stage1 is already MF)
        self.q_stage1_mf = np.zeros(self.n_choices) 
        # Initialize model-based Q-values for stage 1
        self.q_stage1_mb = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities using a hybrid of MF and MB values.
        """
        # Calculate model-based Q-values for stage 1
        # Q_MB(a1) = sum_s' T(s'|a1) * max_a2 Q_MF(s', a2)
        for a1 in range(self.n_choices):
            expected_future_value = 0
            for s_prime in range(self.n_states):
                # T[a1, s_prime] is the probability of going to s_prime given a1
                # max_a2 Q_MF(s', a2) is the best expected reward from stage 2 given state s'
                expected_future_value += self.T[a1, s_prime] * np.max(self.q_stage2[s_prime])
            self.q_stage1_mb[a1] = expected_future_value

        # Combine model-free and model-based values
        # Note: self.q_stage1 is typically used for the final Q-values in the base class,
        # so we'll store the hybrid value there.
        self.q_stage1 = self.w_effective * self.q_stage1_mb + (1 - self.w_effective) * self.q_stage1_mf
        
        return self.softmax(self.q_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update model-free Q-values using standard TD learning.
        """
        # Stage 2 update (applies to model-free Q-values for stage 2)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update for model-free Q-values
        # The target for Q_MF(a1) is the Q_MF(s', a2) after state transition
        delta_1_mf = self.q_stage2[state, action_2] - self.q_stage1_mf[action_1]
        self.q_stage1_mf[action_1] += self.alpha * delta_1_mf

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model incorporates a value decay mechanism, where the learned
    Q-values gradually decrease over time if not reinforced. This captures a form
    of recency bias or memory limitations. For low anxiety individuals (low STAI
    score), it is hypothesized that they experience *slower decay* of values,
    implying better retention of past reward information or less cognitive load.
    This allows them to integrate information over longer timescales.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature for softmax action selection.
    gamma_decay_base: [0, 1] - Base decay factor for Q-values. The actual decay
                                factor `gamma_decay` is modulated by the STAI score.
                                A value closer to 1 means less decay.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.gamma_decay_base = model_parameters
        # Modulate gamma_decay based on STAI: lower STAI -> higher (1 - STAI) -> higher gamma_decay_effective (less decay)
        self.gamma_decay_effective = self.gamma_decay_base + (1 - self.gamma_decay_base) * (1 - self.stai)
        # Ensure gamma_decay_effective stays within [0, 1]
        self.gamma_decay_effective = np.clip(self.gamma_decay_effective, 0, 1)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Called after each trial. Updates last action, state, reward, and applies
        decay to all Q-values.
        """
        super().post_trial(action_1, state, action_2, reward) # Call base class method first

        # Apply decay to all Q-values
        self.q_stage1 *= self.gamma_decay_effective
        self.q_stage2 *= self.gamma_decay_effective
        # Ensure Q-values don't drop below 0 (rewards are binary 0 or 1)
        self.q_stage1 = np.maximum(self.q_stage1, 0)
        self.q_stage2 = np.maximum(self.q_stage2, 0)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```