Here are three cognitive models, each proposing a distinct hypothesis about how the participant's decision-making is influenced by their high anxiety (STAI score = 0.6625) in the two-step task.

### Model 1: Anxiety-Modulated Asymmetric Learning Rates

This model proposes that high anxiety leads to an asymmetric learning process, where negative prediction errors (outcomes worse than expected) are learned from more effectively than positive ones. This could reflect an anxious individual's heightened sensitivity to potential threats or losses, causing them to over-adjust their expectations when faced with unfavorable results.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Asymmetric Learning Rates with Anxiety-Modulated Negative Learning.
    This model proposes that participants learn differently from positive and negative
    prediction errors. Specifically, the learning rate for negative prediction errors
    is modulated by anxiety: higher STAI scores lead to an increased sensitivity
    to outcomes worse than expected (i.e., a higher learning rate for negative prediction errors).
    This could reflect an anxious individual's tendency to dwell on or over-learn from
    unfavorable experiences.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1] - Learning rate for positive prediction errors (reward > expected).
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors (reward < expected).
    alpha_neg_stai_mod: [0, 1] - Modulatory parameter for STAI on the negative learning rate.
                                  A positive value means higher anxiety increases alpha_neg.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.alpha_neg_stai_mod, self.beta = model_parameters

    def init_model(self) -> None:
        # Calculate the effective negative learning rate, modulated by STAI score.
        # Ensure it stays within reasonable bounds [0, 1].
        self.alpha_neg_eff = np.clip(self.alpha_neg_base + self.alpha_neg_stai_mod * self.stai, 0, 1)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values using asymmetric learning rates for positive and negative prediction errors.
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += self.alpha_neg_eff * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += self.alpha_neg_eff * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Learning from Rare Transitions

This model suggests that participants differentiate between common and rare transitions between stages, and anxiety specifically impacts how they learn from rare, unexpected transitions. High anxiety might lead to an over-sensitivity to these rare events, causing the participant to over-adjust their first-stage strategy.

```python
import numpy as np

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Learning from Rare Transitions.
    This model posits that participants distinguish between common and rare transitions
    at the first stage. High anxiety (STAI score) specifically increases the learning
    rate for first-stage value updates when a *rare* transition occurs. This could
    reflect an anxious individual's heightened sensitivity or over-reaction to unexpected
    environmental changes, leading them to adjust their first-stage strategy more
    drastically after rare events.

    Parameter Bounds:
    -----------------
    alpha_s2: [0, 1] - Learning rate for second-stage Q-values.
    alpha_s1_common: [0, 1] - Learning rate for first-stage Q-values following common transitions.
    alpha_s1_rare_base: [0, 1] - Base learning rate for first-stage Q-values following rare transitions.
    alpha_s1_rare_stai_mod: [0, 1] - Modulatory parameter for STAI on rare transition learning rate.
                                   A positive value means higher anxiety increases alpha_s1_rare.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_s2, self.alpha_s1_common, self.alpha_s1_rare_base, self.alpha_s1_rare_stai_mod, self.beta = model_parameters

    def init_model(self) -> None:
        # Calculate the effective rare transition learning rate, modulated by STAI score.
        self.alpha_s1_rare_eff = np.clip(self.alpha_s1_rare_base + self.alpha_s1_rare_stai_mod * self.stai, 0, 1)
        
        # Define common transitions for easy checking
        # Spaceship 0 (A) commonly goes to Planet 0 (X)
        # Spaceship 1 (U) commonly goes to Planet 1 (Y)
        self.common_transitions = {(0, 0), (1, 1)}

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values using different learning rates for first-stage based on transition type.
        """
        # Stage 2 update uses a fixed alpha
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha_s2 * delta_2
        
        # Determine which alpha to use for Stage 1 update based on transition type
        if (action_1, state) in self.common_transitions:
            alpha_s1_to_use = self.alpha_s1_common
        else: # Rare transition
            alpha_s1_to_use = self.alpha_s1_rare_eff
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_s1_to_use * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Reward Sensitivity

This model proposes that high anxiety directly impacts how rewarding positive outcomes feel. Specifically, higher anxiety is hypothesized to reduce the effective magnitude of received rewards, making them less impactful for learning. This dampened reward signal could lead to slower or less effective learning, as the internal value updates are based on a diminished perception of success.

```python
import numpy as np

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Reward Sensitivity.
    This model proposes that high anxiety (STAI score) directly affects how participants
    perceive and learn from rewards. Specifically, higher anxiety is hypothesized to
    reduce the effective magnitude of the received reward, making positive outcomes
    feel less impactful or "less rewarding." This diminished reward sensitivity could
    lead to slower or less effective learning, as the internal value updates are based
    on a dampened reward signal.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    reward_sens_base: [0, 1] - Base level of reward sensitivity.
    reward_sens_stai_mod: [-1, 1] - Modulatory parameter for STAI on reward sensitivity.
                                    A negative value means higher anxiety reduces sensitivity.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.reward_sens_base, self.reward_sens_stai_mod = model_parameters

    def init_model(self) -> None:
        # Calculate the effective reward sensitivity, modulated by STAI score.
        # Clip to ensure it stays within a reasonable range [0, 1].
        self.reward_sens_eff = np.clip(self.reward_sens_base + self.reward_sens_stai_mod * self.stai, 0, 1)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values, applying anxiety-modulated reward sensitivity to the received reward.
        """
        # Apply reward sensitivity to the actual reward before calculating delta_2
        effective_reward = reward * self.reward_sens_eff

        # Stage 2 update
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (uses the updated q_stage2 value, which is based on effective_reward)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```