Here are three new cognitive models, each proposing a different cognitive strategy influenced by the participant's high anxiety (STAI score = 0.6875).

### Proposed Cognitive Models

```python
import numpy as np # numpy is available as np in the environment

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to asymmetric learning, making participants more sensitive to
    negative prediction errors (non-rewards/losses). This model proposes separate learning rates
    for positive and negative prediction errors. The learning rate for negative prediction errors
    is further amplified by the STAI score, reflecting a 'punishment-driven' learning style
    under anxiety.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1] - Learning rate for positive prediction errors.
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors.
    stai_neg_amplification: [0, 2] - Multiplier for how much STAI amplifies the negative learning rate.
                                      A value of 0 means STAI has no effect on negative learning.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.stai_neg_amplification, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate alpha_neg based on STAI score
        current_alpha_neg = self.alpha_neg_base + self.stai_neg_amplification * self.stai
        # Clamp alpha_neg to ensure it stays within reasonable bounds [0, 1]
        current_alpha_neg = np.clip(current_alpha_neg, 0.0, 1.0)

        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += current_alpha_neg * delta_2

        # Stage 1 update - using the updated Q2 value as the target
        # The prediction error for stage 1 is based on the value of the chosen stage 2 action
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += current_alpha_neg * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety impairs the ability to engage in complex model-based planning,
    leading participants to rely more on simpler, habitual (model-free) strategies.
    This model proposes that the STAI score linearly boosts the weight given to the
    model-free component when making first-stage decisions, reducing the influence of
    model-based value calculations.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for Q-values in both stages.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    w_mf_base: [0, 1] - Base weight given to the model-free component (0 = pure MB, 1 = pure MF).
    stai_mf_boost: [0, 1] - How much STAI adds to the model-free weight.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_mf_base, self.stai_mf_boost = model_parameters

    def init_model(self) -> None:
        # Initialize the model-based Q-values for stage 1
        self.q_stage1_mb = np.zeros(self.n_choices)
        # Store the combined Q-values for stage 1 decisions
        self.q_stage1_combined = np.zeros(self.n_choices)

    def pre_trial(self) -> None:
        # Calculate the current model-free weight, boosted by STAI
        current_w_mf = self.w_mf_base + self.stai_mf_boost * self.stai
        # Clamp w_mf between 0 and 1
        self.w_mf = np.clip(current_w_mf, 0.0, 1.0)

        # Calculate model-based Q-values for stage 1: Q_mb(a1) = sum_s P(s|a1) * max_a2 Q_mf(s, a2)
        for a1 in range(self.n_choices): # For each spaceship (0 or 1)
            expected_future_value = 0
            for s_prime in range(self.n_states): # For each possible planet (0 or 1)
                # self.T[a1, s_prime] is the transition probability P(s_prime | a1)
                # np.max(self.q_stage2[s_prime]) is the maximum value achievable from planet s_prime
                expected_future_value += self.T[a1, s_prime] * np.max(self.q_stage2[s_prime])
            self.q_stage1_mb[a1] = expected_future_value
        
        # Combine model-free and model-based Q-values for stage 1 decisions
        # self.q_stage1 is the model-free Q-value for stage 1
        self.q_stage1_combined = self.w_mf * self.q_stage1 + (1 - self.w_mf) * self.q_stage1_mb

    def policy_stage1(self) -> np.ndarray:
        # Use the combined Q-values for stage 1 action selection
        return self.softmax(self.q_stage1_combined, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Model-free updates (as in the base class, using self.alpha)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # The target for stage 1 Q-value is the value of the chosen action at stage 2
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety makes participants more sensitive to rare or unexpected transitions.
    When a rare transition occurs (e.g., choosing spaceship A but landing on planet Y),
    this model proposes that the prediction error for the subsequent reward on that trial
    is amplified. This heightened sensitivity to surprising events, modulated by STAI,
    leads to more volatile and impactful learning following rare transitions.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Base learning rate for Q-values in both stages.
    beta: [0, 10] - Softmax inverse temperature for action selection.
    rare_trans_pe_multiplier_base: [0.5, 2] - Base multiplier for prediction error when a rare transition occurs.
    stai_rare_trans_pe_boost: [0, 2] - How much STAI adds to the rare transition PE multiplier.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rare_trans_pe_multiplier_base, self.stai_rare_trans_pe_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine if the transition was rare
        is_rare_transition = False
        # Spaceship A (0) commonly goes to Planet X (0), rare to Planet Y (1)
        # Spaceship U (1) commonly goes to Planet Y (1), rare to Planet X (0)
        if (action_1 == 0 and state == 1) or \
           (action_1 == 1 and state == 0):
            is_rare_transition = True

        # Calculate the prediction error multiplier
        pe_multiplier = 1.0 # Default multiplier for common transitions
        if is_rare_transition:
            current_pe_multiplier = self.rare_trans_pe_multiplier_base + self.stai_rare_trans_pe_boost * self.stai
            # Clamp multiplier to ensure it's not excessively large or small
            pe_multiplier = np.clip(current_pe_multiplier, 0.1, 5.0) 

        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        # Apply the multiplier to the prediction error
        self.q_stage2[state, action_2] += self.alpha * (delta_2 * pe_multiplier) 

        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        # Apply the multiplier to the prediction error
        self.q_stage1[action_1] += self.alpha * (delta_1 * pe_multiplier) 

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```