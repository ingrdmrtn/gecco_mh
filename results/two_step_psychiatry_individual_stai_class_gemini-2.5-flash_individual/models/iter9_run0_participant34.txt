Here are three new cognitive models, each proposing a distinct hypothesis about how the participant, with their high anxiety (STAI score of 0.9625), makes decisions in the two-step task. Each model incorporates the STAI score to modulate a specific aspect of behavior, ensuring diversity in the proposed mechanisms.

---

### ParticipantModel1: Anxious Goal-Directed Re-evaluation (Stage 1)

This model proposes that highly anxious individuals exhibit a stronger, more immediate influence of the final reward on their initial Stage 1 choices. Instead of solely relying on the learned Q-value of the second stage, their Stage 1 value updates are more directly "pulled" by the actual reward received, with this direct influence amplified by their anxiety level. This could represent a heightened sensitivity to outcomes or a more immediate, less filtered feedback mechanism under stress.

```python
import numpy as np # Ensure numpy is available

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxious Goal-Directed Re-evaluation (Stage 1).
    High anxiety leads to a more immediate re-evaluation of Stage 1 choices based on the *actual reward received*,
    rather than solely the learned Q-value of the second stage. This means a stronger "back-propagation" of the
    final reward to the first stage, bypassing or amplifying the intermediate Q-value of the second stage.
    This could be interpreted as a more direct, goal-directed influence under anxiety, where the final outcome
    has a stronger pull on initial choices.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate for Q-values)
    beta: [0, 10] (Softmax inverse temperature for both stages)
    reward_weight_base: [0, 1] (Base weight given to the final reward when updating Stage 1 Q-values)
    stai_reward_weight_boost: [0, 1] (Factor by which STAI increases the reward weight for Stage 1 updates)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.reward_weight_base, self.stai_reward_weight_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Update values after observing outcome, with anxiety-modulated direct reward influence on Stage 1.
        """
        # Stage 2 update (standard TD learning)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Calculate anxiety-modulated reward weight for Stage 1 update
        # The weight of the final reward in the Stage 1 update is increased by STAI.
        # np.clip ensures the weight stays within [0, 1].
        reward_weight = np.clip(self.reward_weight_base + self.stai * self.stai_reward_weight_boost, 0, 1)
        
        # Stage 1 update: blend the actual reward with the Stage 2 Q-value to form the target for Stage 1.
        # A higher reward_weight means the Stage 1 Q-value is more directly influenced by the final reward.
        blended_target_value = reward_weight * reward + (1 - reward_weight) * self.q_stage2[state, action_2]
        delta_1 = blended_target_value - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

---

### ParticipantModel2: Anxious Switching after Rare Transitions (Stage 1)

This model hypothesizes that high anxiety makes participants particularly sensitive to unexpected (rare) transitions. If a Stage 1 choice leads to a rare planet, highly anxious individuals become more prone to switch away from that initial spaceship on the next trial, regardless of the reward received. This reflects an anxious attempt to avoid unpredictable situations or a general restlessness when expectations are violated.

```python
import numpy as np # Ensure numpy is available

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxious Switching after Rare Transitions (Stage 1).
    High anxiety makes participants particularly prone to *switching* their Stage 1 action if the previous trial
    resulted in a *rare transition*, regardless of the reward received. This reflects an attempt to avoid
    unpredictable situations or a general restlessness under anxiety when things don't go as expected.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate for Q-values)
    beta: [0, 10] (Softmax inverse temperature for both stages)
    rare_switch_bonus_base: [0, 1] (Base bonus added to the unchosen Stage 1 action's Q-value if the last transition was rare)
    stai_rare_switch_boost: [0, 1] (Factor by which STAI amplifies this switch bonus)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rare_switch_bonus_base, self.stai_rare_switch_boost = model_parameters

    def init_model(self) -> None:
        super().init_model()
        self.last_action1 = -1 # Sentinel for no previous choice
        self.last_state = -1   # Sentinel for no previous state
        self.last_transition_rare = False # Flag to track if the last transition was rare

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities, applying an anxiety-amplified
        switch bonus to the unchosen action if the last transition was rare.
        """
        q_stage1_biased = np.copy(self.q_stage1)

        # Apply bias only if a previous choice was made and it resulted in a rare transition
        if self.last_action1 != -1 and self.last_transition_rare:
            # Calculate the anxiety-amplified switch bonus
            switch_bonus = self.rare_switch_bonus_base + (self.stai * self.stai_rare_switch_boost)
            
            # The bonus is applied to the *other* action (the one not chosen last time)
            # to encourage switching away from the action that led to the rare transition.
            unchosen_action = 1 - self.last_action1 
            q_stage1_biased[unchosen_action] += switch_bonus
        
        return self.softmax(q_stage1_biased, self.beta)
    
    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Called after each trial. Updates last_action1, last_state, last_reward,
        and determines if the transition to the current state was rare.
        """
        # Call base method to update last_action1, last_state, last_reward for the *current* trial
        super().post_trial(action_1, state, action_2, reward) 

        # Determine if the transition observed in the *current* trial was rare
        # Spaceship 0 commonly travels to Planet 0, Spaceship 1 commonly travels to Planet 1.
        # This means if action_1 == state, it's a common transition (0.7 probability).
        # If action_1 != state, it's a rare transition (0.3 probability).
        if action_1 != -1 and state != -1: # Ensure valid actions/states
            self.last_transition_rare = (action_1 != state)
        else:
            self.last_transition_rare = False # No previous trial information or invalid state

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

---

### ParticipantModel3: Anxious Outcome-Dependent Beta (Stage 1)

This model posits that high anxiety makes a participant's Stage 1 exploration-exploitation balance highly volatile and dependent on the immediate outcome. After receiving a reward, they become more exploitative (higher `beta`), reinforcing the successful choice. Conversely, after receiving no reward, they become more exploratory or random (lower `beta`), perhaps due to frustration or increased uncertainty under stress. This dynamic modulation of `beta` is amplified by their anxiety level.

```python
import numpy as np # Ensure numpy is available

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxious Outcome-Dependent Beta (Stage 1).
    Under high anxiety, participants' exploration/exploitation tendencies in Stage 1 are strongly modulated by
    the immediate outcome. After a reward, they become more exploitative (higher beta). After no reward,
    they become more exploratory or random (lower beta). This modulation is amplified by their anxiety level,
    reflecting a volatile response to outcomes under stress.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate for Q-values)
    beta_base_stage1: [0, 10] (Base inverse temperature for Stage 1 choices)
    stai_reward_beta_boost: [0, 1] (Factor by which STAI increases beta_stage1 after a reward)
    stai_noreward_beta_reduction: [0, 1] (Factor by which STAI decreases beta_stage1 after no reward)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        # Note: self.beta will be dynamically set in pre_trial.
        # The base class's policy methods will use this dynamically updated self.beta.
        self.alpha, self.beta_base_stage1, self.stai_reward_beta_boost, self.stai_noreward_beta_reduction = model_parameters

    def init_model(self) -> None:
        super().init_model()
        self.last_reward = 0.5 # Initialize last_reward to a neutral value (e.g., 0.5)

    def pre_trial(self) -> None:
        """
        Called before each trial. Adjusts the effective Stage 1 beta based on the last outcome and anxiety.
        This dynamically set beta will then be used by policy_stage1 and policy_stage2.
        """
        current_beta = self.beta_base_stage1

        if self.last_reward == 1: # If the last trial was rewarded
            current_beta += self.stai * self.stai_reward_beta_boost
        elif self.last_reward == 0: # If the last trial was not rewarded
            current_beta -= self.stai * self.stai_noreward_beta_reduction
        
        # Ensure beta stays within a reasonable, positive range to prevent numerical issues
        self.beta = np.clip(current_beta, 0.1, 10.0) 

    # The default value_update and post_trial methods from CognitiveModelBase are sufficient,
    # as post_trial correctly updates self.last_reward for the next trial's beta calculation.
    # The default policy_stage1 and policy_stage2 will use the self.beta value set in pre_trial.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```