Here are 3 new cognitive models, each proposing a distinct hypothesis about how the participant's high anxiety (STAI score 0.6625) might modulate their decision-making in the two-step task.

### Model 1: Anxiety-Modulated Negative Reward Amplification

This model proposes that high anxiety leads to an amplified subjective experience of negative rewards. When a participant receives a negative outcome, an anxious individual will perceive that outcome as "worse" than a less anxious person, leading to a more drastic downward adjustment of their value estimates. This could reflect a pessimistic or catastrophizing learning style.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety amplifies the subjective impact of negative rewards,
    making them feel 'worse' for the purpose of learning. This leads to a more
    drastic downward adjustment of value estimates when negative outcomes are
    experienced, reflecting a pessimistic or catastrophizing learning style.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate.
    beta: [0, 10] - Softmax inverse temperature.
    stai_neg_reward_amplification_factor: [0, 5] - Factor by which STAI amplifies the
                                                    impact of negative rewards on learning.
                                                    Higher values mean more anxious individuals
                                                    experience negative rewards as more detrimental.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stai_neg_reward_amplification_factor = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Adjust the reward based on anxiety if it's a negative outcome
        effective_reward = reward
        if reward < 0: # Focus only on negative rewards
            # Amplify the negative reward magnitude for learning purposes.
            # E.g., if reward=-1, stai=0.66, factor=1, then effective_reward = -1 * (1 + 0.66 * 1) = -1.66
            effective_reward = reward * (1 + self.stai * self.stai_neg_reward_amplification_factor)
            
        # Stage 2 value update using the effective reward
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 value update uses the updated Q2 value, which was influenced by the effective reward
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Stickiness after Rare Transitions

This model posits that high anxiety increases a tendency to repeat the previous stage-1 choice (stickiness). Crucially, this stickiness is amplified specifically after experiencing an unexpected or rare transition, reflecting a potential coping mechanism to reduce uncertainty or avoid re-evaluating options in the face of surprising environmental dynamics.

```python
import numpy as np

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases a 'stickiness' or repetition bias,
    making participants more likely to repeat their previous stage-1 choice.
    This effect is amplified particularly after experiencing a rare (unexpected)
    transition, possibly as a coping mechanism to reduce uncertainty or
    avoid re-evaluation in the face of surprising events.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate.
    beta: [0, 10] - Softmax inverse temperature.
    stickiness_base: [0, 5] - Base stickiness parameter, added to the Q-value of the last chosen action.
    stai_stickiness_boost_rare_trans: [0, 5] - Factor by which STAI amplifies stickiness
                                                specifically after a rare transition.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.stai_stickiness_boost_rare_trans = model_parameters
        
    def init_model(self) -> None:
        # Initialize last_action1 to a value that won't trigger stickiness on the first trial
        self.last_action1 = -1 

    def policy_stage1(self) -> np.ndarray:
        q_values = np.copy(self.q_stage1)
        if self.last_action1 != -1: # Apply stickiness only from the second trial onwards
            stickiness_value = self.stickiness_base

            # Check if the last transition (from previous trial) was rare.
            # T[action_1, state] gives the probability of state given action_1.
            # Common transitions: T[0,0] and T[1,1] (prob 0.7)
            # Rare transitions: T[0,1] and T[1,0] (prob 0.3)
            if self.last_action1 is not None and self.last_state is not None:
                # Use the fixed transition probabilities from the base class to determine if it was rare
                is_rare_transition = (self.T[self.last_action1, self.last_state] < 0.5)
                if is_rare_transition:
                    stickiness_value += self.stai * self.stai_stickiness_boost_rare_trans
            
            q_values[self.last_action1] += stickiness_value
            
        return self.softmax(q_values, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Temporal Discounting of Stage 2 Value

This model proposes that high anxiety leads to increased temporal discounting of future rewards, particularly how the value of the second stage (planet/alien outcome) influences the first-stage (spaceship) decision. Anxious individuals might effectively "devalue" these more distal outcomes, leading to more myopic or immediate-reward-focused choices at the first stage.

```python
import numpy as np

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to increased temporal discounting of future rewards,
    specifically affecting how the value of the second stage (planet/alien outcome)
    informs the first-stage (spaceship) decision. Anxious individuals might
    effectively "devalue" the distal second-stage outcomes, leading to more
    myopic or immediate-reward-focused choices at the first stage.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate.
    beta: [0, 10] - Softmax inverse temperature.
    gamma_base: [0, 1] - Base discount factor for the second-stage value
                         when updating the first-stage value.
    stai_gamma_reduction_factor: [0, 1] - Factor by which STAI reduces the
                                          discount factor (gamma), making
                                          anxious individuals discount more heavily.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.gamma_base, self.stai_gamma_reduction_factor = model_parameters
        
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate the effective discount factor based on STAI
        # Ensure gamma_stai doesn't go below 0 (or some reasonable minimum, e.g., 0.01)
        gamma_stai = max(0.01, self.gamma_base - self.stai * self.stai_gamma_reduction_factor)
        
        # Stage 2 value update (remains standard TD learning)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 value update, incorporating the anxiety-modulated discount factor.
        # The Q-value of the chosen stage-2 state-action pair is discounted
        # before being used as the target for the stage-1 update.
        delta_1 = (gamma_stai * self.q_stage2[state, action_2]) - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```