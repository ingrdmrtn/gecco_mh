```python
import numpy as np
from abc import ABC, abstractmethod

# --- Base Model Definition (Provided) ---
class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition probabilities
        self.trans_counts = np.array([[35, 15], [15, 35]]) # Prior counts for transitions, if needed for learning
        self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True) # Transition probabilities, if needed for direct use
        
        # Choice probability sequences (for fitting, not strictly needed for simulation but good to keep)
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage1[:] = 0.5 # Initialize to neutral values
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        # Default RL update if not overridden
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2 # Assumes self.alpha exists
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1 # Assumes self.alpha exists

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials for fitting. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        # Add a small epsilon to prevent division by zero in case of extreme beta and identical values
        return exp_vals / (np.sum(exp_vals) + 1e-12)

# --- ParticipantModel1 Definition (Provided) ---
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases the learning rate specifically for outcomes observed after rare transitions.
    This model assumes separate base learning rates for common and rare transitions, and the rare learning rate
    is further boosted by the participant's STAI score, reflecting increased attention or sensitivity to unexpected events.
    This could lead anxious individuals to over-update their expectations following surprising outcomes.

    Parameter Bounds:
    -----------------
    alpha_common: [0, 1] - Learning rate for value updates following common transitions.
    alpha_rare_base: [0, 1] - Base learning rate for value updates following rare transitions.
    beta: [0, 10] - Softmax inverse temperature.
    stai_rare_learning_boost: [0, 5] - Factor by which STAI amplifies the rare transition learning rate.
                                       A higher value means more anxious individuals learn more from rare events.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_common, self.alpha_rare_base, self.beta, self.stai_rare_learning_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine if the transition was rare.
        # The transition matrix T is structured such that T[0,0] and T[1,1] are common (0.7),
        # while T[0,1] and T[1,0] are rare (0.3).
        is_rare_transition = (self.T[action_1, state] < 0.5)

        if is_rare_transition:
            # Anxiety boosts the rare learning rate.
            # (1 + stai_rare_learning_boost * self.stai) ensures the boost is proportional to anxiety.
            effective_alpha = self.alpha_rare_base * (1 + self.stai_rare_learning_boost * self.stai)
            # Cap effective_alpha at 1.0 to prevent it from exceeding the theoretical maximum.
            effective_alpha = min(effective_alpha, 1.0)
        else:
            effective_alpha = self.alpha_common

        # Stage 2 value update using the effective alpha
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += effective_alpha * delta_2
        
        # Stage 1 value update also uses the same effective alpha,
        # as it learns from the updated Q2 value which was influenced by the transition type.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += effective_alpha * delta_1

# --- Simulation Function (Your Task) ---
def simulate_model(
    ModelClass,
    n_trials: int,
    stai: float,
    model_parameters: tuple,
    drift1: np.ndarray,
    drift2: np.ndarray,
    drift3: np.ndarray,
    drift4: np.ndarray,
    seed: int | None = None,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Simulate a two-step task trajectory from any current-template model class
    (i.e., a subclass of CognitiveModelBase like ParticipantModel1).

    Task structure:
      - Stage 1: choose action_1 in {0,1}
      - Transition: state in {0,1} sampled from model.T[action_1]
      - Stage 2: choose action_2 in {0,1} conditioned on state
      - Reward: sampled from drifting reward probabilities:
          state 0: [drift1[t], drift2[t]] for actions 0/1
          state 1: [drift3[t], drift4[t]] for actions 0/1

    Parameters
    ----------
    ModelClass : type
        A subclass of CognitiveModelBase (e.g., ParticipantModel1).
    n_trials : int
        Number of trials to simulate.
    stai : float
        Participant anxiety score; passed into the model constructor as self.stai.
    model_parameters : tuple
        Parameters expected by ModelClass.unpack_parameters(...).
    drift1, drift2, drift3, drift4 : array-like (length n_trials)
        Trial-wise reward probabilities for the 4 (state, action) pairs.
    seed : int or None
        RNG seed.

    Returns
    -------
    stage1_choice : np.ndarray, shape (n_trials,)
    state2        : np.ndarray, shape (n_trials,)
    stage2_choice : np.ndarray, shape (n_trials,)
    reward        : np.ndarray, shape (n_trials,)
    """
    rng = np.random.default_rng(seed)

    # Instantiate model (will set up q-values, self.T, etc.)
    model = ModelClass(n_trials=n_trials, stai=float(stai), model_parameters=model_parameters)

    # Initialize arrays to store simulation results
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        model.trial = t
        model.pre_trial()

        # --- Stage 1 choice ---
        p1 = model.policy_stage1()
        # Safety normalize in case an override returns tiny numerical drift
        p1 = np.clip(p1, 1e-12, 1.0)
        p1 = p1 / np.sum(p1)
        a1 = int(rng.choice([0, 1], p=p1))

        # --- Transition ---
        pT = model.T[a1]
        pT = np.clip(pT, 1e-12, 1.0)
        pT = pT / np.sum(pT)
        s = int(rng.choice([0, 1], p=pT))

        # --- Stage 2 choice ---
        p2 = model.policy_stage2(s)
        p2 = np.clip(p2, 1e-12, 1.0)
        p2 = p2 / np.sum(p2)
        a2 = int(rng.choice([0, 1], p=p2))

        # --- Reward from drifting probabilities ---
        # state 0 -> (drift1, drift2); state 1 -> (drift3, drift4)
        if s == 0:
            pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
        else: # s == 1
            pr = float(drift3[t]) if a2 == 0 else float(drift4[t])

        r = int(rng.random() < pr)

        # (Optional) store likelihoods if you want (not required for simulation)
        # model.p_choice_1[t] = p1[a1]
        # model.p_choice_2[t] = p2[a2]

        # --- Learning update + bookkeeping ---
        model.value_update(a1, s, a2, float(r))
        model.post_trial(a1, s, a2, float(r))

        # Store results for the current trial
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```