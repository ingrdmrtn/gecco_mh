class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This model assumes that low anxiety promotes behavioral stability.
    This is captured by two mechanisms:
    1.  A lower learning rate ('alpha'), making value updates less reactive to
        recent outcomes. The effective alpha is scaled by (1 + stai).
    2.  A 'stickiness' bias ('kappa') to repeat the previous first-stage choice.
        This bias is stronger for individuals with lower anxiety, reflecting
        less tendency to switch strategies. The effective kappa is scaled by (1 - stai).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Base learning rate)
    beta:  [0, 10]  (Inverse temperature for softmax)
    kappa: [0, 5]   (Base stickiness weight)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.kappa = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """Computes stage-1 policy with a stickiness bias."""
        # Calculate effective kappa, higher for lower anxiety
        kappa_eff = self.kappa * (1 - self.stai)

        # Create a stickiness vector
        perseveration_bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            perseveration_bonus[self.last_action1] = kappa_eff

        # Add stickiness bonus to Q-values before softmax
        q_values_with_bias = self.q_stage1 + perseveration_bonus
        return self.softmax(q_values_with_bias, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Updates Q-values using an anxiety-modulated learning rate."""
        # Calculate effective alpha, lower for lower anxiety
        alpha_eff = self.alpha * (1 + self.stai)

        # Standard TD learning, but with the effective learning rate
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha_eff * delta_2

        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_eff * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)