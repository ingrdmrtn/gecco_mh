class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model posits that the participant's medium anxiety fosters
    a simple, low-effort heuristic: choice perseveration or "stickiness".
    They are more likely to repeat their previous first-stage choice,
    independent of its learned value. This cognitive inflexibility is a
    common response to anxiety. The strength of this perseverative bias is
    directly proportional to the participant's STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]  (Learning rate)
    beta: [0, 10] (Softmax inverse temperature)
    pi: [0, 5]    (Perseveration strength parameter)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Compute stage-1 action probabilities, adding an anxiety-modulated
        perseveration bonus to the previously chosen action.
        """
        # Start with the learned Q-values
        q_policy = np.copy(self.q_stage1)

        # On all trials after the first, add a perseveration bonus
        if self.trial > 0 and self.last_action1 is not None:
            # The perseveration bonus is scaled by anxiety
            bonus = self.pi * self.stai
            q_policy[self.last_action1] += bonus

        return self.softmax(q_policy, self.beta)

    # The value_update method from the base class is sufficient, as it updates
    # the underlying Q-values without the policy bias. The post_trial method
    # from the base class correctly updates self.last_action1.

cognitive_model3 = make_cognitive_model(ParticipantModel3)