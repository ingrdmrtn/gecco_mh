class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that the participant has a tendency to repeat
    their previous first-stage choice, a phenomenon known as perseveration or
    choice inertia. The strength of this "sticky" bias is directly modulated by the
    participant's anxiety level (stai). For a person with medium anxiety, this
    creates a significant, but not absolute, bonus on the value of repeating the
    last action, which can explain the rapid locking-in to a single spaceship choice.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           (Learning rate for Q-values)
    beta: [0, 10]          (Decision temperature/exploration)
    persev_strength: [0, 5] (Base strength of the perseveration bonus)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.persev_strength = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 choice probabilities by adding a perseveration bonus
        to the Q-value of the action chosen on the previous trial.
        """
        q_effective = self.q_stage1.copy()

        if self.last_action1 is not None:
            # Calculate the bonus, scaled by anxiety
            bonus = self.persev_strength * self.stai
            # Add the bonus to the value of the previously chosen action
            q_effective[self.last_action1] += bonus

        return self.softmax(q_effective, self.beta)

    # This model uses the default value_update and other methods from CognitiveModelBase.

cognitive_model2 = make_cognitive_model(ParticipantModel2)