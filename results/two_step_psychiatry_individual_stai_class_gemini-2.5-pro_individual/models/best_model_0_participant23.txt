class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model suggests that anxiety promotes the generalization of
    negative outcomes. When a reward is worse than expected, the participant not
    only devalues the chosen option but also the other, unchosen option within the
    same state (i.e., on the same planet). The strength of this generalization is
    scaled by the participant's STAI score, reflecting an anxiety-driven bias to
    perceive entire contexts as threatening after a single negative event.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Learning rate)
    beta: [0, 10]   (Inverse temperature for softmax)
    rho: [0, 1]     (Anxiety-driven negative generalization parameter)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rho = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates Q-values with a standard TD rule for the chosen action, and adds
        an anxiety-modulated generalization update for the unchosen action at stage 2
        if the outcome was negative.
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # If the outcome was worse than expected, generalize the negative update
        if delta_2 < 0:
            other_action_2 = 1 - action_2
            generalization_strength = self.alpha * self.rho * self.stai
            self.q_stage2[state, other_action_2] += generalization_strength * delta_2
            
        # Stage 1 update (based on the value of the action actually taken)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)