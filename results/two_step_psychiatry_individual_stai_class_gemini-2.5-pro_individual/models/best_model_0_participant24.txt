class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model posits that anxiety promotes a simple heuristic: 
    repeating the previous action. This "perseveration bias" provides a sense of 
    predictability and avoids the uncertainty of switching. The model adds a 
    "stickiness" bonus to the value of the action chosen on the previous trial. 
    The magnitude of this bonus is directly proportional to the participant's STAI 
    score. This mechanism can account for the long streaks of repeating the same 
    choice seen in the data, framing it as an anxiety-driven strategy to cope 
    with task uncertainty.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    kappa: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.kappa = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 probabilities by adding an anxiety-scaled perseveration
        bonus to the value of the action chosen on the last trial.
        """
        q_effective = self.q_stage1.copy()
        
        # Apply perseveration bonus if not the first trial
        if self.trial > 0 and self.last_action1 is not None:
            # The bonus is the base strength (kappa) scaled by anxiety
            perseveration_bonus = self.kappa * self.stai
            q_effective[self.last_action1] += perseveration_bonus
            
        return self.softmax(q_effective, self.beta)
        
    def policy_stage2(self, state: int) -> np.ndarray:
        """
        Computes stage-2 probabilities by adding an anxiety-scaled perseveration
        bonus for the second-stage choice as well.
        """
        q_effective = self.q_stage2[state].copy()

        # Apply perseveration bonus if not the first trial and we were in the same state
        if self.trial > 0 and self.last_action2 is not None and self.last_state == state:
            perseveration_bonus = self.kappa * self.stai
            q_effective[self.last_action2] += perseveration_bonus
            
        return self.softmax(q_effective, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)