class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that anxiety enhances learning from negative
    outcomes. The participant uses a higher learning rate after not receiving a
    reward, and the magnitude of this increase is scaled by their stai score.
    This makes them particularly sensitive to punishment. The model also includes
    a static perseveration bias to account for choice repetition, but this bias
    is independent of anxiety.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]  (Learning rate for rewards)
    alpha_neg: [0, 1]  (Base learning rate for non-rewards)
    beta: [0, 10]     (Inverse temperature for softmax)
    pi: [0, 10]       (Perseveration strength)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg, self.beta, self.pi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Stage-1 policy includes a static perseveration bonus.
        """
        if self.trial == 0 or self.last_action1 is None:
            return self.softmax(self.q_stage1, self.beta)
            
        perseveration_vector = np.zeros(self.n_choices)
        perseveration_vector[self.last_action1] = 1.0
        effective_q_stage1 = self.q_stage1 + self.pi * perseveration_vector
        return self.softmax(effective_q_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Value update uses different learning rates for rewards and non-rewards,
        with the non-reward learning rate being modulated by anxiety.
        """
        # Anxiety enhances the learning rate for negative outcomes
        alpha_neg_effective = min(1.0, self.alpha_neg * (1 + self.stai))
        
        # Select the learning rate based on the outcome
        alpha = self.alpha_pos if reward > 0 else alpha_neg_effective
        
        # Standard TD learning updates
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)