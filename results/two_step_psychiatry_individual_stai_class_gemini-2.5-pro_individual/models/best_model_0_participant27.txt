class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model suggests that anxiety influences both learning and choice
    biases. Specifically, it proposes two mechanisms:
    1. Asymmetric Learning: The participant is more sensitive to negative outcomes
       (non-rewards) than positive ones. Anxiety (stai) amplifies the learning rate
       for negative prediction errors.
    2. Perseveration: The participant has a tendency to repeat the previous action.
       Anxiety (stai) increases this 'stickiness', leading to more rigid choice patterns.
    This model uses a standard model-based value updating scheme.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]   (learning rate for positive prediction errors)
    alpha_neg_base: [0, 1] (base learning rate for negative prediction errors)
    beta: [0, 10]      (softmax inverse temperature)
    pi: [0, 5]         (perseveration/stickiness strength)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta, self.pi = model_parameters

    def init_model(self) -> None:
        # Anxiety modulates learning from negative outcomes
        self.alpha_neg = self.alpha_neg_base * (1 + self.stai)
        # Anxiety modulates perseveration strength
        self.pi_effective = self.pi * self.stai

    def policy_stage1(self) -> np.ndarray:
        """
        Stage-1 policy includes a perseveration bias, boosting the value
        of the most recently chosen action.
        """
        q_biased = self.q_stage1.copy()
        if self.last_action1 is not None:
            q_biased[self.last_action1] += self.pi_effective
        return self.softmax(q_biased, self.beta)
        
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates values using different learning rates for positive and
        negative prediction errors, following a model-based (SARSA-style) logic.
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)