class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases sensitivity to environmental volatility,
    promoting choice inertia. The model tracks the volatility of the second-stage
    options (aliens). When the agent experiences high volatility (large prediction
    errors), it becomes more likely to repeat its previous first-stage choice.
    This effect is amplified by the participant's STAI score, modeling a retreat
    to habitual behavior under perceived uncertainty.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]              (Learning rate for values)
    beta: [0, 10]             (Softmax inverse temperature)
    stickiness_param: [0, 5]  (Base strength of the inertia bonus)
    eta: [0, 1]                (Learning rate for volatility)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_param, self.eta = model_parameters

    def init_model(self) -> None:
        """Initialize a tracker for option volatility."""
        self.volatility = np.zeros_like(self.q_stage2)

    def policy_stage1(self) -> np.ndarray:
        """
        Adds an inertia bonus to the previous choice, scaled by the experienced
        volatility and anxiety.
        """
        q_effective = self.q_stage1.copy()
        if self.last_action1 is not None:
            # Get the average volatility of the planet visited on the last trial
            last_state_volatility = np.mean(self.volatility[self.last_state])
            
            # Calculate bonus based on volatility, stickiness tendency, and anxiety
            stickiness_bonus = self.stickiness_param * last_state_volatility * self.stai
            q_effective[self.last_action1] += stickiness_bonus
            
        return self.softmax(q_effective, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates Q-values and the volatility tracker.
        """
        # Update stage-2 value
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2

        # Update volatility tracker with the magnitude of the prediction error
        self.volatility[state, action_2] += self.eta * (np.abs(delta_2) - self.volatility[state, action_2])

        # Update stage-1 value using a model-based approach
        delta_1 = np.max(self.q_stage2[state]) - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)