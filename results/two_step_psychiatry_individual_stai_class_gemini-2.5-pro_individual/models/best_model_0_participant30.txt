class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety promotes perseverative behavior following negative feedback.
    This model suggests that after a non-rewarded trial (receiving 0 coins), the participant
    develops a temporary bias to repeat their previous stage-1 choice. This captures
    anxiety-induced cognitive inflexibility. The strength of this perseverative bias is
    directly scaled by the participant's STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]                  (Learning rate)
    beta: [0, 10]                 (Inverse temperature for softmax)
    perseveration_strength: [0, 5] (Base magnitude of the perseverative bias)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.perseveration_strength = model_parameters

    def init_model(self) -> None:
        """Initialize last reward to a non-zero value to prevent perseveration on trial 0."""
        self.last_reward = 1.0

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 action probabilities. If the last trial was not rewarded,
        a bonus is added to the value of the previously chosen action.
        """
        q_effective = self.q_stage1.copy()
        
        # Check if the last trial resulted in no reward and it's not the first trial
        if self.trial > 0 and self.last_reward == 0.0:
            # Anxiety scales the strength of the perseverative bias
            perseveration_bonus = self.perseveration_strength * self.stai
            q_effective[self.last_action1] += perseveration_bonus
            
        return self.softmax(q_effective, self.beta)

    # This model uses the default value_update and post_trial methods from the base class,
    # as the core hypothesis is implemented entirely within the stage-1 policy.

cognitive_model3 = make_cognitive_model(ParticipantModel3)