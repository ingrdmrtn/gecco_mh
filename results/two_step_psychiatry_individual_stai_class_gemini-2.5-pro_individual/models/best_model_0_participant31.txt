class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety induces cognitive inflexibility, leading to a
    tendency to repeat previous actions (perseveration). This model adds a
    'stickiness' bonus to the Q-value of the most recently selected stage-1
    action. The magnitude of this bonus is determined by the participant's
    STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (learning rate)
    beta: [0, 10] (choice temperature)
    perseveration_anxiety: [0, 5] (anxiety's influence on choice stickiness)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.perseveration_anxiety = model_parameters

    def init_model(self) -> None:
        """
        Initialize the perseveration bonus based on anxiety.
        """
        self.perseveration_bonus = self.perseveration_anxiety * self.stai

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 choice probabilities after adding a perseveration
        bonus to the Q-value of the action taken on the previous trial.
        """
        q_effective = self.q_stage1.copy()

        # Add bonus to the last chosen action, if it's not the first trial
        if self.last_action1 is not None:
            q_effective[self.last_action1] += self.perseveration_bonus

        return self.softmax(q_effective, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        The learning rule is the standard TD update from the base class.
        The perseveration effect is purely on the choice policy.
        """
        # Use the standard value update from the base class
        super().value_update(action_1, state, action_2, reward)

cognitive_model3 = make_cognitive_model(ParticipantModel3)