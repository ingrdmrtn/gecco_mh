class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This model captures a perseverative or "sticky" choice strategy,
    where the value of the most recently chosen spaceship is temporarily boosted.
    The strength of this perseveration bonus is modulated by the participant's
    anxiety (stai) score, reflecting an anxiety-driven increase in habitual behavior
    and a reluctance to switch.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]     (Learning rate)
    beta:  [0, 10]    (Choice temperature)
    pi:    [0, 10]    (Perseveration strength)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """Computes stage-1 choice probabilities with an anxiety-scaled perseveration bonus."""
        q_effective = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            # Apply a bonus to the last chosen action, scaled by stai
            perseveration_bonus = self.pi * self.stai
            q_effective[self.last_action1] += perseveration_bonus
            
        return self.softmax(q_effective, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates Q-values using standard temporal difference learning.
        A model-free update is used, where the stage-1 value is updated towards
        the value of the actually experienced stage-2 state.
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)