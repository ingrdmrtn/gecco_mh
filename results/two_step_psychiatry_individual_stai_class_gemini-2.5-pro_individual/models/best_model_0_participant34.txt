class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety promotes choice perseveration ("stickiness"). A bonus value
    is added to the previously chosen stage-1 action, making it more likely to be
    selected again. The magnitude of this bonus is scaled by the participant's STAI score.
    This model uses the standard TD learning from the base class.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      (Learning rate)
    beta: [0, 10]     (Inverse temperature for softmax choice)
    kappa_base: [0, 5] (Base perseveration bonus)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.kappa_base = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """Computes stage-1 action probabilities with an anxiety-driven perseveration bias."""
        # Calculate the stickiness parameter, scaled by anxiety
        kappa = self.kappa_base * self.stai
        
        q_sticky = self.q_stage1.copy()
        
        # On all trials after the first, add the perseveration bonus
        # to the value of the action taken on the previous trial.
        if self.last_action1 is not None:
            q_sticky[self.last_action1] += kappa
            
        return self.softmax(q_sticky, self.beta)

    # This model uses the default value_update method from CognitiveModelBase,
    # as the hypothesis is about choice, not learning.

cognitive_model2 = make_cognitive_model(ParticipantModel2)