class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety impairs goal-directed (model-based) planning, 
    leading to a greater reliance on simpler, habitual (model-free) learning. 
    The stai score negatively modulates the weight (w) given to the model-based 
    value component, effectively making the agent more habitual under high anxiety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          (Learning rate)
    beta: [0, 10]          (Softmax inverse temperature)
    lambda_trace: [0, 1]   (Eligibility trace for model-free updates)
    w_anxiety_factor: [0, 1] (Scaling factor for anxiety's effect on model-based weight)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_trace, self.w_anxiety_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 choice probabilities based on a weighted average of
        model-based and model-free value estimates.
        """
        # 1. Calculate the model-based weight 'w'. Higher anxiety reduces 'w'.
        w = 1.0 - self.w_anxiety_factor * self.stai
        w = np.clip(w, 0, 1)

        # 2. Calculate model-based Q-values
        # Q_mb(a) = sum_s T(s|a) * max_a' Q_s2(s, a')
        q_stage2_max = np.max(self.q_stage2, axis=1)
        q_mb = self.T @ q_stage2_max

        # 3. Calculate model-free Q-values (already stored in self.q_stage1)
        q_mf = self.q_stage1

        # 4. Combine Q-values and compute policy
        q_hybrid = w * q_mb + (1 - w) * q_mf
        return self.softmax(q_hybrid, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates both model-free Q-values. The eligibility trace allows the
        second-stage prediction error to directly update the first-stage value.
        """
        # Standard TD update for stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2

        # Model-free update for stage 1 (SARSA-like)
        # This is where the eligibility trace (lambda) is used
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        self.q_stage1[action_1] += self.alpha * self.lambda_trace * delta_2

cognitive_model1 = make_cognitive_model(ParticipantModel1)