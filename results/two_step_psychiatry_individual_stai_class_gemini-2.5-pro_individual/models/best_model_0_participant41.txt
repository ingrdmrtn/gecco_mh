class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety promotes cognitive rigidity. This manifests in two ways: 1) a reduced learning rate, making the participant less flexible in updating beliefs, and 2) an increased tendency to perseverate, or repeat the previous action. Both of these effects are scaled by the participant's STAI score.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]    (Baseline learning rate)
    alpha_stai: [0, 1]    (Anxiety's effect on reducing the learning rate)
    beta: [0, 10]         (Softmax inverse temperature for choice consistency)
    persev_stai: [0, 5]   (Anxiety's effect on increasing the perseveration bonus)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.alpha_stai, self.beta, self.persev_stai = model_parameters
        
    def pre_trial(self) -> None:
        """Before each trial, calculate the effective learning rate based on anxiety."""
        # This effective alpha will be used by the base class's value_update method.
        # Higher STAI leads to a lower learning rate.
        self.alpha = np.clip(self.alpha_base - self.alpha_stai * self.stai, 0, 1)

    def policy_stage1(self) -> np.ndarray:
        """Computes stage-1 action probabilities including an anxiety-driven perseveration bonus."""
        # Create a temporary copy of Q-values to modify for this trial's choice.
        q_effective = np.copy(self.q_stage1)
        
        # Add a perseveration bonus to the value of the action chosen on the last trial.
        if self.last_action1 is not None:
            perseveration_bonus = self.persev_stai * self.stai
            q_effective[self.last_action1] += perseveration_bonus
            
        return self.softmax(q_effective, self.beta)

    # The base class value_update() method is sufficient, as it will automatically
    # use the `self.alpha` value that we set in the pre_trial() method.

cognitive_model3 = make_cognitive_model(ParticipantModel3)