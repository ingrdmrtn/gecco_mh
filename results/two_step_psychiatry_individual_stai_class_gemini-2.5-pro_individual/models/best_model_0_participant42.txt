class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety induces cognitive rigidity, reducing exploration and
    increasing the tendency to exploit the currently perceived best option. This is
    modeled by anxiety increasing the softmax inverse temperature (beta). A separate
    perseveration parameter captures a baseline choice stickiness.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (learning rate)
    beta: [0, 10]   (base inverse temperature)
    kappa: [0, 5]   (perseveration weight)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.kappa = model_parameters

    def init_model(self) -> None:
        """Calculate an effective beta, amplified by anxiety."""
        # Higher anxiety leads to more deterministic (exploitative) choices
        self.beta_effective = self.beta * (1 + self.stai)
        self.last_action1 = -1 # Initialize for the first trial

    def policy_stage1(self) -> np.ndarray:
        """Policy includes a perseveration bonus and anxiety-modulated stochasticity."""
        q_policy = self.q_stage1.copy()
        if self.last_action1 != -1:
            q_policy[self.last_action1] += self.kappa
        
        return self.softmax(q_policy, self.beta_effective)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Stage-2 policy also uses the anxiety-modulated beta."""
        return self.softmax(self.q_stage2[state], self.beta_effective)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Override to ensure last_action1 is updated for the perseveration mechanism."""
        super().post_trial(action_1, state, action_2, reward)
        # The base class already does this, but being explicit can help clarity.
        self.last_action1 = action_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)