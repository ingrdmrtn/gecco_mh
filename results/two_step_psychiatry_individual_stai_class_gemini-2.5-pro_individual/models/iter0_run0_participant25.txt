Here are three distinct cognitive models to explain the participant's decision-making process.

### Model 1: Anxiety-Amplified Perseveration

This model proposes that the participant has a general tendency to repeat their previous actions, a phenomenon known as perseveration. Crucially, this model hypothesizes that the participant's medium level of anxiety directly amplifies this bias. Anxiety can promote rigid, habitual behaviors as a way to cope with uncertainty. In this model, the value of repeating the last choice is boosted by an amount proportional to the participant's STAI score. This could explain the extreme inflexibility observed in their first-stage choices, where they never deviate from their initial selection.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This model captures an anxiety-amplified perseveration bias. 
    The participant has a tendency to repeat the previous action, and this
    tendency is strengthened by their level of trait anxiety (stai). A higher
    stai score leads to a stronger bias to stick with the last chosen spaceship,
    potentially overriding value-based learning and causing extreme inflexibility.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]     (Learning rate)
    beta: [0, 10]    (Inverse temperature for softmax)
    pi: [0, 10]      (Base perseveration strength)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Stage-1 policy includes a perseveration bonus modulated by anxiety.
        """
        if self.trial == 0 or self.last_action1 is None:
            return self.softmax(self.q_stage1, self.beta)
        
        # Create a one-hot vector for the last action
        perseveration_vector = np.zeros(self.n_choices)
        perseveration_vector[self.last_action1] = 1.0
        
        # Anxiety directly amplifies the strength of the perseveration bias
        anxiety_scaled_perseveration = self.pi * self.stai
        
        # Add the perseveration bonus to the Q-values
        effective_q_stage1 = self.q_stage1 + anxiety_scaled_perseveration * perseveration_vector
        
        return self.softmax(effective_q_stage1, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Enhanced Aversive Learning

This model suggests that anxiety alters the core learning process, specifically how the participant responds to negative outcomes. The hypothesis is that individuals with higher anxiety are more sensitive to punishments or non-rewards. This is implemented as an asymmetric learning rate: a standard rate for positive outcomes (receiving a coin) and an anxiety-enhanced rate for negative outcomes (not receiving a coin). This heightened learning from negative events could make the participant rapidly devalue options associated with failure. To account for the observed choice stickiness, the model also includes a simple perseveration parameter, but unlike Model 1, this bias is not directly modulated by anxiety; instead, anxiety's effect is confined to the learning mechanism.

```python
import numpy as np

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model proposes that anxiety enhances learning from negative
    outcomes. The participant uses a higher learning rate after not receiving a
    reward, and the magnitude of this increase is scaled by their stai score.
    This makes them particularly sensitive to punishment. The model also includes
    a static perseveration bias to account for choice repetition, but this bias
    is independent of anxiety.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]  (Learning rate for rewards)
    alpha_neg: [0, 1]  (Base learning rate for non-rewards)
    beta: [0, 10]     (Inverse temperature for softmax)
    pi: [0, 10]       (Perseveration strength)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg, self.beta, self.pi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Stage-1 policy includes a static perseveration bonus.
        """
        if self.trial == 0 or self.last_action1 is None:
            return self.softmax(self.q_stage1, self.beta)
            
        perseveration_vector = np.zeros(self.n_choices)
        perseveration_vector[self.last_action1] = 1.0
        effective_q_stage1 = self.q_stage1 + self.pi * perseveration_vector
        return self.softmax(effective_q_stage1, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Value update uses different learning rates for rewards and non-rewards,
        with the non-reward learning rate being modulated by anxiety.
        """
        # Anxiety enhances the learning rate for negative outcomes
        alpha_neg_effective = min(1.0, self.alpha_neg * (1 + self.stai))
        
        # Select the learning rate based on the outcome
        alpha = self.alpha_pos if reward > 0 else alpha_neg_effective
        
        # Standard TD learning updates
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Suppressed Directed Exploration

This model posits that the participant's behavior is driven by an aversion to uncertainty, which is exacerbated by anxiety. The model incorporates a "directed exploration" mechanism, where options that have been chosen less frequently receive an "uncertainty bonus," encouraging the agent to try them. The core hypothesis is that anxiety actively suppresses this exploration bonus. A higher STAI score reduces the drive to explore, making the participant stick to familiar options, even if they are not optimal. This could explain why the participant never explored the alternative spaceship, effectively treating the uncertainty of its outcome as more aversive than any potential reward.

```python
import numpy as np

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model suggests that anxiety suppresses directed exploration.
    Choices are driven by a combination of learned value and an exploration bonus
    for less-chosen options (similar to UCB algorithms). The participant's stai
    score acts to diminish this exploration bonus, making them less likely to try
    uncertain options. This leads to increased exploitation and a failure to explore
    the environment, explaining the highly perseverative choice pattern.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]    (Learning rate)
    beta: [0, 10]   (Inverse temperature for softmax)
    kappa: [0, 5]    (Base exploration weight)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.kappa = model_parameters

    def init_model(self) -> None:
        """Initialize a counter for stage-1 actions."""
        self.choice_counts_s1 = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        """
        Stage-1 policy includes an exploration bonus that is negatively
        modulated by the participant's anxiety level.
        """
        # Anxiety suppresses the exploration parameter kappa
        kappa_effective = self.kappa * (1.0 - self.stai)
        
        # Calculate an exploration bonus for each action
        # Bonus is higher for actions chosen less frequently
        # Add a small epsilon to avoid division by zero
        # Use trial + 2 in log to avoid log(1)=0 at trial 0
        numerator = np.log(self.trial + 2)
        denominator = self.choice_counts_s1 + 1e-7
        exploration_bonus = np.sqrt(numerator / denominator)
        
        # Combine Q-values with the anxiety-modulated exploration bonus
        effective_q_stage1 = self.q_stage1 + kappa_effective * exploration_bonus
        
        return self.softmax(effective_q_stage1, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update choice counts after the trial."""
        super().post_trial(action_1, state, action_2, reward)
        self.choice_counts_s1[action_1] += 1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```