Here are three distinct cognitive models to explain the participant's decision-making process.

### Model 1: Anxiety-Modulated Asymmetric Learning

This model proposes that the participant's medium anxiety level makes them more sensitive to negative outcomes than positive ones.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This model captures the idea that anxiety enhances learning from negative prediction errors.
    The participant has separate learning rates for positive and negative outcomes (prediction errors).
    The learning rate for negative outcomes (`alpha_neg`) is increased as a function of their STAI score,
    making them learn more quickly from punishments or worse-than-expected results. This could explain
    why they might quickly abandon an option after a negative reward (like the -1 on trial 6).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (base learning rate)
    beta: [0, 10]  (inverse temperature for choice stochasticity)
    eta: [0, 2]     (anxiety sensitivity parameter, scales the effect of STAI on negative learning)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.eta = model_parameters

    def init_model(self) -> None:
        """Initialize the anxiety-modulated negative learning rate."""
        # Higher STAI score leads to a higher learning rate for negative PEs
        # We clip at 1.0 as it's the conventional upper bound for a learning rate.
        self.alpha_neg = np.clip(self.alpha + self.eta * self.stai, 0, 1)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Implements value updating with separate learning rates for positive and
        negative prediction errors (PEs).
        """
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_neg if delta_2 < 0 else self.alpha
        self.q_stage2[state, action_2] += alpha_2 * delta_2

        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_neg if delta_1 < 0 else self.alpha
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Perseveration

This model tests the hypothesis that anxiety promotes habitual, repetitive behavior as a strategy to cope with uncertainty.

```python
import numpy as np

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model suggests that anxiety promotes choice perseveration,
    or a tendency to repeat the same action. This is a simple heuristic that can reduce
    cognitive load and the stress of making a new decision. The strength of this
    perseveration bias is directly proportional to the participant's STAI score.
    This could explain the participant's long streaks of choosing the same spaceship,
    particularly the extended period of choosing spaceship 1.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (learning rate)
    beta: [0, 10]  (inverse temperature for choice stochasticity)
    pi: [0, 5]     (perseveration weight, scales the bias to repeat actions)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 action probabilities with an added perseveration bias.
        """
        # Start with the learned Q-values
        q_stage1_biased = np.copy(self.q_stage1)

        # Add a bias to the value of the action that was chosen on the previous trial
        if self.trial > 0 and self.last_action1 is not None:
            # The bias strength is modulated by the perseveration parameter and STAI score
            bias = self.pi * self.stai
            q_stage1_biased[self.last_action1] += bias

        return self.softmax(q_stage1_biased, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Model-Based Control

This model explores the idea that anxiety impairs the use of a "model" of the task environment, forcing a greater reliance on simpler, model-free learning.

```python
import numpy as np

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model posits that anxiety impairs sophisticated, model-based planning.
    It implements a hybrid learning algorithm with both model-free and model-based components.
    The model-based component, represented by an eligibility trace (`lambda`), allows outcomes
    from the second stage to directly update the first-stage choice values. We hypothesize
    that anxiety reduces the weight of this model-based contribution (`lambda`), pushing the
    participant towards simpler, less cognitively demanding model-free learning. The weight `lambda`
    is therefore inversely proportional to the STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       (learning rate)
    beta: [0, 10]      (inverse temperature)
    lambda_base: [0, 1] (base eligibility trace, representing max model-based control)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_base = model_parameters

    def init_model(self) -> None:
        """Initialize the effective lambda based on STAI score."""
        # Effective lambda is the base value, reduced by the level of anxiety.
        # At STAI=1, lambda=0 (pure model-free). At STAI=0, lambda=lambda_base.
        self.lambda_eff = self.lambda_base * (1 - self.stai)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates values using a hybrid of model-free (TD) and model-based (eligibility trace)
        learning, where the model-based contribution is modulated by anxiety.
        """
        # Stage 2 PE is calculated first
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Update stage 2 Q-value
        self.q_stage2[state, action_2] += self.alpha * delta_2

        # Model-free update for stage 1 (same as base model)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        mf_update = self.alpha * delta_1
        
        # Model-based update for stage 1 (eligibility trace)
        mb_update = self.alpha * self.lambda_eff * delta_2
        
        # Combine updates
        self.q_stage1[action_1] += mf_update + mb_update

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```