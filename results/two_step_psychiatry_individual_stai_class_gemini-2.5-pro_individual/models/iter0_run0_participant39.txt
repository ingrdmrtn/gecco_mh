Here are three cognitive models to explain the participant's decision-making process.

### **Analysis of Participant Behavior**

This participant has a high anxiety score (STAI = 0.6625). Their behavioral data is marked by profound **perseveration**: they chose spaceship 0 for the first 34 trials almost exclusively, despite mixed outcomes, before switching to spaceship 1 and perseverating on that choice for a period. This behavioral rigidity and reduced exploration are characteristic patterns often associated with high anxiety. The proposed models explore different cognitive mechanisms through which anxiety might produce this behavior.

---

### **Model 1: Anxiety-Driven Perseveration**

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety fosters behavioral rigidity by creating a "stickiness"
    to recent choices. This model proposes that the value of the most recently
    chosen spaceship is artificially inflated, making the participant more likely
    to choose it again on the next trial. The strength of this perseverative
    bias is directly proportional to the participant's anxiety level (stai),
    capturing the idea that anxiety promotes habitual, repetitive responding.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        (Learning rate)
    beta: [0, 10]       (Inverse temperature for choice stochasticity)
    perseveration: [0, 5] (Strength of the perseverative bias)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.perseveration = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 choice probabilities with an anxiety-modulated
        perseveration bonus.
        """
        q_eff = self.q_stage1.copy()
        
        # Add a bonus to the previously chosen action
        if self.last_action1 is not None:
            # The perseveration bonus is scaled by the participant's anxiety
            bonus = self.perseveration * self.stai
            q_eff[self.last_action1] += bonus
            
        return self.softmax(q_eff, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

---

### **Model 2: Anxiety-Biased Hybrid Control**

```python
import numpy as np

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model assumes decision-making arises from a mix of
    goal-directed ("model-based") and habitual ("model-free") control systems.
    High anxiety is hypothesized to bias this balance away from flexible,
    goal-directed planning and towards more rigid, habitual responding.
    The model implements this by having the stai score reduce the weight
    given to the model-based system, effectively making the participant's
    behavior more habitual and less sensitive to the task's transition structure.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        (Learning rate for value updates)
    beta: [0, 10]       (Inverse temperature for choice stochasticity)
    w_base: [0, 1]       (Base weighting of model-based vs. model-free control)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 choice probabilities based on a hybrid of model-based
        and model-free values, where anxiety shifts the balance.
        """
        # 1. Model-based values: Q_MB(a) = sum_s T(s|a) * max_a' Q(s,a')
        max_q_s2 = np.max(self.q_stage2, axis=1)
        q_mb = self.T @ max_q_s2
        
        # 2. Model-free values (simple action values)
        q_mf = self.q_stage1
        
        # 3. Anxiety modulates the weighting between systems
        # Higher stai score reduces model-based contribution
        w = self.w_base * (1 - self.stai)
        
        # 4. Combine values and compute policy
        q_hybrid = w * q_mb + (1 - w) * q_mf
        return self.softmax(q_hybrid, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates stage-2 values and the model-free (q_stage1) values.
        """
        # Standard TD update for stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update the model-free value of the stage-1 action based on the
        # best possible outcome from the resulting state.
        delta_1_mf = np.max(self.q_stage2[state]) - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1_mf

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

---

### **Model 3: Anxiety-Modulated Exploration**

```python
import numpy as np

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety reduces cognitive exploration, leading to more
    deterministic and less flexible choice behavior. This model proposes that
    the participant's anxiety level directly increases their choice certainty
    (exploitation), making them less likely to explore alternative options.
    This is implemented by having the stai score increase the softmax inverse
    temperature (beta), which makes the choice policy more strongly favor the
    option with the highest current value estimate. This can explain long
    streaks of choosing the same option.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        (Learning rate)
    beta_base: [0, 10]   (Baseline inverse temperature)
    beta_stai: [0, 10]   (Anxiety's influence on inverse temperature)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.beta_stai = model_parameters
        # We will compute an effective beta on each trial
        self.effective_beta = 0 

    def pre_trial(self) -> None:
        """
        Before each trial, calculate an "effective beta" based on the
        participant's anxiety score.
        """
        self.effective_beta = self.beta_base + self.beta_stai * self.stai

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 policy using the anxiety-modulated beta.
        """
        return self.softmax(self.q_stage1, self.effective_beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """
        Computes stage-2 policy using the anxiety-modulated beta.
        """
        return self.softmax(self.q_stage2[state], self.effective_beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```