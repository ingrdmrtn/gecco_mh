Here are three new cognitive models to explain the participant's decision-making process.

### Model 1: Anxiety-Modulated Hybrid Control Model

This model tests the hypothesis that anxiety impairs goal-directed, model-based planning. It proposes that the participant uses a hybrid of two systems: a simple "model-free" system that learns the value of spaceships directly from rewards, and a "model-based" system that uses knowledge of the task structure (i.e., which spaceship leads to which planet) to plan. High anxiety (`stai`) is hypothesized to reduce the influence of the sophisticated model-based system, pushing the participant towards more rigid, model-free behavior. This could explain the initial long sequence of choosing the same spaceship, as model-free learning is slower to adapt.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety impairs deliberative, model-based planning, causing
    greater reliance on simpler, habitual model-free learning. This model assumes
    the participant's choices result from a weighted average of a model-based (MB)
    and a model-free (MF) valuation system. The weight given to the MB system is
    inversely proportional to the participant's anxiety (stai), such that higher
    anxiety leads to more model-free, and potentially less optimal, behavior.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       (Learning rate for both systems)
    beta: [0, 10]      (Choice stochasticity)
    w_base: [0, 1]     (Base weight for model-based control)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        """Initialize separate model-free Q-values for stage 1."""
        # self.q_stage1 from base class will represent the model-free values
        self.q_stage1 = np.zeros(self.n_choices) 
        # self.q_stage2 is shared by both systems
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 choice probabilities based on a hybrid of model-based
        and model-free values, with the weighting modulated by anxiety.
        """
        # 1. Model-based valuation: Expected value of a spaceship, considering
        # transition probabilities (self.T) and the best outcome on each planet.
        q_mb_stage1 = np.zeros(self.n_choices)
        max_q_stage2 = np.max(self.q_stage2, axis=1)
        for a in range(self.n_choices):
            q_mb_stage1[a] = np.sum(self.T[a, :] * max_q_stage2)

        # 2. Anxiety-modulated weight for the model-based system
        # Higher stai score reduces the weight
        w = self.w_base * (1 - self.stai)
        w = np.clip(w, 0, 1)

        # 3. Hybrid Q-value is a weighted average
        # self.q_stage1 holds the model-free values
        q_hybrid = w * q_mb_stage1 + (1 - w) * self.q_stage1
        
        return self.softmax(q_hybrid, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates both the shared stage-2 values and the model-free stage-1 values.
        """
        # Standard TD update for stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Standard TD update for the model-free stage 1 system
        # The "reward" for the stage-1 action is the value of the resulting state-action pair
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Asymmetric Learning Rate Model

This model explores the idea that anxiety biases the learning process itself. Specifically, it hypothesizes that individuals with high anxiety are more sensitive to negative outcomes (punishments or lack of reward) than to positive ones. The model implements two distinct learning rates: one for positive prediction errors (`alpha_pos`) and another for negative prediction errors (`alpha_neg`). The learning rate for negative outcomes is amplified by the participant's anxiety level (`stai`), causing them to learn more quickly from "bad news." This could explain why a series of non-rewarding trials might eventually trigger a dramatic shift in strategy.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to a negativity bias in learning, causing the
    participant to learn more from outcomes that are worse than expected. This model
    uses separate learning rates for positive and negative prediction errors. The
    learning rate for negative errors is directly increased by the participant's
    anxiety score (stai), reflecting heightened sensitivity to aversive events.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]   (Learning rate for better-than-expected outcomes)
    beta: [0, 10]      (Choice stochasticity)
    sensitivity: [0, 2] (Parameter scaling the effect of anxiety on negative learning)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.beta, self.sensitivity = model_parameters

    def init_model(self) -> None:
        """Calculate the anxiety-modulated negative learning rate."""
        # The negative learning rate is the base positive rate plus an anxiety-scaled bonus
        self.alpha_neg = self.alpha_pos + self.sensitivity * self.stai
        self.alpha_neg = np.clip(self.alpha_neg, 0, 1)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Overrides the default value update to use asymmetric learning rates for
        both stages of the task.
        """
        # Stage 2 update with asymmetric learning
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += self.alpha_neg * delta_2
            
        # Stage 1 update with asymmetric learning
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += self.alpha_neg * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced Uncertainty Aversion Model

This model proposes that anxiety alters how the participant represents the value of their options, particularly under uncertainty. The core idea is that anxious individuals are averse to ambiguity and will pessimistically devalue choices whose outcomes are more volatile or unpredictable. The model tracks the volatility of each option's value (based on recent prediction errors). When making a choice, the learned values are "dampened" or reduced as a function of their volatility. The strength of this dampening effect is amplified by the participant's anxiety (`stai`), leading highly anxious individuals to strongly avoid options they perceive as uncertain.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety fosters an aversion to uncertainty, leading to a
    pessimistic devaluation of options with volatile outcomes. This model tracks
    the volatility of each option's value. The effective value used for decision-making
    is dampened by this volatility, and the strength of this dampening effect is
    magnified by the participant's anxiety (stai).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       (Learning rate for Q-values)
    beta: [0, 10]      (Choice stochasticity)
    dampening: [0, 2]   (Strength of the uncertainty dampening effect)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.dampening = model_parameters

    def init_model(self) -> None:
        """Initialize volatility trackers for all Q-values."""
        self.volatility_s1 = np.ones(self.n_choices) * 0.1
        self.volatility_s2 = np.ones((self.n_states, self.n_choices)) * 0.1
        self.eta = 0.2  # Fixed learning rate for volatility updates

    def get_dampened_q_values(self, q_values, volatility):
        """Applies anxiety- and volatility-based dampening to Q-values."""
        damp_factor = 1.0 - self.dampening * volatility * self.stai
        damp_factor = np.clip(damp_factor, 0, 1)
        return q_values * damp_factor

    def policy_stage1(self) -> np.ndarray:
        """Computes stage-1 policy using dampened Q-values."""
        q_eff = self.get_dampened_q_values(self.q_stage1, self.volatility_s1)
        return self.softmax(q_eff, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Computes stage-2 policy using dampened Q-values."""
        q_eff = self.get_dampened_q_values(self.q_stage2[state], self.volatility_s2[state])
        return self.softmax(q_eff, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Updates Q-values and then updates the volatility trackers."""
        # 1. Calculate prediction errors *before* updating values
        delta_2 = reward - self.q_stage2[state, action_2]
        # Use the value of the action taken at stage 2 for the stage 1 TD error
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]

        # 2. Update Q-values using standard TD learning
        self.q_stage2[state, action_2] += self.alpha * delta_2
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # 3. Update volatility trackers based on the magnitude of prediction errors
        self.volatility_s2[state, action_2] = (1 - self.eta) * self.volatility_s2[state, action_2] + self.eta * abs(delta_2)
        self.volatility_s1[action_1] = (1 - self.eta) * self.volatility_s1[action_1] + self.eta * abs(delta_1)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```