Here are three new cognitive models to explain the participant's decision-making process.

### Model 1: Anxiety-Suppressed Learning Rate

This model proposes that high anxiety leads to cognitive inflexibility by suppressing the rate at which new information is integrated. The participant becomes "stuck" in their ways because they are slow to update their value estimates based on recent outcomes.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety induces cognitive inflexibility by dampening the learning
    rate. This model posits that an anxious individual is slower to update their
    internal values in response to feedback. This leads to perseverative behavior,
    as the values of established choices are resistant to change, even in the face
    of suboptimal outcomes. The degree of this learning suppression is directly
    proportional to the participant's STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        (Base learning rate)
    beta: [0, 10]       (Inverse temperature for choice stochasticity)
    stai_effect: [0, 1]  (Factor by which anxiety suppresses learning)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stai_effect = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates Q-values using a learning rate that is negatively modulated by anxiety.
        """
        # Anxiety suppresses the learning rate
        effective_alpha = self.alpha * (1 - self.stai_effect * self.stai)
        
        # Standard TD learning with the anxiety-modulated learning rate
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += effective_alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += effective_alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Aversive State Learning

This model suggests that anxious individuals are not only learning the value of their actions but are also particularly sensitive to the affective value of the states they encounter. A negative outcome on a planet makes the planet itself aversive. First-stage choices are then biased to avoid planets that have become associated with negative feelings, an effect that is magnified by anxiety.

```python
import numpy as np

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety promotes learning about the aversive quality of
    environmental states, not just actions. This model learns independent values
    for the planets themselves. A negative outcome makes a planet aversive.
    First-stage choices are then biased to avoid planets associated with negative
    outcomes, reflecting a core feature of anxiety. The strength of this state-avoidance
    bias is scaled by the participant's STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          (Learning rate for action and state values)
    beta: [0, 10]         (Inverse temperature for choice stochasticity)
    planet_weight: [0, 5] (Weight of planet values on choice, modulated by anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.planet_weight = model_parameters

    def init_model(self) -> None:
        """Initialize values for the planets."""
        self.q_planets = np.zeros(self.n_states)

    def policy_stage1(self) -> np.ndarray:
        """
        Stage-1 policy is influenced by both standard action values and the
        anxiety-weighted learned values of the destination planets.
        """
        # Calculate the expected value of reaching each planet
        expected_planet_values = self.T @ self.q_planets
        
        # Anxiety scales the influence of these planet values
        anxiety_driven_bias = self.planet_weight * self.stai * expected_planet_values
        
        # Add this bias to the standard action-values
        q_effective = self.q_stage1 + anxiety_driven_bias
        
        return self.softmax(q_effective, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates standard action values and the values of the planets.
        """
        # Standard TD update for actions
        super().value_update(action_1, state, action_2, reward)
        
        # Update the value of the planet that was visited
        delta_planet = reward - self.q_planets[state]
        self.q_planets[state] += self.alpha * delta_planet

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Enhanced Uncertainty Aversion

This model formalizes the idea that anxiety leads to a preference for certainty. The participant tracks not only the expected reward from each alien but also the variability (uncertainty) of those rewards. The value of a first-stage choice is penalized by the expected uncertainty of its outcome. This penalty is stronger for individuals with higher anxiety, causing them to prefer "safe," predictable options, even if they are less rewarding.

```python
import numpy as np

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety enhances aversion to outcome uncertainty. This model assumes
    the participant tracks both the mean reward and the reward variability for each
    second-stage option (alien). The subjective value of a first-stage choice
    is its learned value, penalized by the expected uncertainty of the planets it
    can lead to. This uncertainty penalty is scaled by the participant's STAI score,
    making anxious individuals favor predictable options.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]                (Learning rate)
    beta: [0, 10]               (Inverse temperature for choice stochasticity)
    uncertainty_penalty: [0, 5]  (Base weight of the uncertainty penalty)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.uncertainty_penalty = model_parameters

    def init_model(self) -> None:
        """Initialize trackers for reward variance."""
        self.var_stage2 = np.zeros_like(self.q_stage2)

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 choice probabilities where values are penalized by
        anxiety-modulated expected outcome uncertainty.
        """
        # Use variance to estimate standard deviation (a measure of uncertainty)
        std_dev_stage2 = np.sqrt(np.maximum(0, self.var_stage2))
        
        # Calculate the average uncertainty associated with each planet
        # We take the mean uncertainty of the two aliens on each planet
        avg_uncertainty_per_planet = np.mean(std_dev_stage2, axis=1)

        # Calculate the expected uncertainty for each stage-1 choice
        expected_uncertainty = self.T @ avg_uncertainty_per_planet
        
        # Anxiety scales the penalty applied for this uncertainty
        anxiety_driven_penalty = self.uncertainty_penalty * self.stai * expected_uncertainty
        
        q_effective = self.q_stage1 - anxiety_driven_penalty
        return self.softmax(q_effective, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """
        Updates both the mean value (q_stage2) and the variance (var_stage2).
        """
        # Update mean value (q_stage2) and model-free value (q_stage1)
        # We call the parent method but need the prediction error from it
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Update variance estimate using the squared prediction error
        # This is a common online method for tracking variance
        delta_var = delta_2**2 - self.var_stage2[state, action_2]
        self.var_stage2[state, action_2] += self.alpha * delta_var

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```