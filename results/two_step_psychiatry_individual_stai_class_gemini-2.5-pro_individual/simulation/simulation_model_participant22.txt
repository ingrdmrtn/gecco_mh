As a cognitive scientist and Python programmer, my goal is to create a robust simulation function. This function will act as a virtual participant, generating behavioral data based on the precise mechanisms defined in a given cognitive model.

The key is to design a general-purpose simulator that can take *any* model conforming to the `CognitiveModelBase` structure and produce a trial-by-trial trajectory of choices and outcomes. The simulator will orchestrate the task flow—presenting choices, transitioning between states, and delivering rewards—while the model instance itself will be responsible for the "cognitive" parts: computing choice probabilities and updating internal value representations.

This separation of concerns makes the simulator powerful and flexible. It can test any hypothesis encoded in a `CognitiveModelBase` subclass without modification.

Here is the simulation code, carefully constructed to meet these requirements.

```python
import numpy as np
from abc import ABC, abstractmethod

# The base class and specific model are included here for context and to make the code runnable.
# In a real-world scenario, these would likely be in separate modules.

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition probabilities
        self.trans_counts = np.array([[35, 15], [15, 35]]) # Common (70%) vs rare (30%) transitions
        self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True)
        
        # Choice probability sequences (for potential logging/fitting)
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety influences the participant's choice policy directly. 
    It creates a conflict between two tendencies: 
    1. Perseveration: An increased tendency to repeat the previous choice, 
       modeled by a 'stickiness' parameter 'p' that is amplified by STAI.
    2. Randomness: A decrease in choice consistency (lower beta), making choices 
       more exploratory or erratic, also modulated by STAI.
    This model captures how anxiety might simultaneously promote rigid habits 
    and uncertain exploration.
    """
    
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.p = model_parameters

    def pre_trial(self) -> None:
        """
        Before each trial, set up the perseveration bonus for the last action.
        """
        self.perseveration_bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            self.perseveration_bonus[self.last_action1] = 1.0

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 policy using Q-values augmented by an anxiety-driven
        perseveration bonus, and with an anxiety-reduced choice consistency.
        """
        # Anxiety increases perseveration strength
        effective_p = self.p * self.stai
        
        # Anxiety decreases choice consistency (inverse temperature)
        effective_beta = self.beta * (1 - self.stai)
        
        # Add perseveration bonus to the learned Q-values
        q_values_with_perseveration = self.q_stage1 + effective_p * self.perseveration_bonus
        
        return self.softmax(q_values_with_perseveration, max(0, effective_beta))


def simulate_model(
    ModelClass,
    n_trials: int,
    stai: float,
    model_parameters: tuple,
    drift1,
    drift2,
    drift3,
    drift4,
    seed: int | None = None,
):
    """
    Simulates a two-step task trajectory from a cognitive model class.

    The function's logic is general and relies on the methods defined in the
    provided ModelClass (e.g., ParticipantModel3) to implement the specific
    cognitive strategy.

    Task structure:
      - Stage 1: choose action_1 in {0,1}
      - Transition: state in {0,1} sampled from model.T[action_1]
      - Stage 2: choose action_2 in {0,1} conditioned on state
      - Reward: sampled from drifting reward probabilities:
          state 0: [drift1[t], drift2[t]] for actions 0/1
          state 1: [drift3[t], drift4[t]] for actions 0/1

    Parameters
    ----------
    ModelClass : type
        A subclass of CognitiveModelBase (e.g., ParticipantModel3).
    n_trials : int
        Number of trials to simulate.
    stai : float
        Participant anxiety score, normalized between 0 and 1.
    model_parameters : tuple
        Parameters expected by ModelClass.unpack_parameters(...).
    drift1, drift2, drift3, drift4 : array-like (length n_trials)
        Trial-wise reward probabilities for the 4 (state, action) pairs.
    seed : int or None
        RNG seed for reproducibility.

    Returns
    -------
    stage1_choice : np.ndarray, shape (n_trials,)
        The sequence of first-stage actions taken.
    state2 : np.ndarray, shape (n_trials,)
        The sequence of second-stage states reached.
    stage2_choice : np.ndarray, shape (n_trials,)
        The sequence of second-stage actions taken.
    reward : np.ndarray, shape (n_trials,)
        The sequence of rewards received.
    """
    rng = np.random.default_rng(seed)

    # Instantiate the specific cognitive model. This object will maintain its own
    # internal state (like Q-values) and provide the logic for decisions and learning.
    model = ModelClass(n_trials=n_trials, stai=float(stai), model_parameters=model_parameters)

    # Initialize arrays to store the simulated data trajectory
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Set the model's internal trial counter
        model.trial = t
        
        # Execute any computations the model needs before a choice is made
        # (e.g., ParticipantModel3 sets up its perseveration bonus here).
        model.pre_trial()

        # --- Stage 1 choice ---
        p1 = model.policy_stage1()
        # Safety normalization to handle potential floating point inaccuracies
        p1 = np.clip(p1, 1e-12, 1.0)
        p1 = p1 / np.sum(p1)
        a1 = int(rng.choice([0, 1], p=p1))

        # --- Transition to Stage 2 ---
        pT = model.T[a1]
        pT = np.clip(pT, 1e-12, 1.0)
        pT = pT / np.sum(pT)
        s = int(rng.choice([0, 1], p=pT))

        # --- Stage 2 choice ---
        p2 = model.policy_stage2(s)
        p2 = np.clip(p2, 1e-12, 1.0)
        p2 = p2 / np.sum(p2)
        a2 = int(rng.choice([0, 1], p=p2))

        # --- Reward Generation ---
        # The environment determines the reward based on the drifting probabilities.
        if s == 0:
            pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
        else:  # s == 1
            pr = float(drift3[t]) if a2 == 0 else float(drift4[t])
        r = int(rng.random() < pr)

        # --- Model Update ---
        # The model updates its internal values based on the trial's events.
        model.value_update(a1, s, a2, float(r))
        
        # The model performs any end-of-trial bookkeeping.
        model.post_trial(a1, s, a2, float(r))

        # Store the results for this trial
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```