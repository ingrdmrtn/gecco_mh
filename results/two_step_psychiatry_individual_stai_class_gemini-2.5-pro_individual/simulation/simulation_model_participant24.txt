An excellent task. As a cognitive scientist and programmer, I understand the crucial distinction between model *fitting* (evaluating a model against existing data) and model *simulation* (generating new data from a model). The latter is essential for model validation, parameter recovery studies, and generating predictions.

The provided `ParticipantModel3` hypothesizes that anxiety (`stai`) enhances a simple perseveration heuristic. The simulation must faithfully execute this mechanism trial-by-trial. This involves:
1.  Calculating choice probabilities at each stage, incorporating the anxiety-scaled "stickiness" bonus.
2.  Sampling an action from these probabilities.
3.  Sampling a state transition and a reward from the environment's dynamics.
4.  Feeding the outcome back into the model's learning rule.
5.  Repeating this process, allowing the model's internal state (Q-values) and its history of choices to evolve over time.

Below is the simulation function, crafted to work seamlessly with the `CognitiveModelBase` structure and the specific logic of `ParticipantModel3`.

```python
import numpy as np
from abc import ABC, abstractmethod

# The CognitiveModelBase and ParticipantModel3 classes are included here for context
# and to make the final code block self-contained and runnable.

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition probabilities
        self.trans_counts = np.array([[35, 15], [15, 35]]) # Prior counts for transitions, if needed for learning
        self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True) # Transition probabilities, if needed for direct use
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model posits that anxiety promotes a simple heuristic: 
    repeating the previous action. This "perseveration bias" provides a sense of 
    predictability and avoids the uncertainty of switching. The model adds a 
    "stickiness" bonus to the value of the action chosen on the previous trial. 
    The magnitude of this bonus is directly proportional to the participant's STAI 
    score. This mechanism can account for the long streaks of repeating the same 
    choice seen in the data, framing it as an anxiety-driven strategy to cope 
    with task uncertainty.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.kappa = model_parameters

    def policy_stage1(self) -> np.ndarray:
        """
        Computes stage-1 probabilities by adding an anxiety-scaled perseveration
        bonus to the value of the action chosen on the last trial.
        """
        q_effective = self.q_stage1.copy()
        
        if self.trial > 0 and self.last_action1 is not None:
            perseveration_bonus = self.kappa * self.stai
            q_effective[self.last_action1] += perseveration_bonus
            
        return self.softmax(q_effective, self.beta)
        
    def policy_stage2(self, state: int) -> np.ndarray:
        """
        Computes stage-2 probabilities by adding an anxiety-scaled perseveration
        bonus for the second-stage choice as well.
        """
        q_effective = self.q_stage2[state].copy()

        if self.trial > 0 and self.last_action2 is not None and self.last_state == state:
            perseveration_bonus = self.kappa * self.stai
            q_effective[self.last_action2] += perseveration_bonus
            
        return self.softmax(q_effective, self.beta)

# --- SIMULATION FUNCTION ---

def simulate_model(
    ModelClass: type,
    n_trials: int,
    stai: float,
    model_parameters: tuple,
    drift1: np.ndarray,
    drift2: np.ndarray,
    drift3: np.ndarray,
    drift4: np.ndarray,
    seed: int | None = None,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Simulates a two-step task trajectory from a cognitive model class.

    This function instantiates a model and simulates its choices and learning
    over a series of trials within a dynamic reward environment.

    Parameters
    ----------
    ModelClass : type
        A subclass of CognitiveModelBase (e.g., ParticipantModel3).
    n_trials : int
        Number of trials to simulate.
    stai : float
        Participant anxiety score, passed to the model constructor.
    model_parameters : tuple
        Parameters for the model (e.g., (alpha, beta, kappa)).
    drift1, drift2, drift3, drift4 : np.ndarray
        Trial-wise reward probabilities for the 4 (state, action) pairs.
    seed : int or None, optional
        Seed for the random number generator for reproducibility.

    Returns
    -------
    stage1_choice : np.ndarray
        The sequence of first-stage actions (0 or 1).
    state2 : np.ndarray
        The sequence of second-stage states (0 or 1).
    stage2_choice : np.ndarray
        The sequence of second-stage actions (0 or 1).
    reward : np.ndarray
        The sequence of rewards received (0 or 1).
    """
    rng = np.random.default_rng(seed)

    # 1. Instantiate the cognitive model
    model = ModelClass(n_trials=n_trials, stai=float(stai), model_parameters=model_parameters)

    # 2. Initialize arrays to store the simulated trajectory
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    # 3. Loop through trials to generate behavior
    for t in range(n_trials):
        model.trial = t
        model.pre_trial()

        # --- Stage 1: Agent makes a choice ---
        p1 = model.policy_stage1()
        a1 = int(rng.choice([0, 1], p=p1))

        # --- Transition: Environment determines the next state ---
        pT = model.T[a1]
        s = int(rng.choice([0, 1], p=pT))

        # --- Stage 2: Agent makes a choice in the new state ---
        p2 = model.policy_stage2(s)
        a2 = int(rng.choice([0, 1], p=p2))

        # --- Outcome: Environment delivers a reward ---
        if s == 0:
            pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
        else: # s == 1
            pr = float(drift3[t]) if a2 == 0 else float(drift4[t])
        r = int(rng.random() < pr)

        # --- Learning: Agent updates its internal values ---
        model.value_update(a1, s, a2, float(r))
        model.post_trial(a1, s, a2, float(r))

        # --- Record trial data ---
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```