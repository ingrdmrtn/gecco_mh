Of course. As a cognitive scientist and Python programmer, I'd be happy to provide the simulation code.

The provided model fitting code is structured excellently, with a base class that defines the core trial loop and abstract methods for the cognitive logic. This makes it straightforward to create a general-purpose simulation function that can work with any model adhering to this structure, including `ParticipantModel2`.

The simulation function will instantiate the provided model class and then, for each trial, "ask" the model what it would do. It will then generate an outcome from the environment (transition, reward) and "tell" the model what happened, allowing it to update its internal values for the next trial.

Here is the simulation code.

```python
import numpy as np

def simulate_model(
    ModelClass,
    n_trials: int,
    stai: float,
    model_parameters: tuple,
    drift1,
    drift2,
    drift3,
    drift4,
    seed: int | None = None,
):
    """
    Simulate a two-step task trajectory from a cognitive model class.

    This function takes a model class that inherits from CognitiveModelBase
    and simulates the behavior of an agent following that model's logic
    within a two-step task environment with drifting reward probabilities.

    Task structure:
      - Stage 1: Choose action_1 in {0, 1}.
      - Transition: A second-stage state in {0, 1} is sampled based on
        the model's transition matrix T given action_1.
      - Stage 2: Choose action_2 in {0, 1} conditioned on the current state.
      - Reward: A reward is sampled from drifting reward probabilities:
          - State 0: [drift1[t], drift2[t]] for actions {0, 1}
          - State 1: [drift3[t], drift4[t]] for actions {0, 1}

    Parameters
    ----------
    ModelClass : type
        A subclass of CognitiveModelBase (e.g., ParticipantModel2).
    n_trials : int
        The number of trials to simulate.
    stai : float
        The participant's anxiety score, passed to the model constructor.
    model_parameters : tuple
        A tuple of parameters expected by the model's unpack_parameters method.
    drift1, drift2, drift3, drift4 : array-like
        Arrays of length n_trials specifying the reward probabilities for the
        four (state, action) pairs at the second stage.
    seed : int or None, optional
        A seed for the random number generator to ensure reproducibility.

    Returns
    -------
    stage1_choice : np.ndarray
        Array of shape (n_trials,) with the agent's first-stage choices.
    state2 : np.ndarray
        Array of shape (n_trials,) with the second-stage states encountered.
    stage2_choice : np.ndarray
        Array of shape (n_trials,) with the agent's second-stage choices.
    reward : np.ndarray
        Array of shape (n_trials,) with the rewards received.
    """
    rng = np.random.default_rng(seed)

    # Instantiate the cognitive model with its parameters
    model = ModelClass(
        n_trials=n_trials, stai=float(stai), model_parameters=model_parameters
    )

    # Initialize arrays to store the simulation trajectory
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Set the model's internal trial counter
        model.trial = t
        
        # --- Pre-trial computations (if any) ---
        model.pre_trial()

        # --- Stage 1: Agent makes a choice ---
        p_stage1 = model.policy_stage1()
        # Safety normalization for sampling
        p_stage1 = np.clip(p_stage1, 1e-12, 1.0)
        p_stage1 /= np.sum(p_stage1)
        a1 = int(rng.choice([0, 1], p=p_stage1))

        # --- Transition: Environment determines the next state ---
        p_transition = model.T[a1]
        p_transition = np.clip(p_transition, 1e-12, 1.0)
        p_transition /= np.sum(p_transition)
        s = int(rng.choice([0, 1], p=p_transition))

        # --- Stage 2: Agent makes a choice in the new state ---
        p_stage2 = model.policy_stage2(s)
        p_stage2 = np.clip(p_stage2, 1e-12, 1.0)
        p_stage2 /= np.sum(p_stage2)
        a2 = int(rng.choice([0, 1], p=p_stage2))

        # --- Reward: Environment provides an outcome ---
        if s == 0:
            reward_prob = float(drift1[t]) if a2 == 0 else float(drift2[t])
        else: # s == 1
            reward_prob = float(drift3[t]) if a2 == 0 else float(drift4[t])
        
        r = int(rng.random() < reward_prob)

        # --- Learning: Agent updates its internal values ---
        model.value_update(a1, s, a2, float(r))
        
        # --- Post-trial bookkeeping ---
        model.post_trial(a1, s, a2, float(r))

        # Store the results for this trial
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```