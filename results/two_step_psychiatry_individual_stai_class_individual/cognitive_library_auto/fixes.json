{
  "p18": {
    "diagnosis": "The 'stai_modulation' field in the spec is set to a string 'inverse_division', which fails to specify which parameter should be modulated. The original model specifically modulates the 'phi' (stickiness) parameter using this function. The spec should use a dictionary to map 'phi' to 'inverse_division'.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_inverse_division",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "phi"
      ],
      "stai_modulation": {
        "phi": "inverse_division"
      },
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "phi": [
          0,
          5
        ]
      },
      "bic": 305.38341476794284
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p19": {
    "diagnosis": "The original model enforces context-dependent perseveration in Stage 2 (stickiness is only applied if the current state matches the previous trial's Stage 2 state), whereas the reconstructed spec uses 'add_perseveration_bonus' which applies stickiness unconditionally based on the last action index. This fails to capture the specific 'safety behavior' hypothesis where repetition is tied to the specific state context.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel1",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_additive",
        "policy::add_context_dependent_perseveration",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stick_base",
        "stick_stai_slope"
      ],
      "stai_modulation": "additive",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stick_base": [
          0,
          5
        ],
        "stick_stai_slope": [
          -5,
          5
        ]
      }
    },
    "primitives_fix": "def add_context_dependent_perseveration(\n    q_values: np.ndarray,\n    last_action: Optional[int],\n    bonus: float,\n    current_state: Optional[int] = None,\n    last_state: Optional[int] = None\n) -> np.ndarray:\n    \"\"\"\n    Adds stickiness bonus to last_action. \n    If state context is provided (e.g. Stage 2), only applies if current_state == last_state.\n    If state context is None (e.g. Stage 1), applies unconditionally.\n    \"\"\"\n    q_new = q_values.copy()\n    should_apply = False\n    if last_action is not None:\n        if current_state is not None and last_state is not None:\n            if current_state == last_state:\n                should_apply = True\n        else:\n            should_apply = True\n            \n    if should_apply:\n        q_new[int(last_action)] += bonus\n    return q_new",
    "reconstructor_fix": "Ensure that when generating 'policy_stage2', the reconstructor detects 'add_context_dependent_perseveration' and passes 'current_state=state' and 'last_state=self.last_state' as arguments."
  },
  "p20": {
    "diagnosis": "The original model applies stickiness in Stage 2 conditionally: the bonus is added only if the current state matches the previous trial's state (`if self.last_state == state`). The reconstructed model uses the generic `add_perseveration_bonus` primitive which applies stickiness unconditionally to the last action regardless of state context.",
    "fix_type": "primitives",
    "fixed_spec": {
      "class": "ParticipantModel3",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_additive",
        "policy::add_state_dependent_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stickiness_base",
        "anxiety_stick"
      ],
      "stai_modulation": "additive",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stickiness_base": [
          0,
          5
        ],
        "anxiety_stick": [
          0,
          5
        ]
      },
      "bic": 381.9787884185716
    },
    "primitives_fix": "def add_state_dependent_perseveration_bonus(\n    q_values: np.ndarray, \n    last_action: Optional[int], \n    bonus: float,\n    last_state: Optional[int] = None,\n    current_state: Optional[int] = None\n) -> np.ndarray:\n    \"\"\"\n    Adds stickiness bonus to last_action. \n    If state context is provided (Stage 2), only adds if current_state matches last_state.\n    \"\"\"\n    q_new = q_values.copy()\n    \n    # Check state dependency if states are provided\n    if last_state is not None and current_state is not None:\n        if last_state != current_state:\n            return q_new\n\n    if last_action is not None:\n        q_new[int(last_action)] += bonus\n        \n    return q_new",
    "reconstructor_fix": "Update the Stage 2 policy assembly logic to pass `last_state` and `current_state` (state) as arguments to the perseveration primitive."
  },
  "p21": {
    "diagnosis": "The original model uses the parameter 'phi' to represent the perseveration scaling factor, which is modulated by STAI. The reconstructor likely relies on standard parameter names (e.g., 'stickiness' or 'perseveration') to wire the 'add_perseveration_bonus' primitive and the STAI modulation logic. Because 'phi' is not a standard name, the reconstructor fails to link it to the perseveration logic, resulting in a model that ignores the stickiness hypothesis. Renaming 'phi' to 'stickiness' in the spec ensures correct assembly.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stickiness"
      ],
      "stai_modulation": "multiplicative",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stickiness": [
          0,
          5
        ]
      },
      "bic": null
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p22": {
    "diagnosis": "The original model implies standard Temporal Difference learning (evidenced by the `alpha` parameter and `CognitiveModelBase` inheritance), but the reconstructed specification is missing the value update primitives (`td_update_stage1`, `td_update_stage2`). Without these primitives, the reconstructed model will not update its Q-values (learning will be disabled), causing a functional mismatch.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel3",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_additive",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stick_base",
        "stick_stai"
      ],
      "stai_modulation": "additive",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stick_base": [
          -2,
          2
        ],
        "stick_stai": [
          0,
          5
        ]
      }
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p23": {
    "diagnosis": "The original model uses the trial outcome (reward) to determine which learning rate to use (alpha_pos vs alpha_neg) for BOTH Stage 1 and Stage 2 updates. The specified primitive `td_update_asymmetric` selects the learning rate based on the 'target' value passed to it. This works for Stage 2 (where target is reward) but fails for Stage 1 (where target is the Stage 2 Q-value), causing the wrong learning rate to be applied.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_affine_amplification",
        "value_update::td_update_reward_based_asymmetric"
      ],
      "parameters": [
        "alpha_base",
        "beta",
        "bias_factor"
      ],
      "stai_modulation": "affine_amplification",
      "bounds": {
        "alpha_base": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "bias_factor": [
          -1,
          2
        ]
      },
      "bic": 353.6022018424507
    },
    "primitives_fix": "def td_update_reward_based_asymmetric(current_q: float, target: float, reward: float, alpha_pos: float, alpha_neg: float) -> float:\n    \"\"\"\n    Updates Q-value using an alpha determined by the reward outcome.\n    Matches p23: if reward > 0.5 use alpha_pos, else alpha_neg.\n    \"\"\"\n    if reward > 0.5:\n        alpha = alpha_pos\n    else:\n        alpha = alpha_neg\n    \n    # Ensure alpha stays within bounds (Original model clips alpha_neg at init)\n    alpha = max(0.0, min(1.0, alpha))\n    \n    return current_q + alpha * (target - current_q)",
    "reconstructor_fix": null
  },
  "p24": {
    "diagnosis": "The 'stai_modulation' field in the Spec is defined as a string 'inverse_linear', which is ambiguous and does not explicitly target the 'w_max' parameter. The Reconstructor fails to link 'w_max' to the modulation function, likely leaving the Model-Based weight unmodulated or causing a parameter mismatch. The Original Model explicitly modulates 'w_max' by STAI to calculate the effective weight. The Spec must define 'stai_modulation' as a mapping to target 'w_max'.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel1",
      "primitives": [
        "helper::compute_mb_values",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2",
        "stai_modulation::stai_inverse_linear",
        "policy::mb_mf_mixture",
        "policy::add_perseveration_bonus",
        "helper::softmax"
      ],
      "parameters": [
        "alpha",
        "beta",
        "w_max",
        "perseveration"
      ],
      "stai_modulation": {
        "w_max": "inverse_linear"
      },
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "w_max": [
          0,
          1
        ],
        "perseveration": [
          0,
          5
        ]
      }
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p25": {
    "diagnosis": "The original model implements a specific STAI modulation 'k_stick * (1.0 + stai)' which is an affine amplification with a fixed bias of 1.0. The current spec uses 'stai_affine_amplification', which requires a learnable 'bias_factor' parameter that does not exist in the model's parameters (only alpha, beta, k_stick are present). A new primitive with a fixed unit bias is required to match the original model's behavior and parameter signature.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_amplification_unit_bias",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "k_stick"
      ],
      "stai_modulation": "unit_amplification",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "k_stick": [
          0,
          5
        ]
      },
      "bic": 119.8063597714025
    },
    "primitives_fix": "def stai_amplification_unit_bias(param: float, stai: float) -> float:\n    \"\"\"\n    Amplifies a base parameter by a factor of (1 + 1.0 * stai).\n    Used for fixed-bias anxiety modulation (e.g. p25).\n    \n    Returns: param * (1.0 + stai)\n    \"\"\"\n    return param * (1.0 + stai)",
    "reconstructor_fix": null
  },
  "p26": {
    "diagnosis": "The reconstructor relies on parameter naming conventions to route parameters to primitives. It likely does not recognize 'stickiness_factor' as a parameter that should be mapped to the perseveration bonus logic (which typically expects 'stickiness' or 'perseveration'). Consequently, the 'add_perseveration_bonus' primitive is not receiving the stickiness value, and the anxiety modulation defined in the spec is effectively orphaned.",
    "fix_type": "reconstructor",
    "fixed_spec": null,
    "primitives_fix": null,
    "reconstructor_fix": "Update the parameter routing logic to explicitly recognize 'stickiness_factor' as a valid parameter name for the perseveration/stickiness bonus component, ensuring it is passed to 'add_perseveration_bonus' (after applying STAI modulation)."
  },
  "p27": {
    "diagnosis": "The 'stai_modulation' field is set to a global string 'multiplicative', which creates ambiguity regarding which parameter should be modulated. The reconstructor likely fails to identify 'pers_k' as the target for anxiety modulation without explicit instruction. The spec should define 'stai_modulation' as a mapping to target 'pers_k' specifically, matching the original logic `bonus = pers_k * stai`.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel3",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "pers_k"
      ],
      "stai_modulation": {
        "pers_k": "multiplicative"
      },
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "pers_k": [
          0,
          5
        ]
      },
      "bic": 0.0
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p28": {
    "diagnosis": "The original model modulates the 'win_bonus' parameter by STAI (bonus = stai * win_bonus) before adding it to the Q-values. The reconstructed model defines 'stai_modulation', but the Reconstructor likely does not automatically apply this modulation to the 'win_bonus' parameter (as it might strictly look for 'stickiness' parameters), resulting in the raw 'win_bonus' being used without anxiety amplification.",
    "fix_type": "reconstructor",
    "fixed_spec": null,
    "primitives_fix": null,
    "reconstructor_fix": "Update the Reconstructor logic to apply the configured 'stai_modulation' function to the 'win_bonus' parameter (e.g., effective_bonus = stai_multiplicative(win_bonus, stai)) before passing it to the 'add_win_stay_bonus' primitive."
  },
  "p29": {
    "diagnosis": "The original model explicitly uses Q-learning (max over Stage 2 values) for the Stage 1 update, whereas the reconstructed model uses the generic 'td_update_stage1' primitive. In this context, the generic update typically relies on the actual next value (SARSA) rather than the maximum possible value, leading to a discrepancy in learning rules.",
    "fix_type": "spec|primitives",
    "fixed_spec": {
      "class": "ParticipantModel3",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::stage1_q_learning_update",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stickiness"
      ],
      "stai_modulation": "multiplicative",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stickiness": [
          0,
          5
        ]
      },
      "bic": 256.5459180268013
    },
    "primitives_fix": "def stage1_q_learning_update(q_stage1: np.ndarray, action: int, q_stage2: np.ndarray, state: int, alpha: float) -> np.ndarray:\n    \"\"\"\n    Updates Stage 1 Q-values using Q-learning (Max over Stage 2).\n    Target = max(Q_stage2[state])\n    \"\"\"\n    q_new = q_stage1.copy()\n    target = np.max(q_stage2[state])\n    prediction_error = target - q_new[action]\n    q_new[action] += alpha * prediction_error\n    return q_new",
    "reconstructor_fix": null
  },
  "p30": {
    "diagnosis": "The reconstructed model spec is missing the value update primitives (`value_update::td_update_stage1` and `value_update::td_update_stage2`). The original `ParticipantModel2` inherits from `CognitiveModelBase`, which implies standard Q-learning behavior for Stage 1 and Stage 2 (utilizing `alpha`). The current spec only includes policy and helper primitives, resulting in a static model that does not learn from rewards.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stick_sensitivity"
      ],
      "stai_modulation": "multiplicative",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stick_sensitivity": [
          0,
          5
        ]
      },
      "bic": 477.67521678131635
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p31": {
    "diagnosis": "The reconstructed model spec is missing the value update primitives (`td_update_stage1`, `td_update_stage2`), which are required for the Model-Free learning strategy (alpha parameter) described in the original model. Additionally, the parameter name `k_anx` is likely not recognized by the reconstructor's routing logic as a perseveration parameter, causing the stickiness bonus to be ignored. Renaming it to `stickiness` ensures the perseveration logic and STAI modulation are correctly applied.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stickiness"
      ],
      "stai_modulation": "multiplicative",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stickiness": [
          0,
          5
        ]
      },
      "bic": 373.64927906442557
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p32": {
    "diagnosis": "The reconstructor relies on parameter naming conventions (e.g., 'stickiness' or 'p') to automatically wire parameters to the `add_perseveration_bonus` primitive. The original model and spec use the parameter name 'k' for the perseveration scaling factor. Consequently, the reconstructor fails to recognize 'k' as the source for the `bonus` argument, leaving the stickiness bonus unconnected or zero.",
    "fix_type": "reconstructor",
    "fixed_spec": null,
    "primitives_fix": null,
    "reconstructor_fix": "Update the parameter wiring logic to explicitly recognize 'k' as a valid parameter name for the 'bonus' argument of the 'policy::add_perseveration_bonus' primitive, ensuring it is correctly modulated by STAI before being passed to the policy."
  },
  "p33": {
    "diagnosis": "The original model implementation uses a SARSA update for Stage 1 (calculating the prediction error using the value of the action actually taken in Stage 2: `q_stage2[state, action_2]`). The reconstructed model uses the generic `td_update_stage1` primitive, which typically implements standard Q-learning (using `max(q_stage2[state])`). This discrepancy in the learning rule (SARSA vs Q-learning) causes the mismatch.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage2",
        "value_update::td_update_stage1_sarsa"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stickiness"
      ],
      "stai_modulation": "multiplicative",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stickiness": [
          0,
          5
        ]
      },
      "bic": 386.37827393527584
    },
    "primitives_fix": "def td_update_stage1_sarsa(q_stage1: np.ndarray, action: int, q_stage2: np.ndarray, state: int, action_2: int, alpha: float) -> np.ndarray:\n    \"\"\"\n    Performs a SARSA update for Stage 1 (using the value of the action actually taken in Stage 2).\n    \"\"\"\n    q_new = q_stage1.copy()\n    target = q_stage2[state, action_2]\n    prediction_error = target - q_new[action]\n    q_new[action] += alpha * prediction_error\n    return q_new",
    "reconstructor_fix": null
  },
  "p34": {
    "diagnosis": "The original model code performs a coupled, sequential value update where Stage 2 is updated first, and the *newly updated* Q-value of the specific action chosen in Stage 2 (SARSA-style) is used to update Stage 1 in the same trial. The reconstructed model specifies separate `td_update_stage1` and `td_update_stage2` primitives. This separation typically leads to independent updates where Stage 1 relies on the pre-update Stage 2 values (or defaults to Q-learning/max over actions), failing to replicate the specific information flow and SARSA logic of the original code.",
    "fix_type": "primitives",
    "fixed_spec": {
      "class": "ParticipantModel3",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::update_sequential_sarsa"
      ],
      "parameters": [
        "alpha",
        "beta",
        "rho"
      ],
      "stai_modulation": "multiplicative",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "rho": [
          0,
          5
        ]
      },
      "bic": 373.8582871089093
    },
    "primitives_fix": "def update_sequential_sarsa(q_stage1: np.ndarray, q_stage2: np.ndarray, action_1: int, state_2: int, action_2: int, reward: float, alpha: float) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Updates Q-values for both stages sequentially.\n    Stage 2 is updated first. Stage 1 is updated using the NEW Stage 2 value of the chosen action (SARSA).\n    \"\"\"\n    # Update Stage 2\n    q2_new = q_stage2.copy()\n    delta_2 = reward - q2_new[state_2, action_2]\n    q2_new[state_2, action_2] += alpha * delta_2\n    \n    # Update Stage 1 using UPDATED Stage 2 value of chosen action\n    q1_new = q_stage1.copy()\n    delta_1 = q2_new[state_2, action_2] - q1_new[action_1]\n    q1_new[action_1] += alpha * delta_1\n    \n    return q1_new, q2_new",
    "reconstructor_fix": null
  },
  "p35": {
    "diagnosis": "The parameter name 'cling_factor' is not recognized by the reconstructor as the input for the 'add_win_stay_bonus' primitive. The reconstructor likely relies on standard parameter naming (e.g., 'win_stay_bonus') to link the parameter to the correct history-dependent policy primitive, or defaults to treating unknown parameters as generic perseveration if not correctly mapped. Renaming the parameter to 'win_stay_bonus' ensures it is correctly wired to the 'add_win_stay_bonus' logic.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel3",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_win_stay_bonus"
      ],
      "parameters": [
        "alpha",
        "beta",
        "win_stay_bonus"
      ],
      "stai_modulation": "multiplicative",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "win_stay_bonus": [
          0,
          5
        ]
      },
      "bic": 283.01581577064576
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p36": {
    "diagnosis": "The reconstructed model relies on implicit wiring between generic `stai_additive` and `add_perseveration_bonus` primitives to implement the anxiety-modulated perseveration. The parameters `p_base` and `p_slope` are not correctly mapped to the modulation logic in the generated pipeline, failing to replicate the specific `p_base + (stai * p_slope)` calculation found in the original `policy_stage1`. A custom primitive is required to explicitly handle this parameter modulation.",
    "fix_type": "primitives",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "policy::add_anxiety_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "p_base",
        "p_slope"
      ],
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "p_base": [
          0,
          5
        ],
        "p_slope": [
          0,
          5
        ]
      }
    },
    "primitives_fix": "def add_anxiety_perseveration_bonus(q_values: np.ndarray, last_action: Optional[int], p_base: float, p_slope: float, stai: float) -> np.ndarray:\n    \"\"\"\n    Adds a perseveration bonus modulated linearly by STAI.\n    Hypothesis: Anxiety increases choice perseveration.\n    Bonus = p_base + (stai * p_slope)\n    \"\"\"\n    q_new = q_values.copy()\n    if last_action is not None:\n        bonus = p_base + (stai * p_slope)\n        q_new[int(last_action)] += bonus\n    return q_new",
    "reconstructor_fix": null
  },
  "p37": {
    "diagnosis": "The original model modulates the parameter 'w_base' by STAI using an inverse linear function (w_effective = w_base * (1 - stai)). The current spec defines 'stai_modulation' as a generic string 'inverse_linear', which likely defaults to modulating a standard parameter like 'stickiness' or 'w' (if present). Since the parameter is named 'w_base', the reconstructor fails to apply the modulation, leaving the mixing weight static. The spec must explicitly map 'w_base' to 'inverse_linear'.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel1",
      "primitives": [
        "helper::softmax",
        "helper::compute_mb_values",
        "stai_modulation::stai_inverse_linear",
        "policy::mb_mf_mixture",
        "value_update::td_update_stage2",
        "value_update::td_update_stage1_direct_reward"
      ],
      "parameters": [
        "alpha",
        "beta",
        "w_base"
      ],
      "stai_modulation": {
        "w_base": "inverse_linear"
      },
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "w_base": [
          0,
          1
        ]
      },
      "bic": 0
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p38": {
    "diagnosis": "The original model employs a SARSA-style update for Stage 1, explicitly updating Q-values towards the Q-value of the chosen action in Stage 2 (`q_stage2[state, action_2]`). The standard `td_update_stage1` primitive typically operates with a pre-calculated target (often `max(q_stage2)` for Q-learning in these tasks) or lacks the access to `action_2` to perform this specific lookup. To match the original behavior, a specific SARSA primitive is needed.",
    "fix_type": "primitives",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_additive",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1_sarsa",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "pers_base",
        "stai_pers"
      ],
      "stai_modulation": "additive",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "pers_base": [
          0,
          5
        ],
        "stai_pers": [
          0,
          5
        ]
      },
      "bic": 384.9927139795815
    },
    "primitives_fix": "def td_update_stage1_sarsa(q_stage1: np.ndarray, action: int, q_stage2: np.ndarray, state: int, action_2: int, alpha: float) -> np.ndarray:\n    \"\"\"\n    Updates Stage 1 Q-values towards the value of the chosen action in Stage 2 (SARSA).\n    \"\"\"\n    q_new = q_stage1.copy()\n    target = q_stage2[state, action_2]\n    prediction_error = target - q_new[action]\n    q_new[action] += alpha * prediction_error\n    return q_new",
    "reconstructor_fix": null
  },
  "p39": {
    "diagnosis": "The spec defines 'stai_modulation' as a global string 'multiplicative', which is ambiguous for a model with multiple parameters. The reconstructor cannot determine that 'persev_w' specifically is the target for STAI modulation, likely resulting in 'persev_w' being treated as a constant stickiness parameter (or not modulated at all) instead of being scaled by STAI as in the original model. The spec needs to explicitly map the 'persev_w' parameter to the 'multiplicative' modulation type.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "persev_w"
      ],
      "stai_modulation": {
        "persev_w": "multiplicative"
      },
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "persev_w": [
          0,
          5
        ]
      },
      "bic": 327.36430783923794
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p40": {
    "diagnosis": "The parameter 'p_scale' is likely not recognized by the reconstructor as the perseveration/stickiness parameter (which typically expects standard names like 'stickiness', 'p', or 'persev'). As a result, the 'add_perseveration_bonus' primitive is not effectively linked to 'p_scale', and the anxiety modulation (stai * p_scale) is not applied to the policy. The logic requires the parameter to be identified as the stickiness weight to be modulated by STAI.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel3",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stickiness"
      ],
      "stai_modulation": "multiplicative",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stickiness": [
          0,
          5
        ]
      }
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p41": {
    "diagnosis": "The reconstructed spec defines 'stai_modulation': 'multiplicative' globally, which implies that STAI modulates all parameters (alpha, beta, stick_factor). The original model only applies multiplicative STAI modulation to 'stick_factor'.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_perseveration_bonus",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2"
      ],
      "parameters": [
        "alpha",
        "beta",
        "stick_factor"
      ],
      "stai_modulation": "multiplicative",
      "stai_modulated_params": [
        "stick_factor"
      ],
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "stick_factor": [
          0,
          5
        ]
      },
      "bic": 271.3039688021288
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p42": {
    "diagnosis": "The reconstructed model uses the generic `decay::apply_memory_decay` primitive, which only decays the unchosen options in the current context (visited state). It fails to implement the specific logic of the original `ParticipantModel2` (p42), which requires decaying the entire unvisited state in Stage 2. Furthermore, the original model explicitly clips the calculated decay rate to [0, 1] before application, whereas the generic additive modulation does not guarantee this constraint.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel2",
      "primitives": [
        "helper::softmax",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2",
        "stai_modulation::stai_additive",
        "decay::decay_stage1_p42",
        "decay::decay_stage2_p42"
      ],
      "parameters": [
        "alpha",
        "beta",
        "decay_base",
        "decay_stai"
      ],
      "stai_modulation": "additive",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "decay_base": [
          0,
          1
        ],
        "decay_stai": [
          0,
          1
        ]
      }
    },
    "primitives_fix": "def decay_stage1_p42(q_stage1: np.ndarray, action: int, decay_rate: float) -> np.ndarray:\n    \"\"\"\n    Decays unchosen Stage 1 options with rate clipping (p42).\n    \"\"\"\n    decay_rate = np.clip(decay_rate, 0.0, 1.0)\n    q_new = q_stage1.copy()\n    unchosen = 1 - action\n    q_new[unchosen] *= (1.0 - decay_rate)\n    return q_new\n\ndef decay_stage2_p42(q_stage2: np.ndarray, state: int, action: int, decay_rate: float) -> np.ndarray:\n    \"\"\"\n    Decays unchosen Stage 2 options in visited state AND all options in unvisited state (p42).\n    \"\"\"\n    decay_rate = np.clip(decay_rate, 0.0, 1.0)\n    q_new = q_stage2.copy()\n    \n    # Decay unchosen in visited state\n    unchosen_visited = 1 - action\n    q_new[state, unchosen_visited] *= (1.0 - decay_rate)\n    \n    # Decay all in unvisited state\n    unvisited_state = 1 - state\n    q_new[unvisited_state, :] *= (1.0 - decay_rate)\n    \n    return q_new",
    "reconstructor_fix": null
  },
  "p43": {
    "diagnosis": "The original model performs the Stage 2 value update (TD) before the Stage 1 Model-Free update, and the Stage 1 update uses the *updated* Stage 2 Q-value (delta_1 = Q2_new - Q1). The current spec lists 'value_update::td_update_stage1' before 'value_update::td_update_stage2', which typically causes the reconstructor to execute the Stage 1 update first using the *old* Stage 2 values. This creates a functional discrepancy in how reward information propagates.",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel1",
      "primitives": [
        "helper::softmax",
        "helper::compute_mb_values",
        "stai_modulation::stai_inverse_linear",
        "policy::mb_mf_mixture",
        "value_update::td_update_stage2",
        "value_update::td_update_stage1"
      ],
      "parameters": [
        "alpha",
        "beta",
        "w_base"
      ],
      "stai_modulation": "inverse_linear",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "w_base": [
          0,
          1
        ]
      },
      "bic": 0
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  },
  "p44": {
    "diagnosis": "The reconstructed model's spec primitives list is missing the standard value update components ('value_update::td_update_stage1' and 'value_update::td_update_stage2'). In the original model, 'super().post_trial' is called to perform standard Q-learning updates, while the 'update_habit_trace' is an addition. The current spec only includes the habit update, causing the model to lack basic reinforcement learning capabilities (Q-values won't update).",
    "fix_type": "spec",
    "fixed_spec": {
      "class": "ParticipantModel1",
      "primitives": [
        "helper::softmax",
        "stai_modulation::stai_multiplicative",
        "policy::add_habit_influence",
        "value_update::td_update_stage1",
        "value_update::td_update_stage2",
        "value_update::update_habit_trace"
      ],
      "parameters": [
        "alpha",
        "beta",
        "habit_weight"
      ],
      "stai_modulation": "multiplicative",
      "bounds": {
        "alpha": [
          0,
          1
        ],
        "beta": [
          0,
          10
        ],
        "habit_weight": [
          0,
          5
        ]
      },
      "bic": 402.74022725825887
    },
    "primitives_fix": null,
    "reconstructor_fix": null
  }
}