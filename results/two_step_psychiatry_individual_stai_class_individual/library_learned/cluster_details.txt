Semantic Clusters
==================================================


## UNPACK_PARAMETERS
----------------------------------------

### unpack_parameters_standard
Mechanisms: set()
STAI: none
Frequency: 16
Participants: ['p18', 'p19', 'p21', 'p22', 'p23', 'p25', 'p27', 'p30', 'p31', 'p32', 'p34', 'p36', 'p37', 'p40', 'p41', 'p43']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters
```

### unpack_parameters_perseveration
Mechanisms: {'perseveration'}
STAI: none
Frequency: 7
Participants: ['p20', 'p24', 'p26', 'p29', 'p33', 'p39', 'p44']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.anxiety_stick = model_parameters
```

### unpack_parameters_win_stay
Mechanisms: {'win_stay'}
STAI: none
Frequency: 2
Participants: ['p28', 'p35']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.win_bonus = model_parameters
```

### unpack_parameters_standard_stai_custom
Mechanisms: set()
STAI: custom
Frequency: 1
Participants: ['p38']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_base, self.stai_pers = model_parameters
```

### unpack_parameters_memory_decay_stai_multiplicative
Mechanisms: {'memory_decay'}
STAI: multiplicative
Frequency: 1
Participants: ['p42']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_base, self.decay_stai = model_parameters
        
        # Calculate total decay rate, clipped to [0, 1]
        raw_decay = self.decay_base + (self.decay_stai * self.stai)
        self.decay_rate = np.clip(raw_decay, 0.0, 1.0)
```

## INIT_MODEL
----------------------------------------

### init_model_perseveration_stai_multiplicative
Mechanisms: {'perseveration'}
STAI: multiplicative
Frequency: 2
Participants: ['p19', 'p38']

Canonical code:
```python
    def init_model(self) -> None:
        # Calculate the stickiness bonus based on anxiety
        self.stickiness = self.stick_base + (self.stick_stai_slope * self.stai)
```

### init_model_perseveration_stai_additive_boost
Mechanisms: {'perseveration'}
STAI: additive_boost
Frequency: 1
Participants: ['p18']

Canonical code:
```python
    def init_model(self) -> None:
        # Calculate effective stickiness based on STAI
        # Low STAI -> Higher effective stickiness
        self.phi_eff = self.phi * (1.0 / (1.0 + self.stai))
```

### init_model_asymmetric_learning_stai_stai_first
Mechanisms: {'asymmetric_learning'}
STAI: stai_first
Frequency: 1
Participants: ['p23']

Canonical code:
```python
    def init_model(self) -> None:
        # Calculate specific learning rates
        self.alpha_pos = self.alpha_base
        
        # Modulate negative learning rate by STAI
        # We clip to ensure it stays within reasonable bounds [0, 1]
        raw_neg = self.alpha_base * (1.0 + self.stai * self.bias_factor)
        self.alpha_neg = np.clip(raw_neg, 0.0, 1.0)
```

### init_model_model_based
Mechanisms: {'model_based'}
STAI: none
Frequency: 1
Participants: ['p37']

Canonical code:
```python
    def init_model(self) -> None:
        # Initialize Model-Based values (transition matrix is fixed in base class)
        # We need separate storage for MF values to combine them later
        self.q_mf = np.zeros(self.n_choices)
```

### init_model_model_based_stai_inverse_linear
Mechanisms: {'model_based'}
STAI: inverse_linear
Frequency: 1
Participants: ['p43']

Canonical code:
```python
    def init_model(self) -> None:
        # Initialize Model-Based values (Q_MB) and Model-Free values (Q_MF)
        self.q_mf = np.zeros(self.n_choices)
        self.q_mb = np.zeros(self.n_choices)
        
        # Calculate effective mixing weight based on STAI
        # Higher STAI reduces the effective w
        self.w_eff = self.w_base * (1.0 - self.stai)
```

### init_model_perseveration
Mechanisms: {'perseveration'}
STAI: none
Frequency: 1
Participants: ['p44']

Canonical code:
```python
    def init_model(self) -> None:
        # Initialize habit strength for the two stage-1 choices (A and U)
        self.habit = np.zeros(self.n_choices)
```

## POLICY_STAGE1
----------------------------------------

### policy_stage1_perseveration_stai_multiplicative
Mechanisms: {'perseveration'}
STAI: multiplicative
Frequency: 11
Participants: ['p20', 'p21', 'p22', 'p27', 'p29', 'p30', 'p31', 'p32', 'p33', 'p34', 'p39']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        bonus = self.stickiness_base + (self.anxiety_stick * self.stai)
        
        # Copy values to avoid modifying the actual learned Q-values permanently
        q_vals = self.q_stage1.copy()
        
        # Add bonus to the previously chosen action (if it exists)
        if self.last_action1 is not None:
            q_vals[self.last_action1] += bonus
            
        return self.softmax(q_vals, self.beta)
```

### policy_stage1_perseveration_stai_stai_first
Mechanisms: {'perseveration'}
STAI: stai_first
Frequency: 5
Participants: ['p26', 'p36', 'p40', 'p41', 'p44']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            # The bonus is proportional to anxiety
            # Higher STAI -> Higher bonus to repeat last action
            bonus[int(self.last_action1)] = self.stai * self.stickiness_factor
        
        # Combine Q-values with stickiness bonus
        combined_values = self.q_stage1 + bonus
        return self.softmax(combined_values, self.beta)
```

### policy_stage1_perseveration
Mechanisms: {'perseveration'}
STAI: none
Frequency: 3
Participants: ['p18', 'p19', 'p38']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bonus to the Q-values before softmax
        q_modified = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            q_modified[self.last_action1] += self.phi_eff
            
        return self.softmax(q_modified, self.beta)
```

### policy_stage1_model_based_perseveration_stai_inverse_linear
Mechanisms: {'perseveration', 'model_based'}
STAI: inverse_linear
Frequency: 1
Participants: ['p24']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values
        # V(State) = max(Q_stage2(State, :))
        v_stage2 = np.max(self.q_stage2, axis=1) 
        # Q_MB = Transition_Matrix * V_stage2
        q_mb = self.T @ v_stage2
        
        # 2. Calculate Mixing Weight based on Anxiety
        # Higher STAI -> Lower w_eff -> More Model-Free behavior
        w_eff = self.w_max * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0, 1)
        
        # 3. Combine MB and MF values (self.q_stage1 is the MF value)
        q_net = w_eff * q_mb + (1 - w_eff) * self.q_stage1
        
        # 4. Add Perseveration
        if self.last_action1 is not None:
            q_net[self.last_action1] += self.perseveration
            
        return self.softmax(q_net, self.beta)
```

### policy_stage1_perseveration_stai_additive_boost
Mechanisms: {'perseveration'}
STAI: additive_boost
Frequency: 1
Participants: ['p25']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        stickiness_bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            # The bonus is scaled by STAI: Higher anxiety -> Higher tendency to repeat
            # Effective stickiness = k_stick * (1 + STAI)
            effective_k = self.k_stick * (1.0 + self.stai)
            stickiness_bonus[self.last_action1] = effective_k
            
        # Add bonus to Q-values before softmax
        return self.softmax(self.q_stage1 + stickiness_bonus, self.beta)
```

### policy_stage1_perseveration_win_stay_stai_stai_first
Mechanisms: {'perseveration', 'win_stay'}
STAI: stai_first
Frequency: 1
Participants: ['p28']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        q_net = self.q_stage1.copy()
        
        # Apply Win-Stay bonus
        if self.last_reward == 1.0 and self.last_action1 is not None:
            bonus = self.stai * self.win_bonus
            q_net[self.last_action1] += bonus
            
        return self.softmax(q_net, self.beta)
```

### policy_stage1_perseveration_win_stay_stai_multiplicative
Mechanisms: {'perseveration', 'win_stay'}
STAI: multiplicative
Frequency: 1
Participants: ['p35']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Copy values to avoid modifying the actual learned Q-values permanently
        current_values = self.q_stage1.copy()
        
        # Apply bonus if the last trial was a "Win"
        if self.last_reward == 1.0 and self.last_action1 is not None:
            bonus = self.cling_factor * self.stai
            current_values[self.last_action1] += bonus
            
        return self.softmax(current_values, self.beta)
```

### policy_stage1_model_based_stai_inverse_linear
Mechanisms: {'model_based'}
STAI: inverse_linear
Frequency: 1
Participants: ['p37']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # self.T is [2x2]: T[a, s] probability of transition
            # But base class T is [[0.7, 0.3], [0.3, 0.7]] which corresponds to actions 0 and 1
            # We assume row 0 is action A, row 1 is action U
            max_q2_s0 = np.max(self.q_stage2[0])
            max_q2_s1 = np.max(self.q_stage2[1])
            q_mb[a] = self.T[a, 0] * max_q2_s0 + self.T[a, 1] * max_q2_s1

        # 2. Modulate mixing weight by STAI
        # High anxiety (stai ~ 1) reduces w_effective towards 0 (Pure MF)
        w_effective = self.w_base * (1.0 - self.stai)
        
        # 3. Combine MB and MF values
        q_combined = w_effective * q_mb + (1 - w_effective) * self.q_mf
        
        return self.softmax(q_combined, self.beta)
```

### policy_stage1_model_based
Mechanisms: {'model_based'}
STAI: none
Frequency: 1
Participants: ['p43']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        for a in range(self.n_choices):
            # T[0] is transition for action 0 (A), T[1] for action 1 (U)
            # T[a][0] is prob of state 0 (X), T[a][1] is prob of state 1 (Y)
            v_state_x = np.max(self.q_stage2[0])
            v_state_y = np.max(self.q_stage2[1])
            
            # Transition matrix self.T is defined as:
            # T[0] = [0.7, 0.3] (Action 0 -> State 0 w/ 0.7)
            # T[1] = [0.3, 0.7] (Action 1 -> State 1 w/ 0.7)
            # Note: The base class defines T as [[0.7, 0.3], [0.3, 0.7]]
            # We assume row 0 corresponds to Action 0, row 1 to Action 1.
            
            self.q_mb[a] = self.T[a, 0] * v_state_x + self.T[a, 1] * v_state_y

        # Combine MF and MB values
        q_net = (1 - self.w_eff) * self.q_mf + self.w_eff * self.q_mb
        return self.softmax(q_net, self.beta)
```

## POLICY_STAGE2
----------------------------------------

### policy_stage2_perseveration
Mechanisms: {'perseveration'}
STAI: none
Frequency: 1
Participants: ['p19']

Canonical code:
```python
    def policy_stage2(self, state: int) -> np.ndarray:
        # Add stickiness bonus only if we are in the same state as the last choice
        # (Context-dependent perseveration)
        qs = self.q_stage2[state].copy()
        if self.last_state == state and self.last_action2 is not None:
            qs[self.last_action2] += self.stickiness
        return self.softmax(qs, self.beta)
```

### policy_stage2_perseveration_stai_multiplicative
Mechanisms: {'perseveration'}
STAI: multiplicative
Frequency: 1
Participants: ['p20']

Canonical code:
```python
    def policy_stage2(self, state: int) -> np.ndarray:
        # We apply stickiness to stage 2 as well, though often less relevant if states change
        # However, if they return to the same state, they might repeat the alien choice.
        # We need to track last action per state or just last action overall?
        # Usually stickiness is just "last button pressed". Here we assume "last alien chosen in this state".
        # Since we don't track last_action_per_state in base, we'll use a simplified heuristic:
        # If the last trial was in this state, we boost that action.
        
        bonus = self.stickiness_base + (self.anxiety_stick * self.stai)
        q_vals = self.q_stage2[state].copy()
        
        if self.last_state == state and self.last_action2 is not None:
             q_vals[self.last_action2] += bonus
             
        return self.softmax(q_vals, self.beta)
```

### policy_stage2_model_based_perseveration
Mechanisms: {'perseveration', 'model_based'}
STAI: none
Frequency: 1
Participants: ['p22']

Canonical code:
```python
    def policy_stage2(self, state: int) -> np.ndarray:
        # We can also apply stickiness to stage 2, or keep it just stage 1.
        # Often stickiness is modeled at the first stage choice. 
        # Here we apply it only to stage 1 as that's the primary strategic choice.
        return self.softmax(self.q_stage2[state], self.beta)
```

## VALUE_UPDATE
----------------------------------------

### value_update_standard
Mechanisms: set()
STAI: none
Frequency: 10
Participants: ['p18', 'p20', 'p25', 'p33', 'p34', 'p36', 'p37', 'p38', 'p41', 'p43']

Canonical code:
```python
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
```

### value_update_asymmetric_learning
Mechanisms: {'asymmetric_learning'}
STAI: none
Frequency: 1
Participants: ['p23']

Canonical code:
```python
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine which alpha to use based on reward
        # Assuming reward is binary 0 or 1
        if reward > 0.5:
            alpha = self.alpha_pos
        else:
            alpha = self.alpha_neg
            
        # Update Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        # Update Stage 1
        # Using the updated stage 2 value as the target for stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1
```

### value_update_model_based
Mechanisms: {'model_based'}
STAI: none
Frequency: 1
Participants: ['p29']

Canonical code:
```python
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD update for stage 1
        # Note: We use the max of stage 2 as the target (Q-learning) to be robust
        target = np.max(self.q_stage2[state])
        delta_1 = target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
```

## POST_TRIAL
----------------------------------------

### post_trial_memory_decay
Mechanisms: {'memory_decay'}
STAI: none
Frequency: 1
Participants: ['p42']

Canonical code:
```python
    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Decay unchosen Stage 1 option
        unchosen_1 = 1 - action_1
        self.q_stage1[unchosen_1] *= (1.0 - self.decay_rate)
        
        # Decay unchosen Stage 2 option in the visited state
        unchosen_2 = 1 - action_2
        self.q_stage2[state, unchosen_2] *= (1.0 - self.decay_rate)
        
        # Note: We could also decay the unvisited state's values, but standard
        # forgetting models often focus on the active context or global decay.
        # Here we decay the unvisited state as well to represent global memory loss.
        unvisited_state = 1 - state
        self.q_stage2[unvisited_state, :] *= (1.0 - self.decay_rate)
```

### post_trial_eligibility_trace_memory_decay_perseveration
Mechanisms: {'perseveration', 'eligibility_trace', 'memory_decay'}
STAI: none
Frequency: 1
Participants: ['p44']

Canonical code:
```python
    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Update Habit trace: Strengthen chosen, weaken unchosen
        # We reuse self.alpha for the habit learning rate to minimize parameters
        self.habit[action_1] += self.alpha * (1 - self.habit[action_1])
        self.habit[1 - action_1] += self.alpha * (0 - self.habit[1 - action_1])
```
