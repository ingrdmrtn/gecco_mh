Cognitive Model Pattern Clusters
==================================================


## UNPACK_PARAMETERS
----------------------------------------

### unpack_parameters_standard
Frequency: 2
STAI patterns: set()
Participants: ['p18', 'p21']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters
```

### unpack_parameters_standard
Frequency: 2
STAI patterns: set()
Participants: ['p29', 'p33']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness = model_parameters
```

### unpack_parameters_standard
Frequency: 2
STAI patterns: set()
Participants: ['p37', 'p43']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p19']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stick_stai_slope = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p20']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.anxiety_stick = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p22']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stick_stai = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p23']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.bias_factor = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p24']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.perseveration = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p25']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.k_stick = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p26']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_factor = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p27']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_k = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p28']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.win_bonus = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p30']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_sensitivity = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p31']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.k_anx = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p32']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.k = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p34']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rho = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p35']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.cling_factor = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p36']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.p_base, self.p_slope = model_parameters
```

### unpack_parameters_stai_custom
Frequency: 1
STAI patterns: {'custom'}
Participants: ['p38']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_base, self.stai_pers = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p39']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.persev_w = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p40']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.p_scale = model_parameters
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p41']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_factor = model_parameters
```

### unpack_parameters_decay_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p42']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_base, self.decay_stai = model_parameters
        
        # Calculate total decay rate, clipped to [0, 1]
        raw_decay = self.decay_base + (self.decay_stai * self.stai)
        self.decay_rate = np.clip(raw_decay, 0.0, 1.0)
```

### unpack_parameters_standard
Frequency: 1
STAI patterns: set()
Participants: ['p44']

Canonical code:
```python
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.habit_weight = model_parameters
```

## INIT_MODEL
----------------------------------------

### init_model_stai_inverse_division
Frequency: 1
STAI patterns: {'inverse_division'}
Participants: ['p18']

Canonical code:
```python
    def init_model(self) -> None:
        # Calculate effective stickiness based on STAI
        # Low STAI -> Higher effective stickiness
        self.phi_eff = self.phi * (1.0 / (1.0 + self.stai))
```

### init_model_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p19']

Canonical code:
```python
    def init_model(self) -> None:
        # Calculate the stickiness bonus based on anxiety
        self.stickiness = self.stick_base + (self.stick_stai_slope * self.stai)
```

### init_model_stai_stai_scaled
Frequency: 1
STAI patterns: {'stai_scaled'}
Participants: ['p23']

Canonical code:
```python
    def init_model(self) -> None:
        # Calculate specific learning rates
        self.alpha_pos = self.alpha_base
        
        # Modulate negative learning rate by STAI
        # We clip to ensure it stays within reasonable bounds [0, 1]
        raw_neg = self.alpha_base * (1.0 + self.stai * self.bias_factor)
        self.alpha_neg = np.clip(raw_neg, 0.0, 1.0)
```

### init_model_standard
Frequency: 1
STAI patterns: set()
Participants: ['p37']

Canonical code:
```python
    def init_model(self) -> None:
        # Initialize Model-Based values (transition matrix is fixed in base class)
        # We need separate storage for MF values to combine them later
        self.q_mf = np.zeros(self.n_choices)
```

### init_model_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p38']

Canonical code:
```python
    def init_model(self) -> None:
        # Calculate effective perseveration bonus
        # P = base + (stai * factor)
        self.perseveration_bonus = self.pers_base + (self.stai * self.stai_pers)
```

### init_model_mb_stai_inverse_linear
Frequency: 1
STAI patterns: {'inverse_linear'}
Participants: ['p43']

Canonical code:
```python
    def init_model(self) -> None:
        # Initialize Model-Based values (Q_MB) and Model-Free values (Q_MF)
        self.q_mf = np.zeros(self.n_choices)
        self.q_mb = np.zeros(self.n_choices)
        
        # Calculate effective mixing weight based on STAI
        # Higher STAI reduces the effective w
        self.w_eff = self.w_base * (1.0 - self.stai)
```

### init_model_standard
Frequency: 1
STAI patterns: set()
Participants: ['p44']

Canonical code:
```python
    def init_model(self) -> None:
        # Initialize habit strength for the two stage-1 choices (A and U)
        self.habit = np.zeros(self.n_choices)
```

## POLICY_STAGE1
----------------------------------------

### policy_stage1_perseveration
Frequency: 1
STAI patterns: set()
Participants: ['p18']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bonus to the Q-values before softmax
        q_modified = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            q_modified[self.last_action1] += self.phi_eff
            
        return self.softmax(q_modified, self.beta)
```

### policy_stage1_perseveration
Frequency: 1
STAI patterns: set()
Participants: ['p19']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bonus to the Q-value of the last chosen action
        qs = self.q_stage1.copy()
        if self.last_action1 is not None:
            qs[self.last_action1] += self.stickiness
        return self.softmax(qs, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p20']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        bonus = self.stickiness_base + (self.anxiety_stick * self.stai)
        
        # Copy values to avoid modifying the actual learned Q-values permanently
        q_vals = self.q_stage1.copy()
        
        # Add bonus to the previously chosen action (if it exists)
        if self.last_action1 is not None:
            q_vals[self.last_action1] += bonus
            
        return self.softmax(q_vals, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p21']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is scaled by the STAI score
            stickiness_val = self.phi * self.stai
            q_values[self.last_action1] += stickiness_val
            
        return self.softmax(q_values, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p22']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Calculate effective stickiness
        eff_stickiness = self.stick_base + (self.stick_stai * self.stai)
        
        # Copy Q-values to avoid modifying the learned values permanently
        q_with_stick = self.q_stage1.copy()
        
        # Add bonus if there was a previous action
        if self.last_action1 is not None:
            q_with_stick[int(self.last_action1)] += eff_stickiness
            
        return self.softmax(q_with_stick, self.beta)
```

### policy_stage1_mb_perseveration_stai_inverse_linear
Frequency: 1
STAI patterns: {'inverse_linear'}
Participants: ['p24']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values
        # V(State) = max(Q_stage2(State, :))
        v_stage2 = np.max(self.q_stage2, axis=1) 
        # Q_MB = Transition_Matrix * V_stage2
        q_mb = self.T @ v_stage2
        
        # 2. Calculate Mixing Weight based on Anxiety
        # Higher STAI -> Lower w_eff -> More Model-Free behavior
        w_eff = self.w_max * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0, 1)
        
        # 3. Combine MB and MF values (self.q_stage1 is the MF value)
        q_net = w_eff * q_mb + (1 - w_eff) * self.q_stage1
        
        # 4. Add Perseveration
        if self.last_action1 is not None:
            q_net[self.last_action1] += self.perseveration
            
        return self.softmax(q_net, self.beta)
```

### policy_stage1_perseveration_stai_additive_boost
Frequency: 1
STAI patterns: {'additive_boost'}
Participants: ['p25']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        stickiness_bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            # The bonus is scaled by STAI: Higher anxiety -> Higher tendency to repeat
            # Effective stickiness = k_stick * (1 + STAI)
            effective_k = self.k_stick * (1.0 + self.stai)
            stickiness_bonus[self.last_action1] = effective_k
            
        # Add bonus to Q-values before softmax
        return self.softmax(self.q_stage1 + stickiness_bonus, self.beta)
```

### policy_stage1_perseveration_stai_stai_scaled
Frequency: 1
STAI patterns: {'stai_scaled'}
Participants: ['p26']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            # The bonus is proportional to anxiety
            # Higher STAI -> Higher bonus to repeat last action
            bonus[int(self.last_action1)] = self.stai * self.stickiness_factor
        
        # Combine Q-values with stickiness bonus
        combined_values = self.q_stage1 + bonus
        return self.softmax(combined_values, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p27']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Copy learned Q-values to avoid modifying the actual value estimates
        q_biased = self.q_stage1.copy()
        
        # Apply perseveration bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is the parameter * anxiety score
            bonus = self.pers_k * self.stai
            q_biased[int(self.last_action1)] += bonus
            
        return self.softmax(q_biased, self.beta)
```

### policy_stage1_perseveration_win_stay_stai_stai_scaled
Frequency: 1
STAI patterns: {'stai_scaled'}
Participants: ['p28']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        q_net = self.q_stage1.copy()
        
        # Apply Win-Stay bonus
        if self.last_reward == 1.0 and self.last_action1 is not None:
            bonus = self.stai * self.win_bonus
            q_net[self.last_action1] += bonus
            
        return self.softmax(q_net, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p29']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Copy learned values to avoid modifying the actual Q-table for updates
        q_vals = self.q_stage1.copy()
        
        # Apply perseveration bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is the base stickiness parameter scaled by anxiety level
            bonus = self.stickiness * self.stai
            q_vals[int(self.last_action1)] += bonus
            
        return self.softmax(q_vals, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p30']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Calculate anxiety-dependent stickiness bonus
        bonus_val = self.stick_sensitivity * self.stai
        
        # Create a temporary view of values to add bias without altering learned Q-values
        q_biased = self.q_stage1.copy()
        
        # Add bonus to the last chosen action if it exists (i.e., not the first trial)
        if self.last_action1 is not None:
            q_biased[int(self.last_action1)] += bonus_val
            
        return self.softmax(q_biased, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p31']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            stickiness_bonus = self.k_anx * self.stai
            q_values[int(self.last_action1)] += stickiness_bonus
            
        return self.softmax(q_values, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p32']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Copy learned values to avoid modifying the actual Q-table for learning
        q_values = self.q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            bonus = self.k * self.stai
            q_values[int(self.last_action1)] += bonus
            
        return self.softmax(q_values, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p33']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        q_modified = self.q_stage1.copy()
        
        # If a previous action exists, add a stickiness bonus scaled by anxiety
        if self.last_action1 is not None:
            # The higher the anxiety, the stronger the urge to repeat the last action
            bonus = self.stickiness * self.stai
            q_modified[int(self.last_action1)] += bonus
            
        return self.softmax(q_modified, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p34']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Copy Q-values to avoid modifying the actual learned values
        decision_values = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is proportional to anxiety (STAI)
            stickiness = self.rho * self.stai
            decision_values[int(self.last_action1)] += stickiness
            
        return self.softmax(decision_values, self.beta)
```

### policy_stage1_perseveration_win_stay_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p35']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Copy values to avoid modifying the actual learned Q-values permanently
        current_values = self.q_stage1.copy()
        
        # Apply bonus if the last trial was a "Win"
        if self.last_reward == 1.0 and self.last_action1 is not None:
            bonus = self.cling_factor * self.stai
            current_values[self.last_action1] += bonus
            
        return self.softmax(current_values, self.beta)
```

### policy_stage1_perseveration_stai_stai_scaled
Frequency: 1
STAI patterns: {'stai_scaled'}
Participants: ['p36']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Calculate effective perseveration parameter
        pers_param = self.p_base + (self.stai * self.p_slope)
        
        # Copy Q-values to avoid modifying the learned values permanently
        q_modified = self.q_stage1.copy()
        
        # Add bonus if a previous action exists
        if self.last_action1 is not None:
            q_modified[self.last_action1] += pers_param
            
        return self.softmax(q_modified, self.beta)
```

### policy_stage1_mb_stai_inverse_linear
Frequency: 1
STAI patterns: {'inverse_linear'}
Participants: ['p37']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # self.T is [2x2]: T[a, s] probability of transition
            # But base class T is [[0.7, 0.3], [0.3, 0.7]] which corresponds to actions 0 and 1
            # We assume row 0 is action A, row 1 is action U
            max_q2_s0 = np.max(self.q_stage2[0])
            max_q2_s1 = np.max(self.q_stage2[1])
            q_mb[a] = self.T[a, 0] * max_q2_s0 + self.T[a, 1] * max_q2_s1

        # 2. Modulate mixing weight by STAI
        # High anxiety (stai ~ 1) reduces w_effective towards 0 (Pure MF)
        w_effective = self.w_base * (1.0 - self.stai)
        
        # 3. Combine MB and MF values
        q_combined = w_effective * q_mb + (1 - w_effective) * self.q_mf
        
        return self.softmax(q_combined, self.beta)
```

### policy_stage1_perseveration
Frequency: 1
STAI patterns: set()
Participants: ['p38']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Add perseveration bonus to the Q-value of the previously chosen action
        q_modified = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            q_modified[self.last_action1] += self.perseveration_bonus
            
        return self.softmax(q_modified, self.beta)
```

### policy_stage1_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p39']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Create a temporary copy of Q-values for decision making
        # so we don't corrupt the actual learned values
        q_decision = self.q_stage1.copy()
        
        # If this isn't the first trial, apply the perseverance bonus
        if self.last_action1 is not None:
            # The bonus is the base weight scaled by the anxiety score
            bonus = self.persev_w * self.stai
            q_decision[int(self.last_action1)] += bonus
            
        return self.softmax(q_decision, self.beta)
```

### policy_stage1_perseveration_stai_stai_scaled
Frequency: 1
STAI patterns: {'stai_scaled'}
Participants: ['p40']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Start with learned values
        q_vals = self.q_stage1.copy()
        
        # Add anxiety-modulated perseveration bonus
        if self.last_action1 is not None:
            bonus = self.stai * self.p_scale
            q_vals[int(self.last_action1)] += bonus
            
        return self.softmax(q_vals, self.beta)
```

### policy_stage1_perseveration_stai_stai_scaled
Frequency: 1
STAI patterns: {'stai_scaled'}
Participants: ['p41']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Add stickiness bonus if there was a previous action
        if self.last_action1 is not None:
            # The bonus is proportional to anxiety
            bonus = self.stai * self.stick_factor
            q_values[int(self.last_action1)] += bonus
            
        return self.softmax(q_values, self.beta)
```

### policy_stage1_mb
Frequency: 1
STAI patterns: set()
Participants: ['p43']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        for a in range(self.n_choices):
            # T[0] is transition for action 0 (A), T[1] for action 1 (U)
            # T[a][0] is prob of state 0 (X), T[a][1] is prob of state 1 (Y)
            v_state_x = np.max(self.q_stage2[0])
            v_state_y = np.max(self.q_stage2[1])
            
            # Transition matrix self.T is defined as:
            # T[0] = [0.7, 0.3] (Action 0 -> State 0 w/ 0.7)
            # T[1] = [0.3, 0.7] (Action 1 -> State 1 w/ 0.7)
            # Note: The base class defines T as [[0.7, 0.3], [0.3, 0.7]]
            # We assume row 0 corresponds to Action 0, row 1 to Action 1.
            
            self.q_mb[a] = self.T[a, 0] * v_state_x + self.T[a, 1] * v_state_y

        # Combine MF and MB values
        q_net = (1 - self.w_eff) * self.q_mf + self.w_eff * self.q_mb
        return self.softmax(q_net, self.beta)
```

### policy_stage1_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p44']

Canonical code:
```python
    def policy_stage1(self) -> np.ndarray:
        # Combine Value (Q) and Habit (H)
        # The weight of the habit component is modulated by anxiety (STAI)
        net_values = self.q_stage1 + (self.habit_weight * self.stai * self.habit)
        return self.softmax(net_values, self.beta)
```

## POLICY_STAGE2
----------------------------------------

### policy_stage2_perseveration
Frequency: 1
STAI patterns: set()
Participants: ['p19']

Canonical code:
```python
    def policy_stage2(self, state: int) -> np.ndarray:
        # Add stickiness bonus only if we are in the same state as the last choice
        # (Context-dependent perseveration)
        qs = self.q_stage2[state].copy()
        if self.last_state == state and self.last_action2 is not None:
            qs[self.last_action2] += self.stickiness
        return self.softmax(qs, self.beta)
```

### policy_stage2_perseveration_stai_multiplicative
Frequency: 1
STAI patterns: {'multiplicative'}
Participants: ['p20']

Canonical code:
```python
    def policy_stage2(self, state: int) -> np.ndarray:
        # We apply stickiness to stage 2 as well, though often less relevant if states change
        # However, if they return to the same state, they might repeat the alien choice.
        # We need to track last action per state or just last action overall?
        # Usually stickiness is just "last button pressed". Here we assume "last alien chosen in this state".
        # Since we don't track last_action_per_state in base, we'll use a simplified heuristic:
        # If the last trial was in this state, we boost that action.
        
        bonus = self.stickiness_base + (self.anxiety_stick * self.stai)
        q_vals = self.q_stage2[state].copy()
        
        if self.last_state == state and self.last_action2 is not None:
             q_vals[self.last_action2] += bonus
             
        return self.softmax(q_vals, self.beta)
```

### policy_stage2_standard
Frequency: 1
STAI patterns: set()
Participants: ['p22']

Canonical code:
```python
    def policy_stage2(self, state: int) -> np.ndarray:
        # We can also apply stickiness to stage 2, or keep it just stage 1.
        # Often stickiness is modeled at the first stage choice. 
        # Here we apply it only to stage 1 as that's the primary strategic choice.
        return self.softmax(self.q_stage2[state], self.beta)
```

## VALUE_UPDATE
----------------------------------------

### value_update_td
Frequency: 8
STAI patterns: set()
Participants: ['p18', 'p20', 'p25', 'p33', 'p34', 'p36', 'p38', 'p41']

Canonical code:
```python
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
```

### value_update_td
Frequency: 1
STAI patterns: set()
Participants: ['p23']

Canonical code:
```python
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine which alpha to use based on reward
        # Assuming reward is binary 0 or 1
        if reward > 0.5:
            alpha = self.alpha_pos
        else:
            alpha = self.alpha_neg
            
        # Update Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        # Update Stage 1
        # Using the updated stage 2 value as the target for stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1
```

### value_update_td
Frequency: 1
STAI patterns: set()
Participants: ['p29']

Canonical code:
```python
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD update for stage 1
        # Note: We use the max of stage 2 as the target (Q-learning) to be robust
        target = np.max(self.q_stage2[state])
        delta_1 = target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
```

### value_update_td
Frequency: 1
STAI patterns: set()
Participants: ['p37']

Canonical code:
```python
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2 (used by both MB and MF systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) update for Stage 1 Model-Free values
        # Using the reward directly for the MF update (ignoring stage 2 value for simplicity in this variant)
        delta_1 = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1
```

### value_update_td
Frequency: 1
STAI patterns: set()
Participants: ['p43']

Canonical code:
```python
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Model-Free update (TD(1) / SARSA-like)
        # Using the value of the state reached (or the Q-value of the second stage choice)
        # Here we use the standard MF update: Q_MF(a1) += alpha * (Q_stage2(s, a2) - Q_MF(a1))
        # Note: We use the updated Q_stage2 value
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1
```

## POST_TRIAL
----------------------------------------

### post_trial_decay
Frequency: 1
STAI patterns: set()
Participants: ['p42']

Canonical code:
```python
    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Decay unchosen Stage 1 option
        unchosen_1 = 1 - action_1
        self.q_stage1[unchosen_1] *= (1.0 - self.decay_rate)
        
        # Decay unchosen Stage 2 option in the visited state
        unchosen_2 = 1 - action_2
        self.q_stage2[state, unchosen_2] *= (1.0 - self.decay_rate)
        
        # Note: We could also decay the unvisited state's values, but standard
        # forgetting models often focus on the active context or global decay.
        # Here we decay the unvisited state as well to represent global memory loss.
        unvisited_state = 1 - state
        self.q_stage2[unvisited_state, :] *= (1.0 - self.decay_rate)
```

### post_trial_standard
Frequency: 1
STAI patterns: set()
Participants: ['p44']

Canonical code:
```python
    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Update Habit trace: Strengthen chosen, weaken unchosen
        # We reuse self.alpha for the habit learning rate to minimize parameters
        self.habit[action_1] += self.alpha * (1 - self.habit[action_1])
        self.habit[1 - action_1] += self.alpha * (0 - self.habit[1 - action_1])
```
