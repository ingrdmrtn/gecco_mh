class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety drives perseverative behavior (choice stickiness).
    This model adds a stickiness bonus to the Stage 1 choice probabilities.
    The magnitude of this stickiness is determined by the STAI score.
    Higher anxiety leads to higher tendency to repeat the previous Stage 1 choice,
    regardless of the outcome (safety behavior).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stick_base: [-5, 5] # Base stickiness parameter
    stai_mod: [0, 10]   # How much STAI increases stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stai_mod = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective stickiness
        # stick_eff = base + (stai * modifier)
        stick_eff = self.stick_base + (self.stai * self.stai_mod)
        
        # Create a bonus vector for the previous action
        stickiness_bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            stickiness_bonus[int(self.last_action1)] = stick_eff
            
        # Add bonus to Q-values before softmax
        # Note: This doesn't change the stored Q-values, only the decision values
        decision_values = self.q_stage1 + stickiness_bonus
        
        return self.softmax(decision_values, self.beta)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)