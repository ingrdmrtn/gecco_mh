class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases choice perseveration (stickiness).
    The participant adds a 'stickiness bonus' to the action chosen in the previous trial.
    The magnitude of this bonus is directly proportional to the STAI score.
    Higher anxiety -> higher tendency to repeat choices (safety/habitual behavior).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stick_k: [0, 5]     # Scaling factor for stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_k = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy values to avoid modifying the actual Q-table
        q_vals = self.q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # Bonus = stick_k * stai
            bonus = self.stick_k * self.stai
            q_vals[int(self.last_action1)] += bonus
            
        return self.softmax(q_vals, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # We apply stickiness only to Stage 1 (spaceship choice) as that is the primary
        # strategic decision, but one could argue for stage 2 as well. 
        # Here we keep stage 2 standard value-based.
        return self.softmax(self.q_stage2[state], self.beta)

    # Uses default value_update (TD learning)

cognitive_model3 = make_cognitive_model(ParticipantModel3)