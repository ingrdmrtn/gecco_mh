class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to perseverative behavior (stickiness), 
    causing the participant to repeat choices regardless of value.
    
    The model adds a 'stickiness' bonus to the Q-values of the previously chosen action.
    The magnitude of this bonus is determined by a base parameter multiplied by STAI.
    
    Q_net(a) = Q_learned(a) + (STAI * phi * IsLastAction(a))
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    phi: [0, 5]  # Stickiness magnitude coefficient
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        # If it's the first trial, last_action1 is None, so no bonus.
        bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            # The bonus is scaled by anxiety (STAI)
            # High anxiety -> higher tendency to repeat
            stickiness_val = self.phi * self.stai
            bonus[int(self.last_action1)] = stickiness_val
            
        # Combine learned value with stickiness
        q_combined = self.q_stage1 + bonus
        
        return self.softmax(q_combined, self.beta)

    # Standard value update (Model-Free TD)
    # We use the default implementation from Base, but we need to ensure
    # unpack_parameters handles the specific params for this class.
    # Since we defined unpack_parameters above, we are good.
    # We rely on the base class value_update which uses self.alpha.

cognitive_model3 = make_cognitive_model(ParticipantModel3)