class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: The participant exhibits choice perseverance ('stickiness') that is modulated 
    by anxiety. Low anxiety participants are hypothesized to be more 'sticky' (confident in 
    their routine), while high anxiety participants might switch more often.
    
    The effective stickiness is: phi_eff = phi * (1 / (1 + stai))
    This formulation means as STAI increases, the effective stickiness decreases.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    phi: [0, 5]     # Base choice stickiness/perseverance bonus
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def init_model(self) -> None:
        # Calculate effective stickiness based on STAI
        # Low STAI -> Higher effective stickiness
        self.phi_eff = self.phi * (1.0 / (1.0 + self.stai))

    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bonus to the Q-values before softmax
        q_modified = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            q_modified[self.last_action1] += self.phi_eff
            
        return self.softmax(q_modified, self.beta)

    # Standard TD learning for value update (same as base, but explicit for clarity)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)