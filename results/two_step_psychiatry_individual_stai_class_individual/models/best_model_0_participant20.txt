class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    The participant gets a 'bonus' added to the Q-value of the action they chose
    on the previous trial. The magnitude of this bonus is determined by STAI.
    
    Q_boosted(a) = Q(a) + (stickiness_base + anxiety_stick * STAI) * I(a == last_action)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta: [0, 10]           # Inverse temperature
    stickiness_base: [0, 5] # Base tendency to repeat choices
    anxiety_stick: [0, 5]   # Additional stickiness due to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.anxiety_stick = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        bonus = self.stickiness_base + (self.anxiety_stick * self.stai)
        
        # Copy values to avoid modifying the actual learned Q-values permanently
        q_vals = self.q_stage1.copy()
        
        # Add bonus to the previously chosen action (if it exists)
        if self.last_action1 is not None:
            q_vals[self.last_action1] += bonus
            
        return self.softmax(q_vals, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # We apply stickiness to stage 2 as well, though often less relevant if states change
        # However, if they return to the same state, they might repeat the alien choice.
        # We need to track last action per state or just last action overall?
        # Usually stickiness is just "last button pressed". Here we assume "last alien chosen in this state".
        # Since we don't track last_action_per_state in base, we'll use a simplified heuristic:
        # If the last trial was in this state, we boost that action.
        
        bonus = self.stickiness_base + (self.anxiety_stick * self.stai)
        q_vals = self.q_stage2[state].copy()
        
        if self.last_state == state and self.last_action2 is not None:
             q_vals[self.last_action2] += bonus
             
        return self.softmax(q_vals, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)