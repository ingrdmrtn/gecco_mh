class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    
    This model is a standard Model-Free learner, but with an added 'stickiness' parameter.
    The stickiness bonus is added to the Q-value of the action taken in the previous trial.
    
    We hypothesize that the magnitude of this stickiness is directly proportional to anxiety.
    Stickiness_bonus = phi * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    phi: [0, 5]         # Perseveration scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is scaled by the STAI score
            stickiness_val = self.phi * self.stai
            q_values[self.last_action1] += stickiness_val
            
        return self.softmax(q_values, self.beta)

    # Standard Q-learning update (inherited logic is sufficient for value update, 
    # but we need to ensure policy_stage2 is standard or also sticky. 
    # Usually stickiness is modeled at the first stage choice in this task).
    # We will leave policy_stage2 as standard softmax over q_stage2.

cognitive_model2 = make_cognitive_model(ParticipantModel2)