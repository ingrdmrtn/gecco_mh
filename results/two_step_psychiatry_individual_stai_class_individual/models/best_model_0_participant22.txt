class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    
    The model adds a 'stickiness' bonus to the Q-values of the action taken in the 
    previous trial. The magnitude of this bonus is determined by a base stickiness 
    plus an anxiety-dependent component.
    
    Q_used(a) = Q_learned(a) + (stickiness_base + stickiness_stai * stai) * I(a == last_action)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta: [0, 10]           # Inverse temperature
    stick_base: [-2, 2]     # Base choice stickiness (positive = repeat, negative = switch)
    stick_stai: [0, 5]      # Additional stickiness due to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stick_stai = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective stickiness
        eff_stickiness = self.stick_base + (self.stick_stai * self.stai)
        
        # Copy Q-values to avoid modifying the learned values permanently
        q_with_stick = self.q_stage1.copy()
        
        # Add bonus if there was a previous action
        if self.last_action1 is not None:
            q_with_stick[int(self.last_action1)] += eff_stickiness
            
        return self.softmax(q_with_stick, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # We can also apply stickiness to stage 2, or keep it just stage 1.
        # Often stickiness is modeled at the first stage choice. 
        # Here we apply it only to stage 1 as that's the primary strategic choice.
        return self.softmax(self.q_stage2[state], self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)