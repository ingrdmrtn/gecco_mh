class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    The participant is more likely to repeat their previous stage-1 action 
    as their anxiety score increases, independent of the reward history.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    k_stick: [0, 5]    # Base stickiness magnitude
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.k_stick = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        stickiness_bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            # The bonus is scaled by STAI: Higher anxiety -> Higher tendency to repeat
            # Effective stickiness = k_stick * (1 + STAI)
            effective_k = self.k_stick * (1.0 + self.stai)
            stickiness_bonus[self.last_action1] = effective_k
            
        # Add bonus to Q-values before softmax
        return self.softmax(self.q_stage1 + stickiness_bonus, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning for values
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)