class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness). The participant 
    receives a 'bonus' added to the Q-value of the previously chosen action at Stage 1. 
    The magnitude of this bonus is directly proportional to their STAI score.
    
    Q_net(a) = Q_learned(a) + (perseveration_param * stai * is_last_action(a))

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    pers_k: [0, 5]      # Perseveration scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_k = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy learned Q-values to avoid modifying the actual value estimates
        q_biased = self.q_stage1.copy()
        
        # Apply perseveration bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is the parameter * anxiety score
            bonus = self.pers_k * self.stai
            q_biased[int(self.last_action1)] += bonus
            
        return self.softmax(q_biased, self.beta)

    # Standard Model-Free updates for value_update (inherited from Base or default logic)
    # We use the default value_update from the base class which is standard TD.

cognitive_model3 = make_cognitive_model(ParticipantModel3)