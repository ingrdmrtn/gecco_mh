class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Amplified "Win-Stay" Bias.
    
    This model posits that anxiety specifically amplifies the "Win-Stay" heuristic.
    While standard RL increases the value of a rewarded action, this model adds an 
    additional, temporary "bonus" to the previously chosen action if and only if 
    it resulted in a reward. This creates a stronger tendency to cling to successful 
    strategies than standard Q-learning would predict, explaining the rigidity 
    observed in the participant's data (sticking to U).
    
    Mechanism:
    If last_reward == 1:
        Q_net(last_action) = Q(last_action) + (stai * win_bonus)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    win_bonus: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.win_bonus = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_net = self.q_stage1.copy()
        
        # Apply Win-Stay bonus
        if self.last_reward == 1.0 and self.last_action1 is not None:
            bonus = self.stai * self.win_bonus
            q_net[self.last_action1] += bonus
            
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)