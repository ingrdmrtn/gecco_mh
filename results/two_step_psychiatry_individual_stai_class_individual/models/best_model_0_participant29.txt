class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    A 'stickiness' bonus is added to the Q-value of the previously chosen action.
    The magnitude of this bonus is proportional to the STAI score.
    
    Q_stage1(a) = Q_learned(a) + (stai * stickiness_param * IsLastAction(a))
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stickiness: [0, 5]  # Base stickiness magnitude
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy learned values to avoid modifying the actual Q-table for updates
        q_vals = self.q_stage1.copy()
        
        # Apply perseveration bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is the base stickiness parameter scaled by anxiety level
            bonus = self.stickiness * self.stai
            q_vals[int(self.last_action1)] += bonus
            
        return self.softmax(q_vals, self.beta)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD update for stage 1
        # Note: We use the max of stage 2 as the target (Q-learning) to be robust
        target = np.max(self.q_stage2[state])
        delta_1 = target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)