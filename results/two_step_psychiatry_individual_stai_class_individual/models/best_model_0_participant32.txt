class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Perseveration (Safety Seeking)]
    This model hypothesizes that anxiety drives a "stickiness" or perseveration 
    bias, where the participant prefers to repeat their previous Stage 1 choice 
    regardless of reward history, treating familiarity as a safety signal.
    
    The model adds a bonus to the Q-value of the previously chosen spaceship.
    The magnitude of this bonus is directly proportional to the STAI score.
    
    Q_net(a) = Q_learned(a) + (k * stai) * I(a == last_action)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Learning rate)
    beta: [0, 10]   (Inverse temperature)
    k: [0, 5]       (Perseveration strength scaling factor)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.k = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy learned values to avoid modifying the actual Q-table for learning
        q_values = self.q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            bonus = self.k * self.stai
            q_values[int(self.last_action1)] += bonus
            
        return self.softmax(q_values, self.beta)

    # policy_stage2 uses default softmax
    # value_update uses default TD learning

cognitive_model3 = make_cognitive_model(ParticipantModel3)