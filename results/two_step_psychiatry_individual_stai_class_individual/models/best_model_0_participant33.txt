class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness). 
    The participant is more likely to repeat their previous Stage 1 choice, 
    regardless of reward, as a form of safety behavior or cognitive rigidity.
    
    The choice stickiness bonus is calculated as: stickiness * stai.
    This bonus is added to the Q-value of the previously chosen action.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    stickiness: [0, 5] # Base perseveration parameter
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_modified = self.q_stage1.copy()
        
        # If a previous action exists, add a stickiness bonus scaled by anxiety
        if self.last_action1 is not None:
            # The higher the anxiety, the stronger the urge to repeat the last action
            bonus = self.stickiness * self.stai
            q_modified[int(self.last_action1)] += bonus
            
        return self.softmax(q_modified, self.beta)

    # Standard Q-learning for value updates (inherited logic is slightly modified below to be explicit)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (Model-Free TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)