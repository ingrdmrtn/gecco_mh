class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases Choice Stickiness (Perseveration).
    High STAI participants are more likely to repeat their previous Stage 1 choice
    regardless of the outcome, reflecting a 'safety behavior' or rigidity.
    
    Mechanism:
    - Standard Q-learning for value estimation.
    - Decision rule includes a 'stickiness' bonus added to the Q-value of the 
      previously chosen action.
    - The magnitude of this bonus is determined by STAI: 
      stickiness_bonus = rho * STAI

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    rho: [0, 5]         # Anxiety-scaling factor for stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rho = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy Q-values to avoid modifying the actual learned values
        decision_values = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is proportional to anxiety (STAI)
            stickiness = self.rho * self.stai
            decision_values[int(self.last_action1)] += stickiness
            
        return self.softmax(decision_values, self.beta)

    # Standard Q-learning update (inherited logic is sufficient if we just want 
    # to test the effect on the policy, but we'll implement the standard one 
    # explicitly to be safe and clear).
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)