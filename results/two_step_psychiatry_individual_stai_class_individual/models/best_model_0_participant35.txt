class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety-Driven Reward Clinging (Win-Stay Amplification).
    
    Cognitive Theory: Unlike general habit formation (which reinforces any chosen action), 
    this model posits that anxiety specifically amplifies the "Win-Stay" heuristic. 
    Anxious participants may cling excessively to a "safe" option that just provided 
    a reward, ignoring the underlying probabilistic structure.
    
    Mechanism: We add a temporary bonus to the Q-value of the previously chosen action 
    ONLY if the last trial was rewarded. This bonus is proportional to STAI.
    
    If LastReward == 1:
        P(a) ~ exp(beta * (Q(a) + cling_factor * stai))
    
    This differs from standard stickiness (which applies regardless of reward) and 
    standard learning (which is permanent). This is a transient bias towards safety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta: [0, 10]           # Inverse temperature
    cling_factor: [0, 5]    # Strength of win-stay bias due to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.cling_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy values to avoid modifying the actual learned Q-values permanently
        current_values = self.q_stage1.copy()
        
        # Apply bonus if the last trial was a "Win"
        if self.last_reward == 1.0 and self.last_action1 is not None:
            bonus = self.cling_factor * self.stai
            current_values[self.last_action1] += bonus
            
        return self.softmax(current_values, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)