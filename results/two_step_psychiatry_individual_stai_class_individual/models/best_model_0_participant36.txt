class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases choice perseveration (stickiness).
    The model adds a bonus to the Q-value of the previously chosen action at Stage 1.
    The magnitude of this bonus is: p_base + (stai * p_anxiety_slope).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    p_base: [0, 5]      # Baseline perseveration bonus
    p_slope: [0, 5]     # Additional perseveration per unit of STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.p_base, self.p_slope = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective perseveration parameter
        pers_param = self.p_base + (self.stai * self.p_slope)
        
        # Copy Q-values to avoid modifying the learned values permanently
        q_modified = self.q_stage1.copy()
        
        # Add bonus if a previous action exists
        if self.last_action1 is not None:
            q_modified[self.last_action1] += pers_param
            
        return self.softmax(q_modified, self.beta)

    # Standard TD learning for value update (Model-Free)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)