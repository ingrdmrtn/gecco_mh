class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety (STAI) modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety reduces the weight of model-based planning, leading to more habitual behavior.
    
    The mixing weight 'w' is dynamically adjusted by STAI:
    w_effective = w_base * (1 - stai)
    
    If STAI is high, w_effective drops, favoring MF control.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base mixing weight (1=Pure MB, 0=Pure MF)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (transition matrix is fixed in base class)
        # We need separate storage for MF values to combine them later
        self.q_mf = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # self.T is [2x2]: T[a, s] probability of transition
            # But base class T is [[0.7, 0.3], [0.3, 0.7]] which corresponds to actions 0 and 1
            # We assume row 0 is action A, row 1 is action U
            max_q2_s0 = np.max(self.q_stage2[0])
            max_q2_s1 = np.max(self.q_stage2[1])
            q_mb[a] = self.T[a, 0] * max_q2_s0 + self.T[a, 1] * max_q2_s1

        # 2. Modulate mixing weight by STAI
        # High anxiety (stai ~ 1) reduces w_effective towards 0 (Pure MF)
        w_effective = self.w_base * (1.0 - self.stai)
        
        # 3. Combine MB and MF values
        q_combined = w_effective * q_mb + (1 - w_effective) * self.q_mf
        
        return self.softmax(q_combined, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2 (used by both MB and MF systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) update for Stage 1 Model-Free values
        # Using the reward directly for the MF update (ignoring stage 2 value for simplicity in this variant)
        delta_1 = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)