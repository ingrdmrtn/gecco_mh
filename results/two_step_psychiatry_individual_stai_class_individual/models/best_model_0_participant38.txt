class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates choice perseveration (stickiness). 
    The participant tends to repeat their previous Stage 1 choice. The strength of this 
    perseveration bonus is determined by a base parameter plus a modulation by their STAI score.
    
    This captures the idea that anxiety might increase rigid or repetitive behavior 
    as a coping mechanism or safety signal.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    pers_base: [0, 5]   # Base perseveration bonus
    stai_pers: [0, 5]   # STAI modulation of perseveration
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_base, self.stai_pers = model_parameters

    def init_model(self) -> None:
        # Calculate effective perseveration bonus
        # P = base + (stai * factor)
        self.perseveration_bonus = self.pers_base + (self.stai * self.stai_pers)

    def policy_stage1(self) -> np.ndarray:
        # Add perseveration bonus to the Q-value of the previously chosen action
        q_modified = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            q_modified[self.last_action1] += self.perseveration_bonus
            
        return self.softmax(q_modified, self.beta)

    # Standard TD learning for value updates (same as base class, but explicit for clarity)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (Model-Free)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)