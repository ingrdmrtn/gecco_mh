class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety (STAI) acts as a cognitive load, reducing the weight (w) of model-based planning 
    in favor of model-free habits. The effective mixing weight is w_base * (1 - stai).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Baseline model-based weight before anxiety modulation
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (Q_MB) and Model-Free values (Q_MF)
        self.q_mf = np.zeros(self.n_choices)
        self.q_mb = np.zeros(self.n_choices)
        
        # Calculate effective mixing weight based on STAI
        # Higher STAI reduces the effective w
        self.w_eff = self.w_base * (1.0 - self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        for a in range(self.n_choices):
            # T[0] is transition for action 0 (A), T[1] for action 1 (U)
            # T[a][0] is prob of state 0 (X), T[a][1] is prob of state 1 (Y)
            v_state_x = np.max(self.q_stage2[0])
            v_state_y = np.max(self.q_stage2[1])
            
            # Transition matrix self.T is defined as:
            # T[0] = [0.7, 0.3] (Action 0 -> State 0 w/ 0.7)
            # T[1] = [0.3, 0.7] (Action 1 -> State 1 w/ 0.7)
            # Note: The base class defines T as [[0.7, 0.3], [0.3, 0.7]]
            # We assume row 0 corresponds to Action 0, row 1 to Action 1.
            
            self.q_mb[a] = self.T[a, 0] * v_state_x + self.T[a, 1] * v_state_y

        # Combine MF and MB values
        q_net = (1 - self.w_eff) * self.q_mf + self.w_eff * self.q_mb
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Model-Free update (TD(1) / SARSA-like)
        # Using the value of the state reached (or the Q-value of the second stage choice)
        # Here we use the standard MF update: Q_MF(a1) += alpha * (Q_stage2(s, a2) - Q_MF(a1))
        # Note: We use the updated Q_stage2 value
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)