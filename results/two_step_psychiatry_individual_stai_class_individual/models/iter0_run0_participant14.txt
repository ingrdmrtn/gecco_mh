Here are three cognitive models hypothesizing different mechanisms for how high anxiety (STAI score) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model tests the hypothesis that high anxiety shifts the balance between model-based (planning) and model-free (habitual) control. Specifically, it posits that higher anxiety reduces the weight of model-based planning (`w`), making the participant rely more on simple temporal difference learning. The STAI score is used to inversely scale the mixing parameter `w`.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety impairs model-based planning.
    This model implements a hybrid reinforcement learning agent where the balance 
    between Model-Based (MB) and Model-Free (MF) control is modulated by anxiety.
    The mixing weight 'w' (0=pure MF, 1=pure MB) is reduced as STAI increases.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Baseline model-based weight
    stai_sens: [0, 2]   # Sensitivity of w to STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.stai_sens = model_parameters

    def init_model(self) -> None:
        # Calculate the effective mixing weight based on STAI
        # Higher STAI reduces w, pushing behavior towards Model-Free
        # We clip to ensure it stays in [0, 1]
        raw_w = self.w_base - (self.stai_sens * self.stai)
        self.w = np.clip(raw_w, 0.0, 1.0)
        
        # Initialize MF values separately
        self.q_mf = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Model-Based Value Calculation:
        # Q_MB(a1) = sum(P(s2|a1) * max(Q_stage2(s2, a2)))
        q_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            for s2 in range(self.n_states):
                # Transition probability from T matrix
                prob = self.T[a1, s2]
                # Max value of next stage
                max_q2 = np.max(self.q_stage2[s2])
                q_mb[a1] += prob * max_q2

        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = self.w * q_mb + (1 - self.w) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (Standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Model-Free update (TD(1) / SARSA-like)
        # Update MF value of chosen spaceship towards the value of the state reached
        # Note: In standard 2-step, MF updates often use the stage 2 value or reward directly.
        # Here we update towards the stage 2 value of the chosen action.
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry
This model hypothesizes that anxiety affects how participants process positive versus negative prediction errors. High anxiety is often associated with a negativity bias or increased sensitivity to punishment/lack of reward. Here, the STAI score modulates the learning rate specifically for negative prediction errors (when reward is 0), potentially causing "over-learning" from failure.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases learning from negative outcomes (negativity bias).
    This model splits the learning rate into alpha_pos (for rewards) and 
    alpha_neg (for omissions). The STAI score amplifies alpha_neg, making the 
    participant react more strongly to getting 0 coins.
    
    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]  # Base learning rate
    beta: [0, 10]       # Inverse temperature
    neg_bias: [0, 5]    # Multiplier for negative learning rate based on STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.neg_bias = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective learning rate for this trial
        # If reward is positive (1), use base alpha.
        # If reward is zero (negative outcome), amplify alpha by STAI factor.
        if reward > 0:
            alpha_eff = self.alpha_base
        else:
            # Amplify learning from negative outcomes based on anxiety
            # We clip to 1.0 to maintain stability
            alpha_eff = np.clip(self.alpha_base * (1.0 + self.neg_bias * self.stai), 0.0, 1.0)

        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha_eff * delta_2
        
        # Stage 1 Update (Model-Free TD)
        # Using the updated stage 2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_eff * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration/Perseveration
This model hypothesizes that anxiety influences choice stickiness (perseveration). High anxiety might lead to "safety behavior" or rigid repetition of choices regardless of value (perseveration), or conversely, erratic switching (exploration) due to worry. This model adds a "stickiness" parameter to the softmax function, where the magnitude of stickiness is directly modulated by the STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety drives perseverative behavior (choice stickiness).
    This model adds a stickiness bonus to the Stage 1 choice probabilities.
    The magnitude of this stickiness is determined by the STAI score.
    Higher anxiety leads to higher tendency to repeat the previous Stage 1 choice,
    regardless of the outcome (safety behavior).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stick_base: [-5, 5] # Base stickiness parameter
    stai_mod: [0, 10]   # How much STAI increases stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stai_mod = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective stickiness
        # stick_eff = base + (stai * modifier)
        stick_eff = self.stick_base + (self.stai * self.stai_mod)
        
        # Create a bonus vector for the previous action
        stickiness_bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            stickiness_bonus[int(self.last_action1)] = stick_eff
            
        # Add bonus to Q-values before softmax
        # Note: This doesn't change the stored Q-values, only the decision values
        decision_values = self.q_stage1 + stickiness_bonus
        
        return self.softmax(decision_values, self.beta)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```