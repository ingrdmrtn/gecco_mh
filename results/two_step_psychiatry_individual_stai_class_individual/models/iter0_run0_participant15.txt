Here are three cognitive models hypothesizing different ways the participant's medium anxiety (STAI = 0.3375) influences their decision-making in the two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that anxiety dictates the balance between Model-Based (planning using the transition matrix) and Model-Free (habitual) control. The STAI score is used as a mixing weight: higher anxiety might lead to more habitual behavior (less cognitive resource for planning) or more model-based behavior (hyper-vigilance). Here, we model anxiety as shifting the weight `w` towards model-free control, assuming anxiety consumes working memory resources needed for model-based planning.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates the balance between Model-Based (MB) and Model-Free (MF) control.
    The participant uses a hybrid strategy where the weight 'w' determines the contribution of MB vs MF values.
    The raw mixing weight is modulated by STAI: w_effective = w_base * (1 - stai).
    This implies higher anxiety reduces model-based planning (lower w), relying more on habits.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base mixing weight (0=Pure MF, 1=Pure MB)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # MF Q-values for stage 1
        self.q_mf = np.zeros(self.n_choices)
        # MB Q-values for stage 1 (computed on the fly)
        self.q_mb = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values: Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        for a in range(self.n_choices):
            # Expected value of best action in each state
            v_states = np.max(self.q_stage2, axis=1)
            self.q_mb[a] = np.sum(self.T[a] * v_states)
            
        # Modulate mixing weight by anxiety
        # Hypothesis: Anxiety reduces MB control capacity
        w_eff = self.w_base * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0, 1)
        
        # Hybrid value
        q_net = w_eff * self.q_mb + (1 - w_eff) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) update for Stage 1 MF values (direct reinforcement from reward)
        # Note: In full hybrid models, we often use eligibility traces, but here we use a simple TD(1)-like update
        # directly updating Q_MF(a1) based on the final reward.
        delta_1 = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry
This model hypothesizes that anxiety affects how the participant learns from positive versus negative prediction errors. Specifically, individuals with anxiety might be more sensitive to negative outcomes (punishment sensitivity) or less sensitive to rewards. We split the learning rate `alpha` into `alpha_pos` and `alpha_neg`, where the STAI score amplifies the learning rate for negative prediction errors (disappointments).

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety creates an asymmetry in learning from positive vs negative prediction errors.
    The STAI score acts as a multiplier for the learning rate when the prediction error is negative.
    This reflects a 'negativity bias' where anxious individuals update their beliefs more drastically
    after worse-than-expected outcomes.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate
    beta: [0, 10]       # Inverse temperature
    neg_bias: [0, 5]    # Sensitivity multiplier for negative errors
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.neg_bias = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha based on sign of error and anxiety
        if delta_2 < 0:
            # Anxiety amplifies learning from negative errors
            # Effective alpha = alpha * (1 + neg_bias * stai)
            eff_alpha_2 = self.alpha * (1.0 + self.neg_bias * self.stai)
        else:
            eff_alpha_2 = self.alpha
            
        eff_alpha_2 = np.clip(eff_alpha_2, 0, 1)
        self.q_stage2[state, action_2] += eff_alpha_2 * delta_2
        
        # Stage 1 Update (TD-0)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 < 0:
            eff_alpha_1 = self.alpha * (1.0 + self.neg_bias * self.stai)
        else:
            eff_alpha_1 = self.alpha
            
        eff_alpha_1 = np.clip(eff_alpha_1, 0, 1)
        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Choice Stickiness
This model hypothesizes that anxiety leads to "safety behavior" or perseveration (stickiness). Instead of purely value-based choices, the participant has a tendency to repeat the previous choice regardless of the outcome, as a way to reduce cognitive load or uncertainty. The STAI score determines the magnitude of this stickiness bonus added to the Q-values of the previously chosen action.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases choice perseveration (stickiness).
    The participant adds a 'stickiness bonus' to the action chosen in the previous trial.
    The magnitude of this bonus is directly proportional to the STAI score.
    Higher anxiety -> higher tendency to repeat choices (safety/habitual behavior).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stick_k: [0, 5]     # Scaling factor for stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_k = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy values to avoid modifying the actual Q-table
        q_vals = self.q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # Bonus = stick_k * stai
            bonus = self.stick_k * self.stai
            q_vals[int(self.last_action1)] += bonus
            
        return self.softmax(q_vals, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # We apply stickiness only to Stage 1 (spaceship choice) as that is the primary
        # strategic decision, but one could argue for stage 2 as well. 
        # Here we keep stage 2 standard value-based.
        return self.softmax(self.q_stage2[state], self.beta)

    # Uses default value_update (TD learning)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```