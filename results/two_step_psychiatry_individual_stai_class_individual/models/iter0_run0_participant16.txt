Here are three cognitive models that hypothesize different ways the participant's high anxiety (STAI = 0.5625) influences their decision-making process.

### Model 1: Anxiety-Driven Perseveration (Stickiness)
This model hypothesizes that high anxiety leads to "safety-seeking" or rigid behavior, manifesting as a tendency to repeat the previous choice regardless of the outcome (perseveration). The model assumes the participant is primarily a Model-Free learner (common in high stress), but adds a "stickiness" bonus to the Stage 1 choice that is directly scaled by their STAI score.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Perseveration]
    This model hypothesizes that high anxiety increases choice perseveration (stickiness).
    The participant is modeled as a Model-Free learner (TD learning), but their 
    probability of repeating the last Stage 1 action is boosted by a stickiness 
    parameter that is scaled by their STAI score.
    
    Mechanism:
    Q_net(a) = Q_MF(a) + (stai * stickiness_factor * IsLastAction(a))

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate
    beta: [0, 10] - Inverse temperature
    stick_factor: [0, 5] - Scaling factor for anxiety-driven stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Base Model-Free values
        q_values = self.q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is proportional to the anxiety score (stai)
            # High anxiety -> High tendency to repeat
            bonus = self.stai * self.stick_factor
            q_values[int(self.last_action1)] += bonus
            
        return self.softmax(q_values, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Impaired Model-Based Planning
This model tests the hypothesis that anxiety consumes cognitive resources, impairing the ability to engage in complex "Model-Based" planning (using the transition matrix). The participant uses a hybrid strategy, but the weight ($w$) assigned to the Model-Based component is inversely proportional to their STAI score. A high STAI score forces the model to rely more on Model-Free (habitual) learning.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Impaired Model-Based Planning]
    This model hypothesizes that anxiety acts as a cognitive load, reducing the 
    participant's ability to use the transition structure (Model-Based control).
    The participant mixes Model-Based (MB) and Model-Free (MF) values.
    The mixing weight 'w' for MB is reduced as STAI increases.

    Mechanism:
    w = w_max * (1 - stai)
    Q_net = w * Q_MB + (1 - w) * Q_MF

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate
    beta: [0, 10] - Inverse temperature
    w_max: [0, 1] - Maximum model-based weight (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values (Planning)
        # Q_MB(a) = Sum(P(s'|a) * max(Q_stage2(s', :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            expected_val = 0
            for s_prime in range(self.n_states):
                # Transition probability T[action, next_state]
                # Note: self.T is defined as [[0.7, 0.3], [0.3, 0.7]]
                # where row is action, col is state probability
                prob = self.T[a, s_prime]
                # Value of the state is max Q of that state
                val_s_prime = np.max(self.q_stage2[s_prime])
                expected_val += prob * val_s_prime
            q_mb[a] = expected_val

        # 2. Calculate Mixing Weight based on STAI
        # Higher anxiety -> Lower w -> Less Model-Based influence
        w = self.w_max * (1.0 - self.stai)
        
        # Ensure w stays in bounds [0, 1] just in case
        w = np.clip(w, 0.0, 1.0)

        # 3. Combine MF (self.q_stage1) and MB
        q_net = w * q_mb + (1 - w) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Loss Sensitivity
This model hypothesizes that high anxiety makes the participant hypersensitive to negative outcomes (omission of reward). The model assumes a simple Model-Free learning mechanism, but the learning rate ($\alpha$) is asymmetric. Specifically, the learning rate for negative prediction errors (disappointment) is amplified by the STAI score, causing the participant to abandon choices quickly after failure.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Loss Sensitivity]
    This model hypothesizes that high anxiety leads to over-learning from negative outcomes.
    The learning rate is asymmetric: a base rate is used for rewards, but for 
    negative prediction errors (losses), the learning rate is boosted by the STAI score.

    Mechanism:
    If delta < 0: alpha_eff = alpha_base * (1 + sensitivity * stai)
    Else:         alpha_eff = alpha_base

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1] - Base learning rate for positive outcomes
    beta: [0, 10] - Inverse temperature
    sensitivity: [0, 5] - Multiplier for anxiety's effect on loss learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha for Stage 2
        if delta_2 < 0:
            # Boost learning from negative errors based on anxiety
            alpha_2 = self.alpha_base * (1.0 + self.sensitivity * self.stai)
            alpha_2 = np.clip(alpha_2, 0.0, 1.0) # Keep within bounds
        else:
            alpha_2 = self.alpha_base
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (TD update using Stage 2 value)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine effective alpha for Stage 1
        if delta_1 < 0:
            alpha_1 = self.alpha_base * (1.0 + self.sensitivity * self.stai)
            alpha_1 = np.clip(alpha_1, 0.0, 1.0)
        else:
            alpha_1 = self.alpha_base

        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```