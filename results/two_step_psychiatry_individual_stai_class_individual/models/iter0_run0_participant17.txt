Here are three cognitive models designed to capture the behavior of a high-anxiety participant (STAI = 0.5375) in a two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that high anxiety (STAI) shifts the balance between model-based (planning) and model-free (habitual) control. High anxiety is often associated with reduced cognitive flexibility and increased reliance on habits. Here, the mixing weight `w` (controlling the trade-off between MB and MF values) is directly modulated by the STAI score.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety (STAI) reduces model-based planning, leading to a 
    dominance of model-free reinforcement learning.
    
    This model implements a hybrid Model-Based (MB) / Model-Free (MF) reinforcement learning agent.
    The weighting parameter `w` determines the balance: w=1 is pure MB, w=0 is pure MF.
    We hypothesize that `w` is inversely proportional to STAI score: higher anxiety -> lower w (more MF).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    w_base: [0, 1]  # Baseline mixing weight (before anxiety modulation)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based transition counts (start with uniform prior)
        # self.T is already defined in base, but we might want to learn it or use fixed.
        # Standard 2-step models often assume fixed transition knowledge or learn it.
        # Here we assume fixed knowledge of the transition matrix self.T as defined in base.
        pass

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Value (TD0)
        q_mf = self.q_stage1
        
        # 2. Model-Based Value (Bellman equation using known T and current Q2)
        # Q_MB(a1) = sum(P(s|a1) * max(Q2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # T[a, s] is prob of transitioning to state s given action a (row-stochastic usually)
            # In base class: self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
            # This implies row 0 is action 0, row 1 is action 1.
            q_mb[a] = self.T[a, 0] * np.max(self.q_stage2[0]) + \
                      self.T[a, 1] * np.max(self.q_stage2[1])
        
        # 3. Anxiety Modulation of Mixing Weight
        # High STAI reduces w. We model this as w = w_base * (1 - STAI)
        # If STAI is high (~0.54), w is roughly half of w_base.
        w_eff = self.w_base * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0.0, 1.0)
        
        # Combined Net Value
        q_net = w_eff * q_mb + (1 - w_eff) * q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) / SARSA-like update for Stage 1 Model-Free value
        # Using the actual reward obtained at stage 2 to update stage 1 directly
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Punishment Sensitivity
This model hypothesizes that high anxiety makes participants hypersensitive to negative outcomes (punishment) or lack of reward. Instead of a single learning rate, the model uses separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The STAI score amplifies the learning rate for negative errors.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases sensitivity to negative prediction errors.
    
    This is a pure Model-Free agent, but with dual learning rates.
    alpha_pos: Learning rate for positive prediction errors.
    alpha_neg_base: Base learning rate for negative prediction errors.
    
    The effective negative learning rate is boosted by STAI:
    alpha_neg_eff = alpha_neg_base + (STAI * 0.5)
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]
    alpha_neg_base: [0, 0.5] # Kept lower to allow STAI to boost it without overflowing 1
    beta: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate based on anxiety
        # High anxiety -> faster learning from disappointment
        alpha_neg_eff = self.alpha_neg_base + (self.stai * 0.5)
        alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)

        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += alpha_neg_eff * delta_2
        
        # --- Stage 1 Update ---
        # Using the updated Q2 value as the target for Q1 (TD-learning)
        target_val = self.q_stage2[state, action_2]
        delta_1 = target_val - self.q_stage1[action_1]
        
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += alpha_neg_eff * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Choice Stickiness (Perseveration)
This model hypothesizes that anxiety leads to "freezing" or repetitive behavior (perseveration) regardless of reward outcomes. This is modeled as a "stickiness" parameter that biases the agent to repeat the previous Stage 1 choice. The magnitude of this stickiness is scaled by the STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to perseverative behavior (stickiness), 
    causing the participant to repeat choices regardless of value.
    
    The model adds a 'stickiness' bonus to the Q-values of the previously chosen action.
    The magnitude of this bonus is determined by a base parameter multiplied by STAI.
    
    Q_net(a) = Q_learned(a) + (STAI * phi * IsLastAction(a))
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    phi: [0, 5]  # Stickiness magnitude coefficient
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        # If it's the first trial, last_action1 is None, so no bonus.
        bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            # The bonus is scaled by anxiety (STAI)
            # High anxiety -> higher tendency to repeat
            stickiness_val = self.phi * self.stai
            bonus[int(self.last_action1)] = stickiness_val
            
        # Combine learned value with stickiness
        q_combined = self.q_stage1 + bonus
        
        return self.softmax(q_combined, self.beta)

    # Standard value update (Model-Free TD)
    # We use the default implementation from Base, but we need to ensure
    # unpack_parameters handles the specific params for this class.
    # Since we defined unpack_parameters above, we are good.
    # We rely on the base class value_update which uses self.alpha.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```