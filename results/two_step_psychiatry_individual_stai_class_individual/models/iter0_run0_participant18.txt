Here are three cognitive models designed to capture different hypotheses about how a low-anxiety participant might perform in this task, particularly focusing on their strong preference for one option (Spaceship A).

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that even low-anxiety participants use a mixture of model-based (planning) and model-free (habitual) learning. Since the participant has low anxiety, this model tests the idea that lower anxiety leads to a higher mixing weight `w` (more model-based control), allowing them to maintain a consistent strategy if they believe it is optimal, rather than switching erratically due to noise. The STAI score modulates the mixing weight `w`.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: This participant uses a hybrid reinforcement learning strategy where the balance 
    between Model-Based (MB) and Model-Free (MF) control is modulated by their anxiety level.
    Low anxiety (low STAI) is hypothesized to correlate with higher Model-Based control (higher w),
    allowing for more stable, goal-directed behavior (sticking to A if it yields rewards).
    
    The mixing weight 'w' is calculated as: w = w_base + (w_mod * (1 - stai))
    This means lower STAI scores boost the model-based contribution.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base mixing weight
    w_mod: [0, 1]       # Modulation strength of STAI on mixing weight
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_mod = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Free (MF) and Model-Based (MB) values
        self.q_mf = np.zeros(self.n_choices)
        self.q_mb = np.zeros(self.n_choices)
        
        # Calculate the mixing weight based on STAI
        # We clamp it between 0 and 1
        raw_w = self.w_base + (self.w_mod * (1.0 - self.stai))
        self.w = np.clip(raw_w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Combine MB and MF values
        q_net = (self.w * self.q_mb) + ((1 - self.w) * self.q_mf)
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 values (Model-Free) - standard TD
        # Note: self.q_stage2 is used as the second-stage value store for both MB and MF calculations
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Update Stage 1 Model-Free values (TD(1) / SARSA-like)
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1
        
        # 3. Update Stage 1 Model-Based values (Bellman equation)
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, a2)))
        for a in range(self.n_choices):
            val_s0 = np.max(self.q_stage2[0])
            val_s1 = np.max(self.q_stage2[1])
            self.q_mb[a] = (self.T[a, 0] * val_s0) + (self.T[a, 1] * val_s1)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Choice Stickiness
This model hypothesizes that the participant's behavior (overwhelmingly choosing 'A') is driven by a "stickiness" or perseverance heuristic, which is modulated by anxiety. Low anxiety might be associated with higher confidence or "stickiness" to a chosen path (positive perseverance), whereas high anxiety might lead to switching. Here, the STAI score inversely scales the stickiness parameter: lower anxiety = higher tendency to repeat the previous choice regardless of reward.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: The participant exhibits choice perseverance ('stickiness') that is modulated 
    by anxiety. Low anxiety participants are hypothesized to be more 'sticky' (confident in 
    their routine), while high anxiety participants might switch more often.
    
    The effective stickiness is: phi_eff = phi * (1 / (1 + stai))
    This formulation means as STAI increases, the effective stickiness decreases.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    phi: [0, 5]     # Base choice stickiness/perseverance bonus
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def init_model(self) -> None:
        # Calculate effective stickiness based on STAI
        # Low STAI -> Higher effective stickiness
        self.phi_eff = self.phi * (1.0 / (1.0 + self.stai))

    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bonus to the Q-values before softmax
        q_modified = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            q_modified[self.last_action1] += self.phi_eff
            
        return self.softmax(q_modified, self.beta)

    # Standard TD learning for value update (same as base, but explicit for clarity)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Learning Rate Asymmetry
This model hypothesizes that anxiety affects how participants learn from positive versus negative prediction errors. A low-anxiety participant might be more balanced or even biased towards learning from positive outcomes (optimism), reinforcing the choice of 'A' when it works, and ignoring when it doesn't. High anxiety is often associated with hypersensitivity to punishment. We model this by splitting the learning rate `alpha` into `alpha_pos` and `alpha_neg`, where the ratio is modulated by STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates the asymmetry between learning from positive vs negative 
    prediction errors. 
    
    We define a base learning rate 'alpha'.
    We define a bias parameter 'k' modulated by STAI.
    
    If STAI is low, we hypothesize a bias towards positive learning (optimism):
    alpha_pos = alpha
    alpha_neg = alpha * stai  (Low STAI -> very small alpha_neg -> ignore failures)
    
    This could explain why the participant sticks with 'A' even if it occasionally fails; 
    they learn strongly from the successes and discount the failures.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Base learning rate
    beta: [0, 10]   # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta = model_parameters

    def init_model(self) -> None:
        # Define asymmetric learning rates based on STAI
        # Low STAI reduces the impact of negative prediction errors
        self.alpha_pos = self.alpha
        self.alpha_neg = self.alpha * self.stai 

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        pe2 = reward - self.q_stage2[state, action_2]
        lr2 = self.alpha_pos if pe2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += lr2 * pe2
        
        # Stage 1 Update
        # We use the updated stage 2 value to drive stage 1
        pe1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        lr1 = self.alpha_pos if pe1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += lr1 * pe1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```