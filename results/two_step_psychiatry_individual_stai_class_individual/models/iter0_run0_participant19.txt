Here are 3 cognitive models hypothesizing different mechanisms for how medium anxiety (STAI ~0.4) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that anxiety levels control the balance between Model-Based (planning) and Model-Free (habitual) learning. A medium anxiety score might reflect a mixed strategy where the participant relies partially on the transition structure (Model-Based) but is also driven by simple reward history (Model-Free). The mixing weight `w` is derived directly from the STAI score.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the balance between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety (STAI) leads to reduced Model-Based planning (lower w).
    
    The mixing weight 'w' determines the contribution of MB vs MF values to the stage 1 choice.
    w = logistic(w_base - w_stai_slope * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [-5, 5]     # Base logit for mixing weight
    w_stai_slope: [0, 5]# Sensitivity of mixing weight to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_stai_slope = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (Q_MB)
        # Q_MB is computed on the fly, but we need storage for MF values
        self.q_mf = np.zeros(self.n_choices)
        
        # Calculate the mixing weight based on STAI once
        # Sigmoid transformation to keep w in [0, 1]
        logit = self.w_base - (self.w_stai_slope * self.stai)
        self.w = 1.0 / (1.0 + np.exp(-logit))

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values: Q_MB(a1) = sum(T(a1, s) * max(Q2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            expected_val = 0
            for s in range(self.n_states):
                # Max value in stage 2 state s
                max_q2 = np.max(self.q_stage2[s])
                expected_val += self.T[a1, s] * max_q2
            q_mb[a1] = expected_val
            
        # Hybrid value: w * Q_MB + (1-w) * Q_MF
        q_net = self.w * q_mb + (1 - self.w) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Model-Free update for Stage 1 (TD(1) style using reward directly)
        # Note: In pure MF, we often update Q1 based on Q2, but here we simplify to direct reward
        # to distinguish it clearly from the MB calculation which uses the transition matrix.
        delta_1 = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that anxiety specifically amplifies the impact of negative outcomes (losses). The participant is "loss averse" in their learning process. When a negative reward is received, the learning rate is boosted by a factor proportional to their STAI score, causing them to flee from punishing options faster than they approach rewarding ones.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases sensitivity to negative outcomes (Loss Aversion in Learning).
    When the reward is negative (or zero, interpreted as lack of gain), the learning rate is 
    scaled up by a factor derived from STAI.
    
    Effective alpha = alpha_base * (1 + loss_sensitivity * STAI) if prediction error is negative.
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]   # Learning rate for positive prediction errors
    loss_mult: [0, 10]  # Multiplier for negative prediction errors based on STAI
    beta: [0, 10]       # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.loss_mult, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine learning rate based on sign of prediction error
        if delta_2 < 0:
            # Anxiety amplifies learning from disappointment
            alpha_2 = self.alpha_pos * (1 + self.loss_mult * self.stai)
            # Cap alpha at 1.0
            alpha_2 = min(alpha_2, 1.0)
        else:
            alpha_2 = self.alpha_pos
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (TD learning from Stage 2 value)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 < 0:
            alpha_1 = self.alpha_pos * (1 + self.loss_mult * self.stai)
            alpha_1 = min(alpha_1, 1.0)
        else:
            alpha_1 = self.alpha_pos
            
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression
This model hypothesizes that anxiety reduces exploration. Anxious individuals might stick to "safe" or known options rather than exploring uncertain ones. Here, the inverse temperature parameter `beta` (which controls the exploit/explore trade-off) is directly modulated by the STAI score. Higher anxiety leads to a higher `beta` (more deterministic/rigid choices).

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety suppresses exploration, leading to more deterministic choice behavior.
    The inverse temperature (beta) is a linear function of STAI.
    
    beta_effective = beta_base + (beta_stai_slope * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]          # Learning rate
    beta_base: [0, 10]     # Baseline exploitation
    beta_stai_slope: [0, 10] # Increase in exploitation per unit of anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.beta_stai_slope = model_parameters

    def init_model(self) -> None:
        # Calculate effective beta once
        self.beta_eff = self.beta_base + (self.beta_stai_slope * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_eff)
        
    # Uses default value_update (TD learning) from base class

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```