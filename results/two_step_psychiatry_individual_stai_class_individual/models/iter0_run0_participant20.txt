Here are three cognitive models designed to capture different hypotheses about how high anxiety (STAI = 0.775) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
**Hypothesis:** High anxiety impairs model-based planning (goal-directed behavior) and promotes reliance on model-free (habitual) learning. This model implements a hybrid reinforcement learning agent where the mixing weight `w` (balance between model-based and model-free control) is directly modulated by the participant's STAI score. A higher STAI reduces `w`, pushing the agent towards pure model-free learning.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety reduces the cognitive resources available for model-based 
    planning. This model is a hybrid learner where the weight (w) given to the 
    model-based system is inversely proportional to the STAI score.
    
    The Stage 1 value is a weighted sum: Q_net = w * Q_MB + (1-w) * Q_MF.
    High STAI -> Low w -> Dominance of Model-Free learning.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Baseline model-based weight for a theoretical 0-anxiety agent
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Free Q-values for Stage 1
        self.q_mf = np.zeros(self.n_choices)
        # Initialize Model-Based Q-values (calculated on the fly usually, but we store for clarity)
        self.q_mb = np.zeros(self.n_choices)
        
        # Calculate the effective mixing weight based on STAI
        # We assume anxiety linearly degrades the baseline weight.
        # If STAI is 1.0, w becomes very small.
        self.w_effective = self.w_base * (1.0 - self.stai)
        
        # Ensure w stays in [0, 1]
        self.w_effective = np.clip(self.w_effective, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values
        # Q_MB(a) = sum(P(s'|a) * max(Q_stage2(s', :)))
        for action in range(self.n_choices):
            expected_val = 0
            for state in range(self.n_states):
                # Transition probability T[action, state]
                # Max value of next stage Q_stage2[state]
                expected_val += self.T[action, state] * np.max(self.q_stage2[state])
            self.q_mb[action] = expected_val
            
        # Combine MF and MB values
        q_net = self.w_effective * self.q_mb + (1 - self.w_effective) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 values (Standard Q-learning)
        # Q2(s, a2) += alpha * (r - Q2(s, a2))
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Update Stage 1 Model-Free values (TD-learning)
        # We use the value of the state reached (Q_stage2[state, action_2]) as the target
        # This is SARSA-like or Q(lambda=0)
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Loss Aversion
**Hypothesis:** High anxiety participants are hypersensitive to negative outcomes (or lack of reward). Instead of treating a reward of 0.0 neutrally, they perceive it as a "loss" or a punishment. This model introduces a `loss_sensitivity` parameter that scales with STAI. When a reward of 0 is received, the prediction error is amplified negatively, causing faster unlearning of choices that lead to failure.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases sensitivity to negative outcomes (omission of reward).
    When reward is 0, the effective reward signal is treated as negative, scaled by STAI.
    
    Effective Reward = Reward if Reward > 0
    Effective Reward = -1 * sensitivity * STAI if Reward == 0
    
    This causes the agent to actively avoid options that yielded nothing, rather than just 
    extinguishing the value slowly.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    sensitivity: [0, 5] # Multiplier for how much STAI affects loss perception
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Transform the reward based on anxiety hypothesis
        if reward == 0:
            effective_reward = -1.0 * self.sensitivity * self.stai
        else:
            effective_reward = reward

        # Standard TD updates using the effective_reward
        
        # Stage 2 update
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update
        # Note: We use the updated Q2 value to drive Q1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced Exploration Noise (Inverse Temperature Modulation)
**Hypothesis:** High anxiety leads to inconsistent decision-making or "noise" rather than strategic exploration. In this model, the STAI score directly degrades the inverse temperature parameter (`beta`). A high STAI lowers the effective beta, flattening the softmax curve and making choices more random (higher entropy), regardless of the learned values. This reflects the "choking under pressure" or cognitive interference aspect of anxiety.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety acts as cognitive noise, reducing the ability to deterministically 
    select the best option. The effective inverse temperature (beta) is reduced by the 
    STAI score.
    
    beta_effective = beta_max / (1 + noise_factor * STAI)
    
    Higher STAI -> Lower beta_effective -> More random choices (higher entropy).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_max: [0, 20]   # Maximum theoretical precision (for STAI=0)
    noise_factor: [0, 10] # How strongly STAI degrades precision
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_max, self.noise_factor = model_parameters
        
        # Calculate the static effective beta for this participant
        # We model anxiety as a divisor that increases temperature (lowers beta)
        self.beta_effective = self.beta_max / (1.0 + self.noise_factor * self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Use the anxiety-modulated beta
        return self.softmax(self.q_stage1, self.beta_effective)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Use the anxiety-modulated beta
        return self.softmax(self.q_stage2[state], self.beta_effective)

    # Standard value update (TD learning)
    # We keep the learning mechanism standard to isolate the effect on choice selection
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```