Here are three cognitive models hypothesizing different ways the participant's medium anxiety level (STAI = 0.4125) influences their decision-making in the two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that anxiety levels control the balance between Model-Based (planning) and Model-Free (habitual) learning. Specifically, it posits that higher anxiety (STAI) reduces the reliance on model-based planning (which requires cognitive effort) and increases reliance on simple model-free reinforcement, perhaps due to cognitive load or stress.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    
    This model implements a hybrid reinforcement learning agent.
    - Model-Free (MF): Updates Q-values based on direct reward experience (TD learning).
    - Model-Based (MB): Computes Q-values by planning using the transition matrix T.
    
    The mixing weight `w` determines the contribution of MB vs MF systems to the final choice.
    We hypothesize that `w` is a function of the STAI score: w = w_base * (1 - stai).
    This implies that higher anxiety reduces model-based planning.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base weight for Model-Based control (before anxiety modulation)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Calculate the effective mixing weight based on STAI
        # Higher STAI -> Lower w -> Less Model-Based, More Model-Free
        # We clamp it to ensure it stays valid [0, 1]
        raw_w = self.w_base * (1.0 - self.stai)
        self.w = np.clip(raw_w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Model-Free Value (from standard Q-learning)
        q_mf = self.q_stage1
        
        # Model-Based Value (Bellman equation using transition matrix T)
        # Q_MB(a1) = sum_s2 [ P(s2|a1) * max_a2 Q(s2, a2) ]
        # We use the max of stage 2 values as the estimate of the state value
        v_stage2 = np.max(self.q_stage2, axis=1) 
        q_mb = self.T @ v_stage2 # Matrix multiplication: (2x2) @ (2,) -> (2,)
        
        # Integrated Value
        q_net = self.w * q_mb + (1 - self.w) * q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for Stage 2 (Model-Free)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Standard TD update for Stage 1 (Model-Free)
        # Note: In a pure hybrid model, MF updates happen regardless of MB planning
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Perseveration (Stickiness)
This model hypothesizes that anxiety increases "stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of the outcome. This is often observed in anxious individuals as a safety behavior or rigidity. The model adds a bonus to the Q-value of the previously chosen action, where the magnitude of this bonus scales with the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    
    This model is a standard Model-Free learner, but with an added 'stickiness' parameter.
    The stickiness bonus is added to the Q-value of the action taken in the previous trial.
    
    We hypothesize that the magnitude of this stickiness is directly proportional to anxiety.
    Stickiness_bonus = phi * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    phi: [0, 5]         # Perseveration scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is scaled by the STAI score
            stickiness_val = self.phi * self.stai
            q_values[self.last_action1] += stickiness_val
            
        return self.softmax(q_values, self.beta)

    # Standard Q-learning update (inherited logic is sufficient for value update, 
    # but we need to ensure policy_stage2 is standard or also sticky. 
    # Usually stickiness is modeled at the first stage choice in this task).
    # We will leave policy_stage2 as standard softmax over q_stage2.

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Learning Rate Asymmetry
This model hypothesizes that anxiety affects how participants learn from positive versus negative prediction errors. Specifically, anxious individuals might be more sensitive to negative outcomes (or lack of reward) than positive ones. This model splits the learning rate into `alpha_pos` and `alpha_neg`, and modulates the negative learning rate based on STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety amplifies learning from negative prediction errors.
    
    This model uses separate learning rates for positive and negative prediction errors.
    - alpha_pos: Base learning rate for positive errors (better than expected).
    - alpha_neg: Learning rate for negative errors (worse than expected).
    
    We hypothesize that `alpha_neg` is boosted by the STAI score, reflecting a 
    negativity bias common in anxiety.
    alpha_neg_effective = alpha_base + (stai * alpha_boost)
    
    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]  # Base learning rate
    alpha_boost: [0, 1] # How much STAI increases sensitivity to negative errors
    beta: [0, 10]       # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.alpha_boost, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective learning rates
        # alpha_pos is just the base rate
        alpha_pos = self.alpha_base
        
        # alpha_neg is enhanced by anxiety
        # We clip to ensure it doesn't exceed 1
        alpha_neg = np.clip(self.alpha_base + (self.stai * self.alpha_boost), 0.0, 1.0)

        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += alpha_neg * delta_2
        
        # --- Stage 1 Update ---
        # Using the updated stage 2 value to drive stage 1 update (TD-1 style)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += alpha_neg * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```