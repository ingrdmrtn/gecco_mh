Here are three cognitive models hypothesizing different mechanisms for how anxiety (STAI) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Balance
This model hypothesizes that anxiety levels shift the balance between Model-Based (planning using the transition matrix) and Model-Free (habitual) control. The participant has a "medium" anxiety score (0.4125). The hypothesis is that higher anxiety might impair model-based planning (which is cognitively demanding) or, conversely, increase reliance on it to avoid uncertainty. Here, we model anxiety as modulating the mixing weight `w` between MB and MF values.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the balance between Model-Based (MB) and Model-Free (MF) control.
    
    This model implements a hybrid reinforcement learning agent.
    - Model-Free (MF) values are learned via TD(lambda).
    - Model-Based (MB) values are computed using the transition matrix T and stage-2 values.
    - The final stage-1 value is a weighted mix: Q_net = w * Q_MB + (1-w) * Q_MF.
    
    The mixing weight 'w' is modulated by the STAI score. We assume a baseline 'w_base'
    which is shifted by anxiety: w = sigmoid(w_base + w_stai_slope * stai).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    elig_trace: [0, 1]  # Eligibility trace decay (lambda)
    w_base: [-5, 5]     # Baseline mixing weight (logit space)
    w_stai_slope: [-5, 5] # Effect of STAI on mixing weight
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.elig_trace, self.w_base, self.w_stai_slope = model_parameters

    def init_model(self) -> None:
        # Separate Q-values for MF and MB
        self.q_mf_stage1 = np.zeros(self.n_choices)
        self.q_mf_stage2 = np.zeros((self.n_states, self.n_choices))
        # MB values are computed on the fly, but we need a place to store the net Q
        self.q_net_stage1 = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum_s2 [ P(s2|a1) * max_a2 Q_MF_stage2(s2, a2) ]
        q_mb_stage1 = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # Transition probabilities for action a
            probs = self.T[a] 
            # Max value of next stage states
            max_vals = np.max(self.q_mf_stage2, axis=1)
            q_mb_stage1[a] = np.dot(probs, max_vals)

        # 2. Calculate mixing weight w based on STAI
        # Using a sigmoid to keep w in [0, 1]
        logit_w = self.w_base + self.w_stai_slope * self.stai
        w = 1.0 / (1.0 + np.exp(-logit_w))

        # 3. Combine
        self.q_net_stage1 = w * q_mb_stage1 + (1 - w) * self.q_mf_stage1
        
        return self.softmax(self.q_net_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Stage 2 is purely model-free in standard 2-step tasks
        return self.softmax(self.q_mf_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD(lambda) updates for Model-Free system
        
        # Stage 2 RPE
        delta_2 = reward - self.q_mf_stage2[state, action_2]
        self.q_mf_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 RPE (using stage 2 value as target)
        delta_1 = self.q_mf_stage2[state, action_2] - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * delta_1
        
        # Eligibility trace update: allow the stage 2 outcome to further update stage 1 choice
        self.q_mf_stage1[action_1] += self.alpha * self.elig_trace * delta_2

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Learning Rate Asymmetry
This model hypothesizes that anxiety affects how participants learn from positive versus negative outcomes. Specifically, anxious individuals might be more sensitive to punishment (or lack of reward) than to reward. This model implements separate learning rates for positive prediction errors (`alpha_pos`) and negative prediction errors (`alpha_neg`), where the ratio or magnitude of these rates is scaled by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces an asymmetry in learning from positive vs negative prediction errors.
    
    The model uses two base learning rates: alpha_pos (for RPE > 0) and alpha_neg (for RPE < 0).
    The STAI score amplifies the learning rate for negative errors, reflecting a 'negativity bias'
    common in anxiety.
    
    Effective alpha_neg = alpha_neg_base * (1 + stai_sensitivity * stai)
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]       # Learning rate for positive RPEs
    alpha_neg_base: [0, 1]  # Base learning rate for negative RPEs
    beta: [0, 10]           # Inverse temperature
    stai_sensitivity: [0, 5]# How strongly STAI amplifies negative learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta, self.stai_sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate
        # We clip it to ensure it doesn't explode beyond 1.0 too easily, though bounds help.
        eff_alpha_neg = min(1.0, self.alpha_neg_base * (1.0 + self.stai_sensitivity * self.stai))
        
        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 >= 0 else eff_alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # --- Stage 1 Update ---
        # We use the updated stage 2 value to drive stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else eff_alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced Choice Perseveration (Stickiness)
This model hypothesizes that anxiety leads to rigid behavior or "stickiness" (perseveration), regardless of reward outcomes. Anxious participants might prefer the familiar (repeating the last choice) to avoid the uncertainty of switching. Here, the STAI score directly scales a "stickiness" parameter that adds a bonus to the previously chosen action in the softmax function.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    
    The model adds a 'stickiness' bonus to the Q-values of the action taken in the 
    previous trial. The magnitude of this bonus is determined by a base stickiness 
    plus an anxiety-dependent component.
    
    Q_used(a) = Q_learned(a) + (stickiness_base + stickiness_stai * stai) * I(a == last_action)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta: [0, 10]           # Inverse temperature
    stick_base: [-2, 2]     # Base choice stickiness (positive = repeat, negative = switch)
    stick_stai: [0, 5]      # Additional stickiness due to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stick_stai = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective stickiness
        eff_stickiness = self.stick_base + (self.stick_stai * self.stai)
        
        # Copy Q-values to avoid modifying the learned values permanently
        q_with_stick = self.q_stage1.copy()
        
        # Add bonus if there was a previous action
        if self.last_action1 is not None:
            q_with_stick[int(self.last_action1)] += eff_stickiness
            
        return self.softmax(q_with_stick, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # We can also apply stickiness to stage 2, or keep it just stage 1.
        # Often stickiness is modeled at the first stage choice. 
        # Here we apply it only to stage 1 as that's the primary strategic choice.
        return self.softmax(self.q_stage2[state], self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```