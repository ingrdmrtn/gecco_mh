```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Hybrid Model (MB/MF).
    
    This model assumes the participant uses a mixture of Model-Based (MB) and 
    Model-Free (MF) strategies. The core hypothesis is that anxiety consumes 
    cognitive resources, reducing the weight of the computationally expensive 
    Model-Based system.
    
    The mixing weight 'w' determines the balance: Q_net = w * Q_MB + (1-w) * Q_MF.
    The effective weight is modulated by STAI: w_eff = w_max * (1 - stai).
    Higher anxiety (STAI) leads to a lower contribution of the Model-Based system.
    
    The model also includes a perseveration parameter to account for the observed stickiness.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        - Learning rate for MF values
    beta: [0, 10]        - Inverse temperature for softmax
    w_max: [0, 1]        - Maximum Model-Based weight (at 0 anxiety)
    perseveration: [0, 5]- Stickiness to previous choice
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.perseveration = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values
        # V(State) = max(Q_stage2(State, :))
        v_stage2 = np.max(self.q_stage2, axis=1) 
        # Q_MB = Transition_Matrix * V_stage2
        q_mb = self.T @ v_stage2
        
        # 2. Calculate Mixing Weight based on Anxiety
        # Higher STAI -> Lower w_eff -> More Model-Free behavior
        w_eff = self.w_max * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0, 1)
        
        # 3. Combine MB and MF values (self.q_stage1 is the MF value)
        q_net = w_eff * q_mb + (1 - w_eff) * self.q_stage1
        
        # 4. Add Perseveration
        if self.last_action1 is not None:
            q_net[self.last_action1] += self.perseveration
            
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Perseveration (Safety Behavior).
    
    This model assumes the participant relies primarily on a Model-Free strategy 
    but exhibits "stickiness" or perseveration that is directly driven by their 
    anxiety level. In clinical literature, anxiety often manifests as rigid, 
    repetitive safety behaviors.
    
    Here, the perseveration bonus added to the previously chosen action is 
    proportional to the STAI score.
    Bonus = pers_scale * STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       - Learning rate
    beta: [0, 10]       - Inverse temperature
    pers_scale: [0, 10] - Scaling factor converting STAI into choice stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_scale = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_vals = self.q_stage1.copy()
        
        # Anxiety-driven stickiness
        if self.last_action1 is not None:
            # The magnitude of the bonus depends on how anxious the participant is
            bonus = self.pers_scale * self.stai
            q_vals[self.last_action1] += bonus
            
        return self.softmax(q_vals, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Loss Sensitivity.
    
    This model hypothesizes that anxiety alters how the participant learns from 
    negative outcomes (Prediction Error < 0). Anxious individuals may be 
    hyper-sensitive to "failure" (receiving 0 coins or a drop in value).
    
    The model uses a standard Model-Free architecture but modifies the learning 
    rate (alpha) when the prediction error is negative.
    alpha_neg = alpha * (1 + loss_sensitivity * STAI).
    
    If loss_sensitivity > 0, the participant updates their values more drastically 
    after a loss than after a win, scaled by their anxiety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]             - Base learning rate (for positive errors)
    beta: [0, 10]             - Inverse temperature
    loss_sensitivity: [0, 5]  - Multiplier for alpha during negative errors
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update (Outcome)
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha for Stage 2
        if delta_2 < 0:
            alpha_eff_2 = self.alpha * (1.0 + self.loss_sensitivity * self.stai)
        else:
            alpha_eff_2 = self.alpha
        
        # Clip to ensure stability
        alpha_eff_2 = np.clip(alpha_eff_2, 0, 1)
        self.q_stage2[state, action_2] += alpha_eff_2 * delta_2
        
        # Stage 1 Update (Transition)
        # Note: Using SARSA-style update from base class logic
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine effective alpha for Stage 1
        if delta_1 < 0:
            alpha_eff_1 = self.alpha * (1.0 + self.loss_sensitivity * self.stai)
        else:
            alpha_eff_1 = self.alpha
            
        alpha_eff_1 = np.clip(alpha_eff_1, 0, 1)
        self.q_stage1[action_1] += alpha_eff_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```