Here are three cognitive models that hypothesize different ways the participant's medium anxiety (STAI = 0.325) influences their decision-making in the two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that anxiety dictates the balance between Model-Based (planning) and Model-Free (habitual) control. Specifically, it proposes that higher anxiety impairs model-based planning, leading to a greater reliance on simple temporal difference learning (Model-Free). The `w` parameter (mixing weight) is dynamically adjusted based on the STAI score.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety (STAI) reduces the weight of model-based planning, forcing reliance on 
    habitual (MF) strategies. The mixing weight 'w' is a function of STAI.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_base: [0, 1]     # Baseline model-based weight
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (Q_MB)
        self.q_mb = np.zeros(self.n_choices)
        # Initialize Model-Free values (Q_MF) - same as base q_stage1
        self.q_mf = np.zeros(self.n_choices)
        
        # Calculate effective mixing weight w based on STAI
        # Hypothesis: Anxiety reduces MB control. 
        # We model this as w = w_base * (1 - stai)
        # If STAI is high, w approaches 0 (pure MF). If STAI is low, w is closer to w_base.
        self.w = self.w_base * (1.0 - self.stai)
        
        # Ensure w stays in [0, 1]
        self.w = np.clip(self.w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values: Q_MB(a) = sum(T(s|a) * max(Q_stage2(s, :)))
        for a in range(self.n_choices):
            # Expected value of best action in the next state
            v_next = 0
            for s_next in range(self.n_states):
                v_next += self.T[a, s_next] * np.max(self.q_stage2[s_next])
            self.q_mb[a] = v_next
            
        # Hybrid value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = self.w * self.q_mb + (1.0 - self.w) * self.q_mf
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (Standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Model-Free update (TD(1) / SARSA-like)
        # Update Q_MF based on the actual reward received at the end
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Stickiness (Perseveration)
This model hypothesizes that anxiety increases "stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of reward. The participant's data shows a very strong repetition of choosing "Spaceship U". This model posits that the STAI score directly scales a choice autocorrelation parameter (`stickiness`), making the participant more rigid in their stage 1 choices.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    The participant is more likely to repeat their previous stage-1 action 
    as their anxiety score increases, independent of the reward history.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    k_stick: [0, 5]    # Base stickiness magnitude
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.k_stick = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        stickiness_bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            # The bonus is scaled by STAI: Higher anxiety -> Higher tendency to repeat
            # Effective stickiness = k_stick * (1 + STAI)
            effective_k = self.k_stick * (1.0 + self.stai)
            stickiness_bonus[self.last_action1] = effective_k
            
        # Add bonus to Q-values before softmax
        return self.softmax(self.q_stage1 + stickiness_bonus, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning for values
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Loss Aversion
This model hypothesizes that anxiety changes how the participant processes negative outcomes (or lack of reward). Specifically, it suggests that anxiety modulates the learning rate for negative prediction errors. A higher STAI score might make the participant "over-learn" from failures (0 coins) compared to successes, or conversely, become insensitive to them (helplessness). Here, we model separate learning rates for positive and negative prediction errors, where the negative learning rate is a function of STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning from negative outcomes (Loss Aversion/Sensitivity).
    The learning rate is split into alpha_pos (for positive RPEs) and alpha_neg (for negative RPEs).
    The alpha_neg is scaled by the STAI score, reflecting altered sensitivity to lack of reward.
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]  # Learning rate for positive prediction errors
    beta: [0, 10]      # Inverse temperature
    neg_bias: [0, 2]   # Multiplier for negative learning rate based on STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.beta, self.neg_bias = model_parameters

    def get_effective_alpha(self, delta: float) -> float:
        if delta >= 0:
            return self.alpha_pos
        else:
            # Alpha_neg is derived from alpha_pos, modulated by STAI and a bias parameter.
            # If neg_bias * STAI > 1, they learn MORE from failure than success.
            # If neg_bias * STAI < 1, they learn LESS from failure.
            alpha_neg = self.alpha_pos * (self.neg_bias * self.stai)
            return np.clip(alpha_neg, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        eff_alpha_2 = self.get_effective_alpha(delta_2)
        self.q_stage2[state, action_2] += eff_alpha_2 * delta_2
        
        # Stage 1 Update
        # Note: Using the updated stage 2 value for the TD target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        eff_alpha_1 = self.get_effective_alpha(delta_1)
        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```