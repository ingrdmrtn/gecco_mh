```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Enhanced Negative Learning]
    This model hypothesizes that high anxiety (STAI) leads to an increased sensitivity 
    to negative prediction errors (outcomes worse than expected). The participant learns 
    more rapidly from disappointments (getting 0 coins or lower value than expected) 
    than from positive surprises. The magnitude of this asymmetry is scaled by their STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Base learning rate for positive prediction errors.
    beta: [0, 10] - Inverse temperature for softmax.
    neg_bias: [0, 5] - Multiplier for STAI to boost learning rate during negative prediction errors.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.neg_bias = model_parameters

    def get_effective_alpha(self, delta: float) -> float:
        """Calculate learning rate depending on whether the prediction error is positive or negative."""
        if delta < 0:
            # Boost learning rate for negative outcomes based on anxiety
            # We clip at 1.0 to maintain stability
            boosted_alpha = self.alpha * (1.0 + self.stai * self.neg_bias)
            return min(boosted_alpha, 1.0)
        else:
            return self.alpha

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.get_effective_alpha(delta_2)
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (TD-0)
        # The value of the chosen stage 1 action moves toward the value of the state reached
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.get_effective_alpha(delta_1)
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Perseveration]
    This model hypothesizes that high anxiety increases "stickiness" or perseveration. 
    Anxious individuals may stick to their previous choice to reduce cognitive load 
    or avoid the uncertainty of switching, regardless of the reward value.
    The degree of stickiness is a base parameter plus an anxiety-dependent component.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    stickiness_factor: [0, 5] - How much the STAI score contributes to choice perseveration.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        bonus = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            # The bonus is proportional to anxiety
            # Higher STAI -> Higher bonus to repeat last action
            bonus[int(self.last_action1)] = self.stai * self.stickiness_factor
        
        # Combine Q-values with stickiness bonus
        combined_values = self.q_stage1 + bonus
        return self.softmax(combined_values, self.beta)

    # Standard value update from base class is sufficient (TD learning)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Model-Based/Model-Free Hybrid]
    This model implements the classic hybrid theory where choices are a mix of 
    Model-Free (MF) and Model-Based (MB) systems.
    Hypothesis: High anxiety consumes working memory resources, reducing the 
    weight (w) of the computationally expensive Model-Based system.
    The mixing weight 'w' is negatively modulated by STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] - Learning rate for MF values.
    beta: [0, 10] - Inverse temperature.
    w_max: [0, 1] - The maximum Model-Based weight a person with 0 anxiety would have.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values (Q_MB)
        # Q_MB(a) = Sum(Transition(a, s') * Max(Q_stage2(s')))
        # We use the max of stage 2 values as the value of the state
        state_values = np.max(self.q_stage2, axis=1) # Shape (2,) for 2 states
        q_mb = np.dot(self.T, state_values) # T is (2,2) [actions x states]
        
        # 2. Retrieve Model-Free Values (Q_MF)
        # The base class q_stage1 acts as our MF values (updated via TD)
        q_mf = self.q_stage1
        
        # 3. Determine Mixing Weight w based on STAI
        # Higher STAI reduces w (less Model-Based control)
        # w = w_max * (1 - STAI). If STAI=1, w=0 (Pure MF).
        w = self.w_max * (1.0 - self.stai)
        w = np.clip(w, 0.0, 1.0)
        
        # 4. Combine
        q_net = w * q_mb + (1 - w) * q_mf
        
        return self.softmax(q_net, self.beta)

    # We use the base class value_update which performs TD updates on q_stage1 (MF)
    # and q_stage2. This is consistent with the hybrid architecture.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```