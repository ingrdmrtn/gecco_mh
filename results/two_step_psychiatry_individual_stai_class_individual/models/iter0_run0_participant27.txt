Here are three cognitive models designed to capture different hypotheses about how this participant (STAI 0.35, medium anxiety) makes decisions.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that the participant uses a mixture of Model-Based (planning using the transition matrix) and Model-Free (habitual) learning. Crucially, it posits that the balance between these systems (`w`) is modulated by their anxiety level. Since the participant has medium anxiety (0.35), this model tests if anxiety shifts reliance towards habits (Model-Free) or planning (Model-Based).

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This participant employs a hybrid Model-Based (MB) and Model-Free (MF) strategy.
    The weighting parameter 'w' (0=MF, 1=MB) is dynamically modulated by the participant's 
    anxiety score (STAI). Higher anxiety might impair MB planning, pushing the agent towards 
    MF habits. The STAI score scales the baseline mixing weight.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Baseline mixing weight (before anxiety modulation)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (transition matrix is fixed in base class)
        # We need separate Q-values for MF to track them distinctly
        self.q_mf = np.zeros(self.n_choices)
        
        # Calculate the effective mixing weight based on STAI
        # Hypothesis: Anxiety reduces Model-Based control.
        # We model this as w = w_base * (1 - stai)
        # If stai is high, w shrinks (more MF). If stai is low, w stays close to w_base.
        self.w_effective = self.w_base * (1.0 - self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Model-Based Value Calculation:
        # Q_MB(a1) = sum(T(a1, s) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # Expected value of the next state
            v_next = 0
            for s in range(self.n_states):
                v_next += self.T[a, s] * np.max(self.q_stage2[s])
            q_mb[a] = v_next

        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = self.w_effective * q_mb + (1 - self.w_effective) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 values (common to both systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Update Stage 1 Model-Free values (TD-learning)
        # Using the stage 2 value as the target for stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1
        
        # Note: Model-Based values are computed on the fly in policy_stage1 using the updated q_stage2

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Loss Aversion
This model hypothesizes that the participant's medium anxiety manifests as specific sensitivity to negative outcomes (or lack of reward). Instead of a complex planning model, this is a simpler Model-Free learner, but with separate learning rates for positive and negative prediction errors. The STAI score amplifies the learning rate for negative outcomes (`alpha_neg`), making the participant quicker to abandon choices that yield zero coins.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: The participant is a Model-Free learner with asymmetric learning rates 
    for positive and negative prediction errors. The anxiety score (STAI) specifically 
    amplifies the 'negative' learning rate, making the participant more sensitive to 
    omissions of reward (loss aversion).
    
    alpha_neg = alpha_base * (1 + stai)
    alpha_pos = alpha_base

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]  # Base learning rate
    beta: [0, 10]       # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Define learning rates modulated by STAI
        # Higher anxiety -> stronger reaction to negative prediction errors
        alpha_pos = self.alpha_base
        alpha_neg = np.clip(self.alpha_base * (1.0 + self.stai), 0.0, 1.0)

        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += alpha_neg * delta_2
        
        # --- Stage 1 Update ---
        # Standard TD(0) update passing value back
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # We apply the same asymmetry to stage 1 updates
        if delta_1 >= 0:
            self.q_stage1[action_1] += alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += alpha_neg * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced Choice Perseveration
This model tests the hypothesis that anxiety leads to "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of the outcome, perhaps as a safety behavior or to reduce cognitive load. The STAI score determines the strength of this perseveration bonus. The underlying learning is a standard Model-Free mechanism, but the choice policy is biased by the previous action, scaled by anxiety.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness). The participant 
    receives a 'bonus' added to the Q-value of the previously chosen action at Stage 1. 
    The magnitude of this bonus is directly proportional to their STAI score.
    
    Q_net(a) = Q_learned(a) + (perseveration_param * stai * is_last_action(a))

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    pers_k: [0, 5]      # Perseveration scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_k = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy learned Q-values to avoid modifying the actual value estimates
        q_biased = self.q_stage1.copy()
        
        # Apply perseveration bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is the parameter * anxiety score
            bonus = self.pers_k * self.stai
            q_biased[int(self.last_action1)] += bonus
            
        return self.softmax(q_biased, self.beta)

    # Standard Model-Free updates for value_update (inherited from Base or default logic)
    # We use the default value_update from the base class which is standard TD.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```