Here are three cognitive models designed to capture different mechanisms by which high anxiety (STAI = 0.6625) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that high anxiety impairs model-based planning (using the transition structure) and biases the participant towards model-free (habitual) learning. The `stai` score determines the mixing weight `w` between model-based and model-free values. A higher STAI score reduces `w`, leading to more reliance on simple TD learning rather than planning.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety (STAI) reduces the contribution of model-based planning 
    in favor of model-free reinforcement learning.
    
    The participant uses a hybrid strategy. The weight 'w' determines the balance:
    Q_net = w * Q_MB + (1 - w) * Q_MF.
    
    The mixing weight 'w' is modulated by STAI:
    w_effective = w_base * (1 - stai)
    
    Since this participant has high anxiety (0.66), they will have a very low effective 'w',
    relying mostly on model-free caching (repeating what was rewarded regardless of transition).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base model-based weight (before anxiety reduction)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Separate storage for Model-Free (MF) and Model-Based (MB) values
        self.q_mf1 = np.zeros(self.n_choices)
        self.q_mf2 = np.zeros((self.n_states, self.n_choices))
        
        # Transition counts for MB learning (optional, but here we use fixed T for simplicity)
        # We use the fixed transition matrix self.T provided in base class

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        q_mb1 = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            for s in range(self.n_states):
                # Max value of next stage
                v_next = np.max(self.q_mf2[s])
                q_mb1[a1] += self.T[a1, s] * v_next

        # 2. Modulate weight by STAI
        # High anxiety -> lower w -> less model-based
        w_eff = self.w_base * (1.0 - self.stai)
        
        # 3. Combine
        q_net = w_eff * q_mb1 + (1 - w_eff) * self.q_mf1
        
        return self.softmax(q_net, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Stage 2 is purely model-free/value-based
        return self.softmax(self.q_mf2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD updates for Model-Free values
        
        # Stage 2 update
        delta_2 = reward - self.q_mf2[state, action_2]
        self.q_mf2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD(1) / SARSA-like for the MF component)
        delta_1 = self.q_mf2[state, action_2] - self.q_mf1[action_1]
        self.q_mf1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that high anxiety makes the participant hypersensitive to lack of reward (losses). Instead of a standard learning rate, the model uses separate learning rates for positive outcomes (`alpha_pos`) and negative outcomes (`alpha_neg`). The `stai` score amplifies the learning rate for negative prediction errors, making the participant abandon choices quickly after failure (0 coins).

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases sensitivity to negative outcomes (loss aversion).
    
    The learning rate depends on the sign of the prediction error (RPE).
    If RPE < 0 (outcome worse than expected), the learning rate is boosted by STAI.
    
    alpha_effective = alpha * (1 + stai * sensitivity) if delta < 0
    alpha_effective = alpha                            if delta >= 0
    
    This explains why a high-anxiety participant might switch strategies rapidly 
    or stick rigidly to a "safe" option if the alternative yields a zero.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate
    beta: [0, 10]       # Inverse temperature
    sensitivity: [0, 5] # Multiplier for anxiety effect on negative learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine alpha for stage 2
        if delta_2 < 0:
            # Boost learning from negative errors based on anxiety
            alpha_2 = self.alpha * (1.0 + self.stai * self.sensitivity)
            # Cap at 1.0
            alpha_2 = min(alpha_2, 1.0)
        else:
            alpha_2 = self.alpha
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # We use the updated stage 2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine alpha for stage 1
        if delta_1 < 0:
            alpha_1 = self.alpha * (1.0 + self.stai * self.sensitivity)
            alpha_1 = min(alpha_1, 1.0)
        else:
            alpha_1 = self.alpha

        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Choice Perseveration (Stickiness)
This model hypothesizes that high anxiety leads to rigid, repetitive behavior (perseveration) as a coping mechanism to reduce cognitive load or uncertainty. The model includes a "stickiness" parameter that biases the choice towards the previously chosen action. The magnitude of this stickiness is directly scaled by the `stai` score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to choice perseveration (stickiness).
    Regardless of reward history, the participant prefers to repeat the 
    previous Stage 1 action to avoid the cognitive effort of re-evaluation.
    
    The choice probability includes a bonus for the last chosen action:
    Q_boosted(a) = Q(a) + (stai * stickiness_param * IsLastAction(a))
    
    This model captures the data pattern where the participant chooses 
    Spaceship U repeatedly despite mixed outcomes.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stickiness: [0, 5]  # Base magnitude of perseveration
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy values to avoid modifying the actual Q-table for learning
        q_vals = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is scaled by anxiety (stai)
            bonus = self.stickiness * self.stai
            q_vals[self.last_action1] += bonus
            
        return self.softmax(q_vals, self.beta)

    # Standard Q-learning for value updates and stage 2 policy
    # (Inherited methods policy_stage2 and value_update are sufficient)
    # We rely on the base class implementation for the core RL logic.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```