Here are three cognitive models that hypothesize different ways high anxiety (STAI > 0.51) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Balance
This model hypothesizes that anxiety shifts the balance between goal-directed (model-based) and habitual (model-free) control. High anxiety is often associated with a reliance on habits (model-free) over computationally expensive planning (model-based). Here, the STAI score determines the mixing weight (`w`) between these two systems.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety (STAI) reduces the weight of the model-based system, leading to more habitual behavior.
    
    The mixing weight 'w' is calculated as a logistic function of STAI, scaled by a parameter 'w_scale'.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_scale: [0, 10]    # Sensitivity of MB/MF balance to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_scale = model_parameters

    def init_model(self) -> None:
        # MF values (Q_TD)
        self.q_mf = np.zeros(self.n_choices)
        # MB values are computed on the fly
        
    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values (Q_MB)
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, a2)))
        q_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            for s in range(self.n_states):
                # Transition probability from T matrix
                prob_s = self.T[a1, s]
                # Max value of next stage
                max_q2 = np.max(self.q_stage2[s])
                q_mb[a1] += prob_s * max_q2
        
        # 2. Calculate mixing weight w based on STAI
        # Higher STAI -> Lower w (less Model-Based)
        # We use a sigmoid-like scaling: w = 1 / (1 + exp(w_scale * (stai - 0.4)))
        # Centered around 0.4 (mid-range anxiety)
        w = 1.0 / (1.0 + np.exp(self.w_scale * (self.stai - 0.4)))
        
        # 3. Combine values: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_mb + (1 - w) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Model-Free update for Stage 1 (TD(1) style for simplicity here, using reward directly)
        # Note: In full TD(lambda), we'd use the stage 2 value, but here we simplify to direct reinforcement
        # to distinguish it clearly from the transition-matrix-dependent MB system.
        delta_1 = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry
This model hypothesizes that anxiety affects how participants learn from positive versus negative prediction errors. Anxious individuals may be hypersensitive to negative outcomes (punishment) or failures to receive rewards. This model splits the learning rate into `alpha_pos` and `alpha_neg`, where the ratio or magnitude is modulated by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates an asymmetry in learning from positive vs. negative prediction errors.
    Specifically, the learning rate for negative prediction errors is amplified by the STAI score.
    
    alpha_eff = alpha_base * (1 + stai) if delta < 0
    alpha_eff = alpha_base            if delta >= 0
    
    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]  # Base learning rate
    beta: [0, 10]       # Inverse temperature
    neg_bias: [0, 5]    # Multiplier for how much STAI amplifies negative learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.neg_bias = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha based on sign of delta and STAI
        if delta_2 < 0:
            # Amplify learning from disappointment based on anxiety
            alpha_2 = self.alpha_base * (1.0 + self.neg_bias * self.stai)
            # Cap at 1.0
            alpha_2 = min(alpha_2, 1.0)
        else:
            alpha_2 = self.alpha_base
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (TD)
        # Using the updated stage 2 value as the target
        target_1 = self.q_stage2[state, action_2]
        delta_1 = target_1 - self.q_stage1[action_1]
        
        if delta_1 < 0:
            alpha_1 = self.alpha_base * (1.0 + self.neg_bias * self.stai)
            alpha_1 = min(alpha_1, 1.0)
        else:
            alpha_1 = self.alpha_base
            
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Choice Perseveration
This model hypothesizes that high anxiety leads to "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of the outcome, perhaps as a safety behavior or to reduce cognitive load. The degree of this perseveration bonus is directly scaled by the STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    A 'stickiness' bonus is added to the Q-value of the previously chosen action.
    The magnitude of this bonus is proportional to the STAI score.
    
    Q_stage1(a) = Q_learned(a) + (stai * stickiness_param * IsLastAction(a))
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stickiness: [0, 5]  # Base stickiness magnitude
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy learned values to avoid modifying the actual Q-table for updates
        q_vals = self.q_stage1.copy()
        
        # Apply perseveration bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is the base stickiness parameter scaled by anxiety level
            bonus = self.stickiness * self.stai
            q_vals[int(self.last_action1)] += bonus
            
        return self.softmax(q_vals, self.beta)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD update for stage 1
        # Note: We use the max of stage 2 as the target (Q-learning) to be robust
        target = np.max(self.q_stage2[state])
        delta_1 = target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```