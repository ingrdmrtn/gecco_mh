Here are three cognitive models hypothesizing different mechanisms for how high anxiety (STAI > 0.51) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that high anxiety impairs model-based control (planning using the transition matrix) and favors model-free control (habitual repetition of rewarded actions). The mixing weight `w` between model-based and model-free values is modulated by the STAI score. Higher anxiety reduces `w` (less model-based).

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the balance between Model-Based (MB) and Model-Free (MF) control.
    High anxiety (high STAI) reduces cognitive resources available for planning, leading to a 
    lower weighting of the Model-Based system (w) and a higher reliance on Model-Free habits.
    
    The mixing weight 'w' is calculated as: w_base * (1 - stai_factor * stai).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Baseline model-based weight
    stai_factor: [0, 1] # Sensitivity of w to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.stai_factor = model_parameters

    def init_model(self) -> None:
        # MF Q-values for stage 1
        self.q_mf = np.zeros(self.n_choices)
        # MB Q-values will be computed dynamically
        
        # Calculate effective w based on anxiety
        # If stai is high, effective w decreases
        self.w_eff = self.w_base * (1.0 - self.stai_factor * self.stai)
        self.w_eff = np.clip(self.w_eff, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # T[a] is the transition prob vector for action a to states 0, 1
            # Note: Base class T is [[0.7, 0.3], [0.3, 0.7]]
            # Row 0 is usually Action A, Row 1 is Action U
            # We assume T[a, s] is prob of state s given action a
            
            # Expected value of best action in next state
            v_next_0 = np.max(self.q_stage2[0])
            v_next_1 = np.max(self.q_stage2[1])
            
            q_mb[a] = self.T[a, 0] * v_next_0 + self.T[a, 1] * v_next_1

        # Hybrid Value
        q_net = self.w_eff * q_mb + (1 - self.w_eff) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (Standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Model-Free update (TD(1) / SARSA-like)
        # Update MF value of chosen stage 1 action towards the actual reward
        delta_1 = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry (Punishment Sensitivity)
This model hypothesizes that high anxiety increases sensitivity to negative outcomes (omission of reward). The learning rate is split into `alpha_pos` (for rewards) and `alpha_neg` (for non-rewards). The STAI score amplifies `alpha_neg`, making the participant learn faster from failures/punishments than from successes.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases sensitivity to negative prediction errors (punishment/omission).
    The learning rate for negative outcomes (alpha_neg) is boosted by the STAI score, 
    while the learning rate for positive outcomes (alpha_pos) remains baseline.
    
    alpha_neg_effective = alpha_neg_base + (stai * sensitivity)
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      # Learning rate for rewards > 0
    alpha_neg_base: [0, 1] # Base learning rate for rewards = 0
    beta: [0, 10]          # Inverse temperature
    sensitivity: [0, 1]    # How much STAI boosts negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta, self.sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective learning rate based on prediction error sign or reward
        # Here we use reward directly as proxy for "good/bad" outcome since reward is binary 0/1
        
        if reward > 0:
            alpha = self.alpha_pos
        else:
            # Boost negative learning rate by anxiety
            alpha = self.alpha_neg_base + (self.stai * self.sensitivity)
            alpha = np.clip(alpha, 0.0, 1.0)

        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        # Stage 1 Update (TD(0) - update towards value of state reached)
        # Value of state reached is max Q in that state
        v_state = np.max(self.q_stage2[state])
        delta_1 = v_state - self.q_stage1[action_1]
        
        # We apply the same asymmetry to stage 1 updates based on the final outcome
        self.q_stage1[action_1] += alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration (Inverse Temperature Modulation)
This model hypothesizes that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety might lead to more erratic behavior (random exploration) due to difficulty concentrating or "choking," effectively lowering the inverse temperature `beta`. Alternatively, it could lead to rigid exploitation. Here we model it as anxiety decreasing `beta` (increasing noise/randomness).

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety acts as a noise parameter, increasing decision randomness.
    The inverse temperature (beta) is reduced by the STAI score, leading to a flatter 
    softmax curve and more exploration/errors regardless of learned values.
    
    beta_effective = beta_max / (1 + stai * noise_factor)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta_max: [0, 10]    # Maximum inverse temperature (low anxiety)
    noise_factor: [0, 5] # Scaling factor for anxiety's effect on noise
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_max, self.noise_factor = model_parameters
        
    def init_model(self) -> None:
        # Calculate effective beta once, assuming trait anxiety is constant
        # Higher STAI -> Higher denominator -> Lower Beta -> More Randomness
        self.beta_eff = self.beta_max / (1.0 + self.stai * self.noise_factor)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_eff)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 towards Stage 2 value (TD-0)
        v_state_2 = np.max(self.q_stage2[state])
        delta_1 = v_state_2 - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```