```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Impaired Model-Based Control]
    This model hypothesizes that the participant uses a hybrid Model-Based (MB) and 
    Model-Free (MF) strategy. Crucially, the weight of the Model-Based component 
    is negatively modulated by anxiety (STAI). Higher anxiety reduces the cognitive 
    resources available for planning (MB), leading to a stronger reliance on 
    Model-Free (habitual) values.

    The mixing weight 'w' is calculated as: w = w_max * (1 - stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Learning rate)
    beta: [0, 10]   (Inverse temperature)
    w_max: [0, 1]   (Maximum Model-Based weight for a non-anxious person)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values (Planning)
        # V(state) = max Q_stage2(state, action)
        v_stage2 = np.max(self.q_stage2, axis=1) 
        
        # Q_MB = Transition_Matrix * V_stage2
        # Action 0 (A) -> 0.7 prob state 0, 0.3 prob state 1
        # Action 1 (U) -> 0.3 prob state 0, 0.7 prob state 1
        q_mb = np.zeros(self.n_choices)
        q_mb[0] = self.T[0, 0] * v_stage2[0] + self.T[0, 1] * v_stage2[1]
        q_mb[1] = self.T[1, 0] * v_stage2[0] + self.T[1, 1] * v_stage2[1]

        # 2. Retrieve Model-Free values (stored in self.q_stage1 by base class update)
        q_mf = self.q_stage1

        # 3. Calculate Mixing Weight based on STAI
        # High anxiety -> Low w -> More MF
        w = self.w_max * (1.0 - self.stai)
        w = np.clip(w, 0.0, 1.0)

        # 4. Combine
        q_net = w * q_mb + (1 - w) * q_mf
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Perseveration]
    This model hypothesizes that the participant relies on a Model-Free strategy 
    but exhibits "stickiness" or perseveration (repeating the last choice) 
    driven by anxiety. The urge to repeat the previous action, regardless of 
    outcome, scales with the STAI score, representing a safety-seeking or 
    compulsive behavior often seen in high anxiety.

    Stickiness bonus = k_anx * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Learning rate)
    beta: [0, 10]   (Inverse temperature)
    k_anx: [0, 5]   (Anxiety-scaling factor for stickiness)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.k_anx = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            stickiness_bonus = self.k_anx * self.stai
            q_values[int(self.last_action1)] += stickiness_bonus
            
        return self.softmax(q_values, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Eligibility Trace]
    This model hypothesizes that anxiety modulates the learning mechanism itself,
    specifically the eligibility trace (lambda). A higher eligibility trace means 
    outcomes at the second stage directly reinforce the first-stage choice, 
    bypassing the state-transition structure. This model suggests high anxiety 
    leads to "hyper-reactive" learning where the final reward is directly 
    attributed to the initial choice, ignoring the intermediate step.

    Lambda = lambda_scale * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]        (Learning rate)
    beta: [0, 10]        (Inverse temperature)
    lambda_scale: [0, 1] (Scaling factor for eligibility trace)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_scale = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective lambda based on anxiety
        # We clip to ensure it stays valid [0, 1]
        lam = np.clip(self.lambda_scale * self.stai, 0.0, 1.0)
        
        # Stage 2 Prediction Error (R - Q2)
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Stage 1 Prediction Error (Q2 - Q1)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Update Stage 2 (Standard TD)
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 (TD(lambda))
        # Q1 updates by its own error (delta_1) PLUS a fraction (lambda) of the future error (delta_2)
        self.q_stage1[action_1] += self.alpha * delta_1 + self.alpha * lam * delta_2

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```