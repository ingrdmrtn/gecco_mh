```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Model-Based/Model-Free Hybrid]
    This model hypothesizes that the participant uses a mixture of Model-Based (planning) 
    and Model-Free (habitual) strategies. Crucially, it posits that higher anxiety (STAI) 
    consumes cognitive resources, thereby reducing the weight of the computationally 
    expensive Model-Based system.
    
    The mixing weight 'w' determines the contribution of the Model-Based system.
    w = w_max * (1 - stai)
    
    If STAI is high, behavior becomes predominantly Model-Free.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Learning rate)
    beta: [0, 10]   (Inverse temperature)
    w_max: [0, 1]   (Maximum model-based weight for a theoretical 0-anxiety participant)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values (Planning)
        # Q_MB(a) = Sum( P(s'|a) * max(Q_stage2(s', :)) )
        # self.T is [2, 2] -> [action, next_state]
        # We compute the max value of each state in stage 2
        max_q_stage2 = np.max(self.q_stage2, axis=1) # [max(Q(s0)), max(Q(s1))]
        
        # Matrix multiplication to get expected value for each stage 1 action
        q_mb = self.T @ max_q_stage2
        
        # 2. Retrieve Model-Free values (Habitual)
        q_mf = self.q_stage1
        
        # 3. Calculate Anxiety-modulated mixing weight
        # Higher STAI -> Lower w -> Less Model-Based influence
        # We clip to ensure w stays in [0, 1] range logically, though bounds handle w_max
        w = self.w_max * (1.0 - self.stai)
        w = np.clip(w, 0.0, 1.0)
        
        # 4. Combine values
        q_net = w * q_mb + (1 - w) * q_mf
        
        return self.softmax(q_net, self.beta)

    # policy_stage2 uses default softmax on q_stage2
    # value_update uses default TD learning for MF values

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Negative Prediction Error Sensitivity]
    This model hypothesizes that anxiety modulates how the participant learns from 
    disappointment. Specifically, it proposes that the learning rate for negative 
    prediction errors (when outcomes are worse than expected) is amplified by the 
    STAI score.
    
    When delta < 0 (outcome worse than expected), the effective learning rate increases:
    alpha_eff = alpha * (1 + phi * stai)
    
    This reflects a "negativity bias" or hypersensitivity to failure common in anxiety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Base learning rate)
    beta: [0, 10]   (Inverse temperature)
    phi: [0, 5]     (Scaling factor for anxiety impact on negative learning)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha for Stage 2
        if delta_2 < 0:
            alpha_2 = self.alpha * (1.0 + self.phi * self.stai)
            # Clip to prevent instability if alpha becomes huge
            alpha_2 = np.clip(alpha_2, 0.0, 1.0) 
        else:
            alpha_2 = self.alpha
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (TD using Stage 2 value)
        # Note: We use the updated Stage 2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine effective alpha for Stage 1
        if delta_1 < 0:
            alpha_1 = self.alpha * (1.0 + self.phi * self.stai)
            alpha_1 = np.clip(alpha_1, 0.0, 1.0)
        else:
            alpha_1 = self.alpha

        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Perseveration (Safety Seeking)]
    This model hypothesizes that anxiety drives a "stickiness" or perseveration 
    bias, where the participant prefers to repeat their previous Stage 1 choice 
    regardless of reward history, treating familiarity as a safety signal.
    
    The model adds a bonus to the Q-value of the previously chosen spaceship.
    The magnitude of this bonus is directly proportional to the STAI score.
    
    Q_net(a) = Q_learned(a) + (k * stai) * I(a == last_action)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Learning rate)
    beta: [0, 10]   (Inverse temperature)
    k: [0, 5]       (Perseveration strength scaling factor)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.k = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy learned values to avoid modifying the actual Q-table for learning
        q_values = self.q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            bonus = self.k * self.stai
            q_values[int(self.last_action1)] += bonus
            
        return self.softmax(q_values, self.beta)

    # policy_stage2 uses default softmax
    # value_update uses default TD learning

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```