Here are three cognitive models hypothesizing different mechanisms for how high anxiety (STAI = 0.75) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that high anxiety impairs model-based control (planning using the transition matrix) and promotes reliance on model-free reinforcement learning (habitual responding). The mixing weight `w` between model-based and model-free values is modulated by the STAI score.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety (STAI) reduces model-based planning and increases reliance on 
    model-free learning. This participant (STAI=0.75) is hypothesized to be predominantly 
    model-free.
    
    The stage-1 value is a weighted combination of Model-Based (MB) and Model-Free (MF) values.
    The weight `w` is dynamically adjusted by STAI: w_effective = w_base * (1 - stai).
    Higher anxiety -> lower w_effective -> less Model-Based influence.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_base: [0, 1]     # Base mixing weight (0=MF, 1=MB) before anxiety modulation
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize separate Q-values for Model-Free (MF) system
        self.q_mf_stage1 = np.zeros(self.n_choices)
        self.q_mf_stage2 = np.zeros((self.n_states, self.n_choices))
        
        # Calculate effective mixing weight based on anxiety
        # High anxiety reduces the contribution of the model-based system
        self.w_effective = self.w_base * (1.0 - self.stai)

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            for s in range(self.n_states):
                # Use the max value of the second stage as the estimated value of that state
                q_mb[a1] += self.T[a1, s] * np.max(self.q_mf_stage2[s])

        # 2. Combine MB and MF values
        # Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = self.w_effective * q_mb + (1.0 - self.w_effective) * self.q_mf_stage1
        
        return self.softmax(q_net, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Stage 2 is purely model-free / simple RL
        return self.softmax(self.q_mf_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard SARSA/Q-learning update for Model-Free values
        
        # Stage 2 update
        pe_2 = reward - self.q_mf_stage2[state, action_2]
        self.q_mf_stage2[state, action_2] += self.alpha * pe_2
        
        # Stage 1 update (TD(0))
        # The value of the state reached is the value of the action taken there (SARSA-like)
        # or max value (Q-learning). Here we use the value of the action taken.
        pe_1 = self.q_mf_stage2[state, action_2] - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * pe_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Perseveration (Stickiness)
This model hypothesizes that high anxiety leads to "safety behavior" or perseveration, manifesting as a tendency to repeat the previous choice regardless of the outcome (stickiness). The degree of choice stickiness is scaled by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness). 
    The participant is more likely to repeat their previous Stage 1 choice, 
    regardless of reward, as a form of safety behavior or cognitive rigidity.
    
    The choice stickiness bonus is calculated as: stickiness * stai.
    This bonus is added to the Q-value of the previously chosen action.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    stickiness: [0, 5] # Base perseveration parameter
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_modified = self.q_stage1.copy()
        
        # If a previous action exists, add a stickiness bonus scaled by anxiety
        if self.last_action1 is not None:
            # The higher the anxiety, the stronger the urge to repeat the last action
            bonus = self.stickiness * self.stai
            q_modified[int(self.last_action1)] += bonus
            
        return self.softmax(q_modified, self.beta)

    # Standard Q-learning for value updates (inherited logic is slightly modified below to be explicit)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (Model-Free TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Loss Aversion (Punishment Sensitivity)
This model hypothesizes that high anxiety makes the participant hypersensitive to the absence of reward (interpreted as a loss or punishment). Instead of a single learning rate, the model uses separate learning rates for positive outcomes (reward > 0) and negative outcomes (reward = 0), where the negative learning rate is amplified by anxiety.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases sensitivity to negative outcomes (loss aversion).
    When a reward is 0 (omission), the learning rate is amplified by the STAI score,
    causing the participant to abandon non-rewarding options more quickly than
    they learn about rewarding ones.
    
    alpha_neg = alpha_pos * (1 + phi * stai)
    where phi is a scaling factor for how much anxiety boosts negative learning.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]  # Learning rate for rewards (reward > 0)
    beta: [0, 10]      # Inverse temperature
    phi: [0, 10]       # Anxiety scaling factor for negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.beta, self.phi = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine learning rate based on outcome
        if reward > 0:
            alpha = self.alpha_pos
        else:
            # Anxiety amplifies learning from failure/omission
            # We clamp alpha to max 1.0 to maintain stability
            alpha_neg = self.alpha_pos * (1.0 + self.phi * self.stai)
            alpha = min(alpha_neg, 1.0)

        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        # Stage 1 update
        # Note: We use the same alpha for stage 1 update based on the final reward outcome
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```