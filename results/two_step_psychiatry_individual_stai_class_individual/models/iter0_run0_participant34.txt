Here are three cognitive models designed to capture different mechanisms by which high anxiety (STAI ~0.96) might influence decision-making in this two-step task.

### Model 1: Anxiety-Driven Model-Based Suppression
This model hypothesizes that high anxiety consumes cognitive resources, reducing the ability to use "model-based" planning (using the transition matrix $T$). Instead of a pure mixture, the STAI score directly modulates the mixing weight $w$ between model-based and model-free control. A high STAI score forces the agent to rely almost exclusively on model-free (habitual) learning.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety (STAI) acts as a cognitive load that suppresses 
    Model-Based (MB) planning. This model implements a hybrid MB/MF reinforcement 
    learning agent where the mixing weight 'w' is negatively modulated by STAI.
    
    Mechanism:
    - Q_MF (Model-Free) is learned via TD-lambda.
    - Q_MB (Model-Based) is computed using the transition matrix T and stage-2 values.
    - Q_net = w * Q_MB + (1-w) * Q_MF
    - The effective weight w_eff is reduced by STAI: w_eff = w_base * (1 - stai^k)
      Since this participant has STAI ~0.96, w_eff will be very low, predicting 
      habitual behavior.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    lambda_elig: [0, 1] # Eligibility trace decay
    w_base: [0, 1]      # Baseline model-based weight (before anxiety reduction)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_elig, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Free Q-values separately
        self.q_mf_stage1 = np.zeros(self.n_choices)
        self.q_mf_stage2 = np.zeros((self.n_states, self.n_choices))
        
        # Calculate effective mixing weight based on anxiety
        # High STAI -> Low w_eff -> Dominance of Model-Free
        self.w_eff = self.w_base * (1.0 - self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values: Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # T maps action -> state probabilities. 
            # T[0] is for action 0 (A), T[1] is for action 1 (U) usually, 
            # but base class T is [[0.7, 0.3], [0.3, 0.7]].
            # Assuming row 0 corresponds to action 0 leading to state 0/1
            expected_val = 0.0
            for s_next in range(self.n_states):
                # Use max of stage 2 values for planning
                max_q2 = np.max(self.q_mf_stage2[s_next])
                expected_val += self.T[a, s_next] * max_q2
            q_mb[a] = expected_val

        # Combine MB and MF
        q_net = self.w_eff * q_mb + (1 - self.w_eff) * self.q_mf_stage1
        return self.softmax(q_net, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_mf_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update (Model-Free)
        pe2 = reward - self.q_mf_stage2[state, action_2]
        self.q_mf_stage2[state, action_2] += self.alpha * pe2
        
        # Stage 1 Update (Model-Free with Eligibility Trace)
        # TD(lambda) update for stage 1
        # The value of the state arrived at is max(Q_stage2) usually, or just Q_stage2(chosen)
        # Standard SARSA-like update for MF path:
        pe1 = self.q_mf_stage2[state, action_2] - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * pe1
        
        # Eligibility trace: Stage 1 choice also gets credit for Stage 2 RPE
        self.q_mf_stage1[action_1] += self.alpha * self.lambda_elig * pe2

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that high anxiety manifests as extreme sensitivity to negative outcomes (or lack of reward). While the standard model treats `0` and `1` rewards symmetrically, this model scales the learning rate for negative prediction errors (disappointments) by the STAI score. A high STAI score amplifies the "pain" of getting 0 coins, causing rapid unlearning of choices that led to failure.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety (STAI) amplifies Loss Aversion (or punishment sensitivity).
    The participant learns differently from positive vs. negative prediction errors.
    
    Mechanism:
    - Standard Q-learning.
    - Two effective learning rates: alpha_pos and alpha_neg.
    - alpha_neg is boosted by STAI: alpha_neg = alpha * (1 + phi * STAI)
    - This means high anxiety participants react much more strongly to getting 0 coins
      than low anxiety participants.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate
    beta: [0, 10]       # Inverse temperature
    phi: [0, 5]         # Anxiety scaling factor for negative prediction errors
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective learning rates
        # If PE is negative (outcome < expectation), we boost learning rate by STAI
        
        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        
        if delta_2 < 0:
            eff_alpha_2 = min(1.0, self.alpha * (1.0 + self.phi * self.stai))
        else:
            eff_alpha_2 = self.alpha
            
        self.q_stage2[state, action_2] += eff_alpha_2 * delta_2
        
        # --- Stage 1 Update ---
        # We use the updated stage 2 value to drive stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 < 0:
            eff_alpha_1 = min(1.0, self.alpha * (1.0 + self.phi * self.stai))
        else:
            eff_alpha_1 = self.alpha

        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Choice Stickiness (Perseveration)
This model hypothesizes that high anxiety leads to rigid, repetitive behavior (perseveration) as a safety mechanism, regardless of reward outcomes. The STAI score determines the strength of a "stickiness" parameter that biases the agent to repeat the previous Stage 1 choice.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases Choice Stickiness (Perseveration).
    High STAI participants are more likely to repeat their previous Stage 1 choice
    regardless of the outcome, reflecting a 'safety behavior' or rigidity.
    
    Mechanism:
    - Standard Q-learning for value estimation.
    - Decision rule includes a 'stickiness' bonus added to the Q-value of the 
      previously chosen action.
    - The magnitude of this bonus is determined by STAI: 
      stickiness_bonus = rho * STAI

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    rho: [0, 5]         # Anxiety-scaling factor for stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rho = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy Q-values to avoid modifying the actual learned values
        decision_values = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is proportional to anxiety (STAI)
            stickiness = self.rho * self.stai
            decision_values[int(self.last_action1)] += stickiness
            
        return self.softmax(decision_values, self.beta)

    # Standard Q-learning update (inherited logic is sufficient if we just want 
    # to test the effect on the policy, but we'll implement the standard one 
    # explicitly to be safe and clear).
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```