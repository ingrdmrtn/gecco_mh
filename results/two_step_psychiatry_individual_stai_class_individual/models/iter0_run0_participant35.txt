Here are three cognitive models designed to capture different hypotheses about how high anxiety (STAI = 0.725) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
**Hypothesis:** High anxiety participants often show a deficit in "model-based" planning (using the transition structure of the task) and rely more on "model-free" habits (repeating what was previously rewarded). This model implements a hybrid reinforcement learning agent where the mixing weight `w` (balance between model-based and model-free control) is directly modulated by the participant's STAI score. Higher anxiety reduces `w`, leading to more model-free behavior.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety impairs model-based planning.
    This model is a hybrid Model-Based (MB) / Model-Free (MF) learner.
    The mixing weight 'w' determines the contribution of MB vs MF values to the stage 1 choice.
    
    Crucially, the effective mixing weight is modulated by STAI:
    w_effective = w_base * (1 - stai)
    
    Since this participant has high anxiety (0.725), they will have a very low w_effective,
    relying mostly on model-free TD learning (habitual control).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_base: [0, 1]     # Base mixing weight (before anxiety modulation)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Free (MF) and Model-Based (MB) values
        self.q_mf = np.zeros(self.n_choices)
        self.q_mb = np.zeros(self.n_choices)
        
        # Calculate effective mixing weight based on anxiety
        # High anxiety -> Low w_effective -> More Model-Free
        self.w_effective = self.w_base * (1.0 - self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Hybrid value: w * MB + (1-w) * MF
        q_net = self.w_effective * self.q_mb + (1 - self.w_effective) * self.q_mf
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 values (Model-Free Q-learning)
        # This is the same for both systems
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2

        # 2. Update Stage 1 Model-Free values (TD(1) / SARSA-like)
        # MF system updates based on the reward received at the end
        delta_1_mf = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1_mf

        # 3. Update Stage 1 Model-Based values (Bellman equation)
        # MB system computes values using the transition matrix T and current stage 2 values
        # V(state) is max Q in that state
        v_stage2 = np.max(self.q_stage2, axis=1) 
        
        # Q_MB(a1) = sum(P(s|a1) * V(s))
        # self.T is [2x2]: T[0] is probs for A (to X, to Y), T[1] is probs for U
        self.q_mb[0] = np.sum(self.T[0] * v_stage2) # Choice A
        self.q_mb[1] = np.sum(self.T[1] * v_stage2) # Choice U

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Punishment Sensitivity
**Hypothesis:** High anxiety is often associated with a hypersensitivity to negative outcomes (loss aversion or punishment sensitivity). This model posits that the participant learns differently from rewards (coins) versus omissions (no coins). Specifically, the learning rate for negative prediction errors (getting 0 when expecting >0) is amplified by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases learning from negative outcomes (punishment sensitivity).
    
    This model splits the learning rate into alpha_pos (for positive prediction errors)
    and alpha_neg (for negative prediction errors).
    
    The base alpha is used for positive errors.
    For negative errors, the learning rate is boosted by anxiety:
    alpha_neg = alpha + (stai * punishment_boost)
    
    This causes the participant to abandon choices that yield 0 coins much faster than
    a low-anxiety participant would.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]            # Base learning rate
    beta: [0, 10]            # Inverse temperature
    punishment_boost: [0, 1] # How much anxiety amplifies negative learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.punishment_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # --- Stage 2 Update ---
        pe2 = reward - self.q_stage2[state, action_2]
        
        # Determine learning rate based on sign of PE
        if pe2 >= 0:
            eff_alpha2 = self.alpha
        else:
            # Boost learning from disappointment based on anxiety
            eff_alpha2 = self.alpha + (self.stai * self.punishment_boost)
            # Clip to ensure stability
            eff_alpha2 = min(eff_alpha2, 1.0)
            
        self.q_stage2[state, action_2] += eff_alpha2 * pe2
        
        # --- Stage 1 Update ---
        # Standard TD update passing value back
        pe1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Apply same logic to stage 1 PE? 
        # Let's apply it here too, assuming general punishment sensitivity
        if pe1 >= 0:
            eff_alpha1 = self.alpha
        else:
            eff_alpha1 = self.alpha + (self.stai * self.punishment_boost)
            eff_alpha1 = min(eff_alpha1, 1.0)

        self.q_stage1[action_1] += eff_alpha1 * pe1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration/Perseveration
**Hypothesis:** Anxiety can lead to rigid, perseverative behavior (sticking to a choice regardless of outcome) as a safety behavior to reduce uncertainty. Alternatively, it might manifest as erratic switching. This model tests the hypothesis that high anxiety increases "stickiness" or choice perseveration. The STAI score directly scales a perseveration parameter added to the softmax function.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases choice perseveration (stickiness).
    
    The model adds a 'stickiness' bonus to the Q-value of the previously chosen action.
    The magnitude of this bonus is determined by a base stickiness parameter 
    scaled by the STAI score.
    
    Q_net(a) = Q(a) + (stai * stickiness_factor * IsLastAction(a))
    
    High anxiety participants will therefore be more likely to repeat their previous 
    stage-1 choice, regardless of the reward outcome, exhibiting behavioral rigidity.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]             # Learning rate
    beta: [0, 10]             # Inverse temperature
    stickiness_factor: [0, 5] # Magnitude of perseveration bonus
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy Q-values to avoid modifying the learned values permanently
        q_biased = self.q_stage1.copy()
        
        # If we have a previous action, add the anxiety-driven stickiness bonus
        if self.last_action1 is not None:
            bonus = self.stai * self.stickiness_factor
            q_biased[int(self.last_action1)] += bonus
            
        return self.softmax(q_biased, self.beta)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```