Here are three cognitive models designed to capture different hypotheses about how high anxiety (STAI > 0.51) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
**Hypothesis:** High anxiety impairs model-based control (planning using the transition matrix) and favors model-free control (habitual repetition of rewarded actions). This model implements a hybrid reinforcement learning agent where the mixing weight `w` (balance between model-based and model-free) is modulated by the STAI score. Higher anxiety reduces `w`, making the agent more model-free.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates the balance between Model-Based (MB) and Model-Free (MF) control.
    High anxiety (high STAI) reduces cognitive resources for planning, leading to a lower mixing weight 'w'
    (more Model-Free behavior).
    
    The mixing weight w is calculated as: w = w_base * (1 - stai * anxiety_sensitivity)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Baseline mixing weight (0=MF, 1=MB)
    anxiety_sens: [0, 1] # Sensitivity to anxiety (0=no effect, 1=strong reduction of MB)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.anxiety_sens = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (Q_MB) and Model-Free values (Q_MF)
        self.q_mf = np.zeros(self.n_choices)
        self.q_mb = np.zeros(self.n_choices)
        
        # Calculate the effective mixing weight based on STAI
        # Higher STAI reduces w, pushing behavior towards Model-Free
        # We clip to ensure it stays in [0, 1]
        raw_w = self.w_base * (1.0 - (self.stai * self.anxiety_sens))
        self.w = np.clip(raw_w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Hybrid value: w * Q_MB + (1-w) * Q_MF
        q_net = self.w * self.q_mb + (1 - self.w) * self.q_mf
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 values (Model-Free TD)
        # Note: In this task, Stage 2 is purely model-free learning of reward probs
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Update Stage 1 Model-Free values (TD(1) / SARSA-like)
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1
        
        # 3. Update Stage 1 Model-Based values (Bellman equation)
        # Q_MB(a1) = P(s1|a1) * max(Q_s1) + P(s2|a1) * max(Q_s2)
        # We use the fixed transition matrix self.T
        for a in range(self.n_choices):
            v_x = np.max(self.q_stage2[0]) # Max value of state X (0)
            v_y = np.max(self.q_stage2[1]) # Max value of state Y (1)
            
            # T[0,0] is A->X, T[0,1] is A->Y
            # T[1,0] is U->X, T[1,1] is U->Y (assuming row 0=A, row 1=U)
            if a == 0: # Action A
                self.q_mb[a] = self.T[0, 0] * v_x + self.T[0, 1] * v_y
            else:      # Action U
                self.q_mb[a] = self.T[1, 0] * v_x + self.T[1, 1] * v_y

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Choice Perseveration
**Hypothesis:** High anxiety increases "stickiness" or choice perseveration, regardless of reward outcomes. Anxious individuals may prefer the safety of the familiar (the previous choice) over exploration. This model adds a perseveration bonus to the Q-values of the previously chosen action, where the magnitude of this bonus scales with the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases choice perseveration (stickiness).
    The model adds a bonus to the Q-value of the previously chosen action at Stage 1.
    The magnitude of this bonus is: p_base + (stai * p_anxiety_slope).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    p_base: [0, 5]      # Baseline perseveration bonus
    p_slope: [0, 5]     # Additional perseveration per unit of STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.p_base, self.p_slope = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective perseveration parameter
        pers_param = self.p_base + (self.stai * self.p_slope)
        
        # Copy Q-values to avoid modifying the learned values permanently
        q_modified = self.q_stage1.copy()
        
        # Add bonus if a previous action exists
        if self.last_action1 is not None:
            q_modified[self.last_action1] += pers_param
            
        return self.softmax(q_modified, self.beta)

    # Standard TD learning for value update (Model-Free)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Learning Rate Asymmetry (Punishment Sensitivity)
**Hypothesis:** Anxious individuals are hypersensitive to negative outcomes (lack of reward). This model posits that the learning rate for negative prediction errors (disappointments) is higher than for positive prediction errors, and this asymmetry is amplified by the STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety amplifies learning from negative prediction errors (punishment/omission sensitivity).
    We use two learning rates: alpha_pos (for RPE > 0) and alpha_neg (for RPE < 0).
    The negative learning rate is boosted by anxiety:
    alpha_neg_effective = alpha_neg_base * (1 + stai * sensitivity)
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      # Learning rate for positive RPE
    alpha_neg_base: [0, 1] # Base learning rate for negative RPE
    beta: [0, 10]          # Inverse temperature
    neg_sens: [0, 5]       # How much STAI boosts negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta, self.neg_sens = model_parameters

    def init_model(self) -> None:
        # Calculate effective negative learning rate based on STAI
        # We clip to ensure it doesn't exceed 1.0 drastically (though >1 is theoretically possible, usually bad for stability)
        raw_neg = self.alpha_neg_base * (1.0 + (self.stai * self.neg_sens))
        self.alpha_neg_eff = np.clip(raw_neg, 0.0, 1.0)

    def get_alpha(self, delta: float) -> float:
        """Select learning rate based on sign of prediction error."""
        if delta >= 0:
            return self.alpha_pos
        else:
            return self.alpha_neg_eff

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.get_alpha(delta_2)
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Using the updated stage 2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.get_alpha(delta_1)
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```