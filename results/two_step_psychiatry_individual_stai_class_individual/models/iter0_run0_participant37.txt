Here are three cognitive models that hypothesize different ways high anxiety (STAI) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Balance
This model hypothesizes that anxiety shifts the balance between model-based (planning) and model-free (habitual) control. High anxiety is often associated with reduced cognitive flexibility and increased reliance on habits. Here, the STAI score modulates the mixing weight `w` between model-based and model-free values.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety (STAI) modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety reduces the weight of model-based planning, leading to more habitual behavior.
    
    The mixing weight 'w' is dynamically adjusted by STAI:
    w_effective = w_base * (1 - stai)
    
    If STAI is high, w_effective drops, favoring MF control.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base mixing weight (1=Pure MB, 0=Pure MF)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (transition matrix is fixed in base class)
        # We need separate storage for MF values to combine them later
        self.q_mf = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # self.T is [2x2]: T[a, s] probability of transition
            # But base class T is [[0.7, 0.3], [0.3, 0.7]] which corresponds to actions 0 and 1
            # We assume row 0 is action A, row 1 is action U
            max_q2_s0 = np.max(self.q_stage2[0])
            max_q2_s1 = np.max(self.q_stage2[1])
            q_mb[a] = self.T[a, 0] * max_q2_s0 + self.T[a, 1] * max_q2_s1

        # 2. Modulate mixing weight by STAI
        # High anxiety (stai ~ 1) reduces w_effective towards 0 (Pure MF)
        w_effective = self.w_base * (1.0 - self.stai)
        
        # 3. Combine MB and MF values
        q_combined = w_effective * q_mb + (1 - w_effective) * self.q_mf
        
        return self.softmax(q_combined, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2 (used by both MB and MF systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) update for Stage 1 Model-Free values
        # Using the reward directly for the MF update (ignoring stage 2 value for simplicity in this variant)
        delta_1 = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that anxiety increases sensitivity to negative outcomes (or lack of reward). Instead of a single learning rate, the model uses separate learning rates for positive prediction errors (gains) and negative prediction errors (losses/omissions). The STAI score amplifies the learning rate for negative outcomes, making the participant abandon unrewarding options more quickly.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases sensitivity to negative prediction errors (loss aversion).
    The learning rate for negative prediction errors (alpha_neg) is scaled up by the STAI score,
    while the positive learning rate (alpha_pos) remains baseline.
    
    alpha_neg_effective = alpha_neg + (stai * 0.5)  # Boosted by anxiety
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]   # Learning rate for positive PE
    alpha_neg: [0, 1]   # Base learning rate for negative PE
    beta: [0, 10]       # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate based on anxiety
        # We clamp it to 1.0 to ensure stability
        alpha_neg_eff = min(1.0, self.alpha_neg + (self.stai * 0.5))
        
        # --- Stage 2 Update ---
        pe2 = reward - self.q_stage2[state, action_2]
        alpha2 = self.alpha_pos if pe2 >= 0 else alpha_neg_eff
        self.q_stage2[state, action_2] += alpha2 * pe2
        
        # --- Stage 1 Update ---
        # Using the updated Q2 value as the target
        target_val = self.q_stage2[state, action_2]
        pe1 = target_val - self.q_stage1[action_1]
        alpha1 = self.alpha_pos if pe1 >= 0 else alpha_neg_eff
        self.q_stage1[action_1] += alpha1 * pe1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration/Exploitation Shift
This model hypothesizes that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety might lead to "safety behaviors" or rigid exploitation of known values, reducing random exploration. Here, the inverse temperature parameter `beta` (which controls choice determinism) is modulated by STAI. Higher anxiety leads to a higher `beta`, resulting in more deterministic (less exploratory) choices.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety reduces exploration (increases exploitation).
    The inverse temperature parameter (beta) is modulated by STAI.
    Higher anxiety leads to a higher effective beta, making choices more deterministic
    and "stiff", reducing the likelihood of exploring lower-value options.
    
    beta_effective = beta_base * (1 + stai * stiffness_factor)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta_base: [0, 10]      # Base inverse temperature
    stiffness: [0, 5]       # How strongly anxiety increases rigidity
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stiffness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Modulate beta based on STAI
        beta_eff = self.beta_base * (1.0 + self.stai * self.stiffness)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Modulate beta based on STAI
        beta_eff = self.beta_base * (1.0 + self.stai * self.stiffness)
        return self.softmax(self.q_stage2[state], beta_eff)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```