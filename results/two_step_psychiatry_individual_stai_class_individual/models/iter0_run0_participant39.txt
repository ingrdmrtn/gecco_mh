Here are three cognitive models designed to capture different mechanisms by which high anxiety (STAI > 0.51) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model tests the hypothesis that high anxiety reduces the contribution of model-based (planning) control in favor of model-free (habitual) control. The STAI score modulates the mixing weight ($w$) between these two systems.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety impairs model-based planning.
    This model implements a hybrid Model-Based (MB) and Model-Free (MF) reinforcement learning agent.
    The mixing parameter 'w' determines the balance between MB and MF control.
    Crucially, the effective 'w' is modulated by the participant's STAI score:
    higher anxiety leads to a lower 'w' (more model-free/habitual behavior).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Baseline mixing weight (0=MF, 1=MB)
    stai_mod: [0, 2]    # Strength of anxiety modulation on w
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.stai_mod = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (transition matrix is fixed in base class)
        # We need separate storage for MF values to combine them later
        self.q_mf_stage1 = np.zeros(self.n_choices)
        self.q_mf_stage2 = np.zeros((self.n_states, self.n_choices))

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values for Stage 1
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # self.T is [2x2]: T[a, s] = prob of transition to s given action a
            # Note: Base class T is [[0.7, 0.3], [0.3, 0.7]] which maps action->state prob
            # We assume action 0 -> state 0 (common), action 1 -> state 1 (common)
            
            # Expected value of best action in state 0
            v_s0 = np.max(self.q_mf_stage2[0])
            # Expected value of best action in state 1
            v_s1 = np.max(self.q_mf_stage2[1])
            
            q_mb[a] = self.T[a, 0] * v_s0 + self.T[a, 1] * v_s1

        # 2. Calculate Effective Mixing Weight w based on STAI
        # Higher STAI reduces w (pushes towards 0/Model-Free)
        # We use a logistic-style clipping or simple subtraction to keep it in [0,1]
        # Here: w = w_base * (1 - stai_mod * normalized_stai)
        # Assuming STAI is roughly 0-1 range based on prompt description.
        effective_w = self.w_base * (1.0 - (self.stai_mod * self.stai))
        effective_w = np.clip(effective_w, 0.0, 1.0)

        # 3. Combine MF and MB values
        self.q_stage1 = effective_w * q_mb + (1 - effective_w) * self.q_mf_stage1
        
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Stage 2 is purely model-free in standard hybrid models
        return self.softmax(self.q_mf_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning for Model-Free values
        
        # Stage 2 update
        delta_2 = reward - self.q_mf_stage2[state, action_2]
        self.q_mf_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD(0))
        # Note: In full hybrid models, this might be TD(lambda), but we use TD(0) for simplicity
        delta_1 = self.q_mf_stage2[state, action_2] - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that high anxiety increases sensitivity to negative outcomes (losses). Instead of affecting the planning strategy, anxiety scales the subjective perception of negative rewards, making the agent learn more drastically from punishments than rewards.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases loss aversion.
    The participant perceives negative rewards more intensely than positive rewards.
    The 'loss_scaling' parameter is multiplied by the STAI score to determine
    how much negative rewards are amplified before the value update.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    loss_sens: [0, 5]   # Sensitivity to losses (base multiplier)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_sens = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Modulate reward perception based on STAI
        subjective_reward = reward
        
        if reward < 0:
            # If the outcome is a loss (e.g. -1 or 0 if treated as relative loss), amplify it
            # High STAI -> Higher multiplier for negative rewards
            # Multiplier = 1 + (Base_Sensitivity * STAI)
            loss_multiplier = 1.0 + (self.loss_sens * self.stai)
            subjective_reward = reward * loss_multiplier
        
        # Standard Q-learning update with subjective reward
        # Stage 2
        delta_2 = subjective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1
        # Using the updated stage 2 value as the target for stage 1 (SARSA-like connection)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration/Noise (Inverse Temperature Modulation)
This model posits that anxiety acts as a distractor or a source of decision noise. Rather than changing how values are learned, anxiety affects the choice rule itself. Specifically, high anxiety lowers the inverse temperature ($\beta$), making choices more random (higher exploration/noise) regardless of learned values.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases decision noise (decreases exploitation).
    High anxiety interferes with the ability to consistently select the high-value option.
    The effective beta (inverse temperature) is reduced by the STAI score.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_max: [0, 20]   # Maximum inverse temperature (at 0 anxiety)
    noise_mod: [0, 10]  # How strongly STAI reduces beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_max, self.noise_mod = model_parameters

    def get_effective_beta(self) -> float:
        # Calculate beta modulated by anxiety
        # effective_beta = beta_max - (noise_mod * STAI)
        # We clip at 0 to prevent negative beta (which would mean seeking low values)
        eff_beta = self.beta_max - (self.noise_mod * self.stai)
        return max(0.0, eff_beta)

    def policy_stage1(self) -> np.ndarray:
        # Use anxiety-modulated beta
        b = self.get_effective_beta()
        return self.softmax(self.q_stage1, b)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Use anxiety-modulated beta
        b = self.get_effective_beta()
        return self.softmax(self.q_stage2[state], b)

    # Standard value update (TD Learning)
    # We use the default implementation from Base, but we must ensure
    # unpack_parameters didn't break the base class expectations if we relied on self.beta there.
    # However, the base class uses self.beta in policy_stage1/2. 
    # Since we overrode policy_stage1/2, the base class 'self.beta' attribute is ignored 
    # during choice, but we should set it just in case or ignore it.
    # The base value_update uses q_stage1/2 which we have.
    
    # Note: We do NOT override value_update here, assuming standard TD learning.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```