```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety-Modulated Arbitration (Hybrid MB/MF).
    
    This model hypothesizes that decision-making relies on a weighted combination of 
    Model-Based (planning) and Model-Free (habitual) systems. The core hypothesis is 
    that anxiety consumes cognitive resources required for Model-Based planning.
    
    The mixing weight 'w' determines the contribution of the Model-Based system.
    This weight is modulated by the participant's STAI score: higher anxiety reduces 
    the influence of the Model-Based system.
    
    w_effective = w_base * (1 - stai)
    Q_net = w_effective * Q_MB + (1 - w_effective) * Q_MF
    
    Since this participant has low anxiety (0.2625), this model predicts they will 
    exhibit stronger Model-Based behavior (utilizing the transition structure) 
    than a high-anxiety participant.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   - Learning rate for Model-Free values
    beta: [0, 10]   - Inverse temperature for softmax
    w_base: [0, 1]  - Maximum Model-Based weight (theoretical weight at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values (Q_MB)
        # V(state) = max(Q_stage2[state])
        v_stage2 = np.max(self.q_stage2, axis=1) 
        
        # Q_MB(action) = Sum(P(state|action) * V(state))
        # self.T is [2x2]: T[0] is probs for Action A (to X, Y), T[1] for Action U
        q_mb = np.dot(self.T, v_stage2)
        
        # 2. Retrieve Model-Free Values (Q_MF)
        # In this framework, self.q_stage1 acts as the cached MF value updated by TD
        q_mf = self.q_stage1
        
        # 3. Calculate Mixing Weight based on STAI
        # Higher STAI -> Lower w (less Model-Based)
        # We clip to ensure it stays valid [0, 1]
        w = np.clip(self.w_base * (1.0 - self.stai), 0.0, 1.0)
        
        # 4. Combine
        q_net = w * q_mb + (1 - w) * q_mf
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety-Modulated Punishment Sensitivity.
    
    This model hypothesizes that anxiety specifically alters how participants learn 
    from negative outcomes (punishment sensitivity). While positive outcomes are 
    learned at a base rate, negative prediction errors are amplified by anxiety.
    
    If prediction error (delta) < 0:
        learning_rate = alpha * (1 + stai * rho)
    Else:
        learning_rate = alpha
        
    For this low-anxiety participant, the learning asymmetry is expected to be small,
    whereas a high-anxiety participant would react much more strongly to coin losses.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   - Base learning rate
    beta: [0, 10]   - Inverse temperature
    rho: [0, 10]    - Anxiety scaling factor for negative prediction errors
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rho = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine learning rate for Stage 2
        if delta_2 < 0:
            lr_2 = self.alpha * (1.0 + self.stai * self.rho)
        else:
            lr_2 = self.alpha
        
        # Clip LR to prevent instability
        lr_2 = np.clip(lr_2, 0.0, 1.0)
        self.q_stage2[state, action_2] += lr_2 * delta_2
        
        # Stage 1 Update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine learning rate for Stage 1
        if delta_1 < 0:
            lr_1 = self.alpha * (1.0 + self.stai * self.rho)
        else:
            lr_1 = self.alpha
            
        lr_1 = np.clip(lr_1, 0.0, 1.0)
        self.q_stage1[action_1] += lr_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety-Modulated Perseveration.
    
    This model hypothesizes that anxiety drives a "safety-seeking" behavior, 
    manifesting as a tendency to repeat the previous choice (perseveration) 
    regardless of the reward outcome. This is modeled as a choice "bonus" 
    added to the previously selected action, scaled by the STAI score.
    
    Q_net(a) = Q_learned(a) + (stai * p_scale) if a == last_action
    
    This participant's low anxiety suggests a lower tendency for anxiety-driven 
    perseveration compared to high-anxiety individuals, allowing their choices 
    to be driven more purely by value.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]    - Learning rate
    beta: [0, 10]    - Inverse temperature
    p_scale: [0, 5]  - Scaling factor for anxiety-driven perseveration bonus
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.p_scale = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Start with learned values
        q_vals = self.q_stage1.copy()
        
        # Add anxiety-modulated perseveration bonus
        if self.last_action1 is not None:
            bonus = self.stai * self.p_scale
            q_vals[int(self.last_action1)] += bonus
            
        return self.softmax(q_vals, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```