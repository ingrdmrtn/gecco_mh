Here are three cognitive models designed to capture different hypotheses about how high anxiety (STAI) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
**Hypothesis:** High anxiety participants often show reduced model-based control (planning) and increased reliance on model-free (habitual) learning. This model implements a hybrid reinforcement learning agent where the mixing weight `w` (balance between model-based and model-free) is directly modulated by the participant's STAI score. Higher anxiety reduces `w`, leading to more model-free behavior.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety (STAI) modulates the balance between Model-Based (MB) and Model-Free (MF) control.
    High anxiety reduces the contribution of the MB system (planning), leading to more habitual choices.
    
    The mixing weight 'w' is calculated as: w = w_base * (1 - stai)
    where w_base is the maximum model-based weight for a calm participant.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base model-based weight (before anxiety modulation)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (transition probabilities are fixed in base class self.T)
        # Initialize Model-Free values
        self.q_mf_stage1 = np.zeros(self.n_choices)
        self.q_mb_stage1 = np.zeros(self.n_choices)
        
        # Calculate the effective mixing weight based on anxiety
        # Higher STAI -> Lower w -> More Model-Free
        # We clip STAI to [0, 1] just in case, though input is usually bounded.
        stai_clipped = np.clip(self.stai, 0, 1)
        self.w = self.w_base * (1.0 - stai_clipped)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values for Stage 1
        # Q_MB(s1, a) = sum(T(s1, a, s2) * max(Q_stage2(s2, :)))
        for a in range(self.n_choices):
            v_s2_0 = np.max(self.q_stage2[0])
            v_s2_1 = np.max(self.q_stage2[1])
            self.q_mb_stage1[a] = self.T[a, 0] * v_s2_0 + self.T[a, 1] * v_s2_1
            
        # Combine MB and MF values
        q_net = self.w * self.q_mb_stage1 + (1 - self.w) * self.q_mf_stage1
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update (TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Model-Free Update (TD(1) / SARSA-like)
        # Using the value of the state reached (or the reward directly if we simplify)
        # Standard hybrid models often use the stage 2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Perseveration (Stickiness)
**Hypothesis:** Anxiety increases behavioral rigidity or "stickiness." This model posits that high anxiety participants are more likely to repeat their previous Stage 1 choice regardless of the outcome (reward or transition). The STAI score scales a perseveration parameter `p` added to the Q-values of the previously chosen action.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases choice perseveration (stickiness).
    The model adds a 'stickiness' bonus to the previously chosen action in Stage 1.
    The magnitude of this bonus is scaled by the STAI score.
    
    Q_net(a) = Q(a) + (stai * stickiness_factor * I(a == last_action))
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stick_factor: [0, 5]# Magnitude of stickiness scaling
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Add stickiness bonus if there was a previous action
        if self.last_action1 is not None:
            # The bonus is proportional to anxiety
            bonus = self.stai * self.stick_factor
            q_values[int(self.last_action1)] += bonus
            
        return self.softmax(q_values, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning for both stages
        # Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1
        # Using the value of the chosen stage 2 action as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Learning Rate Asymmetry (Loss Aversion)
**Hypothesis:** Anxiety is associated with increased sensitivity to negative outcomes (or lack of reward). This model implements separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The STAI score amplifies the learning rate for negative errors (`alpha_neg`), making the participant learn faster from failures/omissions than from successes.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety creates an asymmetry in learning from positive vs negative prediction errors.
    High anxiety participants update their values more aggressively when outcomes are worse than expected.
    
    alpha_pos = base_alpha
    alpha_neg = base_alpha * (1 + stai * sensitivity_gain)
    
    Parameter Bounds:
    -----------------
    base_alpha: [0, 1]      # Base learning rate
    beta: [0, 10]           # Inverse temperature
    sensitivity_gain: [0, 5]# How much STAI amplifies negative learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.base_alpha, self.beta, self.sensitivity_gain = model_parameters

    def init_model(self) -> None:
        # Calculate the specific learning rates
        self.alpha_pos = self.base_alpha
        
        # Ensure alpha_neg doesn't explode, but scales with STAI
        # If STAI is high, alpha_neg becomes larger than alpha_pos
        multiplier = 1.0 + (self.stai * self.sensitivity_gain)
        self.alpha_neg = np.clip(self.base_alpha * multiplier, 0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Target is the updated Q-value of stage 2
        target_1 = self.q_stage2[state, action_2]
        delta_1 = target_1 - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```