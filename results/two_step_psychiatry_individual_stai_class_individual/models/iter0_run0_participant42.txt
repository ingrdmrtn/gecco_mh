```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Model-Based/Model-Free Hybrid]
    This model hypothesizes that the participant uses a hybrid of Model-Based (MB) and 
    Model-Free (MF) strategies. Crucially, the balance (w) between these strategies 
    is modulated by their anxiety level (STAI). High anxiety is hypothesized to 
    reduce Model-Based control (planning) and increase reliance on Model-Free 
    (habitual) control.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       - Learning rate
    beta: [0, 10]       - Inverse temperature
    w_base: [0, 1]      - Baseline mixing weight (0=Pure MF, 1=Pure MB)
    w_stai_mod: [-1, 1] - Modulation of mixing weight by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_stai_mod = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values
        # Q_MB(a) = Sum(T(s'|a) * Max(Q_stage2(s', :)))
        # self.T is shape (n_choices, n_states) -> (2, 2)
        # self.q_stage2 is shape (n_states, n_choices) -> (2, 2)
        
        # Max value of each state in stage 2
        max_q2 = np.max(self.q_stage2, axis=1) # Shape (2,)
        
        # Expected value for each stage 1 action based on transition matrix
        q_mb = np.dot(self.T, max_q2)
        
        # 2. Retrieve Model-Free Values (stored in self.q_stage1)
        q_mf = self.q_stage1
        
        # 3. Calculate Mixing Weight w based on STAI
        # w = w_base + (w_stai_mod * stai)
        # We clip w to be between 0 and 1
        raw_w = self.w_base + (self.w_stai_mod * self.stai)
        w = np.clip(raw_w, 0.0, 1.0)
        
        # 4. Combine
        q_net = w * q_mb + (1 - w) * q_mf
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Loss Aversion in Learning]
    This model hypothesizes that high anxiety leads to an asymmetry in how prediction 
    errors are processed. Specifically, anxious individuals are hypothesized to 
    learn more rapidly from negative prediction errors (losses/omissions) than 
    positive ones. The STAI score boosts the learning rate specifically when 
    outcomes are worse than expected.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Base learning rate (for positive RPEs)
    beta: [0, 10]          - Inverse temperature
    loss_boost: [0, 5]     - Multiplier for STAI to boost alpha during negative RPEs
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha for stage 2
        if delta_2 < 0:
            # Boost alpha based on STAI for negative errors
            # alpha_neg = alpha + (loss_boost * STAI * (1-alpha)) to keep it somewhat bounded
            # Simplified: alpha_neg = alpha * (1 + boost * STAI), clipped at 1.0
            alpha_eff_2 = np.clip(self.alpha * (1.0 + self.loss_boost * self.stai), 0.0, 1.0)
        else:
            alpha_eff_2 = self.alpha
            
        self.q_stage2[state, action_2] += alpha_eff_2 * delta_2
        
        # Stage 1 Update
        # TD Error: V(state) - Q(action1)
        # Here we use Q_stage2 of the chosen action as the proxy for V(state) (SARSA-like)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine effective alpha for stage 1
        if delta_1 < 0:
            alpha_eff_1 = np.clip(self.alpha * (1.0 + self.loss_boost * self.stai), 0.0, 1.0)
        else:
            alpha_eff_1 = self.alpha

        self.q_stage1[action_1] += alpha_eff_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Constrained Exploration]
    This model hypothesizes that anxiety reduces exploration, leading to more 
    deterministic, exploitative behavior. The inverse temperature parameter (beta) 
    is directly modulated by the STAI score. Higher anxiety results in a higher 
    beta, making the softmax function sharper and the participant more likely 
    to strictly choose the option with the highest value.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta_base: [0, 10]     - Baseline inverse temperature
    beta_stai_slope: [0, 10] - How much STAI increases beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.beta_stai_slope = model_parameters
        
        # Calculate the effective beta once, as STAI is constant for the participant
        # beta = beta_base + (slope * STAI)
        self.effective_beta = self.beta_base + (self.beta_stai_slope * self.stai)
        
        # Ensure beta doesn't explode or go negative (though bounds usually handle this)
        self.effective_beta = np.maximum(0.0, self.effective_beta)

    def policy_stage1(self) -> np.ndarray:
        # Use the STAI-modulated beta
        return self.softmax(self.q_stage1, self.effective_beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Use the STAI-modulated beta
        return self.softmax(self.q_stage2[state], self.effective_beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```