Here are three cognitive models hypothesizing different ways the participant's medium anxiety (STAI = 0.325) influences their decision-making in the two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Balance
This model hypothesizes that anxiety levels shift the balance between Model-Based (planning using the transition matrix) and Model-Free (habitual TD learning) strategies. Specifically, it posits that higher anxiety (STAI) reduces the cognitive resources available for model-based planning, leading to a greater reliance on model-free learning. The mixing weight `w` is modulated by STAI.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety (STAI) acts as a cognitive load, reducing the weight (w) of model-based planning 
    in favor of model-free habits. The effective mixing weight is w_base * (1 - stai).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Baseline model-based weight before anxiety modulation
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (Q_MB) and Model-Free values (Q_MF)
        self.q_mf = np.zeros(self.n_choices)
        self.q_mb = np.zeros(self.n_choices)
        
        # Calculate effective mixing weight based on STAI
        # Higher STAI reduces the effective w
        self.w_eff = self.w_base * (1.0 - self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        for a in range(self.n_choices):
            # T[0] is transition for action 0 (A), T[1] for action 1 (U)
            # T[a][0] is prob of state 0 (X), T[a][1] is prob of state 1 (Y)
            v_state_x = np.max(self.q_stage2[0])
            v_state_y = np.max(self.q_stage2[1])
            
            # Transition matrix self.T is defined as:
            # T[0] = [0.7, 0.3] (Action 0 -> State 0 w/ 0.7)
            # T[1] = [0.3, 0.7] (Action 1 -> State 1 w/ 0.7)
            # Note: The base class defines T as [[0.7, 0.3], [0.3, 0.7]]
            # We assume row 0 corresponds to Action 0, row 1 to Action 1.
            
            self.q_mb[a] = self.T[a, 0] * v_state_x + self.T[a, 1] * v_state_y

        # Combine MF and MB values
        q_net = (1 - self.w_eff) * self.q_mf + self.w_eff * self.q_mb
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Model-Free update (TD(1) / SARSA-like)
        # Using the value of the state reached (or the Q-value of the second stage choice)
        # Here we use the standard MF update: Q_MF(a1) += alpha * (Q_stage2(s, a2) - Q_MF(a1))
        # Note: We use the updated Q_stage2 value
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry
This model hypothesizes that anxiety biases how participants learn from positive versus negative outcomes. Specifically, it proposes that anxiety amplifies the learning rate for negative prediction errors (disappointments) or dampens the learning rate for positive ones, reflecting a "negativity bias" common in anxiety. Here, we model separate learning rates for positive and negative prediction errors, where the negative learning rate is scaled up by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates a negativity bias in learning.
    The participant has separate learning rates for positive (alpha_pos) and 
    negative (alpha_neg) prediction errors. The STAI score amplifies the 
    learning rate for negative errors: alpha_neg_eff = alpha_neg * (1 + STAI).
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]   # Learning rate for positive RPE
    alpha_neg: [0, 1]   # Base learning rate for negative RPE
    beta: [0, 10]       # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg, self.beta = model_parameters

    def init_model(self) -> None:
        # Scale the negative learning rate by anxiety
        # If STAI is high, negative outcomes have a stronger impact on value updates
        self.alpha_neg_eff = min(1.0, self.alpha_neg * (1.0 + self.stai))

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        pe2 = reward - self.q_stage2[state, action_2]
        alpha2 = self.alpha_pos if pe2 >= 0 else self.alpha_neg_eff
        self.q_stage2[state, action_2] += alpha2 * pe2
        
        # Stage 1 Update
        # Using the updated stage 2 value as the target
        pe1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha1 = self.alpha_pos if pe1 >= 0 else self.alpha_neg_eff
        self.q_stage1[action_1] += alpha1 * pe1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration/Perseveration
This model hypothesizes that anxiety influences the "stickiness" or perseveration of choices. It suggests that anxious individuals might exhibit "safety behavior" by repeating previously chosen actions regardless of reward (perseveration) to reduce uncertainty, or conversely, erratic switching. Here, we model a choice autocorrelation (stickiness) parameter `phi` that is directly modulated by STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety influences choice perseveration (stickiness).
    A base stickiness parameter (phi) is added to the Q-values of the 
    previously chosen action. This stickiness is modulated by STAI:
    phi_eff = phi * (1 + STAI), suggesting anxious individuals are more 
    prone to repetitive 'safety' behaviors or habit-like repetition.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    phi: [0, 5]         # Base choice stickiness/perseveration
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def init_model(self) -> None:
        # Effective stickiness increases with anxiety
        self.phi_eff = self.phi * (1.0 + self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bonus to the Q-values before softmax
        q_augmented = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            q_augmented[self.last_action1] += self.phi_eff
            
        return self.softmax(q_augmented, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # We apply stickiness to stage 2 as well, though often less relevant
        # assuming the "safety" behavior applies to the initial choice primarily.
        # However, for consistency, we apply it to the specific state's actions.
        # Note: last_action2 tracks the action taken in the *previous trial's* stage 2.
        # We only apply stickiness if the previous trial visited the *same* state.
        
        q_augmented = self.q_stage2[state].copy()
        
        if self.last_state == state and self.last_action2 is not None:
            q_augmented[self.last_action2] += self.phi_eff
            
        return self.softmax(q_augmented, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```