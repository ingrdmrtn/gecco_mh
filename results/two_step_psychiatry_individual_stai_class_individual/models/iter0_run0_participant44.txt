```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Model-Based/Model-Free Hybrid]
    This model hypothesizes that the participant uses a mixture of Model-Based (planning) 
    and Model-Free (habitual) strategies. Crucially, it posits that high anxiety (STAI) 
    impairs Model-Based control, shifting the balance towards Model-Free habits.
    
    The mixing weight 'w' determines the contribution of the Model-Based system.
    w = w_base * (1 - stai)
    
    If STAI is high, w becomes small, and the agent relies mostly on Model-Free TD values.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   - Learning rate for MF values and Stage 2 values
    beta: [0, 10]   - Inverse temperature for softmax
    w_base: [0, 1]  - Baseline weight for Model-Based control (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values (Planning)
        # Q_MB(a) = Sum_s' [ T(a, s') * Max_a' Q_stage2(s', a') ]
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            expected_val = 0
            for s_prime in range(self.n_states):
                # Probability of transition a -> s_prime
                prob = self.T[a, s_prime]
                # Max value available in state s_prime
                max_q2 = np.max(self.q_stage2[s_prime])
                expected_val += prob * max_q2
            q_mb[a] = expected_val

        # 2. Retrieve Model-Free Values (Habit)
        # self.q_stage1 tracks the standard TD values
        q_mf = self.q_stage1

        # 3. Combine based on Anxiety
        # Higher anxiety -> Lower w -> More MF
        w = self.w_base * (1.0 - self.stai)
        
        # Ensure w stays in [0, 1] just in case
        w = np.clip(w, 0.0, 1.0)

        q_net = w * q_mb + (1 - w) * q_mf
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Perseverance (Stickiness)]
    This model hypothesizes that high anxiety leads to "safety behaviors," manifesting 
    as a tendency to repeat the previous choice regardless of the outcome (perseverance).
    
    The model adds a "stickiness" bonus to the Q-value of the previously chosen action.
    The magnitude of this bonus is directly scaled by the participant's STAI score.
    
    Q_net(a) = Q_TD(a) + (phi * stai) * I(a == last_action)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   - Learning rate
    beta: [0, 10]   - Inverse temperature
    phi: [0, 5]     - Perseverance scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Base values from TD learning
        values = self.q_stage1.copy()
        
        # Add stickiness bonus if there was a previous action
        if self.last_action1 is not None:
            stickiness_bonus = self.phi * self.stai
            values[int(self.last_action1)] += stickiness_bonus
            
        return self.softmax(values, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Asymmetric Learning]
    This model hypothesizes that anxiety creates a bias in how prediction errors are processed.
    Specifically, high anxiety individuals may be hyper-sensitive to negative outcomes (0 coins),
    updating their value estimates more aggressively after a loss than after a win.
    
    The learning rate for negative outcomes (reward=0) is boosted by the STAI score.
    alpha_neg = alpha * (1 + neg_bias * stai)
    alpha_pos = alpha

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      - Base learning rate (for positive outcomes)
    beta: [0, 10]      - Inverse temperature
    neg_bias: [0, 5]   - Multiplier for anxiety's effect on negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.neg_bias = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective learning rate based on outcome
        if reward == 0:
            # Boost learning rate for negative outcomes based on anxiety
            alpha_eff = self.alpha * (1.0 + self.neg_bias * self.stai)
            # Clip to ensure stability
            alpha_eff = min(alpha_eff, 1.0)
        else:
            # Standard learning rate for positive outcomes
            alpha_eff = self.alpha

        # Update Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha_eff * delta_2
        
        # Update Stage 1
        # Note: We use the same alpha_eff for consistency in reaction to the trial outcome
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_eff * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```