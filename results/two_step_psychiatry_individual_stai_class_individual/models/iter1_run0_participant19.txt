```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases perseveration (stickiness).
    Participants with higher anxiety are hypothesized to rely more on repetitive 'safety' behaviors,
    preferring to repeat their previous choice regardless of reward.
    This model adds a 'stickiness' bonus to the value of the previously chosen action.
    The magnitude of this bonus is modulated by the participant's STAI score.

    stickiness = stick_base + (stick_stai_slope * stai)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    stick_base: [0, 5]
    stick_stai_slope: [-5, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stick_stai_slope = model_parameters

    def init_model(self) -> None:
        # Calculate the stickiness bonus based on anxiety
        self.stickiness = self.stick_base + (self.stick_stai_slope * self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bonus to the Q-value of the last chosen action
        qs = self.q_stage1.copy()
        if self.last_action1 is not None:
            qs[self.last_action1] += self.stickiness
        return self.softmax(qs, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Add stickiness bonus only if we are in the same state as the last choice
        # (Context-dependent perseveration)
        qs = self.q_stage2[state].copy()
        if self.last_state == state and self.last_action2 is not None:
            qs[self.last_action2] += self.stickiness
        return self.softmax(qs, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning rate (volatility estimation).
    Anxious individuals may perceive the environment as more volatile, leading to 
    a higher learning rate (alpha) to adapt quickly to changes. Conversely, they might
    be more rigid (lower alpha). This model allows STAI to linearly scale the learning rate.

    alpha = clip(alpha_base + (alpha_stai_slope * stai), 0, 1)

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]
    alpha_stai_slope: [-1, 1]
    beta: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.alpha_stai_slope, self.beta = model_parameters

    def init_model(self) -> None:
        # Calculate effective alpha modulated by STAI
        raw_alpha = self.alpha_base + (self.alpha_stai_slope * self.stai)
        self.alpha = np.clip(raw_alpha, 0.0, 1.0)

    # Uses default policy_stage1, policy_stage2, and value_update
    # The base class uses self.alpha, which we have modified in init_model

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces a negativity bias in learning.
    Anxious participants are hypothesized to be more sensitive to negative outcomes (punishment)
    than positive outcomes. This model uses separate learning rates for positive and negative
    prediction errors, where the negative learning rate is amplified by the STAI score.

    alpha_pos = alpha_base
    alpha_neg = alpha_base * (1 + (neg_bias_stai * stai))

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]
    beta: [0, 10]
    neg_bias_stai: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.neg_bias_stai = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate
        # If neg_bias_stai is positive, anxiety increases learning from disappointment
        alpha_neg = np.clip(self.alpha_base * (1.0 + self.neg_bias_stai * self.stai), 0.0, 1.0)
        
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        # Use alpha_neg if prediction error is negative, else alpha_base
        alpha_2 = alpha_neg if delta_2 < 0 else self.alpha_base
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = alpha_neg if delta_1 < 0 else self.alpha_base
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```