Here are three new cognitive models exploring different mechanisms by which high anxiety (STAI = 0.775) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model tests the hypothesis that high anxiety impairs model-based planning (using the transition matrix `T`) and favors model-free learning (habitual caching of values). The mixing weight `w` between these two systems is modulated by the STAI score.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety shifts the balance between Model-Based (MB) and Model-Free (MF) control.
    High anxiety (STAI) reduces the weight (w) given to the MB system, leading to more habitual behavior.
    
    The final Q-value for Stage 1 is a weighted sum:
    Q_net = w * Q_MB + (1 - w) * Q_MF
    
    Where w is derived from a base parameter modulated by STAI:
    w = w_base * (1 - stai_impact * STAI)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Baseline model-based weight
    stai_impact: [0, 1] # How strongly STAI reduces MB control
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.stai_impact = model_parameters

    def init_model(self) -> None:
        # Separate storage for Model-Free values
        self.q_mf_stage1 = np.zeros(self.n_choices)
        self.q_mf_stage2 = np.zeros((self.n_states, self.n_choices))
        
        # Calculate the fixed mixing weight for this participant
        # We clip to ensure w stays in [0, 1]
        raw_w = self.w_base * (1.0 - self.stai_impact * self.stai)
        self.w = np.clip(raw_w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(P(s|a1) * max(Q_MF_stage2(s, :)))
        # We use the max of stage 2 values as the estimated value of the state
        q_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            for s in range(self.n_states):
                # Transition probability T[a1, s]
                # Value of state s is max(Q_stage2[s])
                q_mb[a1] += self.T[a1, s] * np.max(self.q_mf_stage2[s])

        # 2. Combine with Model-Free values
        q_net = self.w * q_mb + (1.0 - self.w) * self.q_mf_stage1
        
        return self.softmax(q_net, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Stage 2 is purely model-free in standard MB/MF hybrids
        return self.softmax(self.q_mf_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard SARSA/TD updates for the Model-Free system
        
        # Stage 2 update
        delta_2 = reward - self.q_mf_stage2[state, action_2]
        self.q_mf_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD(1) style - using actual reward/outcome)
        # Note: In a pure MF agent, this might be TD(0) or TD(1). 
        # Here we update Q_MF_stage1 based on the Q_MF_stage2 value
        delta_1 = self.q_mf_stage2[state, action_2] - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Exploration/Exploitation Shift
This model hypothesizes that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety might lead to "safety behaviors" or rigid exploitation (higher beta), or conversely, erratic behavior (lower beta). Here, we model anxiety as increasing the inverse temperature `beta`, making choices more deterministic/rigid based on current values.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases decision rigidity (exploitation).
    High STAI scores increase the inverse temperature (beta), making the participant
    stick more strictly to the option with the slightly higher value, reducing exploration.
    
    Effective Beta = beta_base * (1 + rigidity_factor * STAI)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          # Learning rate
    beta_base: [0, 10]     # Baseline inverse temperature
    rigidity_factor: [0, 5]# Multiplier for STAI's effect on beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.rigidity_factor = model_parameters
        
        # Calculate effective beta once
        self.effective_beta = self.beta_base * (1.0 + self.rigidity_factor * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.effective_beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.effective_beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Stickiness (Perseveration)
This model hypothesizes that anxious individuals are more prone to perseveration (repeating the last choice) regardless of the outcome, perhaps as a coping mechanism to reduce cognitive load or avoid decision conflict. The degree of "stickiness" is modulated by STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    The participant gets a 'bonus' added to the Q-value of the action they chose
    on the previous trial. The magnitude of this bonus is determined by STAI.
    
    Q_boosted(a) = Q(a) + (stickiness_base + anxiety_stick * STAI) * I(a == last_action)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta: [0, 10]           # Inverse temperature
    stickiness_base: [0, 5] # Base tendency to repeat choices
    anxiety_stick: [0, 5]   # Additional stickiness due to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_base, self.anxiety_stick = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        bonus = self.stickiness_base + (self.anxiety_stick * self.stai)
        
        # Copy values to avoid modifying the actual learned Q-values permanently
        q_vals = self.q_stage1.copy()
        
        # Add bonus to the previously chosen action (if it exists)
        if self.last_action1 is not None:
            q_vals[self.last_action1] += bonus
            
        return self.softmax(q_vals, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # We apply stickiness to stage 2 as well, though often less relevant if states change
        # However, if they return to the same state, they might repeat the alien choice.
        # We need to track last action per state or just last action overall?
        # Usually stickiness is just "last button pressed". Here we assume "last alien chosen in this state".
        # Since we don't track last_action_per_state in base, we'll use a simplified heuristic:
        # If the last trial was in this state, we boost that action.
        
        bonus = self.stickiness_base + (self.anxiety_stick * self.stai)
        q_vals = self.q_stage2[state].copy()
        
        if self.last_state == state and self.last_action2 is not None:
             q_vals[self.last_action2] += bonus
             
        return self.softmax(q_vals, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```