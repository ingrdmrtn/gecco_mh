Here are 3 new cognitive models that hypothesize different mechanisms for how high anxiety (STAI = 0.6625) influences decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based Control
This model hypothesizes that high anxiety impairs the use of "model-based" planning (using the transition matrix `T` to plan ahead). Instead of a pure model-free or pure model-based approach, this model uses a hybrid strategy where the weight `w` given to the model-based system is inversely proportional to anxiety. High anxiety participants rely more on simple model-free habits (TD learning) because cognitive resources are depleted by anxiety.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety reduces model-based planning capacity.
    This model implements a hybrid Reinforcement Learning agent (Daw et al., 2011).
    The final Stage 1 value is a weighted mix of Model-Free (MF) and Model-Based (MB) values:
    Q_net = w * Q_MB + (1 - w) * Q_MF
    
    Crucially, the mixing weight 'w' is modulated by anxiety:
    w_effective = w_max * (1 - stai)
    
    High anxiety (high STAI) reduces 'w_effective', forcing the agent to rely 
    more on Model-Free (habitual) learning, which might explain the rigid 
    preference for one spaceship despite rare transitions.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    w_max: [0, 1]   # Maximum model-based weight possible (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def init_model(self) -> None:
        # Calculate the effective model-based weight based on anxiety
        # If STAI is 1.0, w_effective becomes 0 (pure model-free)
        # If STAI is 0.0, w_effective is w_max
        self.w_effective = self.w_max * (1.0 - self.stai)

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Value (Standard Q-learning)
        q_mf = self.q_stage1
        
        # 2. Model-Based Value (Bellman equation using transition matrix T)
        # Q_MB(a1) = sum(P(s2|a1) * max(Q_stage2(s2, a2)))
        # We use the max Q-value of the second stage as the estimated value of that state
        v_stage2 = np.max(self.q_stage2, axis=1) # Max value for each state (X, Y)
        q_mb = np.zeros(self.n_choices)
        
        # T is shape (2, 2) -> [action, state]
        # T[0] is probs for Action 0 -> [State 0, State 1]
        q_mb[0] = np.dot(self.T[0], v_stage2)
        q_mb[1] = np.dot(self.T[1], v_stage2)
        
        # 3. Combined Value
        q_net = self.w_effective * q_mb + (1 - self.w_effective) * q_mf
        
        return self.softmax(q_net, self.beta)

    # Use standard value update for Q-values (Model-Free update)
    # The Model-Based part is computed on the fly using the static T matrix and current Q2 values.

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that high anxiety makes participants hypersensitive to the *absence* of reward (which they perceive as a loss or failure). In standard RL, receiving 0 is just a neutral update. Here, receiving 0 coins is treated as a negative prediction error with a higher learning rate or magnitude, driven by anxiety. This "fear of missing out" might drive them to stick with a "safe" option (Spaceship U) if they believe the other option is risky, or simply distort their value learning.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases sensitivity to negative outcomes (Loss Aversion).
    
    Standard RL updates Q-values based on prediction error (R - Q).
    This model splits the learning rate into alpha_pos (for positive errors) 
    and alpha_neg (for negative errors).
    
    The anxiety score (stai) amplifies the learning rate for negative prediction errors:
    alpha_neg_effective = alpha * (1 + stai * loss_sensitivity)
    
    This means high anxiety participants react more strongly when they expect a reward 
    but get nothing (0 coins), potentially driving rapid unlearning of 'bad' options 
    or reinforcing 'safe' habits.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]            # Base learning rate
    beta: [0, 10]            # Inverse temperature
    loss_sensitivity: [0, 5] # How much STAI amplifies negative learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective learning rate for negative errors
        # We clamp it at 1.0 to prevent instability
        alpha_neg = min(1.0, self.alpha * (1.0 + self.stai * self.loss_sensitivity))
        
        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha * delta_2
        else:
            self.q_stage2[state, action_2] += alpha_neg * delta_2
        
        # --- Stage 1 Update ---
        # TD(1) style update: Stage 1 value moves toward Stage 2 value
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha * delta_1
        else:
            self.q_stage1[action_1] += alpha_neg * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression
This model hypothesizes that anxiety suppresses exploration. High anxiety individuals are often characterized by behavioral inhibition and risk avoidance. In RL terms, this translates to a lower "temperature" in the softmax function (making choices more deterministic/greedy) or a specific penalty for switching. This model proposes that the inverse temperature `beta` is not static but is dynamically increased by the STAI score, making the agent more likely to exploit the current best option and less likely to explore alternatives.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety suppresses exploration (High Exploitation).
    
    The 'beta' parameter in softmax controls the exploration-exploitation trade-off.
    High beta means deterministic choice (exploitation); low beta means random choice (exploration).
    
    This model posits that beta is a function of a base parameter and the anxiety score:
    beta_effective = beta_base + (stai * anxiety_rigidity)
    
    A high STAI score increases beta, making the probability distribution over choices 
    sharper. This explains why the participant might lock onto Spaceship U and rarely 
    deviate, effectively ignoring the stochastic nature of the task.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]            # Learning rate
    beta_base: [0, 10]       # Baseline inverse temperature
    anxiety_rigidity: [0, 10]# How much anxiety increases exploitation
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.anxiety_rigidity = model_parameters
        
    def init_model(self) -> None:
        # Calculate the effective beta once, as STAI is constant for the participant
        self.beta_effective = self.beta_base + (self.stai * self.anxiety_rigidity)

    def policy_stage1(self) -> np.ndarray:
        # Use the anxiety-modulated beta
        return self.softmax(self.q_stage1, self.beta_effective)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Use the anxiety-modulated beta
        return self.softmax(self.q_stage2[state], self.beta_effective)

    # Standard value update
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```