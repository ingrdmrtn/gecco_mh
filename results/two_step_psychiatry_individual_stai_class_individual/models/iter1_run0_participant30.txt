```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety impairs Model-Based (MB) planning.
    
    Cognitive Theory:
    High anxiety consumes working memory resources required for Model-Based calculations 
    (simulating future states using the transition matrix). Consequently, anxious individuals 
    rely less on the Model-Based system and more on the computationally cheaper Model-Free system.
    
    Mechanism:
    The agent computes a weighted average of Model-Based values (derived from the transition matrix)
    and Model-Free values (learned from direct experience). The weight of the Model-Based component (w)
    is negatively modulated by the STAI score.
    
    w_effective = w_max * (1 - stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    w_max: [0, 1]  # Maximum weight given to Model-Based system (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s)))
        # self.T is [2, 2] where rows are actions (A, U) and cols are states (X, Y)
        # self.q_stage2 is [2, 2] where rows are states (X, Y) and cols are actions
        
        # Value of the best action in each state
        v_stage2 = np.max(self.q_stage2, axis=1) 
        
        # Expected value of each stage 1 action based on transition probabilities
        q_mb = self.T @ v_stage2 
        
        # 2. Retrieve Model-Free Values
        # The base class updates self.q_stage1 using TD errors, making it the Model-Free value
        q_mf = self.q_stage1
        
        # 3. Calculate Mixing Weight modulated by Anxiety
        # Higher STAI -> Lower w_eff -> More reliance on Model-Free
        w_eff = np.clip(self.w_max * (1.0 - self.stai), 0.0, 1.0)
        
        # 4. Combined Net Value
        q_net = w_eff * q_mb + (1.0 - w_eff) * q_mf
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety drives choice stickiness (perseveration).
    
    Cognitive Theory:
    Anxious individuals often exhibit rigid behavior or "safety behaviors" to reduce 
    decision conflict and uncertainty. They are more likely to repeat their previous 
    choice regardless of the outcome, using repetition as a coping mechanism.
    
    Mechanism:
    A "stickiness bonus" is added to the Q-value of the previously chosen action 
    during the selection process. The magnitude of this bonus scales with the STAI score.
    
    bonus = stick_sensitivity * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    stick_sensitivity: [0, 5] # Scaling factor for anxiety-induced stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_sensitivity = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate anxiety-dependent stickiness bonus
        bonus_val = self.stick_sensitivity * self.stai
        
        # Create a temporary view of values to add bias without altering learned Q-values
        q_biased = self.q_stage1.copy()
        
        # Add bonus to the last chosen action if it exists (i.e., not the first trial)
        if self.last_action1 is not None:
            q_biased[int(self.last_action1)] += bonus_val
            
        return self.softmax(q_biased, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates a subjective "loss" for non-rewarded outcomes.
    
    Cognitive Theory:
    Anxious individuals often have a heightened sensitivity to negative feedback. 
    In this task, receiving 0 coins is objectively neutral, but an anxious participant 
    may perceive it as a painful failure or loss.
    
    Mechanism:
    The reward signal is transformed such that 0 outcomes are treated as negative values.
    The magnitude of this negativity is proportional to the STAI score.
    
    Effective Reward (if 0 coins) = -1 * loss_factor * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    loss_factor: [0, 5] # How strongly anxiety turns lack of reward into subjective loss
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_factor = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Transform the reward signal based on anxiety
        if reward == 0:
            # Perceived as a loss
            r_eff = -1.0 * self.loss_factor * self.stai
        else:
            # Perceived normally
            r_eff = reward
            
        # Update Stage 2 values using the effective reward
        delta_2 = r_eff - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 values (TD chaining)
        # Note: We use the updated Q2 value, which will drift negative if failures occur,
        # causing the agent to actively avoid paths that lead to "loss".
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```