Here are three new cognitive models that explore different mechanisms by which anxiety (STAI) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Learning Rates (Dual Alpha)
This model hypothesizes that anxiety differentially affects how participants learn from positive versus negative outcomes. High anxiety individuals often exhibit a "negativity bias," learning more rapidly from punishments (or lack of reward) than from rewards.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety induces a negativity bias in learning rates.
    Instead of a single learning rate, this model uses separate learning rates for 
    positive (reward > 0) and negative (reward = 0) prediction errors.
    
    The anxiety score (stai) modulates the balance between these rates:
    - alpha_pos = base_alpha * (1 - stai)  [High anxiety learns less from gains]
    - alpha_neg = base_alpha * (1 + stai)  [High anxiety learns more from losses]
    
    This captures the tendency of anxious individuals to over-weigh failures.

    Parameter Bounds:
    -----------------
    base_alpha: [0, 0.5]  # Base learning rate (scaled down to allow room for modulation)
    beta: [0, 10]         # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.base_alpha, self.beta = model_parameters

    def init_model(self) -> None:
        # Modulate learning rates based on anxiety
        # We clip to ensure they stay within reasonable bounds [0, 1]
        self.alpha_pos = np.clip(self.base_alpha * (1.0 - self.stai * 0.5), 0.01, 1.0)
        self.alpha_neg = np.clip(self.base_alpha * (1.0 + self.stai * 0.5), 0.01, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 > 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (Model-Free TD)
        # Note: Using the updated Q2 value for the TD target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 > 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Exploration (Inverse Temperature Modulation)
This model hypothesizes that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety might lead to more erratic behavior (random exploration) due to difficulty concentrating or "choking under pressure," effectively lowering the inverse temperature (beta). Alternatively, it could lead to rigid, deterministic behavior. Here, we model anxiety as noise.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety acts as a noise parameter, degrading the precision of choice.
    
    The inverse temperature 'beta' controls choice stochasticity.
    Here, the effective beta is reduced by anxiety:
    beta_effective = beta_max / (1 + noise_sensitivity * stai)
    
    Higher anxiety leads to a lower effective beta, resulting in more random choices 
    (flatter softmax), representing uncertainty or cognitive interference.
    
    This model uses a pure Model-Free TD learning strategy.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta_max: [0, 20]       # Maximum theoretical precision (low anxiety)
    noise_sensitivity: [0, 5] # How strongly anxiety degrades precision
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_max, self.noise_sensitivity = model_parameters

    def init_model(self) -> None:
        # Calculate effective beta once at initialization
        # As STAI increases, the denominator grows, reducing beta_effective
        self.beta_effective = self.beta_max / (1.0 + self.noise_sensitivity * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_effective)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_effective)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced Perseveration (Stickiness)
This model hypothesizes that anxious individuals are prone to repetitive behaviors or "stickiness," regardless of reward outcomes. They might stick to the previous choice simply to reduce cognitive load or avoid the anxiety of making a new decision.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases choice perseveration (stickiness).
    
    This model adds a 'stickiness' bonus to the Q-value of the previously chosen action.
    The magnitude of this bonus is directly scaled by the STAI score.
    
    Q_net(a) = Q_learned(a) + (stai * stickiness_weight * IsLastAction(a))
    
    High anxiety participants will have a strong bias to repeat their last Stage 1 choice,
    making them slower to switch even when rewards drop.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]             # Learning rate
    beta: [0, 10]             # Inverse temperature
    stickiness_weight: [0, 5] # Base magnitude of the stickiness bias
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness_weight = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy current learned values
        q_net = self.q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is the weight scaled by the anxiety score
            bonus = self.stickiness_weight * self.stai
            q_net[int(self.last_action1)] += bonus
            
        return self.softmax(q_net, self.beta)

    # Standard TD learning for value updates
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```