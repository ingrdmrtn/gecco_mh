```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces a "Negativity Bias" in learning rates.
    High anxiety (STAI) participants are hypersensitive to worse-than-expected outcomes.
    This model implements asymmetric learning rates where the learning rate (alpha)
    is boosted specifically when the prediction error is negative. The magnitude
    of this boost is proportional to the participant's STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Base learning rate for positive/neutral outcomes
    beta: [0, 10]        # Inverse temperature
    neg_boost: [0, 10]   # Multiplier for alpha when RPE < 0, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.neg_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # --- Stage 2 Update ---
        # Calculate Prediction Error (RPE)
        pe_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha based on sign of RPE
        if pe_2 < 0:
            # Boost learning from negative surprises based on anxiety
            eff_alpha_2 = self.alpha * (1.0 + self.stai * self.neg_boost)
            eff_alpha_2 = np.clip(eff_alpha_2, 0.0, 1.0)
        else:
            eff_alpha_2 = self.alpha
            
        self.q_stage2[state, action_2] += eff_alpha_2 * pe_2
        
        # --- Stage 1 Update ---
        # TD(0) update: Target is the value of the state reached (Q_stage2)
        pe_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Apply same negativity bias logic to Stage 1
        if pe_1 < 0:
            eff_alpha_1 = self.alpha * (1.0 + self.stai * self.neg_boost)
            eff_alpha_1 = np.clip(eff_alpha_1, 0.0, 1.0)
        else:
            eff_alpha_1 = self.alpha

        self.q_stage1[action_1] += eff_alpha_1 * pe_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety drives "Safety Behavior" via Perseveration.
    Anxious individuals often exhibit rigidity or a tendency to stick with 
    familiar choices to minimize cognitive load or perceived risk.
    This model adds a "perseverance bonus" to the Q-value of the previously 
    chosen Stage 1 action. The magnitude of this stickiness is modulated by STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta: [0, 10]        # Inverse temperature
    persev_w: [0, 5]     # Weight of perseverance bonus, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.persev_w = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Create a temporary copy of Q-values for decision making
        # so we don't corrupt the actual learned values
        q_decision = self.q_stage1.copy()
        
        # If this isn't the first trial, apply the perseverance bonus
        if self.last_action1 is not None:
            # The bonus is the base weight scaled by the anxiety score
            bonus = self.persev_w * self.stai
            q_decision[int(self.last_action1)] += bonus
            
        return self.softmax(q_decision, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates an intrinsic "Need for Control".
    Anxious participants may find the "Common" transitions (high probability) 
    intrinsically rewarding because they represent control/predictability, 
    while "Rare" transitions are intrinsically punishing (loss of control).
    This model adds an internal reward/penalty to the Stage 1 update based on 
    the transition type, scaled by STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta: [0, 10]        # Inverse temperature
    control_r: [0, 2]    # Magnitude of intrinsic control reward, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.control_r = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Stage 2 update (aliens don't have transition types)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # --- Determine Transition Type ---
        # Action 0 (A) -> State 0 (X) is Common
        # Action 1 (U) -> State 1 (Y) is Common
        is_common = False
        if action_1 == 0 and state == 0: is_common = True
        elif action_1 == 1 and state == 1: is_common = True
        
        # --- Calculate Intrinsic Control Signal ---
        # Positive if common (control), negative if rare (chaos).
        # The strength of this feeling depends on anxiety (STAI).
        intrinsic_val = self.control_r * self.stai
        control_signal = intrinsic_val if is_common else -intrinsic_val
        
        # --- Stage 1 Update ---
        # The target is the value of the state reached PLUS the intrinsic control signal
        # This biases the agent to choose actions that lead to predictable outcomes
        target = self.q_stage2[state, action_2] + control_signal
        
        delta_1 = target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```