Here are three new cognitive models that explore different mechanisms for how high anxiety (STAI) might influence decision-making in this task.

### Model 1: Anxiety-Driven Model-Based Control
This model tests the hypothesis that high anxiety promotes a shift towards model-based control (planning) rather than simple habit (model-free) learning. The STAI score modulates the mixing weight `w` between model-based and model-free values. A high STAI score increases the reliance on the transition matrix `T` to compute values, reflecting a desire to control outcomes through explicit knowledge of the environment structure.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Model-Based Control]
    This model hypothesizes that anxiety drives participants to rely more on 
    explicit planning (model-based control) to mitigate uncertainty. The mixing 
    weight 'w' (trade-off between model-based and model-free values) is 
    modulated by the STAI score. Higher anxiety leads to a higher 'w', 
    meaning more reliance on the transition structure of the task.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta: [0, 10]          - Inverse temperature
    w_base: [0, 1]         - Baseline model-based weight
    w_stai_slope: [0, 1]   - Sensitivity of w to STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_stai_slope = model_parameters
        
        # Calculate effective mixing weight w
        # w is bounded between 0 (pure model-free) and 1 (pure model-based)
        raw_w = self.w_base + (self.w_stai_slope * self.stai)
        self.w = np.clip(raw_w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Model-Based Value Calculation:
        # Q_MB(s1, a) = sum(T(s1, a, s2) * max(Q_stage2(s2, :)))
        # Here, T is static [0.7, 0.3] structure
        
        # Value of state X (index 0) and Y (index 1) is the max Q-value available there
        v_x = np.max(self.q_stage2[0])
        v_y = np.max(self.q_stage2[1])
        
        # Q_MB for Choice A (index 0) -> 0.7*X + 0.3*Y
        q_mb_a = self.T[0, 0] * v_x + self.T[0, 1] * v_y
        
        # Q_MB for Choice U (index 1) -> 0.3*X + 0.7*Y
        q_mb_u = self.T[1, 0] * v_x + self.T[1, 1] * v_y
        
        q_mb = np.array([q_mb_a, q_mb_u])
        
        # Integrated Q-value: w * Q_MB + (1-w) * Q_MF
        q_net = self.w * q_mb + (1 - self.w) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Standard TD update for Stage 1 (Model-Free component)
        # Note: In hybrid models, q_stage1 usually represents the MF value
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Punishment Sensitivity
This model hypothesizes that anxiety specifically amplifies the learning signal from negative outcomes (or lack of reward). Instead of a single learning rate, the model uses separate learning rates for positive and negative prediction errors. The STAI score specifically boosts the learning rate for negative prediction errors (`alpha_neg`), making the participant quicker to abandon options that yield zero coins.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Punishment Sensitivity]
    This model hypothesizes that high anxiety makes individuals hypersensitive 
    to negative outcomes (omission of reward). The learning rate is split into 
    alpha_pos (for positive RPEs) and alpha_neg (for negative RPEs). 
    The STAI score acts as a multiplier on the negative learning rate, 
    causing faster avoidance learning when rewards are not received.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      - Learning rate for positive prediction errors
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors
    stai_sens: [0, 5]      - Multiplier for STAI effect on alpha_neg
    beta: [0, 10]          - Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.stai_sens, self.beta = model_parameters
        
        # Calculate effective negative learning rate
        # alpha_neg = base + (sensitivity * STAI * base)
        # This implies anxiety amplifies the base negative learning rate
        self.alpha_neg = self.alpha_neg_base * (1.0 + self.stai_sens * self.stai)
        self.alpha_neg = np.clip(self.alpha_neg, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Using the updated Q2 value for the TD error calculation
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Stickiness (Perseveration)
This model tests the hypothesis that anxiety increases "stickiness" or choice perseverationâ€”a tendency to repeat the previous choice regardless of reward history, perhaps as a safety behavior or to reduce cognitive load. The STAI score determines the magnitude of a choice bonus added to the previously selected action in Stage 1.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Stickiness]
    This model hypothesizes that anxiety increases choice perseveration (stickiness).
    Regardless of value learning, anxious individuals may prefer to repeat 
    familiar actions to reduce cognitive load or uncertainty. 
    The STAI score scales a 'stickiness' bonus added to the Q-value of the 
    previously chosen action in Stage 1.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta: [0, 10]          - Inverse temperature
    stick_base: [0, 5]     - Baseline stickiness bonus
    stick_stai: [0, 5]     - Additional stickiness per unit of STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stick_stai = model_parameters
        
        # Calculate total stickiness parameter
        self.stickiness = self.stick_base + (self.stick_stai * self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Copy Q-values to avoid modifying the actual learned values
        q_modified = self.q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            q_modified[self.last_action1] += self.stickiness
            
        return self.softmax(q_modified, self.beta)

    # Standard value update (TD learning)
    # We use the default implementation from Base, so no override needed for value_update
    # However, we need to ensure last_action1 is tracked, which the Base class does.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```