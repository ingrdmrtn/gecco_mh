```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition matrix
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = np.zeros((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    """Create function interface from model class."""
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Impaired Model-Based Control]
    This model hypothesizes that anxiety consumes cognitive resources, reducing the 
    participant's ability to use Model-Based (MB) planning.
    
    The stage-1 choice is a hybrid of Model-Free (MF) values (learned via TD) and 
    Model-Based (MB) values (computed from the transition matrix).
    The weight `w` assigned to the MB system is reduced by the STAI score.
    
    w_effective = w_max * (1 - stai)
    Q_net = w_effective * Q_MB + (1 - w_effective) * Q_MF

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   - Learning rate for MF values
    beta: [0, 10]   - Inverse temperature
    w_max: [0, 1]   - Maximum weight for MB system (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Values (from standard TD learning in base class)
        q_mf = self.q_stage1
        
        # 2. Model-Based Values
        # V(State) is the max Q-value available in that state
        v_state_0 = np.max(self.q_stage2[0])
        v_state_1 = np.max(self.q_stage2[1])
        
        # Q_MB(Action) = Sum(P(State|Action) * V(State))
        q_mb = np.zeros(self.n_choices)
        # Action 0 (A) -> 0.7 to State 0, 0.3 to State 1
        q_mb[0] = self.T[0, 0] * v_state_0 + self.T[0, 1] * v_state_1
        # Action 1 (U) -> 0.3 to State 0, 0.7 to State 1
        q_mb[1] = self.T[1, 0] * v_state_0 + self.T[1, 1] * v_state_1
        
        # 3. Hybridization modulated by Anxiety
        # Higher anxiety -> lower w_effective -> more Model-Free behavior
        w_effective = self.w_max * (1.0 - self.stai)
        w_effective = np.clip(w_effective, 0.0, 1.0)
        
        q_net = w_effective * q_mb + (1.0 - w_effective) * q_mf
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Negative Bias in Learning]
    This model hypothesizes that anxious individuals are hypersensitive to negative outcomes 
    (omission of reward). They update their value estimates more drastically when 
    expectations are violated negatively (disappointment) compared to positive surprises.
    
    The learning rate `alpha` is boosted by the STAI score specifically when the 
    prediction error is negative.
    
    If delta < 0: alpha_eff = alpha * (1 + phi * stai)
    Else:         alpha_eff = alpha

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   - Base learning rate
    beta: [0, 10]   - Inverse temperature
    phi: [0, 5]     - Scaling factor for negative prediction error sensitivity
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha based on sign of error
        if delta_2 < 0:
            alpha_eff_2 = self.alpha * (1.0 + self.phi * self.stai)
            # Clip to ensure stability (max 1.0)
            alpha_eff_2 = min(alpha_eff_2, 1.0)
        else:
            alpha_eff_2 = self.alpha
            
        self.q_stage2[state, action_2] += alpha_eff_2 * delta_2
        
        # Stage 1 Update (TD)
        # We use the updated Q2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Apply same bias logic to stage 1 errors? 
        # Usually bias is about external reward, but let's apply consistency.
        if delta_1 < 0:
            alpha_eff_1 = self.alpha * (1.0 + self.phi * self.stai)
            alpha_eff_1 = min(alpha_eff_1, 1.0)
        else:
            alpha_eff_1 = self.alpha

        self.q_stage1[action_1] += alpha_eff_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Memory Decay]
    This model hypothesizes that high anxiety consumes working memory capacity, 
    leading to faster forgetting of learned values.
    
    Before each trial, all Q-values decay towards a neutral value (0.5).
    The rate of decay is proportional to the participant's STAI score.
    
    Q(t) = (1 - decay) * Q(t-1) + decay * 0.5
    decay = gamma * stai

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   - Learning rate
    beta: [0, 10]   - Inverse temperature
    gamma: [0, 1]   - Decay scaling factor (controls how much anxiety causes forgetting)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.gamma = model_parameters

    def pre_trial(self) -> None:
        # Calculate decay rate based on anxiety
        decay_rate = self.gamma * self.stai
        decay_rate = np.clip(decay_rate, 0.0, 1.0)
        
        # Neutral value (0.5 assumes rewards are 0 or 1)
        neutral_val = 0.5
        
        # Decay Stage 1 values
        self.q_stage1 = (1.0 - decay_rate) * self.q_stage1 + decay_rate * neutral_val
        
        # Decay Stage 2 values
        self.q_stage2 = (1.0 - decay_rate) * self.q_stage2 + decay_rate * neutral_val

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```