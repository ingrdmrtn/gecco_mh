```python
import numpy as np
from abc import ABC, abstractmethod

# Base Class (Included for context, not modified in output)
# ... (CognitiveModelBase definition) ...

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Model-Based vs. Model-Free Control.
    
    High anxiety impairs Model-Based (planning) reasoning, leading to a reliance 
    on Model-Free (habitual) control. This model implements a hybrid RL agent 
    where the weight of the Model-Based system is reduced by the participant's 
    anxiety (STAI) score.
    
    Mechanism:
    Q_net = w * Q_MB + (1 - w) * Q_MF
    w = w_max * (1 - stai)
    
    If STAI is high, 'w' becomes small, and the participant relies mostly on 
    cached Q-values (Model-Free) rather than planning using the transition matrix.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate for Model-Free values
    beta: [0, 10]   # Inverse temperature
    w_max: [0, 1]   # Maximum Model-Based weight (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values (Planning)
        # Q_MB(a) = Sum(P(s|a) * V(s)) where V(s) = max(Q_stage2(s, :))
        # We use the fixed transition matrix self.T
        v_stage2 = np.max(self.q_stage2, axis=1)  # Max value of each state (X, Y)
        q_mb = np.dot(self.T, v_stage2)           # Expected value given transitions

        # 2. Calculate Mixing Weight modulated by Anxiety
        # High anxiety -> Low w -> Low MB influence
        w = self.w_max * (1.0 - self.stai)
        
        # 3. Combine with Model-Free values (self.q_stage1)
        q_net = w * q_mb + (1 - w) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Cumulative Habit Formation.
    
    Unlike simple "stickiness" (repeating the last choice), this model posits 
    that anxiety accelerates the formation of strong, cumulative habits. 
    A separate 'habit trace' tracks the history of choices. High anxiety 
    increases the influence of this habit trace on the decision policy.
    
    Mechanism:
    Habit(t+1) = Habit(t) + alpha * (Choice - Habit(t))
    Q_net = Q_value + (stai * habit_weight * Habit)
    
    This explains the participant's rigid adherence to spaceship U even 
    when rewards are intermittent.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate (shared for Q and Habit)
    beta: [0, 10]        # Inverse temperature
    habit_weight: [0, 5] # Strength of habit influence
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.habit_weight = model_parameters

    def init_model(self) -> None:
        # Initialize habit strengths for Stage 1 choices (A and U)
        self.habit_trace = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Add anxiety-weighted habit bonus to Q-values
        # The influence of habit is directly scaled by STAI
        habit_bonus = self.stai * self.habit_weight * self.habit_trace
        q_net = self.q_stage1 + habit_bonus
        return self.softmax(q_net, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Update habit trace: Strengthen chosen, weaken unchosen
        # We use a vector update: chosen gets pushed towards 1, unchosen towards 0
        # Using self.alpha as the habit learning rate for parsimony
        one_hot_action = np.zeros(self.n_choices)
        one_hot_action[action_1] = 1.0
        
        self.habit_trace += self.alpha * (one_hot_action - self.habit_trace)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Risk Aversion (Mean-Variance Tradeoff).
    
    Anxious individuals are often intolerant of uncertainty. This model 
    assumes the participant tracks not just the mean value (Q) but also the 
    variance (uncertainty/risk) of the rewards for each Stage 1 option.
    High anxiety leads to a penalty for options with high variance.
    
    Mechanism:
    Utility = Q - (stai * risk_penalty * sqrt(Variance))
    
    The participant might stick to U if A (despite being 0 variance) has 
    a much lower mean, or if the variance of U is tolerated because the 
    mean is sufficiently high.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]         # Learning rate for Mean and Variance
    beta: [0, 10]         # Inverse temperature
    risk_penalty: [0, 5]  # Penalty weight for variance
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.risk_penalty = model_parameters

    def init_model(self) -> None:
        # Initialize variance tracking for Stage 1 choices
        self.q_var_stage1 = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Calculate Risk-Adjusted Utility
        # Utility = Mean - (Anxiety * Penalty * StdDev)
        std_dev = np.sqrt(self.q_var_stage1)
        utility = self.q_stage1 - (self.stai * self.risk_penalty * std_dev)
        
        return self.softmax(utility, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Standard Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Stage 1 Mean Update
        # We use the Stage 2 value as the target for Stage 1 (TD-0)
        target = self.q_stage2[state, action_2]
        delta_1 = target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # 3. Stage 1 Variance Update
        # Var <- Var + alpha * ((Error)^2 - Var)
        # The error here is the difference between the observed outcome (target) and the mean
        sq_error = delta_1 ** 2
        self.q_var_stage1[action_1] += self.alpha * (sq_error - self.q_var_stage1[action_1])

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```