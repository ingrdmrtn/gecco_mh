Here are three new cognitive models exploring different mechanisms by which high anxiety (STAI = 0.725) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model tests the hypothesis that high anxiety impairs complex "model-based" planning (using the transition matrix) and favors simpler "model-free" learning (repeating rewarded actions). The STAI score determines the mixing weight between these two systems.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety shifts the balance from Model-Based (MB) to Model-Free (MF) control.
    
    High anxiety consumes cognitive resources, impairing the ability to use the transition 
    structure (MB). Instead, the participant relies more on simple temporal difference learning (MF).
    
    The mixing weight 'w' is modulated by STAI:
    w_effective = w_base * (1 - stai)
    
    If w_effective is close to 1, behavior is Model-Based.
    If w_effective is close to 0, behavior is Model-Free.
    Since this participant has high anxiety (0.725), w_effective will be low, predicting MF behavior.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    w_base: [0, 1]  # Baseline model-based weight (for a hypothetical 0-anxiety person)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (Q_MB)
        # Q_MB for stage 1 is calculated on the fly using transition matrix T and stage 2 values
        pass

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Value (Standard Q-learning)
        q_mf = self.q_stage1
        
        # 2. Model-Based Value (Bellman equation using known T)
        # Q_MB(a1) = P(s1|a1)*max(Q2(s1)) + P(s2|a1)*max(Q2(s2))
        # We use the max of stage 2 values as the estimate of the state value
        v_stage2 = np.max(self.q_stage2, axis=1) # Max value for each state (X, Y)
        q_mb = np.zeros(self.n_choices)
        
        # For action 0 (Spaceship A) -> transitions to state 0 (X) with prob 0.7, state 1 (Y) with prob 0.3
        q_mb[0] = self.T[0, 0] * v_stage2[0] + self.T[0, 1] * v_stage2[1]
        # For action 1 (Spaceship U) -> transitions to state 0 (X) with prob 0.3, state 1 (Y) with prob 0.7
        q_mb[1] = self.T[1, 0] * v_stage2[0] + self.T[1, 1] * v_stage2[1]
        
        # 3. Mix them based on anxiety
        # Higher STAI reduces the effective weight of the model-based system
        w_eff = self.w_base * (1.0 - self.stai)
        
        q_net = (1 - w_eff) * q_mf + w_eff * q_mb
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for MF values
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD(1) - using the actual reward to update stage 1 directly, typical for simple MF)
        # Note: In pure MF, we often update Q1 based on Q2, but here we use a simple TD(1) approximation 
        # or SARSA-like update. Let's stick to the base class logic but ensure we update the MF table.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Punishment Sensitivity
This model tests the hypothesis that high anxiety leads to an overreaction to negative outcomes (zero rewards). Instead of learning symmetrically from gains and losses, the participant updates their value estimates much more aggressively when they receive no coins.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety amplifies the learning rate specifically for negative prediction errors (punishment/omission).
    
    We split the learning rate into alpha_pos (for positive RPEs) and alpha_neg (for negative RPEs).
    The base alpha is used for positive updates.
    For negative updates, the learning rate is boosted by the STAI score:
    alpha_neg = alpha * (1 + stai * sensitivity_boost)
    
    This means high anxiety participants will decrease the value of an action much faster 
    after a failure than they increase it after a success.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]             # Base learning rate (for positive outcomes)
    beta: [0, 10]             # Inverse temperature
    sensitivity_boost: [0, 5] # How much STAI amplifies the negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.sensitivity_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate Stage 2 RPE
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine learning rate based on sign of RPE
        if delta_2 >= 0:
            eff_alpha_2 = self.alpha
        else:
            # Boost learning rate for negative outcomes based on anxiety
            eff_alpha_2 = self.alpha * (1.0 + self.stai * self.sensitivity_boost)
            # Cap at 1.0 to maintain stability
            eff_alpha_2 = min(eff_alpha_2, 1.0)
            
        self.q_stage2[state, action_2] += eff_alpha_2 * delta_2
        
        # Calculate Stage 1 RPE
        # We use the updated Q2 value to drive Q1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 >= 0:
            eff_alpha_1 = self.alpha
        else:
            eff_alpha_1 = self.alpha * (1.0 + self.stai * self.sensitivity_boost)
            eff_alpha_1 = min(eff_alpha_1, 1.0)

        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Noise (Inverse Temperature Modulation)
This model tests the hypothesis that anxiety creates "neural noise" or decision instability, effectively lowering the inverse temperature (beta) in the softmax function. A highly anxious participant might know the best option but fail to select it consistently due to arousal-induced randomness.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety reduces decision consistency (increases exploration/noise).
    
    Instead of a fixed beta, the effective beta is reduced by the STAI score.
    High anxiety leads to more random choices (lower beta), regardless of learned values.
    
    beta_effective = beta_max / (1 + stai * noise_factor)
    
    If STAI is high, the denominator is large, making beta_effective small (more random).
    If STAI is low, beta_effective approaches beta_max (more deterministic).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta_max: [0, 20]    # Maximum consistency (for a 0-anxiety person)
    noise_factor: [0, 10]# Scaling factor for how anxiety reduces consistency
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_max, self.noise_factor = model_parameters
        
        # Pre-calculate effective beta since STAI is constant for the participant
        self.beta_effective = self.beta_max / (1.0 + self.stai * self.noise_factor)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_effective)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_effective)

    # Standard value update
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```