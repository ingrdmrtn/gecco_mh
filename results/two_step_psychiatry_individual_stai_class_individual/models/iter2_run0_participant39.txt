Here are 3 new cognitive models exploring different mechanisms by which anxiety (STAI) might influence decision-making in the two-step task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model hypothesizes that high anxiety impairs complex model-based planning (using the transition matrix `T`) and favors simpler model-free learning (TD learning). The STAI score acts as a mixing parameter: higher anxiety leads to a lower weight on the model-based component.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety impairs Model-Based control.
    High anxiety consumes working memory resources, making it harder to maintain 
    and use the cognitive map (transition matrix). This model implements a hybrid 
    Model-Based (MB) / Model-Free (MF) controller. The mixing weight `w` determines 
    the balance. Crucially, `w` is not a free parameter but is derived directly 
    from the STAI score and a sensitivity parameter `stai_sens`.
    
    Higher STAI -> Lower `w` (Less Model-Based, More Model-Free).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stai_sens: [0, 5]   # Sensitivity of MB-weight to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stai_sens = model_parameters

    def init_model(self) -> None:
        # Calculate the mixing weight w based on STAI.
        # We model w as decaying with anxiety: w = 1 / (1 + sensitivity * stai)
        # If stai is 0, w=1 (pure MB). If stai is high, w approaches 0 (pure MF).
        self.w = 1.0 / (1.0 + self.stai_sens * self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Model-Free Value (TD)
        q_mf = self.q_stage1
        
        # Model-Based Value (Bellman)
        # Q_MB(a) = sum_s' T(s'|a) * max_a' Q2(s', a')
        # We use the max of stage 2 values as the value of the state
        v_stage2 = np.max(self.q_stage2, axis=1) # Shape (2,)
        q_mb = np.zeros(self.n_choices)
        
        # T is shape (2, 2) -> T[action, state] roughly, but here T is defined as:
        # T[0] = probs for action 0 -> [0.7, 0.3] (Planet X, Planet Y)
        # T[1] = probs for action 1 -> [0.3, 0.7] (Planet X, Planet Y)
        # Note: Base class defines T as [[0.7, 0.3], [0.3, 0.7]]
        # Row 0 corresponds to Action A (0), Row 1 to Action U (1)
        
        q_mb[0] = np.dot(self.T[0], v_stage2)
        q_mb[1] = np.dot(self.T[1], v_stage2)
        
        # Hybrid Value
        q_net = self.w * q_mb + (1 - self.w) * q_mf
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Negative Bias (Pessimism)
This model hypothesizes that anxious individuals have a biased perception of rewards, specifically over-weighting negative outcomes or under-weighting positive ones. Instead of a standard learning rate, the effective reward is transformed based on the STAI score, making the agent "pessimistic" about the value of the environment.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces Pessimism (Negative Bias).
    Anxious individuals may perceive rewards as less valuable or losses as more 
    painful. This model scales the perceived reward by a factor derived from STAI.
    Specifically, positive rewards are dampened by high anxiety.
    
    Effective Reward = Reward * (1 - pessimism_factor * STAI)
    
    If STAI is high, positive rewards feel smaller, leading to slower value 
    accumulation and potentially faster abandonment of choices that don't pay off immediately.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta: [0, 10]        # Inverse temperature
    pessimism: [0, 1]    # How much STAI dampens positive rewards
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pessimism = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Apply pessimism bias
        # If reward is positive, reduce it based on anxiety.
        # If reward is negative or zero, we leave it (or could amplify it, but let's stick to dampening gain).
        
        effective_reward = reward
        if reward > 0:
            # Dampening factor: 1.0 when stai=0, decreases as stai increases
            # We clip to ensure it doesn't flip sign
            dampener = max(0.0, 1.0 - (self.pessimism * self.stai))
            effective_reward = reward * dampener
            
        # Standard TD update with effective_reward
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration (Uncertainty Intolerance)
This model hypothesizes that anxiety manifests as "Intolerance of Uncertainty." Instead of avoiding risk, the anxious participant might paradoxically explore *more* when values are uncertain or low, trying to resolve ambiguity. Alternatively, high anxiety might lead to erratic switching (high temperature) when things aren't going well. Here, we model STAI as modulating the inverse temperature (`beta`), but specifically making behavior more stochastic (lower beta) when the participant is anxious, reflecting a "panic" or "scrambling" response to find a better outcome.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases decision noise (Scrambling).
    High anxiety interferes with the ability to consistently select the optimal 
    option, effectively lowering the inverse temperature (beta). 
    
    Here, the effective beta is the baseline beta reduced by the STAI score.
    Beta_effective = Beta_base / (1 + noise_scale * STAI)
    
    This implies that highly anxious participants are more random/exploratory 
    and less exploitative of their learned values.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta_base: [0, 10]   # Baseline inverse temperature (for low anxiety)
    noise_scale: [0, 5]  # How strongly STAI increases noise (reduces beta)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.noise_scale = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective beta
        # Higher STAI -> Higher denominator -> Lower effective beta -> More noise
        beta_eff = self.beta_base / (1.0 + self.noise_scale * self.stai)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Apply same noise modulation to stage 2
        beta_eff = self.beta_base / (1.0 + self.noise_scale * self.stai)
        return self.softmax(self.q_stage2[state], beta_eff)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```