Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based Control
This model tests the hypothesis that anxiety impairs model-based reasoning. In the two-step task, "model-based" control involves using the transition matrix (knowledge of which spaceship goes where) to plan. "Model-free" control just repeats rewarded actions. This model proposes that higher anxiety reduces the weight ($w$) given to the model-based system, forcing the participant to rely more on simple model-free reinforcement.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Model-Based Control]
    This model hypothesizes that anxiety acts as a cognitive load that reduces 
    the capacity for model-based planning. The parameter 'w' (mixing weight) 
    determines the balance between Model-Based (MB) and Model-Free (MF) control.
    Here, 'w' is negatively modulated by STAI: higher anxiety leads to lower 'w',
    meaning less model-based planning and more reliance on simple habit (MF).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       - Learning rate
    beta: [0, 10]       - Inverse temperature
    w_max: [0, 1]       - Maximum model-based weight (at 0 anxiety)
    stai_damp: [0, 1]   - Strength of anxiety's dampening effect on w
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.stai_damp = model_parameters
        
        # Calculate mixing weight w based on STAI
        # w = w_max * (1 - stai_damp * stai)
        # If anxiety is high and dampening is strong, w approaches 0 (pure Model-Free)
        self.w = self.w_max * (1.0 - (self.stai_damp * self.stai))
        self.w = np.clip(self.w, 0.0, 1.0)

    def init_model(self) -> None:
        # Model-Free Q-values (TD learning)
        self.q_mf = np.zeros(self.n_choices)
        # Model-Based Q-values are computed on the fly

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values
        # Q_MB(a) = sum(P(s|a) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            for s in range(self.n_states):
                # Max value of next state
                v_state = np.max(self.q_stage2[s])
                q_mb[a] += self.T[a, s] * v_state

        # 2. Combine with Model-Free values using weight w
        q_net = (self.w * q_mb) + ((1 - self.w) * self.q_mf)
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 values (common to both systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 Model-Free values (TD(1) style for simplicity or SARSA)
        # Here we use the standard TD update from the base class logic but applied to q_mf
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry
This model hypothesizes that anxiety biases how people learn from positive versus negative outcomes. Specifically, anxious individuals might be hypersensitive to negative outcomes (punishment/omission of reward) or have blunted learning from positive outcomes. This model splits the learning rate $\alpha$ into positive and negative components, where the balance is shifted by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Learning Rate Asymmetry]
    This model hypothesizes that anxiety alters the balance between learning from 
    positive prediction errors (better than expected) and negative prediction errors 
    (worse than expected). The STAI score modulates the ratio between alpha_pos 
    and alpha_neg. High anxiety might amplify learning from disappointment (alpha_neg)
    relative to reward (alpha_pos).

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]   - Base learning rate
    beta: [0, 10]        - Inverse temperature
    bias_stai: [-1, 1]   - Modulation factor. 
                           Positive: Anxiety increases alpha_neg, decreases alpha_pos.
                           Negative: Anxiety increases alpha_pos, decreases alpha_neg.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.bias_stai = model_parameters
        
        # Calculate modulation
        mod = self.bias_stai * self.stai
        
        # Apply modulation to create asymmetric learning rates
        # We clip to ensure they stay in [0, 1]
        self.alpha_pos = np.clip(self.alpha_base * (1.0 - mod), 0.0, 1.0)
        self.alpha_neg = np.clip(self.alpha_base * (1.0 + mod), 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        pe2 = reward - self.q_stage2[state, action_2]
        alpha2 = self.alpha_pos if pe2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha2 * pe2
        
        # Stage 1 Update
        # Note: Base class uses Q-value of next state. 
        pe1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha1 = self.alpha_pos if pe1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha1 * pe1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression
This model hypothesizes that anxiety reduces exploration. High-anxiety individuals may prefer to exploit known options rather than explore uncertain ones. This is implemented by modulating the inverse temperature parameter $\beta$ (which controls randomness/exploration) based on the STAI score. A higher STAI leads to a higher $\beta$, resulting in more deterministic (less exploratory) choices.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Exploration Suppression]
    This model hypothesizes that anxiety reduces the willingness to explore.
    In reinforcement learning, the 'beta' parameter (inverse temperature) controls
    the exploration-exploitation trade-off. Higher beta means more exploitation 
    (choosing the best option deterministically). This model posits that beta 
    increases linearly with the STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        - Learning rate
    beta_min: [0, 10]    - Minimum inverse temperature (at 0 anxiety)
    beta_slope: [0, 10]  - Increase in beta per unit of STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_min, self.beta_slope = model_parameters
        
        # Calculate effective beta
        # High anxiety -> High beta -> Low exploration (deterministic choice)
        self.beta_eff = self.beta_min + (self.beta_slope * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_eff)

    # Use standard value update from base class
    # value_update is inherited

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```