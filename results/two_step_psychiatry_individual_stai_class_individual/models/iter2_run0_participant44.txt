Here are three cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this task.

### Model 1: Anxiety-Impaired Model-Based Control
This model tests the hypothesis that high anxiety consumes cognitive resources (e.g., working memory), impairing the participant's ability to use a Model-Based (planning) strategy. Instead, anxious participants rely more heavily on Model-Free (habitual) control. The model implements a hybrid reinforcement learning agent where the mixing weight $w$ (controlling the balance between Model-Based and Model-Free values) is negatively modulated by the STAI score.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Impaired Model-Based Control]
    High anxiety consumes cognitive resources, reducing the capacity for 
    Model-Based (planning) control and increasing reliance on Model-Free (habitual) control.
    
    The mixing weight 'w' between MB and MF systems is negatively scaled by STAI.
    w = w_max * (1 - STAI)
    
    Q_net = w * Q_MB + (1-w) * Q_MF

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   - Learning rate
    beta: [0, 10]   - Inverse temperature
    w_max: [0, 1]   - Maximum model-based weight (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values
        # V(state) = max(Q_stage2[state])
        v_x = np.max(self.q_stage2[0])
        v_y = np.max(self.q_stage2[1])
        
        # Q_MB = T * V
        # A (idx 0) -> 0.7 X + 0.3 Y
        # U (idx 1) -> 0.3 X + 0.7 Y
        q_mb = np.array([
            0.7 * v_x + 0.3 * v_y,
            0.3 * v_x + 0.7 * v_y
        ])
        
        # 2. Calculate Mixing Weight modulated by STAI
        # Higher anxiety -> Lower w (Less MB, More MF)
        w = self.w_max * (1.0 - self.stai)
        w = np.clip(w, 0.0, 1.0)
        
        # 3. Combine MB and MF values
        # self.q_stage1 represents the Model-Free values (updated via TD in value_update)
        q_net = w * q_mb + (1 - w) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Pessimism (Unchosen Decay)
This model hypothesizes that anxious individuals develop a negative bias towards options they do not choose. The longer an option (like Spaceship U) is ignored, the more "dangerous" or poor it is perceived to be. This is modeled as a cumulative value decay applied to the unchosen option in Stage 1, with the decay rate scaled by STAI. This mechanism creates a "tunnel vision" effect where the participant becomes locked into their initial preference.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Pessimism on Unchosen Options]
    Anxious individuals may develop a negative bias towards options they do not choose,
    perceiving them as increasingly risky or poor the longer they are ignored.
    
    This is modeled as a cumulative value decay (penalty) on the unchosen stage-1 option,
    scaled by STAI.
    
    Q(unchosen) = Q(unchosen) - (decay_rate * STAI)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      - Learning rate
    beta: [0, 10]      - Inverse temperature
    decay_rate: [0, 1] - Magnitude of pessimism decay per trial
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_rate = model_parameters

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Call base to update last_action tracking
        super().post_trial(action_1, state, action_2, reward)
        
        # Apply pessimism penalty to the unchosen stage 1 action
        # If they chose 0, unchosen is 1. If they chose 1, unchosen is 0.
        unchosen_action = 1 - action_1
        
        # The penalty is proportional to anxiety
        penalty = self.decay_rate * self.stai
        
        # Decrease the value of the path not taken
        self.q_stage1[unchosen_action] -= penalty

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Blunted Negative Learning
This model hypothesizes that high anxiety leads to a "safety signal maintenance" strategy where the participant ignores or down-weights negative outcomes (0 coins) to preserve their sense of safety in the chosen option. The model uses asymmetric learning rates: the learning rate for negative prediction errors is reduced by the STAI score. This explains why the participant might stick with Spaceship A even after receiving no coinsâ€”they simply don't update their value estimate downwards as much as a non-anxious person would.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Blunted Negative Learning]
    High anxiety might lead to avoidance of processing negative information (safety signal maintenance).
    The participant ignores or down-weights prediction errors when the outcome is worse than expected 
    (negative delta), effectively maintaining high value estimates for the "safe" option despite failures.
    
    alpha_neg = alpha * (1 - blunt_factor * STAI)
    alpha_pos = alpha

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        - Base learning rate (for positive errors)
    beta: [0, 10]        - Inverse temperature
    blunt_factor: [0, 1] - How much STAI reduces learning from negative errors
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.blunt_factor = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha for stage 2 based on sign of prediction error
        if delta_2 < 0:
            # Blunted learning for negative outcomes
            eff_alpha_2 = self.alpha * (1.0 - (self.blunt_factor * self.stai))
            eff_alpha_2 = max(0.0, eff_alpha_2)
        else:
            # Normal learning for positive outcomes
            eff_alpha_2 = self.alpha
            
        self.q_stage2[state, action_2] += eff_alpha_2 * delta_2
        
        # --- Stage 1 Update ---
        # TD update: Target is the value of the state we landed in (Q2 of chosen action)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 < 0:
            eff_alpha_1 = self.alpha * (1.0 - (self.blunt_factor * self.stai))
            eff_alpha_1 = max(0.0, eff_alpha_1)
        else:
            eff_alpha_1 = self.alpha

        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```