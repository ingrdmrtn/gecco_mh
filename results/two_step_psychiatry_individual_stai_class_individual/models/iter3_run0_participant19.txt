Here are three new cognitive models exploring different mechanisms of how anxiety (STAI) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model hypothesizes that anxiety affects the balance between model-based (planning) and model-free (habitual) control. Specifically, it tests if higher anxiety leads to a reliance on habitual strategies (lower `w`) or perhaps over-thinking/planning (higher `w`). The mixing parameter `w` determines how much the Stage 1 value is derived from the transition matrix (Model-Based) versus direct TD learning (Model-Free).

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the balance between Model-Based (MB) and Model-Free (MF) control.
    
    The parameter 'w' (weight) controls the mix: Q_net = w * Q_MB + (1-w) * Q_MF.
    This model posits that 'w' is a linear function of the STAI score.
    w = w_base + (w_slope * stai)
    
    If w_slope is negative, anxiety reduces model-based planning (more habitual).
    If w_slope is positive, anxiety increases model-based planning (more deliberation).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base mixing weight
    w_slope: [-1, 1]    # Sensitivity of w to STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_slope = model_parameters

    def init_model(self) -> None:
        # Calculate the mixing weight w based on anxiety
        # Clamp w between 0 and 1 to ensure validity
        raw_w = self.w_base + (self.w_slope * self.stai)
        self.w = np.clip(raw_w, 0.0, 1.0)
        
        # Initialize Model-Free Q-values (Q_MF)
        self.q_mf = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Calculate Model-Based Q-values (Q_MB)
        # Q_MB(a1) = sum(T(a1, s2) * max(Q_stage2(s2, :)))
        q_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            for s2 in range(self.n_states):
                # Transition probability T[a1, s2]
                # Value of state s2 is max of its stage 2 Q-values
                q_mb[a1] += self.T[a1, s2] * np.max(self.q_stage2[s2])
        
        # Combine MB and MF values
        q_net = (self.w * q_mb) + ((1 - self.w) * self.q_mf)
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 values (standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 Model-Free values (TD(1) / SARSA-like)
        # Using the actual reward obtained at stage 2 to update stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry (Pos/Neg Bias)
This model hypothesizes that anxiety creates a bias in how participants learn from positive versus negative prediction errors. High anxiety might make individuals hyper-sensitive to negative outcomes (learning more from disappointment) or blunted to positive ones. The learning rate `alpha` is split into `alpha_pos` and `alpha_neg`, where the ratio or difference is modulated by STAI.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates an asymmetry in learning from positive vs negative prediction errors.
    
    We define a base learning rate 'alpha'.
    We define a 'bias' parameter modulated by STAI.
    
    alpha_pos = alpha
    alpha_neg = alpha * (1 + bias_slope * stai)
    
    If bias_slope > 0, anxiety increases learning from negative errors (hypersensitivity to failure).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate for positive errors
    beta: [0, 10]       # Inverse temperature
    bias_slope: [-2, 5] # How much STAI scales the negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.bias_slope = model_parameters

    def init_model(self) -> None:
        # Calculate alpha_neg based on STAI
        # We clamp alpha_neg to [0, 1] to maintain stability
        raw_alpha_neg = self.alpha * (1.0 + self.bias_slope * self.stai)
        self.alpha_neg = np.clip(raw_alpha_neg, 0.0, 1.0)
        self.alpha_pos = self.alpha

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        lr_2 = self.alpha_pos if delta_2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += lr_2 * delta_2
        
        # Stage 1 Update
        # Note: Standard TD uses the value of the next state, here approximated by Q2
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        lr_1 = self.alpha_pos if delta_1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += lr_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration/Exploitation Trade-off (Inverse Temperature Modulation)
This model hypothesizes that anxiety directly impacts the randomness of choice (exploration vs. exploitation). High anxiety might lead to more erratic behavior (high noise/low beta) due to panic, or conversely, very rigid behavior (high beta) due to a need for control. Here, the inverse temperature `beta` is a function of STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the exploration-exploitation trade-off (inverse temperature beta).
    
    beta = beta_base * exp(beta_stai_coeff * stai)
    
    If beta_stai_coeff > 0: Anxiety increases beta (more rigid/deterministic choices).
    If beta_stai_coeff < 0: Anxiety decreases beta (more random/noisy choices).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta_base: [0, 10]      # Base inverse temperature
    beta_stai_coeff: [-5, 5]# Coefficient for STAI modulation
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.beta_stai_coeff = model_parameters

    def init_model(self) -> None:
        # Calculate effective beta
        # Using exponential to ensure beta stays positive
        self.effective_beta = self.beta_base * np.exp(self.beta_stai_coeff * self.stai)
        
        # Cap beta to prevent overflow issues in softmax
        self.effective_beta = np.clip(self.effective_beta, 0.0, 20.0)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.effective_beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.effective_beta)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```