Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this task.

### Model 1: Anxiety-Gated Learning Asymmetry
This model tests the hypothesis that high anxiety leads to a "defensive" learning style where negative outcomes are ignored or discounted to preserve perceived control.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Gated Learning Asymmetry.
    
    This model posits that anxiety modulates the learning rate specifically for 
    negative prediction errors (outcomes worse than expected). High anxiety 
    participants may exhibit a "freezing" of beliefs in the face of failure, 
    effectively ignoring 0-coin outcomes to maintain their current policy.
    
    Mechanism:
    If delta < 0 (negative PE):
        alpha_effective = alpha * (1 - stai * neg_learning_dampener)
    Else:
        alpha_effective = alpha
        
    This explains the participant's persistence on spaceship U despite receiving 
    multiple 0-coin outcomes; they simply do not update their value of U downwards.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]                 # Base learning rate
    beta: [0, 10]                 # Inverse temperature
    neg_learning_dampener: [0, 1] # How much anxiety reduces learning from failure
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.neg_learning_dampener = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update (Standard)
        delta_2 = reward - self.q_stage2[state, action_2]
        # Apply anxiety dampening if the outcome was worse than expected
        alpha_2 = self.alpha
        if delta_2 < 0:
            alpha_2 *= (1.0 - self.stai * self.neg_learning_dampener)
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Note: We use the updated Q2 value for the Stage 1 target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha
        if delta_1 < 0:
            alpha_1 *= (1.0 - self.stai * self.neg_learning_dampener)
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Tunnel Vision (Decay)
This model tests the hypothesis that anxiety consumes cognitive resources, leading to a "tunnel vision" effect where the values of unchosen options decay rapidly.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Induced Tunnel Vision (Value Decay).
    
    This model suggests that anxiety accelerates the forgetting (decay) of 
    unchosen options. While the participant updates the chosen path normally, 
    the value of the unchosen spaceship decays towards zero. High anxiety 
    increases this decay rate.
    
    Mechanism:
    Q(unchosen) = Q(unchosen) * (1 - decay_rate * STAI)
    
    This explains the "locking in" behavior. Once the participant starts choosing U, 
    the value of A decays. Even if U yields mediocre rewards, if A has decayed 
    to near-zero, U remains the superior choice, preventing exploration.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    decay_factor: [0, 1] # Base decay rate scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_factor = model_parameters

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Identify unchosen action in Stage 1
        unchosen_action = 1 - action_1
        
        # Calculate effective decay based on anxiety
        # High anxiety -> stronger decay (more forgetting of the road not taken)
        effective_decay = self.decay_factor * self.stai
        
        # Apply decay to the unchosen option
        self.q_stage1[unchosen_action] *= (1.0 - effective_decay)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Eligibility Trace
This model tests the hypothesis that anxiety affects the structural credit assignment (eligibility traces) between stages.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Eligibility Trace (TD-Lambda).
    
    This model investigates how anxiety affects the 'eligibility trace' (lambda),
    which determines how much the Stage 1 choice is directly reinforced by the 
    Stage 2 reward versus the Stage 2 value estimate.
    
    Hypothesis: Anxiety impairs the maintenance of eligibility traces (working memory),
    reducing lambda. This makes the participant rely more on the stored value of 
    the Stage 2 state (TD(0)) rather than the immediate reward outcome (TD(1)).
    
    Mechanism:
    lambda_eff = lambda_base * (1 - STAI)
    Q1_update += alpha * (delta1 + lambda_eff * delta2)
    
    A lower lambda (due to high anxiety) decouples Stage 1 from the noisy 
    trial-by-trial rewards, leading to more rigid/stable behavior driven by 
    slowly evolving Stage 2 values.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    lambda_base: [0, 1] # Base eligibility trace parameter
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_base = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective lambda modulated by anxiety
        # High anxiety reduces lambda -> closer to TD(0)
        lambda_eff = self.lambda_base * (1.0 - self.stai)
        
        # Calculate Stage 2 Prediction Error
        # Note: We calculate this BEFORE updating Q2 to get the true TD error for the trace
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Update Stage 2
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1
        # delta_1 represents the TD(0) error (bootstrapping from Q2)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # The update combines TD(0) error and the trace of the TD(1) error
        # If lambda is 0, this is pure TD(0). If lambda is 1, the reward fully propagates.
        self.q_stage1[action_1] += self.alpha * (delta_1 + lambda_eff * delta_2)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```