```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety accelerates memory decay for unchosen options.
    
    Cognitive Theory: Anxious individuals may suffer from reduced working memory capacity 
    due to cognitive interference (worry). This leads to faster forgetting of the 
    value of options that are not currently being attended to (unchosen options).
    
    Mechanism: At the end of each trial, the Q-value of the unchosen Stage 1 spaceship 
    decays towards 0. The rate of decay is proportional to the STAI score.
    
    Q_unchosen(t+1) = Q_unchosen(t) * (1 - (decay_rate * stai))
    
    This can lead to perseveration on a suboptimal option simply because the alternative 
    has been "forgotten" and its value has decayed to near zero.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    decay_rate: [0, 1]  # Scaling factor for anxiety-induced decay
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_rate = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for chosen path
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Anxiety-induced decay for the unchosen Stage 1 option
        unchosen_action_1 = 1 - action_1
        decay_factor = self.decay_rate * self.stai
        # Ensure decay factor doesn't exceed 1 (though bounds usually prevent this)
        decay_factor = np.clip(decay_factor, 0, 1)
        
        self.q_stage1[unchosen_action_1] *= (1.0 - decay_factor)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety induces a "Confirmation Bias" that ignores negative outcomes.
    
    Cognitive Theory: High anxiety can lead to rigid beliefs and an avoidance of 
    updating internal models when faced with failure (defensive processing). 
    The participant may discount "punishment" (0 reward) signals to maintain 
    their current course of action, leading to perseveration.
    
    Mechanism: The learning rate (alpha) is selectively reduced for negative outcomes (reward=0).
    The magnitude of this reduction is determined by the STAI score.
    
    If Reward == 0:
        alpha_effective = alpha * (1 - (ignore_factor * stai))
    Else:
        alpha_effective = alpha
        
    High anxiety -> Low learning from failure -> Perseveration.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          # Base learning rate
    beta: [0, 10]          # Inverse temperature
    ignore_factor: [0, 1]  # How much anxiety dampens learning from 0 rewards
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.ignore_factor = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective alpha based on outcome and anxiety
        if reward == 0:
            # Reduce alpha based on anxiety
            reduction = self.ignore_factor * self.stai
            reduction = np.clip(reduction, 0, 1)
            eff_alpha = self.alpha * (1 - reduction)
        else:
            eff_alpha = self.alpha

        # Update Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += eff_alpha * delta_2
        
        # Update Stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += eff_alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety drives the formation of cumulative habits (Action Priming).
    
    Cognitive Theory: Unlike simple "stickiness" (repeating the very last choice), 
    habits build up over time with repetition and decay slowly. Anxiety accelerates 
    the formation of these motor habits as a way to reduce cognitive load.
    
    Mechanism: We maintain a separate 'habit' value for each Stage 1 choice.
    Every time an action is chosen, its habit strength increases proportional to STAI.
    On every trial, all habit strengths decay slightly.
    
    Habit(a) = Habit(a) * decay + (IsChosen(a) * build_rate * stai)
    Q_net(a) = Q_learned(a) + Habit(a)
    
    This explains long streaks of behavior that are resistant to change even after 
    a few bad outcomes, as the 'habit' value buffers the drop in Q-value.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta: [0, 10]           # Inverse temperature
    habit_build_rate: [0, 2]# How fast habits accumulate with anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.habit_build_rate = model_parameters

    def init_model(self) -> None:
        # Initialize habit values for the two spaceships
        self.habit_values = np.zeros(self.n_choices)
        self.habit_decay = 0.9  # Fixed decay parameter for the habit trace itself

    def policy_stage1(self) -> np.ndarray:
        # Combine learned Q-values with cumulative habit values
        q_net = self.q_stage1 + self.habit_values
        return self.softmax(q_net, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Decay all habits
        self.habit_values *= self.habit_decay
        
        # Strengthen habit for the chosen action, modulated by anxiety
        # High anxiety -> stronger habit formation
        habit_boost = self.habit_build_rate * self.stai
        self.habit_values[action_1] += habit_boost

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```