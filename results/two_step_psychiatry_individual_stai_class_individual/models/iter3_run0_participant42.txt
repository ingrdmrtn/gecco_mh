Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based Weighting
This model tests the hypothesis that anxiety consumes working memory resources, reducing the ability to use "model-based" (planning) strategies. High anxiety participants will rely more on "model-free" (habitual) learning. The STAI score inversely scales the mixing weight `w` between model-based and model-free values.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety Impairs Model-Based Planning]
    This model implements a hybrid reinforcement learning agent that mixes 
    Model-Based (MB) and Model-Free (MF) values. 
    
    Hypothesis: Anxiety (STAI) acts as a cognitive load that reduces the 
    weight (w) placed on Model-Based planning. Higher STAI leads to lower w,
    meaning more reliance on simple Model-Free habits.
    
    w = w_max * (1 - stai_impact * STAI)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       - Learning rate
    beta: [0, 10]       - Inverse temperature
    w_max: [0, 1]       - Maximum model-based weight (at 0 anxiety)
    stai_impact: [0, 1] - How strongly STAI reduces w
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.stai_impact = model_parameters
        
        # Calculate mixing weight w based on STAI
        # w represents the contribution of Model-Based values
        # We clamp it between 0 and 1
        raw_w = self.w_max * (1.0 - (self.stai_impact * self.stai))
        self.w = np.clip(raw_w, 0.0, 1.0)

    def init_model(self) -> None:
        # Initialize Model-Free Q-values for stage 1 separately
        self.q_mf_stage1 = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # Expected value of best action in the resulting states
            # T[a, s] is prob of transitioning to state s given action a
            # We assume the agent knows the transition matrix T (0.7/0.3)
            expected_val = 0
            for s in range(self.n_states):
                expected_val += self.T[a, s] * np.max(self.q_stage2[s])
            q_mb[a] = expected_val
            
        # Mix MF and MB values
        q_net = (self.w * q_mb) + ((1 - self.w) * self.q_mf_stage1)
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 values (Standard Q-learning)
        # Q2(s, a2) += alpha * (r - Q2(s, a2))
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Update Stage 1 Model-Free values (TD-learning)
        # Q_MF(a1) += alpha * (Q2(s, a2) - Q_MF(a1))
        # Note: We use the value of the chosen stage 2 action as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry
This model hypothesizes that anxiety creates a bias towards negative information. Anxious individuals might learn faster from "disappointments" (negative prediction errors) than from positive surprises. The STAI score modulates the ratio between a positive learning rate and a negative learning rate.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety Amplifies Negative Learning]
    This model hypothesizes that anxiety biases learning rates. 
    Specifically, higher STAI scores increase the learning rate for 
    negative prediction errors (alpha_neg) relative to positive ones (alpha_pos).
    
    alpha_pos = base_alpha
    alpha_neg = base_alpha + (stai * alpha_boost)
    
    Parameter Bounds:
    -----------------
    base_alpha: [0, 1]    - Base learning rate for positive errors
    alpha_boost: [0, 1]   - Additional learning rate for negative errors per unit STAI
    beta: [0, 10]         - Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.base_alpha, self.alpha_boost, self.beta = model_parameters
        
        self.alpha_pos = self.base_alpha
        # Ensure alpha_neg doesn't exceed 1
        self.alpha_neg = np.clip(self.base_alpha + (self.stai * self.alpha_boost), 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        pe2 = reward - self.q_stage2[state, action_2]
        lr2 = self.alpha_pos if pe2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += lr2 * pe2
        
        # Stage 1 Update
        # Using the updated stage 2 value as the target
        pe1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        lr1 = self.alpha_pos if pe1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += lr1 * pe1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression
This model hypothesizes that anxiety reduces exploration. Anxious individuals may be more risk-averse or intolerant of uncertainty, leading them to exploit known high-value options more deterministically. Here, STAI increases the inverse temperature parameter `beta` (making choices more deterministic/greedy) specifically after a loss, reflecting a "hunker down" response to threat.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Post-Error Anxiety Rigidity]
    This model hypothesizes that anxiety causes a temporary increase in 
    decision rigidity (higher beta) immediately after receiving a zero reward (loss).
    
    If the previous trial was a loss (reward=0), the beta is boosted by STAI.
    If the previous trial was a win, beta is baseline.
    
    beta_effective = beta_base + (STAI * rigidity_factor) [if last_reward == 0]
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]           - Learning rate
    beta_base: [0, 10]      - Baseline inverse temperature
    rigidity_factor: [0, 10]- How much STAI increases beta after a loss
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.rigidity_factor = model_parameters

    def get_current_beta(self) -> float:
        # If it's the first trial, or we won last time, use base beta
        if self.last_reward is None or self.last_reward > 0:
            return self.beta_base
        else:
            # We lost last time. Anxiety makes us rigid.
            return self.beta_base + (self.stai * self.rigidity_factor)

    def policy_stage1(self) -> np.ndarray:
        beta = self.get_current_beta()
        return self.softmax(self.q_stage1, beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        beta = self.get_current_beta()
        return self.softmax(self.q_stage2[state], beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```