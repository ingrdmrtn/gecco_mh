```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the eligibility trace (lambda) in reinforcement learning.
    Anxious individuals may have altered credit assignment mechanisms. 
    Higher anxiety might lead to a stronger link between distal rewards and initial choices (higher lambda) due to hyper-vigilance,
    or conversely, cognitive load might reduce this link (lower lambda).
    This model implements TD(lambda) where lambda is a function of STAI.

    lambda = clip(lam_base + lam_slope * stai, 0, 1)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    lam_base: [0, 1]
    lam_slope: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lam_base, self.lam_slope = model_parameters

    def init_model(self) -> None:
        # Calculate lambda based on anxiety
        self.lam = self.lam_base + (self.lam_slope * self.stai)
        # Clip to valid range [0, 1]
        if self.lam < 0: self.lam = 0.0
        if self.lam > 1: self.lam = 1.0

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD(lambda) update logic
        
        # 1. Prediction error at stage 2 (outcome)
        # Note: We use the Q-value *before* update for the TD error calculation
        q2_val = self.q_stage2[state, action_2]
        delta_2 = reward - q2_val
        
        # 2. Prediction error at stage 1 (transition)
        # We use the value of the chosen action in stage 2 as the target for stage 1 (SARSA-like)
        q1_val = self.q_stage1[action_1]
        delta_1 = q2_val - q1_val

        # Update Stage 2 Q-values
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 Q-values
        # The update includes the direct error (delta_1) and the eligibility trace of the stage 2 error (lambda * delta_2)
        self.q_stage1[action_1] += self.alpha * (delta_1 + self.lam * delta_2)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces reactive changes in exploration/exploitation after negative outcomes.
    Anxious individuals may become more rigid (higher beta) or more chaotic (lower beta) 
    immediately after experiencing a loss or lack of reward (reward <= 0).
    This model modulates the inverse temperature (beta) for the *next* trial based on the *previous* outcome.

    beta_trial = beta_base * (1 + reactivity * stai)   if last_reward <= 0
    beta_trial = beta_base                             if last_reward > 0

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta_base: [0, 10]
    reactivity: [-1, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.reactivity = model_parameters

    def get_current_beta(self) -> float:
        # If previous trial resulted in loss/no-reward, modulate beta
        if self.last_reward is not None and self.last_reward <= 0:
            # Reactivity scales with STAI
            factor = 1.0 + (self.reactivity * self.stai)
            # Ensure beta stays non-negative
            return max(0.0, self.beta_base * factor)
        return self.beta_base

    def policy_stage1(self) -> np.ndarray:
        beta = self.get_current_beta()
        return self.softmax(self.q_stage1, beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        beta = self.get_current_beta()
        return self.softmax(self.q_stage2[state], beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the learning rate of the environmental transition model.
    Anxious individuals might be hyper-vigilant to changes in environmental statistics (transition probabilities).
    This model is a Model-Based learner that updates its internal transition matrix T_est.
    The learning rate for T (eta) is modulated by STAI.
    
    eta = clip(eta_base + eta_slope * stai, 0, 1)
    
    Stage 1 values are computed purely from the model (Model-Based) using the learned T and current Q2 values.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       (Learning rate for Stage 2 values)
    beta: [0, 10]
    eta_base: [0, 1]    (Base learning rate for transitions)
    eta_slope: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.eta_base, self.eta_slope = model_parameters

    def init_model(self) -> None:
        # Initialize estimated transition matrix (A->X, U->Y mostly)
        # Rows: Actions (0:A, 1:U), Cols: States (0:X, 1:Y)
        # Start with the instructed priors
        self.T_est = np.array([[0.7, 0.3], [0.3, 0.7]])
        
        # Calculate transition learning rate
        self.eta = self.eta_base + (self.eta_slope * self.stai)
        if self.eta < 0: self.eta = 0.0
        if self.eta > 1: self.eta = 1.0

    def pre_trial(self) -> None:
        # Compute Model-Based values for Stage 1
        # Q_MB(a) = Sum_s' T(a, s') * Max_a' Q2(s', a')
        for a in range(self.n_choices):
            val = 0.0
            for s_prime in range(self.n_states):
                # Value of state s_prime is max Q-value in that state
                v_s_prime = np.max(self.q_stage2[s_prime])
                val += self.T_est[a, s_prime] * v_s_prime
            self.q_stage1[a] = val

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 values (Model-Free / TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Update Transition Matrix (Model Learning)
        # Observed transition: action_1 -> state
        # Create target vector: 1 at observed state, 0 at other
        target = np.zeros(self.n_states)
        target[state] = 1.0
        
        # T_new = T_old + eta * (Target - T_old)
        self.T_est[action_1] += self.eta * (target - self.T_est[action_1])
        
        # Normalize to ensure probabilities sum to 1
        self.T_est[action_1] /= np.sum(self.T_est[action_1])

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```