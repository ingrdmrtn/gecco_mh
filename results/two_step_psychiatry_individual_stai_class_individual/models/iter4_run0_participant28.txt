```python
import numpy as np
from abc import ABC, abstractmethod

# Base Class (Included for context, not modified in output)
class CognitiveModelBase(ABC):
    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = np.zeros((self.n_states, self.n_choices))
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        pass

    def init_model(self) -> None:
        pass

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

# --- Proposed Models ---

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Amplified Reward Sensitivity.
    
    This model posits that high anxiety leads to a hypersensitivity to positive 
    outcomes (rewards). Anxious participants subjectively perceive rewards as 
    larger than they are, leading to inflated Q-values.
    
    When Q-values are inflated, the difference between options (Q_best - Q_worst) 
    increases, which effectively acts like a higher inverse temperature (beta), 
    causing more deterministic/rigid behavior after a win. This explains the 
    participant's strong adherence to a choice once it has paid off.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    amp: [0, 10]        # Anxiety amplification factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.amp = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Amplify the reward based on STAI
        # If STAI is high, the effective reward is significantly > 1.0
        reward_effective = reward * (1.0 + self.stai * self.amp)
        
        # Standard TD update with effective reward
        delta_2 = reward_effective - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Blunted Negative Learning.
    
    This model suggests that anxiety causes an asymmetry in learning from 
    prediction errors. Specifically, high anxiety dampens the learning rate 
    for negative prediction errors (losses/omissions).
    
    This "head-in-the-sand" mechanism means the participant updates their 
    values normally when they win, but very slowly when they lose. This 
    explains why the participant persists with Spaceship U despite receiving 
    multiple 0-coin outcomes; the negative feedback fails to degrade the 
    value of the chosen option.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate (for positive updates)
    beta: [0, 10]       # Inverse temperature
    asym: [0, 1]        # Asymmetry strength (0=no effect, 1=max blunting by STAI)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.asym = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha based on sign of prediction error
        if delta_2 < 0:
            alpha_eff_2 = self.alpha * (1.0 - self.stai * self.asym)
        else:
            alpha_eff_2 = self.alpha
            
        self.q_stage2[state, action_2] += alpha_eff_2 * delta_2
        
        # Stage 1 Update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 < 0:
            alpha_eff_1 = self.alpha * (1.0 - self.stai * self.asym)
        else:
            alpha_eff_1 = self.alpha
            
        self.q_stage1[action_1] += alpha_eff_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Tunnel Vision (Unchosen Decay).
    
    This model proposes that anxiety induces a form of "tunnel vision" where 
    the participant rapidly forgets the value of unchosen options. 
    
    While the chosen option is updated via RL, the unchosen option's value 
    decays towards zero. High anxiety accelerates this decay. This creates 
    a self-reinforcing loop: if the participant chooses U and gets even 
    intermittent rewards, the value of A decays to zero, making U appear 
    permanently superior regardless of its absolute value.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    decay: [0, 1]       # Base decay rate scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay = model_parameters

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Identify the unchosen action in Stage 1
        unchosen_action = 1 - action_1
        
        # Decay the value of the unchosen action
        # Decay factor is modulated by STAI: High STAI -> Stronger decay (factor < 1)
        decay_factor = 1.0 - (self.decay * self.stai)
        
        # Ensure decay factor doesn't go negative (though bounds usually prevent this)
        decay_factor = max(0.0, decay_factor)
        
        self.q_stage1[unchosen_action] *= decay_factor

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```