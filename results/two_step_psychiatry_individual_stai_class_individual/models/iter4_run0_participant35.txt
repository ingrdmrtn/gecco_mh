Here are 3 new cognitive models exploring different mechanisms for how high anxiety (STAI = 0.725) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model tests the classic hypothesis that anxiety disrupts complex "Model-Based" planning (which uses the transition structure) and favors simpler "Model-Free" learning (which just repeats rewarded actions). High anxiety reduces the weight ($w$) given to the model-based system.

```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety impairs Model-Based (MB) control, shifting the participant 
    towards Model-Free (MF) strategies.
    
    Cognitive Theory: High anxiety consumes working memory resources required for 
    MB planning (calculating expected value based on transition probabilities). 
    Therefore, higher STAI scores lead to a lower mixing weight 'w', relying more 
    on simple TD learning (MF).
    
    Mechanism:
    Q_MB = Transition_Matrix * max(Q_stage2)
    Q_MF = Standard TD learning values
    Q_net = w * Q_MB + (1-w) * Q_MF
    
    The mixing weight 'w' is calculated as: w = w_base * (1 - stai)
    High anxiety -> w approaches 0 (pure Model-Free).
    Low anxiety -> w approaches w_base.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    w_base: [0, 1]  # Baseline model-based weight for a non-anxious person
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # We need separate storage for MF values because the base class 
        # q_stage1 will be overwritten by the hybrid value if we aren't careful.
        # Actually, the base class value_update modifies q_stage1. 
        # We will treat self.q_stage1 as the MF value.
        pass

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values
        # Q_MB(a1) = sum(P(s|a1) * max_a2 Q_stage2(s, a2))
        # self.T is [2x2]: T[0] is probs for action 0 -> [state0, state1]
        # But the base class T is defined as [[0.7, 0.3], [0.3, 0.7]] 
        # implying row 0 is action A (common->X(0)), row 1 is action U (common->Y(1))
        
        # Max value of stage 2 for each state
        max_q2 = np.max(self.q_stage2, axis=1) # [max_val_state0, max_val_state1]
        
        # Q_MB = T dot max_q2
        q_mb = np.dot(self.T, max_q2)
        
        # 2. Calculate Mixing Weight modulated by Anxiety
        # If STAI is 1.0, w becomes 0 (pure MF). If STAI is 0, w is w_base.
        w = self.w_base * (1.0 - self.stai)
        
        # 3. Combine
        # self.q_stage1 represents the Model-Free values (updated via TD in value_update)
        q_net = w * q_mb + (1 - w) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

    # We use the default value_update which implements TD (Model-Free) learning 
    # on self.q_stage1 and self.q_stage2.

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion (Asymmetric Learning)
This model hypothesizes that anxiety makes participants learn differently from positive versus negative outcomes. Specifically, high anxiety might make "bad news" (zero reward) much more salient than "good news," causing rapid abandonment of strategies after failure.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases sensitivity to negative outcomes (Loss Aversion in learning).
    
    Cognitive Theory: Anxious individuals have a negativity bias. They update their 
    value estimates more drastically after a lack of reward (0 coins) compared to 
    receiving a reward.
    
    Mechanism: We use two learning rates: alpha_pos and alpha_neg.
    alpha_neg is boosted by the STAI score.
    
    alpha_effective = alpha_base if reward > 0
    alpha_effective = alpha_base + (stai * punishment_sensitivity) if reward == 0
    
    This explains why the participant might switch rapidly or stick rigidly 
    depending on the specific sequence of losses.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]            # Base learning rate for rewards
    beta: [0, 10]                 # Inverse temperature
    punishment_sensitivity: [0, 1]# How much anxiety boosts learning from failure
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.punishment_sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine learning rate based on outcome
        if reward > 0:
            alpha = self.alpha_base
        else:
            # Boost learning rate for negative outcomes based on anxiety
            # We clip at 1.0 to ensure stability
            alpha = min(1.0, self.alpha_base + (self.stai * self.punishment_sensitivity))

        # Standard TD update with the dynamic alpha
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression (Inverse Temperature Modulation)
This model suggests that anxiety affects the decision rule itself (the softmax function) rather than the learning process. High anxiety might lead to "freezing" or rigid exploitation of the currently best option to reduce uncertainty, effectively increasing the inverse temperature ($\beta$).

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety suppresses exploration, leading to deterministic behavior.
    
    Cognitive Theory: Anxious individuals are intolerant of uncertainty. To minimize 
    uncertainty, they exploit the current best option more rigidly rather than 
    exploring alternatives. This manifests as a higher 'beta' (inverse temperature) 
    in the softmax function.
    
    Mechanism:
    beta_effective = beta_min + (stai * beta_gain)
    
    A high STAI score pushes the softmax function towards a "winner-take-all" (argmax) 
    behavior, making the participant stick to one spaceship until values change drastically.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta_min: [0, 5]   # Baseline exploitation for low anxiety
    beta_gain: [0, 10] # How much anxiety increases exploitation/rigidity
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_min, self.beta_gain = model_parameters

    def init_model(self) -> None:
        # Calculate the fixed effective beta for this participant based on their anxiety
        self.beta_effective = self.beta_min + (self.stai * self.beta_gain)

    def policy_stage1(self) -> np.ndarray:
        # Use the anxiety-modulated beta
        return self.softmax(self.q_stage1, self.beta_effective)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Use the anxiety-modulated beta
        return self.softmax(self.q_stage2[state], self.beta_effective)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```