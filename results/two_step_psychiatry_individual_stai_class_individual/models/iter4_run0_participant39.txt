Here are 3 new cognitive models that hypothesize different mechanisms for how high anxiety (STAI = 0.66) influences decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model tests the hypothesis that high anxiety impairs complex "Model-Based" planning (using the transition matrix) and favors simpler "Model-Free" learning (repeating what was rewarded). High anxiety consumes working memory resources, making the computationally expensive Model-Based strategy harder to maintain.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety impairs Model-Based (MB) control, favoring Model-Free (MF).
    High anxiety (STAI) acts as a resource constraint, reducing the weight (w)
    placed on the MB system. The final Q-value is a weighted mix:
    Q_net = w * Q_MB + (1-w) * Q_MF.
    Here, w is modeled as: w_base * (1 - stai). Higher anxiety -> lower w -> more MF.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_base: [0, 1]     # Baseline model-based weight (for STAI=0)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Separate Q-tables for MF and MB
        self.q_mf = np.zeros(self.n_choices)
        self.q_mb = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Calculate Model-Based values using the transition matrix T and Stage 2 max values
        # Q_MB(a) = sum(P(s|a) * max(Q_stage2(s, :)))
        for a in range(self.n_choices):
            expected_val = 0
            for s in range(self.n_states):
                expected_val += self.T[a, s] * np.max(self.q_stage2[s])
            self.q_mb[a] = expected_val

        # Calculate mixing weight w based on STAI
        # If STAI is high (near 1), w becomes small -> Pure MF
        w = self.w_base * (1.0 - self.stai)
        
        # Combine MF and MB values
        q_net = w * self.q_mb + (1 - w) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Stage 2 update (TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Model-Free Stage 1 update (TD(1) / Direct reinforcement from reward)
        # Note: In full hybrid models, this often uses eligibility traces. 
        # Here we use a simple direct update from the final reward to Stage 1 MF value.
        delta_1_mf = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1_mf

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced "Safe Harbor" Bias
This model hypothesizes that anxious individuals don't just persevere (stickiness), but specifically overvalue the "common" transition because it feels safer or more predictable. They might treat the rare transition as an error or a "dangerous" deviation, effectively penalizing the value of actions that lead to unexpected states, or boosting actions that lead to expected states, proportional to their anxiety.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates a "Safe Harbor" bias where common transitions are 
    intrinsically more valuable (or rare ones are punishing) due to a desire for predictability.
    
    When updating Stage 1 values, the reward signal is augmented by a "predictability bonus"
    if the transition was common, scaled by STAI.
    Effective Reward = External Reward + (STAI * bonus_k * is_common_transition)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    bonus_k: [0, 2]    # Magnitude of the predictability bonus
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.bonus_k = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 normally
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Determine if transition was common or rare
        # T[action_1, state] gives probability. If > 0.5, it's common.
        is_common = self.T[action_1, state] > 0.5
        
        # 3. Calculate internal reward augmentation
        # High anxiety (STAI) amplifies the subjective value of the "safe" (common) outcome.
        internal_bonus = 0.0
        if is_common:
            internal_bonus = self.bonus_k * self.stai
            
        # 4. Update Stage 1 using the augmented reward
        # We use the Stage 2 value as the proxy for the reward of Stage 1, plus the bonus
        current_val_s2 = self.q_stage2[state, action_2]
        total_signal = current_val_s2 + internal_bonus
        
        delta_1 = total_signal - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Learning Rate Asymmetry (Positive vs Negative)
This model hypothesizes that high anxiety creates a hypersensitivity to negative outcomes (punishment) or a blunted response to positive outcomes (anhedonia/caution). Instead of a single learning rate `alpha`, the model uses two: `alpha_pos` and `alpha_neg`. The balance between these is modulated by STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning asymmetry. Anxious individuals may learn 
    faster from negative prediction errors (threat vigilance) than positive ones.
    
    We define a base learning rate `alpha`.
    If prediction error (delta) > 0: effective_alpha = alpha * (1 - STAI * asym_k)
    If prediction error (delta) < 0: effective_alpha = alpha * (1 + STAI * asym_k)
    
    This implies high anxiety dampens positive learning and boosts negative learning.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Base learning rate
    beta: [0, 10]      # Inverse temperature
    asym_k: [0, 1]     # Asymmetry strength factor (0 = symmetric, 1 = max asymmetry)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.asym_k = model_parameters

    def get_effective_alpha(self, delta: float) -> float:
        # Calculate modulation factor based on STAI and asymmetry parameter
        mod = self.stai * self.asym_k
        
        if delta >= 0:
            # Dampen positive learning: alpha * (1 - mod)
            # We clip to ensure it doesn't go below 0
            return max(0.0, self.alpha * (1.0 - mod))
        else:
            # Boost negative learning: alpha * (1 + mod)
            # We clip to ensure it doesn't exceed 1
            return min(1.0, self.alpha * (1.0 + mod))

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.get_effective_alpha(delta_2)
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Using the updated Stage 2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.get_effective_alpha(delta_1)
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```