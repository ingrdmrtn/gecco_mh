Here are 3 new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this two-step task.

### Model 1: Anxiety-Driven Model-Based Suppression
This model tests the hypothesis that high anxiety consumes working memory resources, suppressing "model-based" (planning) strategies in favor of "model-free" (habitual) strategies. The STAI score determines the mixing weight ($w$) between these two systems, with higher anxiety leading to a lower $w$ (less model-based control).

```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Model-Based Suppression]
    This model hypothesizes that anxiety consumes cognitive resources, reducing the 
    ability to use model-based planning. The participant uses a hybrid reinforcement 
    learning agent where the balance between Model-Based (MB) and Model-Free (MF) 
    control is determined by a weighting parameter 'w'. 
    
    Here, 'w' is not fixed but is a function of STAI: 
    w = w_max * (1 - (stai_sensitivity * stai))
    
    Higher anxiety reduces 'w', making the agent more model-free (habitual).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta: [0, 10]          - Inverse temperature
    w_max: [0, 1]          - Maximum model-based weight (at 0 anxiety)
    stai_sens: [0, 1]      - Sensitivity of w to STAI score
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.stai_sens = model_parameters
        
        # Calculate mixing weight w based on STAI
        # w represents the contribution of the Model-Based system
        # We clip it to be between 0 and 1
        raw_w = self.w_max * (1.0 - (self.stai_sens * self.stai))
        self.w = np.clip(raw_w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Model-Free Q-values (from TD learning)
        q_mf = self.q_stage1
        
        # Model-Based Q-values (Bellman equation using transition matrix T)
        # Q_MB(a) = sum(T(s|a) * max(Q_stage2(s, :)))
        # We use the max of stage 2 values as the estimated value of the state
        v_stage2 = np.max(self.q_stage2, axis=1)
        q_mb = np.dot(self.T, v_stage2) # T is (2,2), v_stage2 is (2,) -> (2,)
        
        # Integrated Q-values
        q_net = (self.w * q_mb) + ((1 - self.w) * q_mf)
        
        return self.softmax(q_net, self.beta)

    # We use the standard value_update from Base for the MF part and Stage 2
    # The MB part is computed on the fly in policy_stage1 using the fixed T matrix

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Negative Learning Bias
This model hypothesizes that anxious individuals are hypersensitive to negative outcomes (punishment or lack of reward). Instead of a single learning rate, the model uses separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The STAI score amplifies the learning rate for negative errors.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Negative Learning Bias]
    This model hypothesizes that anxiety leads to a hypersensitivity to negative 
    prediction errors (disappointments). The model splits the learning rate into 
    alpha_pos (for positive RPEs) and alpha_neg (for negative RPEs).
    
    The base negative learning rate is boosted by the STAI score:
    alpha_neg_effective = alpha_neg_base + (stai_boost * stai)
    
    This means anxious participants update their values more drastically when 
    outcomes are worse than expected.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      - Learning rate for positive prediction errors
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors
    stai_boost: [0, 1]     - How much STAI increases the negative learning rate
    beta: [0, 10]          - Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.stai_boost, self.beta = model_parameters
        
        # Calculate effective negative learning rate
        self.alpha_neg = np.clip(self.alpha_neg_base + (self.stai_boost * self.stai), 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 > 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (TD)
        # Note: In standard TD, the target is the value of the next state (Q_stage2)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 > 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Exploration (Inverse Temperature)
This model hypothesizes that anxiety reduces exploration, leading to more deterministic (exploitative) behavior. The "temperature" of the softmax function regulates the randomness of choices. Here, the inverse temperature $\beta$ (which controls determinism) is scaled by the STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Exploration]
    This model hypothesizes that anxiety reduces the willingness to explore, 
    leading to more rigid, deterministic choices (higher exploitation).
    
    The inverse temperature parameter 'beta' determines the stochasticity of choice.
    Here, beta is defined as a baseline plus an anxiety-dependent component:
    beta = beta_base * (1 + (stai_rigidity * stai))
    
    High anxiety results in a higher beta, making the softmax function sharper 
    and choices less random.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta_base: [0, 10]     - Baseline inverse temperature
    stai_rigidity: [0, 5]  - Multiplier for how much STAI increases beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stai_rigidity = model_parameters
        
        # Calculate effective beta
        # If stai_rigidity is high, beta becomes very large for anxious people
        self.beta = self.beta_base * (1.0 + (self.stai_rigidity * self.stai))

    # We use standard policy and value updates, but the beta used in 
    # policy_stage1 and policy_stage2 (via self.beta) is now anxiety-modulated.
    
    # No overrides needed for policy or update methods as they use self.beta and self.alpha

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```