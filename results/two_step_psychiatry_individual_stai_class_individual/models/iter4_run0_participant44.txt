```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Relief Seeking (Asymmetric Learning)]
    Anxious individuals may view rewards as "relief" from threat, reinforcing safety behaviors strongly,
    while ignoring lack of reward (0) to avoid confronting the failure of their safety strategy.
    
    Mechanism:
    The learning rate is modulated by the outcome and the participant's anxiety (STAI).
    - Positive outcomes (Reward=1) are learned with a boosted rate: alpha * (1 + relief_bias * STAI)
    - Negative outcomes (Reward=0) are learned with a suppressed rate: alpha * (1 - relief_bias * STAI)
    
    This asymmetry causes the participant to "lock in" to a choice that provides intermittent rewards,
    ignoring the times it fails.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       - Base learning rate
    beta: [0, 10]       - Inverse temperature
    relief_bias: [0, 1] - Degree of asymmetry modulated by anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.relief_bias = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate anxiety-modulated modifier
        mod = self.relief_bias * self.stai
        
        # Determine effective learning rate based on outcome
        if reward > 0.5: 
            # Relief: Boost learning from success
            eff_alpha = min(1.0, self.alpha * (1.0 + mod))
        else: 
            # Denial: Suppress learning from failure
            eff_alpha = max(0.0, self.alpha * (1.0 - mod))
            
        # Standard TD update with effective alpha
        # Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += eff_alpha * delta_2
        
        # Stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += eff_alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Habitual Perseveration]
    High anxiety consumes cognitive resources, leading to a reliance on simple motor repetition (perseveration)
    rather than value-based deliberation. The higher the anxiety, the harder it is to switch away from
    the previous action.
    
    Mechanism:
    A "stickiness" bonus is added to the Q-value of the previously chosen action during selection.
    Crucially, the magnitude of this bonus is directly scaled by the STAI score.
    
    Q_net(a) = Q(a) + (stick_gain * STAI * IsLastAction(a))

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      - Learning rate
    beta: [0, 10]      - Inverse temperature
    stick_gain: [0, 5] - Strength of anxiety-driven perseveration
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_gain = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy values to avoid modifying the actual Q-table permanently for this step
        q_mod = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            # Add bonus to the last chosen action, scaled by anxiety
            # High STAI -> High bonus -> Strong tendency to repeat
            bonus = self.stick_gain * self.stai
            q_mod[int(self.last_action1)] += bonus
            
        return self.softmax(q_mod, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Ambiguity Aversion]
    Anxious individuals are intolerant of uncertainty. They penalize options that they have not explored frequently,
    preferring the "devil they know" (high visit count) over the unknown (low visit count).
    
    Mechanism:
    An ambiguity penalty is subtracted from the value of options based on how rarely they have been visited.
    This penalty decays as the option is explored, but the initial aversion is scaled by STAI.
    
    Penalty = ambiguity_scale * STAI * exp(-visit_count)
    Q_net = Q - Penalty

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta: [0, 10]          - Inverse temperature
    ambiguity_scale: [0, 5]- Magnitude of penalty for unknown options
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.ambiguity_scale = model_parameters

    def init_model(self) -> None:
        # Track how many times each stage 1 option has been chosen
        self.counts = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Calculate penalty for each option
        # exp(-count) starts at 1.0 (count=0) and decays towards 0 as count increases
        uncertainty = np.exp(-self.counts)
        
        # The penalty is driven by anxiety (STAI)
        penalty = self.ambiguity_scale * self.stai * uncertainty
        
        # Effective Q values (Value - AnxietyPenalty)
        q_net = self.q_stage1 - penalty
        
        return self.softmax(q_net, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        # Increment visit count for the chosen option
        self.counts[action_1] += 1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```