```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition matrix
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = np.zeros((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    """Create function interface from model class."""
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates Stage-Specific Exploration (Beta Asymmetry).
    
    Anxiety may differentially affect decision-making at the planning stage (Stage 1) 
    versus the immediate reaction stage (Stage 2). High anxiety might induce more 
    randomness (lower beta) or rigidity (higher beta) specifically in the initial 
    choice between spaceships, while leaving the reaction to aliens relatively intact.
    
    This model allows the Stage 1 inverse temperature (beta1) to vary with STAI,
    while Stage 2 uses a separate, fixed beta2.

    beta1 = beta1_base + (beta1_stai_slope * stai)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta1_base: [0, 10]
    beta1_stai_slope: [-10, 10]
    beta2: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta1_base, self.beta1_stai_slope, self.beta2 = model_parameters

    def init_model(self) -> None:
        # Calculate the anxiety-modulated beta for stage 1
        self.beta1 = self.beta1_base + (self.beta1_stai_slope * self.stai)
        # Ensure beta1 stays non-negative
        self.beta1 = max(0.0, self.beta1)

    def policy_stage1(self) -> np.ndarray:
        # Use the anxiety-modulated beta1
        return self.softmax(self.q_stage1, self.beta1)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Use the fixed beta2
        return self.softmax(self.q_stage2[state], self.beta2)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates Risk Aversion (Mean-Variance Trade-off).
    
    Anxious individuals are hypothesized to be more risk-averse. They may track not just 
    the expected value (mean Q) but also the variability (variance) of rewards associated 
    with each choice. High anxiety leads to a stronger penalty for options with high variance.
    
    Utility = Q - (risk_param * sqrt(Variance))
    risk_param = risk_base + (risk_stai_slope * stai)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    risk_base: [-5, 5]
    risk_stai_slope: [-5, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.risk_base, self.risk_stai_slope = model_parameters

    def init_model(self) -> None:
        # Initialize variance trackers for Stage 1 choices
        self.var_stage1 = np.zeros(self.n_choices)
        
        # Calculate risk parameter based on anxiety
        self.risk_param = self.risk_base + (self.risk_stai_slope * self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Calculate utility: Mean - Risk_Penalty
        # Add epsilon to sqrt to avoid numerical issues
        utility = self.q_stage1 - (self.risk_param * np.sqrt(self.var_stage1 + 1e-6))
        return self.softmax(utility, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for means
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Update Variance estimate for Stage 1 choice based on the final reward received
        # Var_new = Var_old + alpha * ((Reward - Q_old)^2 - Var_old)
        # Note: We use the squared prediction error relative to the Stage 1 Q-value
        sq_error = (reward - self.q_stage1[action_1])**2
        self.var_stage1[action_1] += self.alpha * (sq_error - self.var_stage1[action_1])

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates reliance on a Win-Stay Lose-Shift (WSLS) heuristic.
    
    This model posits that participants use a mixture of Reinforcement Learning (RL) 
    and a simple heuristic strategy (WSLS). Anxiety is hypothesized to influence the 
    weight placed on the heuristic strategy (e.g., to reduce cognitive load).
    
    P(choice) = (1 - w) * P_RL + w * P_WSLS
    w = w_base + (w_slope * stai)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    w_base: [0, 1]
    w_slope: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_slope = model_parameters

    def init_model(self) -> None:
        # Calculate mixture weight
        raw_w = self.w_base + (self.w_slope * self.stai)
        # Clamp w to [0, 1]
        self.w_heuristic = np.clip(raw_w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # 1. RL Policy
        p_rl = self.softmax(self.q_stage1, self.beta)
        
        # 2. WSLS Policy
        p_wsls = np.zeros(self.n_choices)
        
        if self.last_action1 is None:
            # First trial: uniform random
            p_wsls[:] = 0.5
        else:
            if self.last_reward > 0:
                # Win-Stay: Repeat last Stage 1 choice
                p_wsls[self.last_action1] = 1.0
            else:
                # Lose-Shift: Switch to the other Stage 1 choice
                p_wsls[1 - self.last_action1] = 1.0
        
        # 3. Mixture
        return (1 - self.w_heuristic) * p_rl + self.w_heuristic * p_wsls

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```