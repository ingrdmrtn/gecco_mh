```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition matrix
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = np.zeros((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    """Create function interface from model class."""
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Amplified "Win-Stay" Bias.
    
    This model posits that anxiety specifically amplifies the "Win-Stay" heuristic.
    While standard RL increases the value of a rewarded action, this model adds an 
    additional, temporary "bonus" to the previously chosen action if and only if 
    it resulted in a reward. This creates a stronger tendency to cling to successful 
    strategies than standard Q-learning would predict, explaining the rigidity 
    observed in the participant's data (sticking to U).
    
    Mechanism:
    If last_reward == 1:
        Q_net(last_action) = Q(last_action) + (stai * win_bonus)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    win_bonus: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.win_bonus = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_net = self.q_stage1.copy()
        
        # Apply Win-Stay bonus
        if self.last_reward == 1.0 and self.last_action1 is not None:
            bonus = self.stai * self.win_bonus
            q_net[self.last_action1] += bonus
            
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Uncertainty Avoidance (Time-Based).
    
    This model hypothesizes that anxious individuals are intolerant of uncertainty 
    and ambiguity. As time passes without choosing an option, the uncertainty 
    associated with that option grows. This model penalizes options based on the 
    time elapsed since they were last chosen. This creates a "comfort zone" effect 
    where the participant becomes increasingly reluctant to switch away from their 
    current choice pattern, as the alternative becomes "scarier" over time.
    
    Mechanism:
    Penalty = stai * avoidance_weight * log(trials_since_last_choice + 1)
    Q_net = Q - Penalty
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    avoidance_weight: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.avoidance_weight = model_parameters

    def init_model(self) -> None:
        # Track how many trials have passed since each action was last chosen
        self.trials_since_choice = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Calculate penalty based on time since last choice
        # Using log to prevent penalty from exploding too infinitely, but still growing
        penalty = self.stai * self.avoidance_weight * np.log(self.trials_since_choice + 1)
        
        q_net = self.q_stage1 - penalty
        return self.softmax(q_net, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Increment counter for all actions
        self.trials_since_choice += 1
        # Reset counter for the chosen action
        self.trials_since_choice[action_1] = 0

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Dependent Choice Inertia.
    
    This model proposes that anxiety creates a direct resistance to switching 
    actions (inertia), independent of the learned values. Unlike stickiness 
    (which is often implemented as a value bonus), this model acts directly on 
    the switching probability, dampening it. High anxiety acts as a "brake" 
    on the decision to change strategy, even if Q-values suggest a switch might 
    be optimal.
    
    Mechanism:
    P_switch_raw = Softmax(Q)[switch_action]
    P_switch_final = P_switch_raw * (1 - stai * inertia_factor)
    P_stay_final = 1 - P_switch_final
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    inertia_factor: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.inertia_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Get standard softmax probabilities
        probs = self.softmax(self.q_stage1, self.beta)
        
        # If this is the first trial, no inertia applies
        if self.last_action1 is None:
            return probs
        
        # Identify stay and switch indices
        stay_idx = self.last_action1
        switch_idx = 1 - stay_idx
        
        # Apply inertia: dampen the probability of switching
        # The dampening is proportional to STAI
        dampening = self.stai * self.inertia_factor
        
        # Ensure dampening doesn't make prob negative (though bounds should prevent this)
        dampening = np.clip(dampening, 0, 1)
        
        probs[switch_idx] *= (1.0 - dampening)
        
        # Re-normalize (effectively adding the lost mass to the stay option)
        probs[stay_idx] = 1.0 - probs[switch_idx]
        
        return probs

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```