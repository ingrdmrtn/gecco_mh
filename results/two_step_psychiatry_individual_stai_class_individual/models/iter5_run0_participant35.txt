Here are three new cognitive models exploring different mechanisms for how anxiety (STAI) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model tests the hypothesis that anxiety shifts the balance between goal-directed (Model-Based) and habitual (Model-Free) control. High anxiety is often associated with a reliance on simpler, habitual strategies (Model-Free) over computationally expensive planning (Model-Based).

```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety shifts the balance from Model-Based (MB) to Model-Free (MF) control.
    
    Cognitive Theory: High anxiety consumes working memory resources, making it harder 
    to maintain and use the transition model (T) of the task. Therefore, higher STAI 
    scores should correlate with a lower mixing weight (w), favoring Model-Free learning 
    (TD learning) over Model-Based planning.
    
    Mechanism:
    Q_MB(a1) = T(a1, s1)*V(s1) + T(a1, s2)*V(s2)
    Q_MF(a1) is learned via TD.
    Q_net = w * Q_MB + (1-w) * Q_MF
    
    The mixing weight 'w' is modulated by STAI:
    w = w_base * (1 - stai)
    
    If STAI is high (near 1), w becomes small, leading to pure Model-Free behavior.
    If STAI is low, w is closer to w_base.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_base: [0, 1]     # Baseline model-based weight for a non-anxious person
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # We need separate storage for Model-Free Q-values at stage 1
        self.q_mf_stage1 = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values
        # V(state) is max Q(state, action)
        v_stage2 = np.max(self.q_stage2, axis=1) 
        
        # Q_MB = Transition_Matrix * V_stage2
        # T is shape (2, 2) -> [action, state]
        # We want Q_MB[action] = sum(P(s|a) * V(s))
        q_mb_stage1 = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            q_mb_stage1[a] = np.sum(self.T[a] * v_stage2)

        # 2. Calculate Mixing Weight modulated by Anxiety
        # Higher anxiety -> lower w -> more Model-Free
        w = self.w_base * (1.0 - self.stai)
        
        # 3. Combine
        q_net = w * q_mb_stage1 + (1 - w) * self.q_mf_stage1
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) update for Stage 1 Model-Free values
        # In a full TD(lambda) model this would be more complex, but here we simplify:
        # The MF system updates stage 1 based on the stage 2 value or reward.
        # Standard Daw et al. (2011) uses the stage 2 value as the target for stage 1.
        
        # Update MF Stage 1 using the value of the state reached
        v_state_reached = np.max(self.q_stage2[state])
        delta_1 = v_state_reached - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion (Asymmetric Learning)
This model hypothesizes that anxiety specifically amplifies the learning signal from negative outcomes (or lack of reward), making the participant quicker to abandon choices that fail to pay off.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases sensitivity to negative prediction errors (Loss Aversion).
    
    Cognitive Theory: Anxious individuals are often hyper-vigilant to threat or failure. 
    In this reinforcement learning context, receiving 0 coins is a "loss" or punishment.
    Anxiety acts as a multiplier on the learning rate specifically when the prediction 
    error is negative (outcome < expectation).
    
    Mechanism:
    If delta < 0: alpha_effective = alpha * (1 + penalty_boost * stai)
    If delta > 0: alpha_effective = alpha
    
    This means high anxiety participants will drastically reduce the value of a spaceship/alien 
    after a failure, leading to "win-stay, lose-shift" behavior that is more reactive to losses.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Base learning rate for positive updates
    beta: [0, 10]           # Inverse temperature
    penalty_boost: [0, 5]   # Multiplier for negative updates based on STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.penalty_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha for Stage 2
        if delta_2 < 0:
            alpha_2 = self.alpha * (1.0 + self.penalty_boost * self.stai)
            # Cap alpha at 1.0 to prevent instability
            alpha_2 = min(alpha_2, 1.0)
        else:
            alpha_2 = self.alpha
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # We use the updated stage 2 value to drive stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine effective alpha for Stage 1
        if delta_1 < 0:
            alpha_1 = self.alpha * (1.0 + self.penalty_boost * self.stai)
            alpha_1 = min(alpha_1, 1.0)
        else:
            alpha_1 = self.alpha

        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression (Inverse Temperature Modulation)
This model suggests that anxiety reduces exploration by sharpening the decision curve (increasing beta), causing the participant to rigidly exploit the currently best-known option.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety suppresses exploration by sharpening the softmax function.
    
    Cognitive Theory: Anxiety is associated with intolerance of uncertainty. To minimize 
    uncertainty, anxious individuals may exploit their current knowledge more rigidly 
    rather than exploring alternatives. This manifests as a higher inverse temperature (beta).
    
    Mechanism:
    beta_effective = beta_base + (stiffness_factor * stai)
    
    A high STAI score increases beta, making the softmax function steeper. This results 
    in deterministic behavior where the option with even a slightly higher value is 
    chosen with near 100% probability, reducing random exploration.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]            # Learning rate
    beta_base: [0, 10]       # Baseline inverse temperature
    stiffness_factor: [0, 10]# How much anxiety increases rigidity
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stiffness_factor = model_parameters

    def init_model(self) -> None:
        # Calculate the fixed effective beta for this participant
        self.beta_effective = self.beta_base + (self.stiffness_factor * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_effective)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_effective)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```