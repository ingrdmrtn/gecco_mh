Here are three new cognitive models that incorporate the participant's anxiety score (STAI) to explain their decision-making behavior.

### Model 1: Anxiety-Modulated Eligibility Trace
```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety enhances Model-Free "Habit" formation via Eligibility Traces.
    
    Standard TD learning (TD(0)) updates the first-stage value based on the 
    second-stage value. However, anxious individuals may rely more on "direct" 
    reinforcement from the final outcome, bypassing the model of the transition structure.
    
    This model implements a TD(lambda) update where the eligibility trace parameter 
    (lambda) is modulated by STAI. Higher anxiety leads to a higher lambda, 
    meaning the Stage 1 choice is more strongly reinforced by the Stage 2 reward 
    directly (Model-Free), rather than just the Stage 2 value estimate.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta: [0, 10]        # Inverse temperature
    lambda_scale: [0, 1.5] # Scales STAI to determine lambda (clamped at 1.0)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_scale = model_parameters

    def init_model(self) -> None:
        # Calculate the eligibility trace parameter based on anxiety
        # lambda = 0: Pure TD(0) (Update S1 based on S2 value)
        # lambda = 1: Pure Monte Carlo/MF (Update S1 based on Reward)
        self.lambda_val = np.clip(self.lambda_scale * self.stai, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update with Anxiety-Modulated Eligibility Trace
        # Q1_new = Q1 + alpha * [ (Q2 - Q1) + lambda * (Reward - Q2) ]
        # This blends the prediction from the next state (Q2) and the actual reward
        q2_val = self.q_stage2[state, action_2]
        q1_val = self.q_stage1[action_1]
        
        # The TD error from the transition (S1 -> S2)
        td_error_transition = q2_val - q1_val
        
        # The TD error from the outcome (S2 -> Reward), scaled by lambda
        td_error_outcome = reward - q2_val
        
        # Combined update
        total_delta = td_error_transition + (self.lambda_val * td_error_outcome)
        self.q_stage1[action_1] += self.alpha * total_delta

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Decision Rigidity
```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases Decision Rigidity (Exploitation Bias).
    
    Anxiety is often associated with a "freezing" response or an intolerance 
    of uncertainty/conflict. To minimize cognitive conflict, anxious participants 
    may exhibit "sharpened" decision policies, sticking deterministically to 
    the option with the slightly higher value rather than exploring.
    
    This model hypothesizes that STAI modulates the inverse temperature (beta).
    Higher anxiety leads to a higher effective beta, making choices more 
    deterministic and less sensitive to small value differences (or noise).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta_base: [0, 10]   # Baseline inverse temperature
    rigidity: [0, 5]     # How strongly STAI increases beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.rigidity = model_parameters
        
    def init_model(self) -> None:
        # Calculate effective beta once, as STAI is constant for the participant
        # beta_eff = beta_base * (1 + rigidity * STAI)
        self.beta_eff = self.beta_base * (1.0 + self.rigidity * self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Use the anxiety-modulated effective beta
        return self.softmax(self.q_stage1, self.beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Use the anxiety-modulated effective beta
        return self.softmax(self.q_stage2[state], self.beta_eff)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Risk Aversion
```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety drives Risk Aversion via Variance Penalty.
    
    Anxious individuals are often characterized not just by sensitivity to loss, 
    but by an aversion to unpredictability (variance). 
    
    This model tracks the variance of rewards for each action in addition to 
    the mean Q-value. During decision making, the value of an option is 
    penalized by its estimated variance, scaled by the participant's STAI score.
    This makes anxious participants avoid "risky" options (high variance) even 
    if they have decent mean rewards.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate for both Mean and Variance
    beta: [0, 10]        # Inverse temperature
    risk_w: [0, 5]       # Weight of the variance penalty, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.risk_w = model_parameters

    def init_model(self) -> None:
        # Initialize variance tracking matrices
        self.var_stage1 = np.zeros(self.n_choices)
        self.var_stage2 = np.zeros((self.n_states, self.n_choices))

    def get_risk_adjusted_values(self, q_values, variances):
        # Effective Value = Q - (STAI * risk_weight * sqrt(Variance))
        penalty = self.stai * self.risk_w * np.sqrt(variances)
        return q_values - penalty

    def policy_stage1(self) -> np.ndarray:
        # Adjust Q-values by risk penalty before softmax
        adj_q = self.get_risk_adjusted_values(self.q_stage1, self.var_stage1)
        return self.softmax(adj_q, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Adjust Q-values by risk penalty before softmax
        adj_q = self.get_risk_adjusted_values(self.q_stage2[state], self.var_stage2[state])
        return self.softmax(adj_q, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # --- Stage 2 Update ---
        # Prediction error for mean
        pe_2 = reward - self.q_stage2[state, action_2]
        
        # Update Variance estimate: V <- V + alpha * ((R - Q)^2 - V)
        # We use the squared prediction error as the instantaneous variance sample
        var_error_2 = (pe_2**2) - self.var_stage2[state, action_2]
        self.var_stage2[state, action_2] += self.alpha * var_error_2
        
        # Update Mean Q
        self.q_stage2[state, action_2] += self.alpha * pe_2
        
        # --- Stage 1 Update ---
        # For Stage 1, the "reward" is the value of the state we landed in (Q_stage2)
        # This is a simplification for variance tracking in hierarchical tasks
        current_val_s2 = self.q_stage2[state, action_2]
        pe_1 = current_val_s2 - self.q_stage1[action_1]
        
        # Update Variance estimate for Stage 1
        var_error_1 = (pe_1**2) - self.var_stage1[action_1]
        self.var_stage1[action_1] += self.alpha * var_error_1
        
        # Update Mean Q for Stage 1
        self.q_stage1[action_1] += self.alpha * pe_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```