Here are three new cognitive models exploring different mechanisms by which anxiety (STAI) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Trade-off
This model tests the hypothesis that anxiety shifts the balance between goal-directed (model-based) and habitual (model-free) control. High anxiety is often associated with reduced cognitive resources, potentially leading to a greater reliance on simpler, model-free strategies (ignoring the transition structure) rather than computationally expensive model-based planning.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Shift to Model-Free Control]
    This model implements a hybrid reinforcement learning agent that mixes 
    Model-Based (MB) and Model-Free (MF) values. 
    Hypothesis: Higher anxiety (STAI) reduces the weight (w) placed on Model-Based planning,
    biasing the participant towards Model-Free (habitual) control.
    
    The mixing weight 'w' is calculated as: w = w_max * (1 - stai * sensitivity)
    where w=1 is pure MB and w=0 is pure MF.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       - Learning rate
    beta: [0, 10]       - Inverse temperature
    w_max: [0, 1]       - Maximum model-based weight (at 0 anxiety)
    sensitivity: [0, 1] - How strongly STAI reduces the MB weight
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.sensitivity = model_parameters
        
        # Calculate mixing weight w based on STAI
        # Higher STAI -> Lower w (less Model-Based)
        # We clip to ensure w stays in [0, 1]
        raw_w = self.w_max * (1.0 - (self.stai * self.sensitivity))
        self.w = np.clip(raw_w, 0.0, 1.0)

    def init_model(self) -> None:
        # Initialize Model-Free Q-values for stage 1
        self.q_mf = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # Expected value of best action in the next state
            v_next = np.max(self.q_stage2, axis=1) 
            q_mb[a] = np.sum(self.T[a] * v_next)
            
        # Mix MB and MF values
        q_net = (self.w * q_mb) + ((1 - self.w) * self.q_mf)
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 values (common to both systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 Model-Free values (TD(1) style for simplicity or SARSA)
        # Here we use the standard TD update from the reward at stage 2
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Loss Aversion
This model hypothesizes that anxious individuals are more sensitive to negative outcomes (or lack of reward) than positive ones. Instead of a single learning rate, the model uses separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The STAI score amplifies the learning rate for negative errors, making the participant quicker to abandon choices that yield disappointment.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Amplified Negative Learning]
    This model hypothesizes that anxiety increases sensitivity to negative prediction errors.
    The participant has a base learning rate for positive outcomes (alpha_pos).
    The learning rate for negative outcomes (alpha_neg) is boosted by the STAI score.
    
    alpha_neg = alpha_neg_base + (stai * neg_boost)
    
    This implies anxious individuals learn faster from failure/omission of reward.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      - Learning rate for positive RPE
    alpha_neg_base: [0, 1] - Base learning rate for negative RPE
    neg_boost: [0, 1]      - Scaling factor for STAI on negative learning
    beta: [0, 10]          - Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.neg_boost, self.beta = model_parameters
        
        # Calculate anxiety-modulated negative learning rate
        self.alpha_neg = self.alpha_neg_base + (self.stai * self.neg_boost)
        self.alpha_neg = np.clip(self.alpha_neg, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Note: In standard TD, the reward for stage 1 is 0, so the "reward" is the value of the next state
        # We use the value of the chosen stage 2 action as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced Exploration Suppression (Inverse Temperature Modulation)
This model posits that anxiety reduces exploration. Anxious individuals may prefer to exploit known high-value options rather than explore uncertain ones to avoid potential threats. This is modeled by modulating the inverse temperature parameter ($\beta$) of the softmax function. A higher STAI score leads to a higher $\beta$, resulting in more deterministic (greedy) choices and less random exploration.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Exploitation / Reduced Exploration]
    This model hypothesizes that anxiety reduces the willingness to explore.
    High anxiety leads to more deterministic choice behavior (higher beta),
    sticking strictly to the option with the highest estimated value.
    
    beta_effective = beta_base * (1 + (stai * rigidity_scale))
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta_base: [0, 10]     - Baseline inverse temperature (low anxiety)
    rigidity_scale: [0, 5] - How much STAI increases beta (determinism)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.rigidity_scale = model_parameters
        
        # Calculate effective beta
        # If STAI is high, beta becomes very large -> greedy behavior
        self.beta_effective = self.beta_base * (1.0 + (self.stai * self.rigidity_scale))

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_effective)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_effective)
        
    # Standard value update (TD learning)
    # Inherited from Base

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```