Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this task.

### Model 1: Anxiety-Driven Habituation
**Hypothesis:** High anxiety promotes a shift from goal-directed (value-based) control to habitual (repetition-based) control. This model tracks a separate "habit" trace that strengthens with every choice of an action, regardless of the outcome. The influence of this habit trace on the final choice is scaled by the participant's STAI score.
**Mechanism:** A habit trace `H` updates via `H(a) += alpha * (1 - H(a))` for the chosen action and decays for the unchosen. The decision variable is `Q + habit_weight * STAI * H`.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Habituation]
    Anxiety promotes reliance on habitual responses over value-based calculations.
    This model maintains a 'habit strength' trace for each stage-1 action, which
    accumulates simply by choosing the action (regardless of reward).
    
    The influence of this habit strength on choice is scaled by STAI.
    High anxiety -> Stronger tendency to repeat frequent choices (Habit).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        - Learning rate for both Value and Habit
    beta: [0, 10]        - Inverse temperature
    habit_weight: [0, 5] - Scaling factor for anxiety's effect on habit influence
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.habit_weight = model_parameters

    def init_model(self) -> None:
        # Initialize habit strength for the two stage-1 choices (A and U)
        self.habit = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Combine Value (Q) and Habit (H)
        # The weight of the habit component is modulated by anxiety (STAI)
        net_values = self.q_stage1 + (self.habit_weight * self.stai * self.habit)
        return self.softmax(net_values, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Update Habit trace: Strengthen chosen, weaken unchosen
        # We reuse self.alpha for the habit learning rate to minimize parameters
        self.habit[action_1] += self.alpha * (1 - self.habit[action_1])
        self.habit[1 - action_1] += self.alpha * (0 - self.habit[1 - action_1])

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Negative Learning Freeze
**Hypothesis:** Anxious individuals may exhibit a specific insensitivity to negative feedback (failure) as a defense mechanism to maintain perceived control or safety. While they learn normally from rewards, they "freeze" or ignore prediction errors when the outcome is worse than expected (negative prediction error). This asymmetry explains sticking to a choice despite frequent zero-reward outcomes.
**Mechanism:** The learning rate `alpha` is dynamically suppressed by STAI when the prediction error is negative. `alpha_effective = alpha * (1 - freeze_factor * STAI)` if `delta < 0`.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Negative Learning Freeze]
    High anxiety leads to an avoidance of processing failure signals.
    When a negative prediction error occurs (outcome < expectation), the learning
    rate is suppressed proportional to STAI. This causes the participant to 
    'ignore' losses and persist with their current strategy (perseveration), 
    while still learning normally from wins.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]         - Base learning rate (for positive errors)
    beta: [0, 10]         - Inverse temperature
    neg_freeze: [0, 1]    - Proportion of suppression for negative errors due to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.neg_freeze = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha for Stage 2
        alpha_2 = self.alpha
        if delta_2 < 0:
            # Suppress learning from negative errors based on anxiety
            alpha_2 *= (1.0 - (self.neg_freeze * self.stai))
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Note: In this base class structure, Stage 1 updates from Stage 2 value (TD-0)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine effective alpha for Stage 1
        alpha_1 = self.alpha
        if delta_1 < 0:
            alpha_1 *= (1.0 - (self.neg_freeze * self.stai))
            
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated "Win-Stay" Amplification
**Hypothesis:** Anxiety increases the reliance on simple, reactive heuristics like "Win-Stay" over complex value integration. Specifically, when a choice results in a reward, anxious participants receive an additional "safety signal" bonus that makes them much more likely to repeat that specific action immediately, overriding the long-term Q-value.
**Mechanism:** If the previous trial was rewarded, a temporary bonus is added to the Q-value of the previously chosen action. The magnitude of this bonus is scaled by STAI. This differs from general stickiness (which applies regardless of outcome).

```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Win-Stay Amplification]
    Anxiety amplifies the reactive "Win-Stay" heuristic. 
    If the previous trial resulted in a reward (coin), anxious participants 
    perceive the chosen option as a "safe haven" and receive a temporary 
    value bonus for that option on the next trial.
    
    This bonus is added to the Q-value only for the immediate next trial 
    and is conditional on the previous reward being positive.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta: [0, 10]          - Inverse temperature
    win_bonus: [0, 5]      - Magnitude of the safety bonus scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.win_bonus = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy values to avoid modifying the actual Q-table
        current_values = self.q_stage1.copy()
        
        # Apply Win-Stay Bonus
        if self.last_action1 is not None and self.last_reward is not None:
            if self.last_reward > 0:
                # Bonus is proportional to Anxiety (STAI)
                bonus = self.win_bonus * self.stai
                current_values[int(self.last_action1)] += bonus
        
        return self.softmax(current_values, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```