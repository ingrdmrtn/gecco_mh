```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning from Rare Transitions.
    Standard Model-Free learning can be disrupted or enhanced by 'rare' transitions (unexpected outcomes).
    Anxious individuals (high STAI) might be hyper-reactive to these violations of expectation,
    assigning a different learning rate to trials where the transition was rare (probability < 0.5).
    
    alpha_rare = alpha_rare_base + (alpha_rare_slope * stai)
    
    If the transition was common, a fixed alpha_common is used. This tests if anxiety specifically
    alters how participants update values following surprising state transitions.

    Parameter Bounds:
    -----------------
    alpha_common: [0, 1]
    beta: [0, 10]
    alpha_rare_base: [0, 1]
    alpha_rare_slope: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_common, self.beta, self.alpha_rare_base, self.alpha_rare_slope = model_parameters

    def init_model(self) -> None:
        # Pre-calculate the rare learning rate based on STAI
        self.alpha_rare = self.alpha_rare_base + (self.alpha_rare_slope * self.stai)
        # Clip to valid range [0, 1] to prevent instability
        self.alpha_rare = np.clip(self.alpha_rare, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine if transition was rare
        # T[action, state] gives probability of that transition.
        transition_prob = self.T[action_1, state]
        
        # Select learning rate based on transition probability
        if transition_prob < 0.5:
            current_alpha = self.alpha_rare
        else:
            current_alpha = self.alpha_common

        # Standard TD update with selected alpha
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += current_alpha * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += current_alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety drives 'Freezing' or Perseveration specifically after losses.
    General stickiness applies to all choices. This model hypothesizes that anxiety specifically
    modulates the tendency to repeat a choice after a negative outcome (reward <= 0).
    This reflects a 'safety seeking' or 'freezing' response where the participant clings to 
    the previous choice despite failure, inhibiting the adaptive 'Lose-Shift' strategy.

    stickiness_loss = stick_base + (stick_slope * stai)
    (Stickiness after wins is assumed to be 0 for this model to isolate the loss-reaction).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    stick_base: [0, 5]
    stick_slope: [-5, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stick_slope = model_parameters

    def init_model(self) -> None:
        self.stickiness_loss = self.stick_base + (self.stick_slope * self.stai)

    def policy_stage1(self) -> np.ndarray:
        qs = self.q_stage1.copy()
        
        # Apply stickiness only if the last trial existed and resulted in a loss (or zero reward)
        if self.last_action1 is not None and self.last_reward is not None:
            if self.last_reward <= 0:
                qs[self.last_action1] += self.stickiness_loss
                
        return self.softmax(qs, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases Uncertainty Aversion (Variance Penalty).
    Anxious individuals are often intolerant of uncertainty. This model tracks the variance 
    of rewards for each option and subtracts a penalty from the Q-value.
    The magnitude of this penalty is modulated by STAI.
    
    penalty_weight = pen_base + (pen_slope * stai)
    Effective Value = Q - (penalty_weight * Variance)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    pen_base: [0, 5]
    pen_slope: [-5, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pen_base, self.pen_slope = model_parameters

    def init_model(self) -> None:
        self.penalty_weight = self.pen_base + (self.pen_slope * self.stai)
        # Initialize second moment estimates (E[R^2]) to track variance
        self.q_sq_stage1 = np.zeros(self.n_choices)
        self.q_sq_stage2 = np.zeros((self.n_states, self.n_choices))

    def policy_stage1(self) -> np.ndarray:
        # Calculate variance: E[R^2] - (E[R])^2
        # We approximate stage 1 variance based on stage 1 Q-values
        variance = self.q_sq_stage1 - (self.q_stage1 ** 2)
        # Clip variance to be non-negative (handle numerical errors)
        variance = np.maximum(variance, 0)
        
        adjusted_values = self.q_stage1 - (self.penalty_weight * variance)
        return self.softmax(adjusted_values, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        variance = self.q_sq_stage2[state] - (self.q_stage2[state] ** 2)
        variance = np.maximum(variance, 0)
        
        adjusted_values = self.q_stage2[state] - (self.penalty_weight * variance)
        return self.softmax(adjusted_values, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q update (First Moment)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Second moment (Reward^2) update
        # For stage 2, it's direct reward squared
        r_sq = reward ** 2
        delta_sq_2 = r_sq - self.q_sq_stage2[state, action_2]
        self.q_sq_stage2[state, action_2] += self.alpha * delta_sq_2
        
        # For stage 1, we propagate the second moment estimate from stage 2
        target_sq_1 = self.q_sq_stage2[state, action_2]
        delta_sq_1 = target_sq_1 - self.q_sq_stage1[action_1]
        self.q_sq_stage1[action_1] += self.alpha * delta_sq_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```