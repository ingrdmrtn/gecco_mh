```python
import numpy as np
from abc import ABC, abstractmethod

# Base Class (Included for context, not modified)
class CognitiveModelBase(ABC):
    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = np.zeros((self.n_states, self.n_choices))
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        pass

    def init_model(self) -> None:
        pass

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

# --- New Models ---

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Enhanced Model-Based Planning.
    While some theories suggest anxiety impairs cognitive function, others suggest
    it leads to over-thinking and hyper-vigilance. This model hypothesizes that
    higher anxiety leads to a stronger reliance on Model-Based (MB) planning 
    over Model-Free (MF) habits.
    
    The Stage 1 policy is a mix of MF values (learned via TD) and MB values 
    (computed via the transition matrix). The weight of the MB component 
    increases with STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate for MF values
    beta: [0, 10]       # Inverse temperature
    mb_amp: [0, 5]      # Amplification of Model-Based control by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.mb_amp = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values
        # V(state) = max(Q_stage2[state])
        v_stage2 = np.max(self.q_stage2, axis=1) 
        
        # Q_MB(action) = Sum(P(state|action) * V(state))
        # self.T is [2x2]: T[action, state]
        q_mb = np.dot(self.T, v_stage2)
        
        # 2. Calculate Mixing Weight based on Anxiety
        # Higher anxiety -> Higher weight on MB
        # We use a sigmoid-like clipping or just linear scaling.
        # Here we treat mb_amp as a scaling factor for the MB contribution.
        w_mb = self.stai * self.mb_amp
        
        # 3. Combine MF (self.q_stage1) and MB
        # Note: self.q_stage1 is updated via TD in value_update (Model-Free)
        q_net = self.q_stage1 + (w_mb * q_mb)
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Counterfactual Updating ("What If" Thinking).
    Anxious individuals often ruminate on unchosen options ("Did I make a mistake?").
    This model hypothesizes that anxiety drives counterfactual learning at Stage 2.
    When the participant observes a reward for their chosen alien, they also update 
    the value of the *unchosen* alien, assuming an inverse correlation or simply 
    decaying it, driven by a "fear of missing out" or regret minimization mechanism.
    
    Here, we implement a mechanism where the unchosen option is updated towards 
    the *opposite* of the received reward (or a neutral baseline), scaled by STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    cf_k: [0, 1]        # Counterfactual update rate scaler
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.cf_k = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard update for chosen action
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Counterfactual update for UNCHOSEN action at Stage 2
        unchosen_a2 = 1 - action_2
        
        # Hypothesis: Anxious people assume the unchosen option might have been better 
        # or worse depending on the outcome. 
        # Simple implementation: They update the unchosen option with a fraction of the 
        # prediction error, effectively generalizing the outcome to the state (planet)
        # rather than just the specific alien.
        cf_learning_rate = self.alpha * self.stai * self.cf_k
        
        # We apply the same delta to the unchosen option. 
        # If I got a reward, I assume the whole planet is good (generalization).
        # If I got no reward, I assume the whole planet is bad.
        self.q_stage2[state, unchosen_a2] += cf_learning_rate * delta_2
        
        # Standard Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Surprise Gating.
    Anxious individuals may have a lower tolerance for uncertainty or "noise".
    When a "Rare" transition occurs (e.g., choosing Spaceship A but going to Planet Y),
    an anxious participant might treat this as an anomaly or an error and suppress 
    learning to protect their internal model from volatility.
    
    This model scales the Stage 1 learning rate by STAI specifically during 
    rare transitions. High anxiety -> Low learning from rare events.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate
    beta: [0, 10]       # Inverse temperature
    gate_k: [0, 5]      # Strength of gating (suppression) by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.gate_k = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine if transition was Common or Rare
        # T = [[0.7, 0.3], [0.3, 0.7]]
        # Common: (A=0 -> X=0) or (U=1 -> Y=1)
        is_common = (action_1 == state)
        
        # Calculate effective alpha for Stage 1
        if not is_common:
            # Rare transition: Suppress learning based on anxiety
            # Factor decreases as STAI * gate_k increases
            suppression = self.stai * self.gate_k
            # Clamp suppression to avoid negative alpha, though simple max(0, ...) works
            alpha_eff = self.alpha * max(0.0, 1.0 - suppression)
        else:
            # Common transition: Normal learning
            alpha_eff = self.alpha

        # Stage 2 update (standard)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (modulated)
        # Note: We use the updated Q2 value for the TD error
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_eff * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```