Here are 3 new cognitive models exploring different mechanisms for how anxiety (STAI) might influence decision-making in this two-step task.

### Model 1: Anxiety-Driven Model-Based Suppression
This model tests the hypothesis that high anxiety consumes working memory resources, impairing the ability to use "Model-Based" (planning) strategies. Instead of a simple mixture, this model assumes a baseline model-based weight that is *suppressed* by the participant's anxiety level.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Model-Based Suppression]
    This model hypothesizes that anxiety consumes cognitive resources required for 
    model-based planning. The participant uses a hybrid Model-Based (MB) and 
    Model-Free (MF) strategy. The weight of the MB component (w) is determined 
    by a maximum capacity parameter, which is then reduced proportionally by the 
    participant's STAI score. High anxiety leads to more Model-Free behavior.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      - Learning rate
    beta: [0, 10]      - Inverse temperature
    w_capacity: [0, 1] - Maximum possible model-based weight (at 0 anxiety)
    stai_cost: [0, 1]  - Reduction in MB weight per unit of STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_capacity, self.stai_cost = model_parameters
        
        # Calculate effective model-based weight w
        # w = capacity - (cost * anxiety), clamped between 0 and 1
        raw_w = self.w_capacity - (self.stai_cost * self.stai)
        self.w = np.clip(raw_w, 0.0, 1.0)

    def init_model(self) -> None:
        # We need separate storage for Model-Free Q-values for stage 1
        self.q_mf_stage1 = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Model-Based Value Calculation:
        # Q_MB(a1) = sum(T(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # Transition probabilities for action a
            probs = self.T[a] 
            # Expected value of best action in next state
            max_q2 = np.max(self.q_stage2, axis=1)
            q_mb[a] = np.dot(probs, max_q2)
            
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = self.w * q_mb + (1 - self.w) * self.q_mf_stage1
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 values (Standard Q-learning)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 Model-Free values (TD-learning using Stage 2 value)
        # Note: Standard MF update often uses Q(s2, a2) as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Punishment Sensitivity
This model hypothesizes that anxious individuals are hypersensitive to negative outcomes (receiving 0 coins). Instead of a single learning rate, the model splits learning into positive (reward) and negative (omission) updates. The learning rate for negative outcomes is amplified by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Punishment Sensitivity]
    This model posits that anxiety specifically amplifies learning from negative 
    prediction errors (loss/omission of reward). The model uses separate learning 
    rates for positive and negative updates. The negative learning rate is 
    calculated as a base rate plus a boost proportional to the STAI score.
    This leads to faster avoidance of choices that yield 0 coins.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      - Learning rate for positive prediction errors
    alpha_neg_base: [0, 1] - Base learning rate for negative prediction errors
    stai_sens: [0, 2]      - Sensitivity of negative learning rate to STAI
    beta: [0, 10]          - Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.stai_sens, self.beta = model_parameters
        
        # Calculate effective negative learning rate
        # We clamp it to [0, 1] to ensure stability
        self.alpha_neg = np.clip(self.alpha_neg_base + (self.stai_sens * self.stai), 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Choose learning rate based on sign of prediction error
        lr_2 = self.alpha_pos if delta_2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += lr_2 * delta_2
        
        # --- Stage 1 Update ---
        # Using the updated Q-value from stage 2 as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Choose learning rate based on sign of prediction error
        lr_1 = self.alpha_pos if delta_1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += lr_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Exploration (Inverse Temperature)
This model suggests that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety might lead to more deterministic (rigid) behavior to reduce uncertainty, or conversely, more erratic behavior (panic). Here, we model anxiety as modulating the inverse temperature parameter ($\beta$).

```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Decision Noise]
    This model hypothesizes that anxiety directly alters the randomness of choice 
    (exploration vs exploitation). The inverse temperature parameter (beta) is 
    not fixed but is a function of the STAI score. 
    Beta = beta_base * (1 + stai_mod * STAI).
    If stai_mod is positive, anxiety makes choices more rigid (higher beta).
    If stai_mod is negative, anxiety makes choices more noisy (lower beta).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       - Learning rate
    beta_base: [0, 10]  - Baseline inverse temperature (at 0 anxiety)
    stai_mod: [-1, 5]   - Modulation factor for STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stai_mod = model_parameters
        
        # Calculate effective beta
        # We ensure beta doesn't go below 0
        self.beta_eff = max(0.0, self.beta_base * (1.0 + self.stai_mod * self.stai))

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_eff)

    # Standard value update (TD learning)
    # No override needed for value_update as we use standard Q-learning

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```