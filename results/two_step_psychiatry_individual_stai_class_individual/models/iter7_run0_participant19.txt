```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases memory decay for unchosen options.
    High anxiety consumes cognitive resources (working memory load), causing the value 
    of unchosen options to decay towards zero faster than in low anxiety individuals.
    
    decay = decay_base + (decay_stai_slope * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    decay_base: [0, 1]
    decay_stai_slope: [-1, 1]
    """
    
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_base, self.decay_stai_slope = model_parameters

    def init_model(self) -> None:
        # Calculate decay rate based on STAI
        raw_decay = self.decay_base + (self.decay_stai_slope * self.stai)
        self.decay = np.clip(raw_decay, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for chosen path
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Decay for the unchosen Stage 1 option
        unchosen_a1 = 1 - action_1
        self.q_stage1[unchosen_a1] *= (1.0 - self.decay)

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces 'freezing' (perseveration) specifically after negative outcomes.
    Unlike general stickiness, this model posits that anxiety leads to a repetition of the 
    previous choice only when the previous trial resulted in a loss or omission (reward <= 0).
    
    freeze_bonus = freeze_base + (freeze_stai_slope * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    freeze_base: [0, 5]
    freeze_stai_slope: [-5, 5]
    """
    
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.freeze_base, self.freeze_stai_slope = model_parameters

    def init_model(self) -> None:
        self.freeze_bonus = self.freeze_base + (self.freeze_stai_slope * self.stai)

    def policy_stage1(self) -> np.ndarray:
        qs = self.q_stage1.copy()
        # Apply freeze bonus if last reward was non-positive
        if self.last_action1 is not None and self.last_reward is not None:
            if self.last_reward <= 0:
                qs[self.last_action1] += self.freeze_bonus
        return self.softmax(qs, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety alters the balance between distal (Stage 1) and proximal (Stage 2) learning.
    Anxious individuals may focus more on immediate outcomes (Stage 2) and struggle to 
    propagate this information to distal choices (Stage 1).
    This model modulates the Stage 1 learning rate relative to Stage 2 based on STAI.
    
    alpha_stage1 = alpha_stage2 * (ratio_base + ratio_stai_slope * stai)
    
    Parameter Bounds:
    -----------------
    alpha_stage2: [0, 1]
    beta: [0, 10]
    ratio_base: [0, 2]
    ratio_stai_slope: [-2, 2]
    """
    
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_stage2, self.beta, self.ratio_base, self.ratio_stai_slope = model_parameters

    def init_model(self) -> None:
        ratio = self.ratio_base + (self.ratio_stai_slope * self.stai)
        # Ensure ratio is non-negative
        ratio = max(0.0, ratio)
        self.alpha_stage1 = np.clip(self.alpha_stage2 * ratio, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 with alpha_stage2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha_stage2 * delta_2
        
        # Update Stage 1 with modulated alpha_stage1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha_stage1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```