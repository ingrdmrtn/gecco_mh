Here are 3 new cognitive models that explore different mechanisms by which high anxiety (STAI) might influence decision-making in this task, specifically focusing on how anxiety alters learning rates, model-based reasoning, and sensitivity to negative outcomes.

### Model 1: Anxiety-Modulated Learning Rate Asymmetry
This model hypothesizes that anxious individuals learn differently from positive versus negative outcomes. Specifically, high anxiety might lead to a "negativity bias" where negative prediction errors (disappointments) are weighted more heavily than positive ones, causing rapid avoidance of actions that lead to failure.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces a "Negativity Bias" in learning rates.
    High anxiety individuals may be hypersensitive to negative outcomes (punishment/omission of reward)
    while being less sensitive to positive outcomes. This model splits the learning rate alpha
    into alpha_pos and alpha_neg. The balance between them is shifted by the STAI score.
    
    Mechanism:
    alpha_pos = alpha_base * (1 - stai * bias_strength)
    alpha_neg = alpha_base * (1 + stai * bias_strength)
    
    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]   # Base learning rate
    beta: [0, 10]        # Inverse temperature
    bias_strength: [0, 1]# How strongly STAI skews the learning rates
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.bias_strength = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective learning rates based on STAI
        # High STAI -> lower alpha_pos, higher alpha_neg
        # We clamp to [0, 1] to ensure stability
        eff_alpha_pos = np.clip(self.alpha_base * (1.0 - self.stai * self.bias_strength), 0.0, 1.0)
        eff_alpha_neg = np.clip(self.alpha_base * (1.0 + self.stai * self.bias_strength), 0.0, 1.0)

        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = eff_alpha_pos if delta_2 >= 0 else eff_alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # We use the updated Q2 value to drive Q1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = eff_alpha_pos if delta_1 >= 0 else eff_alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Model-Based Suppression
This model tests the hypothesis that high anxiety consumes cognitive resources (working memory), thereby suppressing complex "Model-Based" (MB) planning in favor of simpler "Model-Free" (MF) habits. The standard two-step task analysis distinguishes between these strategies. Here, we implement a hybrid model where the mixing weight `w` (0=pure MF, 1=pure MB) is reduced by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety consumes cognitive resources, suppressing Model-Based control.
    The model calculates both Model-Free (TD) and Model-Based (Transition-dependent) values.
    The final choice is a weighted mix. The weight 'w' determines the MB contribution.
    We hypothesize that 'w' is negatively modulated by STAI: higher anxiety -> less MB, more MF.
    
    w_effective = w_max * (1 - stai * suppression_k)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]         # Learning rate
    beta: [0, 10]         # Inverse temperature
    w_max: [0, 1]         # Maximum model-based weight (at 0 anxiety)
    suppression_k: [0, 1] # How strongly STAI reduces the MB weight
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.suppression_k = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Value (Standard Q-learning)
        q_mf = self.q_stage1
        
        # 2. Model-Based Value (Bellman equation using transition matrix T)
        # Q_MB(a1) = P(s1|a1)*max(Q2(s1)) + P(s2|a1)*max(Q2(s2))
        # Note: self.T is [2x2], rows are actions (A, U) mapped to prob of state 0 (X).
        # But standard T is usually defined as T[action, next_state].
        # Base class T is [[0.7, 0.3], [0.3, 0.7]].
        # Row 0 is Action A -> [0.7 prob State X, 0.3 prob State Y]
        # Row 1 is Action U -> [0.3 prob State X, 0.7 prob State Y]
        
        v_stage2 = np.max(self.q_stage2, axis=1) # Max value of each state [V(X), V(Y)]
        q_mb = np.zeros(self.n_choices)
        
        # Calculate MB values
        q_mb[0] = self.T[0, 0] * v_stage2[0] + self.T[0, 1] * v_stage2[1] # Action A
        q_mb[1] = self.T[1, 0] * v_stage2[0] + self.T[1, 1] * v_stage2[1] # Action U
        
        # 3. Mix them based on anxiety
        w_eff = np.clip(self.w_max * (1.0 - self.stai * self.suppression_k), 0.0, 1.0)
        q_net = (1 - w_eff) * q_mf + w_eff * q_mb
        
        return self.softmax(q_net, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced "Safe Harbor" Heuristic
This model hypothesizes that anxious participants don't just maximize reward, but actively seek to minimize uncertainty or variance. Instead of standard RL, they might employ a heuristic where they prefer the "Common" transition because it feels safer/more predictable, regardless of the reward history, but this preference is only overridden if the reward difference is very large.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases preference for "Common" transitions (Predictability Bias).
    Anxious individuals may find rare transitions aversive or disorienting.
    This model adds a fixed "Predictability Bonus" to the Q-values of the actions 
    that lead to the currently most valuable state via a COMMON transition.
    Essentially, if State X is good, I prefer Action A (Common->X) more than Action U (Rare->X),
    even if the math says otherwise. The magnitude of this bias scales with STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta: [0, 10]        # Inverse temperature
    pred_bias: [0, 2]    # Magnitude of predictability bias, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pred_bias = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_vals = self.q_stage1.copy()
        
        # Determine which state is currently "better" based on Stage 2 values
        # We look at the max value available in each state
        values_states = np.max(self.q_stage2, axis=1) # [Val(X), Val(Y)]
        
        # Calculate the bias amount
        bias = self.pred_bias * self.stai
        
        # If State X (index 0) is better or equal to State Y (index 1)
        if values_states[0] >= values_states[1]:
            # Action A (0) commonly leads to X. Action U (1) rarely leads to X.
            # Anxious agent prefers the reliable path to the good state.
            q_vals[0] += bias
        else:
            # State Y is better.
            # Action U (1) commonly leads to Y. Action A (0) rarely leads to Y.
            q_vals[1] += bias
            
        return self.softmax(q_vals, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```