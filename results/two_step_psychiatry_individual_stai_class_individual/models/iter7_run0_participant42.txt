```python
import numpy as np
from abc import ABC, abstractmethod

# Base Class (Included for context, not modified)
class CognitiveModelBase(ABC):
    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = np.zeros((self.n_states, self.n_choices))
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        pass

    def init_model(self) -> None:
        pass

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

# =============================================================================
# Model 1: Anxiety-Modulated Subjective Utility (Loss Aversion)
# =============================================================================

class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Subjective Utility]
    This model hypothesizes that anxious individuals perceive a lack of reward (0 coins)
    not as a neutral event, but as a subjective loss (negative utility).
    The magnitude of this "pain of loss" is scaled by the participant's STAI score.
    This drives them to avoid options that yield 0 coins more strongly than a standard
    reinforcement learner would.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta: [0, 10]          - Inverse temperature
    loss_sens: [0, 5]      - Sensitivity to loss (0 reward) per unit of STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_sens = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective reward
        # If reward is 0, it is perceived as a negative value based on anxiety
        if reward == 0:
            effective_reward = -1.0 * (self.loss_sens * self.stai)
        else:
            effective_reward = reward

        # Standard TD update with effective_reward
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


# =============================================================================
# Model 2: Anxiety-Modulated Memory Decay
# =============================================================================

class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Memory Decay]
    This model hypothesizes that anxiety consumes cognitive resources (working memory),
    leading to faster decay (forgetting) of learned values for options that are not
    currently being chosen. High anxiety accelerates this decay, causing the participant
    to revert to neutral values (0) for unvisited states/actions more quickly.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          - Learning rate
    beta: [0, 10]          - Inverse temperature
    decay_base: [0, 1]     - Baseline decay rate
    decay_stai: [0, 1]     - Additional decay per unit of STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_base, self.decay_stai = model_parameters
        
        # Calculate total decay rate, clipped to [0, 1]
        raw_decay = self.decay_base + (self.decay_stai * self.stai)
        self.decay_rate = np.clip(raw_decay, 0.0, 1.0)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Decay unchosen Stage 1 option
        unchosen_1 = 1 - action_1
        self.q_stage1[unchosen_1] *= (1.0 - self.decay_rate)
        
        # Decay unchosen Stage 2 option in the visited state
        unchosen_2 = 1 - action_2
        self.q_stage2[state, unchosen_2] *= (1.0 - self.decay_rate)
        
        # Note: We could also decay the unvisited state's values, but standard
        # forgetting models often focus on the active context or global decay.
        # Here we decay the unvisited state as well to represent global memory loss.
        unvisited_state = 1 - state
        self.q_stage2[unvisited_state, :] *= (1.0 - self.decay_rate)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


# =============================================================================
# Model 3: Anxiety-Impaired Credit Assignment
# =============================================================================

class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Impaired Credit Assignment]
    This model hypothesizes that anxiety specifically impairs the "credit assignment"
    process between Stage 2 outcomes and Stage 1 choices. While the participant
    learns immediate associations (Stage 2) normally, high anxiety weakens the 
    transfer of value back to the initial choice (Stage 1), effectively decoupling
    the stages. This is modeled by a Stage 1 learning rate that is a fraction of
    the Stage 2 learning rate, with the impairment scaling with STAI.

    Parameter Bounds:
    -----------------
    alpha_2: [0, 1]        - Stage 2 learning rate (immediate outcomes)
    beta: [0, 10]          - Inverse temperature
    impairment: [0, 1]     - Proportional reduction of Stage 1 alpha per unit STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_2, self.beta, self.impairment = model_parameters
        
        # Calculate Stage 1 learning rate
        # alpha_1 decreases as anxiety increases
        # If impairment is 0, alpha_1 == alpha_2
        # If impairment is high and stai is high, alpha_1 approaches 0
        factor = 1.0 - (self.impairment * self.stai)
        factor = np.clip(factor, 0.0, 1.0)
        self.alpha_1 = self.alpha_2 * factor

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 with base learning rate
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha_2 * delta_2
        
        # Update Stage 1 with impaired learning rate
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```