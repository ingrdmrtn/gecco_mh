```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition matrix
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = np.zeros((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    """Create function interface from model class."""
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases state generalization (overgeneralization).
    Anxious participants may blur the distinction between the two planets (states).
    When they learn about an alien on one planet, they partially update the value 
    of the corresponding alien on the other planet. This reflects a tendency to 
    generalize threat or reward across contexts.
    
    The generalization factor 'kappa' determines how much the update in the current 
    state bleeds over to the other state.
    kappa = gen_base + (gen_slope * stai)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    gen_base: [0, 1]
    gen_slope: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.gen_base, self.gen_slope = model_parameters

    def init_model(self) -> None:
        # Calculate generalization factor based on anxiety
        self.kappa = self.gen_base + (self.gen_slope * self.stai)
        # Clamp kappa to [0, 1] to prevent instability
        self.kappa = np.clip(self.kappa, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update (Aliens)
        # Calculate prediction error for the visited state
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Update visited state
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Generalize update to the OTHER state (same action)
        other_state = 1 - state
        self.q_stage2[other_state, action_2] += self.alpha * delta_2 * self.kappa
        
        # Stage 1 Update (Spaceships) - Standard TD
        # Note: We use the updated value of the current state for the TD target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases reactive "Lose-Shift" behavior.
    Anxious individuals may be hypersensitive to negative outcomes (losses).
    Instead of purely value-based learning, they may employ a heuristic "flight" 
    response: if the last outcome was a loss (reward <= 0), they feel a strong 
    urge to switch to the other spaceship in the next trial, regardless of the 
    long-term value.
    
    This is modeled as a bonus added to the unchosen option's value in Stage 1 
    specifically after a loss.
    shift_bonus = shift_base + (shift_slope * stai)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    shift_base: [0, 5]
    shift_slope: [-5, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.shift_base, self.shift_slope = model_parameters

    def init_model(self) -> None:
        self.shift_bonus = self.shift_base + (self.shift_slope * self.stai)

    def policy_stage1(self) -> np.ndarray:
        qs = self.q_stage1.copy()
        
        # Apply Lose-Shift bonus if the last trial resulted in a loss/no reward
        if self.last_reward is not None and self.last_reward <= 0:
            if self.last_action1 is not None:
                # Add bonus to the action NOT chosen last time
                other_action = 1 - self.last_action1
                qs[other_action] += self.shift_bonus
                
        return self.softmax(qs, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases counterfactual updating ("Regret").
    Anxious participants may engage in more counterfactual processing, worrying 
    about "what if" they had chosen differently. They update the value of the 
    unchosen option in Stage 2 based on the inverse of the received reward 
    (assuming a competitive or zero-sum structure between aliens).
    
    The learning rate for this counterfactual update is modulated by STAI.
    alpha_cf = cf_base + (cf_slope * stai)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    cf_base: [0, 1]
    cf_slope: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.cf_base, self.cf_slope = model_parameters

    def init_model(self) -> None:
        self.alpha_cf = self.cf_base + (self.cf_slope * self.stai)
        # Clamp to reasonable learning rate bounds
        self.alpha_cf = np.clip(self.alpha_cf, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard update for the chosen option
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Counterfactual update for the UNCHOSEN option in Stage 2
        # Assume unchosen would have yielded the opposite reward (or negative of current)
        unchosen_action_2 = 1 - action_2
        fictitious_reward = -reward
        delta_cf = fictitious_reward - self.q_stage2[state, unchosen_action_2]
        self.q_stage2[state, unchosen_action_2] += self.alpha_cf * delta_cf
        
        # Standard Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```