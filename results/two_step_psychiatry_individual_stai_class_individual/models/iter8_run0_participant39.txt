Here are 3 new cognitive models exploring different mechanisms of how anxiety (STAI) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that high anxiety consumes working memory resources, impairing the ability to use complex "Model-Based" (planning) strategies. Instead, anxious participants rely more on "Model-Free" (habitual) learning. The mixing weight `w` between these two systems is directly suppressed by the STAI score.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety impairs Model-Based control (Resource Depletion).
    High anxiety consumes cognitive resources, reducing the weight (w) given to 
    model-based planning (which uses the transition matrix T).
    The mixing weight 'w' is modeled as a base capacity minus an anxiety penalty.
    
    Model-Based (MB) value: Q_MB(s1, a) = sum(T[s1, a, s2] * max(Q_s2))
    Model-Free (MF) value: Standard TD learning.
    Net Value = w * Q_MB + (1-w) * Q_MF

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_capacity: [0, 1]  # Maximum model-based capacity
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_capacity = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values
        # Q_MB(a) = P(State X | a) * V(State X) + P(State Y | a) * V(State Y)
        # V(State) is max(Q_stage2[State])
        v_stage2 = np.max(self.q_stage2, axis=1) # [V(X), V(Y)]
        
        # self.T is [2, 2] -> [[P(X|A), P(Y|A)], [P(X|U), P(Y|U)]]
        # Note: Base class T is [[0.7, 0.3], [0.3, 0.7]] which corresponds to row 0=A, row 1=U
        q_mb = np.dot(self.T, v_stage2)

        # 2. Calculate Mixing Weight w modulated by STAI
        # Higher STAI reduces w. We clamp w between 0 and 1.
        # We assume w_capacity is the potential, and STAI acts as a direct subtractive load.
        # Heuristic: w = w_capacity * (1 - STAI)
        w = self.w_capacity * (1.0 - self.stai)
        w = np.clip(w, 0.0, 1.0)

        # 3. Combine
        q_net = w * q_mb + (1 - w) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Negative Learning Bias
This model hypothesizes that anxious individuals are hypersensitive to negative outcomes (punishment) and learn more rapidly from them than from positive outcomes. The learning rate `alpha` is split into `alpha_pos` and `alpha_neg`, where `alpha_neg` is amplified by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety amplifies learning from negative prediction errors.
    Anxious individuals may have a 'negativity bias', updating their value 
    estimates more drastically when outcomes are worse than expected.
    
    The learning rate for negative prediction errors (alpha_neg) is boosted 
    by the STAI score relative to a base learning rate.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]   # Base learning rate for positive errors
    beta: [0, 10]        # Inverse temperature
    neg_amp: [0, 5]      # Multiplier for STAI to boost negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.neg_amp = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha based on sign of prediction error
        if delta_2 < 0:
            # Boost alpha by anxiety factor for negative errors
            # We clip to 1.0 to maintain stability
            alpha_eff = self.alpha_base + (self.neg_amp * self.stai)
            alpha_eff = min(alpha_eff, 1.0)
        else:
            alpha_eff = self.alpha_base

        self.q_stage2[state, action_2] += alpha_eff * delta_2
        
        # --- Stage 1 Update ---
        # Using the updated stage 2 value for TD error
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Apply same logic to stage 1
        if delta_1 < 0:
            alpha_eff_1 = self.alpha_base + (self.neg_amp * self.stai)
            alpha_eff_1 = min(alpha_eff_1, 1.0)
        else:
            alpha_eff_1 = self.alpha_base
            
        self.q_stage1[action_1] += alpha_eff_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression (Inverse Temperature Modulation)
This model hypothesizes that anxiety leads to risk aversion manifested as reduced exploration (higher exploitation). Anxious participants might "freeze" on the option they currently perceive as best, effectively having a higher inverse temperature (`beta`) in the softmax function.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety suppresses exploration (increases exploitation).
    High anxiety leads to a desire for control and predictability, reducing 
    stochastic exploration. This is modeled by increasing the inverse 
    temperature (beta) parameter in the softmax function as a function of STAI.
    
    Effective Beta = beta_base * (1 + stiff_k * STAI)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_base: [0, 10]  # Baseline inverse temperature
    stiff_k: [0, 10]    # Stiffness factor: how much STAI increases beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stiff_k = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective beta
        # If STAI is high, beta becomes very large -> deterministic choice (exploitation)
        beta_eff = self.beta_base * (1.0 + self.stiff_k * self.stai)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Apply same stiffness to second stage
        beta_eff = self.beta_base * (1.0 + self.stiff_k * self.stai)
        return self.softmax(self.q_stage2[state], beta_eff)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```