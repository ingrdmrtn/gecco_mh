### Cognitive Model 1: Anxiety-Modulated Transition Learning
```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the learning rate of the environmental structure (Transition Matrix).
    
    While the base model assumes a fixed transition matrix T, this model assumes the participant 
    actively learns the transition probabilities (A->X, U->Y) over time. 
    The rate at which they update their belief about these transitions is modulated by anxiety.
    High anxiety might lead to volatile structural beliefs (high alpha_T) or rigid beliefs (low alpha_T).
    The model uses these learned probabilities to compute Stage 1 values (Model-Based control).

    alpha_T = alpha_t_base + (alpha_t_slope * stai)
    (Clipped between 0 and 1)

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Reward learning rate)
    beta: [0, 10]
    alpha_t_base: [0, 1]
    alpha_t_slope: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.alpha_t_base, self.alpha_t_slope = model_parameters

    def init_model(self) -> None:
        # Initialize estimated transition matrix (start with uniform or slight prior)
        # T_est[a, 0] is probability of action 'a' leading to state 0 (Planet X)
        self.T_est = np.array([[0.5, 0.5], [0.5, 0.5]])
        
        # Calculate anxiety-modulated transition learning rate
        self.alpha_t = self.alpha_t_base + (self.alpha_t_slope * self.stai)
        self.alpha_t = np.clip(self.alpha_t, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Model-Based Valuation: Q1(a) = T(a, s') * V(s')
        # V(s') is estimated as max(Q2(s', :))
        v_stage2 = np.max(self.q_stage2, axis=1) # Shape (2,)
        
        # Compute MB values
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # Expected value = P(X|a)*V(X) + P(Y|a)*V(Y)
            q_mb[a] = self.T_est[a, 0] * v_stage2[0] + self.T_est[a, 1] * v_stage2[1]
            
        return self.softmax(q_mb, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Update Transition Belief
        # Outcome: Did we go to state 0 (Planet X)?
        outcome_is_X = 1.0 if state == 0 else 0.0
        
        # Prediction error for transition
        pe_t = outcome_is_X - self.T_est[action_1, 0]
        
        # Update probability of going to X
        self.T_est[action_1, 0] += self.alpha_t * pe_t
        # Ensure probability of going to Y is complementary
        self.T_est[action_1, 1] = 1.0 - self.T_est[action_1, 0]

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Cognitive Model 2: Anxiety-Modulated Loss Valuation
```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety alters the subjective magnitude of negative outcomes (Loss Valuation).
    
    Unlike learning rate modulation (which affects speed of update), this model hypothesizes that 
    anxiety changes the *asymptotic value* of losses. Anxious participants may perceive a 
    -1 coin loss as subjectively more severe (e.g., -2 or -5), leading to stronger avoidance 
    that persists even after learning stabilizes.
    
    If reward < 0:
        effective_reward = reward * (mag_base + mag_slope * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    mag_base: [0, 5]
    mag_slope: [-5, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.mag_base, self.mag_slope = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate subjective reward
        r_eff = reward
        if reward < 0:
            multiplier = self.mag_base + (self.mag_slope * self.stai)
            multiplier = max(0.0, multiplier) # Ensure magnitude doesn't flip sign
            r_eff = reward * multiplier
            
        # Standard TD update with effective reward
        delta_2 = r_eff - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Cognitive Model 3: Anxiety-Modulated Neutral Outcome Valuation
```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety alters the subjective valuation of neutral outcomes (0 coins).
    
    In a task with gains (+1) and losses (-1), a "0" outcome is ambiguous. 
    Anxious individuals might perceive "0" as a relief (positive valence relative to loss) 
    or as a failure/missed opportunity (negative valence relative to gain).
    This model assigns a subjective utility to 0 rewards based on STAI.
    
    If reward == 0:
        effective_reward = zero_val_base + (zero_val_slope * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    zero_val_base: [-1, 1]
    zero_val_slope: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.zero_val_base, self.zero_val_slope = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate subjective reward for neutral outcomes
        r_eff = reward
        if reward == 0:
            r_eff = self.zero_val_base + (self.zero_val_slope * self.stai)
            
        # Standard TD update with effective reward
        delta_2 = r_eff - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```