Here are 3 new cognitive models designed to capture the behavior of a high-anxiety participant who exhibits strong stickiness to choices followed by sudden shifts.

### Model 1: Anxiety-Modulated "Win-Stay, Lose-Shift" Hybrid
**Hypothesis:** High anxiety participants often rely on simple heuristics under uncertainty rather than full reinforcement learning. This model implements a hybrid strategy: it uses standard Q-learning, but mixes it with a heuristic "Win-Stay, Lose-Shift" (WSLS) tendency. The influence of the WSLS heuristic is modulated by the participant's anxiety (STAI). Specifically, high anxiety might increase reliance on this reactive strategy over the accumulated value history, making them sensitive to the most recent outcome despite a long history of rewards.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases reliance on reactive "Win-Stay, Lose-Shift" (WSLS) heuristics.
    Instead of purely relying on integrated Q-values, the participant's choice probability 
    is a mixture of the Q-value softmax and a deterministic WSLS rule based on the last trial.
    The mixing weight (wsls_weight) is scaled by STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature for Q-learning component
    w_heuristic: [0, 1] # Base weight of the heuristic, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_heuristic = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate standard Softmax probabilities from Q-values
        p_rl = self.softmax(self.q_stage1, self.beta)
        
        # 2. Calculate Heuristic probabilities (WSLS)
        p_wsls = np.ones(self.n_choices) / self.n_choices # Default uniform if no history
        
        if self.last_action1 is not None and self.last_reward is not None:
            p_wsls = np.zeros(self.n_choices)
            if self.last_reward > 0:
                # Win-Stay: Probability 1.0 on the last action
                p_wsls[int(self.last_action1)] = 1.0
            else:
                # Lose-Shift: Probability 1.0 on the OTHER action
                # Assuming 2 choices: 0 and 1. The other is 1 - last_action.
                p_wsls[1 - int(self.last_action1)] = 1.0
        
        # 3. Mix them based on anxiety
        # Higher STAI -> Higher weight on the heuristic
        # We clamp the weight to [0, 1] to ensure valid probabilities
        mixture_weight = np.clip(self.w_heuristic * self.stai, 0.0, 0.99)
        
        p_final = (1 - mixture_weight) * p_rl + mixture_weight * p_wsls
        return p_final

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced "Safe Haven" Bias
**Hypothesis:** Anxious individuals may treat specific options as "safe havens" regardless of their objective reward history, especially if they have encountered aversive outcomes (losses) elsewhere. This model posits that anxiety introduces a static bias towards one specific spaceship (e.g., Spaceship A, index 0) which is perceived as "safer" or "default." This bias is not learned but is an intrinsic preference magnitude scaled by STAI, effectively acting as a prior that distorts the Q-values.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates a static "Safe Haven" bias.
    The participant has an intrinsic preference for one option (arbitrarily set to Option A/0 here)
    that acts as a safety signal. This bias is added to the Q-value of Option 0 before 
    softmax selection. The magnitude of this safety bias is determined by STAI.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    safety_bias: [0, 5] # Magnitude of bias towards Option 0, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.safety_bias = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_biased = self.q_stage1.copy()
        
        # Add a static bias to action 0 (Spaceship A), scaled by anxiety
        # This represents the "safe" default choice.
        bias_amount = self.safety_bias * self.stai
        q_biased[0] += bias_amount
        
        return self.softmax(q_biased, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Model-Based/Model-Free Trade-off
**Hypothesis:** The two-step task is classically used to distinguish Model-Based (MB) planning (using the transition matrix) from Model-Free (MF) learning (ignoring structure). High anxiety consumes working memory resources, potentially impairing complex Model-Based planning. This model implements a mixture of MB and MF strategies, where the mixing parameter `w` (weight of MB) is *negatively* modulated by STAI. Higher anxiety reduces the contribution of the Model-Based system, forcing reliance on the simpler Model-Free system.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety impairs Model-Based (MB) planning, forcing reliance on Model-Free (MF).
    The stage 1 choice is a weighted combination of MF Q-values (TD learning) and 
    MB Q-values (computed via the transition matrix T and stage 2 values).
    The weight of the MB system (w_mb) is reduced as STAI increases.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_max: [0, 1]       # Maximum possible weight for Model-Based system (at STAI=0)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Values (self.q_stage1 is the MF value)
        q_mf = self.q_stage1
        
        # 2. Model-Based Values
        # Q_MB(a1) = Sum_s [ P(s|a1) * Max_a2 Q_stage2(s, a2) ]
        # We use the max of stage 2 values as the estimate of state value V(s)
        v_stage2 = np.max(self.q_stage2, axis=1) # Shape (2,)
        q_mb = np.zeros(self.n_choices)
        
        # self.T is shape (2, 2) -> [action, state] mapping roughly
        # T[0] is probs for action 0 -> [p(s0), p(s1)]
        # T[1] is probs for action 1 -> [p(s0), p(s1)]
        q_mb[0] = np.dot(self.T[0], v_stage2)
        q_mb[1] = np.dot(self.T[1], v_stage2)
        
        # 3. Calculate Mixing Weight
        # Hypothesis: Higher STAI reduces MB weight.
        # w_net = w_max * (1 - STAI)
        # If STAI is 1.0, purely Model-Free. If STAI is 0, mix is w_max MB.
        w_net = self.w_max * (1.0 - self.stai)
        w_net = np.clip(w_net, 0.0, 1.0)
        
        # 4. Combine Values
        q_net = (1 - w_net) * q_mf + w_net * q_mb
        
        return self.softmax(q_net, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```