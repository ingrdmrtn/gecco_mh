To simulate the behavior of `ParticipantModel2` (Anxiety-driven Perseveration), we need to instantiate the model class and step through the trials, generating choices based on the model's internal probabilities and updating its state based on the outcomes defined by the drifting reward probabilities.

Here is the simulation code:

```python
import numpy as np

def simulate_model(
    ModelClass,
    n_trials: int,
    stai: float,
    model_parameters: tuple,
    drift1, drift2, drift3, drift4,
    seed: int | None = None,
):
    """
    Simulate a two-step task trajectory using ParticipantModel2 (Anxiety-driven Perseveration).

    Task structure:
      - Stage 1: choose action_1 in {0,1} based on Q-values + anxiety-scaled stickiness.
      - Transition: state in {0,1} sampled from model.T[action_1].
      - Stage 2: choose action_2 in {0,1} conditioned on state.
      - Reward: sampled from drifting reward probabilities.

    Parameters
    ----------
    ModelClass : type
        The class ParticipantModel2.
    n_trials : int
        Number of trials to simulate.
    stai : float
        Participant anxiety score.
    model_parameters : tuple
        (alpha, beta, phi) expected by ParticipantModel2.
    drift1, drift2, drift3, drift4 : array-like (length n_trials)
        Trial-wise reward probabilities for the 4 (state, action) pairs.
    seed : int or None
        RNG seed.

    Returns
    -------
    stage1_choice : np.ndarray, shape (n_trials,)
    state2        : np.ndarray, shape (n_trials,)
    stage2_choice : np.ndarray, shape (n_trials,)
    reward        : np.ndarray, shape (n_trials,)
    """
    rng = np.random.default_rng(seed)

    # Instantiate the specific model (ParticipantModel2)
    # This initializes Q-values, transition matrix T, and unpacks parameters (alpha, beta, phi)
    model = ModelClass(n_trials=n_trials, stai=float(stai), model_parameters=model_parameters)

    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        model.trial = t
        model.pre_trial()

        # --- Stage 1 choice ---
        # The model.policy_stage1() method handles the anxiety-driven stickiness logic
        p1 = model.policy_stage1()
        
        # Safety normalization
        p1 = np.clip(p1, 1e-12, 1.0)
        p1 = p1 / np.sum(p1)
        
        # Generate choice
        a1 = int(rng.choice([0, 1], p=p1))

        # --- Transition ---
        # Standard probabilistic transition based on the model's T matrix
        pT = model.T[a1]
        pT = np.clip(pT, 1e-12, 1.0)
        pT = pT / np.sum(pT)
        s = int(rng.choice([0, 1], p=pT))

        # --- Stage 2 choice ---
        # Standard softmax selection based on stage 2 Q-values
        p2 = model.policy_stage2(s)
        p2 = np.clip(p2, 1e-12, 1.0)
        p2 = p2 / np.sum(p2)
        a2 = int(rng.choice([0, 1], p=p2))

        # --- Reward Generation ---
        # Determine probability of reward based on current state, action, and drift arrays
        if s == 0:
            pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
        else:
            pr = float(drift3[t]) if a2 == 0 else float(drift4[t])

        r = int(rng.random() < pr)

        # --- Learning Update ---
        # Update Q-values (Model-Free learning)
        model.value_update(a1, s, a2, float(r))
        
        # Update history (crucial for the stickiness logic in the next trial)
        model.post_trial(a1, s, a2, float(r))

        # Store data
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```