Here is the simulation code for the provided cognitive model. It is designed to work with the `CognitiveModelBase` structure and specifically accommodates the logic of `ParticipantModel3` (Anxiety-Driven Reward Clinging) by ensuring the `pre_trial`, `policy_stage1`, `value_update`, and `post_trial` methods are called in the correct order during the loop.

```python
import numpy as np

def simulate_model(
    ModelClass,
    n_trials: int,
    stai: float,
    model_parameters: tuple,
    drift1, drift2, drift3, drift4,
    seed: int | None = None,
):
    """
    Simulate a two-step task trajectory using the provided CognitiveModel class.
    
    This function instantiates the model and steps through trials, generating
    choices based on the model's policy methods and updating the model's internal
    state (Q-values, history) based on the outcomes.

    Parameters
    ----------
    ModelClass : type
        A subclass of CognitiveModelBase (e.g., ParticipantModel3).
    n_trials : int
        Number of trials to simulate.
    stai : float
        Participant anxiety score.
    model_parameters : tuple
        Parameters specific to the model (e.g., alpha, beta, cling_factor).
    drift1, drift2, drift3, drift4 : array-like
        Trial-wise reward probabilities for the 4 (state, action) pairs.
    seed : int or None
        Random number generator seed.

    Returns
    -------
    stage1_choice : np.ndarray
        Choices made in stage 1 (0 or 1).
    state2 : np.ndarray
        The state transitioned to (0 or 1).
    stage2_choice : np.ndarray
        Choices made in stage 2 (0 or 1).
    reward : np.ndarray
        Outcomes received (0 or 1).
    """
    rng = np.random.default_rng(seed)

    # Instantiate the model
    # This sets up internal storage (Q-values) and unpacks parameters
    model = ModelClass(n_trials=n_trials, stai=float(stai), model_parameters=model_parameters)

    # Initialize output arrays
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Update model's internal trial counter
        model.trial = t
        
        # 1. Pre-trial hooks (if any)
        model.pre_trial()

        # 2. Stage 1 Choice
        # Get probabilities from the model
        p1 = model.policy_stage1()
        # Numerical safety clipping and renormalization
        p1 = np.clip(p1, 1e-12, 1.0)
        p1 = p1 / np.sum(p1)
        # Sample action
        a1 = int(rng.choice([0, 1], p=p1))

        # 3. Transition to Stage 2
        # Get transition probabilities based on stage 1 choice
        pT = model.T[a1]
        pT = np.clip(pT, 1e-12, 1.0)
        pT = pT / np.sum(pT)
        # Sample state
        s = int(rng.choice([0, 1], p=pT))

        # 4. Stage 2 Choice
        # Get probabilities conditioned on the state
        p2 = model.policy_stage2(s)
        p2 = np.clip(p2, 1e-12, 1.0)
        p2 = p2 / np.sum(p2)
        # Sample action
        a2 = int(rng.choice([0, 1], p=p2))

        # 5. Reward Generation
        # Determine probability of reward based on current drift values
        if s == 0:
            pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
        else:
            pr = float(drift3[t]) if a2 == 0 else float(drift4[t])
        
        # Sample reward
        r = int(rng.random() < pr)

        # 6. Model Learning & History Update
        # Update Q-values
        model.value_update(a1, s, a2, float(r))
        # Store history (crucial for ParticipantModel3's "last_reward" check)
        model.post_trial(a1, s, a2, float(r))

        # Store data
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```