To simulate the behavior of the `ParticipantModel1` (which hypothesizes that anxiety modulates the trade-off between Model-Based and Model-Free control), we need a function that instantiates the model and steps through the trials, generating choices based on the model's internal probabilities rather than just calculating likelihoods.

Here is the simulation code:

```python
import numpy as np

def simulate_model(
    ModelClass,
    n_trials: int,
    stai: float,
    model_parameters: tuple,
    drift1, drift2, drift3, drift4,
    seed: int | None = None,
):
    """
    Simulate a two-step task trajectory using the provided CognitiveModel class.
    
    This function instantiates the model and steps through trials. At each step,
    it asks the model for action probabilities (policy), samples an action,
    determines the state transition and reward based on the task structure and
    drifting probabilities, and then updates the model's internal values.

    Parameters
    ----------
    ModelClass : type
        A subclass of CognitiveModelBase (e.g., ParticipantModel1).
    n_trials : int
        Number of trials to simulate.
    stai : float
        Participant anxiety score; passed into the model constructor.
    model_parameters : tuple
        Parameters expected by ModelClass.unpack_parameters(...).
    drift1, drift2, drift3, drift4 : array-like (length n_trials)
        Trial-wise reward probabilities for the 4 (state, action) pairs.
        drift1: State 0, Action 0
        drift2: State 0, Action 1
        drift3: State 1, Action 0
        drift4: State 1, Action 1
    seed : int or None
        RNG seed for reproducibility.

    Returns
    -------
    stage1_choice : np.ndarray, shape (n_trials,)
    state2        : np.ndarray, shape (n_trials,)
    stage2_choice : np.ndarray, shape (n_trials,)
    reward        : np.ndarray, shape (n_trials,)
    """
    rng = np.random.default_rng(seed)

    # Instantiate the model
    # This sets up internal Q-values, mixing weights (w_eff), and transition matrices
    model = ModelClass(n_trials=n_trials, stai=float(stai), model_parameters=model_parameters)

    # Initialize output arrays
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        model.trial = t
        model.pre_trial()

        # --- Stage 1: Choice ---
        # Get action probabilities from the model (mix of MB and MF based on STAI)
        p1 = model.policy_stage1()
        
        # Numerical stability clipping and renormalization
        p1 = np.clip(p1, 1e-12, 1.0)
        p1 = p1 / np.sum(p1)
        
        # Sample Action 1
        a1 = int(rng.choice([0, 1], p=p1))

        # --- Transition ---
        # Determine state transition based on model.T (fixed task structure)
        pT = model.T[a1]
        pT = np.clip(pT, 1e-12, 1.0)
        pT = pT / np.sum(pT)
        
        # Sample State
        s = int(rng.choice([0, 1], p=pT))

        # --- Stage 2: Choice ---
        # Get action probabilities for the specific state reached
        p2 = model.policy_stage2(s)
        
        # Numerical stability clipping and renormalization
        p2 = np.clip(p2, 1e-12, 1.0)
        p2 = p2 / np.sum(p2)
        
        # Sample Action 2
        a2 = int(rng.choice([0, 1], p=p2))

        # --- Reward Outcome ---
        # Determine reward probability based on current drift values
        if s == 0:
            pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
        else:
            pr = float(drift3[t]) if a2 == 0 else float(drift4[t])

        # Sample Reward
        r = int(rng.random() < pr)

        # --- Model Update ---
        # Store probabilities for logging (optional, but good practice)
        model.p_choice_1[t] = p1[a1]
        model.p_choice_2[t] = p2[a2]

        # Update model values (Q-learning / MB-planning updates happen here)
        model.value_update(a1, s, a2, float(r))
        
        # Bookkeeping
        model.post_trial(a1, s, a2, float(r))

        # Store simulation data
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward
```