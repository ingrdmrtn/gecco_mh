class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates choice perseveration (stickiness).
    
    We add a 'stickiness' bonus to the Q-value of the previously chosen action.
    The magnitude of this bonus is determined by a base parameter and the STAI score.
    
    Low anxiety (STAI < 0.31) -> Lower stickiness (more exploration/switching).
    High anxiety -> Higher stickiness (repetitive choice).
    
    Q(a) = Q_learned(a) + stickiness_bonus * I(a == last_action)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    pers_base: [0, 5]  # Base perseveration bonus
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_base = model_parameters

    def init_model(self) -> None:
        # Modulate perseveration by STAI
        # If STAI is high, we increase perseveration.
        # We scale STAI to have a meaningful impact on the bonus.
        # pers_param = base + slope * stai
        self.pers_param = self.pers_base * (1.0 + self.stai) 

    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Add stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            q_values[int(self.last_action1)] += self.pers_param
            
        return self.softmax(q_values, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)