class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Perseveration (Stickiness).
    
    This model hypothesizes that high anxiety causes "cognitive rigidity" or perseveration. 
    The participant is more likely to repeat their previous Stage 1 choice, regardless of 
    whether it was rewarded, as a coping mechanism for uncertainty.
    
    A "stickiness" bonus is added to the Q-value of the previously chosen action.
    The magnitude of this bonus is directly scaled by the STAI score.
    
    Q_net(a) = Q_learned(a) + (stickiness_param * STAI * IsLastAction(a))

    Parameter Bounds:
    -----------------
    alpha: [0, 1]     (Learning rate)
    beta: [0, 10]     (Inverse temperature)
    stickiness: [0, 5] (Base magnitude of perseveration)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy learned values so we don't permanently alter them with the bias
        q_biased = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is scaled by anxiety (STAI)
            bonus = self.stickiness * self.stai
            q_biased[int(self.last_action1)] += bonus
            
        return self.softmax(q_biased, self.beta)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)