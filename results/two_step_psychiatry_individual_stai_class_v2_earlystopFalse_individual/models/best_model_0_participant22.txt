class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: This model assumes the participant exhibits choice perseveration (stickiness) 
    modulated by their anxiety level. Higher anxiety might lead to more rigid behavior, 
    causing the participant to repeat the previous Stage 1 action regardless of reward.
    The 'perseveration' parameter is added to the Q-values of the previously chosen action.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    pers_base: [-2, 2]  # Base perseveration bonus
    pers_stai: [0, 5]   # Scaling factor for STAI on perseveration
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_base, self.pers_stai = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective perseveration bonus
        # We assume anxiety increases the magnitude of the base tendency (whether stickiness or switching)
        # or adds a specific stickiness bias.
        effective_pers = self.pers_base + (self.pers_stai * self.stai)
        
        q_biased = self.q_stage1.copy()
        
        # Add bonus to the previously chosen action if it exists
        if self.last_action1 is not None:
            q_biased[self.last_action1] += effective_pers
            
        return self.softmax(q_biased, self.beta)

    # Standard value update (TD learning)
    # We use the default implementation from Base Class, so no override needed for value_update
    # unless we wanted to change the learning rule itself.

cognitive_model2 = make_cognitive_model(ParticipantModel2)