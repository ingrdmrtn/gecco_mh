class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety (STAI) increases choice perseveration (stickiness).
    The participant is more likely to repeat their previous stage-1 choice, 
    regardless of the outcome. This is modeled by adding a bonus to the Q-value 
    of the previously chosen action.
    
    Q(action) = Q_learned(action) + (perseveration_param * STAI * is_last_action)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    pers_w: [0, 5]      # Perseveration weight scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_w = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy current Q-values to avoid modifying the learned values permanently
        q_effective = self.q_stage1.copy()
        
        # Apply perseveration bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is proportional to the anxiety score
            bonus = self.pers_w * self.stai
            q_effective[int(self.last_action1)] += bonus
            
        return self.softmax(q_effective, self.beta)

    # policy_stage2 uses default softmax from base class

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning for value updates
        # Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1
        # Using the value of the state reached (Q(s2, a2)) as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)