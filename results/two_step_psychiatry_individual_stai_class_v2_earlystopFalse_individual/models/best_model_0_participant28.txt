class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety amplifies "Win-Stay" behavior (Safety Clinging).
    
    Anxious participants may be strongly motivated to repeat actions that previously 
    resulted in success (safety signal), exhibiting a "Win-Stay" bias that is 
    stronger than standard reinforcement learning would predict. This is a 
    conditional stickiness that only activates after a reward.
    
    Mechanism:
    In Stage 1 policy:
    Logits = Q_stage1 * beta
    If last_reward == 1:
        Logits[last_action1] += w_win * STAI

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    w_win: [0, 5] (Strength of win-conditional stickiness)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_win = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate base logits
        logits = self.q_stage1 * self.beta
        
        # Apply conditional stickiness bias
        if self.last_action1 is not None and self.last_reward == 1.0:
            logits[int(self.last_action1)] += self.w_win * self.stai
            
        # Softmax on modified logits
        centered = logits - np.max(logits)
        exp_vals = np.exp(centered)
        return exp_vals / np.sum(exp_vals)

cognitive_model2 = make_cognitive_model(ParticipantModel2)