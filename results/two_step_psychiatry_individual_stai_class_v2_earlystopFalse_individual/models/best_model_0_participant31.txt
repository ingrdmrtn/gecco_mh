class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: High anxiety suppresses exploration and increases choice perseveration (stickiness).
    
    The inverse temperature 'beta' is scaled up by STAI: beta_eff = beta * (1 + stai).
    Higher beta means more exploitation/less randomness.
    
    Additionally, a choice stickiness bonus is added to the previously chosen action,
    scaled by STAI: stickiness_eff = stickiness * (1 + stai).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Base inverse temperature
    stickiness: [0, 5]  # Base choice stickiness bonus
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective beta
        beta_eff = self.beta * (1.0 + self.stai)
        
        # Calculate stickiness bonus
        stickiness_eff = self.stickiness * (1.0 + self.stai)
        
        # Add stickiness to Q-values for selection only (not for learning)
        q_modified = self.q_stage1.copy()
        if self.last_action1 is not None:
            q_modified[int(self.last_action1)] += stickiness_eff
            
        return self.softmax(q_modified, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Calculate effective beta
        beta_eff = self.beta * (1.0 + self.stai)
        
        # Note: Stickiness usually applies more strongly to the first stage choice in this task,
        # but we can apply a smaller effect or no effect here. 
        # For this model, we apply standard softmax at stage 2.
        return self.softmax(self.q_stage2[state], beta_eff)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)