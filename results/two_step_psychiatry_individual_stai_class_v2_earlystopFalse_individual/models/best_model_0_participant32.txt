class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates choice perseveration (stickiness).
    Higher anxiety leads to a stronger tendency to repeat the previous stage-1 action,
    regardless of the reward outcome. This reflects a 'safety behavior' or habit
    often seen in anxious individuals.
    
    The perseveration bonus is added to the Q-value of the previously chosen action.
    The magnitude of this bonus is determined by a base perseveration parameter (persev)
    scaled by the participant's STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    persev: [0, 5]  # Base perseveration strength
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.persev = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate perseveration bonus
        # Bonus is applied to the action taken in the previous trial
        q_modified = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            # Anxiety-modulated perseveration: 
            # Effective stickiness = base_persev * (1 + stai)
            # This implies higher anxiety amplifies the tendency to stick.
            stickiness = self.persev * (1.0 + self.stai)
            q_modified[int(self.last_action1)] += stickiness
            
        return self.softmax(q_modified, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning for value updates
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (Model-Free TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)