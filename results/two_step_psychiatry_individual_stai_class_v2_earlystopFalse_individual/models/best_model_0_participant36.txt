class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    Anxious individuals may prefer the familiar to avoid uncertainty. This model adds a 
    'stickiness' bonus to the previously chosen action at Stage 1. The magnitude of this 
    bonus is determined by a base parameter plus a component scaled by STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    pers_base: [0, 5]   # Base perseveration bonus
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_base = model_parameters

    def init_model(self) -> None:
        # Effective perseveration increases with STAI
        self.pers_effective = self.pers_base * (1.0 + self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Add stickiness bonus to Q-values before softmax
        q_modified = self.q_stage1.copy()
        
        if self.last_action1 is not None:
            # Add bonus to the index of the last action
            # Note: last_action1 is stored as float/int in base, cast to int for indexing
            idx = int(self.last_action1)
            q_modified[idx] += self.pers_effective
            
        return self.softmax(q_modified, self.beta)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)