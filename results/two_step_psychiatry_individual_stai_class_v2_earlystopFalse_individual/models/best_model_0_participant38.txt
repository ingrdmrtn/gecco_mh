class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety influences choice perseveration (stickiness).
    The participant has a tendency to repeat the previous Stage 1 action.
    The strength of this tendency (stickiness) is modulated by STAI.
    
    Q_net(a) = Q_stage1(a) + stickiness_bonus * I(a == last_action)
    
    Where stickiness_bonus = kappa * STAI.
    If kappa is positive, anxiety increases repetition (safety seeking).
    If kappa is negative, anxiety increases switching (restlessness).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    kappa: [-5, 5]      # Anxiety modulation of stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.kappa = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate stickiness bonus
        stickiness = self.kappa * self.stai
        
        # Create a vector of bonuses
        bonus_vec = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            bonus_vec[int(self.last_action1)] = stickiness
            
        # Add bonus to Q-values before softmax
        # Note: We are adding this inside the exponent effectively, 
        # or biasing the Q-values directly.
        augmented_values = self.q_stage1 + bonus_vec
        
        return self.softmax(augmented_values, self.beta)

    # Standard value update (TD learning)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)