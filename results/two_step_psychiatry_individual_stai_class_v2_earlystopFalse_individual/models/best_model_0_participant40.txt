class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: The participant uses a Win-Stay Lose-Shift strategy augmented by choice perseveration.
    Anxiety (STAI) modulates the perseveration parameter 'p'.
    Low anxiety is hypothesized to reduce perseveration (stickiness), allowing for more 
    flexible switching (lower 'p_eff').
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate for value tracking (used to define 'Win' vs 'Loss')
    beta: [0, 10]       # Inverse temperature
    p_base: [0, 5]      # Base perseveration bonus
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.p_base = model_parameters

    def init_model(self) -> None:
        # We track simple values to determine if a result was a "Win" or "Loss" relative to expectation
        self.q_values = np.zeros(self.n_choices) 
        self.prev_choice = -1

    def policy_stage1(self) -> np.ndarray:
        # Calculate perseveration bonus
        # Effective perseveration increases with anxiety. 
        # p_eff = p_base * (1 + stai)
        # Low anxiety -> p_eff is closer to p_base.
        p_eff = self.p_base * (1.0 + self.stai)
        
        logits = np.copy(self.q_values)
        
        # Add stickiness to the previous choice
        if self.prev_choice != -1:
            logits[self.prev_choice] += p_eff
            
        return self.softmax(logits, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Stage 2 is treated as random/exploration in this heuristic model 
        # or simple value tracking. We'll use simple value tracking from base class q_stage2
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 values (standard Q-learning)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Heuristic Update for Stage 1:
        # If Reward > Expectation (Win), increase value of chosen action.
        # If Reward < Expectation (Loss), decrease value.
        # We use the Stage 1 Q-value as the "Expectation".
        
        delta_1 = reward - self.q_values[action_1]
        
        # WSLS logic implemented via learning rate:
        # If delta is positive (Win), we reinforce.
        # If delta is negative (Loss), we devalue.
        self.q_values[action_1] += self.alpha * delta_1
        
        self.prev_choice = action_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)