class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: High anxiety (STAI) increases choice perseveration ('stickiness').
    The participant has a tendency to repeat the last stage-1 action, 
    modulated by their anxiety level.
    
    The model adds a 'stickiness' bonus to the Q-value of the previously chosen action.
    The magnitude of this bonus is determined by a base stickiness parameter plus 
    an anxiety-dependent component.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stickiness: [0, 5]  # Base stickiness magnitude
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stickiness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective stickiness based on anxiety
        # Higher STAI -> higher tendency to stick
        effective_stickiness = self.stickiness * (1.0 + self.stai)
        
        q_modified = self.q_stage1.copy()
        if self.last_action1 is not None:
            q_modified[self.last_action1] += effective_stickiness
            
        return self.softmax(q_modified, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Model-free update for stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)