class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Negative Learning Bias
    
    This model hypothesizes that anxiety increases sensitivity to negative outcomes 
    (omission of reward). The participant is modeled as a pure Model-Free learner, 
    but with asymmetric learning rates for positive (reward=1) and negative (reward=0) 
    prediction errors. The learning rate for negative outcomes is boosted by the 
    participant's STAI score.
    
    alpha_pos = alpha_base
    alpha_neg = alpha_base * (1 + stai)
    
    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]
    beta: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective learning rate based on outcome
        if reward > 0:
            alpha_eff = self.alpha_base
        else:
            # Boost learning from failure based on anxiety
            alpha_eff = self.alpha_base * (1.0 + self.stai)
            alpha_eff = min(alpha_eff, 1.0) # Cap at 1.0

        # Stage 2 Update (RPE)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha_eff * delta_2
        
        # Stage 1 Update (TD(1) / Direct reinforcement from stage 2 outcome)
        # Note: We use the updated stage 2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_eff * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)