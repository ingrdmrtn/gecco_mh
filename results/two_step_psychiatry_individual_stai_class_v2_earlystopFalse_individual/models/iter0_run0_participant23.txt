```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Hybrid Learning]
    This model hypothesizes that the participant uses a hybrid of Model-Based (MB) and Model-Free (MF) strategies.
    Crucially, it posits that higher anxiety (STAI) consumes cognitive resources, reducing the weight of the 
    computationally expensive Model-Based system.
    
    The net Q-value is a weighted sum: Q_net = w * Q_MB + (1-w) * Q_MF.
    The weighting parameter 'w' is modeled as: w = w_base - (w_anxiety * stai).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    w_base: [0, 1] (Base weight for Model-Based system)
    w_anxiety: [0, 2] (How strongly anxiety reduces MB weight)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_anxiety = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values
        # Q_MB(a) = Sum(T(a, s') * V(s')) where V(s') = max(Q_stage2(s', :))
        q_mb = np.zeros(self.n_choices)
        for action in range(self.n_choices):
            # Value of next states based on current stage 2 Q-values
            values_next_states = np.max(self.q_stage2, axis=1)
            # Expected value using transition matrix
            q_mb[action] = np.dot(self.T[action], values_next_states)

        # 2. Calculate Mixing Weight modulated by STAI
        # Higher anxiety reduces w_mb. Clipped to ensure [0, 1]
        w_mb = self.w_base - (self.w_anxiety * self.stai)
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # 3. Combine MB and MF values
        q_net = (w_mb * q_mb) + ((1 - w_mb) * self.q_stage1)

        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Perseveration]
    This model hypothesizes that anxiety increases "stickiness" or perseveration. 
    Anxious individuals may prefer repeating previous choices to avoid the uncertainty of switching 
    or to reduce decision-making load.
    
    A "perseveration bonus" is added to the Q-value of the previously chosen action.
    Bonus = p_base + (p_anxiety * stai).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    p_base: [0, 2] (Baseline perseveration)
    p_anxiety: [0, 5] (Additional perseveration due to anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.p_base, self.p_anxiety = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Apply perseveration bonus if a previous action exists
        if self.last_action1 is not None:
            bonus = self.p_base + (self.p_anxiety * self.stai)
            q_values[int(self.last_action1)] += bonus
            
        return self.softmax(q_values, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Punishment Sensitivity]
    This model hypothesizes that anxiety makes individuals hypersensitive to negative outcomes (omission of reward).
    When a reward is not received (reward = 0), the learning rate is effectively increased, causing 
    a faster downward adjustment of value.
    
    If Reward == 0: effective_alpha = alpha * (1 + sensitivity * stai).
    If Reward == 1: effective_alpha = alpha.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    punish_sens: [0, 10] (Multiplier for learning rate on loss, scaled by STAI)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.punish_sens = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective learning rate
        eff_alpha = self.alpha
        
        # If outcome is negative (no gold), anxiety amplifies learning
        if reward < 0.5:
            eff_alpha = self.alpha * (1.0 + self.punish_sens * self.stai)
            # Clip to ensure stability (learning rate shouldn't explode)
            eff_alpha = min(eff_alpha, 1.0)

        # Standard TD update with effective alpha
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += eff_alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += eff_alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```