```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Model-Based/Model-Free Hybrid]
    This model hypothesizes that the participant uses a hybrid of model-based (planning) 
    and model-free (habitual) strategies. Crucially, it posits that higher anxiety (STAI) 
    impairs model-based control, shifting the balance towards model-free learning.
    The parameter `w_factor` determines how strongly STAI reduces the model-based weight.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta: [0, 10] (Inverse temperature)
    w_factor: [0, 5] (Strength of anxiety-induced reduction in model-based control)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values (Q_MB)
        # V(state) = max_a Q_stage2(state, a)
        v_stage2 = np.max(self.q_stage2, axis=1)
        # Q_MB(action) = sum_s T(s|action) * V(s)
        # self.T is shape (2, 2) representing P(next_state | action)
        q_mb = self.T @ v_stage2

        # 2. Calculate Mixing Weight (w)
        # Hypothesis: Higher STAI reduces w (dominance of model-based system).
        # We assume a baseline w of 1.0 (fully MB) which is degraded by anxiety.
        w = np.clip(1.0 - (self.stai * self.w_factor), 0.0, 1.0)

        # 3. Combine with Model-Free Values (self.q_stage1)
        q_net = w * q_mb + (1 - w) * self.q_stage1

        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Negative Prediction Error Bias]
    This model hypothesizes that anxiety increases sensitivity to negative outcomes ("disappointments").
    When a prediction error is negative (outcome worse than expected), the learning rate is 
    amplified by the participant's STAI score. This leads to faster unlearning of actions 
    that fail to yield rewards.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Base learning rate)
    beta: [0, 10] (Inverse temperature)
    neg_bias: [0, 10] (Scaling factor for negative prediction errors based on STAI)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.neg_bias = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Calculate effective alpha for Stage 2
        alpha_2 = self.alpha
        if delta_2 < 0:
            # Amplify learning from negative errors based on anxiety
            alpha_2 = np.clip(self.alpha * (1.0 + self.stai * self.neg_bias), 0.0, 1.0)
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Calculate effective alpha for Stage 1
        alpha_1 = self.alpha
        if delta_1 < 0:
            # Amplify learning from negative errors based on anxiety
            alpha_1 = np.clip(self.alpha * (1.0 + self.stai * self.neg_bias), 0.0, 1.0)

        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Perseveration]
    This model hypothesizes that anxiety drives a "safety behavior" or perseveration strategy,
    where the participant prefers to repeat their previous Stage 1 choice to reduce decision conflict
    or uncertainty. The strength of this "stickiness" is directly modulated by the STAI score.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta: [0, 10] (Inverse temperature)
    persev_param: [0, 5] (Base perseveration strength, scaled by STAI)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.persev_param = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Start with learned Q-values
        logits = self.q_stage1.copy()
        
        # Add perseveration bonus if a previous action exists
        if self.last_action1 is not None:
            # The bonus is proportional to anxiety (STAI) * parameter
            bonus = self.stai * self.persev_param
            logits[int(self.last_action1)] += bonus
            
        return self.softmax(logits, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```