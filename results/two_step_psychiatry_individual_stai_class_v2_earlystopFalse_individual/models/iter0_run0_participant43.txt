```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Hybrid Control
    
    This model hypothesizes that the participant uses a mixture of Model-Based (MB) and 
    Model-Free (MF) strategies. Crucially, the balance between these strategies is 
    modulated by anxiety (STAI). Higher anxiety is hypothesized to disrupt cognitive 
    resources required for model-based planning, leading to a greater reliance on 
    model-free (habitual) control.
    
    The mixing weight `w` (for MB control) is calculated as:
    w = w_scale * (1 - stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    w_scale: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_scale = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values (Q_MB)
        # Q_MB(a) = Sum_s' [ T(s'|a) * Max_a' Q_stage2(s', a') ]
        # self.T is shape (2, 2) -> (action, state)
        # self.q_stage2 is shape (2, 2) -> (state, action)
        
        # Max value of each state in stage 2
        v_stage2 = np.max(self.q_stage2, axis=1) # Shape (2,)
        
        # Expected value for each stage 1 action based on transition matrix
        q_mb = np.dot(self.T, v_stage2) # Shape (2,)
        
        # 2. Calculate Mixing Weight modulated by STAI
        # Higher STAI -> Lower w (Less Model-Based)
        w = self.w_scale * (1.0 - self.stai)
        w = np.clip(w, 0.0, 1.0)
        
        # 3. Combine Values
        # self.q_stage1 acts as the Model-Free value (Q_MF)
        q_net = w * q_mb + (1.0 - w) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Negative Learning Bias
    
    This model hypothesizes that anxiety increases sensitivity to negative outcomes 
    (omission of reward). The participant is modeled as a pure Model-Free learner, 
    but with asymmetric learning rates for positive (reward=1) and negative (reward=0) 
    prediction errors. The learning rate for negative outcomes is boosted by the 
    participant's STAI score.
    
    alpha_pos = alpha_base
    alpha_neg = alpha_base * (1 + stai)
    
    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]
    beta: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective learning rate based on outcome
        if reward > 0:
            alpha_eff = self.alpha_base
        else:
            # Boost learning from failure based on anxiety
            alpha_eff = self.alpha_base * (1.0 + self.stai)
            alpha_eff = min(alpha_eff, 1.0) # Cap at 1.0

        # Stage 2 Update (RPE)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha_eff * delta_2
        
        # Stage 1 Update (TD(1) / Direct reinforcement from stage 2 outcome)
        # Note: We use the updated stage 2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_eff * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Induced Perseveration
    
    This model hypothesizes that anxiety leads to rigid, repetitive behavior ("stickiness" 
    or perseveration) as a safety strategy. The participant is more likely to repeat 
    their previous stage-1 choice regardless of the outcome. The strength of this 
    perseveration bias is directly scaled by the STAI score.
    
    Q_net(a) = Q_TD(a) + (pers_weight * stai) * I(a == last_action)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    pers_weight: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_weight = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_vals = self.q_stage1.copy()
        
        # Add perseveration bonus if there was a previous action
        if self.last_action1 is not None:
            bonus = self.pers_weight * self.stai
            q_vals[int(self.last_action1)] += bonus
            
        return self.softmax(q_vals, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```