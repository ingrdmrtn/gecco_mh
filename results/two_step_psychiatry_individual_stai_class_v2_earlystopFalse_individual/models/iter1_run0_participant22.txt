Here are three new cognitive models exploring different mechanisms for how anxiety (STAI) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model tests the hypothesis that anxiety affects the balance between Model-Based (planning using transition probabilities) and Model-Free (habitual) control. Medium-to-high anxiety might impair cognitive resources required for model-based planning, leading to a greater reliance on model-free strategies.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: This model implements a hybrid Model-Based (MB) and Model-Free (MF) reinforcement learning agent.
    The weighting parameter 'w' determines the balance between these two systems.
    We hypothesize that anxiety (STAI) modulates this balance: higher anxiety reduces model-based planning 
    (lower w), pushing the participant towards model-free habits.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_base: [0, 1]     # Base mixing weight (0=MF, 1=MB)
    w_stai_slope: [-1, 1] # Effect of STAI on w (negative implies anxiety reduces MB)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_stai_slope = model_parameters

    def init_model(self) -> None:
        # Calculate the effective mixing weight 'w' based on STAI
        # We clip it to ensure it stays within [0, 1]
        raw_w = self.w_base + (self.w_stai_slope * self.stai)
        self.w = np.clip(raw_w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Model-Free Q-values are in self.q_stage1
        
        # Calculate Model-Based Q-values
        # Q_MB(s1, a) = sum(T(s1, a, s2) * max(Q_stage2(s2, :)))
        # Here, T is static (self.T). 
        # self.T shape is (2, 2) -> [action, next_state]
        # self.q_stage2 shape is (2, 2) -> [state, action]
        
        max_q2 = np.max(self.q_stage2, axis=1) # Max value of next states
        q_mb = np.zeros(self.n_choices)
        
        # For action 0 (Spaceship A)
        q_mb[0] = self.T[0, 0] * max_q2[0] + self.T[0, 1] * max_q2[1]
        # For action 1 (Spaceship U)
        q_mb[1] = self.T[1, 0] * max_q2[0] + self.T[1, 1] * max_q2[1]
        
        # Combined Q-values
        q_net = (1 - self.w) * self.q_stage1 + self.w * q_mb
        
        return self.softmax(q_net, self.beta)

    # Use standard value update for MF values (q_stage1 and q_stage2)
    # The base class implementation does TD(0) for stage 1, which is standard for the MF part of hybrid models
    # However, hybrid models usually update stage 1 MF values using lambda=1 (eligibility traces) or just direct reward.
    # For simplicity in this constrained template, we stick to the base TD update which approximates MF learning.

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that anxiety modulates how the participant processes positive versus negative prediction errors. Specifically, anxiety might increase sensitivity to "losses" (or lack of reward), making the learning rate for negative prediction errors higher than for positive ones.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases sensitivity to negative outcomes (loss aversion in learning).
    This model uses separate learning rates for positive and negative prediction errors.
    The 'alpha_neg' is modulated by STAI: higher anxiety leads to faster learning from 
    disappointments (0 reward), potentially causing rapid switching behavior.
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]     # Learning rate for positive RPEs
    alpha_neg_base: [0, 1]# Base learning rate for negative RPEs
    stai_sens: [0, 5]     # Multiplier for STAI effect on negative learning rate
    beta: [0, 10]         # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.stai_sens, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate
        alpha_neg_eff = self.alpha_neg_base + (self.stai_sens * self.stai)
        alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)
        
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 >= 0 else alpha_neg_eff
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Note: Base class uses TD(0): delta_1 = Q2 - Q1. 
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else alpha_neg_eff
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration/Exploitation (Inverse Temperature Modulation)
This model suggests that anxiety directly impacts decision noise (exploration vs. exploitation). High anxiety might lead to "noise" or inconsistent choices (low beta), or conversely, rigid, deterministic behavior (high beta). This model allows the inverse temperature parameter $\beta$ to be a function of the STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the exploration-exploitation trade-off.
    Instead of a fixed beta, the inverse temperature is a function of STAI.
    This tests if the participant becomes more random (anxious confusion) or 
    more deterministic (anxious rigidity) based on their anxiety level.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_base: [0, 10]  # Base inverse temperature
    beta_stai: [-5, 5]  # Effect of STAI on beta. Positive = rigidity, Negative = noise.
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.beta_stai = model_parameters

    def init_model(self) -> None:
        # Calculate effective beta once, as STAI is constant per participant
        raw_beta = self.beta_base + (self.beta_stai * self.stai)
        # Ensure beta doesn't go negative (which would invert preferences)
        self.effective_beta = np.maximum(raw_beta, 0.0)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.effective_beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.effective_beta)

    # Standard value update from base class is sufficient
    # as we are only modifying the selection policy.

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```