Here are three new cognitive models that hypothesize different ways anxiety (STAI) modulates decision-making in the two-step task.

### Model 1: Anxiety-Modulated Learning Rate
```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Learning Rate]
    This model hypothesizes that anxiety affects the rate at which the participant 
    updates their value expectations (learning rate alpha). 
    
    High anxiety might lead to "hyper-learning" (high alpha, chasing recent outcomes) 
    or rigidity (low alpha, ignoring new information).
    
    alpha = alpha_base + (alpha_anx * stai)
    
    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]
    alpha_anx: [-1, 1]
    beta: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.alpha_anx, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate anxiety-modulated learning rate
        # Clipped to ensure it remains a valid probability [0, 1]
        alpha = np.clip(self.alpha_base + (self.alpha_anx * self.stai), 0.0, 1.0)
        
        # Stage 2 Update (TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        # Stage 1 Update (TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Exploration (Temperature)
```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Exploration]
    This model hypothesizes that anxiety affects the exploration-exploitation trade-off.
    Anxiety modulates the inverse temperature parameter (beta) of the softmax function.
    
    Positive beta_anx implies anxiety reduces exploration (tunnel vision/freezing).
    Negative beta_anx implies anxiety increases erratic behavior.
    
    beta = beta_base + (beta_anx * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta_base: [0, 10]
    beta_anx: [-5, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.beta_anx = model_parameters

    def get_modulated_beta(self) -> float:
        return np.clip(self.beta_base + (self.beta_anx * self.stai), 0.0, 10.0)

    def policy_stage1(self) -> np.ndarray:
        beta = self.get_modulated_beta()
        return self.softmax(self.q_stage1, beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        beta = self.get_modulated_beta()
        return self.softmax(self.q_stage2[state], beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Structural Learning (Hybrid)
```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Structural Learning]
    This model hypothesizes that anxiety affects how quickly the participant updates 
    their internal model of the environment's structure (the transition matrix T).
    
    The agent uses a Hybrid strategy (50% Model-Based, 50% Model-Free).
    The transition learning rate (lr_T) is modulated by anxiety.
    
    lr_T = lr_T_base + (lr_T_anx * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    lr_T_base: [0, 1]
    lr_T_anx: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lr_T_base, self.lr_T_anx = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based Values
        # V(state) = max Q_stage2(state, :)
        v_stage2 = np.max(self.q_stage2, axis=1) 
        
        # Q_MB(action) = sum(T[action, state] * V(state))
        q_mb = self.T @ v_stage2
        
        # 2. Combine with Model-Free Values (Fixed 50/50 weighting)
        q_net = 0.5 * self.q_stage1 + 0.5 * q_mb
        
        return self.softmax(q_net, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard value update for Q-values
        super().post_trial(action_1, state, action_2, reward)
        
        # Anxiety-modulated update of the Transition Matrix
        lr_T = np.clip(self.lr_T_base + (self.lr_T_anx * self.stai), 0.0, 1.0)
        
        # Create target vector (1 for observed state, 0 for other)
        target = np.zeros(self.n_states)
        target[state] = 1.0
        
        # Delta rule update for transition probabilities: T_new = T_old + lr * (Target - T_old)
        self.T[action_1] += lr_T * (target - self.T[action_1])

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```