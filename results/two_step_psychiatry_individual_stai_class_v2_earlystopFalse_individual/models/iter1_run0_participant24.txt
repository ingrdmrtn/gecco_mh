```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates the exploration-exploitation trade-off (inverse temperature beta).
    
    This model posits that anxiety levels directly influence the stochasticity of choices.
    High anxiety might lead to more random behavior (low beta, high noise) due to 
    cognitive interference, or more rigid behavior (high beta) due to safety behaviors.
    
    beta_effective = beta_base + (beta_stai * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta_base: [0, 10]
    beta_stai: [-10, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.beta_stai = model_parameters

    def init_model(self) -> None:
        # Calculate effective beta based on STAI
        # We clip to ensure beta remains non-negative
        self.beta = np.maximum(0.0, self.beta_base + (self.beta_stai * self.stai))

    # We do not need to override policy methods because the base class 
    # uses self.beta, which we have now modulated in init_model.

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates memory decay (forgetting) of learned values.
    
    Anxiety consumes cognitive resources (Attentional Control Theory), potentially 
    leading to faster decay of value representations in working memory.
    On each trial, Q-values decay towards a neutral prior (0.5).
    
    decay_rate = decay_base + (decay_stai * stai)
    Q(t) = Q(t-1) * (1 - decay_rate) + 0.5 * decay_rate
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    decay_base: [0, 1]
    decay_stai: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_base, self.decay_stai = model_parameters

    def init_model(self) -> None:
        # Calculate effective decay rate and clip to [0, 1]
        self.decay_eff = np.clip(self.decay_base + (self.decay_stai * self.stai), 0.0, 1.0)

    def pre_trial(self) -> None:
        """Apply forgetting to Q-values before the trial begins."""
        if self.decay_eff > 0:
            # Decay Stage 1 values
            self.q_stage1 = self.q_stage1 * (1 - self.decay_eff) + 0.5 * self.decay_eff
            # Decay Stage 2 values
            self.q_stage2 = self.q_stage2 * (1 - self.decay_eff) + 0.5 * self.decay_eff

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates the eligibility trace (credit assignment) for the first-stage choice.
    
    This model tests if anxiety affects how the first-stage choice is updated.
    A parameter lambda controls the mixture between updating based on the 
    second-stage value (TD-0 / chaining) vs. the final reward (Monte Carlo / TD-1).
    
    Target = lambda * Reward + (1 - lambda) * Q_stage2
    
    High anxiety might shift reliance towards immediate outcomes (Reward) or 
    predictive values (Q_stage2).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    lambda_base: [0, 1]
    lambda_stai: [-1, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_base, self.lambda_stai = model_parameters

    def init_model(self) -> None:
        # Calculate effective lambda and clip to [0, 1]
        self.lambda_eff = np.clip(self.lambda_base + (self.lambda_stai * self.stai), 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 value (Standard TD)
        # Q2(s, a2) <- Q2(s, a2) + alpha * (R - Q2(s, a2))
        q2_val_before = self.q_stage2[state, action_2] # Store for TD(0) part
        delta_2 = reward - q2_val_before
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Update Stage 1 value using the eligibility trace mixture
        # Target = lambda * Reward + (1 - lambda) * Q2_before_update
        # Note: In pure TD(lambda), we often use the current estimate, but here we define
        # the target as a mixture of the raw reward and the stored value of the next state.
        
        target = (self.lambda_eff * reward) + ((1 - self.lambda_eff) * q2_val_before)
        
        delta_1 = target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```