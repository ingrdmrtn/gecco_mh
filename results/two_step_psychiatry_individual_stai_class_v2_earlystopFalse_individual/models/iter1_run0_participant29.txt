Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this task.

### Model 1: Anxiety-Modulated Rigidity
```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Rigidity (Exploration Suppression)
    
    This model hypothesizes that high anxiety acts as a stressor that reduces 
    exploratory behavior, leading to "rigid" or overly deterministic choices. 
    Instead of a fixed softmax temperature, the inverse temperature (beta) 
    increases with the participant's anxiety level.
    
    beta_effective = beta_base + (stiffness * stai)
    
    A higher beta means the participant is more likely to strictly choose the 
    option with the highest value, explaining long streaks of repeating the 
    same choice (low noise).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta_base: [0, 10]
    stiffness: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stiffness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective beta based on anxiety
        beta_eff = self.beta_base + (self.stiffness * self.stai)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Apply the same rigidity to stage 2
        beta_eff = self.beta_base + (self.stiffness * self.stai)
        return self.softmax(self.q_stage2[state], beta_eff)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Loss Denial
```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Loss Denial (Asymmetric Learning)
    
    This model hypothesizes that anxious participants exhibit a specific bias 
    against learning from negative outcomes (losses). To maintain a sense of 
    control or avoid the cognitive load of re-evaluating their strategy, 
    they "deny" or under-weight instances where they receive 0 coins.
    
    The learning rate for losses is suppressed by the STAI score:
    alpha_loss = alpha * (1 - (denial_factor * stai))
    
    This explains why the participant might persist with a choice (like Spaceship 1)
    despite receiving multiple 0 outcomes.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    denial_factor: [0, 2]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.denial_factor = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective learning rate
        eff_alpha = self.alpha
        
        # If the outcome was a loss (0 coins), suppress the learning rate
        if reward < 0.5:
            # Calculate suppression multiplier (clamped to 0 to prevent negative alpha)
            suppression = max(0.0, 1.0 - (self.denial_factor * self.stai))
            eff_alpha = self.alpha * suppression
            
        # Update Stage 2 values
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += eff_alpha * delta_2
        
        # Update Stage 1 values
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += eff_alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Safety Seeking
```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Safety Seeking (Intrinsic Reward)
    
    This model hypothesizes that anxious individuals derive intrinsic value 
    from "safe" (predictable) transitions, regardless of the monetary reward. 
    Common transitions (Spaceship A->Planet X, Spaceship U->Planet Y) are 
    perceived as safe.
    
    The participant adds an intrinsic "safety bonus" to the value of the 
    Stage 1 choice if the transition was common.
    
    Bonus = safety_val * stai
    
    This reinforces the choice of a spaceship that reliably leads to its 
    expected planet, even if the aliens on that planet are not currently 
    giving gold.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    safety_val: [0, 2]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.safety_val = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 (Alien) purely based on external reward (coins)
        # The alien itself isn't "safe", the transition to the planet is.
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Determine if transition was common
        # Based on trans_counts [[35, 15], [15, 35]], 0->0 and 1->1 are common.
        is_common = (action_1 == state)
        
        # Calculate intrinsic safety bonus
        bonus = (self.safety_val * self.stai) if is_common else 0.0
        
        # Update Stage 1 (Spaceship)
        # The value of the spaceship is the value of the destination + safety bonus
        delta_1 = (self.q_stage2[state, action_2] + bonus) - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```