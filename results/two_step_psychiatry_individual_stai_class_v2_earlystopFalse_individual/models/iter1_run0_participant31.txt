Here are three new cognitive models exploring different mechanisms for how high anxiety (STAI = 0.575) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model tests the hypothesis that high anxiety impairs model-based planning (using the transition structure) and favors model-free learning (habitual caching of values). The mixing weight `w` determines the balance, and STAI is hypothesized to reduce `w` (pushing it towards 0, i.e., model-free).

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: High anxiety impairs model-based control, shifting the participant towards 
    model-free strategies.
    
    The parameter 'w' controls the balance between Model-Based (MB) and Model-Free (MF) systems.
    w = 1 implies pure MB, w = 0 implies pure MF.
    
    We model the effective mixing weight as: w_eff = w_base * (1 - stai).
    Since this participant has high anxiety (0.575), this will significantly dampen the 
    model-based contribution compared to a low anxiety individual.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base mixing weight (before anxiety modulation)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values (Q_MB)
        # Q_MB depends on the transition matrix T and stage 2 values
        self.q_mb = np.zeros(self.n_choices)

    def pre_trial(self) -> None:
        # Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        # We assume the participant knows the transition matrix T (fixed or learned, here fixed prior)
        for a in range(self.n_choices):
            expected_val = 0
            for s in range(self.n_states):
                # Max Q-value at stage 2 for state s
                max_q2 = np.max(self.q_stage2[s])
                expected_val += self.T[a, s] * max_q2
            self.q_mb[a] = expected_val

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective mixing weight modulated by anxiety
        # High anxiety reduces w_eff, making it more model-free
        w_eff = self.w_base * (1.0 - self.stai)
        
        # Clip to ensure valid range [0, 1]
        w_eff = np.clip(w_eff, 0, 1)
        
        # Combine Model-Free (q_stage1) and Model-Based (q_mb) values
        q_net = w_eff * self.q_mb + (1 - w_eff) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning for Model-Free values
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (Model-Free)
        # TD(1) style: update Q1 based on Q2
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that high anxiety makes participants more sensitive to negative outcomes (or lack of reward) than positive ones. Instead of a single learning rate, we use separate learning rates for positive and negative prediction errors, where the negative learning rate is amplified by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: High anxiety increases sensitivity to negative prediction errors (loss aversion).
    
    We use two learning rates: alpha_pos for positive prediction errors and alpha_neg for negative ones.
    The base negative learning rate is modulated by STAI:
    alpha_neg_eff = alpha_neg_base + (stai * 0.5).
    
    This implies that anxious individuals update their value estimates more drastically 
    after disappointments (0 reward) than after successes.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]       # Learning rate for positive PE
    alpha_neg_base: [0, 1]  # Base learning rate for negative PE
    beta: [0, 10]           # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate
        # We scale it such that high anxiety increases the rate, capped at 1.0
        alpha_neg_eff = np.clip(self.alpha_neg_base + (self.stai * 0.5), 0, 1)
        
        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        
        if delta_2 >= 0:
            alpha_2 = self.alpha_pos
        else:
            alpha_2 = alpha_neg_eff
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # --- Stage 1 Update ---
        # Using the updated Q2 value for the TD error
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 >= 0:
            alpha_1 = self.alpha_pos
        else:
            alpha_1 = alpha_neg_eff
            
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven "Win-Stay, Lose-Shift" Heuristic
This model suggests that under high anxiety, complex value learning is abandoned in favor of a simpler heuristic strategy at Stage 1: "Win-Stay, Lose-Shift" (WSLS). The STAI score determines the probability of relying on this heuristic versus the learned Q-values.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: High anxiety promotes reliance on simple reactive heuristics like 
    Win-Stay, Lose-Shift (WSLS) over value-based learning at the first stage.
    
    The model calculates a mixture of a Q-learning policy and a WSLS policy.
    The weight of the WSLS policy (phi) is determined by STAI:
    phi = phi_base * stai.
    
    If the previous trial was rewarded, the WSLS policy assigns probability 1 to the same action.
    If unrewarded, it assigns probability 1 to the other action.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature for Q-learning component
    phi_base: [0, 1]    # Base weight for the heuristic component
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.phi_base = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute standard Softmax Policy based on Q-values
        policy_q = self.softmax(self.q_stage1, self.beta)
        
        # 2. Compute WSLS Policy
        policy_wsls = np.ones(self.n_choices) / self.n_choices # Default uniform if no history
        
        if self.last_action1 is not None and self.last_reward is not None:
            prev_a = int(self.last_action1)
            policy_wsls = np.zeros(self.n_choices)
            
            if self.last_reward > 0:
                # Win-Stay
                policy_wsls[prev_a] = 1.0
            else:
                # Lose-Shift
                policy_wsls[1 - prev_a] = 1.0
        
        # 3. Mix policies based on anxiety
        # Higher anxiety -> higher phi -> more reliance on WSLS
        phi = np.clip(self.phi_base * self.stai, 0, 1)
        
        final_policy = (1 - phi) * policy_q + phi * policy_wsls
        
        # Ensure numerical stability
        final_policy = np.clip(final_policy, 1e-10, 1.0)
        final_policy /= np.sum(final_policy)
        
        return final_policy

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning update is still maintained for the Q-component
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```