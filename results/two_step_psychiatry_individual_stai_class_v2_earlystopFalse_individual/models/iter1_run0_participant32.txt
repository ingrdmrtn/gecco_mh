Here are three new cognitive models exploring different mechanisms for how anxiety (STAI) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model tests the hypothesis that anxiety shifts the balance between goal-directed (model-based) and habitual (model-free) control. Specifically, higher anxiety might impair the computationally expensive model-based system, leading to a greater reliance on model-free learning.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety (STAI) reduces the weight (w) of the model-based system, leading to more
    habitual behavior. The mixing parameter 'w' determines the contribution of MB values
    to the final stage-1 choice.
    
    w_effective = w_base * (1 - stai)
    If w_effective is close to 1, behavior is purely Model-Based.
    If w_effective is close to 0, behavior is purely Model-Free.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    w_base: [0, 1]  # Base weight for model-based control
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize transition counts for Model-Based learning
        # We start with the prior counts given in the base class
        self.transition_counts = self.trans_counts.copy()

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Value (Q_MF)
        # This is simply self.q_stage1, updated via TD
        q_mf = self.q_stage1

        # 2. Model-Based Value (Q_MB)
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        # Calculate transition probabilities from counts
        row_sums = self.transition_counts.sum(axis=1, keepdims=True)
        trans_probs = self.transition_counts / np.maximum(row_sums, 1)
        
        # Max Q-value at stage 2 for each state
        max_q2 = np.max(self.q_stage2, axis=1)
        
        # Bellman equation for Model-Based values
        q_mb = np.dot(trans_probs, max_q2)

        # 3. Integrated Value
        # Anxiety reduces the contribution of the Model-Based system
        # We clamp the modulation to ensure w stays in [0, 1]
        w_eff = self.w_base * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0.0, 1.0)
        
        q_net = w_eff * q_mb + (1 - w_eff) * q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 values (common to both systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 Model-Free values (TD-1)
        # Note: In full MB/MF models, this is often SARSA(lambda), but simple TD is used here for parsimony
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Update Transition Model (Model-Based)
        # Increment count for the observed transition
        self.transition_counts[action_1, state] += 1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that anxiety increases sensitivity to negative outcomes (or lack of reward). In this task, receiving 0 coins is the "loss" condition. Anxious individuals might update their value estimates more drastically after a failure (0 coins) than after a success (1 coin), reflecting a negativity bias.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning rates asymmetrically (Loss Aversion / Negativity Bias).
    Anxious individuals learn more from negative outcomes (0 reward) than positive ones (1 reward).
    
    We define two learning rates:
    alpha_pos: Learning rate for rewards (1)
    alpha_neg: Learning rate for non-rewards (0)
    
    The 'alpha_neg' is amplified by the STAI score:
    alpha_neg_effective = alpha_neg_base * (1 + stai)
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      # Learning rate for positive outcomes
    alpha_neg_base: [0, 1] # Base learning rate for negative outcomes
    beta: [0, 10]          # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective learning rate based on outcome
        if reward > 0.5: # Positive outcome
            alpha = self.alpha_pos
        else: # Negative outcome
            # Anxiety amplifies learning from negative prediction errors
            alpha = self.alpha_neg_base * (1.0 + self.stai)
            alpha = np.clip(alpha, 0.0, 1.0)

        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        # Stage 1 Update
        # We use the same alpha for consistency, though mechanisms could differ
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression
This model suggests that anxiety reduces exploration. Anxious individuals may prefer "safe" or known options and avoid uncertainty. This is modeled by modulating the inverse temperature parameter ($\beta$) of the softmax function. Higher anxiety leads to a higher $\beta$ (lower temperature), resulting in more deterministic (exploitative) choices and less random exploration.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety suppresses exploration (increases exploitation).
    Higher anxiety leads to more deterministic choices, effectively increasing the 
    inverse temperature (beta) of the softmax function.
    
    beta_effective = beta_base * (1 + rho * stai)
    
    Where 'rho' is a sensitivity parameter scaling how much anxiety tightens the decision policy.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]     # Learning rate
    beta_base: [0, 10]# Base inverse temperature
    rho: [0, 5]       # Anxiety sensitivity scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.rho = model_parameters

    def get_effective_beta(self) -> float:
        # Calculate anxiety-modulated beta
        # Higher STAI -> Higher Beta -> More deterministic (less exploration)
        return self.beta_base * (1.0 + self.rho * self.stai)

    def policy_stage1(self) -> np.ndarray:
        beta_eff = self.get_effective_beta()
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        beta_eff = self.get_effective_beta()
        return self.softmax(self.q_stage2[state], beta_eff)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```