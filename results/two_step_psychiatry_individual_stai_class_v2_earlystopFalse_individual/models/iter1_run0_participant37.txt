Here are three cognitive models that hypothesize different mechanisms for how anxiety (STAI) modulates decision-making in this task.

### Model 1: Anxiety-Modulated Memory Decay
```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety accelerates memory decay (forgetting).
    High anxiety participants may struggle to maintain stable value representations 
    over time due to cognitive interference or worry.
    
    We model this by decaying Q-values towards a neutral prior (0.5) before every trial.
    The rate of decay is proportional to the participant's STAI score.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    decay_max: [0, 1]   # Maximum decay rate scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_max = model_parameters

    def init_model(self) -> None:
        # Calculate participant-specific decay rate based on anxiety
        # If STAI is high, decay is stronger.
        self.decay = self.decay_max * self.stai

    def pre_trial(self) -> None:
        # Decay values towards 0.5 (neutral expectation)
        # Formula: Q_new = Q_old * (1 - decay) + 0.5 * decay
        self.q_stage1 = self.q_stage1 * (1.0 - self.decay) + 0.5 * self.decay
        self.q_stage2 = self.q_stage2 * (1.0 - self.decay) + 0.5 * self.decay

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Choice Stickiness
```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases choice perseveration (stickiness).
    High anxiety participants may exhibit "safety behavior" by repeating 
    their previous Stage 1 choice to avoid the cognitive load of decision-making 
    or the perceived risk of switching, regardless of the actual value.
    
    We add a 'stickiness' bonus to the Q-value of the previously chosen action.
    The magnitude of this bonus is modulated by the STAI score.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stick_max: [0, 5]   # Maximum stickiness bonus scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_max = model_parameters

    def init_model(self) -> None:
        # Stickiness magnitude increases with anxiety
        self.stickiness = self.stick_max * self.stai

    def policy_stage1(self) -> np.ndarray:
        # Create a copy of values to modify with stickiness
        q_mod = self.q_stage1.copy()
        
        # Add bonus to the last chosen action if it exists
        if self.last_action1 is not None:
            q_mod[int(self.last_action1)] += self.stickiness
            
        return self.softmax(q_mod, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Eligibility Trace (TD-Lambda)
```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the eligibility trace (lambda) in Model-Free learning.
    
    We hypothesize that anxiety affects how credit is assigned to the first-stage choice.
    A parameter 'lambda' controls the mix between TD(0) (chaining values via state structure) 
    and TD(1) (learning directly from the final reward).
    
    High anxiety (high STAI) leads to more reactive processing (higher lambda), 
    where the participant ignores the intermediate state structure and associates 
    the first choice directly with the outcome.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta: [0, 10]        # Inverse temperature
    lambda_scale: [0, 1] # Scales how strongly STAI induces TD(1) behavior
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_scale = model_parameters

    def init_model(self) -> None:
        # Lambda is modulated by anxiety. 
        # Low anxiety -> Lambda near 0 (TD(0), uses structure).
        # High anxiety -> Lambda increases (TD(1), ignores structure).
        self.lambda_val = self.lambda_scale * self.stai

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (Standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD(lambda) mixture)
        # We construct a target that is a mix of the next state value (TD-0) 
        # and the actual reward (TD-1).
        
        # Value of the state we landed in (TD(0) target)
        v_next_state = self.q_stage2[state, action_2] 
        
        # Mixed target: (1-lambda)*V(s') + lambda*R
        target = (1 - self.lambda_val) * v_next_state + self.lambda_val * reward
        
        delta_1 = target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```