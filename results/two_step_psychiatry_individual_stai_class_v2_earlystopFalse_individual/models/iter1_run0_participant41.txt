Here are three new cognitive models that incorporate the participant's STAI (anxiety) score to explain their decision-making behavior.

### Model 1: Anxiety-Modulated Memory Decay
This model hypothesizes that anxiety leads to **attentional narrowing**, causing the participant to faster forget the value of options they are not currently choosing. While the chosen option is updated via reinforcement learning, the unchosen options decay in value proportional to the participant's anxiety level.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety accelerates memory decay for unchosen options.
    High anxiety leads to 'attentional narrowing' or resource depletion, causing 
    values of unchosen actions to decay towards zero faster than in low anxiety.
    
    decay_rate = lambda_param * STAI
    Q_unchosen = Q_unchosen * (1 - decay_rate)
    
    This occurs alongside standard TD learning for the chosen option.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate for chosen options
    beta: [0, 10]        # Inverse temperature
    lambda_param: [0, 1] # Scaling factor for anxiety-induced decay
    """
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_param = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate anxiety-dependent decay rate
        decay = self.lambda_param * self.stai
        decay = np.clip(decay, 0, 1)
        
        # --- Stage 2 Update ---
        # Standard TD update for the chosen action
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Decay for the unchosen action in the current state
        unchosen_a2 = 1 - action_2
        self.q_stage2[state, unchosen_a2] *= (1.0 - decay)
        
        # --- Stage 1 Update ---
        # Standard TD update for the chosen action
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Decay for the unchosen action
        unchosen_a1 = 1 - action_1
        self.q_stage1[unchosen_a1] *= (1.0 - decay)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Punishment Sensitivity
This model hypothesizes that anxiety creates a **negativity bias**. Anxious individuals are often hypersensitive to negative outcomes (omission of reward). In this model, the learning rate is boosted specifically when the reward is 0, with the magnitude of this boost determined by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases Punishment Sensitivity.
    The learning rate is dynamically boosted when the outcome is negative (reward = 0),
    reflecting a 'negativity bias' or hypersensitivity to failure common in anxiety.
    
    If reward == 0:
        alpha_effective = alpha * (1 + rho * STAI)
    Else:
        alpha_effective = alpha
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Base learning rate (for rewards)
    beta: [0, 10]   # Inverse temperature
    rho: [0, 10]    # Anxiety-dependent multiplier for punishment learning
    """
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rho = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine effective alpha based on outcome and anxiety
        if reward == 0:
            # Boost learning rate for losses based on anxiety
            eff_alpha = self.alpha * (1.0 + self.rho * self.stai)
            eff_alpha = np.clip(eff_alpha, 0, 1) # Ensure it stays within bounds
        else:
            eff_alpha = self.alpha
            
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += eff_alpha * delta_2
        
        # Stage 1 Update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += eff_alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Rigidity (Inverse Temperature)
This model hypothesizes that anxiety modulates the **exploration-exploitation trade-off**. High anxiety is often associated with rigid, safety-seeking behavior and reduced exploration. Here, the STAI score scales the inverse temperature ($\beta$), making choices more deterministic (rigid) as anxiety increases.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates the exploration-exploitation trade-off.
    High anxiety leads to more rigid, exploitative behavior (higher beta) and 
    reduced stochasticity/exploration.
    
    beta_effective = beta_base * (1 + gamma * STAI)
    
    This captures the 'freezing' or rigid avoidance behavior often seen in high anxiety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta_base: [0, 10] # Baseline inverse temperature
    gamma: [0, 5]      # Anxiety scaling factor for rigidity
    """
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.gamma = model_parameters

    def get_effective_beta(self) -> float:
        # Scale beta by anxiety
        return self.beta_base * (1.0 + self.gamma * self.stai)

    def policy_stage1(self) -> np.ndarray:
        # Use effective beta for stage 1 choices
        return self.softmax(self.q_stage1, self.get_effective_beta())

    def policy_stage2(self, state: int) -> np.ndarray:
        # Use effective beta for stage 2 choices
        return self.softmax(self.q_stage2[state], self.get_effective_beta())

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```