```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates the 'eligibility trace' (lambda) in reinforcement learning.
    High anxiety impairs the cognitive ability to link distal outcomes (rewards) to 
    earlier choices (Stage 1), effectively reducing the eligibility trace parameter lambda.
    
    This model implements a TD(lambda) update where lambda is inversely proportional to STAI.
    A lower lambda means the participant relies more on the immediate value of the 
    transition (Stage 2 state value) rather than the final reward when updating Stage 1.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta: [0, 10]        # Inverse temperature
    lambda_base: [0, 1]  # Base eligibility trace parameter
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_base = model_parameters

    def init_model(self) -> None:
        # Calculate effective lambda based on anxiety
        # Higher STAI -> Lower lambda (more TD(0)-like, less credit assignment to stage 1)
        # We clip to ensure it stays non-negative
        self.lambda_eff = max(0.0, self.lambda_base * (1.0 - self.stai))

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate Stage 2 Prediction Error (RPE)
        # Note: We calculate this before updating Q2 to capture the surprise
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Update Stage 2 Value
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 Value
        # Part 1: Standard TD(0) update (driven by Stage 2 value)
        # We use the updated Q2 here as per the base class convention
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Part 2: Eligibility Trace update (driven by Stage 2 RPE)
        # This propagates the reward surprise directly to Stage 1, scaled by lambda
        self.q_stage1[action_1] += self.alpha * self.lambda_eff * delta_2

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases behavioral rigidity and determinism (Exploitation).
    High anxiety leads to a higher inverse temperature (beta), causing the participant 
    to choose the option with the slightly higher value with much higher probability, 
    reducing exploration.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta_base: [0, 10]      # Base inverse temperature
    stai_scaling: [0, 5]    # How strongly STAI amplifies beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stai_scaling = model_parameters

    def init_model(self) -> None:
        # Calculate effective beta
        # High anxiety amplifies beta, making choices more deterministic (sticky to best value)
        self.beta_eff = self.beta_base * (1.0 + self.stai_scaling * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_eff)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety leads to 'freezing' or reduced belief updating.
    High anxiety dampens the learning rate (alpha), causing the participant to 
    update their value estimates more slowly in response to feedback. This results 
    in behavior that looks unresponsive to recent outcomes.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]      # Base learning rate
    beta: [0, 10]           # Inverse temperature
    dampening: [0, 1]       # Factor by which STAI reduces alpha
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.dampening = model_parameters

    def init_model(self) -> None:
        # Calculate effective alpha
        # Higher STAI -> Lower alpha (slower learning)
        self.alpha_eff = self.alpha_base * (1.0 - self.dampening * self.stai)
        # Ensure alpha doesn't go negative
        self.alpha_eff = max(0.0, self.alpha_eff)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Use the anxiety-dampened learning rate
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha_eff * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha_eff * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```