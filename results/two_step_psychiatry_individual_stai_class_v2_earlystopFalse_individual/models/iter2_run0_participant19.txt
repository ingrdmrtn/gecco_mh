```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Heuristic Reliance (Win-Stay, Lose-Shift)]
    This model hypothesizes that anxiety reduces cognitive capacity, leading participants 
    to rely more on simple heuristic strategies like "Win-Stay, Lose-Shift" (WSLS) 
    alongside standard reinforcement learning. The influence of the WSLS heuristic 
    is assumed to be proportional to the participant's anxiety level (STAI).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    wsls_weight: [0, 5] (Weight of the heuristic bias)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.wsls_weight = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_vals = self.q_stage1.copy()
        
        # Apply WSLS bias if there is a previous trial
        if self.last_action1 is not None and self.last_reward is not None:
            wsls_bias = np.zeros(self.n_choices)
            
            # Win-Stay: If reward > 0, boost previous action
            if self.last_reward > 0:
                wsls_bias[int(self.last_action1)] = 1.0
            # Lose-Shift: If reward <= 0, boost the unchosen action
            else:
                wsls_bias[1 - int(self.last_action1)] = 1.0
            
            # The heuristic weight is modulated by anxiety
            q_vals += self.wsls_weight * self.stai * wsls_bias

        return self.softmax(q_vals, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Reward Valuation]
    This model hypothesizes that anxiety alters the subjective valuation of outcomes 
    (reward sensitivity). High anxiety might lead to hyper-reactivity (amplifying 
    perceived value) or blunted affect (dampening perceived value). The model scales 
    the effective reward received based on the STAI score before updating Q-values.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    reward_scaling: [-1, 2] (Scales reward magnitude: >0 amplifies, <0 dampens)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.reward_scaling = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective reward modulated by anxiety
        # If reward_scaling is positive, anxiety amplifies the reward signal.
        # If negative, it dampens it.
        effective_reward = reward * (1.0 + self.reward_scaling * self.stai)
        
        # Standard TD update with effective_reward
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Intolerance of Uncertainty]
    This model hypothesizes that anxious participants have a specific aversion to 
    unexpected or "rare" transitions (Intolerance of Uncertainty). Regardless of 
    the reward outcome, the model penalizes the Stage 1 action that led to a 
    rare transition. The magnitude of this penalty is proportional to anxiety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    uncertainty_pen: [0, 2] (Penalty magnitude for rare transitions)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.uncertainty_pen = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update first
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Determine if transition was rare
        # Transition counts: [[35, 15], [15, 35]]
        # Action 0 -> State 1 is rare. Action 1 -> State 0 is rare.
        is_rare = (action_1 == 0 and state == 1) or (action_1 == 1 and state == 0)
        
        if is_rare:
            # Apply penalty proportional to anxiety
            penalty = self.uncertainty_pen * self.stai
            self.q_stage1[action_1] -= penalty

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```