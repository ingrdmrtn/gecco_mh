Here are 3 new cognitive models exploring different mechanisms for how anxiety (STAI) might influence decision-making in this task, specifically focusing on the observed behavior of strong choice persistence.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model tests the hypothesis that anxiety shifts the balance between Model-Based (planning) and Model-Free (habitual) control. Specifically, it posits that medium-to-high anxiety participants might rely more on Model-Free strategies (habits) because they are cognitively less demanding, or conversely, they might over-think (Model-Based). Given the data shows repetitive behavior, a strong Model-Free component or a "safe" Model-Based prior is likely. Here, we model the mixing weight `w` as a function of STAI.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    The mixing weight 'w' determines the contribution of MB values to the final decision.
    w = w_base + (w_mod * stai)
    If w is close to 1, behavior is Model-Based. If close to 0, it is Model-Free.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_base: [0, 1]     # Base mixing weight
    w_mod: [-1, 1]     # Modulation of mixing weight by anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.w_mod = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based transition matrix (fixed for simplicity or could be learned)
        # Using the true transition probabilities provided in base class
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]]) 

    def policy_stage1(self) -> np.ndarray:
        # Model-Free Q-values (from self.q_stage1)
        q_mf = self.q_stage1
        
        # Model-Based Q-values
        # Q_MB(a) = sum(P(s|a) * max(Q_stage2(s, :)))
        # We use the max of stage 2 values as the estimate of state value
        v_stage2 = np.max(self.q_stage2, axis=1)
        q_mb = self.T @ v_stage2
        
        # Calculate effective w based on STAI
        # Clip to ensure it stays in [0, 1]
        w_eff = np.clip(self.w_base + (self.w_mod * self.stai), 0.0, 1.0)
        
        # Combined Q-values
        q_net = (1 - w_eff) * q_mf + w_eff * q_mb
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) / SARSA update for Stage 1 (Model-Free)
        # Using the actual reward from stage 2 to update stage 1 directly (TD(1)-like)
        # or using Q_stage2 (TD(0)). Let's use TD(1) logic often used in hybrid models
        # Q_MF(s1, a1) <- Q_MF(s1, a1) + alpha * (r - Q_MF(s1, a1))
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry
This model hypothesizes that anxiety affects how participants learn from positive versus negative prediction errors. Specifically, anxious individuals might be hypersensitive to negative outcomes (punishment sensitivity) or have blunted responses to reward. This model splits the learning rate `alpha` into `alpha_pos` and `alpha_neg`, where the balance is shifted by the STAI score.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates an asymmetry in learning from positive vs negative prediction errors.
    The learning rate for negative prediction errors (disappointments) is amplified by anxiety.
    
    alpha_neg_effective = alpha * (1 + bias * stai)
    alpha_pos_effective = alpha
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate
    beta: [0, 10]       # Inverse temperature
    bias: [0, 5]        # Multiplier for negative learning rate based on anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.bias = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective learning rates
        alpha_pos = self.alpha
        # Bias amplifies alpha for negative PEs based on STAI
        alpha_neg = np.clip(self.alpha * (1.0 + self.bias * self.stai), 0.0, 1.0)

        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        eff_alpha_2 = alpha_pos if delta_2 >= 0 else alpha_neg
        self.q_stage2[state, action_2] += eff_alpha_2 * delta_2
        
        # Stage 1 Update
        # Note: In standard 2-step, Stage 1 update is often driven by Stage 2 Q-value
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        eff_alpha_1 = alpha_pos if delta_1 >= 0 else alpha_neg
        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven "Safe Option" Bias (Fixed Prior)
This model proposes that anxiety doesn't just change learning or stickiness, but creates a static bias towards specific options perceived as "safer" or "default". Given the participant almost always chooses Spaceship 1, this model tests if anxiety correlates with a fixed bias parameter added to one of the choices (arbitrarily mapped to index 1 here, or learned). We model a bias towards the most frequently chosen option (Choice 1) that scales with STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety introduces a static 'safety bias' towards a specific option (Choice 1),
    independent of reward history. This reflects a tendency to stick to a default or 'safe' choice
    to avoid uncertainty.
    
    Q_net(a) = Q_learned(a) + (bias_strength * stai * IsChoice1(a))
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    bias_strength: [0, 5] # Magnitude of the static bias towards Choice 1
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.bias_strength = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_vals = self.q_stage1.copy()
        
        # Apply static bias to Choice 1 (index 1)
        # The strength of this bias is modulated by the participant's anxiety level
        bias = self.bias_strength * self.stai
        q_vals[1] += bias
        
        return self.softmax(q_vals, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```