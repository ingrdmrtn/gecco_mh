Here are three new cognitive models that test different hypotheses about how anxiety (STAI) modulates decision-making in this task.

### Model 1: Anxiety-Modulated Subjective Loss
```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Subjective Loss]
    This model hypothesizes that anxious individuals exhibit a "negativity bias" in valuation,
    where the absence of a reward (0 coins) is not perceived as a neutral outcome, but as a 
    subjective punishment (loss). The magnitude of this subjective loss is proportional to anxiety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    loss_sens: [0, 5] (Scales the subjective punishment of 0 rewards)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_sens = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective reward
        # If reward is 0, treat it as negative based on anxiety
        if reward < 0.5:
            r_effective = -1.0 * self.loss_sens * self.stai
        else:
            r_effective = reward

        # Standard Q-learning update with effective reward
        # Stage 2 update
        delta_2 = r_effective - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD-0)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Spatial Bias
```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Spatial Bias]
    This model hypothesizes that anxiety induces a "safety bias" or default preference 
    towards the first available option (Spaceship 0/A), possibly perceived as the default 
    or safe choice. This bias acts as a fixed offset in the decision policy, independent 
    of learned values.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    bias_param: [0, 5] (Strength of bias towards Option 0, scaled by STAI)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.bias_param = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy learned values
        logits = self.q_stage1.copy()
        
        # Add static bias to Option 0, modulated by anxiety
        # Higher anxiety -> stronger preference for the "default" option 0
        logits[0] += self.bias_param * self.stai
        
        return self.softmax(logits, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Direct Reinforcement
```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Direct Reinforcement]
    This model hypothesizes that anxiety shifts learning from a Model-Based (or chained TD-0) 
    strategy to a simpler Direct Reinforcement (TD-1) strategy. Anxious participants may 
    ignore the intermediate state structure and reinforce the Stage 1 choice directly based 
    on the final outcome.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    lambda_scale: [0, 3] (Determines mixing weight of Direct Reinforcement, scaled by STAI)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_scale = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (Standard)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Calculate mixing parameter lambda based on anxiety
        # lambda_val = 0 -> Pure TD(0) (Chaining / Structure-sensitive)
        # lambda_val = 1 -> Pure TD(1) (Direct Reinforcement / Structure-ignoring)
        lambda_val = np.clip(self.lambda_scale * self.stai, 0.0, 1.0)
        
        # Stage 1 update: Mixture of TD(0) target (Q_stage2) and TD(1) target (Reward)
        target_td0 = self.q_stage2[state, action_2]
        target_td1 = reward
        
        combined_target = (1 - lambda_val) * target_td0 + lambda_val * target_td1
        
        delta_1 = combined_target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```