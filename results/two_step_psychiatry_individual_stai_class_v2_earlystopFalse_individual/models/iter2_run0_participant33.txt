Here are three new cognitive models expressed as Python functions.

### Model 1: Anxiety-Enhanced Reward Sensitivity (Safety Signal)
```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Enhanced Reward Sensitivity (Safety Signals).
    
    This model hypothesizes that anxious individuals perceive rewards (gold coins) 
    as salient "safety signals" in an uncertain environment. Consequently, they 
    over-value positive outcomes compared to non-anxious individuals.
    
    The effective reward signal used for learning is amplified by the STAI score.
    This leads to stronger reinforcement of rewarded actions (Win-Stay behavior) 
    without necessarily affecting the reaction to punishments (0 coins).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta: [0, 10]           # Inverse temperature
    safety_amp: [0, 5]      # Amplification factor for rewards, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.safety_amp = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Amplify the reward signal based on anxiety
        # If reward is 1, effective_reward > 1. If reward is 0, it remains 0.
        effective_reward = reward * (1.0 + self.safety_amp * self.stai)
        
        # Standard TD update with effective_reward
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Attentional Narrowing (Forgetting)
```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Attentional Narrowing (Forgetting).
    
    This model hypothesizes that high anxiety consumes cognitive resources, leading 
    to "attentional narrowing." Participants focus intensely on their chosen path 
    but rapidly forget or discount the value of the option they *did not* choose.
    
    This is implemented as a decay applied to the unchosen Stage 1 option's Q-value 
    on each trial. The rate of decay is proportional to the STAI score. This can 
    lead to perseveration if the chosen option remains reasonably good, as the 
    alternative fades from memory.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta: [0, 10]           # Inverse temperature
    decay_rate: [0, 1]      # Decay rate for unchosen options, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_rate = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard update for the chosen path
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Decay the unchosen Stage 1 option
        # Anxiety determines how much the unchosen option fades
        unchosen_action = 1 - action_1
        decay_factor = self.decay_rate * self.stai
        # Ensure decay factor doesn't exceed 1 (though bounds usually handle this)
        decay_factor = np.clip(decay_factor, 0, 1)
        
        self.q_stage1[unchosen_action] *= (1.0 - decay_factor)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Direct Reinforcement (Structure Ignoring)
```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Direct Reinforcement (Structure Ignoring).
    
    This model hypothesizes that anxiety impairs the ability to use the task structure 
    (the two-step chain). Instead of updating Stage 1 values based on the value of 
    the resulting Stage 2 state (TD-0 / chaining), anxious participants update Stage 1 
    values based directly on the final reward (TD-1 / direct reinforcement).
    
    The model mixes these two update targets. High anxiety shifts the weight towards 
    the direct reward, effectively ignoring the intermediate planet state.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta: [0, 10]           # Inverse temperature
    direct_weight: [0, 1]   # Weight of direct reward update, scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.direct_weight = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 normally
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Calculate mixing weight based on anxiety
        # w = 0: Pure TD-0 (Standard Base)
        # w = 1: Pure TD-1 (Direct Reinforcement)
        w = np.clip(self.direct_weight * self.stai, 0, 1)
        
        # Target for Stage 1 update
        # Standard target: Value of the state we landed in (Q_stage2)
        target_standard = self.q_stage2[state, action_2]
        # Direct target: The raw reward received
        target_direct = reward
        
        combined_target = (1 - w) * target_standard + w * target_direct
        
        delta_1 = combined_target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```