```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases loss aversion (subjective punishment).
    Anxious individuals may perceive the absence of a reward (0 coins) not as a neutral event, 
    but as a negative outcome or "loss". This model modifies the reward signal: while a win is +1, 
    a loss is treated as a negative value scaled by STAI. This drives avoidance of non-rewarding options.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    loss_sens: [0, 5]   # Sensitivity to loss (0 outcome), scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_sens = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective reward
        # If reward is 0, treat it as a negative value (loss)
        # Magnitude depends on STAI and loss_sens parameter
        if reward < 0.5: # Effectively 0
            reward_effective = -1.0 * self.loss_sens * self.stai
        else:
            reward_effective = reward

        # Standard TD update using the effective reward
        delta_2 = reward_effective - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety increases uncertainty aversion (ambiguity avoidance).
    Anxious individuals often prefer familiar options and avoid those with uncertain outcomes.
    This model tracks how long it has been since each Stage 1 spaceship was chosen.
    It applies a penalty to the Q-values of options that haven't been chosen recently, 
    encouraging perseveration on the currently active option. The penalty strength is modulated by STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    uncert_pen: [0, 2]  # Penalty for uncertainty (time since last choice)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.uncert_pen = model_parameters

    def init_model(self) -> None:
        # Track trials since each option was last chosen
        self.trials_since_chosen = np.zeros(self.n_choices)

    def pre_trial(self) -> None:
        # Increment counter for all options at start of trial
        self.trials_since_chosen += 1.0

    def policy_stage1(self) -> np.ndarray:
        # Apply uncertainty penalty
        # Penalty = base_param * STAI * trials_since_chosen
        # The longer an option is unchosen, the more "scary" (uncertain) it becomes
        penalty = self.uncert_pen * self.stai * self.trials_since_chosen
        q_modified = self.q_stage1 - penalty
        
        return self.softmax(q_modified, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        # Reset counter for the chosen option
        self.trials_since_chosen[int(action_1)] = 0.0

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety promotes direct reinforcement over temporal difference learning.
    Standard TD learning updates Stage 1 values based on the value of the Stage 2 state (Q2).
    However, anxious individuals may be hyper-reactive to the immediate outcome (Reward), 
    bypassing the model structure. This model updates Stage 1 values using a weighted mix 
    of the standard TD error and a direct Reward prediction error. The weight of the direct 
    update increases with STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta: [0, 10]        # Inverse temperature
    direct_w: [0, 1]     # Weight of direct reinforcement (vs TD), scaled by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.direct_w = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 normally
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Calculate two types of errors for Stage 1
        # 1. Standard TD Error: driven by the value of the state we landed in
        delta_1_td = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # 2. Direct Reinforcement Error: driven directly by the reward obtained
        delta_1_direct = reward - self.q_stage1[action_1]
        
        # Determine mixing weight based on STAI
        # If STAI is high, weight shifts towards direct reinforcement
        w = self.direct_w * self.stai
        # Clamp w to [0, 1] to ensure stability
        w = 0.0 if w < 0.0 else (1.0 if w > 1.0 else w)
        
        # Combined update
        delta_final = (1.0 - w) * delta_1_td + w * delta_1_direct
        self.q_stage1[action_1] += self.alpha * delta_final

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```