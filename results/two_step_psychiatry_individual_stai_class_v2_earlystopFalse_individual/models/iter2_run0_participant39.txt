Here are 3 new cognitive models exploring different mechanisms for how high anxiety (STAI = 0.6625) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model tests the hypothesis that high anxiety impairs model-based planning. It implements a hybrid reinforcement learning agent where the balance between model-based (planning) and model-free (habitual) control is determined by the STAI score. Higher anxiety reduces the weight `w` of the model-based system, leading to more habitual behavior.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety impairs model-based planning, leading to a reliance on model-free habits.
    
    This model blends Model-Based (MB) and Model-Free (MF) values.
    The mixing weight 'w' is modulated by STAI:
    w_effective = w_max * (1 - STAI)
    
    Higher STAI -> Lower w -> More Model-Free behavior.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_max: [0, 1]      # Maximum model-based weight (achieved at STAI=0)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def init_model(self) -> None:
        # Model-based transition matrix (fixed knowledge of task structure)
        # 0->0 (70%), 0->1 (30%); 1->0 (30%), 1->1 (70%)
        self.T_model = np.array([[0.7, 0.3], [0.3, 0.7]])

    def policy_stage1(self) -> np.ndarray:
        # Model-Free Value (TD)
        q_mf = self.q_stage1
        
        # Model-Based Value (Planning)
        # Q_MB(a) = sum(P(s'|a) * max(Q_stage2(s', a')))
        v_stage2 = np.max(self.q_stage2, axis=1) # Max value of each state
        q_mb = self.T_model @ v_stage2
        
        # Calculate effective weight based on anxiety
        # High anxiety (STAI ~ 1) -> w approaches 0 (Pure MF)
        # Low anxiety (STAI ~ 0) -> w approaches w_max
        w_effective = self.w_max * (1.0 - self.stai)
        
        # Integrated Q-value
        q_net = w_effective * q_mb + (1 - w_effective) * q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for Model-Free values
        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD(0))
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry
This model hypothesizes that anxiety creates a bias in how positive versus negative outcomes are processed. Specifically, high anxiety might amplify learning from negative prediction errors (punishment sensitivity) or dampen learning from positive ones (anhedonia-like). Here, we model anxiety as scaling the learning rate for negative prediction errors specifically.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety amplifies learning from negative outcomes (negative prediction errors).
    
    The model uses two learning rates: alpha_pos and alpha_neg.
    alpha_neg is dynamically scaled by STAI:
    alpha_neg_effective = alpha_base + (stai * alpha_boost)
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]   # Learning rate for positive RPEs
    alpha_base: [0, 1]  # Base learning rate for negative RPEs
    alpha_boost: [0, 1] # Additional sensitivity to negative RPEs due to anxiety
    beta: [0, 10]       # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_base, self.alpha_boost, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate
        alpha_neg_eff = self.alpha_base + (self.stai * self.alpha_boost)
        # Clip to ensure it stays valid [0, 1]
        alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)

        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += alpha_neg_eff * delta_2
        
        # --- Stage 1 Update ---
        # Note: We use the updated Q2 value for the TD target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += alpha_neg_eff * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression
This model tests the hypothesis that anxiety reduces exploration. Anxious individuals may prefer exploiting known safe options rather than exploring uncertain ones. This is modeled by modulating the inverse temperature parameter ($\beta$) of the softmax function. Higher anxiety leads to a higher $\beta$ (lower temperature), resulting in more deterministic (exploitative) choices.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety suppresses exploration, leading to more deterministic (exploitative) choices.
    
    The inverse temperature (beta) is modulated by STAI.
    beta_effective = beta_base * (1 + (stai * stiff_factor))
    
    Higher STAI -> Higher Beta -> Sharper Softmax -> Less Exploration.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_base: [0, 10]  # Base inverse temperature
    stiff_factor: [0, 5]# How much anxiety stiffens the policy
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stiff_factor = model_parameters

    def get_effective_beta(self) -> float:
        # Calculate beta modulated by anxiety
        return self.beta_base * (1.0 + (self.stai * self.stiff_factor))

    def policy_stage1(self) -> np.ndarray:
        beta_eff = self.get_effective_beta()
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        beta_eff = self.get_effective_beta()
        return self.softmax(self.q_stage2[state], beta_eff)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```