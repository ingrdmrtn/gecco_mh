Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) modulates decision-making in this task.

### Model 1: Anxiety-Driven Hyper-Plasticity
```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Hyper-Plasticity]
    This model hypothesizes that anxiety increases the learning rate (hyper-plasticity). 
    Anxious individuals may be hyper-vigilant to recent outcomes, causing them to 
    update their value estimates more drastically than low-anxiety individuals. 
    This leads to volatile value representations that chase recent rewards or punishments.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1] (Base learning rate)
    beta: [0, 10] (Inverse temperature)
    lr_scale: [0, 5] (Scaling factor for anxiety's effect on learning rate)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.lr_scale = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective alpha based on STAI
        # We clamp to [0, 1] to ensure mathematical stability
        # Higher STAI -> Higher learning rate
        alpha_eff = self.alpha_base * (1.0 + self.lr_scale * self.stai)
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)
        
        # Stage 2 Update (TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha_eff * delta_2
        
        # Stage 1 Update (TD)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_eff * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Decision Noise
```python
class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Decision Noise]
    This model hypothesizes that anxiety acts as a cognitive distractor, increasing 
    noise in the decision-making process. Higher anxiety leads to a lower effective 
    inverse temperature (beta), resulting in more random exploration and less 
    exploitation of learned values, regardless of the actual value differences.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta_base: [0, 10] (Base inverse temperature)
    noise_scale: [0, 10] (Scaling factor for anxiety's noise induction)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.noise_scale = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Effective beta decreases as anxiety increases
        # beta_eff = beta_base / (1 + scale * STAI)
        beta_eff = self.beta_base / (1.0 + self.noise_scale * self.stai)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Apply the same noise modulation to the second stage
        beta_eff = self.beta_base / (1.0 + self.noise_scale * self.stai)
        return self.softmax(self.q_stage2[state], beta_eff)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Habitual Control (Choice Kernel)
```python
class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Habitual Control]
    This model hypothesizes that anxiety increases reliance on a "Choice Kernel" (habit),
    which tracks the frequency of past choices over time. Unlike simple stickiness 
    (which only looks at the very last trial), this kernel integrates history. 
    Anxious participants are assumed to rely more on this familiar routine to reduce 
    decision load.

    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta: [0, 10] (Inverse temperature)
    habit_weight: [0, 5] (Strength of the anxiety-driven habit bias)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.habit_weight = model_parameters

    def init_model(self) -> None:
        # Initialize choice kernel for the two spaceships
        self.choice_kernel = np.zeros(self.n_choices)
        # Fixed decay rate for the kernel (e.g., 0.3) to represent medium-term habit
        # This distinguishes it from 1-step stickiness (decay=1.0)
        self.kernel_decay = 0.3 

    def policy_stage1(self) -> np.ndarray:
        # The policy combines Model-Free Q-values with the Habit Kernel
        # The influence of the habit is modulated by STAI
        habit_bonus = self.habit_weight * self.stai * self.choice_kernel
        net_values = self.q_stage1 + habit_bonus
        return self.softmax(net_values, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Call base method to store last actions
        super().post_trial(action_1, state, action_2, reward)
        
        # Update Choice Kernel for Stage 1 actions
        # K_new = K_old + decay * (Indicator - K_old)
        # This moves the kernel towards 1 for the chosen action, and towards 0 for unchosen
        for a in range(self.n_choices):
            indicator = 1.0 if a == action_1 else 0.0
            self.choice_kernel[a] += self.kernel_decay * (indicator - self.choice_kernel[a])

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```