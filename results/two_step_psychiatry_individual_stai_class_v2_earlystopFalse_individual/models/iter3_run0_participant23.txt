```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Negativity Bias]
    This model hypothesizes that anxiety specifically modulates learning from negative prediction errors 
    (disappointments), while learning from positive prediction errors remains constant.
    Anxious individuals may be hypersensitive (or hyposensitive) to outcomes that are worse than expected,
    updating their beliefs more drastically when things go wrong.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1] (Learning rate for positive RPEs)
    beta: [0, 10]
    alpha_neg_base: [0, 1] (Base learning rate for negative RPEs)
    alpha_neg_anx: [-1, 1] (Modulation of negative learning rate by anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.beta, self.alpha_neg_base, self.alpha_neg_anx = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate alpha_neg based on STAI
        alpha_neg = self.alpha_neg_base + (self.alpha_neg_anx * self.stai)
        alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        # Use alpha_pos if outcome was better than expected, alpha_neg if worse
        alpha_2 = self.alpha_pos if delta_2 >= 0 else alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # We apply the same asymmetry logic to the Stage 1 prediction error
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Induced Decision Noise]
    This model hypothesizes that anxiety acts as a distractor, increasing the probability of 
    random "lapses" in decision-making (epsilon-greedy noise) rather than altering the 
    value learning process itself. High anxiety leads to more random exploration or errors
    due to reduced cognitive control.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    eps_base: [0, 1] (Baseline noise level)
    eps_anx: [0, 1] (Additional noise due to anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.eps_base, self.eps_anx = model_parameters
        # Calculate epsilon once as it is a trait-like modulation in this model
        self.epsilon = np.clip(self.eps_base + (self.eps_anx * self.stai), 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Mixture of Softmax (Goal-directed) and Uniform Random (Noise)
        softmax_probs = self.softmax(self.q_stage1, self.beta)
        random_probs = np.ones(self.n_choices) / self.n_choices
        return (1 - self.epsilon) * softmax_probs + self.epsilon * random_probs

    def policy_stage2(self, state: int) -> np.ndarray:
        softmax_probs = self.softmax(self.q_stage2[state], self.beta)
        random_probs = np.ones(self.n_choices) / self.n_choices
        return (1 - self.epsilon) * softmax_probs + self.epsilon * random_probs

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Stage-2 Learning]
    This model hypothesizes that anxiety specifically impacts the learning rate at the second stage 
    (direct outcome processing). Anxious individuals might have heightened plasticity 
    in response to immediate alien feedback (Stage 2), while their higher-level planning 
    updates (Stage 1) remain unaffected.

    Parameter Bounds:
    -----------------
    alpha_1: [0, 1] (Stage 1 learning rate)
    beta: [0, 10]
    alpha_2_base: [0, 1] (Base Stage 2 learning rate)
    alpha_2_anx: [-1, 1] (Modulation of Stage 2 learning rate by anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_1, self.beta, self.alpha_2_base, self.alpha_2_anx = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate alpha_2 based on STAI
        alpha_2 = self.alpha_2_base + (self.alpha_2_anx * self.stai)
        alpha_2 = np.clip(alpha_2, 0.0, 1.0)
        
        # Stage 2 Update (uses modulated alpha_2)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (uses fixed alpha_1)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```