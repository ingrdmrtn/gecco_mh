```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates a reactive Win-Stay/Lose-Shift (WSLS) heuristic.
    While the agent learns values via RL, they are also subject to an immediate bias 
    based on the previous outcome. High anxiety might amplify this reactive behavior,
    causing the participant to stick to winning options or switch from losing ones 
    more strongly than reinforcement learning alone would predict.
    
    If previous reward = 1: Add bonus to previous action Q-value (Win-Stay).
    If previous reward = 0: Subtract bonus from previous action Q-value (Lose-Shift).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    wsls_base: [0, 5]   # Base magnitude of the WSLS bias
    wsls_stai: [-5, 5]  # Modulation by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.wsls_base, self.wsls_stai = model_parameters

    def init_model(self) -> None:
        # Calculate effective WSLS strength
        self.wsls_eff = self.wsls_base + (self.wsls_stai * self.stai)

    def policy_stage1(self) -> np.ndarray:
        q_effective = self.q_stage1.copy()
        
        # Apply heuristic bias based on the immediately preceding trial
        if self.last_action1 is not None and self.last_reward is not None:
            # If Win (1.0), boost the last action (Stay)
            if self.last_reward > 0.5:
                q_effective[int(self.last_action1)] += self.wsls_eff
            # If Lose (0.0), penalize the last action (Shift)
            else:
                q_effective[int(self.last_action1)] -= self.wsls_eff
                
        return self.softmax(q_effective, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates Loss Aversion (Subjective Valuation of Failure).
    Anxious individuals may perceive a lack of reward (0 coins) not as a neutral outcome, 
    but as a negative outcome (pain of loss). This model modifies the reward function
    such that 0 coins is treated as a negative value, with the magnitude scaled by anxiety.
    
    Effective Reward = Reward (if 1) 
    Effective Reward = -1 * (rho_base + rho_stai * STAI) (if 0)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    rho_base: [0, 5]    # Base loss aversion magnitude
    rho_stai: [-5, 5]   # Modulation by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rho_base, self.rho_stai = model_parameters

    def init_model(self) -> None:
        self.rho_eff = self.rho_base + (self.rho_stai * self.stai)
        # Ensure rho_eff is non-negative (magnitude of loss)
        if self.rho_eff < 0: self.rho_eff = 0

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective reward
        if reward > 0.5:
            r_eff = 1.0
        else:
            # Treat 0 reward as a negative value (loss)
            r_eff = -1.0 * self.rho_eff
            
        # Standard TD update with modified reward signal
        delta_2 = r_eff - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety differentially modulates learning at Stage 2 (Concrete/Alien level).
    The model uses separate learning rates for Stage 1 (Spaceship choice) and Stage 2 (Alien choice).
    Anxiety modulates the Stage 2 learning rate, reflecting altered processing of immediate, 
    concrete outcomes (aliens) versus abstract choices (spaceships).
    
    Parameter Bounds:
    -----------------
    alpha1: [0, 1]      # Fixed learning rate for Stage 1
    beta: [0, 10]
    alpha2_base: [0, 1] # Base learning rate for Stage 2
    alpha2_stai: [-1, 1]# Modulation of Stage 2 learning rate by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha1, self.beta, self.alpha2_base, self.alpha2_stai = model_parameters

    def init_model(self) -> None:
        # Calculate effective alpha2 and clip to valid range [0, 1]
        val = self.alpha2_base + (self.alpha2_stai * self.stai)
        self.alpha2_eff = np.clip(val, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 with anxiety-modulated rate
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha2_eff * delta_2
        
        # Update Stage 1 with fixed rate
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```