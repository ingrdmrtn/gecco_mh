Here are 3 new cognitive models exploring different mechanisms for how high anxiety (STAI = 0.8125) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based Control
This model tests the hypothesis that high anxiety impairs model-based planning. In the two-step task, "model-based" control means using knowledge of the transition structure (which spaceship goes to which planet) to update stage-1 values based on stage-2 outcomes. "Model-free" control just reinforces the chosen spaceship based on the reward. This model proposes that the weight given to the model-based component is inversely proportional to anxiety.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety reduces model-based (MB) planning in favor of model-free (MF) learning.
    
    The stage-1 value update is a weighted mixture of MF and MB updates.
    The mixing weight 'w' represents the strength of MB control.
    We hypothesize that 'w' is modulated by STAI: w_effective = w_max * (1 - sensitivity * STAI).
    Higher anxiety leads to lower 'w', making the agent more model-free.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_max: [0, 1]       # Maximum model-based weight (at 0 anxiety)
    sensitivity: [0, 1] # How strongly anxiety reduces MB control
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Stage 2 values (Model-Free)
        # Q2(s, a2) = Q2(s, a2) + alpha * (r - Q2(s, a2))
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2

        # 2. Compute Model-Based value for Stage 1
        # V_MB(a1) = sum(P(s|a1) * max(Q2(s, :)))
        # We use the fixed transition matrix self.T provided in base class
        # self.T[a1, s] is prob of transition from action a1 to state s
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # Expected value of next states
            expected_val = 0
            for s_prime in range(self.n_states):
                expected_val += self.T[a, s_prime] * np.max(self.q_stage2[s_prime])
            q_mb[a] = expected_val

        # 3. Compute Model-Free value for Stage 1 (TD)
        # Q_MF(a1) = Q_MF(a1) + alpha * (Q2(s, a2) - Q_MF(a1))
        # Note: In standard hybrid models, Q_MF is updated via TD(1) or TD(0). 
        # Here we use a simple TD(0) update to the stored q_stage1 which acts as our MF value.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

        # 4. Combine for next choice policy
        # The stored self.q_stage1 is effectively the MF value.
        # We don't permanently overwrite it with the MB mix, but we need to store the mix 
        # or handle it in policy_stage1. 
        # However, the standard way to implement the hybrid model in this framework 
        # without adding extra state variables for separate Q_MF and Q_MB storage 
        # is to assume q_stage1 tracks the MF value, and we mix it during decision making.
        pass 

    def policy_stage1(self) -> np.ndarray:
        # Calculate current MB values on the fly
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            for s_prime in range(self.n_states):
                q_mb[a] += self.T[a, s_prime] * np.max(self.q_stage2[s_prime])
        
        # Calculate effective weight based on anxiety
        # w_effective is bounded between 0 and 1
        w_effective = self.w_max * (1.0 - self.sensitivity * self.stai)
        w_effective = np.clip(w_effective, 0.0, 1.0)

        # Mixed Q-value: w * Q_MB + (1-w) * Q_MF
        q_net = w_effective * q_mb + (1.0 - w_effective) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Negative Learning Bias
This model hypothesizes that high anxiety leads to a "negativity bias" or hypersensitivity to punishment (omission of reward). The participant learns more from failures (0 coins) than from successes (1 coin).

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety increases the learning rate for negative outcomes (0 reward).
    
    The model uses two learning rates: alpha_pos for rewards and alpha_neg for non-rewards.
    The alpha_neg is modulated by STAI: alpha_neg = alpha_base + (boost * STAI).
    This implies anxious individuals update their beliefs more drastically after failure.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]   # Learning rate for positive rewards
    alpha_base: [0, 1]  # Base learning rate for negative rewards
    beta: [0, 10]       # Inverse temperature
    boost: [0, 1]       # How much STAI boosts negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_base, self.beta, self.boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine learning rate based on outcome
        if reward > 0:
            alpha = self.alpha_pos
        else:
            # Boost negative learning rate by anxiety
            alpha = self.alpha_base + (self.boost * self.stai)
            alpha = min(alpha, 1.0) # Clip to valid range

        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        # Stage 1 Update
        # Using the outcome of stage 2 to update stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Reduction (Temperature Modulation)
This model suggests that anxiety reduces exploration. Instead of changing how values are learned, anxiety changes how choices are made by sharpening the softmax function (increasing inverse temperature `beta`). Anxious participants might rigidly stick to the option they currently believe is best, exploring less.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety reduces exploration by increasing the inverse temperature (beta).
    
    The effective beta is calculated as: beta_eff = beta_base * (1 + stiff_param * STAI).
    Higher beta means more deterministic choices (exploitation), effectively reducing 
    random exploration.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_base: [0, 10]  # Baseline inverse temperature
    stiff_param: [0, 5] # Stiffness parameter: how much anxiety hardens the policy
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stiff_param = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Modulate beta by STAI
        beta_eff = self.beta_base * (1.0 + self.stiff_param * self.stai)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Apply same modulation to stage 2
        beta_eff = self.beta_base * (1.0 + self.stiff_param * self.stai)
        return self.softmax(self.q_stage2[state], beta_eff)

    # Standard value update
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```