Here are 3 new cognitive models exploring different mechanisms for how high anxiety (STAI = 0.5375) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model tests the classic hypothesis that anxiety disrupts model-based planning (using the transition structure) and favors model-free (habitual) control. High anxiety participants are expected to rely less on the transition matrix `T` when updating Stage 1 values.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety reduces model-based control (w).
    This model implements a hybrid reinforcement learning agent that combines Model-Free (MF) 
    and Model-Based (MB) value estimation. The weighting parameter 'w' determines the balance.
    We hypothesize that higher anxiety (STAI) reduces 'w', pushing the participant towards 
    pure model-free strategies.
    
    w_effective = w_base * (1 - stai)
    
    If w_effective is close to 1, the agent is Model-Based.
    If w_effective is close to 0, the agent is Model-Free.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    w_base: [0, 1]  # Base model-based weight (before anxiety reduction)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Free Q-values (TD)
        self.q_mf = np.zeros(self.n_choices)
        # Initialize Model-Based Q-values
        self.q_mb = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective weight modulated by anxiety
        # High anxiety reduces w, making it more model-free
        w_eff = self.w_base * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0.0, 1.0)
        
        # Compute Model-Based values: Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        # We use the fixed transition matrix self.T
        for a in range(self.n_choices):
            expected_val = 0
            for s in range(self.n_states):
                expected_val += self.T[a, s] * np.max(self.q_stage2[s])
            self.q_mb[a] = expected_val
            
        # Integrated values
        q_net = w_eff * self.q_mb + (1 - w_eff) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (common to both systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Model-Free update (TD-1)
        # Note: In full hybrid models, this is often TD(1) using the reward directly, 
        # or TD(0) using the stage 2 value. Here we use a simple TD update towards Q2.
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that high anxiety makes participants hypersensitive to negative outcomes (losses or lack of reward). Instead of treating 0 coins as neutral, anxious participants might perceive it as a "painful" loss, effectively learning more from failures than successes.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety induces asymmetric learning rates (Loss Aversion).
    Anxious participants may over-weight negative prediction errors (when reward is 0)
    compared to positive prediction errors.
    
    We split alpha into alpha_pos (for RPE > 0) and alpha_neg (for RPE < 0).
    The anxiety score (stai) amplifies the learning rate for negative outcomes.
    
    alpha_neg_effective = alpha_neg_base * (1 + stai)

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      # Learning rate for positive RPEs
    alpha_neg_base: [0, 1] # Base learning rate for negative RPEs
    beta: [0, 10]          # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate
        alpha_neg_eff = self.alpha_neg_base * (1.0 + self.stai)
        alpha_neg_eff = np.clip(alpha_neg_eff, 0.0, 1.0)

        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 >= 0 else alpha_neg_eff
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # We use the updated Q2 value to drive Q1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else alpha_neg_eff
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression (Inverse Temperature Modulation)
This model suggests that anxiety reduces the willingness to explore. High anxiety participants might "freeze" on their current best guess, exhibiting more deterministic behavior (higher beta) to avoid the uncertainty of exploration.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety suppresses exploration (increases Beta).
    High anxiety leads to risk aversion or intolerance of uncertainty, manifesting 
    as a higher inverse temperature (beta). The participant exploits their current 
    knowledge more rigidly rather than exploring alternatives.
    
    beta_effective = beta_base * (1 + stiffness * stai)
    
    Where 'stiffness' controls how strongly anxiety hardens the policy.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta_base: [0, 10] # Base inverse temperature
    stiffness: [0, 5]  # Multiplier for anxiety effect on beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stiffness = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Modulate beta based on STAI
        beta_eff = self.beta_base * (1.0 + self.stiffness * self.stai)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Modulate beta based on STAI
        beta_eff = self.beta_base * (1.0 + self.stiffness * self.stai)
        return self.softmax(self.q_stage2[state], beta_eff)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```