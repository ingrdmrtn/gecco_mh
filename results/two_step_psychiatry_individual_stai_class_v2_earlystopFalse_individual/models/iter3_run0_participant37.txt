```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Reward Devaluation (Pessimism).
    High anxiety participants may subjectively devalue positive outcomes, perceiving a 'win' 
    as less valuable than low-anxiety participants. This dampens the accumulation 
    of value (Q-values) and reduces the drive to exploit high-reward options, 
    reflecting a form of anhedonia or pessimism often associated with anxiety.
    
    Mechanism:
    The effective reward used for updating Q-values is scaled down by anxiety:
    effective_reward = reward * (1.0 - devaluation_factor * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    devaluation_factor: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.devaluation_factor = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Apply devaluation based on anxiety
        # If stai is high, effective reward is lower.
        deval = np.clip(self.devaluation_factor * self.stai, 0, 1)
        effective_reward = reward * (1.0 - deval)
        
        # Standard Q-learning update with effective_reward
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 based on the new Stage 2 value
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Induced State Generalization.
    Anxious participants may over-generalize outcomes, updating beliefs about 
    unvisited states based on current experiences. If they receive a reward (or lack thereof) 
    on one planet, they update the value of the other planet in the same direction, 
    assuming a global environmental quality ("the world is good/bad today").
    
    Mechanism:
    Update visited state normally.
    Update unvisited state: Q(other) += alpha * (gen_rate * stai) * (reward - Q(other))
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    gen_rate: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.gen_rate = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard update for the visited state
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Generalization update for the unvisited state (other planet)
        # We apply the update to the same action index in the other state
        other_state = 1 - state
        gen_alpha = self.alpha * self.gen_rate * self.stai
        
        delta_gen = reward - self.q_stage2[other_state, action_2]
        self.q_stage2[other_state, action_2] += gen_alpha * delta_gen
        
        # Stage 1 update (standard)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Reliance on Win-Stay Lose-Shift (WSLS).
    Under high anxiety, participants may bypass complex value computation and rely 
    on a simple reactive heuristic: repeat the Stage 1 choice if rewarded, switch if not.
    This model mixes the Softmax policy with a deterministic WSLS policy, where the
    reliance on the heuristic increases with anxiety.
    
    Mechanism:
    P(choice) = (1 - w) * Softmax(Q) + w * P_WSLS
    w = heuristic_weight * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    heuristic_weight: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.heuristic_weight = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Value-based probabilities (Softmax)
        p_val = self.softmax(self.q_stage1, self.beta)
        
        # 2. Compute Heuristic probabilities (WSLS)
        p_wsls = np.zeros(self.n_choices)
        
        if self.last_reward is None:
            # First trial: no heuristic available, default to uniform
            p_wsls = np.ones(self.n_choices) / self.n_choices
        else:
            if self.last_reward > 0:
                # Win: Stay with the previous Stage 1 action
                p_wsls[int(self.last_action1)] = 1.0
            else:
                # Lose: Shift to the other Stage 1 action
                p_wsls[1 - int(self.last_action1)] = 1.0
        
        # 3. Mix them based on anxiety
        w = np.clip(self.heuristic_weight * self.stai, 0, 1)
        return (1 - w) * p_val + w * p_wsls

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```