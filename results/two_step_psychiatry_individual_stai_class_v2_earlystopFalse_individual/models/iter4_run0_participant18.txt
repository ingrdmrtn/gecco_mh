```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates punishment sensitivity (Loss Aversion).
    
    We hypothesize that anxious individuals process the absence of reward (0 coins)
    as a negative outcome (punishment) rather than a neutral event.
    The magnitude of this subjective punishment is proportional to their STAI score.
    
    Effective Reward R*:
    If Reward = 1: R* = 1
    If Reward = 0: R* = - (loss_sens * STAI)
    
    This leads to active avoidance of non-rewarding options, rather than just unlearning.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    loss_sens: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_sens = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Transform reward based on anxiety
        effective_reward = reward
        if reward == 0:
            # The "pain" of losing is proportional to anxiety
            effective_reward = -1.0 * (self.loss_sens * self.stai)
            
        # Update Stage 2
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning rate asymmetry (Negativity Bias).
    
    Anxious individuals may be more sensitive to "bad news" (negative prediction errors)
    than "good news". We hypothesize that STAI specifically increases the learning rate
    for negative prediction errors, causing faster abandonment of options that disappoint.
    
    alpha_pos = alpha_base
    alpha_neg = alpha_base + (alpha_boost * STAI)

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]
    beta: [0, 10]
    alpha_boost: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.alpha_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate Stage 2 PE
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine alpha for Stage 2
        if delta_2 < 0:
            alpha_2 = np.clip(self.alpha_base + (self.alpha_boost * self.stai), 0, 1)
        else:
            alpha_2 = self.alpha_base
            
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Calculate Stage 1 PE
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine alpha for Stage 1
        if delta_1 < 0:
            alpha_1 = np.clip(self.alpha_base + (self.alpha_boost * self.stai), 0, 1)
        else:
            alpha_1 = self.alpha_base
            
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety promotes reactive heuristic use (Win-Stay Lose-Shift).
    
    High anxiety may reduce the cognitive resources available for value-based learning,
    increasing reliance on simple reactive heuristics like Win-Stay Lose-Shift (WSLS).
    
    The policy is a mixture of Reinforcement Learning (RL) and WSLS.
    P(choice) = (1 - w) * P_RL + w * P_WSLS
    
    The weight 'w' is determined by STAI: w = w_scale * STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    w_scale: [0, 2] # Scaling factor to map STAI to mixture weight [0,1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_scale = model_parameters

    def init_model(self) -> None:
        # Calculate mixture weight based on anxiety
        self.w = np.clip(self.w_scale * self.stai, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # 1. RL Policy
        p_rl = self.softmax(self.q_stage1, self.beta)
        
        # 2. WSLS Policy
        p_wsls = np.zeros(self.n_choices)
        if self.last_action1 is not None:
            last_a = int(self.last_action1)
            # If we got a reward last time, stay.
            if self.last_reward > 0:
                p_wsls[last_a] = 1.0
            # If we didn't get a reward, switch.
            else:
                p_wsls[1 - last_a] = 1.0
        else:
            # First trial: uniform
            p_wsls[:] = 0.5
            
        # 3. Mixture
        return (1.0 - self.w) * p_rl + self.w * p_wsls

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```