Here are 3 new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this task, specifically addressing the observed behavior of sticking to one option.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Hybrid
This model hypothesizes that anxiety dictates the balance between Model-Based (planning) and Model-Free (habitual) control. The participant's medium anxiety might push them towards a specific mix where they rely on habits (Model-Free) but anxiety modulates the weight ($w$) given to the Model-Based controller. Specifically, higher anxiety might impair the cognitive resources needed for Model-Based control, reducing $w$.

```python
import numpy as np

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety (STAI) modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety consumes cognitive resources, reducing the weight (w) of the Model-Based system.
    
    Q_net = w * Q_MB + (1-w) * Q_MF
    w = w_max * (1 - stai)  # Anxiety reduces the maximum possible MB contribution
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_max: [0, 1]      # Maximum model-based weight (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based transition matrix (fixed for simplicity as per instructions, or could be learned)
        # Using the provided counts in base class: self.trans_counts
        pass

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Q-values (self.q_stage1 is used as Q_MF)
        q_mf = self.q_stage1
        
        # 2. Model-Based Q-values
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        # We use the transition matrix self.T derived from self.trans_counts
        # self.T[action, state]
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            expected_val = 0
            for s in range(self.n_states):
                # Probability of transition a -> s
                prob = self.T[a, s]
                # Max value of next state
                max_q2 = np.max(self.q_stage2[s])
                expected_val += prob * max_q2
            q_mb[a] = expected_val
            
        # 3. Calculate Weight w based on STAI
        # Hypothesis: Anxiety reduces MB capacity.
        # w represents the contribution of the goal-directed system.
        w = self.w_max * (1.0 - self.stai)
        
        # 4. Integrated Q-values
        q_net = w * q_mb + (1 - w) * q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) / SARSA update for Stage 1 (Model-Free component)
        # Using the actual reward from stage 2 to update stage 1 directly (TD(1)-like)
        # or using Q_stage2 (TD(0)). Let's use standard TD(0) as per base but explicit.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Asymmetric Learning Rates
This model hypothesizes that anxiety creates a bias in how positive versus negative prediction errors are processed. Specifically, anxious individuals might be more sensitive to negative outcomes (punishment/lack of reward) or less sensitive to positive ones. This model splits the learning rate $\alpha$ into $\alpha_{pos}$ and $\alpha_{neg}$, where the balance is shifted by the STAI score.

```python
import numpy as np

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning asymmetry. Anxious individuals learn differently 
    from positive vs negative prediction errors.
    
    alpha_pos = base_alpha * (1 - stai * asymmetry)
    alpha_neg = base_alpha * (1 + stai * asymmetry)
    
    If asymmetry > 0, anxiety suppresses learning from gains and boosts learning from losses.
    
    Parameter Bounds:
    -----------------
    base_alpha: [0, 1]   # Base learning rate
    beta: [0, 10]        # Inverse temperature
    asymmetry: [0, 1]    # Strength of anxiety-driven bias towards negative learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.base_alpha, self.beta, self.asymmetry = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective learning rates based on STAI
        # We clamp them to [0, 1] to ensure stability
        alpha_pos = np.clip(self.base_alpha * (1.0 - self.stai * self.asymmetry), 0.0, 1.0)
        alpha_neg = np.clip(self.base_alpha * (1.0 + self.stai * self.asymmetry), 0.0, 1.0)

        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += alpha_neg * delta_2
        
        # --- Stage 1 Update ---
        # Using the updated Q2 value for the TD error
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += alpha_neg * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced "Safe Option" Bias
This model hypothesizes that anxiety introduces a fixed bias towards a specific option perceived as "safe" or "default" (in this case, likely Spaceship 1 given the data). Instead of general stickiness (repeating *any* last action), this models a specific prior preference that resists updating, where the strength of this resistance is proportional to anxiety.

```python
import numpy as np

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates a static bias towards a 'default' or 'safe' option (Action 1),
    regardless of reward history. This acts as a prior on the Q-values.
    
    Q_stage1(a) = Q_learned(a) + bias_term
    where bias_term for Action 1 = bias_magnitude * stai
    
    This explains the extreme preference for one spaceship if the participant views it as the 'correct' or 'safe' choice.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]          # Learning rate
    beta: [0, 10]          # Inverse temperature
    bias_magnitude: [0, 5] # Strength of the static bias towards Action 1
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.bias_magnitude = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_vals = self.q_stage1.copy()
        
        # Apply anxiety-driven static bias to Action 1
        # We assume Action 1 is the 'safe' bias target based on the participant's data trace
        bias = self.bias_magnitude * self.stai
        q_vals[1] += bias
        
        return self.softmax(q_vals, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```