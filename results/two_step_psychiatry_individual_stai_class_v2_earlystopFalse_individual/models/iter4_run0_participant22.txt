```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Subjective Valuation (Loss Aversion).
    This model hypothesizes that anxious individuals process the "no reward" (0 coins) outcome 
    not as a neutral event, but as a subjective punishment (negative value).
    The magnitude of this negative valuation is modulated by the STAI score.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    neg_val_base: [0, 2]  # Base magnitude of negative subjective value for 0 reward
    neg_val_stai: [-2, 2] # Modulation by STAI
    """
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.neg_val_base, self.neg_val_stai = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate subjective reward
        # If reward is 1, it remains 1.
        # If reward is 0, it becomes a negative value modulated by anxiety.
        effective_reward = reward
        if reward == 0:
            # Calculate penalty magnitude
            penalty = self.neg_val_base + (self.neg_val_stai * self.stai)
            # Ensure it acts as a punishment (negative value)
            effective_reward = -1.0 * abs(penalty)

        # Standard TD update with effective_reward
        # Update Stage 2
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Direct vs. Serial Learning.
    This model hypothesizes that anxiety affects the structural depth of learning.
    'Serial' learning updates Stage 1 values based on Stage 2 values (chaining).
    'Direct' learning updates Stage 1 values based directly on the reward (ignoring the transition).
    High anxiety may promote a simpler, 'Direct' stimulus-response strategy.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    w_direct_base: [0, 1] # Base weight for direct learning (0=Pure Serial, 1=Pure Direct)
    w_direct_stai: [-1, 1] # Modulation by STAI
    """
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_direct_base, self.w_direct_stai = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate weight for direct learning
        w = self.w_direct_base + (self.w_direct_stai * self.stai)
        w = np.clip(w, 0.0, 1.0) # Ensure between 0 and 1
        
        # Stage 2 update (standard)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update: Mixture of Serial (from Q2) and Direct (from Reward)
        target_serial = self.q_stage2[state, action_2]
        target_direct = reward
        
        # The target value for Q1 is a weighted average
        mixed_target = (1 - w) * target_serial + w * target_direct
        
        delta_1 = mixed_target - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Uncertainty Avoidance.
    This model hypothesizes that anxiety correlates with Intolerance of Uncertainty.
    It tracks the visitation counts of Stage 1 choices.
    Anxiety modulates an 'uncertainty bonus' added to Q-values.
    Negative weights imply uncertainty avoidance (anxiety), positive weights imply novelty seeking.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    iu_base: [-2, 2] # Base uncertainty weight
    iu_stai: [-2, 2] # STAI modulation of uncertainty weight
    """
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.iu_base, self.iu_stai = model_parameters

    def init_model(self) -> None:
        # Initialize counts for Stage 1 choices
        self.counts_stage1 = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Calculate uncertainty weight based on STAI
        w_uncert = self.iu_base + (self.iu_stai * self.stai)
        
        # Uncertainty metric: Inverse square root of visit counts (UCB-like)
        # Add 1 to avoid division by zero
        uncertainty = 1.0 / np.sqrt(self.counts_stage1 + 1)
        
        # Add bonus/penalty to Q-values before softmax
        q_biased = self.q_stage1 + (w_uncert * uncertainty)
        
        return self.softmax(q_biased, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Call base method to update last_action, etc.
        super().post_trial(action_1, state, action_2, reward)
        # Increment count for the chosen Stage 1 action
        self.counts_stage1[action_1] += 1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```