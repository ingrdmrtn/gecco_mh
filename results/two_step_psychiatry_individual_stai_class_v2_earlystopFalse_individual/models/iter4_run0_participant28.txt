Here are three new cognitive models exploring different mechanisms by which high anxiety (STAI = 0.6625) might influence decision-making in this two-step task.

### Model 1: Anxiety-Driven Model-Based Suppression
**Hypothesis:** High anxiety consumes cognitive resources (working memory), impairing the ability to maintain and use a complex "model-based" strategy (planning based on transition probabilities). Instead, anxious participants rely more on "model-free" (habitual) learning. In this model, the mixing weight `w` between model-based and model-free control is inversely proportional to STAI score.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety acts as a cognitive load that suppresses Model-Based (MB) planning.
    
    The participant uses a hybrid Model-Based / Model-Free strategy.
    The weighting parameter 'w' determines the balance:
        w = w_max * (1 - STAI)
    
    Higher anxiety (STAI) leads to lower 'w', forcing reliance on Model-Free (habitual) learning.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Learning rate)
    beta: [0, 10]   (Inverse temperature)
    w_max: [0, 1]   (Maximum possible model-based weight for a calm person)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def init_model(self) -> None:
        # Calculate the fixed mixing weight based on anxiety
        # If STAI is high (near 1), w approaches 0 (pure Model-Free).
        # If STAI is low (near 0), w approaches w_max.
        self.w = self.w_max * (1.0 - self.stai)
        
        # Initialize transition matrix (fixed prior)
        # Row 0: Space A -> [Planet X, Planet Y] (Common, Rare)
        # Row 1: Space U -> [Planet X, Planet Y] (Rare, Common)
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]])

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Value (Q_MF)
        q_mf = self.q_stage1
        
        # 2. Model-Based Value (Q_MB)
        # Q_MB(action) = Sum(P(state|action) * max(Q_stage2(state)))
        # We assume the agent knows the transition structure T
        max_q2 = np.max(self.q_stage2, axis=1) # Max value for each state (planet)
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            q_mb[a] = np.sum(self.T[a] * max_q2)
            
        # 3. Hybrid Value
        q_net = (1 - self.w) * q_mf + self.w * q_mb
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning for Model-Free values
        
        # Stage 2 update
        pe2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * pe2
        
        # Stage 1 update (TD(1) / SARSA-like logic for MF path)
        # Note: In hybrid models, MF Q1 is often updated by the Q2 value or reward directly.
        # Here we use the standard TD update from the base class logic but explicit here for clarity
        pe1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * pe1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Exploration (Epsilon-Greedy Modulation)
**Hypothesis:** Anxiety manifests as behavioral restlessness or uncertainty, leading to increased random exploration ("noise") rather than directed exploitation. This model posits that the `epsilon` parameter in an epsilon-greedy choice rule is a function of the STAI score. Higher anxiety leads to more random choices, overriding the learned values.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases random exploration (noise) in decision making.
    
    The model uses a standard TD learning mechanism but selects actions using
    an epsilon-greedy policy where epsilon is scaled by STAI.
    
    epsilon = base_epsilon + (stai * epsilon_sensitivity)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Learning rate)
    base_epsilon: [0, 0.5] (Baseline randomness)
    eps_sens: [0, 0.5] (Sensitivity of randomness to anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.base_epsilon, self.eps_sens = model_parameters

    def init_model(self) -> None:
        # Calculate effective epsilon
        # Capped at 1.0 theoretically, but bounds keep it reasonable
        self.epsilon = self.base_epsilon + (self.stai * self.eps_sens)
        self.epsilon = min(max(self.epsilon, 0.0), 1.0)

    def _epsilon_greedy_prob(self, q_values):
        n_opts = len(q_values)
        probs = np.ones(n_opts) * (self.epsilon / n_opts)
        best_action = np.argmax(q_values)
        probs[best_action] += (1.0 - self.epsilon)
        return probs

    def policy_stage1(self) -> np.ndarray:
        return self._epsilon_greedy_prob(self.q_stage1)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self._epsilon_greedy_prob(self.q_stage2[state])

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD Learning
        # Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Perseveration (Stickiness)
**Hypothesis:** Anxious individuals may exhibit "safety behaviors" or rigidity, sticking to previous choices regardless of reward outcomes to avoid the uncertainty of switching. This model introduces a "stickiness" parameter that biases the choice probability towards the previously chosen action (repetition), with the strength of this bias amplified by the STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases choice perseveration (stickiness).
    
    The model adds a 'stickiness' bonus to the Q-value of the previously chosen action.
    The magnitude of this bonus is determined by STAI.
    
    Stickiness Bonus = stick_base + (stai * stick_mod)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   (Learning rate)
    beta: [0, 10]   (Inverse temperature)
    stick_mod: [0, 5] (How much anxiety increases the tendency to repeat choices)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_mod = model_parameters

    def init_model(self) -> None:
        # We track the very last action taken at stage 1 to apply the bonus
        self.prev_action1 = None

    def policy_stage1(self) -> np.ndarray:
        q_vals = self.q_stage1.copy()
        
        # Apply stickiness bonus if a previous action exists
        if self.prev_action1 is not None:
            # The bonus is purely a function of anxiety here (assuming base stickiness is 0 or absorbed)
            # High anxiety -> High bonus to repeat -> Rigid behavior
            bonus = self.stai * self.stick_mod
            q_vals[self.prev_action1] += bonus
            
        return self.softmax(q_vals, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        self.prev_action1 = action_1

    # Standard value update (TD)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```