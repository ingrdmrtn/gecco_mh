Here are three new cognitive models that explore different mechanisms for how anxiety (STAI) might influence decision-making in this task, specifically focusing on model-based control, punishment sensitivity, and uncertainty-driven exploration.

### Model 1: Anxiety-Modulated Model-Based Control
This model tests the hypothesis that high anxiety impairs model-based reasoning. In the two-step task, "model-based" control involves using the transition matrix (knowledge of which spaceship goes to which planet) to plan. "Model-free" control just repeats what was rewarded. This model proposes a hybrid strategy where the weight of the model-based component ($w$) is inversely proportional to anxiety.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety impairs model-based planning.
    
    This model implements a hybrid reinforcement learning agent (Daw et al., 2011).
    The agent computes both Model-Free (MF) and Model-Based (MB) values for Stage 1.
    The final Q-value is a weighted sum: Q_net = w * Q_MB + (1-w) * Q_MF.
    
    Crucially, the mixing weight 'w' is modulated by STAI.
    High anxiety reduces 'w', making the agent more model-free (habitual).
    w_effective = w_base * (1 - stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    w_base: [0, 1]  # Base model-based weight (before anxiety reduction)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Based values separately
        # We need to track MF values explicitly since q_stage1 in base is generic
        self.q_mf = np.zeros(self.n_choices)
        
    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based Values for Stage 1
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # self.T is transition matrix [action, state]
            # We use the max Q-value of the next stage as the estimated value
            q_mb[a] = np.sum(self.T[a] * np.max(self.q_stage2, axis=1))
            
        # 2. Calculate effective weight w based on STAI
        # Higher STAI -> Lower w (less model-based)
        # We clip to ensure it stays in [0, 1]
        w_eff = np.clip(self.w_base * (1.0 - self.stai), 0.0, 1.0)
        
        # 3. Combine MF and MB values
        q_net = w_eff * q_mb + (1 - w_eff) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (Standard Q-learning)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 MF update (TD(1) - updates based on final reward)
        # Note: In pure TD(0) it would be based on stage 2 value, but TD(1) is common in hybrid models
        # Here we use a simple TD(1) approximation: update Q_MF(a1) towards reward
        delta_1 = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Punishment Sensitivity
This model hypothesizes that anxiety creates a bias towards learning from negative outcomes (or lack of reward). Instead of a single learning rate, the agent has separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). Anxiety is hypothesized to amplify the learning rate for negative errors.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases sensitivity to negative prediction errors (punishment/omission).
    
    The model uses dual learning rates: alpha_pos and alpha_neg.
    The base alpha_neg is modulated by STAI:
    alpha_neg_eff = alpha_neg_base + (stai * (1 - alpha_neg_base))
    
    This means high anxiety participants update their values more drastically
    when outcomes are worse than expected (e.g., getting 0 coins).

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]       # Learning rate for positive RPE
    alpha_neg_base: [0, 1]  # Base learning rate for negative RPE
    beta: [0, 10]           # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate
        # Scales from alpha_neg_base up to 1.0 as STAI increases
        alpha_neg_eff = self.alpha_neg_base + (self.stai * (1.0 - self.alpha_neg_base))
        
        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += self.alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += alpha_neg_eff * delta_2
        
        # --- Stage 1 Update ---
        # Using the updated stage 2 value as the target for stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += self.alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += alpha_neg_eff * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Uncertainty Aversion
This model hypothesizes that anxious individuals are averse to uncertainty. Instead of just tracking value ($Q$), the agent tracks the uncertainty (variance) of the outcome associated with each Stage 1 choice. High anxiety leads to a penalty subtracted from the value of options that have high outcome variance, discouraging the selection of "risky" or uncertain transitions.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety leads to uncertainty aversion.
    
    The agent tracks not just the mean value (Q) but also the variance of the rewards/values.
    In Stage 1, the choice policy is based on: Q_net = Q_stage1 - (penalty_factor * Uncertainty).
    
    The penalty_factor is directly determined by STAI: penalty = rho * STAI.
    Uncertainty is approximated by the variance of the prediction error history.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate for value
    beta: [0, 10]   # Inverse temperature
    rho: [0, 5]     # Scaling factor for uncertainty penalty
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rho = model_parameters

    def init_model(self) -> None:
        # Initialize uncertainty (variance) estimates for Stage 1 choices
        # Start with some non-zero uncertainty
        self.uncertainty_stage1 = np.ones(self.n_choices) * 0.25 

    def policy_stage1(self) -> np.ndarray:
        # Calculate penalty based on STAI
        penalty_weight = self.rho * self.stai
        
        # Subtract uncertainty penalty from Q-values
        # High anxiety -> larger penalty for uncertain options
        adjusted_values = self.q_stage1 - (penalty_weight * self.uncertainty_stage1)
        
        return self.softmax(adjusted_values, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Update
        # Target is the value of the state we landed in
        target_val = self.q_stage2[state, action_2]
        delta_1 = target_val - self.q_stage1[action_1]
        
        # Update Value
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Update Uncertainty (Variance of prediction error)
        # We approximate variance update: V_new = V_old + alpha * (delta^2 - V_old)
        # This tracks the magnitude of recent surprises
        sq_error = delta_1 ** 2
        self.uncertainty_stage1[action_1] += self.alpha * (sq_error - self.uncertainty_stage1[action_1])

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```