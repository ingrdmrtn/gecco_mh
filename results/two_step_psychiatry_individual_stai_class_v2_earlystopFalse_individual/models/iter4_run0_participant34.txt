Here are 3 new cognitive models exploring different mechanisms by which high anxiety (STAI) might influence decision-making in the two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Trade-off
This model tests the hypothesis that high anxiety impairs model-based planning. Model-based control relies on the transition matrix to plan actions, while model-free control relies on simple caching of rewards. High anxiety might consume cognitive resources, pushing the participant towards a more habitual, model-free strategy.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety reduces the weight of model-based (MB) planning 
    in favor of model-free (MF) learning. The mixing parameter `w` determines 
    the balance (1 = pure MB, 0 = pure MF). We hypothesize `w` is negatively 
    correlated with STAI.
    
    Mechanism:
    w_effective = w_max * (1 - STAI)
    If STAI is high (~1.0), w_effective approaches 0 (Model-Free).
    If STAI is low (~0.0), w_effective approaches w_max (Model-Based).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_max: [0, 1]       # Maximum model-based weight (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Model-Free Value (Q_MF)
        q_mf = self.q_stage1
        
        # Model-Based Value (Q_MB)
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        # We use the fixed transition matrix self.T
        # self.T[a1, s] is prob of transitioning to state s given action a1
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            for s in range(self.n_states):
                q_mb[a] += self.T[a, s] * np.max(self.q_stage2[s])
        
        # Calculate effective mixing weight based on STAI
        # High STAI -> Low w -> More MF
        w_eff = self.w_max * (1.0 - self.stai)
        
        # Integrated Value
        q_net = w_eff * q_mb + (1 - w_eff) * q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update (Standard Q-learning)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Update (TD(0) / Model-Free)
        # Note: In hybrid models, q_stage1 usually tracks the MF value
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that high anxiety makes participants hypersensitive to negative outcomes (receiving 0 coins). Instead of a single learning rate, the model uses separate learning rates for positive prediction errors (better than expected) and negative prediction errors (worse than expected). The learning rate for negative errors is amplified by STAI.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to increased sensitivity to negative prediction errors 
    (loss aversion). The learning rate for negative prediction errors (alpha_neg) 
    is boosted by the STAI score relative to a base rate.

    Mechanism:
    alpha_pos = alpha_base
    alpha_neg = alpha_base + (k_boost * STAI)
    (Clipped to max 1.0)

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]  # Base learning rate
    beta: [0, 10]       # Inverse temperature
    k_boost: [0, 1]     # Scaling factor for anxiety-driven negative learning boost
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.k_boost = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate dynamic learning rates
        alpha_pos = self.alpha_base
        alpha_neg = min(1.0, self.alpha_base + (self.k_boost * self.stai))

        # Stage 2 Update
        pe2 = reward - self.q_stage2[state, action_2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        self.q_stage2[state, action_2] += lr2 * pe2
        
        # Stage 1 Update
        # Using the updated stage 2 value as the target
        pe1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        self.q_stage1[action_1] += lr1 * pe1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression (Inverse Temperature Modulation)
This model tests the hypothesis that high anxiety reduces exploration, leading to more deterministic (exploitative) choices. In reinforcement learning, the inverse temperature parameter ($\beta$) controls the randomness of choice. A higher $\beta$ means choices are more deterministic (greedy). This model posits that $\beta$ increases with STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety suppresses exploration, leading to more rigid/deterministic 
    behavior. This is modeled by modulating the inverse temperature (beta) 
    with the STAI score. Higher STAI results in a higher beta (less randomness).

    Mechanism:
    beta_effective = beta_min + (slope * STAI)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_min: [0, 10]   # Minimum inverse temperature (at 0 anxiety)
    slope: [0, 10]      # How much anxiety increases beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_min, self.slope = model_parameters

    def policy_stage1(self) -> np.ndarray:
        beta_eff = self.beta_min + (self.slope * self.stai)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        beta_eff = self.beta_min + (self.slope * self.stai)
        return self.softmax(self.q_stage2[state], beta_eff)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```