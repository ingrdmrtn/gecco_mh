Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) modulates decision-making in this task.

### Model 1: Anxiety-Modulated Reward Valuation
This model hypothesizes that anxiety alters the subjective magnitude of rewards. Anxious individuals might be hypersensitive to positive outcomes (seeking relief) or anhedonic (undervaluing rewards). Here, the effective reward used for learning is scaled by the STAI score.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates subjective reward valuation.
    The participant scales the objective reward by a factor dependent on their anxiety.
    
    Effective Reward = Reward * (1 + theta * STAI)
    
    If theta > 0, anxiety increases reward sensitivity.
    If theta < 0, anxiety decreases reward sensitivity (blunting).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    theta: [-2, 2]  # Valuation scaling factor
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.theta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Modulate reward perception based on anxiety
        eff_reward = reward * (1 + self.theta * self.stai)
        
        # Update Stage 2
        delta_2 = eff_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Impaired Credit Assignment
This model hypothesizes that anxiety consumes cognitive resources, making it harder to propagate information from the second stage back to the first stage. While they learn about the aliens (Stage 2) normally, the update to the spaceships (Stage 1) is dampened by their anxiety level.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety impairs credit assignment to the first stage choice.
    While Stage 2 learning is normal, the propagation of value to Stage 1 
    is dampened by anxiety.
    
    alpha_stage1 = alpha * (1 - zeta * STAI)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    zeta: [0, 2]   # Dampening factor (higher zeta = more impairment per unit anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.zeta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update uses standard alpha
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update is dampened by anxiety
        # We clip to ensure the learning rate doesn't invert (become negative)
        dampener = max(0.0, 1.0 - (self.zeta * self.stai))
        alpha_1 = self.alpha * dampener
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced Pessimism (Zero-is-Loss)
This model hypothesizes that anxious individuals interpret neutral outcomes (receiving 0 coins) not as a lack of change, but as a negative outcome (a loss). This "pessimism bias" is proportional to their STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces pessimism where neutral outcomes (0 reward)
    are perceived as losses.
    
    If Reward == 0: Effective Reward = -1 * pi * STAI
    If Reward == 1: Effective Reward = 1
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    pi: [0, 5]     # Pessimism scale
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pi = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Apply pessimism bias to neutral outcomes
        if reward == 0:
            eff_reward = -1.0 * self.pi * self.stai
        else:
            eff_reward = reward
            
        # Update Stage 2
        delta_2 = eff_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```