```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition probabilities
        self.trans_counts = np.array([[35, 15], [15, 35]]) # Prior counts for transitions, if needed for learning
        self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True) # Transition probabilities, if needed for direct use
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    """Create function interface from model class."""
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Mean-Variance Risk Aversion.
    
    Anxious individuals are hypothesized to be risk-averse. They track not only the 
    expected value (Q) of options but also the variance (uncertainty) of the rewards.
    They penalize options with high reward variance. The magnitude of this penalty 
    is proportional to their STAI score.
    
    Q_effective = Q_mean - (risk_scale * STAI) * sqrt(Variance)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    risk_scale: [0, 5]  # Scaling factor for risk aversion penalty
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.risk_scale = model_parameters

    def init_model(self) -> None:
        # Initialize variance tracking for Stage 2 options (Aliens)
        # We assume initial variance is 0 or small.
        self.var_stage2 = np.zeros((self.n_states, self.n_choices))

    def policy_stage2(self, state: int) -> np.ndarray:
        # Calculate risk-adjusted values for Stage 2
        penalty = self.risk_scale * self.stai
        risk_adjusted_values = self.q_stage2[state] - penalty * np.sqrt(self.var_stage2[state])
        return self.softmax(risk_adjusted_values, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Mean (Q) and Variance (Var) for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Update Variance: Var <- Var + alpha * (delta^2 - Var)
        # This is a standard online variance update approximation
        self.var_stage2[state, action_2] += self.alpha * (delta_2**2 - self.var_stage2[state, action_2])
        
        # Update Mean
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # For Stage 1, we update based on the (potentially risk-adjusted) value of Stage 2?
        # To keep it simple and consistent with TD, we update Stage 1 Q-values based on the 
        # *actual* updated Q-value of Stage 2 (the mean), but the choice at Stage 2 was driven by risk.
        # Alternatively, Stage 1 could also track variance, but let's assume the risk aversion 
        # is primarily about the immediate reward outcome at Stage 2.
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Eligibility Traces (Rumination).
    
    Anxiety might increase the "eligibility trace" (lambda) in a TD(lambda) learning rule.
    High anxiety (rumination) causes the participant to link the final outcome (reward) 
    more strongly back to the initial choice (Stage 1), bypassing the intermediate state structure.
    This effectively blends Model-Free TD(0) (chaining) with Monte-Carlo-like updates.
    
    lambda = lambda_scale * STAI (clipped to [0, 1])

    Parameter Bounds:
    -----------------
    alpha: [0, 1]         # Learning rate
    beta: [0, 10]         # Inverse temperature
    lambda_scale: [0, 2]  # Scaling factor for eligibility trace
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_scale = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate anxiety-dependent lambda
        lam = np.clip(self.lambda_scale * self.stai, 0.0, 1.0)
        
        # 1. Stage 1 update based on Stage 2 value (TD(0) part)
        # The prediction error at the transition: delta_1 = Q2(s, a2) - Q1(a1)
        # Note: In standard TD(lambda), we accumulate traces. Here we implement the 
        # equivalent update for the 2-step case explicitly.
        
        # Current estimate of Stage 2 value (before reward update, technically, 
        # but in this simplified loop we often use the value available).
        # Let's use the standard SARSA-like flow.
        
        # Prediction error from Stage 1 to Stage 2
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # 2. Stage 2 update based on Reward
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 3. Eligibility Trace update for Stage 1
        # Stage 1 gets an additional update proportional to the Stage 2 prediction error
        # scaled by lambda. This links Reward directly to Action 1.
        self.q_stage1[action_1] += self.alpha * lam * delta_2

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Rare Transition Punishment (Frustration).
    
    Anxious individuals may interpret "rare" or unexpected transitions as inherently 
    negative events (loss of control), regardless of the subsequent reward. 
    They apply a specific penalty to the Stage 1 action that resulted in a rare transition.
    
    If transition is rare: Q_stage1[action] -= (punish_scale * STAI)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]         # Learning rate
    beta: [0, 10]         # Inverse temperature
    punish_scale: [0, 5]  # Magnitude of penalty for rare transitions
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.punish_scale = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD updates
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Rare Transition Punishment
        # Common transitions: Action 0 -> State 0, Action 1 -> State 1
        # Rare transitions: Action 0 -> State 1, Action 1 -> State 0
        is_rare = (action_1 == 0 and state == 1) or (action_1 == 1 and state == 0)
        
        if is_rare:
            penalty = self.punish_scale * self.stai
            self.q_stage1[action_1] -= penalty

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```