Here are three new cognitive models that test different hypotheses about how anxiety (STAI) modulates decision-making in this task.

### Model 1: Anxiety-Modulated Distal Credit Assignment
**Hypothesis**: This model hypothesizes that anxiety specifically impairs the ability to assign credit to distal choices (Stage 1) while leaving immediate learning (Stage 2) intact. High anxiety participants may struggle to update the value of the spaceship (Stage 1) based on the eventual outcome, effectively decoupling the initial choice from the reward. This is implemented by scaling down the Stage 1 learning rate (`alpha_1`) as a function of STAI.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety impairs distal credit assignment.
    The learning rate for Stage 1 (alpha_1) is reduced by anxiety, while 
    Stage 2 learning (alpha_2) remains baseline.
    
    alpha_1 = alpha_base / (1 + deficit_scale * stai)
    alpha_2 = alpha_base

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]    # Base learning rate
    beta: [0, 10]         # Inverse temperature
    deficit_scale: [0, 10]# Scaling factor for anxiety-induced deficit
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.deficit_scale = model_parameters

    def init_model(self) -> None:
        # Calculate the anxiety-modulated learning rate for stage 1
        # Higher anxiety -> Larger denominator -> Lower alpha_1
        self.alpha_1 = self.alpha_base / (1.0 + self.deficit_scale * self.stai)
        self.alpha_2 = self.alpha_base

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update (Immediate outcome) - Uses base alpha
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha_2 * delta_2
        
        # Stage 1 Update (Distal outcome) - Uses anxiety-reduced alpha
        # We use a TD-0 style update where the value of the state reached (max or actual)
        # acts as the target. Here we use the value of the chosen stage 2 action.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced "Choking" (Stage 2 Noise)
**Hypothesis**: This model hypothesizes that anxiety manifests as increased decision noise (lower precision) specifically at Stage 2, when the reward/punishment is imminent. While participants may plan effectively at Stage 1, the pressure of the final selection leads to more random behavior ("choking") in high-anxiety individuals. This is modeled by having a separate, lower inverse temperature (`beta`) for Stage 2 that decreases as STAI increases.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases decision noise specifically at Stage 2 (Choking effect).
    
    beta_stage1 = beta_base
    beta_stage2 = beta_base / (1 + noise_factor * stai)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]         # Learning rate
    beta_base: [0, 10]    # Base inverse temperature (Stage 1)
    noise_factor: [0, 10] # Factor by which anxiety reduces Stage 2 precision
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.noise_factor = model_parameters

    def init_model(self) -> None:
        # Stage 1 beta is the base beta
        self.beta_1 = self.beta_base
        # Stage 2 beta is reduced by anxiety (higher noise)
        self.beta_2 = self.beta_base / (1.0 + self.noise_factor * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_1)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_2)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Rare Transition Aversion (Control Sensitivity)
**Hypothesis**: This model hypothesizes that anxious individuals have a specific aversion to "loss of control" represented by rare transitions. Regardless of the reward obtained, the occurrence of a rare transition is treated as a negative event. The model subtracts a penalty term from the value update of the chosen spaceship if a rare transition occurred. The magnitude of this penalty is scaled by STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety induces a penalty for Rare transitions (Control Aversion).
    If a transition is Rare, the update to Stage 1 value includes a penalty term,
    effectively devaluing actions that lead to unexpected states.
    
    Target = Reward - (penalty_param * stai) [if Rare]

    Parameter Bounds:
    -----------------
    alpha: [0, 1]          # Learning rate
    beta: [0, 10]          # Inverse temperature
    penalty_param: [0, 5]  # Magnitude of penalty for rare transitions
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.penalty_param = model_parameters

    def init_model(self) -> None:
        # Define common transitions for reference: 
        # Spaceship A (0) -> Planet X (0) is Common
        # Spaceship U (1) -> Planet Y (1) is Common
        pass

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine if transition was rare
        # Common: (0->0) or (1->1). Rare: (0->1) or (1->0)
        is_rare = (action_1 != state)
        
        # Calculate effective reward/target for Stage 1
        # We use the Stage 2 value as the base proxy for the outcome
        outcome_val = self.q_stage2[state, action_2]
        
        if is_rare:
            # Apply anxiety-modulated penalty
            penalty = self.penalty_param * self.stai
            outcome_val -= penalty
            
        # Update Stage 2 (Standard)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 with the penalized outcome value
        delta_1 = outcome_val - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```