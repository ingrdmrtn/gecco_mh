```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition probabilities
        self.trans_counts = np.array([[35, 15], [15, 35]]) # Prior counts for transitions, if needed for learning
        self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True) # Transition probabilities, if needed for direct use
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    """Create function interface from model class."""
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates the eligibility trace (lambda) in reinforcement learning.
    High anxiety impairs the ability to maintain a long-term credit assignment trace,
    effectively reducing 'lambda'. This makes the participant rely more on the immediate
    transition value (TD(0)) rather than the full path outcome (TD(1)).
    
    The effective lambda is calculated as: lambda_eff = lambda_base * (1 - stai).
    High STAI -> Lower Lambda -> Less credit assigned to Stage 1 choice based on Stage 2 reward.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    lambda_base: [0, 1] # Base eligibility trace parameter
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_base = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective lambda based on anxiety
        # High anxiety reduces the eligibility trace
        lambda_eff = self.lambda_base * (1.0 - self.stai)
        lambda_eff = np.clip(lambda_eff, 0.0, 1.0)

        # Stage 2 update (Standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (TD(lambda))
        # The update includes the standard TD error from the transition (delta_1)
        # PLUS the eligibility trace component from the second stage reward (delta_2)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        self.q_stage1[action_1] += self.alpha * delta_1 + self.alpha * lambda_eff * delta_2

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates the balance between Model-Based (MB) and Model-Free (MF) control.
    High anxiety acts as a cognitive load, depleting the resources required for MB planning.
    Therefore, high STAI scores reduce the weight (w) assigned to the MB system, leading
    to more habitual (MF) behavior.

    The mixing weight is: w_eff = w_max * (1 - stai).
    High STAI -> Lower w_eff -> More Model-Free behavior.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate for MF values
    beta: [0, 10]       # Inverse temperature
    w_max: [0, 1]       # Maximum weight for Model-Based system (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate Model-Based values on the fly
        # Q_MB(a1) = sum( P(s2|a1) * max(Q2(s2)) )
        q_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            # self.T[a1] is the probability distribution over states given action a1
            # We compute expected value of the best action at the next state
            expected_val = 0
            for s2 in range(self.n_states):
                expected_val += self.T[a1, s2] * np.max(self.q_stage2[s2])
            q_mb[a1] = expected_val

        # Calculate effective mixing weight
        w_eff = self.w_max * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0.0, 1.0)

        # Combine MB and MF values
        q_net = w_eff * q_mb + (1.0 - w_eff) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety causes asymmetric learning from negative outcomes (suppression).
    High anxiety participants may exhibit a form of avoidance or denial where they 
    down-weight negative prediction errors (losses or lower-than-expected rewards).
    This leads to 'stickiness' because the value of a chosen option degrades very slowly
    even when it yields poor returns.

    The learning rate for negative prediction errors is scaled down by STAI:
    alpha_neg = alpha / (1 + suppression * stai).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate (for positive errors)
    beta: [0, 10]       # Inverse temperature
    suppression: [0, 10]# Strength of negative learning suppression due to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.suppression = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate asymmetric learning rates
        alpha_pos = self.alpha
        alpha_neg = self.alpha / (1.0 + self.suppression * self.stai)
        
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = alpha_pos if delta_2 >= 0 else alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = alpha_pos if delta_1 >= 0 else alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```