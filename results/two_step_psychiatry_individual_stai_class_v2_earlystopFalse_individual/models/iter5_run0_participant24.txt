Here are 3 new cognitive models exploring different mechanisms for how anxiety (STAI) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based Control
This model tests the hypothesis that anxiety interferes with complex, model-based planning. In the two-step task, "model-based" behavior involves using the transition structure (which spaceship goes to which planet) to update values. "Model-free" behavior just repeats what was rewarded. This model proposes that higher anxiety reduces the weight ($w$) given to the model-based system, making the participant rely more on simple model-free reinforcement.

```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety reduces Model-Based control (reduces 'w').
    
    This model implements a hybrid Model-Based (MB) / Model-Free (MF) reinforcement learning agent.
    The weighting parameter 'w' determines the balance between MB and MF systems.
    We hypothesize that anxiety (STAI) acts as a cognitive load or distraction, 
    reducing the capacity for MB planning.
    
    w_effective = w_max * (1 - (stai_factor * stai))
    
    If stai_factor is positive, higher anxiety leads to lower 'w' (more model-free).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_max: [0, 1]       # Maximum model-based weight (at 0 anxiety)
    stai_factor: [0, 2] # Sensitivity of w to STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.stai_factor = model_parameters

    def init_model(self) -> None:
        # MF values for stage 1
        self.q_mf_stage1 = np.zeros(self.n_choices)
        # MB values are computed on the fly, but we need transition matrix
        # We assume fixed transition probabilities as per task description (70/30)
        self.T = np.array([[0.7, 0.3], [0.3, 0.7]]) 

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values for Stage 1
        # Q_MB(s1, a) = sum(T(s1, a, s2) * max(Q_stage2(s2, :)))
        # self.q_stage2 holds the values of the second stage states (aliens)
        max_q_stage2 = np.max(self.q_stage2, axis=1) # Max value for each planet
        q_mb_stage1 = self.T @ max_q_stage2

        # 2. Calculate Effective w
        # Clip w to be between 0 and 1
        w_eff = self.w_max * (1.0 - (self.stai_factor * self.stai))
        w_eff = np.clip(w_eff, 0.0, 1.0)

        # 3. Combine MF and MB values
        # self.q_mf_stage1 tracks the direct reinforcement of stage 1 choices
        q_net = (1 - w_eff) * self.q_mf_stage1 + w_eff * q_mb_stage1
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for Stage 2 (Model-Free)
        # This is shared by both systems
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 Model-Free values
        # TD(1) style: update stage 1 based on the final reward
        delta_1 = reward - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven Loss Aversion
This model hypothesizes that anxiety modulates how strongly negative outcomes (losses) are weighted compared to positive outcomes. Specifically, anxious individuals might learn more drastically from a lack of reward (0 coins), treating it as a significant "loss" signal, or conversely, they might be hyper-sensitive to rewards to alleviate anxiety. We implement this by having separate learning rates for positive and negative prediction errors, where the negative learning rate is modulated by STAI.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety modulates learning from negative prediction errors (Loss Aversion/Sensitivity).
    
    We split the learning rate alpha into alpha_pos (for RPE > 0) and alpha_neg (for RPE < 0).
    The negative learning rate is modulated by STAI.
    
    alpha_neg_eff = alpha_neg_base + (stai_sens * stai)
    
    If stai_sens is positive, anxious people update their values more strongly 
    after a disappointment (getting less than expected).

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      # Learning rate for positive RPEs
    alpha_neg_base: [0, 1] # Base learning rate for negative RPEs
    beta: [0, 10]          # Inverse temperature
    stai_sens: [-1, 1]     # How STAI modulates alpha_neg
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.beta, self.stai_sens = model_parameters

    def get_alpha(self, delta: float) -> float:
        if delta >= 0:
            return self.alpha_pos
        else:
            alpha_neg = self.alpha_neg_base + (self.stai_sens * self.stai)
            return np.clip(alpha_neg, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.get_alpha(delta_2)
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Using the updated stage 2 value to drive stage 1 (SARSA-like or TD(0))
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.get_alpha(delta_1)
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced Choice Stickiness (Perseveration)
This model tests the hypothesis that anxiety leads to rigid, repetitive behavior (perseveration) regardless of reward history. This is distinct from learning; it's a tendency to repeat the last motor action (choice of spaceship) simply because it was just performed. We model this as a "stickiness" bonus added to the Q-value of the previously chosen option, where the magnitude of this bonus is scaled by the STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety increases choice perseveration (stickiness).
    
    Anxious individuals may exhibit repetitive behaviors or 'stick' to a choice 
    to reduce cognitive load or uncertainty, regardless of the outcome.
    We add a 'stickiness' bonus to the Q-value of the previously chosen action 
    at Stage 1.
    
    stickiness_bonus = stick_base + (stick_stai * stai)
    
    Q_net(a) = Q_stage1(a) + (stickiness_bonus if a == last_action1 else 0)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    stick_base: [-2, 2] # Base stickiness (positive = repeat, negative = switch)
    stick_stai: [-2, 2] # How STAI modulates stickiness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.stick_base, self.stick_stai = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective stickiness
        stick_eff = self.stick_base + (self.stick_stai * self.stai)
        
        # Copy Q-values to avoid modifying the learned values permanently
        q_vals = self.q_stage1.copy()
        
        # Add stickiness bonus if there was a previous action
        if self.last_action1 is not None:
            q_vals[int(self.last_action1)] += stick_eff
            
        return self.softmax(q_vals, self.beta)

    # Standard policy_stage2 (no stickiness assumed for aliens usually, or kept simple)
    # Standard value_update

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```