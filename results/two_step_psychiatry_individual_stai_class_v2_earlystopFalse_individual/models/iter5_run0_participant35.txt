```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition probabilities
        self.trans_counts = np.array([[35, 15], [15, 35]]) # Prior counts for transitions, if needed for learning
        self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True) # Transition probabilities, if needed for direct use
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    """Create function interface from model class."""
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety accelerates the forgetting of unchosen options (Memory Decay).
    
    High anxiety impairs working memory maintenance. In this model, the value of the 
    unchosen spaceship decays towards zero on every trial. This can lead to 
    perseveration on the chosen option if the alternative's value decays faster 
    than the chosen option's value drops (even with occasional losses).
    
    decay = decay_scale * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    decay_scale: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_scale = model_parameters

    def init_model(self) -> None:
        self.decay = self.decay_scale * self.stai

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard update for chosen path
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Decay for the unchosen Stage 1 option
        unchosen_action = 1 - action_1
        self.q_stage1[unchosen_action] *= (1.0 - self.decay)

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces pessimistic initial expectations.
    
    Anxious individuals often anticipate negative outcomes. This model initializes 
    Q-values based on anxiety level. Higher anxiety leads to lower initial values (pessimism).
    This creates a dynamic where the first positive reward generates a large positive 
    prediction error, potentially locking in a preference for the first successful choice 
    more strongly than a neutral initialization would.
    
    Q_init = 0.5 * (1 - pessimism * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    pessimism: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pessimism = model_parameters

    def init_model(self) -> None:
        # Calculate initial value. 
        # If pessimism=0 or stai=0, init=0.5 (neutral).
        # If pessimism=1 and stai=1, init=0.0 (pessimistic).
        init_val = 0.5 * (1.0 - self.pessimism * self.stai)
        
        self.q_stage1 = np.full(self.n_choices, init_val)
        self.q_stage2 = np.full((self.n_states, self.n_choices), init_val)

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates subjective loss aversion for non-rewarding outcomes.
    
    In the standard task, 0 coins is a neutral outcome. This model hypothesizes that 
    anxious participants perceive 0 coins as a negative outcome (punishment/loss).
    The magnitude of this subjective loss is modulated by anxiety.
    
    If reward == 0: effective_reward = -1 * loss_sensitivity * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    loss_sensitivity: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_sensitivity = model_parameters

    def init_model(self) -> None:
        self.penalty = self.loss_sensitivity * self.stai

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Transform reward
        if reward < 1e-5: # Effectively 0
            r_eff = -self.penalty
        else:
            r_eff = reward
            
        # Update using effective reward
        delta_2 = r_eff - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```