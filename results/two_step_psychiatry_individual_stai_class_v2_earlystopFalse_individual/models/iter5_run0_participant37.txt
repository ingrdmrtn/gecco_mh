```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition probabilities
        self.trans_counts = np.array([[35, 15], [15, 35]]) # Prior counts for transitions, if needed for learning
        self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True) # Transition probabilities, if needed for direct use
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    """Create function interface from model class."""
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Induced Risk Aversion (Mean-Variance Tradeoff).
    Anxious individuals are often characterized by risk aversion. In this model, 
    the participant tracks not just the expected value (Q) of the stage-2 options, 
    but also the variance of the rewards received.
    
    When making a choice, the utility of an option is penalized by its variance, 
    scaled by the participant's anxiety level.
    Utility = Q - (risk_penalty * stai) * Variance
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    risk_penalty: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.risk_penalty = model_parameters

    def init_model(self) -> None:
        # Initialize variance tracking for Stage 2 (2 states x 2 choices)
        # Initial variance is 0.25 (assuming binary 0/1 rewards with p=0.5 start)
        self.var_stage2 = 0.25 * np.ones((self.n_states, self.n_choices))

    def policy_stage2(self, state: int) -> np.ndarray:
        # Calculate risk-adjusted utility
        # Penalty scales with anxiety and the observed variance of the option
        penalty = self.risk_penalty * self.stai * self.var_stage2[state]
        utility = self.q_stage2[state] - penalty
        return self.softmax(utility, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Variance estimate for Stage 2
        # Var_new = Var_old + alpha * ((Reward - Q_old)^2 - Var_old)
        # This approximates the running variance
        sq_error = delta_2 ** 2
        self.var_stage2[state, action_2] += self.alpha * (sq_error - self.var_stage2[state, action_2])
        
        # Standard TD update for Stage 1 (Model-Free)
        # Note: Stage 1 doesn't have direct variance tracking in this simple implementation,
        # relying on the propagated value from Stage 2.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Dependent Distal Noise.
    Anxiety consumes cognitive resources (working memory), which disproportionately 
    affects complex, distal planning (Stage 1) while sparing simpler, proximal 
    reactions (Stage 2).
    
    Mechanism:
    The inverse temperature (beta) for Stage 1 is reduced (noisier) as anxiety increases.
    The inverse temperature for Stage 2 remains the baseline beta.
    
    beta_stage1 = beta / (1 + noise_factor * stai)
    beta_stage2 = beta
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    noise_factor: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.noise_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective beta for stage 1
        # High anxiety -> High denominator -> Low beta -> More random choice
        beta_s1 = self.beta / (1.0 + self.noise_factor * self.stai)
        return self.softmax(self.q_stage1, beta_s1)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Stage 2 uses the baseline beta (proximal choice is less affected)
        return self.softmax(self.q_stage2[state], self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Dynamic Transition Beliefs.
    Anxious individuals may perceive the environment as more volatile. Instead of 
    assuming fixed transition probabilities for the spaceships, they update their 
    internal model of the transitions (A->X, A->Y) rapidly based on outcomes.
    
    Mechanism:
    The participant uses a Model-Based strategy for Stage 1.
    The transition matrix T is updated after every trial.
    The learning rate for T scales with anxiety: lr_T = t_learn_factor * stai.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1] (for reward value learning)
    beta: [0, 10]
    t_learn_factor: [0, 1] (scales the transition learning rate)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.t_learn_factor = model_parameters

    def init_model(self) -> None:
        # Start with the base class transition probabilities (0.7/0.3)
        # We will update self.T dynamically
        pass

    def policy_stage1(self) -> np.ndarray:
        # Pure Model-Based Control for Stage 1
        # Calculate expected value of each spaceship based on current T and Stage 2 values
        # V(state) = max(Q_stage2[state])
        v_states = np.max(self.q_stage2, axis=1) # [V(X), V(Y)]
        
        # Q_MB(action) = T(X|action)*V(X) + T(Y|action)*V(Y)
        q_mb = self.T @ v_states
        
        return self.softmax(q_mb, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # 1. Update Reward Values (Stage 2)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # 2. Update Transition Model (Dynamic World View)
        # Learning rate for transitions depends on anxiety
        lr_t = self.t_learn_factor * self.stai
        
        # Update the row corresponding to the chosen spaceship (action_1)
        # If we went to 'state', probability of that transition increases
        # T[a1, state] += lr * (1 - T[a1, state])
        # T[a1, other] += lr * (0 - T[a1, other])
        
        # Current probs
        p_observed = self.T[action_1, state]
        
        # Update towards 1.0 for the observed state
        self.T[action_1, state] += lr_t * (1.0 - p_observed)
        
        # Update towards 0.0 for the unobserved state
        other_state = 1 - state
        self.T[action_1, other_state] += lr_t * (0.0 - self.T[action_1, other_state])

        # Note: We do not update q_stage1 (MF value) as this model relies on MB for Stage 1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```