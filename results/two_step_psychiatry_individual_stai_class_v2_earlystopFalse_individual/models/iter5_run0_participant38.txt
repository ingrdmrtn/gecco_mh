Here are 3 new cognitive models that explore different mechanisms for how anxiety (STAI) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based vs. Model-Free Control
This model tests the hypothesis that anxiety shifts the balance between model-based (planning) and model-free (habitual) control. High anxiety might impair cognitive resources required for model-based planning, leading to a greater reliance on model-free learning.

```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    The weighting parameter 'w' determines the mix: Q_net = w * Q_MB + (1-w) * Q_MF.
    Here, 'w' is a logistic function of STAI: w = 1 / (1 + exp(-slope * (STAI - 0.4))).
    We fit the 'slope' parameter. If slope is negative, higher anxiety reduces MB control.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    slope: [-10, 10]    # Sensitivity of MB-weighting to STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.slope = model_parameters

    def init_model(self) -> None:
        # Initialize transition model (counts)
        self.trans_counts = np.array([[0.0, 0.0], [0.0, 0.0]]) 
        # We need separate Q-values for MF learning
        self.q_mf = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values
        # Estimate transition probs: T(s'|a)
        row_sums = self.trans_counts.sum(axis=1, keepdims=True)
        # Add small prior to avoid division by zero
        T_est = (self.trans_counts + 1.0) / (row_sums + 2.0)
        
        # Q_MB(a) = sum_s' T(s'|a) * max_a' Q_stage2(s', a')
        max_q2 = np.max(self.q_stage2, axis=1) # Max value of each state
        q_mb = T_est @ max_q2
        
        # 2. Compute Mixing Weight w based on STAI
        # Logistic function centered near typical STAI mean (approx 0.4)
        # w represents the weight of Model-Based control
        w = 1.0 / (1.0 + np.exp(-self.slope * (self.stai - 0.4)))
        
        # 3. Combine
        q_net = w * q_mb + (1 - w) * self.q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 values (common to both MB and MF)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Model-Free Stage 1 values (TD(1) like update)
        delta_1 = self.q_stage2[state, action_2] - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1
        
        # Update Transition Model
        # Map action_1 (0 or 1) to row index. 
        # Assuming action 0 -> row 0, action 1 -> row 1 is the structure of trans_counts
        self.trans_counts[action_1, state] += 1.0

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry (Pos/Neg Bias)
This model hypothesizes that anxiety affects how participants learn from positive versus negative prediction errors. Anxious individuals might be hypersensitive to negative outcomes (punishment) or hyposensitive to rewards.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the learning rate specifically for negative prediction errors.
    We have a base learning rate 'alpha'.
    For negative prediction errors (RPE < 0), the effective learning rate is alpha * multiplier.
    The multiplier is derived from STAI: multiplier = 1 + rho * STAI.
    If rho > 0, anxiety amplifies learning from bad outcomes.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate (for positive RPEs)
    beta: [0, 10]       # Inverse temperature
    rho: [-5, 5]        # Anxiety modulation of negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.rho = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate negative learning rate modifier
        neg_lr_mod = 1.0 + self.rho * self.stai
        # Ensure it stays non-negative roughly, though simple clipping is safer
        neg_lr_mod = max(0.0, neg_lr_mod)

        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        
        if delta_2 >= 0:
            eff_alpha_2 = self.alpha
        else:
            eff_alpha_2 = self.alpha * neg_lr_mod
            # Clip to max 1.0 to maintain stability
            eff_alpha_2 = min(1.0, eff_alpha_2)
            
        self.q_stage2[state, action_2] += eff_alpha_2 * delta_2
        
        # --- Stage 1 Update ---
        # Using the updated Q2 value for the TD error
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 >= 0:
            eff_alpha_1 = self.alpha
        else:
            eff_alpha_1 = self.alpha * neg_lr_mod
            eff_alpha_1 = min(1.0, eff_alpha_1)

        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration (Inverse Temperature Modulation)
This model suggests that anxiety influences the randomness of choice (exploration vs. exploitation). High anxiety might lead to more erratic behavior (high noise/exploration) or rigid behavior (low noise).

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the inverse temperature (beta), affecting exploration/exploitation balance.
    Beta is not constant but a function of STAI.
    beta_eff = beta_base * exp(tau * STAI)
    
    If tau < 0: Higher anxiety -> Lower beta -> More random/exploratory choices.
    If tau > 0: Higher anxiety -> Higher beta -> More deterministic/rigid choices.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_base: [0, 10]  # Baseline inverse temperature
    tau: [-5, 5]        # Anxiety modulation coefficient
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.tau = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective beta
        beta_eff = self.beta_base * np.exp(self.tau * self.stai)
        # Cap beta to prevent overflow issues
        beta_eff = min(beta_eff, 20.0)
        
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Calculate effective beta
        beta_eff = self.beta_base * np.exp(self.tau * self.stai)
        beta_eff = min(beta_eff, 20.0)
        
        return self.softmax(self.q_stage2[state], beta_eff)

    # Standard value update
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```