```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates strong initial priors (prejudice) towards specific options, 
    which must be unlearned. High anxiety participants start with a skewed belief 
    about the value of the spaceships, rather than a neutral initialization.
    
    This model posits that anxiety manifests as a "jump to conclusions" bias before 
    the task even begins, creating a strong initial preference that resists change 
    until sufficient evidence (via alpha) accumulates to overcome it.

    Mechanism:
    The initial Q-values for Stage 1 are not 0, but are skewed based on STAI.
    Q_stage1[0] = init_bias * STAI
    Q_stage1[1] = -init_bias * STAI
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    init_bias: [-1, 1] # Positive values bias towards option 0, negative towards 1
    """
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.init_bias = model_parameters

    def init_model(self) -> None:
        # Override initialization of Q-values with anxiety-driven bias
        bias_amount = self.init_bias * self.stai
        self.q_stage1 = np.array([bias_amount, -bias_amount])
        # Stage 2 remains neutral as the participant has no prior info on aliens
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety imposes a global cognitive load that degrades working memory 
    for all value representations over time. Unlike "attentional narrowing" which 
    affects only unchosen options, this hypothesis suggests anxiety causes a 
    generalized rapid decay of all learned associations (both Stage 1 and Stage 2).
    
    Mechanism:
    At the end of every trial, ALL Q-values (Stage 1 and Stage 2) decay towards 
    zero based on the STAI score.
    
    Q(t+1) = Q(t) * (1 - (decay_rate * STAI))

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    decay_rate: [0, 1]
    """
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_rate = model_parameters

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Calculate decay factor modulated by anxiety
        decay = self.decay_rate * self.stai
        decay = np.clip(decay, 0, 1)
        
        # Apply global decay to all value representations
        self.q_stage1 *= (1.0 - decay)
        self.q_stage2 *= (1.0 - decay)

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety leads to a narrowing of temporal focus (myopia). Anxious 
    participants prioritize learning about immediate outcomes (Stage 2 aliens) 
    over distal predictors (Stage 1 spaceships).
    
    This results in a mismatch where the participant learns the value of the aliens 
    very quickly (high Stage 2 learning rate) but updates their choice of spaceship 
    slowly (standard Stage 1 learning rate).
    
    Mechanism:
    The learning rate for Stage 2 (alpha_2) is amplified by STAI relative to 
    the base learning rate (alpha).
    
    alpha_stage2 = alpha * (1 + focus_scale * STAI)
    alpha_stage1 = alpha

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    focus_scale: [0, 5] # Allows Stage 2 learning to be significantly higher
    """
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.focus_scale = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate stage-specific learning rate for Stage 2
        # We clip to 1.0 to ensure stability
        alpha_2 = self.alpha * (1.0 + self.focus_scale * self.stai)
        alpha_2 = np.clip(alpha_2, 0, 1)
        
        # Stage 2 Update (Immediate outcome) - uses amplified alpha
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update (Distal predictor) - uses base alpha
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```