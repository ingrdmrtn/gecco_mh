### Model 1: Anxiety-Induced Loss Aversion
```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety-Induced Loss Aversion.
    High anxiety individuals perceive the absence of reward (0 coins) as a 
    subjective loss or punishment, rather than a neutral outcome. 
    
    This model modifies the reward signal used for value updates. If the 
    outcome is 0 coins, the effective reward becomes negative, with a magnitude 
    scaled by the participant's STAI score. This drives the agent to avoid 
    non-rewarding options more aggressively than a standard agent.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]             # Learning rate
    beta: [0, 10]             # Inverse temperature
    loss_sensitivity: [0, 5]  # Magnitude of subjective loss for 0-reward outcomes
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.loss_sensitivity = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective reward
        # If reward is 1, use 1. If 0, use a negative value proportional to anxiety.
        effective_reward = reward
        if reward == 0:
            effective_reward = -1.0 * self.loss_sensitivity * self.stai

        # Standard TD update with effective reward
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Memory Decay
```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety-Modulated Memory Decay.
    High anxiety consumes cognitive resources (working memory load), leading to 
    faster forgetting of the values of unchosen options.
    
    On each trial, the Q-values of the unchosen stage-1 option decay towards 0 
    at a rate determined by the STAI score. This can explain perseveration 
    (if the alternative is forgotten/devalued) or switching (if the current 
    option's value drops below the decaying alternative).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    decay_scale: [0, 1] # Scaling factor for anxiety-dependent decay
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.decay_scale = model_parameters

    def pre_trial(self) -> None:
        # Decay unchosen options from the previous trial
        if self.last_action1 is not None:
            unchosen_action = 1 - self.last_action1
            # Decay rate is proportional to anxiety
            decay_rate = np.clip(self.decay_scale * self.stai, 0.0, 1.0)
            # Decay value towards 0
            self.q_stage1[unchosen_action] *= (1.0 - decay_rate)

    # Standard value update (override required to prevent using base class default if needed, 
    # but here we just use standard TD, so we can rely on base or explicit it for clarity)
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Eligibility Traces
```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety-Modulated Eligibility Traces (Direct Reinforcement).
    High anxiety promotes habitual, model-free learning where the stage-1 choice 
    is directly reinforced by the final outcome, bypassing the stage-2 value estimation.
    
    This is implemented via an eligibility trace parameter (lambda) that scales with STAI.
    Higher STAI -> Higher lambda -> More direct reinforcement (TD(1)-like behavior),
    meaning the agent learns 'Action 1 -> Reward' directly, ignoring the intermediate state.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]         # Learning rate
    beta: [0, 10]         # Inverse temperature
    lambda_scale: [0, 1]  # Scaling factor for eligibility trace based on STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lambda_scale = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate lambda based on STAI, clipped to [0, 1]
        lambda_val = np.clip(self.lambda_scale * self.stai, 0.0, 1.0)
        
        # Store current values before update
        q2_val = self.q_stage2[state, action_2]
        q1_val = self.q_stage1[action_1]
        
        # Stage 2 Prediction Error
        delta_2 = reward - q2_val
        
        # Stage 1 Prediction Error (TD(0))
        delta_1 = q2_val - q1_val
        
        # Update Stage 2
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 with eligibility trace
        # Q1 += alpha * (TD_error_1 + lambda * TD_error_2)
        self.q_stage1[action_1] += self.alpha * (delta_1 + lambda_val * delta_2)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```