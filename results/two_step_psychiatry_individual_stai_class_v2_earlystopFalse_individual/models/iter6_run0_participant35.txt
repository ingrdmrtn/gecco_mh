```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety promotes reactive "Win-Stay, Lose-Shift" (WSLS) heuristics over value-based learning.
    
    High anxiety consumes cognitive resources, leading participants to rely on simple, 
    reactive heuristics at the first stage rather than integrating value history.
    This model adds a heuristic bias to the Stage 1 policy.
    If the previous trial was rewarded, the bias favors repeating the choice (Win-Stay).
    If unrewarded, the bias favors switching (Lose-Shift).
    The strength of this heuristic bias is modulated by STAI.
    
    w_heuristic = w_wsls * stai
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    w_wsls: [0, 5]   # Scaling factor for the WSLS heuristic based on anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_wsls = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate heuristic vector
        heuristic = np.zeros(self.n_choices)
        
        # Only apply heuristic if there is a previous trial to react to
        if self.last_action1 is not None:
            prev_a = int(self.last_action1)
            if self.last_reward > 0:
                # Win-Stay: Bonus to previous action
                heuristic[prev_a] = 1.0
            else:
                # Lose-Shift: Penalty to previous action (equivalent to bonus for other)
                heuristic[prev_a] = -1.0
        
        # Calculate effective weight based on anxiety
        w_eff = self.w_wsls * self.stai
        
        # Combine learned Q-values with reactive heuristic
        combined_values = self.q_stage1 + w_eff * heuristic
        
        return self.softmax(combined_values, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates a "Negative Learning Bias" (Pessimism).
    
    Anxious individuals may be hypersensitive to negative outcomes (omission of reward).
    This model posits that anxiety amplifies the learning rate specifically when 
    prediction errors are negative (outcomes are worse than expected).
    
    alpha_neg = alpha * (1 + neg_bias * stai)
    alpha_pos = alpha
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    neg_bias: [0, 5] # Multiplier for learning from negative prediction errors
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.neg_bias = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine alpha based on sign of prediction error
        if delta_2 < 0:
            alpha_eff_2 = self.alpha * (1.0 + self.neg_bias * self.stai)
        else:
            alpha_eff_2 = self.alpha
            
        self.q_stage2[state, action_2] += alpha_eff_2 * delta_2
        
        # Stage 1 Update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        if delta_1 < 0:
            alpha_eff_1 = self.alpha * (1.0 + self.neg_bias * self.stai)
        else:
            alpha_eff_1 = self.alpha
            
        self.q_stage1[action_1] += alpha_eff_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety causes "Tunnel Vision," impairing distal learning.
    
    Anxiety narrows attentional focus to immediate cues (Stage 2 state -> Reward) 
    while impairing the ability to credit distal choices (Stage 1 action -> Stage 2 state).
    This model implements a reduced learning rate for Stage 1 updates proportional to anxiety,
    while Stage 2 learning remains intact.
    
    alpha_stage1 = alpha * (1 - tunnel_factor * stai)
    alpha_stage2 = alpha
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    tunnel_factor: [0, 1] # Degree to which anxiety suppresses Stage 1 learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.tunnel_factor = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update (Immediate context) - Unaffected
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Update (Distal context) - Suppressed by anxiety
        # Calculate effective alpha for stage 1
        suppression = self.tunnel_factor * self.stai
        # Ensure alpha doesn't go negative
        alpha_1_eff = self.alpha * max(0.0, (1.0 - suppression))
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_1_eff * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```