Here are 3 new cognitive models exploring different mechanisms for how anxiety (STAI) might influence decision-making in this task, specifically addressing the observed perseveration and reward processing.

### Model 1: Anxiety-Modulated Learning Asymmetry
This model hypothesizes that anxiety affects how participants learn from positive versus negative prediction errors. Specifically, it proposes that anxiety modulates the learning rate for negative outcomes (disappointments), potentially making the participant more sensitive to losses or lack of reward, or conversely, ignoring them to maintain a "safe" default strategy. Given the high repetition of choice 1 despite mixed rewards, this model tests if anxiety suppresses learning from negative outcomes (low `alpha_neg`), effectively locking the participant into their initial preference.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the asymmetry between learning from positive and negative prediction errors.
    The participant has a base learning rate for positive outcomes, but their learning rate for 
    negative outcomes (getting 0 coins) is scaled by their anxiety level. 
    
    If the scaling factor is low, high anxiety might suppress learning from failures (ignoring 0s), 
    leading to persistence. If high, it might cause over-reaction.
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]      # Learning rate for positive prediction errors
    neg_scale_base: [0, 5] # Base scaling factor for negative learning rate
    beta: [0, 10]          # Inverse temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.neg_scale_base, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate Stage 2 Prediction Error
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine learning rate based on sign of PE and anxiety
        # We model the negative learning rate as dependent on STAI
        # alpha_neg = alpha_pos * neg_scale_base * (1 +/- STAI effect)
        # Here we try: alpha_neg is modulated by STAI directly.
        # A simple interaction: alpha_neg = alpha_pos * neg_scale_base * stai
        # This implies anxiety changes the ratio of positive/negative learning.
        
        alpha_neg = self.alpha_pos * self.neg_scale_base * self.stai
        
        # Clip alpha_neg to be reasonable [0, 1]
        alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
        
        eff_alpha_2 = self.alpha_pos if delta_2 >= 0 else alpha_neg
        self.q_stage2[state, action_2] += eff_alpha_2 * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        eff_alpha_1 = self.alpha_pos if delta_1 >= 0 else alpha_neg
        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Driven "Safe Option" Bias
This model hypothesizes that anxiety creates a prior bias towards specific options perceived as "safe" or "default" before any learning occurs. Instead of just perseverating on the *last* choice, the participant might have a fixed bias towards Spaceship A (Action 1) that is amplified by their anxiety level. This models a trait-like avoidance of exploration where high anxiety leads to a stronger adherence to a pre-selected strategy (Action 1), treating it as the "correct" or "safe" choice regardless of trial-by-trial history.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces a static bias towards a 'default' or 'safe' option (Action 1).
    This is not history-dependent perseveration, but a fixed bias added to the Q-value of Action 1.
    The magnitude of this bias is determined by the anxiety score (STAI).
    
    This explains the data if the participant views Spaceship 1 as the 'correct' task instruction 
    or safer bet and anxiety makes them rigid in this belief.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    bias_factor: [0, 10]# Multiplier for STAI to create the bias magnitude
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.bias_factor = model_parameters

    def policy_stage1(self) -> np.ndarray:
        values = self.q_stage1.copy()
        
        # Add a static bias to Action 1 (index 1 in 0-indexed array? No, usually 0 or 1)
        # The data shows choices of "1.0". Assuming 0-indexed actions 0 and 1.
        # If data says "1.0", that usually maps to index 1.
        # Let's assume the participant prefers Action 1 (index 1).
        
        # Bias magnitude scales with anxiety
        bias = self.bias_factor * self.stai
        
        # Apply bias to Action 1. 
        # Note: If the data implies Action 1 is the preferred one, we boost index 1.
        values[1] += bias
        
        return self.softmax(values, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Induced Inverse Temperature (Exploration/Exploitation) Modulation
This model hypothesizes that anxiety directly affects the exploration-exploitation trade-off parameter ($\beta$). Instead of affecting values or learning rates, anxiety might make the participant's choice policy more deterministic (stiff). A higher anxiety score leads to a higher $\beta$, causing the participant to exploit their current best option (or initial guess) more rigidly and explore less. This "freezing" behavior is a common response to anxiety.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the inverse temperature (beta) parameter.
    Higher anxiety leads to 'stiff' behavior (higher beta), reducing exploration 
    and causing the participant to stick rigidly to the option with slightly higher value.
    
    We model beta as a baseline plus an anxiety-dependent component.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]        # Learning rate
    beta_base: [0, 10]   # Baseline inverse temperature
    beta_stai_slope: [0, 20] # How much STAI increases stiffness
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.beta_stai_slope = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective beta based on anxiety
        # High anxiety -> High beta -> Deterministic choice
        effective_beta = self.beta_base + (self.beta_stai_slope * self.stai)
        
        return self.softmax(self.q_stage1, effective_beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Apply the same anxiety-modulated stiffness to stage 2
        effective_beta = self.beta_base + (self.beta_stai_slope * self.stai)
        return self.softmax(self.q_stage2[state], effective_beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```