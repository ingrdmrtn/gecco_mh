Here are the 3 new cognitive models.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Gated Rare Transition Learning.
    High anxiety individuals may filter out "surprising" or rare events to maintain 
    cognitive stability or reduce processing of complex probabilistic structures. 
    When a rare transition occurs (e.g., Spaceship A -> Planet Y), the participant 
    dampens the learning update for the Stage 1 choice, effectively treating the 
    outcome as an outlier that shouldn't affect the value of the chosen spaceship.

    Mechanism:
    If the transition was rare (probability < 0.5):
        alpha_stage1 = alpha * (1 - gate_strength * stai)
    Else:
        alpha_stage1 = alpha
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    gate_strength: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.gate_strength = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard update for Stage 2 (Alien value)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Determine if transition was rare
        # Transition matrix T is [[0.7, 0.3], [0.3, 0.7]]
        # action_1=0 (A) usually goes to state=0 (X)
        # action_1=1 (U) usually goes to state=1 (Y)
        is_common = (action_1 == state) 
        
        # Calculate effective alpha for Stage 1
        if is_common:
            eff_alpha = self.alpha
        else:
            # Rare transition: dampen learning based on anxiety
            eff_alpha = self.alpha * (1.0 - self.gate_strength * self.stai)
            # Ensure non-negative
            eff_alpha = max(0.0, eff_alpha)

        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += eff_alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven Win-Stay Bias.
    Anxious participants may exhibit a specific "safety behavior" where they 
    compulsively repeat choices that recently led to a reward, regardless of 
    the underlying value estimation. This is a specific form of perseveration 
    triggered only by positive outcomes (Win-Stay), driven by the relief of 
    avoiding a negative outcome.

    Mechanism:
    In Stage 1 policy calculation:
    If last_reward == 1.0:
        Add (ws_strength * stai) to the Q-value of the previously chosen action.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    ws_strength: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.ws_strength = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy Q-values to avoid modifying the actual value estimates
        logits = self.q_stage1.copy()
        
        # Apply Win-Stay bonus if applicable
        if self.last_reward == 1.0 and self.last_action1 is not None:
            bonus = self.ws_strength * self.stai
            logits[int(self.last_action1)] += bonus
            
        return self.softmax(logits, self.beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Stage-Split Learning.
    Anxiety may create a dissociation between learning from immediate, concrete outcomes 
    (Stage 2: Aliens giving gold) and abstract, predictive cues (Stage 1: Spaceships). 
    This model hypothesizes that high anxiety dampens the learning rate for the 
    distal predictor (Stage 1) relative to the proximal outcome (Stage 2), 
    reflecting a deficit in credit assignment to earlier causes.

    Mechanism:
    alpha_stage2 = alpha
    alpha_stage1 = alpha * (1 - s1_damp * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    s1_damp: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.s1_damp = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (Proximal/Concrete) - uses base alpha
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (Distal/Abstract) - uses dampened alpha
        eff_alpha_1 = self.alpha * (1.0 - self.s1_damp * self.stai)
        eff_alpha_1 = max(0.0, eff_alpha_1)
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```