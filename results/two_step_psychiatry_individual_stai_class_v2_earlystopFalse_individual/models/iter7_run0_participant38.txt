Here are three new cognitive models that hypothesize different ways anxiety (STAI) modulates decision-making in this task.

### Model 1: Anxiety-Modulated Loss Learning (Valence Asymmetry)
This model tests the hypothesis that anxiety specifically alters how participants learn from negative outcomes (disappointments). High anxiety might lead to "over-learning" from failure (hypersensitivity to punishment) or "ignoring" failure (avoidance).

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning from negative prediction errors.
    While the learning rate for positive outcomes is fixed at 'alpha',
    the learning rate for negative outcomes is scaled by STAI.
    
    If delta < 0: alpha_neg = alpha * (1 + eta * STAI)
    If delta > 0: alpha_pos = alpha
    
    If eta > 0: Anxiety increases sensitivity to losses (punishment hypersensitivity).
    If eta < 0: Anxiety decreases sensitivity to losses (avoidance/denial).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Base learning rate
    beta: [0, 10]       # Inverse temperature
    eta: [-2, 5]        # Anxiety modulation of negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.eta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Determine effective alpha for Stage 2
        if delta_2 < 0:
            eff_alpha_2 = self.alpha * (1 + self.eta * self.stai)
        else:
            eff_alpha_2 = self.alpha
            
        # Clip to ensure stability [0, 1]
        eff_alpha_2 = np.clip(eff_alpha_2, 0.0, 1.0)
        self.q_stage2[state, action_2] += eff_alpha_2 * delta_2
        
        # Stage 1 Update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Determine effective alpha for Stage 1
        if delta_1 < 0:
            eff_alpha_1 = self.alpha * (1 + self.eta * self.stai)
        else:
            eff_alpha_1 = self.alpha
            
        eff_alpha_1 = np.clip(eff_alpha_1, 0.0, 1.0)
        self.q_stage1[action_1] += eff_alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Exploration (Decision Noise)
This model tests the hypothesis that anxiety affects the exploration-exploitation trade-off. Anxious individuals might be more rigid/deterministic (higher beta) to reduce uncertainty, or more erratic/random (lower beta) due to panic or inability to focus.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the inverse temperature (beta) parameter.
    This controls the randomness of choices.
    
    beta_effective = beta_base * exp(mu * STAI)
    
    If mu > 0: Anxiety increases beta (more rigid/deterministic choices).
    If mu < 0: Anxiety decreases beta (more random/erratic choices).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_base: [0, 10]  # Baseline inverse temperature
    mu: [-5, 5]         # Anxiety modulation of temperature
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.mu = model_parameters
        
        # Pre-calculate the effective beta based on STAI
        # Using exp to ensure beta stays positive
        self.beta = self.beta_base * np.exp(self.mu * self.stai)

    # No need to override policy methods as they use self.beta, 
    # which we have already modulated in unpack_parameters.

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Lose-Shift (Reactive Control)
This model tests the hypothesis that anxiety drives a specific reactive heuristic: "Lose-Shift". While standard RL integrates values slowly, anxious participants might have a strong urge to immediately switch away from an option that just yielded no reward, regardless of its long-term value.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates a 'Lose-Shift' heuristic.
    If the previous trial resulted in no reward (0 coins), the participant
    is biased against repeating that Stage 1 choice.
    
    Q_net(a) = Q_stage1(a) - chi * STAI * I(a == last_action && last_reward == 0)
    
    If chi > 0: Anxiety increases the tendency to switch after a loss.
    If chi < 0: Anxiety increases the tendency to stay after a loss (perseveration despite failure).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    chi: [-5, 5]        # Anxiety modulation of Lose-Shift penalty
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.chi = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Copy values to avoid modifying the learned Q-values permanently
        adjusted_values = self.q_stage1.copy()
        
        # Apply Lose-Shift penalty if applicable
        if self.last_action1 is not None and self.last_reward is not None:
            if self.last_reward == 0.0:
                penalty = self.chi * self.stai
                adjusted_values[int(self.last_action1)] -= penalty
        
        return self.softmax(adjusted_values, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```