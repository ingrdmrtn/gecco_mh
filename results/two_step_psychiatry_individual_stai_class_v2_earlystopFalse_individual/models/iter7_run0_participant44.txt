Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this task.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety creates a strong initial prior (bias) for a specific 'safe' option.
    High anxiety individuals enter the task with a preconceived preference (e.g., for Option 0),
    represented by initializing the Q-value of Option 0 to a positive value proportional to STAI.
    This differs from a static policy bias as the value can be unlearned, though it sets a strong starting point
    that explains early perseveration.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    prior_bias: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.prior_bias = model_parameters

    def init_model(self) -> None:
        # Initialize Option 0 with a bias proportional to anxiety
        # This simulates a pre-existing belief that Option 0 is the "safe" choice.
        self.q_stage1[0] = self.prior_bias * self.stai
        # Option 1 remains at 0 (neutral)

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety drives 'Confirmation Bias' via devaluation of unchosen options.
    To reduce cognitive dissonance and decision conflict, anxious individuals actively devalue 
    the option they did *not* choose. This reinforces their current choice path (rigidity), 
    making them less likely to switch even after mediocre outcomes, as the alternative 
    is perceived as increasingly worse.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    devaluation_rate: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.devaluation_rate = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard update for the chosen option
        super().value_update(action_1, state, action_2, reward)
        
        # Devalue the unchosen option
        unchosen_action = 1 - action_1
        # The unchosen option's value drifts downward proportional to anxiety
        self.q_stage1[unchosen_action] -= self.devaluation_rate * self.stai

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety amplifies the subjective value of rewards (Relief Maximization).
    For anxious individuals, receiving a reward (gold coin) provides a 'relief' signal that is 
    subjectively more valuable than for non-anxious individuals. This effectively scales up 
    the Q-values for rewarded actions, leading to higher exploitation (higher effective beta) 
    for winning options, causing the participant to lock in on a rewarding option more aggressively.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    relief_factor: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.relief_factor = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Scale reward based on anxiety
        # If reward is 1, effective_reward > 1. If reward is 0, it stays 0.
        # This creates a larger contrast between winning and losing for anxious participants.
        effective_reward = reward * (1.0 + self.relief_factor * self.stai)
        
        # Use effective_reward for updates instead of the raw reward
        delta_2 = effective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```