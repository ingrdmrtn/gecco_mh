Here are 3 new cognitive models that explore different mechanisms for how anxiety (STAI) might influence decision-making in this task, specifically focusing on reward sensitivity, model-based vs. model-free control, and learning rates.

### Model 1: Anxiety-Modulated Reward Sensitivity
This model hypothesizes that anxiety affects how subjectively valuable rewards are perceived. High anxiety might blunt the positive impact of rewards (anhedonia-like) or heighten sensitivity to feedback. Since the participant has low anxiety, this model tests if their reward sensitivity is distinct from the standard `1.0`.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates reward sensitivity.
    
    Instead of treating the reward as exactly 0 or 1, the subjective utility of the reward
    is scaled by a sensitivity parameter that depends on STAI.
    
    Effective Reward = Reward * (sensitivity_base + sensitivity_mod * STAI)
    
    Low anxiety participants might have higher reward sensitivity (more engagement),
    while high anxiety might blunt reward processing.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    sens_base: [0, 2]   # Base reward sensitivity
    sens_mod: [-1, 1]   # Modulation of sensitivity by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.sens_base, self.sens_mod = model_parameters

    def init_model(self) -> None:
        # Calculate effective sensitivity once based on static STAI
        self.effective_sensitivity = self.sens_base + (self.sens_mod * self.stai)
        # Ensure sensitivity doesn't go negative
        self.effective_sensitivity = max(0.0, self.effective_sensitivity)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Scale the reward
        subjective_reward = reward * self.effective_sensitivity
        
        # Standard TD learning with subjective reward
        delta_2 = subjective_reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update based on Stage 2 value (TD(0))
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Model-Based Weight (Hybrid Learner)
This model implements a classic hybrid reinforcement learning agent that mixes Model-Free (MF) and Model-Based (MB) strategies. The key hypothesis is that anxiety consumes cognitive resources (working memory), reducing the ability to use the computationally expensive Model-Based strategy. Therefore, higher STAI scores should lead to a lower mixing weight `w` (more Model-Free behavior).

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety reduces Model-Based control (Hybrid Learner).
    
    The agent computes Q-values using both Model-Free (TD) and Model-Based (Bellman) methods.
    The final Q-value for Stage 1 is a weighted sum:
    Q_net = w * Q_MB + (1 - w) * Q_MF
    
    The weight 'w' is modulated by STAI:
    w = w_max * (1 - stai)  
    (Higher anxiety -> lower w -> less Model-Based)

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta: [0, 10]      # Inverse temperature
    w_max: [0, 1]      # Maximum model-based weight (at 0 anxiety)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max = model_parameters

    def init_model(self) -> None:
        # Calculate mixing weight w based on STAI
        # We assume anxiety linearly degrades MB capacity
        self.w = self.w_max * (1.0 - self.stai)
        self.w = np.clip(self.w, 0.0, 1.0)
        
        # Initialize transition model (fixed for simplicity as per task description, 
        # but could be learned. Here we use the provided T matrix).
        # self.T is shape (2, 2) -> P(state | action) roughly, 
        # but in this task structure: Action 0 -> State 0 (common), Action 1 -> State 1 (common)
        # The provided self.T is actually transition counts. Let's normalize it properly.
        self.trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]]) # Approximate common/rare structure

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Value (self.q_stage1 is the MF value)
        q_mf = self.q_stage1
        
        # 2. Model-Based Value
        # Q_MB(a1) = sum(P(s2|a1) * max(Q_stage2(s2, :)))
        # We assume the agent knows the transition structure (0->0 common, 1->1 common)
        # Max Q value at stage 2 for each state
        v_stage2 = np.max(self.q_stage2, axis=1) 
        
        q_mb = np.zeros(self.n_choices)
        # Action 0 (Spaceship A) -> State 0 (70%), State 1 (30%)
        q_mb[0] = 0.7 * v_stage2[0] + 0.3 * v_stage2[1]
        # Action 1 (Spaceship U) -> State 0 (30%), State 1 (70%)
        q_mb[1] = 0.3 * v_stage2[0] + 0.7 * v_stage2[1]
        
        # 3. Combined Value
        q_net = self.w * q_mb + (1 - self.w) * q_mf
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for the Model-Free components
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) / SARSA-like update for Stage 1 MF value
        # Note: In pure hybrid models, Stage 1 MF is often updated via TD(1) using the reward directly
        # or TD(0) using Stage 2 value. We stick to the base class TD(0) logic for MF.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Learning Rate Asymmetry (Pos/Neg)
This model hypothesizes that anxiety biases learning from positive versus negative prediction errors. Anxious individuals might be hyper-sensitive to negative outcomes (loss aversion/threat detection) or less sensitive to positive ones.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety creates asymmetry in learning from positive vs negative prediction errors.
    
    We split the learning rate alpha into alpha_pos and alpha_neg.
    The balance between them is modulated by STAI.
    
    alpha_pos = alpha_base
    alpha_neg = alpha_base * (1 + bias_param * STAI)
    
    If bias_param > 0, higher anxiety leads to stronger learning from negative errors (disappointments).

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]   # Base learning rate
    beta: [0, 10]        # Inverse temperature
    bias_param: [0, 5]   # How much STAI amplifies negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.bias_param = model_parameters

    def init_model(self) -> None:
        self.alpha_pos = self.alpha_base
        # Modulate negative learning rate by anxiety
        self.alpha_neg = self.alpha_base * (1.0 + self.bias_param * self.stai)
        # Clip to ensure stability
        self.alpha_neg = min(1.0, self.alpha_neg)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # We use the updated Stage 2 value for the TD target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```