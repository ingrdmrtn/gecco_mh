```python
class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Surprise Aversion]
    This model hypothesizes that anxious individuals find unexpected (rare) 
    transitions intrinsically aversive. They penalize the value of the chosen 
    spaceship if it leads to a rare planet, regardless of the actual monetary 
    reward received. This "surprise penalty" is proportional to their anxiety,
    discouraging choices that feel unreliable.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    penalty_k: [0, 5] (Scales the subjective penalty for rare transitions)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.penalty_k = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine if transition was rare
        # 0->0 and 1->1 are Common. 0->1 and 1->0 are Rare.
        is_rare = (action_1 != state)
        
        # Calculate subjective penalty
        # If anxious, the penalty is larger.
        surprise_penalty = self.penalty_k * self.stai * float(is_rare)
        
        # Update Stage 2 (Standard Q-learning)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1
        # The target for Stage 1 is the value of the state reached (Q2), 
        # MINUS the surprise penalty. This makes the action that caused the 
        # rare transition less desirable.
        target_stage1 = self.q_stage2[state, action_2] - surprise_penalty
        delta_1 = target_stage1 - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)


class ParticipantModel2(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Modulated Dynamic Vigilance (Pearce-Hall)]
    This model hypothesizes that anxiety correlates with hyper-vigilance to 
    prediction errors. Instead of a fixed learning rate, the learning rate 
    dynamically increases when the model encounters a large prediction error 
    (surprise). The sensitivity of this dynamic adjustment is modulated by 
    the STAI score, causing anxious individuals to react strongly to volatility.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1] (Baseline learning rate)
    beta: [0, 10]
    vigilance: [0, 5] (Scaling factor for PE-dependent learning rate increase)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.vigilance = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        
        # Dynamic alpha for Stage 2
        # alpha = base + vigilance * stai * |PE|
        alpha_2 = self.alpha_base + self.vigilance * self.stai * abs(delta_2)
        alpha_2 = np.clip(alpha_2, 0, 1) # Ensure bounds
        
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        
        # Dynamic alpha for Stage 1
        alpha_1 = self.alpha_base + self.vigilance * self.stai * abs(delta_1)
        alpha_1 = np.clip(alpha_1, 0, 1)
        
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)


class ParticipantModel3(CognitiveModelBase):
    """
    [HYPOTHESIS: Anxiety-Driven Counterfactual "Grass-is-Greener" Bias]
    This model hypothesizes that anxious individuals engage in counterfactual 
    updating ("worrying" about the other option). They update the unchosen 
    spaceship's value assuming the alternative outcome would have been the 
    opposite of what they received (e.g., if they got 0, they assume the other 
    was 1). This bias is scaled by anxiety.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    cf_k: [0, 1] (Weight of the counterfactual update relative to direct learning)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.cf_k = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard updates for chosen path
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1
        
        # Counterfactual update for the unchosen Stage 1 action
        unchosen_action_1 = 1 - action_1
        
        # Construct fictitious reward: "The grass is greener"
        # If I got low reward (<=0), I assume other was high (1).
        # If I got high reward (>0), I assume other was low (0).
        r_proxy = 1.0 if reward <= 0 else 0.0
        
        # The target value for the unchosen option is this fictitious reward
        # We update it with a learning rate scaled by anxiety and cf_k
        cf_alpha = self.alpha * self.cf_k * self.stai
        cf_alpha = np.clip(cf_alpha, 0, 1)
        
        delta_cf = r_proxy - self.q_stage1[unchosen_action_1]
        self.q_stage1[unchosen_action_1] += cf_alpha * delta_cf

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```