### Model 1: Anxiety-Modulated Rare Transition Deterrence
```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Participants with higher anxiety may be intolerant of uncertainty or unexpected events.
    When a 'rare' transition occurs (e.g., Spaceship A -> Planet Y), the participant applies a 
    'deterrence' penalty to that spaceship on the next trial, regardless of the reward outcome.
    This models a heuristic avoidance of unreliable options, where the strength of avoidance is 
    modulated by STAI.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    deter_base: [-2, 2]  # Base deterrence (positive = avoid, negative = seek)
    deter_stai: [0, 5]   # STAI scaling of deterrence
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.deter_base, self.deter_stai = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_mod = self.q_stage1.copy()
        
        # Check if previous trial existed
        if self.last_action1 is not None and self.last_state is not None:
            # Determine if the last transition was rare
            # Common: 0->0, 1->1. Rare: 0->1, 1->0.
            is_rare = (self.last_action1 != self.last_state)
            
            if is_rare:
                # Calculate deterrence penalty
                deterrence = self.deter_base + (self.deter_stai * self.stai)
                
                # Apply penalty to the previously chosen action (reduce its value)
                # If deterrence is positive, probability of choosing it again decreases
                q_mod[self.last_action1] -= deterrence
        
        return self.softmax(q_mod, self.beta)

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Split Learning Rates
```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety differentially impacts the learning of high-level choices (Stage 1) 
    versus low-level associations (Stage 2). This model proposes that the Stage 1 learning rate 
    (updating spaceship values) is modulated by STAI, reflecting altered flexibility or volatility 
    in decision-making, while Stage 2 learning (alien reward probability) remains independent.

    Parameter Bounds:
    -----------------
    alpha1_base: [0, 1]
    alpha1_stai: [-1, 1] # How STAI shifts the stage 1 learning rate
    alpha2: [0, 1]       # Fixed learning rate for stage 2
    beta: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha1_base, self.alpha1_stai, self.alpha2, self.beta = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective alpha for stage 1 based on STAI
        eff_alpha1 = self.alpha1_base + (self.alpha1_stai * self.stai)
        eff_alpha1 = np.clip(eff_alpha1, 0.0, 1.0) # Ensure bounds
        
        # Stage 2 update (Standard TD using alpha2)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha2 * delta_2
        
        # Stage 1 update (TD using Stage 2 value and eff_alpha1)
        # We update Stage 1 value towards the value of the state we landed in
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += eff_alpha1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Outcome-Dependent Perseveration
```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: This model posits that perseveration (stickiness) is not uniform but depends on the 
    previous outcome (Reward vs No Reward). Anxiety specifically modulates the tendency to 
    stick with an option after a LOSS (No Reward). High anxiety might lead to 'freezing' (high stickiness 
    after loss) or 'flight' (switching after loss), distinct from behavior after a win.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    pers_win: [0, 5]        # Fixed stickiness bonus after a Reward (Win-Stay)
    pers_loss_base: [-2, 2] # Base stickiness after No Reward
    pers_loss_stai: [-2, 2] # STAI modulation of stickiness after No Reward
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pers_win, self.pers_loss_base, self.pers_loss_stai = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_mod = self.q_stage1.copy()
        
        # Apply perseveration bonus based on the outcome of the previous trial
        if self.last_action1 is not None and self.last_reward is not None:
            if self.last_reward > 0:
                # Win-Stay: Add fixed bonus
                q_mod[self.last_action1] += self.pers_win
            else:
                # Lose-Stay/Switch: Add modulated bonus
                # Positive value = Lose-Stay (Freeze), Negative value = Lose-Switch
                eff_pers_loss = self.pers_loss_base + (self.pers_loss_stai * self.stai)
                q_mod[self.last_action1] += eff_pers_loss
                
        return self.softmax(q_mod, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```