Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this task, specifically focusing on the observed behavior of sticking to one option.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that anxiety affects the balance between goal-directed (Model-Based) and habitual (Model-Free) control. The literature suggests stress and anxiety can shift control towards habitual systems. Here, the mixing weight `w` is modulated by STAI, where higher anxiety reduces the contribution of the Model-Based system.

```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition probabilities (fixed for MB)
        self.trans_counts = np.array([[0.7, 0.3], [0.3, 0.7]]) 
        
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        self.q_stage1 = np.zeros(self.n_choices) # MF values
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        pass

    def init_model(self) -> None:
        pass

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the balance between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety is hypothesized to impair MB planning, leading to a greater reliance on MF strategies.
    
    The mixing weight `w` determines the contribution of MB values to the final decision.
    We model `w` as `w_base * (1 - stai)`, meaning higher anxiety reduces the MB weight.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    w_base: [0, 1]  # Base model-based weight (before anxiety modulation)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Calculate Model-Based values for Stage 1
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # Using fixed transition probabilities (0.7 common, 0.3 rare)
            # Assuming action 0 -> state 0 is common, action 1 -> state 1 is common
            if a == 0:
                probs = [0.7, 0.3] # Prob of reaching state 0, state 1
            else:
                probs = [0.3, 0.7]
            
            # Expected value of best action in next state
            v_state0 = np.max(self.q_stage2[0])
            v_state1 = np.max(self.q_stage2[1])
            q_mb[a] = probs[0] * v_state0 + probs[1] * v_state1

        # 2. Calculate Anxiety-Modulated Weight
        # Higher STAI reduces w, making the agent more Model-Free
        w_eff = self.w_base * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0, 1)

        # 3. Combine MF and MB values
        q_net = w_eff * q_mb + (1 - w_eff) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for MF values
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry
This model hypothesizes that anxiety creates a bias in how positive and negative prediction errors are processed. Anxious individuals might be hypersensitive to negative outcomes (punishment) or less sensitive to positive ones (reward).

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning rates asymmetrically.
    Anxious individuals may learn differently from positive vs negative prediction errors.
    Here, we define a base learning rate `alpha` and a bias parameter `bias`.
    
    The effective learning rate for positive prediction errors (PPE) is `alpha`.
    The effective learning rate for negative prediction errors (NPE) is `alpha * bias_mod`.
    
    The `bias_mod` is influenced by STAI: `bias_mod = bias_param * (1 + stai)`.
    If bias_param > 1, anxiety amplifies learning from negative outcomes (hypersensitivity to failure).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Base learning rate (for positive errors)
    beta: [0, 10]      # Inverse temperature
    bias_param: [0, 5] # Multiplier for negative learning rate base
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.bias_param = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective negative learning rate multiplier
        # Higher anxiety -> stronger reaction to negative errors if bias_param > 0
        neg_alpha_mult = self.bias_param * (1.0 + self.stai)
        
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            alpha_eff = self.alpha
        else:
            alpha_eff = self.alpha * neg_alpha_mult
            alpha_eff = min(alpha_eff, 1.0) # Cap at 1
            
        self.q_stage2[state, action_2] += alpha_eff * delta_2
        
        # Stage 1 Update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            alpha_eff = self.alpha
        else:
            alpha_eff = self.alpha * neg_alpha_mult
            alpha_eff = min(alpha_eff, 1.0)

        self.q_stage1[action_1] += alpha_eff * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression
This model hypothesizes that anxiety reduces exploration. Anxious individuals often prefer familiar, safe options and avoid uncertainty. This is modeled by modulating the inverse temperature parameter `beta` with STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety suppresses exploration (increases exploitation).
    Higher anxiety leads to more deterministic choice behavior (higher inverse temperature beta),
    making the participant stick rigidly to the option with the slightly higher value and ignore
    potential information gain from exploring the other option.
    
    We model the effective beta as: `beta_eff = beta_base * (1 + sensitivity * stai)`.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_base: [0, 10]  # Base inverse temperature
    sensitivity: [0, 5] # How strongly STAI increases beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.sensitivity = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective beta
        # Higher STAI -> Higher Beta -> More deterministic (less exploration)
        beta_eff = self.beta_base * (1.0 + self.sensitivity * self.stai)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Same modulation for stage 2
        beta_eff = self.beta_base * (1.0 + self.sensitivity * self.stai)
        return self.softmax(self.q_stage2[state], beta_eff)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```