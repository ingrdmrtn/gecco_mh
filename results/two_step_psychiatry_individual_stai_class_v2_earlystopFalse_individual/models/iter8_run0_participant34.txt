Here are three new cognitive models exploring different mechanisms for how high anxiety (STAI = 0.9625) might influence decision-making in this task.

### Model 1: Anxiety-Modulated Model-Based Control
This model tests the hypothesis that high anxiety impairs model-based planning. In the two-step task, "model-based" control means using the transition matrix (knowledge of which spaceship goes where) to update stage-1 values based on stage-2 outcomes. "Model-free" control just reinforces the chosen spaceship. This model posits that the balance `w` between these systems is inversely related to anxiety: higher anxiety leads to more model-free (habitual) behavior.

```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety reduces model-based control.
    The participant uses a mixture of model-based (MB) and model-free (MF) strategies.
    The mixing weight 'w' determines the contribution of the MB system.
    We hypothesize that 'w' is modulated by STAI: w = w_max * (1 - STAI^k).
    Since this participant has very high STAI (~0.96), they might be almost entirely model-free.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_max: [0, 1]       # Maximum model-based weight (at 0 anxiety)
    k_impair: [0, 5]    # Exponent controlling how sharply anxiety impairs MB control
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_max, self.k_impair = model_parameters

    def init_model(self) -> None:
        # Calculate the fixed weight for this participant based on their STAI
        # If STAI is high (near 1), (1 - STAI^k) approaches 0, reducing w.
        self.w = self.w_max * (1.0 - (self.stai ** self.k_impair))
        
        # Initialize separate MF and MB values for stage 1
        self.q_stage1_mf = np.zeros(self.n_choices)
        self.q_stage1_mb = np.zeros(self.n_choices)

    def policy_stage1(self) -> np.ndarray:
        # Compute Model-Based values on the fly
        # Q_MB(a1) = sum_s T(s|a1) * max_a2 Q_stage2(s, a2)
        for a1 in range(self.n_choices):
            val = 0.0
            for s in range(self.n_states):
                # Transition probability T[a1, s]
                # We use the fixed transition matrix defined in base class
                prob = self.T[a1, s]
                max_q2 = np.max(self.q_stage2[s])
                val += prob * max_q2
            self.q_stage1_mb[a1] = val
            
        # Combined value: w * MB + (1-w) * MF
        q_net = self.w * self.q_stage1_mb + (1.0 - self.w) * self.q_stage1_mf
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (common to both systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Model-Free update (TD-1)
        # Note: In full 2-step models, this is often TD(lambda). 
        # Here we use a simple TD-0 update from the stage 2 value for simplicity in this specific hypothesis.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1_mf[action_1]
        self.q_stage1_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion
This model hypothesizes that high anxiety makes participants hypersensitive to negative outcomes (getting 0 coins). Instead of a single learning rate, the model splits learning into positive (`alpha_pos`) and negative (`alpha_neg`) updates. The negative learning rate is boosted by the STAI score, implying that anxious individuals update their value estimates more drastically after a failure than after a success.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety leads to asymmetric learning rates (Loss Aversion).
    The participant learns differently from positive (reward=1) vs negative (reward=0) outcomes.
    The negative learning rate is amplified by anxiety: alpha_neg = alpha_base * (1 + STAI * k_anx).
    This makes the participant flee from options that yield no reward more quickly.

    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1]   # Learning rate for rewards
    alpha_base: [0, 1]  # Base learning rate for non-rewards
    beta: [0, 10]       # Inverse temperature
    k_anx: [0, 5]       # Anxiety amplification factor for negative learning
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_base, self.beta, self.k_anx = model_parameters

    def init_model(self) -> None:
        # Calculate the anxiety-modulated negative learning rate
        self.alpha_neg = self.alpha_base * (1.0 + self.stai * self.k_anx)
        # Clip to ensure stability, though bounds usually handle this
        self.alpha_neg = min(self.alpha_neg, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Determine which alpha to use based on the reward
        # In this task, reward is binary 0 or 1.
        if reward > 0.5:
            alpha = self.alpha_pos
        else:
            alpha = self.alpha_neg

        # Stage 2 update
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha * delta_2
        
        # Stage 1 update
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression
This model suggests that high anxiety reduces exploration. Anxious individuals might prefer to exploit known safe options rather than explore uncertain ones. This is modeled by modulating the inverse temperature parameter `beta` with STAI. A higher `beta` means more deterministic (greedy) choice. The model posits `beta` increases with STAI.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: High anxiety suppresses exploration (increases exploitation).
    The inverse temperature 'beta' controls the randomness of choice.
    We hypothesize beta is a linear function of STAI: beta = beta_min + slope * STAI.
    High anxiety results in a very high beta, leading to rigid, deterministic choices 
    (sticking to the option with slightly higher value).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_min: [0, 5]    # Baseline inverse temperature (low anxiety)
    slope: [0, 10]      # How much anxiety increases beta
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_min, self.slope = model_parameters

    def init_model(self) -> None:
        # Calculate the effective beta for this participant
        self.beta_eff = self.beta_min + (self.slope * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_eff)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD learning
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```