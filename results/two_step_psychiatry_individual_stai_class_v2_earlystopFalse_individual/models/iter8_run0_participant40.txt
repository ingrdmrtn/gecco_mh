Here are 3 new cognitive models that explore different mechanisms for how low anxiety might influence decision-making in this task.

### Model 1: Model-Based vs. Model-Free Hybrid with Anxiety-Modulated Mixing
This model tests the hypothesis that anxiety levels influence the balance between Model-Based (planning) and Model-Free (habitual) control. Low anxiety participants (like this one) might rely more on computationally expensive Model-Based strategies because they are less cognitively burdened by worry.

```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: The participant uses a hybrid Model-Based (MB) and Model-Free (MF) reinforcement learning strategy.
    The weighting parameter 'w' determines the balance between these systems.
    Anxiety (STAI) modulates this balance: Low anxiety allows for higher Model-Based control (higher w),
    while high anxiety pushes towards Model-Free habits (lower w).
    
    Mechanism:
    w_effective = w_base * (1 - stai)
    If stai is low, w_effective is closer to w_base (more MB).
    If stai is high, w_effective shrinks (more MF).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta: [0, 10]       # Inverse temperature
    w_base: [0, 1]      # Base mixing weight (0=Pure MF, 1=Pure MB)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # MF Q-values for stage 1
        self.q_mf = np.zeros(self.n_choices)
        # MB Q-values are computed on the fly using transition matrix and stage 2 values

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based values for Stage 1
        # Q_MB(a1) = sum(P(s2|a1) * max(Q_stage2(s2, a2)))
        # We use the fixed transition probabilities self.T defined in base class
        # self.T[action, state] gives prob of transitioning to state given action
        
        # Max Q-value for each state in stage 2
        max_q2 = np.max(self.q_stage2, axis=1) 
        
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            q_mb[a] = np.sum(self.T[a] * max_q2)

        # 2. Calculate effective mixing weight based on anxiety
        # Low anxiety -> higher w -> more model-based
        w_eff = self.w_base * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0, 1)

        # 3. Combine MF and MB values
        q_net = w_eff * q_mb + (1 - w_eff) * self.q_mf

        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard Q-learning for Stage 2 (used by both MB and MF systems)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # TD(1) update for Stage 1 MF values
        # The MF system updates stage 1 based on the final reward
        delta_1 = reward - self.q_mf[action_1]
        self.q_mf[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Asymmetric Learning Rates Modulated by Anxiety
This model hypothesizes that anxiety affects how participants learn from positive versus negative prediction errors. Low anxiety individuals might have a "positivity bias" or balanced learning, whereas high anxiety is often associated with hypersensitivity to negative outcomes.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: The participant learns differently from positive and negative prediction errors.
    Anxiety (STAI) modulates the ratio between the learning rate for positive errors (alpha_pos)
    and negative errors (alpha_neg).
    
    Mechanism:
    alpha_pos = alpha_base
    alpha_neg = alpha_base * (1 + k_asym * stai)
    
    If k_asym is positive, higher anxiety leads to stronger learning from negative outcomes (punishment sensitivity).
    Since this participant has low anxiety, they should show relatively balanced learning or less sensitivity to loss compared to high anxiety peers.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1]  # Base learning rate
    beta: [0, 10]       # Inverse temperature
    k_asym: [0, 5]      # Scaling factor for anxiety's effect on negative learning rate
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.k_asym = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective learning rates
        alpha_pos = self.alpha_base
        # Anxiety amplifies the negative learning rate
        alpha_neg = self.alpha_base * (1.0 + self.k_asym * self.stai)
        
        # Clip to ensure stability
        alpha_neg = np.clip(alpha_neg, 0, 1)

        # --- Stage 2 Update ---
        delta_2 = reward - self.q_stage2[state, action_2]
        if delta_2 >= 0:
            self.q_stage2[state, action_2] += alpha_pos * delta_2
        else:
            self.q_stage2[state, action_2] += alpha_neg * delta_2
        
        # --- Stage 1 Update ---
        # Using the updated stage 2 value as the target (SARSA-like or TD-0)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        if delta_1 >= 0:
            self.q_stage1[action_1] += alpha_pos * delta_1
        else:
            self.q_stage1[action_1] += alpha_neg * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Dependent Exploration (Inverse Temperature Modulation)
This model tests the "anxiety-induced randomization" or "choking under pressure" hypothesis, but inversely. It posits that anxiety directly affects the randomness of choice (exploration/exploitation balance). Low anxiety participants might be more deterministic (higher beta), while anxiety adds noise.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety acts as a noise parameter, degrading the precision of choices.
    The inverse temperature (beta) is modulated by STAI.
    
    Mechanism:
    beta_effective = beta_max / (1 + noise_scale * stai)
    
    Low anxiety -> Low denominator -> High beta (precise, exploitative choices).
    High anxiety -> High denominator -> Low beta (random, noisy choices).

    Parameter Bounds:
    -----------------
    alpha: [0, 1]       # Learning rate
    beta_max: [0, 20]   # Maximum theoretical inverse temperature (at 0 anxiety)
    noise_scale: [0, 10]# Sensitivity of beta to anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_max, self.noise_scale = model_parameters

    def init_model(self) -> None:
        # Calculate the effective beta once, as STAI is constant for the participant
        self.beta_eff = self.beta_max / (1.0 + self.noise_scale * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_eff)
        
    # Standard value update from base class is sufficient
    # def value_update(...) uses self.alpha which is set in unpack_parameters

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```