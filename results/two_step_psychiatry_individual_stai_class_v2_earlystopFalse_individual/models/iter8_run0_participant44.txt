Here are three new cognitive models exploring different mechanisms of how anxiety (STAI) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based Control (Hybrid Model)
This model tests the hypothesis that high anxiety impairs the use of "model-based" (planning) strategies, causing participants to rely more on "model-free" (habitual) learning. In the classic two-step task analysis, behavior is often a mix of model-based (planning using transition probabilities) and model-free (repeating rewarded actions) control. Here, the mixing weight `w` is modulated by STAI.

```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety impairs model-based planning.
    This model implements a hybrid reinforcement learning agent that combines 
    Model-Free (MF) and Model-Based (MB) values.
    The weight `w` determines the balance: w=1 is pure MB, w=0 is pure MF.
    We hypothesize that higher anxiety (STAI) reduces `w`, pushing the participant 
    towards habitual (MF) control.
    
    w = w_base * (1 - stai * impairment_factor)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta: [0, 10] (Inverse temperature)
    w_base: [0, 1] (Baseline model-based weight for low anxiety)
    impairment_factor: [0, 1] (How strongly anxiety reduces MB control)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base, self.impairment_factor = model_parameters

    def init_model(self) -> None:
        # Initialize Model-Free Q-values (Stage 1 and Stage 2)
        self.q_mf_stage1 = np.zeros(self.n_choices)
        self.q_mf_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        # Calculate the effective mixing weight w based on STAI
        # High STAI reduces w, leading to less model-based control
        self.w = self.w_base * (1.0 - (self.stai * self.impairment_factor))
        self.w = np.clip(self.w, 0.0, 1.0)

    def policy_stage1(self) -> np.ndarray:
        # Model-Based Value Calculation for Stage 1
        # Q_MB(s1, a) = sum(P(s2|s1,a) * max(Q_MF(s2, a')))
        # We use the fixed transition matrix self.T
        # self.T[action, state] is prob of transitioning to state given action (0->A, 1->U)
        # Note: In this task, Action 0 (A) -> State 0 (X) w/ 0.7, State 1 (Y) w/ 0.3
        #       Action 1 (U) -> State 1 (Y) w/ 0.7, State 0 (X) w/ 0.3
        
        # Calculate max Q value for each second-stage state
        max_q_stage2 = np.max(self.q_mf_stage2, axis=1) # Shape (2,)
        
        # Compute MB values
        q_mb_stage1 = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # T[a] is the row of transition probs for action a
            q_mb_stage1[a] = np.dot(self.T[a], max_q_stage2)
            
        # Combine MF and MB values
        q_net = self.w * q_mb_stage1 + (1 - self.w) * self.q_mf_stage1
        
        return self.softmax(q_net, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Stage 2 is purely model-free (no further planning)
        return self.softmax(self.q_mf_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Update Stage 2 MF values (Standard Q-learning)
        delta_2 = reward - self.q_mf_stage2[state, action_2]
        self.q_mf_stage2[state, action_2] += self.alpha * delta_2
        
        # Update Stage 1 MF values (TD(1) / SARSA-like update using the reward)
        # Note: In hybrid models, Stage 1 MF is often updated directly from the reward 
        # or the stage 2 value. Here we use the reward to drive the MF habit.
        delta_1 = reward - self.q_mf_stage1[action_1]
        self.q_mf_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Learning Rate Asymmetry (Pessimism Bias)
This model hypothesizes that anxiety affects how participants learn from positive versus negative prediction errors. Specifically, high anxiety might lead to "pessimistic learning," where negative outcomes (omission of reward) have a larger impact on value updates than positive outcomes.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety amplifies learning from negative prediction errors.
    High anxiety individuals may be more sensitive to "bad news" (reward = 0) 
    than "good news" (reward = 1).
    
    We define two learning rates:
    alpha_pos: Learning rate for positive prediction errors (RPE > 0)
    alpha_neg: Learning rate for negative prediction errors (RPE < 0)
    
    alpha_neg is modulated by STAI:
    alpha_neg = alpha_pos * (1 + pessimism_factor * STAI)
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1] (Base learning rate for positive errors)
    beta: [0, 10] (Inverse temperature)
    pessimism_factor: [0, 5] (Scaling factor for anxiety-induced negative learning boost)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.beta, self.pessimism_factor = model_parameters

    def init_model(self) -> None:
        # Calculate the negative learning rate based on STAI
        # If pessimism_factor is high and STAI is high, alpha_neg becomes larger than alpha_pos
        self.alpha_neg = self.alpha_pos * (1.0 + self.pessimism_factor * self.stai)
        # Clip to ensure stability, though theoretically it could go > 1 in some frameworks, 
        # usually we keep it <= 1 for simple Q-learning.
        self.alpha_neg = np.clip(self.alpha_neg, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        pe2 = reward - self.q_stage2[state, action_2]
        alpha2 = self.alpha_pos if pe2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha2 * pe2
        
        # Stage 1 Update
        # We use the updated Q2 value to drive Q1 update (TD-learning)
        pe1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha1 = self.alpha_pos if pe1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha1 * pe1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression (Inverse Temperature Modulation)
This model posits that anxiety reduces exploration and leads to more deterministic (exploitative) behavior, regardless of whether that behavior is optimal. This is modeled by modulating the inverse temperature parameter `beta` with STAI. A higher `beta` means choices are more strictly determined by value differences (less random exploration).

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety suppresses exploration (increases exploitation).
    Anxious individuals may find uncertainty aversive and thus stick rigidly 
    to the option with the slightly higher value, reducing random exploration.
    
    This is implemented by scaling the inverse temperature (beta) with STAI.
    beta_effective = beta_base * (1 + stiffening_factor * STAI)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta_base: [0, 10] (Baseline inverse temperature)
    stiffening_factor: [0, 5] (How much anxiety increases rigidity/beta)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stiffening_factor = model_parameters

    def init_model(self) -> None:
        # Calculate effective beta
        # High STAI -> Higher Beta -> Sharper softmax (less exploration)
        self.beta_effective = self.beta_base * (1.0 + self.stiffening_factor * self.stai)

    def policy_stage1(self) -> np.ndarray:
        return self.softmax(self.q_stage1, self.beta_effective)

    def policy_stage2(self, state: int) -> np.ndarray:
        return self.softmax(self.q_stage2[state], self.beta_effective)

    # Standard value update
    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```