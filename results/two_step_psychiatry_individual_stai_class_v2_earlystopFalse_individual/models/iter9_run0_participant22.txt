```python
import numpy as np
from abc import ABC, abstractmethod

class CognitiveModelBase(ABC):
    """
    Base class for cognitive models in a two-step task.
    
    Override methods to implement participant-specific cognitive strategies.
    """

    def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
        # Task structure
        self.n_trials = n_trials
        self.n_choices = 2
        self.n_states = 2
        self.stai = stai
        
        # Transition probabilities
        self.trans_counts = np.array([[35, 15], [15, 35]]) # Prior counts for transitions, if needed for learning
        self.T = self.trans_counts / self.trans_counts.sum(axis=1, keepdims=True) # Transition probabilities, if needed for direct use
        
        # Choice probability sequences
        self.p_choice_1 = np.zeros(n_trials)
        self.p_choice_2 = np.zeros(n_trials)
        
        # Value representations
        self.q_stage1 = np.zeros(self.n_choices)
        self.q_stage2 = 0.5 * np.ones((self.n_states, self.n_choices))
        
        # Trial tracking
        self.trial = 0
        self.last_action1 = None
        self.last_action2 = None
        self.last_state = None
        self.last_reward = None
        
        # Initialize
        self.unpack_parameters(model_parameters)
        self.init_model()

    @abstractmethod
    def unpack_parameters(self, model_parameters: tuple) -> None:
        """Unpack model_parameters into named attributes."""
        pass

    def init_model(self) -> None:
        """Initialize model state. Override to set up additional variables."""
        pass

    def policy_stage1(self) -> np.ndarray:
        """Compute stage-1 action probabilities. Override to customize."""
        return self.softmax(self.q_stage1, self.beta)

    def policy_stage2(self, state: int) -> np.ndarray:
        """Compute stage-2 action probabilities. Override to customize."""
        return self.softmax(self.q_stage2[state], self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Update values after observing outcome. Override to customize."""
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

    def pre_trial(self) -> None:
        """Called before each trial. Override to add computations."""
        pass

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        """Called after each trial. Override to add computations."""
        self.last_action1 = action_1
        self.last_action2 = action_2
        self.last_state = state
        self.last_reward = reward

    def run_model(self, action_1, state, action_2, reward) -> float:
        """Run model over all trials. Usually don't override."""
        for self.trial in range(self.n_trials):
            a1, s = int(action_1[self.trial]), int(state[self.trial])
            a2, r = int(action_2[self.trial]), float(reward[self.trial])
            
            self.pre_trial()
            self.p_choice_1[self.trial] = self.policy_stage1()[a1]
            self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
            self.value_update(a1, s, a2, r)
            self.post_trial(a1, s, a2, r)
        
        return self.compute_nll()
    
    def compute_nll(self) -> float:
        """Compute negative log-likelihood."""
        eps = 1e-12
        return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
    
    def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
        """Softmax action selection."""
        centered = values - np.max(values)
        exp_vals = np.exp(beta * centered)
        return exp_vals / np.sum(exp_vals)

def make_cognitive_model(ModelClass):
    """Create function interface from model class."""
    def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
        n_trials = len(action_1)
        stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
        model = ModelClass(n_trials, stai_val, model_parameters)
        return model.run_model(action_1, state, action_2, reward)
    return cognitive_model

class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety specifically modulates the learning rate for the second stage (Alien outcomes),
    reflecting altered sensitivity to direct reward feedback, while navigation learning (Stage 1) remains stable.
    High anxiety might lead to rapid over-adjustment of alien values (high alpha2) or rigidity (low alpha2).
    
    Parameter Bounds:
    -----------------
    alpha1: [0, 1]       # Learning rate for Stage 1 (Spaceships)
    beta: [0, 10]        # Inverse temperature
    alpha2_base: [0, 1]  # Base learning rate for Stage 2 (Aliens)
    alpha2_stai: [-1, 1] # Modulation of Stage 2 learning rate by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha1, self.beta, self.alpha2_base, self.alpha2_stai = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective alpha for stage 2 based on STAI
        alpha2_eff = self.alpha2_base + (self.alpha2_stai * self.stai)
        alpha2_eff = np.clip(alpha2_eff, 0, 1)
        
        # Stage 2 update (Alien values)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += alpha2_eff * delta_2
        
        # Stage 1 update (Spaceship values) using fixed alpha1
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)

class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates decision noise (beta) specifically after a loss (0 coins).
    Anxious individuals may exhibit different levels of exploration/scrambling (low beta) or rigidity (high beta)
    immediately following a negative outcome compared to a positive one. This captures "panic switching" or "freezing".
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]           # Learning rate
    beta_win: [0, 10]       # Inverse temperature after a reward (1.0)
    beta_loss_base: [0, 10] # Base inverse temperature after a loss (0.0)
    beta_loss_stai: [-5, 5] # Modulation of loss-beta by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_win, self.beta_loss_base, self.beta_loss_stai = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Determine beta based on last reward
        if self.last_reward is None or self.last_reward == 1.0:
            current_beta = self.beta_win
        else:
            # Modulate beta after loss
            current_beta = self.beta_loss_base + (self.beta_loss_stai * self.stai)
            current_beta = np.maximum(0, current_beta) # Ensure non-negative
            
        return self.softmax(self.q_stage1, current_beta)

cognitive_model2 = make_cognitive_model(ParticipantModel2)

class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: The participant employs a Model-Based strategy where the transition matrix is learned online.
    Anxiety modulates the learning rate of this transition matrix, reflecting hyper-vigilance or rapid updating
    of environmental structure beliefs (e.g., believing the spaceship reliability has changed).
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]          # Learning rate for values (Stage 2)
    beta: [0, 10]          # Inverse temperature
    lr_trans_base: [0, 1]  # Base learning rate for transition matrix
    lr_trans_stai: [-1, 1] # Modulation of transition learning rate by STAI
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.lr_trans_base, self.lr_trans_stai = model_parameters
        
    def init_model(self) -> None:
        # Initialize transition matrix belief (start with standard prior assumption)
        self.T_belief = np.array([[0.7, 0.3], [0.3, 0.7]]) 

    def policy_stage1(self) -> np.ndarray:
        # Model-Based Value Calculation
        # Q_MB(a1) = sum(T_belief(a1, s) * max(Q_stage2(s, :)))
        q_mb = np.zeros(self.n_choices)
        for a1 in range(self.n_choices):
            for s in range(self.n_states):
                # Use max value of stage 2 state (greedy planning)
                val_s = np.max(self.q_stage2[s])
                q_mb[a1] += self.T_belief[a1, s] * val_s
        
        # Use Model-Based values for decision
        return self.softmax(q_mb, self.beta)

    def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        super().post_trial(action_1, state, action_2, reward)
        
        # Update Transition Matrix Belief
        lr_trans = self.lr_trans_base + (self.lr_trans_stai * self.stai)
        lr_trans = np.clip(lr_trans, 0, 1)
        
        # Create a one-hot vector for the observed state
        observed_trans = np.zeros(self.n_states)
        observed_trans[state] = 1.0
        
        # Update the row corresponding to action_1 (Leaky integration)
        self.T_belief[action_1] = (1 - lr_trans) * self.T_belief[action_1] + lr_trans * observed_trans

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```