Here are three new cognitive models that explore different mechanisms for how anxiety (STAI) might influence decision-making in this task, specifically focusing on the participant's observed behavior (strong preference for one option) and medium anxiety level.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that anxiety affects the balance between Model-Based (planning using transition probabilities) and Model-Free (habitual) control. Medium-to-high anxiety might disrupt the cognitive resources required for model-based planning, leading to a reliance on model-free strategies.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the trade-off between Model-Based (MB) and Model-Free (MF) control.
    Higher anxiety consumes working memory resources, reducing the weight (w) placed on 
    model-based planning. The mixing parameter 'w' determines the contribution of MB values 
    to the final stage-1 decision.
    
    w_effective = w_base * (1 - stai)
    
    If w is close to 1, the agent is purely Model-Based.
    If w is close to 0, the agent is purely Model-Free.
    Higher STAI reduces the effective w, pushing the agent towards Model-Free behavior.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]   # Learning rate
    beta: [0, 10]   # Inverse temperature
    w_base: [0, 1]  # Base weight for Model-Based control (before anxiety modulation)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # 1. Model-Free Value (Q_MF)
        # This is simply self.q_stage1, updated via TD in value_update
        
        # 2. Model-Based Value (Q_MB)
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        # We use the fixed transition matrix self.T for simplicity (or could learn it)
        # self.T[a1, s] is prob of transition to s given a1
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # Expected value of the best action at the next stage
            expected_val = 0
            for s in range(self.n_states):
                expected_val += self.T[a, s] * np.max(self.q_stage2[s])
            q_mb[a] = expected_val
            
        # 3. Hybrid Value
        # Modulate w based on anxiety: Higher anxiety -> Lower w (Less MB)
        # We clamp the modulation to ensure w stays in [0, 1]
        w_eff = self.w_base * (1.0 - self.stai)
        w_eff = np.clip(w_eff, 0.0, 1.0)
        
        q_net = w_eff * q_mb + (1 - w_eff) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (Standard Q-learning)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (Model-Free TD)
        # Note: In hybrid models, the MF value is usually updated via TD(1) or TD(0)
        # Here we use TD(0) using the stage 2 value
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Loss Aversion in Learning
This model hypothesizes that anxiety creates an asymmetry in how positive and negative prediction errors are processed. Specifically, anxious individuals might be more sensitive to "disappointments" (worse than expected outcomes) or less sensitive to rewards, effectively exhibiting a learning bias.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates learning rates asymmetrically.
    Anxious individuals may have a heightened sensitivity to negative prediction errors 
    (when outcomes are worse than expected) or a blunted response to positive ones.
    
    We define two learning rates:
    alpha_pos: Learning rate for positive prediction errors (RPE > 0)
    alpha_neg: Learning rate for negative prediction errors (RPE < 0)
    
    The STAI score modulates the ratio between these.
    alpha_neg_effective = alpha_base * (1 + bias * stai)
    alpha_pos_effective = alpha_base
    
    This implies anxiety amplifies learning from negative surprises.

    Parameter Bounds:
    -----------------
    alpha_base: [0, 1] # Base learning rate
    beta: [0, 10]      # Inverse temperature
    bias: [0, 5]       # Strength of anxiety-induced negative bias
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_base, self.beta, self.bias = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective learning rates
        alpha_pos = self.alpha_base
        # Anxiety increases the weight of negative errors
        alpha_neg = self.alpha_base * (1.0 + self.bias * self.stai)
        # Clip to ensure stability
        alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

        # Stage 2 Update
        pe2 = reward - self.q_stage2[state, action_2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        self.q_stage2[state, action_2] += lr2 * pe2
        
        # Stage 1 Update
        # Using the updated Q2 value for the TD target
        pe1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        self.q_stage1[action_1] += lr1 * pe1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Exploration Suppression (Uncertainty Avoidance)
This model hypothesizes that anxiety reduces exploration. Anxious individuals often prefer familiar, safe options over uncertain ones. This is modeled by adding an "uncertainty penalty" to options that haven't been chosen recently, or conversely, a "familiarity bonus" that scales with anxiety.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety leads to uncertainty avoidance, manifesting as reduced exploration.
    Instead of standard softmax exploration, anxiety adds a penalty to the inverse temperature (beta)
    or directly penalizes the value of options based on their uncertainty (variance/inverse counts).
    
    Here, we implement it as 'Inverse Temperature Modulation'.
    Higher anxiety leads to a higher beta (more deterministic/exploitative choices),
    making the agent stick rigidly to the option with slightly higher value and ignore
    exploration opportunities.
    
    beta_effective = beta_base * (1 + stiff * stai)
    
    'stiff' parameter controls how much anxiety stiffens the policy.

    Parameter Bounds:
    -----------------
    alpha: [0, 1]      # Learning rate
    beta_base: [0, 10] # Base inverse temperature
    stiff: [0, 10]     # Stiffness factor modulated by anxiety
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta_base, self.stiff = model_parameters

    def policy_stage1(self) -> np.ndarray:
        # Calculate effective beta
        # Higher STAI -> Higher Beta -> Less Randomness/Exploration
        beta_eff = self.beta_base * (1.0 + self.stiff * self.stai)
        return self.softmax(self.q_stage1, beta_eff)

    def policy_stage2(self, state: int) -> np.ndarray:
        # Apply same stiffness to stage 2
        beta_eff = self.beta_base * (1.0 + self.stiff * self.stai)
        return self.softmax(self.q_stage2[state], beta_eff)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```