Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) influences decision-making in this task.

### Model 1: Anxiety-Modulated Credit Assignment
This model hypothesizes that anxiety affects the "credit assignment" process—specifically, how much the outcome of the second stage (the alien's reward) updates the value of the first stage choice (the spaceship). High anxiety might disrupt this distal learning (making it harder to link outcomes to initial choices) or enhance it.

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates the credit assignment to the first stage choice.
    While the immediate learning (Stage 2) is standard, the update of the 
    Stage 1 choice (distal predictor) is scaled by anxiety.
    
    alpha_stage1 = alpha * (1 + credit_mod * stai)
    alpha_stage2 = alpha
    
    If credit_mod is negative, anxiety reduces the impact of outcomes on spaceship values.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    credit_mod: [-2, 2]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.credit_mod = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 update (Standard TD)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (Modulated by Anxiety)
        # Calculate effective learning rate for stage 1
        mod_factor = 1.0 + (self.credit_mod * self.stai)
        alpha_1 = np.clip(self.alpha * mod_factor, 0.0, 1.0)
        
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Pessimism (Initial Beliefs)
This model hypothesizes that anxiety manifests as "pessimism" regarding the expected value of the environment before learning occurs. Instead of starting with neutral expectations (0.5), anxious participants start with lower initial value estimates, which affects their early exploration and reaction to rewards.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety induces pessimism in initial value expectations.
    Anxious participants start with lower Q-values (priors), affecting early 
    exploration and the reaction to the first few rewards.
    
    Q_init = 0.5 - (pessimism * stai)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    pessimism: [0, 1]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.pessimism = model_parameters

    def init_model(self) -> None:
        # Calculate initial value based on STAI
        # Default is usually 0.5 or 0. We subtract a bias proportional to anxiety.
        init_val = 0.5 - (self.pessimism * self.stai)
        
        # Clip to ensure reasonable bounds (e.g. 0 to 0.5)
        init_val = np.clip(init_val, 0.0, 1.0)
        
        # Overwrite the default initializations
        self.q_stage1 = np.full(self.n_choices, init_val)
        self.q_stage2 = np.full((self.n_states, self.n_choices), init_val)

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Modulated Loss Reactivity
This model hypothesizes that anxiety specifically modulates behavior after a "loss" (receiving 0 coins). Unlike general stickiness, this model proposes that anxiety drives a specific reaction to failure—either "freezing" (repeating the bad choice, high reactivity) or "flight" (switching away, low/negative reactivity).

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety modulates 'Loss-Reactivity' (Stickiness after a loss).
    Anxious participants may be more likely to repeat a choice immediately after 
    receiving no reward (compulsivity) or switch (flight), compared to after a win.
    
    Bonus = reactivity * stai * I(last_reward == 0) * I(action == last_action)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    reactivity: [-5, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.reactivity = model_parameters

    def policy_stage1(self) -> np.ndarray:
        bonus_vec = np.zeros(self.n_choices)
        
        # Apply bonus only if there was a previous trial and the reward was 0 (Loss)
        if self.last_action1 is not None and self.last_reward == 0.0:
            stickiness = self.reactivity * self.stai
            bonus_vec[int(self.last_action1)] = stickiness
            
        # Add bonus to Q-values before softmax
        return self.softmax(self.q_stage1 + bonus_vec, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```