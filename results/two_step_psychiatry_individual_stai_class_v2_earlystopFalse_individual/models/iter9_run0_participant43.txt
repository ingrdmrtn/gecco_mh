Here are three new cognitive models that hypothesize different mechanisms for how anxiety (STAI) modulates decision-making in this task.

### Model 1: Anxiety-Modulated Subjective Loss Perception
This model proposes that anxiety alters the **valuation** of outcomes. Specifically, it hypothesizes that anxious individuals perceive the omission of reward (0 coins) not as a neutral event, but as a negative outcome (punishment).

```python
class ParticipantModel1(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Subjective Loss Perception
    
    This model hypothesizes that anxious individuals perceive the omission of reward (0 coins)
    not as a neutral event, but as a negative outcome (punishment). The magnitude of this
    negative utility is proportional to their STAI score. This reflects a "negativity bias"
    where neutral outcomes are interpreted threateningly.
    
    If reward == 1: utility = 1
    If reward == 0: utility = -1 * penalty_strength * STAI
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    penalty_strength: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.penalty_strength = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate subjective utility
        if reward == 1:
            utility = 1.0
        else:
            # Transform neutral outcome (0) into a negative utility based on anxiety
            utility = -1.0 * self.penalty_strength * self.stai
            
        # Stage 2 Update (RPE using subjective utility)
        delta_2 = utility - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Update (TD using updated Stage 2 value)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Modulated Stage-1 Learning Inhibition
This model focuses on **credit assignment**. It hypothesizes that anxiety consumes cognitive resources (e.g., working memory), making it harder to link outcomes back to distal causes (Stage 1 choices), while learning from immediate causes (Stage 2 choices) remains intact.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Modulated Stage-1 Learning Inhibition
    
    This model hypothesizes that anxiety consumes cognitive resources, specifically 
    impairing the ability to assign credit to distal choices (Stage 1). Learning 
    from immediate outcomes (Stage 2) is unaffected, but the propagation of value 
    back to Stage 1 is dampened by anxiety.
    
    alpha_stage2 = alpha
    alpha_stage1 = alpha * (1 - inhibition_factor * STAI)
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    inhibition_factor: [0, 1] (Scales how much STAI reduces Stage 1 learning)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.inhibition_factor = model_parameters

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Calculate effective alpha for stage 1
        # We clip the inhibition to ensure alpha doesn't become negative
        inhibition = self.inhibition_factor * self.stai
        alpha_s1 = self.alpha * max(0.0, 1.0 - inhibition)
        
        # Stage 2 Update (Standard learning rate)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 Update (Inhibited learning rate)
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += alpha_s1 * delta_1

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven "Safety Seeking" (Win-Stay Bias)
This model hypothesizes a specific **heuristic bias**. It suggests that anxious individuals have an exaggerated "Win-Stay" tendency. When they find a safe (rewarding) option, they cling to it more strongly than less anxious individuals, treating the reward as a safety signal.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    HYPOTHESIS: Anxiety-Driven "Safety Seeking" (Win-Stay Bias)
    
    This model hypothesizes that anxious individuals have an exaggerated tendency 
    to repeat choices that led to a reward ("Safety Seeking"). This is implemented 
    as a transient bias added to the Q-values during Stage 1 choice selection, 
    proportional to STAI, specifically after a win.
    
    If last_reward == 1:
        Q_biased[last_action1] = Q[last_action1] + safety_bonus * STAI
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    safety_bonus: [0, 5]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.safety_bonus = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_values = self.q_stage1.copy()
        
        # Apply Safety Bonus if last trial was a win
        # This biases the softmax to repeat the successful action
        if self.last_reward == 1.0 and self.last_action1 is not None:
            bonus = self.safety_bonus * self.stai
            q_values[int(self.last_action1)] += bonus
            
        return self.softmax(q_values, self.beta)

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```