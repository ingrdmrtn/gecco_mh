Here are three new cognitive models exploring different mechanisms for how anxiety (STAI) might influence decision-making in this two-step task.

### Model 1: Anxiety-Modulated Model-Based Control
This model tests the hypothesis that high anxiety impairs the complex, cognitive "model-based" planning system, forcing reliance on the simpler "model-free" system. In the two-step task, model-based control uses the transition matrix to plan, while model-free control just reinforces actions that led to reward. Here, the mixing weight `w` (trade-off between model-based and model-free) is inversely proportional to anxiety.

```python
import numpy as np
from abc import ABC, abstractmethod

class ParticipantModel1(CognitiveModelBase):
    """
    Hypothesis: Anxiety impairs model-based planning (resource depletion).
    High anxiety consumes cognitive resources, reducing the ability to use the 
    transition structure (Model-Based control). The mixing weight 'w' starts at a 
    baseline and is reduced by the STAI score.
    
    Q_net = w * Q_MB + (1-w) * Q_MF
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta: [0, 10] (Inverse temperature)
    w_base: [0, 1] (Baseline model-based weight for a theoretical 0-anxiety person)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.w_base = model_parameters

    def init_model(self) -> None:
        # Calculate the effective mixing weight w based on STAI
        # We clamp it between 0 and 1.
        # Hypothesis: Higher STAI reduces w.
        # We model this as w = w_base * (1 - STAI)
        # If STAI is 1.0 (max anxiety), w becomes 0 (pure model-free).
        self.w = np.clip(self.w_base * (1.0 - self.stai), 0.0, 1.0)
        
        # Initialize Model-Based values (Q_MB)
        # Q_MB is computed on the fly, but we need storage for Q_MF (Model-Free)
        # The base class q_stage1 will serve as Q_MF for stage 1.
        # The base class q_stage2 is used by both systems (terminal values).

    def policy_stage1(self) -> np.ndarray:
        # 1. Compute Model-Based Values (Q_MB)
        # Q_MB(a1) = sum(P(s|a1) * max(Q_stage2(s, :)))
        # We use the fixed transition matrix self.T
        # self.T[action, state] is probability of transition
        
        # Max value of stage 2 for each state
        v_stage2 = np.max(self.q_stage2, axis=1) 
        
        q_mb = np.zeros(self.n_choices)
        for a in range(self.n_choices):
            # T is shape (2, 2) -> [action, next_state] usually, but here defined as [35, 15] counts
            # The base class defines self.T as row-normalized.
            # Assuming row 0 is action A, row 1 is action U? 
            # The task description says "Spaceship A... Spaceship U".
            # Let's assume index 0 -> A, index 1 -> U.
            q_mb[a] = np.sum(self.T[a] * v_stage2)
            
        # 2. Combine with Model-Free Values (self.q_stage1)
        q_net = self.w * q_mb + (1 - self.w) * self.q_stage1
        
        return self.softmax(q_net, self.beta)

    # policy_stage2 uses default softmax on q_stage2

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Standard TD update for Model-Free values
        
        # Stage 2 update (Terminal)
        delta_2 = reward - self.q_stage2[state, action_2]
        self.q_stage2[state, action_2] += self.alpha * delta_2
        
        # Stage 1 update (Model-Free TD(1) or TD(0))
        # Simple SARSA-like update for MF stage 1: Q(a1) <- Q(a1) + alpha * (Q(s2, a2) - Q(a1))
        # Note: In pure MB/MF models, MF often uses lambda. Here we use simple TD(0) to stage 2 value.
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        self.q_stage1[action_1] += self.alpha * delta_1

cognitive_model1 = make_cognitive_model(ParticipantModel1)
```

### Model 2: Anxiety-Induced Win-Stay/Lose-Shift Amplification
This model hypothesizes that anxiety increases reactive, heuristic behavior. Instead of careful value learning, anxious individuals might rely heavily on a primitive "Win-Stay, Lose-Shift" (WSLS) strategy at the first stage. This heuristic overrides learned values based on the immediate previous outcome.

```python
class ParticipantModel2(CognitiveModelBase):
    """
    Hypothesis: Anxiety amplifies reactive Win-Stay/Lose-Shift (WSLS) heuristics.
    Anxious individuals are hypersensitive to immediate outcomes.
    We add a 'stickiness' bonus to the previously chosen action if rewarded,
    and a 'repulsion' penalty if unrewarded. The magnitude of this heuristic
    is scaled by STAI.
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1] (Learning rate)
    beta: [0, 10] (Inverse temperature)
    heuristic_gain: [0, 5] (Base magnitude of the WSLS effect)
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta, self.heuristic_gain = model_parameters

    def policy_stage1(self) -> np.ndarray:
        q_vals = self.q_stage1.copy()
        
        # Apply WSLS heuristic if there was a previous trial
        if self.last_action1 is not None:
            # Calculate heuristic strength based on anxiety
            # Higher anxiety -> Stronger reaction to last outcome
            strength = self.heuristic_gain * self.stai
            
            if self.last_reward > 0:
                # Win-Stay: Add bonus to the last action
                q_vals[self.last_action1] += strength
            else:
                # Lose-Shift: Subtract penalty (or add negative) to last action
                # Effectively pushing towards the other option
                q_vals[self.last_action1] -= strength
                
        return self.softmax(q_vals, self.beta)

    # Standard value update for underlying values
    # The heuristic is a transient bias added at decision time, not learned.

cognitive_model2 = make_cognitive_model(ParticipantModel2)
```

### Model 3: Anxiety-Driven Punishment Sensitivity (Asymmetric Learning)
This model posits that anxiety fundamentally alters how prediction errors are processed. Specifically, high anxiety is associated with a "negativity bias" or hypersensitivity to punishment (lack of reward). This model implements separate learning rates for positive and negative prediction errors, where the learning rate for negative errors is amplified by the STAI score.

```python
class ParticipantModel3(CognitiveModelBase):
    """
    Hypothesis: Anxiety causes hypersensitivity to negative prediction errors.
    We use dual learning rates: alpha_pos for positive RPEs, and alpha_neg for negative RPEs.
    The negative learning rate is modulated by STAI: alpha_neg = alpha_base * (1 + sensitivity * STAI).
    This means anxious participants update their values more drastically after failure.
    
    Parameter Bounds:
    -----------------
    alpha_pos: [0, 1] (Learning rate for positive outcomes)
    alpha_neg_base: [0, 1] (Base learning rate for negative outcomes)
    neg_sensitivity: [0, 5] (How much STAI amplifies negative learning)
    beta: [0, 10]
    """

    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha_pos, self.alpha_neg_base, self.neg_sensitivity, self.beta = model_parameters

    def init_model(self) -> None:
        # Calculate the effective negative learning rate
        # We allow it to go above 1 theoretically, but practically it implies rapid overwriting.
        # We clip to [0, 1] to maintain stability.
        raw_alpha_neg = self.alpha_neg_base * (1.0 + self.neg_sensitivity * self.stai)
        self.alpha_neg = np.clip(raw_alpha_neg, 0.0, 1.0)

    def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
        # Stage 2 Update
        delta_2 = reward - self.q_stage2[state, action_2]
        alpha_2 = self.alpha_pos if delta_2 >= 0 else self.alpha_neg
        self.q_stage2[state, action_2] += alpha_2 * delta_2
        
        # Stage 1 Update
        # Using the updated stage 2 value as the target
        delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
        alpha_1 = self.alpha_pos if delta_1 >= 0 else self.alpha_neg
        self.q_stage1[action_1] += alpha_1 * delta_1

cognitive_model3 = make_cognitive_model(ParticipantModel3)
```