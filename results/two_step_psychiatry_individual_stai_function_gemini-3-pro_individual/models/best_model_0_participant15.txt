def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid Model with Anxiety-Modulated Choice Stickiness.
    
    This model adds a 'stickiness' bonus to the action chosen in the previous trial.
    The magnitude of this stickiness is modulated by the participant's STAI score.
    Higher anxiety is hypothesized to increase choice repetition (perseveration).
    
    stickiness = stick_base + (stai_stick_sensitivity * normalized_stai)
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0) control.
    stick_base: [0, 5] Baseline stickiness bonus.
    stai_stick_sensitivity: [0, 5] How strongly STAI increases stickiness.
    """
    learning_rate, beta, w, stick_base, stai_stick_sensitivity = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    norm_stai = (stai_val - 20) / 60.0
    norm_stai = np.clip(norm_stai, 0, 1)

    stickiness = stick_base + (stai_stick_sensitivity * norm_stai)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # No previous action for the first trial
    
    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if prev_action_1 != -1:
            q_net[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        prev_action_1 = a1
        
        state_idx = int(state[trial])

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        r = reward[trial]

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2


        value_state_reached = q_stage2_mf[state_idx, a2]
        delta_stage1 = value_state_reached - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss