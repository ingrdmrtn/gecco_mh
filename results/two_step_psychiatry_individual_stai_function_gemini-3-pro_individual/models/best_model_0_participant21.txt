def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Driven Perseveration.
    
    This model introduces a choice perseveration parameter (stickiness) that is 
    modulated by the STAI score. The hypothesis is that higher anxiety leads to 
    higher repetition of the previous choice (cognitive rigidity), independent of 
    reward learning.

    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Weight for model-based control (fixed mixing).
    perseveration_base: [0, 5] Base tendency to repeat the previous choice.
    stai_perseveration_mod: [0, 5] How much STAI amplifies perseveration.
    """
    lr, beta, w, perseveration_base, stai_perseveration_mod = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]


    perseveration = perseveration_base + (stai_perseveration_mod * stai_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    prev_choice_1 = -1

    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if prev_choice_1 != -1:
            q_net[prev_choice_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]

        prev_choice_1 = int(action_1[trial])
        state_idx = int(state[trial])

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        r = reward[trial]
        chosen_a2 = int(action_2[trial])
        chosen_a1 = int(action_1[trial])

        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss