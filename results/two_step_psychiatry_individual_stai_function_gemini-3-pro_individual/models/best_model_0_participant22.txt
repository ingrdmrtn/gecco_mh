def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated Transition Uncertainty.
    
    This model hypothesizes that anxiety degrades the accuracy of the agent's 
    internal model of the environment. While the objective common transition 
    probability is 0.7, anxious individuals may perceive the transitions as 
    more uncertain (closer to 0.5), reducing the effectiveness of Model-Based 
    planning. It also includes a general choice stickiness parameter.

    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature (softmax).
    - w: [0, 1] Weight of Model-Based system (0=MF, 1=MB).
    - stickiness: [0, 5] Bonus for repeating the previous Stage 1 choice.
    - trans_deg_stai: [0, 0.4] Amount STAI reduces the perceived common transition probability.
      (Perceived p = 0.7 - trans_deg_stai * stai).
    """
    learning_rate, beta, w, stickiness, trans_deg_stai = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]


    perceived_common_p = 0.7 - (trans_deg_stai * stai_score)

    if perceived_common_p < 0.5:
        perceived_common_p = 0.5
        
    transition_matrix = np.array([[perceived_common_p, 1-perceived_common_p], 
                                  [1-perceived_common_p, perceived_common_p]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss