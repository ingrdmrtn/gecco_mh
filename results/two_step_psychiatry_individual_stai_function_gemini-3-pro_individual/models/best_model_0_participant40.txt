def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-modulated Perseveration Model.
    
    This model hypothesizes that anxiety (STAI) influences the tendency to repeat 
    previous choices (perseveration or "stickiness") regardless of reward history.
    Higher anxiety may lead to safety behaviors (repeating choices) or avoidance,
    modeled as an additive bias to the logits of the previously chosen action.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Mixing weight (0=Model-Free, 1=Model-Based).
    pers_slope: [0,5] - Sensitivity of perseveration to STAI. 
                        Bonus = pers_slope * STAI.
    """
    learning_rate, beta, w, pers_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]


    pers_bonus = pers_slope * stai_val

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1

    for trial in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        logits = beta * q_net

        if prev_a1 != -1:
            logits[int(prev_a1)] += pers_bonus

        exp_q1 = np.exp(logits - np.max(logits)) 
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        prev_a1 = action_1[trial]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss