Here are three cognitive models that incorporate the anxiety (STAI) score into the decision-making process for the two-step task.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that anxiety levels influence the balance between Model-Based (planning) and Model-Free (habitual) control. Specifically, higher anxiety might impair Model-Based planning due to cognitive load or stress, leading to a greater reliance on Model-Free strategies. The `w` parameter (weight) is modulated by the `stai` score.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated MB/MF Trade-off.
    
    This model assumes that the balance between Model-Based (planning) and 
    Model-Free (habitual) control is influenced by anxiety. A base mixing weight (w_base)
    is adjusted by the STAI score. Higher anxiety reduces the weight of model-based
    planning (w), reflecting reduced cognitive flexibility or planning capacity under stress.

    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w_base: [0, 1] Base weight for model-based control (before anxiety modulation).
    stai_sensitivity: [0, 1] How strongly STAI reduces the model-based weight.
    """
    lr, beta, w_base, stai_sensitivity = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0] # STAI is constant for the participant

    # Calculate effective mixing weight w based on anxiety
    # We clip to ensure w stays in [0, 1]. 
    # Logic: Higher anxiety (stai) reduces w (shift towards model-free).
    w = w_base * (1.0 - (stai_sensitivity * stai_val))
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2) # Model-free Q-values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Q-values for stage 2 (2 states, 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Transition matrix * max(Stage 2 values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax policy
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        # Standard Model-Free Q-learning for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Learning Updates ---
        # Prediction errors
        # Note: We use a simple TD(0) / SARSA-like structure for the template logic
        
        # Stage 2 update (State 2 -> Reward)
        r = reward[trial]
        chosen_a2 = int(action_2[trial])
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Stage 1 update (State 1 -> State 2)
        # Using the value of the state actually reached
        chosen_a1 = int(action_1[trial])
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Learning Rate Modulation
This model posits that anxiety affects how quickly participants update their beliefs (learning rate). High anxiety might lead to hyper-reactivity to feedback (higher learning rate, "jumping to conclusions") or rigidity. Here, we model it as a multiplicative modifier on the learning rate, allowing the data to determine if anxiety increases or decreases learning speed.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Driven Learning Rate Modulation.
    
    This model assumes anxiety affects plasticity (learning rate).
    The STAI score scales the learning rate. A parameter `stai_mod` determines
    the direction and magnitude. If stai_mod > 0.5 (in normalized space),
    anxiety increases learning rate; otherwise it dampens it.
    
    Parameters:
    base_lr: [0, 1] Base learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Fixed mixing weight between MB and MF.
    stai_mod: [0, 1] Modifier for learning rate based on anxiety.
    """
    base_lr, beta, w, stai_mod = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Modulate learning rate based on STAI
    # We map stai_mod to a multiplier range (e.g., 0.5x to 1.5x) centered around 1
    # If stai_mod is high, anxiety increases LR. If low, anxiety decreases LR.
    multiplier = 0.5 + (stai_mod * 1.5) 
    
    # Calculate effective learning rate
    # The effect is proportional to the STAI score relative to a baseline of 0.4 (approx mean)
    lr = base_lr * (1.0 + (stai_val - 0.4) * (multiplier - 1.0))
    lr = np.clip(lr, 0.01, 1.0) # Ensure valid bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        r = reward[trial]
        chosen_a2 = int(action_2[trial])
        chosen_a1 = int(action_1[trial])

        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Decision Noise (Temperature Modulation)
This model suggests that anxiety primarily acts as a disruptor of consistent decision-making, increasing exploration or noise. This is modeled by modifying the `beta` (inverse temperature) parameter. Higher anxiety leads to a lower `beta` (more randomness/noise), representing the "choking under pressure" or difficulty concentrating often associated with high STAI scores.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Induced Decision Noise.
    
    This model posits that anxiety increases decision noise (randomness).
    The STAI score is used to down-regulate the inverse temperature (beta).
    High anxiety results in a lower effective beta (flatter softmax),
    meaning choices are less deterministic and less driven by value differences.

    Parameters:
    lr: [0, 1] Learning rate.
    beta_max: [0, 10] Maximum inverse temperature (for low anxiety).
    w: [0, 1] Mixing weight (MB vs MF).
    stai_noise_impact: [0, 1] Strength of anxiety's noise-inducing effect.
    """
    lr, beta_max, w, stai_noise_impact = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective beta
    # As STAI increases, beta decreases (more noise).
    # stai_noise_impact controls how severe this drop-off is.
    beta = beta_max * (1.0 - (stai_val * stai_noise_impact))
    beta = np.maximum(beta, 0.0) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        r = reward[trial]
        chosen_a2 = int(action_2[trial])
        chosen_a1 = int(action_1[trial])

        # Update Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```