Here are three cognitive models designed to explain the participant's behavior, incorporating their medium anxiety (STAI) score into the decision-making process.

### Model 1: Anxiety-Modulated Model-Based/Model-Free Trade-off
This model hypothesizes that anxiety (STAI) influences the balance between Model-Based (planning) and Model-Free (habitual) control. Medium anxiety might lead to a specific mixing weight `w` that is a function of the STAI score, rather than a free parameter alone. Specifically, this model tests if anxiety biases the user towards habitual (MF) control by modulating the mixing weight.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated Hybrid Model.
    
    This model assumes a hybrid reinforcement learning agent where the balance 
    between Model-Based (MB) and Model-Free (MF) control at the first stage 
    is modulated by the participant's anxiety level (STAI).
    
    The mixing weight 'w' is derived from a baseline parameter and the STAI score.
    Higher anxiety is hypothesized to reduce model-based planning capacity.

    Parameters:
    - learning_rate: [0, 1] Rate at which Q-values are updated.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_base: [0, 1] Baseline weight for Model-Based control (before anxiety modulation).
    - w_stai_sens: [0, 1] Sensitivity of the mixing weight to the STAI score.
    """
    learning_rate, beta, w_base, w_stai_sens = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate effective mixing weight w.
    # We model it such that higher STAI reduces w (more Model-Free).
    # We clip to ensure it stays valid [0, 1].
    w = w_base - (w_stai_sens * stai_score)
    w = np.clip(w, 0.0, 1.0)

    # Transition matrix (fixed for this task structure)
    # Row 0: transitions from Choice 0 (A) -> 70% to State 0 (X), 30% to State 1 (Y)
    # Row 1: transitions from Choice 1 (U) -> 30% to State 0 (X), 70% to State 1 (Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values initialization
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (2 states x 2 actions)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based value calculation: V(state) = max(Q_stage2)
        # Q_MB = Transition_Matrix * V(state)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: combination of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax selection for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        
        # Standard Q-learning for Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Value Updates ---
        # Stage 1 MF update (TD(1) logic or SARSA-like for the transition)
        # Note: In pure Daw tasks, Q_MF(s1) is often updated by Q_MF(s2) or Reward.
        # Here we use the standard TD error from the second stage value.
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Learning Rate Asymmetry
This model suggests that anxiety affects *how* the participant learns from outcomes. Anxious individuals might be more sensitive to negative outcomes (lack of reward) or "loss" signals. This model splits the learning rate into positive (`alpha_pos`) and negative (`alpha_neg`) components, where the magnitude of the negative learning rate is scaled by the STAI score.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: STAI-Scaled Asymmetric Learning Rates.
    
    This model posits that anxiety influences sensitivity to prediction errors.
    Specifically, the learning rate for negative prediction errors (when reward is 0)
    is scaled by the STAI score.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (Reward = 1).
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - alpha_stai_scaling: [0, 1] How much STAI amplifies the negative learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_eligibility: [0, 1] Eligibility trace decay (reinforces stage 1 choice based on stage 2 outcome).
    """
    alpha_pos, alpha_neg_base, alpha_stai_scaling, beta, lambda_eligibility = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate effective negative learning rate
    # Higher anxiety -> higher learning rate from failure (hypersensitivity to error)
    alpha_neg = alpha_neg_base + (alpha_stai_scaling * stai_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updating ---
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        r = reward[trial]
        
        # Calculate Stage 2 Prediction Error
        pe_2 = r - q_stage2[state_idx, chosen_a2]
        
        # Select learning rate based on sign of PE
        lr = alpha_pos if pe_2 >= 0 else alpha_neg
        
        # Update Stage 2 Q-value
        q_stage2[state_idx, chosen_a2] += lr * pe_2
        
        # Update Stage 1 Q-value using eligibility trace (TD(lambda) logic)
        # The update is driven by the stage 2 PE, scaled by lambda
        q_stage1[chosen_a1] += lr * lambda_eligibility * pe_2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Exploratory Noise
This model hypothesizes that anxiety disrupts decision consistency. Instead of affecting learning rates or planning depth, the STAI score is used to modify the "temperature" of the softmax function. A base beta is modified by the STAI score, testing the hypothesis that medium anxiety might lead to more erratic (or conversely, more rigid) choice behavior compared to the baseline.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: STAI-Modulated Inverse Temperature (Exploration/Exploitation).
    
    This model assumes the participant uses a Model-Based strategy, but their
    decision consistency (beta) is a function of their anxiety.
    
    Beta_effective = Beta_base * (1 + (stai_param * (stai - 0.4)))
    
    This allows anxiety to either sharpen (make more deterministic) or flatten
    (make more random) the probability distribution.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta_base: [0, 10] Base inverse temperature.
    - stai_mod: [0, 5] Magnitude of STAI modulation on beta.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    """
    learning_rate, beta_base, stai_mod, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Modulate Beta based on STAI. 
    # We center STAI around a rough population mean (0.4) to allow bidirectional effects.
    # If stai_mod is high, anxiety changes randomness significantly.
    beta_effective = beta_base * (1.0 + stai_mod * (stai_score - 0.4))
    beta_effective = np.clip(beta_effective, 0.0, 10.0) # Ensure valid bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Calculate Model-Based values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax with modulated beta
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Updates ---
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        
        # TD Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # TD Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```