Here are three new cognitive models that incorporate the anxiety (STAI) score into the decision-making process in different ways.

### Model 1: Anxiety-Modulated Learning Rates
This model hypothesizes that high anxiety leads to "hyper-learning" or over-sensitivity to prediction errors. Instead of modulating the balance between model-based and model-free control, anxiety here scales the learning rate directly. High anxiety participants might update their beliefs too drastically after every outcome, making their behavior volatile.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Learning Rate Model.
    
    Hypothesis: Anxiety (stai) increases the learning rate, representing 
    heightened sensitivity to prediction errors and environmental volatility.
    The effective learning rate is a function of a base rate and the STAI score.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Weight for model-based control (mixing parameter).
    - lr_mod: [0, 1] Strength of anxiety modulation on learning rate.
    """
    lr_base, beta, w, lr_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Effective learning rate scales with anxiety. 
    # We clip to ensure it stays in [0, 1].
    # If lr_mod is high, anxiety pushes learning rate towards 1.
    learning_rate = np.clip(lr_base + (lr_mod * stai_score), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Update Stage 2 Q-values
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        # Update Stage 1 Q-values (Model-Free)
        # Using SARSA-like update with the Q-value of the state actually reached
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Exploration (Temperature Scaling)
This model posits that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety might lead to more "noisy" or random behavior (higher entropy) due to difficulty concentrating or indecisiveness. Alternatively, it could lead to rigid exploitation. Here, we model anxiety as modulating the inverse temperature parameter `beta`.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Inverse Temperature Model.
    
    Hypothesis: Anxiety (stai) alters the decision noise (beta). 
    Higher anxiety reduces beta (increases temperature), leading to more 
    random/exploratory choices, reflecting uncertainty or difficulty selecting the optimal action.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-value updates.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - beta_decay: [0, 1] Factor by which anxiety reduces beta.
    """
    learning_rate, beta_base, w, beta_decay = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Effective beta is reduced by anxiety. 
    # If beta_decay is 0, anxiety has no effect. 
    # If beta_decay is 1, max anxiety (stai=1) would reduce beta to 0 (pure random).
    beta = beta_base * (1.0 - (beta_decay * stai_score))
    # Ensure beta doesn't go negative
    beta = max(0.0, beta)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Choice Perseveration
This model suggests that anxiety leads to repetitive behaviors or "stickiness." High anxiety participants might stick to their previous choice regardless of the outcome (perseveration) as a safety behavior or to reduce cognitive load. The STAI score modulates the strength of this perseveration bonus.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Choice Perseveration Model.
    
    Hypothesis: Anxiety (stai) increases choice stickiness (perseveration).
    Anxious individuals may repeat previous stage-1 choices to reduce cognitive load 
    or avoid the regret of switching.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - p_mod: [0, 5] Strength of anxiety-driven perseveration bonus.
    """
    learning_rate, beta, w, p_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Perseveration bonus scales with anxiety
    perseveration_weight = p_mod * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the Q-values before softmax
        # This is a temporary addition for choice selection, not a Q-value update
        q_net_perseveration = q_net.copy()
        if last_action_1 != -1:
            q_net_perseveration[last_action_1] += perseveration_weight
        
        exp_q1 = np.exp(beta * q_net_perseveration)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        last_action_1 = a1 # Update last action

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```