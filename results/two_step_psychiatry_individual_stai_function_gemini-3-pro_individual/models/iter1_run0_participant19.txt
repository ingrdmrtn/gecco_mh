Here are three new cognitive models that incorporate the STAI anxiety score into the learning and decision-making processes in distinct, theoretically grounded ways.

### Model 1: Anxiety-Modulated Punishment Sensitivity
This model hypothesizes that anxiety increases the learning rate specifically for negative outcomes (loss aversion in learning). High anxiety participants update their value estimates more drastically after a loss (negative reward) than low anxiety participants.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Punishment Sensitivity.
    
    Hypothesis: Higher STAI scores lead to faster learning (higher learning rates)
    specifically when outcomes are negative (punishments), reflecting a 
    negativity bias or increased sensitivity to threat common in anxiety.
    
    Parameters:
    - learning_rate_base: [0, 1] Baseline learning rate for positive/neutral outcomes.
    - neg_lr_scale: [0, 5] Multiplier for the learning rate when reward is negative, scaled by STAI.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    """
    learning_rate_base, neg_lr_scale, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning Rate Modulation ---
        # If reward is negative, boost learning rate based on STAI
        current_lr = learning_rate_base
        if reward[trial] < 0:
            # Scale up LR by anxiety level. 
            # If STAI is high, the multiplier increases.
            current_lr = learning_rate_base * (1.0 + (neg_lr_scale * stai_val))
            current_lr = np.min([current_lr, 1.0]) # Cap at 1.0

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Perseveration (Stickiness)
This model posits that anxiety leads to rigid, repetitive behavior (perseveration). Instead of affecting the MB/MF balance, the STAI score drives a "stickiness" parameter, making the participant more likely to repeat their previous first-stage choice regardless of the calculated value.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Induced Choice Stickiness.
    
    Hypothesis: Anxiety increases 'stickiness' (perseveration). High STAI 
    participants are more likely to repeat their previous Stage 1 choice 
    to reduce cognitive load or avoid decision conflict.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - stick_stai_slope: [0, 5] How strongly STAI increases the stickiness bonus.
    """
    learning_rate, beta, w, stick_stai_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate stickiness bonus based on anxiety
    # Higher anxiety = higher bonus added to the previously chosen option
    stickiness_bonus = stick_stai_slope * stai_val 

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness to the action taken in the PREVIOUS trial
        if trial > 0:
            prev_action = action_1[trial - 1]
            q_net[prev_action] += stickiness_bonus

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Eligibility Traces (Lambda)
This model suggests that anxiety affects how credit is assigned to past actions. It uses a TD($\lambda$) reinforcement learning update for the Model-Free system. The parameter $\lambda$ (eligibility trace) is determined by the STAI score. A higher $\lambda$ means the Stage 1 choice is updated more strongly by the final reward (rather than just the Stage 2 value), bypassing the two-step structure.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Eligibility Traces.
    
    Hypothesis: STAI modulates the eligibility trace parameter (lambda).
    This determines if the Stage 1 MF value is updated via the Stage 2 value (TD-0)
    or directly via the outcome (TD-1). Anxiety may cause 'over-attribution' 
    of the final outcome to the initial choice (high lambda).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - lambda_scale: [0, 1] Scales STAI to produce the eligibility trace lambda.
    """
    learning_rate, beta, w, lambda_scale = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Lambda is derived from STAI. 
    # If lambda is 0, Stage 1 updates only from Stage 2 prediction (Standard TD).
    # If lambda is 1, Stage 1 updates directly from Reward (Monte Carlo-like).
    lambda_val = np.clip(lambda_scale * stai_val, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates with Eligibility Trace ---
        
        # Calculate RPEs
        # Delta 1: Difference between Stage 2 value and Stage 1 value
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Delta 2: Difference between Reward and Stage 2 value
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 (Standard)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 (TD-Lambda)
        # Stage 1 learns from its own RPE, PLUS a portion (lambda) of Stage 2's RPE.
        # This effectively connects Stage 1 choice to the final Reward.
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + (lambda_val * delta_stage2))

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```