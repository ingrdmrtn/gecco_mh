Here are three new cognitive models based on the two-step task, incorporating STAI scores to explain individual variability in decision-making.

### Model 1: STAI-Modulated Learning Rate Asymmetry
This model hypothesizes that anxiety (STAI) affects how participants learn from positive versus negative prediction errors. High anxiety might lead to increased sensitivity to negative outcomes (loss aversion or safety signaling) or different learning rates for stage 2 updates.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Asymmetric Learning Rates Modulated by STAI.
    
    Hypothesis: Anxiety (STAI) alters the balance between learning from positive
    vs. negative prediction errors. Specifically, higher anxiety increases the learning rate
    for negative prediction errors (punishment sensitivity) relative to positive ones.
    
    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors.
    lr_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    stai_amp: [0, 5] Amplification factor of STAI on the negative learning rate.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Fixed Model-Based/Model-Free mixing weight.
    """
    lr_pos, lr_neg_base, stai_amp, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Modulate negative learning rate by STAI
    # If prediction error is negative, the effective LR increases with anxiety
    lr_neg = lr_neg_base * (1.0 + stai_amp * stai_val)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        current_lr = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, act2] += current_lr * delta_stage2
        
        # Stage 1 Update (SARSA-style TD(0))
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        # We use the same asymmetry logic for stage 1 updates
        current_lr_s1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[act1] += current_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: STAI-Dependent Exploration (Inverse Temperature)
This model posits that anxiety influences decision noise or exploration. High anxiety might lead to more rigid, exploitative behavior (higher beta) or, conversely, erratic behavior (lower beta) depending on the stress response. Here we model it as anxiety increasing `beta` (reducing randomness/exploration).

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: STAI-Dependent Inverse Temperature (Beta).
    
    Hypothesis: Anxiety modulates the exploration-exploitation trade-off.
    Higher anxiety leads to higher beta (less exploration/more deterministic choices),
    reflecting a desire to stick to what is known or safe.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature.
    stai_beta_slope: [0, 5] How strongly STAI increases beta.
    w: [0, 1] Model-Based/Model-Free mixing weight.
    """
    lr, beta_base, stai_beta_slope, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective beta based on STAI
    beta = beta_base + (stai_beta_slope * stai_val)
    # Ensure beta doesn't explode, though high beta is valid
    beta = np.clip(beta, 0.0, 20.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: STAI-Modulated Choice Perseveration
This model suggests that anxiety drives "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward outcome. This is often observed in anxious individuals as a safety behavior or cognitive inflexibility.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: STAI-Modulated Choice Perseveration (Stickiness).
    
    Hypothesis: Anxiety increases choice stickiness (perseveration). 
    Anxious individuals are more likely to repeat their previous Stage 1 choice, 
    independent of the value learning.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based/Model-Free mixing weight.
    persev_base: [0, 5] Baseline perseveration bonus.
    stai_persev: [0, 5] Additional perseveration bonus proportional to STAI.
    """
    lr, beta, w, persev_base, stai_persev = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective perseveration parameter
    persev_weight = persev_base + (stai_persev * stai_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the Q-value of the previously chosen action
        q_net_persev = q_net.copy()
        if last_action_1 != -1:
            q_net_persev[last_action_1] += persev_weight
        
        exp_q1 = np.exp(beta * q_net_persev)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        # Update tracker
        last_action_1 = act1
        
        # --- Stage 2 Choice ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```