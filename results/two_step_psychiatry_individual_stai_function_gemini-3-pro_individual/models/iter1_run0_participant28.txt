Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process in distinct ways.

### Cognitive Model 1: Anxiety-Modulated Perseveration
This model hypothesizes that anxiety increases "stickiness" or perseveration. High-anxiety participants may feel safer repeating their previous choice (a safety behavior) rather than switching, regardless of the actual value of the options.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF Model with Anxiety-Modulated Perseveration (Stickiness).
    
    Hypothesis: High anxiety increases the tendency to repeat the previous Stage 1 choice
    (perseveration), regardless of the outcome or model-based values. This 'stickiness'
    acts as a heuristic to reduce decision uncertainty.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1] (mixing weight for MB/MF)
    stickiness_scale: [0,5] (Scaling factor: how much STAI adds to the stickiness bonus)
    """
    learning_rate, beta, w, stickiness_scale = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the previous action to apply stickiness
    last_action_1 = -1
    
    # Calculate the magnitude of the stickiness bonus based on anxiety
    stick_bonus = stickiness_scale * stai_val

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of Model-Based and Model-Free values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate base logits
        logits = beta * q_net
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            logits[int(last_action_1)] += stick_bonus
            
        # Softmax calculation
        exp_q1 = np.exp(logits - np.max(logits)) # Subtract max for numerical stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])
        
        # Update tracker for next trial
        last_action_1 = action_1[trial]

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        logits_2 = beta * qs_stage2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Update Stage 1 MF values
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        # Update Stage 2 MF values
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 2: Anxiety-Modulated Negative Learning Bias
This model hypothesizes that anxiety creates an asymmetry in learning. Anxious individuals may be hypersensitive to negative prediction errors (outcomes that are worse than expected), learning faster from them than from positive errors.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF Model with Anxiety-Modulated Negative Learning Bias.
    
    Hypothesis: Anxious individuals are hypersensitive to negative prediction errors 
    (worse than expected outcomes). STAI scales the learning rate specifically 
    when prediction error is negative, leading to faster avoidance learning.
    
    Bounds:
    lr_pos: [0,1] (Base learning rate for positive prediction errors)
    beta: [0,10]
    w: [0,1]
    neg_bias_scale: [0,10] (Multiplier for STAI to boost learning rate when delta < 0)
    """
    lr_pos, beta, w, neg_bias_scale = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate the multiplier for the learning rate when error is negative
    # If delta < 0, effective_lr = lr_pos * (1 + bias * stai)
    neg_multiplier = 1.0 + (neg_bias_scale * stai_val)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        logits = beta * q_net
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        logits_2 = beta * qs_stage2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Asymmetric update for Stage 1
        lr_1 = lr_pos
        if delta_stage1 < 0:
            lr_1 = lr_pos * neg_multiplier
        lr_1 = min(lr_1, 1.0) # Clip to valid range
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        
        # Asymmetric update for Stage 2
        lr_2 = lr_pos
        if delta_stage2 < 0:
            lr_2 = lr_pos * neg_multiplier
        lr_2 = min(lr_2, 1.0) # Clip to valid range
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Cognitive Model 3: Anxiety-Induced Decision Noise
This model hypothesizes that high anxiety acts as a distractor or stressor that degrades the precision of decision-making. High STAI scores reduce the effective `beta` (inverse temperature), making choices more random (noisier) and less driven by learned values.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF Model with Anxiety-Induced Decision Noise.
    
    Hypothesis: High anxiety acts as a distractor, increasing decision noise and 
    reducing the influence of learned values on choice (exploration/randomness). 
    STAI scales down the inverse temperature (beta).
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10] (Baseline inverse temperature for low anxiety)
    w: [0,1]
    noise_factor: [0,5] (Strength of STAI's reduction of beta)
    """
    learning_rate, beta_base, w, noise_factor = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective beta
    # Higher anxiety -> Higher denominator -> Lower Beta -> More random/noisy
    effective_beta = beta_base / (1.0 + (noise_factor * stai_val))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use effective_beta instead of a fixed beta
        logits = effective_beta * q_net
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        logits_2 = effective_beta * qs_stage2
        exp_q2 = np.exp(logits_2 - np.max(logits_2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Update Stage 1
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        # Update Stage 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```