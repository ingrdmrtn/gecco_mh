Here are three cognitive models incorporating STAI scores into the two-step task decision-making process.

### Model 1: Anxiety-Modulated Learning Rate Asymmetry
This model hypothesizes that high anxiety leads to a bias in how prediction errors are processed. Specifically, highly anxious individuals might learn more from negative outcomes (or lack of reward) than positive ones, or vice versa. Given the high STAI score (0.9625), this model scales the learning rate for negative prediction errors differently based on anxiety.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Learning Rate Asymmetry.

    Hypothesis: Anxiety (stai) modulates the ratio between learning from positive
    vs. negative prediction errors. High anxiety amplifies learning from
    negative surprises (punishment sensitivity).

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based vs Model-Free (fixed).
    - anxiety_sens: [0, 5] Scaling factor for how much STAI increases negative learning rate.
    """
    lr_pos, beta, w, anxiety_sens = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate negative learning rate based on anxiety
    # If anxiety is high, lr_neg becomes larger relative to lr_pos
    # Bounded to [0, 1]
    lr_neg = lr_pos * (1.0 + anxiety_sens * stai_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        chosen_a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        chosen_a2 = int(action_2[trial])

        # --- Updates ---
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        lr_1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[chosen_a1] += lr_1 * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        lr_2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, chosen_a2] += lr_2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Choice Randomness (Noise)
This model posits that high anxiety acts as a distractor, reducing the precision of choices. Instead of altering the balance between model-based and model-free systems, anxiety here directly degrades the softmax inverse temperature (`beta`). A high STAI score effectively lowers `beta`, making choices more random (noisier) and less driven by value differences.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Choice Randomness.

    Hypothesis: High anxiety acts as cognitive noise, reducing the precision 
    of value-based choices. The effective inverse temperature (beta) is 
    attenuated by the STAI score.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta_base: [0, 10] Baseline inverse temperature (max precision).
    - w: [0, 1] Mixing weight for Model-Based vs Model-Free.
    - noise_factor: [0, 1] How strongly anxiety reduces beta.
    """
    learning_rate, beta_base, w, noise_factor = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Effective beta decreases as anxiety increases
    # If noise_factor is 0, beta is unaffected. If 1, high anxiety can drive beta near 0.
    beta_eff = beta_base * (1.0 - (noise_factor * stai_score))
    beta_eff = np.maximum(beta_eff, 0.0) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        chosen_a1 = int(action_1[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        chosen_a2 = int(action_2[trial])

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Perseveration
This model suggests that anxiety increases "stickiness" or perseverationâ€”a tendency to repeat the previous choice regardless of reward. Highly anxious individuals may fall back on repetitive motor responses as a coping mechanism or due to cognitive rigidity. The STAI score scales a perseveration bonus added to the Q-values of the previously chosen action.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Perseveration.

    Hypothesis: Anxiety increases choice stickiness (perseveration). 
    A bonus is added to the previously chosen action's value, scaled by STAI.

    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Mixing weight (MB vs MF).
    - pers_k: [0, 5] Base perseveration strength.
    """
    learning_rate, beta, w, pers_k = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Perseveration bonus is amplified by anxiety
    perseveration_weight = pers_k * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # No previous action at start

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to Q-values temporarily for choice
        q_choice = q_net.copy()
        if last_action_1 != -1:
            q_choice[last_action_1] += perseveration_weight
        
        exp_q1 = np.exp(beta * q_choice)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        chosen_a1 = int(action_1[trial])
        last_action_1 = chosen_a1

        # --- Stage 2 Choice ---
        # (Standard softmax for stage 2, no perseveration assumed here for simplicity)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        chosen_a2 = int(action_2[trial])

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```