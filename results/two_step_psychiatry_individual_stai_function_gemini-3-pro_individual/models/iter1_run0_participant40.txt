Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process in different ways.

### Model 1: STAI-Modulated Inverse Temperature (Beta)
This model tests the hypothesis that anxiety affects the exploration-exploitation trade-off. High anxiety might lead to "choking" (rigidity/high beta) or panic/confusion (randomness/low beta). The STAI score modulates the inverse temperature parameter `beta`.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model where Inverse Temperature (Beta) is modulated by STAI.
    
    Hypothesis: Anxiety affects the consistency of choices. A high STAI score
    modulates the beta parameter, making the participant either more rigid
    (exploitative) or more random (exploratory/noisy).

    Parameters:
    learning_rate: [0,1] - Value update rate.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    beta_base: [0,10] - Baseline inverse temperature.
    beta_slope: [-10,10] - How much STAI shifts beta from the baseline.
    """
    learning_rate, w, beta_base, beta_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Modulate beta based on anxiety score
    # Centered at 0.4 (approx cut-off for low/medium anxiety)
    beta = beta_base + beta_slope * (stai_val - 0.4)
    beta = np.clip(beta, 0.0, 10.0) # Ensure beta stays within valid bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Value Updating ---
        # Update Stage 1 (TD(0) update using Stage 2 values)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 (Prediction error from reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: STAI-Modulated Learning Rate
This model tests the hypothesis that anxiety affects plasticity or volatility estimation. Anxious individuals might react more strongly to prediction errors (higher learning rate) or be more resistant to updating beliefs (lower learning rate).

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model where the Learning Rate is modulated by STAI.
    
    Hypothesis: Anxiety affects how quickly participants update their value 
    estimates. High anxiety might correlate with a higher learning rate 
    (over-reacting to recent outcomes).

    Parameters:
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (0=MF, 1=MB).
    lr_base: [0,1] - Baseline learning rate.
    lr_slope: [-1,1] - How much STAI shifts the learning rate.
    """
    beta, w, lr_base, lr_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Modulate learning rate based on anxiety score
    lr = lr_base + lr_slope * (stai_val - 0.4)
    lr = np.clip(lr, 0.0, 1.0) # Ensure learning rate stays [0,1]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: STAI-Modulated Loss Sensitivity
This model tests the hypothesis that anxiety increases sensitivity to negative outcomes (loss aversion). The STAI score amplifies the subjective magnitude of negative rewards, making punishments feel more severe to anxious participants.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model where negative reward perception is modulated by STAI.
    
    Hypothesis: Anxiety is associated with increased loss aversion. This model
    scales negative rewards based on the STAI score. If the participant receives
    a negative reward, its effective value is amplified by their anxiety level.

    Parameters:
    learning_rate: [0,1] - Value update rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    loss_sens: [0, 5] - Sensitivity multiplier. If >0, anxiety amplifies losses.
    """
    learning_rate, beta, w, loss_sens = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate loss multiplier based on anxiety
    # If loss_sens is 1.0 and STAI is 0.5, a reward of -1 becomes -1.5
    loss_factor = 1.0 + (loss_sens * stai_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Value Updating ---
        # Modulate reward if it is a loss (negative)
        r = reward[trial]
        if r < 0:
            r = r * loss_factor
            
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```