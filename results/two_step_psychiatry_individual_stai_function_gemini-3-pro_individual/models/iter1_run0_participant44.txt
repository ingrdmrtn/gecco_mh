Here are three new cognitive models expressed as Python functions. These models explore different mechanisms by which anxiety (STAI) might influence decision-making parameters like learning rates, exploration (beta), and perseveration.

### Model 1: Anxiety-Modulated Learning Rates
This model hypothesizes that anxiety affects how quickly participants update their beliefs. Specifically, high anxiety might lead to "over-learning" from negative outcomes (or just generally higher volatility), represented by a scaling of the learning rate based on the STAI score.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Learning Rate Model.

    Hypothesis: Higher anxiety (STAI) increases the learning rate, making the participant 
    more reactive to recent outcomes (higher volatility).
    
    Parameters:
    - alpha_base: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - alpha_mod: [0, 1] Strength of anxiety modulation on learning rate.
    """
    alpha_base, beta, alpha_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Anxiety increases the effective learning rate
    learning_rate = alpha_base + (alpha_mod * stai_score)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Pure Model-Free setup for simplicity to isolate LR effects, 
    # but using the template structure which implies potential for hybrid.
    # Here we treat stage 1 as purely driving towards the best stage 2 value (TD(1) style).
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 Q-values
        prediction_error_2 = r - q_stage2[state_idx, act2]
        q_stage2[state_idx, act2] += learning_rate * prediction_error_2
        
        # Update Stage 1 Q-values (Direct reinforcement from reward, ignoring transition structure)
        # This is a standard Model-Free approach.
        prediction_error_1 = r - q_stage1[act1]
        q_stage1[act1] += learning_rate * prediction_error_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Choice Stickiness (Perseveration)
This model posits that anxiety leads to rigid, repetitive behavior. Instead of modulating the balance between planning (MB) and habit (MF), anxiety introduces a "stickiness" parameter, making the participant more likely to repeat their previous Stage 1 choice regardless of the outcome.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Choice Stickiness Model.
    
    Hypothesis: High anxiety increases 'stickiness' (perseveration), causing participants
    to repeat their previous stage-1 choice regardless of reward history.
    
    Parameters:
    - learning_rate: [0, 1] Rate of Q-value updating.
    - beta: [0, 10] Inverse temperature.
    - stick_base: [0, 5] Baseline stickiness parameter.
    - stick_mod: [0, 5] Modulation of stickiness by anxiety.
    """
    learning_rate, beta, stick_base, stick_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Stickiness increases with anxiety
    stickiness = stick_base + (stick_mod * stai_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice
    last_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Add stickiness bonus to the Q-values before softmax
        q_net = q_stage1_mf.copy()
        if last_choice_1 != -1:
            q_net[last_choice_1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        last_choice_1 = act1
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        r = reward[trial]
        
        # Standard TD updates (Model-Free)
        # Stage 2 update
        q_stage2_mf[state_idx, act2] += learning_rate * (r - q_stage2_mf[state_idx, act2])
        
        # Stage 1 update (using the reward directly for simplicity in this MF formulation)
        q_stage1_mf[act1] += learning_rate * (r - q_stage1_mf[act1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Exploration (Inverse Temperature)
This model suggests that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety might lead to more random, erratic behavior (lower beta) due to difficulty concentrating, or conversely, extremely rigid exploitation (higher beta). Here, we model it such that anxiety reduces the `beta` parameter, leading to "noisier" choices (lower signal-to-noise ratio in value-based decision making).

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Inverse Temperature (Beta) Model.
    
    Hypothesis: High anxiety reduces the inverse temperature (beta), leading to 
    more stochastic/random choices (exploration or noise) and less reliance on learned values.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_decay: [0, 10] Amount beta decreases as anxiety increases.
    - w: [0, 1] Weight for Model-Based control (fixed).
    """
    learning_rate, beta_base, beta_decay, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Beta decreases as anxiety increases (more noise/exploration)
    beta = beta_base - (beta_decay * stai_score)
    beta = np.maximum(beta, 0.0) # Ensure beta doesn't go negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice (Hybrid MB/MF) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Learning ---
        r = reward[trial]
        
        # TD Learning
        # Update Stage 2
        q_stage2_mf[state_idx, act2] += learning_rate * (r - q_stage2_mf[state_idx, act2])
        
        # Update Stage 1 (TD(1) style, driven by reward directly)
        q_stage1_mf[act1] += learning_rate * (r - q_stage1_mf[act1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```