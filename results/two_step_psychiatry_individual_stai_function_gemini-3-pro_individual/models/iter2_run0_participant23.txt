Here are three new cognitive models based on the provided template and task description.

### Model 1: Anxiety-Modulated Eligibility Trace (Credit Assignment)
This model hypothesizes that anxiety interferes with **credit assignment**. In Reinforcement Learning, the "eligibility trace" (often denoted by lambda) determines how much the final reward (Stage 2) updates the value of the initial choice (Stage 1). A low eligibility trace implies "fragmented" learning, where the agent learns the immediate transition but fails to propagate the final reward back to the first decision. This model proposes that higher STAI scores reduce this eligibility trace.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    RL Model where STAI modulates the Eligibility Trace (lambda).
    
    Hypothesis: High anxiety (STAI) disrupts the cognitive map's ability to 
    propagate credit from the final reward back to the first stage choice.
    Higher anxiety leads to a lower lambda, resulting in more fragmented learning.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weight for model-based control (0=MF, 1=MB).
    lambda_base: [0, 1] - Baseline eligibility trace (efficiency of Stage 2 reward updating Stage 1).
    stai_lambda_damp: [0, 2] - Strength of STAI's dampening effect on lambda.
    """
    learning_rate, beta, w, lambda_base, stai_lambda_damp = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Calculate effective lambda based on STAI
    # Higher anxiety reduces lambda towards 0
    lambda_eff = lambda_base / (1.0 + stai_lambda_damp * current_stai)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Action value updating
        act1 = int(action_1[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        delta_stage2 = r - q_stage2_mf[state_idx, act2]

        # Update Stage 1: Standard TD update + Eligibility Trace from Stage 2
        q_stage1_mf[act1] += learning_rate * delta_stage1 + learning_rate * lambda_eff * delta_stage2
        
        # Update Stage 2: Standard TD update
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Negativity Bias (Asymmetric Learning)
This model hypothesizes that anxiety creates a **negativity bias** in learning. Anxious individuals may be more sensitive to negative prediction errors (outcomes that are worse than expected, i.e., receiving 0 coins when expecting some). This model scales the learning rate for negative prediction errors based on the STAI score, causing the participant to "over-learn" from disappointments.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    RL Model where STAI modulates Learning Rate Asymmetry (Negativity Bias).
    
    Hypothesis: Anxious participants are more sensitive to negative outcomes (omission of reward).
    When the prediction error is negative (disappointment), the learning rate is boosted by STAI.

    Parameters:
    learning_rate: [0, 1] - Baseline learning rate.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weight for model-based control.
    stai_neg_bias: [0, 10] - Scaling factor for learning rate when prediction error is negative.
    """
    learning_rate, beta, w, stai_neg_bias = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        act1 = int(action_1[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        delta_stage2 = r - q_stage2_mf[state_idx, act2]

        # FILL IN ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[act1] += learning_rate * delta_stage1

        # FILL IN ACTION VALUE UPDATING FOR CHOICE 2
        # Determine effective learning rate based on sign of prediction error
        if delta_stage2 < 0:
            # Boost learning rate for negative surprises (Negativity Bias)
            lr_eff = learning_rate * (1.0 + stai_neg_bias * current_stai)
            # Cap LR at 1.0 to maintain stability
            lr_eff = min(lr_eff, 1.0)
        else:
            lr_eff = learning_rate
            
        q_stage2_mf[state_idx, act2] += lr_eff * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Memory Decay (Interference)
This model hypothesizes that anxiety consumes cognitive resources (working memory), leading to faster **decay (forgetting)** of learned values for options that are *not* currently chosen. While the participant updates the chosen option normally, the values of unchosen options decay back toward neutrality (0) more rapidly in high-anxiety individuals.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    RL Model where STAI modulates the Decay Rate of unchosen options.
    
    Hypothesis: Anxiety acts as a cognitive load, causing faster forgetting (decay)
    of the value of options that were not chosen on the current trial.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weight for model-based control.
    decay_base: [0, 1] - Baseline decay rate for unchosen options.
    stai_decay_sens: [0, 5] - Sensitivity of decay rate to STAI score.
    """
    learning_rate, beta, w, decay_base, stai_decay_sens = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Calculate effective decay rate
    decay_rate = decay_base * (1.0 + stai_decay_sens * current_stai)
    # Clamp decay rate to [0, 1]
    decay_rate = min(max(decay_rate, 0.0), 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        act1 = int(action_1[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        delta_stage2 = r - q_stage2_mf[state_idx, act2]

        # FILL IN ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[act1] += learning_rate * delta_stage1
        # Decay unchosen spaceship
        q_stage1_mf[1 - act1] *= (1.0 - decay_rate)

        # FILL IN ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        # Decay unchosen alien on the current planet
        q_stage2_mf[state_idx, 1 - act2] *= (1.0 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```