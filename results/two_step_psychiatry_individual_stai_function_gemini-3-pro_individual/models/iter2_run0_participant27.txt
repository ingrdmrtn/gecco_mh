Here are three new cognitive models formulated as Python functions.

### Model 1: Anxiety-Modulated Punishment Sensitivity
**Hypothesis:** Anxious individuals exhibit a "negativity bias," learning more aggressively from negative prediction errors (disappointments) than from positive ones. This model links the STAI score to an increase in the learning rate specifically when outcomes are worse than expected (negative RPE), while the baseline learning rate applies to positive outcomes.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Punishment Sensitivity.
    
    Hypothesis: Higher anxiety increases the learning rate for negative prediction 
    errors (outcomes worse than expected), reflecting a negativity bias.
    
    Parameters:
    lr_base: [0, 1] Baseline learning rate for positive prediction errors.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    stai_neg_boost: [0, 5] Scaling factor. Increases learning rate for negative RPEs based on STAI.
    """
    lr_base, beta, w, stai_neg_boost = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate the punishment-specific learning rate
    # If STAI is high, lr_neg becomes significantly larger than lr_base
    lr_neg = lr_base * (1.0 + (stai_neg_boost * stai_val))
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        # Apply boosted learning rate if prediction error is negative (disappointment)
        current_lr_2 = lr_neg if delta_stage2 < 0 else lr_base
        q_stage2_mf[state_idx, act2] += current_lr_2 * delta_stage2

        # Stage 1 Update
        # TD(0) update: value of state reached - value of chosen action
        value_state_reached = q_stage2_mf[state_idx, act2]
        delta_stage1 = value_state_reached - q_stage1_mf[act1]
        
        # Apply boosted learning rate if prediction error is negative
        current_lr_1 = lr_neg if delta_stage1 < 0 else lr_base
        q_stage1_mf[act1] += current_lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Decision Noise
**Hypothesis:** Anxiety acts as a cognitive load or distractor, reducing the precision of value-based choices. This model posits that higher STAI scores dampen the inverse temperature ($\beta$), leading to more random (noisier) exploration and less exploitation of learned values.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model where Anxiety increases Decision Noise.
    
    Hypothesis: Anxiety acts as a distractor, effectively lowering the inverse 
    temperature (beta), resulting in more random/noisy choices.
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature (for low anxiety).
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    stai_noise: [0, 5] Scaling factor. Higher STAI reduces effective beta.
    """
    lr, beta_base, w, stai_noise = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Effective Beta is reduced by anxiety.
    # If stai_noise is 0, beta = beta_base.
    # As stai increases, the denominator grows, and beta decreases (more noise).
    beta_eff = beta_base / (1.0 + (stai_noise * stai_val))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use anxiety-modulated beta
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Use anxiety-modulated beta
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += lr * delta_stage2

        value_state_reached = q_stage2_mf[state_idx, act2]
        delta_stage1 = value_state_reached - q_stage1_mf[act1]
        q_stage1_mf[act1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Perseveration
**Hypothesis:** Anxiety is often associated with rigid, repetitive "safety behaviors." This model suggests that higher STAI scores lead to higher "stickiness" or perseveration, where the participant is more likely to repeat their previous Stage 1 choice regardless of the reward outcome.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Driven Perseveration (Stickiness).
    
    Hypothesis: Higher anxiety leads to 'safety behaviors' or rigidity, modeled
    as a tendency to repeat the previous Stage 1 choice (perseveration).
    
    Parameters:
    lr: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    stai_stickiness: [0, 5] Magnitude of the perseveration bonus scaled by STAI.
    """
    lr, beta, w, stai_stickiness = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate the stickiness bonus based on anxiety
    stickiness_magnitude = stai_stickiness * stai_val

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_act1 = None

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        logits = beta * q_net
        if prev_act1 is not None:
            logits[prev_act1] += stickiness_magnitude
            
        exp_q1 = np.exp(logits) # Logits already include beta scaling
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])
        
        # Store for next trial
        prev_act1 = act1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += lr * delta_stage2

        value_state_reached = q_stage2_mf[state_idx, act2]
        delta_stage1 = value_state_reached - q_stage1_mf[act1]
        q_stage1_mf[act1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```