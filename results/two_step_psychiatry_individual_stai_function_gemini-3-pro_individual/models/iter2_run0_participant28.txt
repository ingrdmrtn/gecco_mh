Here are 3 new cognitive models that incorporate the STAI anxiety score into the decision-making process using different mechanisms than the previous feedback.

### Model 1: Anxiety-Modulated Plasticity (Learning Rate)
**Hypothesis:** High anxiety is associated with environmental hyper-vigilance and over-reaction to prediction errors. In this model, the STAI score increases the effective learning rate, causing the participant to update their value estimates more drastically after every outcome, potentially making behavior more volatile.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Plasticity (Learning Rate) Model.
    
    Hypothesis: High anxiety (STAI) leads to hyper-responsiveness to outcomes.
    The effective learning rate is boosted by the STAI score.
    
    Bounds:
    lr_base: [0,1] (Base learning rate)
    beta: [0,10] (Inverse temperature)
    w: [0,1] (Mixing weight, fixed for this model, not modulated)
    lr_mod: [0,1] (Sensitivity of learning rate to anxiety)
    """
    lr_base, beta, w, lr_mod = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Effective learning rate increases with anxiety
    # We clip to ensure it stays valid [0, 1]
    learning_rate = lr_base + (stai_val * lr_mod)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: Mix of MB and MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
  
        # Update Stage 1 (TD(1) approach for MF)
        # Note: We use the stage 2 value to drive the stage 1 MF update (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Decision Noise (Inverse Temperature)
**Hypothesis:** Anxiety acts as neural or cognitive noise that interferes with value-based selection. High STAI scores reduce the inverse temperature ($\beta$), making choices "flatter" (more random) and less sensitive to the calculated value differences (whether MB or MF).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Decision Noise Model.
    
    Hypothesis: Anxiety introduces noise into the decision process.
    Higher STAI scores reduce the inverse temperature (beta), leading to 
    more exploratory/random behavior regardless of value.
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10] (Base inverse temperature)
    w: [0,1] (Mixing weight)
    noise_factor: [0,1] (How strongly anxiety reduces beta)
    """
    learning_rate, beta_base, w, noise_factor = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    # Effective Beta decreases as anxiety increases (more noise/randomness)
    # If noise_factor is high, anxiety significantly flattens the softmax
    beta_effective = beta_base * (1.0 - (stai_val * noise_factor))
    beta_effective = np.maximum(beta_effective, 0.0) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use anxiety-modulated beta
        exp_q1 = np.exp(beta_effective * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_effective * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Perseveration (Stickiness)
**Hypothesis:** Anxious individuals often exhibit "safety behaviors" or rigidity, preferring to repeat previous actions to avoid the uncertainty of switching. In this model, the STAI score scales a "stickiness" parameter, adding a bonus to the previously chosen action at Stage 1, independent of reward history.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Perseveration (Stickiness) Model.
    
    Hypothesis: High anxiety leads to repetitive 'safety' behaviors.
    The STAI score scales the choice stickiness (tendency to repeat last action),
    overriding value-based learning.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    stick_base: [0,5] (Base magnitude of the stickiness bonus)
    """
    learning_rate, beta, w, stick_base = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    # Stickiness magnitude is driven by anxiety
    stickiness_bonus = stick_base * stai_val

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to Q-values before softmax
        # We create a temporary Q vector for decision making so we don't corrupt the learned Qs
        decision_q = q_net_stage1.copy()
        if last_action_1 != -1:
            decision_q[last_action_1] += stickiness_bonus
            
        exp_q1 = np.exp(beta * decision_q)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        last_action_1 = a1 # Update for next trial

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```