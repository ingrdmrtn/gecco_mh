Here are three new cognitive models that incorporate STAI scores to explain decision-making behavior in the two-step task.

### Model 1: Anxiety-Driven Hyper-Learning
This model hypothesizes that anxiety increases the learning rate. Highly anxious individuals may perceive the environment as more volatile or threatening, leading them to update their value estimates too aggressively in response to recent outcomes (over-reaction to prediction errors), rather than integrating evidence slowly over time.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Driven Hyper-Learning.
    
    Hypothesis: High anxiety leads to volatility in value estimation. 
    Participants with high STAI scores have a higher learning rate, causing 
    them to over-update value expectations based on the most recent trial 
    (chasing noise).
    
    Parameters:
    - lr_base: [0, 1] The baseline learning rate for a non-anxious person.
    - lr_anxiety_slope: [0, 1] How much STAI increases the learning rate.
      effective_lr = lr_base + (lr_anxiety_slope * stai)
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    """
    lr_base, lr_anxiety_slope, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate anxiety-modulated learning rate
    learning_rate = lr_base + (lr_anxiety_slope * stai_val)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Value Updating (using anxiety-modulated learning_rate)
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Decision Noise
This model hypothesizes that anxiety acts as a disruptor to decision consistency. Instead of affecting how values are learned (learning rate) or planned (MB weight), anxiety degrades the precision of the choice mechanism itself. High anxiety lowers the inverse temperature ($\beta$), leading to more stochastic (random) behavior regardless of the calculated values.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Induced Decision Noise.
    
    Hypothesis: Anxiety acts as cognitive noise. Higher STAI scores reduce 
    the inverse temperature (beta), making choices more random and less 
    driven by learned values (whether MB or MF).
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta_max: [0, 10] The maximum beta (choice consistency) at 0 anxiety.
    - anxiety_noise: [0, 1] Factor by which STAI reduces beta.
      beta_effective = beta_max * (1 - anxiety_noise * stai)
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    """
    learning_rate, beta_max, anxiety_noise, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate anxiety-modulated beta
    beta_effective = beta_max * (1.0 - (anxiety_noise * stai_val))
    beta_effective = np.clip(beta_effective, 0.0, 10.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_effective here
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Policy
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Value Updating
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Perseveration (Safety Behavior)
This model hypothesizes that anxiety drives "stickiness" or perseveration. Anxious participants may engage in safety behaviors by repeating their previous Stage 1 choice, regardless of whether that choice was rewarded or what the model-based system suggests. This is modeled as an autocorrelation term ("stickiness") that scales with the STAI score.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Driven Perseveration (Safety Behavior).
    
    Hypothesis: Anxiety increases 'stickiness' to the previous choice. 
    High anxiety participants exhibit a bias to repeat the last Stage 1 
    action (perseveration), modeled as a bonus added to the Q-value of 
    the previously chosen option.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stick_anxiety: [0, 5] How much STAI contributes to choice repetition bonus.
      stickiness_bonus = stick_anxiety * stai
    """
    learning_rate, beta, w, stick_anxiety = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate stickiness magnitude based on anxiety
    stickiness_bonus = stick_anxiety * stai_val

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stickiness
    last_action_1 = -1 

    for trial in range(n_trials):
        # Stage 1 Policy
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to q_net for the previously chosen action
        q_net_sticky = q_net.copy()
        if last_action_1 != -1:
            q_net_sticky[last_action_1] += stickiness_bonus
        
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update tracker
        last_action_1 = int(action_1[trial])
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # Stage 2 Policy
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Value Updating
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```