Here are three new cognitive models that incorporate the participant's STAI anxiety score into the decision-making process.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated Perseveration (Stickiness).
    
    Hypothesis: High anxiety increases the tendency to repeat the previous Stage 1 choice 
    (perseveration), serving as a safety behavior or avoidance of decision effort ("stickiness").
    This model adds a 'stickiness' bonus to the Q-value of the previously chosen action,
    where the magnitude of the bonus is scaled by the participant's STAI score.
    
    Parameters:
    learning_rate: [0, 1] - Update rate for Q-values.
    beta: [0, 10] - Inverse temperature (choice consistency).
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0) values.
    perseveration_base: [0, 2] - Baseline bonus added to the previously chosen action.
    stai_p_sens: [0, 2] - Sensitivity parameter: how much STAI increases the perseveration bonus.
    """
    learning_rate, beta, w, perseveration_base, stai_p_sens = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
    
    # Calculate effective perseveration bonus based on anxiety
    perseveration_bonus = perseveration_base + (stai_p_sens * participant_stai)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits and add perseveration bonus to the previous choice
        logits = beta * q_net
        if last_action_1 != -1:
            logits[int(last_action_1)] += perseveration_bonus
            
        # Softmax with stability fix (substracting max)
        exp_q1 = np.exp(logits - np.max(logits))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Updates
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Modulated Learning Rate (Hyper-Reactivity).
    
    Hypothesis: High anxiety is associated with increased sensitivity to environmental 
    volatility. Anxious participants may treat prediction errors as signals of 
    rapid change, resulting in a higher learning rate (over-updating beliefs).
    
    Parameters:
    lr_base: [0, 1] - Baseline learning rate for a theoretical 0-anxiety participant.
    lr_stai_slope: [0, 1] - How much the learning rate increases per unit of STAI.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    """
    lr_base, lr_stai_slope, beta, w = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
    
    # Calculate effective learning rate, ensuring it stays within bounds [0, 1]
    learning_rate = lr_base + (lr_stai_slope * participant_stai)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Updates
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Modulated Negativity Bias (Asymmetric Learning).
    
    Hypothesis: Anxious individuals exhibit a "negativity bias," learning more strongly 
    from negative outcomes (omission of reward) than positive ones. This model 
    boosts the learning rate specifically when the prediction error is negative, 
    proportional to the STAI score.
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate for positive prediction errors.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    neg_bias_stai: [0, 5] - Multiplier for learning rate when PE is negative, scaled by STAI.
    """
    learning_rate, beta, w, neg_bias_stai = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # policy for the second choice
        qs_current_state = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_current_state)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # Updates
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Determine effective LR based on sign of delta (Negativity Bias)
        eff_lr = learning_rate
        if delta_stage2 < 0:
            # Boost LR for negative PE based on STAI
            eff_lr = learning_rate * (1.0 + neg_bias_stai * participant_stai)
        
        eff_lr = np.clip(eff_lr, 0.0, 1.0)
        q_stage2_mf[state_idx, int(action_2[trial])] += eff_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Apply same bias to stage 1 update if delta is negative
        eff_lr_1 = learning_rate
        if delta_stage1 < 0:
            eff_lr_1 = learning_rate * (1.0 + neg_bias_stai * participant_stai)
        eff_lr_1 = np.clip(eff_lr_1, 0.0, 1.0)
        
        q_stage1_mf[int(action_1[trial])] += eff_lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```