Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process for the two-step task.

### Model 1: Anxiety-Driven Learning Rate Asymmetry
This model hypothesizes that anxiety biases how people learn from prediction errors. Specifically, high anxiety might lead to hypersensitivity to negative outcomes (or lack of reward) compared to positive ones. Instead of a single learning rate, this model modulates the learning rate for negative prediction errors based on the STAI score.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Driven Learning Rate Asymmetry.
    
    Hypothesis: Higher anxiety (STAI) increases the learning rate specifically 
    for negative prediction errors (when reward is less than expected), 
    reflecting a negative bias or increased sensitivity to punishment/omission.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    - anxiety_neg_mod: [0, 1] How much STAI amplifies the learning rate for negative RPEs.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    """
    alpha_pos, alpha_neg_base, anxiety_neg_mod, beta, w = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective negative learning rate based on anxiety
    # We clip it to ensure it stays valid [0, 1]
    alpha_neg_effective = alpha_neg_base + (participant_stai * anxiety_neg_mod)
    alpha_neg_effective = np.clip(alpha_neg_effective, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        # Use alpha_pos if PE is positive, alpha_neg_effective if negative
        lr_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg_effective
        q_stage1_mf[int(action_1[trial])] += lr_s1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        lr_s2 = alpha_pos if delta_stage2 >= 0 else alpha_neg_effective
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_s2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Inverse Temperature (Exploration Noise)
This model posits that anxiety affects the exploration-exploitation trade-off. High anxiety is often associated with behavioral inhibition or inconsistent responding under uncertainty. Here, STAI modulates the `beta` (inverse temperature) parameter. A higher STAI might lower `beta`, leading to more random (noisy) choices, or raise it (if the parameter allows), leading to more deterministic choices. This model tests if anxiety acts as a "noise" generator in decision-making.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Modulated Inverse Temperature.
    
    Hypothesis: Anxiety (STAI) disrupts the precision of action selection.
    High anxiety leads to a lower effective beta (higher temperature), 
    resulting in more stochastic/noisy choices.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta_base: [0, 10] Baseline inverse temperature for low anxiety.
    - anxiety_noise: [0, 5] Factor by which STAI reduces beta (adds noise).
    - w: [0, 1] Mixing weight for Model-Based vs Model-Free.
    """
    learning_rate, beta_base, anxiety_noise, w = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Effective beta is reduced by anxiety (more random/noisy behavior)
    # Ensure beta doesn't go below 0
    beta_effective = beta_base - (participant_stai * anxiety_noise)
    beta_effective = np.maximum(beta_effective, 0.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Eligibility Trace Decay
This model modifies the standard reinforcement learning update by introducing an eligibility trace parameter (`lambda`) that is controlled by anxiety. In Reinforcement Learning, `lambda` controls how much the Stage 1 value is updated by the Stage 2 reward directly. A high `lambda` connects the final outcome more strongly to the first choice. This model hypothesizes that high anxiety might impair the ability to link distal rewards to initial choices (reduced `lambda`), effectively making the agent more myopic or "forgetful" of the causal chain.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Induced Eligibility Trace Decay.
    
    Hypothesis: Anxiety (STAI) affects the eligibility trace (lambda), 
    determining how strongly the Stage 2 reward updates the Stage 1 choice directly.
    Higher anxiety reduces lambda, making the agent more myopic (disconnecting 
    Stage 1 choice from the final reward).
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based vs Model-Free.
    - lambda_base: [0, 1] Baseline eligibility trace parameter.
    - anxiety_decay: [0, 1] How much STAI reduces lambda.
    """
    learning_rate, beta, w, lambda_base, anxiety_decay = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective lambda
    # High anxiety reduces lambda -> less credit assignment to Stage 1 from Stage 2 reward
    lambda_eff = lambda_base * (1.0 - (participant_stai * anxiety_decay))
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Standard TD(0) update for Stage 1 based on State 2 value
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Eligibility Trace Update (TD(lambda) logic simplified)
        # We allow the Stage 2 prediction error to ALSO update Stage 1 Q-values, scaled by lambda
        q_stage1_mf[int(action_1[trial])] += learning_rate * lambda_eff * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```