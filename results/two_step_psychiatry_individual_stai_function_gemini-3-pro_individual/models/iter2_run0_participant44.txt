Here are three new cognitive models. These models explore different mechanisms by which anxiety (STAI) might influence decision-making, moving beyond the simple modulation of the model-based weight $w$ explored in the previous feedback.

### Model 1: Anxiety Modulates Learning Rate (Stress-Induced Rigidity)
This model tests the hypothesis that high anxiety reduces cognitive flexibility by lowering the learning rate. Anxious individuals may be slower to update their beliefs in response to new reward information, exhibiting a form of "stress-induced rigidity."

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated Learning Rate.
    
    Hypothesis: High anxiety (STAI) reduces the learning rate, leading to slower
    updating of value expectations (rigidity).
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control (fixed for this model).
    - lr_mod: [0, 1] Strength of anxiety modulation on learning rate.
    """
    lr_base, beta, w, lr_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Anxiety reduces the effective learning rate
    # Higher STAI -> Lower learning rate
    learning_rate = lr_base * (1.0 - (lr_mod * stai_score))
    learning_rate = np.clip(learning_rate, 0.01, 1.0) # Ensure it doesn't hit 0 exactly

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        r = reward[trial]
        
        # TD Error Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # TD Error Stage 1 (SARSA-like update using stage 2 value)
        # We use the value of the state we actually arrived at
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety Increases Choice Randomness (Noise)
This model tests the hypothesis that anxiety acts as a distractor or noise generator. Instead of altering how values are calculated or weighted, anxiety decreases the precision of the choice mechanism itself. High anxiety lowers the inverse temperature ($\beta$), making choices more stochastic and less driven by learned values.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Induced Choice Noise (Inverse Temperature Modulation).
    
    Hypothesis: High anxiety introduces noise into the decision process,
    effectively lowering the beta (inverse temperature) parameter.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - beta_mod: [0, 5] Strength of anxiety modulation reducing beta.
    """
    learning_rate, beta_base, w, beta_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Anxiety reduces beta (increases temperature/randomness)
    # Ensure beta doesn't go negative
    beta = beta_base - (beta_mod * stai_score)
    beta = np.maximum(beta, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety Increases Punishment Sensitivity (Loss Aversion)
This model posits that anxiety specifically alters how *negative* prediction errors are processed. Anxious individuals might overweigh "disappointments" (when outcomes are worse than expected) or reward omissions. Here, we model this by having a separate, higher learning rate for negative prediction errors in high-anxiety individuals, effectively making them learn faster from failure/lack of reward than from success.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Modulated Asymmetric Learning (Punishment Sensitivity).
    
    Hypothesis: Anxiety increases the learning rate specifically for negative
    prediction errors (disappointments), making the agent more sensitive to
    lack of reward.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - anxiety_sens: [0, 1] How much anxiety boosts the negative learning rate.
    """
    lr_pos, beta, w, anxiety_sens = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate negative learning rate based on anxiety
    # High anxiety -> higher learning rate for negative outcomes
    # Base negative LR is assumed equal to positive LR, plus anxiety boost
    lr_neg = lr_pos + (anxiety_sens * stai_score)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]

        # --- Updates with Asymmetric Learning Rate ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        if delta_stage2 >= 0:
            eff_lr_2 = lr_pos
        else:
            eff_lr_2 = lr_neg
        q_stage2_mf[state_idx, act2] += eff_lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        # We apply the asymmetry to the Stage 1 update as well
        if delta_stage1 >= 0:
            eff_lr_1 = lr_pos
        else:
            eff_lr_1 = lr_neg
            
        q_stage1_mf[act1] += eff_lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```