Here are three new cognitive models that incorporate STAI anxiety scores into the decision-making process.

### Model 1: Anxiety-Modulated Choice Stickiness
This model posits that anxiety influences the tendency to perseverate (stick) on the previously chosen spaceship, regardless of reward history. High anxiety might lead to more rigid, habitual behavior (high stickiness) as a maladaptive coping mechanism to reduce decision load.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid Model with Anxiety-Modulated Choice Stickiness.
    
    This model adds a 'stickiness' bonus to the action chosen in the previous trial.
    The magnitude of this stickiness is modulated by the participant's STAI score.
    Higher anxiety is hypothesized to increase choice repetition (perseveration).
    
    stickiness = stick_base + (stai_stick_sensitivity * normalized_stai)
    
    Parameters:
    learning_rate: [0, 1] Rate at which Q-values are updated.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0) control.
    stick_base: [0, 5] Baseline stickiness bonus.
    stai_stick_sensitivity: [0, 5] How strongly STAI increases stickiness.
    """
    learning_rate, beta, w, stick_base, stai_stick_sensitivity = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Normalize STAI to 0-1 range (approx range 20-80)
    norm_stai = (stai_val - 20) / 60.0
    norm_stai = np.clip(norm_stai, 0, 1)
    
    # Calculate effective stickiness
    stickiness = stick_base + (stai_stick_sensitivity * norm_stai)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # No previous action for the first trial
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the net Q-value of the previous action
        if prev_action_1 != -1:
            q_net[prev_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Store action for next trial's stickiness
        prev_action_1 = a1
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Stage 2 Update (Model-Free)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update (Model-Free)
        # Using the value of the state reached (TD(0))
        value_state_reached = q_stage2_mf[state_idx, a2]
        delta_stage1 = value_state_reached - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Asymmetric Learning
This model investigates if anxiety creates a "negativity bias" in learning. It allows for different learning rates for positive outcomes (finding gold) vs. negative outcomes (no gold). The hypothesis is that high STAI scores selectively amplify learning from failures/omissions.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid Model with Anxiety-Modulated Asymmetric Learning Rates.
    
    This model splits the learning rate into positive and negative components.
    The base learning rate applies to rewards. However, the learning rate for 
    omissions (0 reward) is modulated by STAI.
    
    lr_neg = lr_base * (1 + stai_bias * normalized_stai)
    
    If stai_bias > 0, anxious participants learn faster from negative outcomes.
    
    Parameters:
    lr_base: [0, 1] Base learning rate (applied to positive rewards).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight (MB vs MF).
    stai_neg_bias: [0, 5] Multiplier for STAI effect on negative learning rate.
    """
    lr_base, beta, w, stai_neg_bias = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    norm_stai = (stai_val - 20) / 60.0
    norm_stai = np.clip(norm_stai, 0, 1)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        # Determine effective learning rate based on outcome
        if r > 0.5: # Reward received
            current_lr = lr_base
        else: # Omission
            current_lr = lr_base * (1.0 + stai_neg_bias * norm_stai)
            current_lr = np.clip(current_lr, 0, 1)
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        
        # Stage 1 Update
        value_state_reached = q_stage2_mf[state_idx, a2]
        delta_stage1 = value_state_reached - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Transition Uncertainty
This model proposes that anxiety degrades the accuracy of the Model-Based planner's internal model. While the true transition probability is 0.7, highly anxious participants might doubt this structure, perceiving the world as more volatile or uncertain (closer to 0.5/0.5).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid Model with Anxiety-Modulated Transition Belief.
    
    The Model-Based system relies on a transition matrix. This model assumes
    anxiety introduces uncertainty into this internal model.
    The subjective probability of the common transition (p_common) decreases
    from 0.7 towards 0.5 as anxiety increases.
    
    p_common = 0.7 - (stai_uncertainty * norm_stai * 0.2)
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Mixing weight.
    stai_uncertainty: [0, 1] Factor reducing belief in the common transition.
                             1.0 = reduces 0.7 to 0.5 at max anxiety.
    """
    learning_rate, beta, w, stai_uncertainty = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    norm_stai = (stai_val - 20) / 60.0
    norm_stai = np.clip(norm_stai, 0, 1)
    
    # Distort the transition matrix based on anxiety
    # True common prob is 0.7. Max distortion pushes it to 0.5.
    distortion = stai_uncertainty * norm_stai * 0.2
    p_common = 0.7 - distortion
    p_rare = 1.0 - p_common
    
    # Subjective transition matrix used by the MB system
    subjective_trans_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the distorted matrix for planning
        q_stage1_mb = subjective_trans_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        value_state_reached = q_stage2_mf[state_idx, a2]
        delta_stage1 = value_state_reached - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```