Here are 3 new cognitive models based on the two-step task structure, incorporating anxiety (STAI) in distinct ways.

### Model 1: Anxiety-Modulated Learning Rate Asymmetry
This model hypothesizes that high anxiety alters how participants learn from positive versus negative prediction errors. Specifically, anxious individuals might be more sensitive to "worse than expected" outcomes (negative prediction errors) or have difficulty updating from them compared to positive ones.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Asymmetric Learning Rate Model based on Anxiety.
    
    Hypothesis: Anxiety (stai) modulates the ratio between learning from positive
    and negative prediction errors. High anxiety might amplify learning from 
    negative outcomes (punishment sensitivity) or dampen it. Here, we define 
    separate learning rates for positive and negative errors, where the negative
    learning rate is scaled by the STAI score.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    """
    lr_pos, lr_neg_base, beta, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Anxiety modulates sensitivity to negative prediction errors
    # If anxiety is high, lr_neg increases relative to base.
    # We constrain it to be within [0, 1]
    lr_neg = lr_neg_base * (1 + stai_score)
    if lr_neg > 1.0: lr_neg = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Stage 2 Update (Reward Prediction Error)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        current_lr = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        
        # Stage 1 Update (Temporal Difference Error)
        # Using the updated Q-value from stage 2 to drive Stage 1 learning (SARSA-like or Q-learning-like)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        current_lr_s1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += current_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Exploration (Stochasticity)
This model posits that anxiety affects decision noise (exploration) specifically in the first stage of decision making, where uncertainty is higher due to the probabilistic transition structure. Anxious individuals may be more risk-averse or conversely more erratic (higher temperature/lower beta) under uncertainty.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Inverse Temperature (Beta) Model.
    
    Hypothesis: Anxiety (stai) impacts the exploration-exploitation trade-off.
    High anxiety might lead to more 'noisy' or erratic behavior (lower beta)
    due to difficulty resolving uncertainty, or potentially higher rigidity.
    Here we model beta as a function of STAI.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_mod: [0, 10] Magnitude of anxiety modulation on beta.
    - w: [0, 1] Mixing weight for Model-Based vs Model-Free.
    """
    learning_rate, beta_base, beta_mod, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Effective beta is reduced by anxiety (increased noise/exploration)
    # Alternatively could be additive. We test a subtractive interference model.
    beta = beta_base - (beta_mod * stai_score)
    if beta < 0: beta = 0.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        # We use the same beta for stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) which controls how much credit is assigned to the first-stage choice based on the second-stage outcome. It tests if anxiety affects the ability to link distant outcomes to initial choices (temporal credit assignment).

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Eligibility Trace (Lambda) Model.
    
    Hypothesis: Anxiety affects temporal credit assignment. The parameter lambda
    controls the strength of the update to the first-stage choice based on the 
    second-stage reward directly (skipping the state-value link). 
    Anxiety might impair this direct link (lower lambda) or enhance focus on 
    recent outcomes.
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_base: [0, 1] Base eligibility trace parameter.
    - w: [0, 1] Mixing weight for MB vs MF.
    """
    learning_rate, beta, lambda_base, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Lambda is modulated by anxiety. 
    # We hypothesize anxiety reduces the effective eligibility trace (forgetting/distraction).
    eligibility_lambda = lambda_base * (1.0 - (0.5 * stai_score))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update with Eligibility Trace
        # Standard TD(0) update:
        # delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # TD(1) / Lambda logic: The stage 1 choice gets credit for the Stage 2 RPE
        # scaled by lambda.
        
        # 1. Update based on transition (TD error at stage 1 transition)
        delta_transition = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_transition
        
        # 2. Update based on final reward (Eligibility trace from Stage 2 RPE)
        q_stage1_mf[a1] += learning_rate * eligibility_lambda * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```