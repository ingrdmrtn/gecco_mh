Here are three new cognitive models that explore different mechanisms by which anxiety (STAI) might influence decision-making in the two-step task, specifically focusing on choice perseveration, learning rates, and punishment sensitivity.

### Model 1: Anxiety-Driven Perseveration
This model hypothesizes that anxiety increases "stickiness" or perseveration. High anxiety might make participants more likely to repeat their previous Stage 1 choice regardless of the outcome, representing a safety behavior or cognitive rigidity under stress.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Driven Perseveration.
    
    This model introduces a choice perseveration parameter (stickiness) that is 
    modulated by the STAI score. The hypothesis is that higher anxiety leads to 
    higher repetition of the previous choice (cognitive rigidity), independent of 
    reward learning.

    Parameters:
    lr: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax choice.
    w: [0, 1] Weight for model-based control (fixed mixing).
    perseveration_base: [0, 5] Base tendency to repeat the previous choice.
    stai_perseveration_mod: [0, 5] How much STAI amplifies perseveration.
    """
    lr, beta, w, perseveration_base, stai_perseveration_mod = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective perseveration based on anxiety
    # Higher STAI -> Higher tendency to stick to previous choice
    perseveration = perseveration_base + (stai_perseveration_mod * stai_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 
    
    # Store previous choice for perseveration logic (initialized to -1 for none)
    prev_choice_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Model-Based and Model-Free values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the previous choice
        if prev_choice_1 != -1:
            q_net[prev_choice_1] += perseveration

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update tracker
        prev_choice_1 = int(action_1[trial])
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        r = reward[trial]
        chosen_a2 = int(action_2[trial])
        chosen_a1 = int(action_1[trial])

        # Standard TD learning
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Learning Rate Asymmetry
This model suggests that anxiety affects how people learn from positive versus negative prediction errors. Specifically, it posits that anxiety might amplify learning from negative outcomes (or lack of reward), making the learning rate for "bad news" dependent on the STAI score.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Modulated Learning Rate Asymmetry.
    
    This model separates learning rates for positive (lr_pos) and negative (lr_neg) 
    prediction errors. The negative learning rate is modulated by STAI, reflecting 
    the hypothesis that anxious individuals are hypersensitive to negative feedback 
    or absence of reward.

    Parameters:
    lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    lr_neg_base: [0, 1] Base learning rate for negative prediction errors (RPE < 0).
    stai_neg_mod: [0, 1] How much STAI increases the negative learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF mixing weight.
    """
    lr_pos, lr_neg_base, stai_neg_mod, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective negative learning rate
    # Bounded between 0 and 1
    lr_neg = np.clip(lr_neg_base + (stai_neg_mod * stai_val), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        r = reward[trial]
        chosen_a2 = int(action_2[trial])
        chosen_a1 = int(action_1[trial])

        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        effective_lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, chosen_a2] += effective_lr_s2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        effective_lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[chosen_a1] += effective_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Exploration Suppression (Beta Modulation)
This model proposes that anxiety reduces exploration. Instead of affecting learning rates or mixing weights, STAI modulates the `beta` (inverse temperature) parameter. Higher anxiety leads to a higher beta, resulting in more deterministic, exploitative choices and less random exploration.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Driven Exploration Suppression.
    
    This model posits that anxiety reduces temperature (increases beta), making
    choices more deterministic and reducing random exploration. Anxious individuals
    may feel less safe exploring and prefer to exploit known high-value options.

    Parameters:
    lr: [0, 1] Learning rate.
    beta_base: [0, 10] Base inverse temperature.
    stai_beta_mod: [0, 10] Increase in beta per unit of STAI score.
    w: [0, 1] MB/MF mixing weight.
    """
    lr, beta_base, stai_beta_mod, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective beta
    # Higher anxiety -> Higher beta -> sharper probabilities (less exploration)
    beta = beta_base + (stai_beta_mod * stai_val)
    # Ensure beta doesn't explode, though bounds handle param search
    beta = np.clip(beta, 0, 20) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updates ---
        r = reward[trial]
        chosen_a2 = int(action_2[trial])
        chosen_a1 = int(action_1[trial])

        delta_stage2 = r - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```