Here are 3 new cognitive models that incorporate the STAI anxiety score into the decision-making process using mechanisms distinct from previous attempts (simple weight mixing or stickiness).

### Model 1: Anxiety-Modulated Inverse Temperature (Decision Rigidity)
This model hypothesizes that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety increases "rigidity" (higher inverse temperature $\beta$), making the participant more likely to deterministically exploit small differences in value rather than explore.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Inverse Temperature (Decision Rigidity).
    
    Hypothesis: High anxiety increases the inverse temperature (beta), leading to 
    more deterministic/rigid choices (less exploration). The effective beta is 
    amplified by the STAI score.
    
    Bounds:
    learning_rate: [0,1]
    beta_base: [0,10] (Base inverse temperature)
    w: [0,1] (Mixing weight, 1=MB, 0=MF)
    stai_rigidity: [0, 5] (Scaling factor for how much STAI increases beta)
    """
    learning_rate, beta_base, w, stai_rigidity = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate effective beta based on anxiety
    # High STAI increases beta (reducing noise/exploration)
    beta_eff = beta_base * (1.0 + (stai_val * stai_rigidity))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use anxiety-modulated beta
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta_eff * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Eligibility Trace (Credit Assignment)
This model focuses on the "eligibility trace" parameter $\lambda$. Standard Model-Free learning updates the first stage choice based on the second stage value (TD(0)). However, a full reinforcement update connects the final reward back to the first choice (TD(1)). This model posits that anxiety disrupts the ability to maintain eligibility traces (linking distal causes to effects), pushing behavior towards TD(0).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Eligibility Trace (Lambda).
    
    Hypothesis: Anxiety disrupts the maintenance of eligibility traces.
    Higher STAI scores reduce the lambda parameter, meaning the first-stage 
    choice is updated less by the final reward and more by the second-stage 
    state value (pure TD(0)).
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    lambda_base: [0,1] (Baseline eligibility trace)
    """
    learning_rate, beta, w, lambda_base = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Anxiety reduces the eligibility trace.
    # If STAI is high, lambda_eff approaches 0 (Pure TD(0)).
    # If STAI is low, lambda_eff stays close to lambda_base.
    lambda_eff = lambda_base * (1.0 - (stai_val * 0.8))
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Updates ---
        # Standard TD error for stage 1 (based on stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Standard TD error for stage 2 (based on reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Stage 2 update
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update: TD(lambda)
        # We update Stage 1 by its own error, PLUS a fraction (lambda) of Stage 2's error
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lambda_eff * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Negative Learning Bias
This model proposes an asymmetric learning rate. It hypothesizes that anxious individuals are hypersensitive to negative outcomes (omission of reward). When a reward is 0 (disappointment), the learning rate is boosted by the STAI score, causing faster updating for negative events compared to positive ones.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Negative Learning Bias.
    
    Hypothesis: High anxiety results in hypersensitivity to negative prediction errors
    (lack of reward). The learning rate is dynamically boosted when the reward is 0,
    proportional to the STAI score.
    
    Bounds:
    lr_base: [0,1] (Base learning rate for rewards)
    beta: [0,10]
    w: [0,1]
    neg_bias: [0,5] (Multiplier for STAI to boost learning rate on loss)
    """
    lr_base, beta, w, neg_bias = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Determine Effective Learning Rate ---
        # If reward is 0 (negative outcome), boost learning rate by anxiety factor
        if r == 0:
            lr_eff = lr_base * (1.0 + (stai_val * neg_bias))
        else:
            lr_eff = lr_base
            
        # Clip to ensure stability
        lr_eff = np.clip(lr_eff, 0.0, 1.0)
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_eff * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_eff * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```