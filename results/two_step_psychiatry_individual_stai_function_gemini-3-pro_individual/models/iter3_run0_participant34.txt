Here are 3 new cognitive models that incorporate the STAI anxiety score into the decision-making process in distinct ways.

### Model 1: Anxiety-Modulated Model-Based Suppression
This model tests the hypothesis that high anxiety consumes working memory resources, thereby inhibiting computationally expensive Model-Based (planning) control and forcing reliance on Model-Free (habitual) control.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Model-Based Suppression.
    
    Hypothesis: High anxiety (STAI) acts as a cognitive load, reducing the 
    weight (w) placed on Model-Based planning. Highly anxious individuals 
    will rely more on Model-Free (habitual) values.

    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w_max: [0, 1] The maximum possible mixing weight for Model-Based control (at 0 anxiety).
    - anxiety_suppression: [0, 1] How strongly STAI reduces w_max.
    """
    learning_rate, beta, w_max, anxiety_suppression = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective w based on anxiety
    # If anxiety_suppression is 0, w is constant. 
    # If high, w decreases as STAI increases.
    w_effective = w_max * (1.0 - (anxiety_suppression * stai_score))
    w_effective = np.clip(w_effective, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF using the anxiety-modulated weight
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update values
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Choice Perseveration (Stickiness)
This model tests the hypothesis that anxiety leads to rigid, safety-seeking behavior. Instead of altering learning or planning, anxiety adds a "stickiness" bias, causing the participant to repeat their previous Stage 1 choice regardless of the outcome.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Choice Perseveration.
    
    Hypothesis: Anxiety increases 'stickiness' (choice autocorrelation). 
    Anxious participants are more likely to repeat the previous Stage 1 choice 
    to minimize decision conflict or cognitive effort, regardless of reward history.

    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Fixed MB/MF weight.
    - stick_base: [0, 5] Baseline tendency to repeat choice.
    - stick_anx: [0, 5] Additional stickiness scaled by STAI score.
    """
    learning_rate, beta, w, stick_base, stick_anx = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate total stickiness bonus
    stickiness_bonus = stick_base + (stick_anx * stai_score)
    last_action = -1 # No previous action for first trial

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply softmax logic manually to add stickiness
        # We calculate logits = beta * Q, then add stickiness to the previous choice index
        logits = beta * q_net
        if last_action != -1:
            logits[last_action] += stickiness_bonus
            
        # Safe softmax
        max_logit = np.max(logits)
        exp_q1 = np.exp(logits - max_logit)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])
        
        # Store action for next trial's stickiness
        last_action = int(action_1[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update values
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Amplified Inverse Temperature (Tunnel Vision)
This model tests the hypothesis that anxiety causes "tunnel vision" or heightened exploitation. High anxiety sharpens the softmax curve (increases beta), making the participant more deterministic and less likely to explore options with slightly lower estimated values.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Amplified Inverse Temperature.
    
    Hypothesis: Anxiety modulates the exploration-exploitation trade-off.
    High STAI amplifies 'beta' (inverse temperature), leading to more 
    deterministic (rigid) choices and reduced exploration.

    Parameters:
    - learning_rate: [0, 1]
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Fixed MB/MF weight.
    - beta_anx_scale: [0, 5] Multiplier for how much STAI increases beta.
    """
    learning_rate, beta_base, w, beta_anx_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective beta
    # beta_eff = beta_base * (1 + scale * STAI)
    # If scale is high, high anxiety results in very high beta (determinism)
    beta_effective = beta_base * (1.0 + beta_anx_scale * stai_score)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_effective instead of raw beta
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        # Also apply anxiety rigidity to second stage
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update values
        chosen_a1 = int(action_1[trial])
        chosen_a2 = int(action_2[trial])
        
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```