Here are three new cognitive models. These models introduce mechanisms distinct from the previously attempted parameter combinations (which focused on learning rates, eligibility traces, and mixing weights). Instead, these models explore **Exploration/Exploitation balance (Temperature)**, **Choice Perseveration (Stickiness)**, and **Reward Sensitivity**.

### Model 1: Anxiety-Modulated Exploration (Temperature)
This model hypothesizes that anxiety affects the randomness of choice behavior. High anxiety might lead to more erratic behavior (panic/high noise) or, conversely, rigid exploitation of current knowledge. Here, STAI modulates the inverse temperature parameter ($\beta$).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model where STAI modulates the Inverse Temperature (Beta).
    
    The model assumes anxiety impacts the exploration-exploitation trade-off.
    beta = beta_base + (beta_mod * stai)
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_mod: [-5, 5] Modulation of beta by STAI.
    """
    learning_rate, w, beta_base, beta_mod = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    # Calculate effective beta based on anxiety
    # We clip to ensure beta remains within a reasonable positive range [0, 20]
    beta_eff = beta_base + (beta_mod * stai_val)
    beta_eff = np.clip(beta_eff, 0.0, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        # [FILL IN THE POLICY FOR THE SECOND CHOICE]
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # SARSA-style update for Stage 1 as implied by template variable setup
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Perseveration (Stickiness)
This model hypothesizes that anxiety increases the tendency to repeat previous choices (perseveration) to reduce cognitive load or avoid the uncertainty of switching. STAI modulates a "stickiness" parameter added to the Q-value of the previously chosen spaceship.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Choice Stickiness.
    
    Adds a 'stickiness' bonus to the previously chosen action in Stage 1.
    stickiness = stick_base + (stick_mod * stai)
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick_base: [0, 5] Baseline tendency to repeat choices.
    - stick_mod: [-5, 5] Modulation of stickiness by STAI.
    """
    learning_rate, beta, w, stick_base, stick_mod = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate effective stickiness
    stickiness = stick_base + (stick_mod * stai_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous choice

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-values before softmax
        q_net_stick = q_net.copy()
        if last_action_1 != -1:
            q_net_stick[last_action_1] += stickiness

        exp_q1 = np.exp(beta * q_net_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Save action for next trial's stickiness
        last_action_1 = a1

        # --- Stage 2 Policy ---
        # [FILL IN THE POLICY FOR THE SECOND CHOICE]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Reward Sensitivity
This model hypothesizes that anxiety alters the subjective valuation of the reward. Anxious individuals might exhibit blunted reward sensitivity (anhedonia) or hypersensitivity to feedback. STAI scales the reward signal used in the prediction error calculation.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Reward Sensitivity.
    
    The effective reward experienced is scaled by STAI.
    R_eff = Reward * (sens_base + sens_mod * stai)
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - sens_base: [0, 2] Baseline reward sensitivity.
    - sens_mod: [-1, 1] Modulation of sensitivity by STAI.
    """
    learning_rate, beta, w, sens_base, sens_mod = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate effective reward sensitivity
    # We clip to prevent negative perception of positive rewards (unless intended, but usually kept >0)
    sensitivity = sens_base + (sens_mod * stai_val)
    sensitivity = np.clip(sensitivity, 0.0, 5.0) 
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        # [FILL IN THE POLICY FOR THE SECOND CHOICE]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Apply sensitivity scaling to the received reward
        effective_reward = reward[trial] * sensitivity
        
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, a2]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```