Here are the three proposed cognitive models.

### Model 1: Anxiety-Modulated Choice Perseveration
This model hypothesizes that anxiety increases "stickiness" or perseveration. Anxious individuals may prefer to repeat previous choices to avoid the cognitive load of decision-making or to seek safety in familiarity, regardless of the model-based or model-free value calculations.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-modulated Choice Perseveration.
    
    Hypothesis: High anxiety (STAI) increases the tendency to repeat the 
    previous Stage 1 choice (perseveration), modeled as a value bonus 
    added to the previously chosen action.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for Model-Based control (1=MB, 0=MF).
    - stick_stai: [0, 5] Strength of perseveration scaled by anxiety.
    """
    learning_rate, beta, w, stick_stai = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add anxiety-modulated stickiness bonus to the last chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stick_stai * stai_score

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]
        
        # Track last action for next trial
        last_action_1 = act1

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # Update Stage 1 (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Learning Rate
This model hypothesizes that anxiety correlates with hypersensitivity to outcomes. Anxious participants may update their value estimates more aggressively (higher learning rate) in response to prediction errors, leading to more volatile value representations.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-modulated Learning Rate.
    
    Hypothesis: Anxiety leads to hyper-sensitivity to prediction errors.
    The effective learning rate is boosted by the STAI score, causing 
    anxious individuals to over-update value estimates after outcomes.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    - lr_stai_slope: [0, 1] Factor by which STAI increases the learning rate.
    """
    lr_base, beta, w, lr_stai_slope = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate effective learning rate (bounded at 1.0)
    learning_rate = lr_base * (1.0 + lr_stai_slope * stai_score)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Decision Rigidity (Beta)
This model hypothesizes that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety leads to higher "rigidity" (higher inverse temperature `beta`), making participants less likely to explore lower-value options and more likely to exploit the currently perceived best option deterministically.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-modulated Inverse Temperature (Beta).
    
    Hypothesis: High anxiety leads to cognitive rigidity and reduced exploration.
    This is modeled by increasing the inverse temperature (beta) as a function 
    of the STAI score, making choices more deterministic/greedy.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    - beta_stai_mod: [0, 5] Amount beta increases per unit of STAI.
    """
    learning_rate, beta_base, w, beta_stai_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    # Calculate effective beta (bounded at 20 for numerical stability)
    beta = beta_base + (beta_stai_mod * stai_score)
    beta = np.clip(beta, 0.0, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```