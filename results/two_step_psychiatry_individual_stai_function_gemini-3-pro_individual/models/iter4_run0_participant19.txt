Here are 3 new cognitive models based on the provided template and participant data.

### Model 1: STAI-Modulated Inverse Temperature (Anxiety-Induced Rigidity)
**Hypothesis:** Anxiety affects the exploration-exploitation trade-off. High anxiety (high STAI) leads to "choking" or rigidity, characterized by a higher inverse temperature ($\beta$). This causes the participant to exploit small differences in value deterministically rather than exploring options, effectively reducing decision noise but potentially getting stuck in suboptimal loops.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model where Anxiety (STAI) modulates choice rigidity (Beta).
    
    Hypothesis: Higher anxiety leads to reduced exploration and higher rigidity.
    The effective inverse temperature (beta) increases as STAI increases.

    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - w: [0, 1] Weight for Model-Based control.
    - beta_base: [0, 5] Baseline inverse temperature.
    - beta_stai_slope: [0, 10] Scaling factor for STAI on beta.
    """
    learning_rate, w, beta_base, beta_stai_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective beta based on STAI
    # If slope is positive, anxiety makes choices more deterministic (rigid).
    beta = beta_base + (beta_stai_slope * stai_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: STAI-Modulated Punishment Sensitivity (Asymmetric Learning)
**Hypothesis:** Anxious individuals are often more sensitive to negative feedback than positive feedback. This model implements a split learning rate. The learning rate for negative prediction errors (punishments/disappointments) is boosted by the STAI score, causing the participant to react more strongly to bad outcomes than good ones.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with STAI-modulated asymmetric learning rates.
    
    Hypothesis: Anxiety increases sensitivity to negative prediction errors.
    If a prediction error is negative, the learning rate is boosted by the STAI score.

    Parameters:
    - lr_base: [0, 1] Baseline learning rate for positive updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    - stai_neg_bias: [0, 1] How much STAI adds to the learning rate for negative errors.
    """
    lr_base, beta, w, stai_neg_bias = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Determine LR for Stage 1
        current_lr_1 = lr_base
        if delta_stage1 < 0:
            current_lr_1 = lr_base + (stai_neg_bias * stai_val)
            # Ensure LR doesn't exceed 1
            if current_lr_1 > 1.0: current_lr_1 = 1.0

        q_stage1_mf[action_1[trial]] += current_lr_1 * delta_stage1

        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Determine LR for Stage 2 (Primary Reward Prediction Error)
        current_lr_2 = lr_base
        if delta_stage2 < 0:
            current_lr_2 = lr_base + (stai_neg_bias * stai_val)
            if current_lr_2 > 1.0: current_lr_2 = 1.0
            
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: STAI-Distorted Transition Belief (Subjective Probability)
**Hypothesis:** High anxiety distorts the internal model of the environment (Model-Based system). While the true transition probability is 0.7, anxious individuals may perceive the environment as more chaotic or uncertain. This model proposes that STAI lowers the subjective probability of the "common" transition used in the planning phase, effectively flattening the transition matrix and reducing the efficacy of Model-Based control.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model where STAI distorts the Model-Based transition matrix.
    
    Hypothesis: Anxiety reduces confidence in the stability of the environment.
    Higher STAI reduces the subjective probability of the 'common' transition 
    (normally 0.7) towards 0.5 (randomness).

    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    - distortion_factor: [0, 0.5] How much STAI reduces the common transition prob.
    """
    learning_rate, beta, w, distortion_factor = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate subjective transition probability
    # True common prob is 0.7. Anxiety reduces this belief.
    # e.g., if distortion is 0.2 and STAI is 0.5, prob becomes 0.7 - 0.1 = 0.6
    subjective_p_common = 0.7 - (distortion_factor * stai_val)
    
    # Clamp to ensure it doesn't flip (must remain >= 0.5 to maintain 'common' definition)
    if subjective_p_common < 0.5:
        subjective_p_common = 0.5

    # Build subjective transition matrix used for MB planning
    subjective_trans_matrix = np.array([
        [subjective_p_common, 1 - subjective_p_common], 
        [1 - subjective_p_common, subjective_p_common]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the DISTORTED/SUBJECTIVE matrix for planning
        q_stage1_mb = subjective_trans_matrix @ max_q_stage2
        
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```