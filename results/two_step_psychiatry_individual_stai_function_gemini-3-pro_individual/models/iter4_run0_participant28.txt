Here are three new cognitive models implemented as Python functions, incorporating the participant's STAI (anxiety) score into the decision-making mechanics.

### Model 1: Asymmetric Learning Rates (Anxiety-Induced Pessimism)
This model hypothesizes that anxiety creates a "negativity bias." Participants with higher STAI scores are more sensitive to negative prediction errors (when outcomes are worse than expected) than positive ones.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Asymmetric Learning Rate Model (Pessimism Bias).
    
    Hypothesis: Anxiety (STAI) amplifies learning from negative prediction errors.
    When the outcome is worse than expected (delta < 0), the learning rate is 
    boosted by the STAI score, leading to rapid avoidance of disappointed expectations.
    
    Bounds:
    lr_base: [0,1] (Base learning rate for positive updates)
    beta: [0,10] (Inverse temperature)
    w: [0,1] (Mixing weight for MB vs MF)
    stai_neg_bias: [0,5] (Multiplier for negative learning rate boost)
    """
    lr_base, beta, w, stai_neg_bias = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Apply asymmetric learning rate based on sign of delta
        if delta_stage1 < 0:
            # Boost learning from disappointment based on anxiety
            lr_eff_1 = lr_base * (1.0 + (stai_val * stai_neg_bias))
            lr_eff_1 = min(lr_eff_1, 1.0) # Cap at 1.0
        else:
            lr_eff_1 = lr_base
            
        q_stage1_mf[a1] += lr_eff_1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        if delta_stage2 < 0:
            lr_eff_2 = lr_base * (1.0 + (stai_val * stai_neg_bias))
            lr_eff_2 = min(lr_eff_2, 1.0)
        else:
            lr_eff_2 = lr_base
            
        q_stage2_mf[state_idx, a2] += lr_eff_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Dampened Reward Sensitivity (Anhedonia)
This model hypothesizes that high anxiety blunts the subjective valuation of rewards. Instead of modulating the learning rate (how fast they learn), this modulates the *magnitude* of the reward signal itself, effectively acting as a scaling factor on the asymptote of the Q-values.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Dampened Reward Sensitivity Model.
    
    Hypothesis: High anxiety is associated with reduced reward sensitivity (anhedonia).
    The effective reward received is scaled down as STAI increases. This results in 
    lower Q-values overall and potentially less deterministic choice behavior 
    (as Q-values are closer to 0) unless beta compensates.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    stai_dampening: [0,1] (Strength of reward reduction due to anxiety)
    """
    learning_rate, beta, w, stai_dampening = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1 (Standard TD update)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 with Dampened Reward
        # Effective reward is reduced by anxiety level
        # If stai_dampening is 0, reward is normal. If 1, high anxiety significantly reduces it.
        effective_reward = reward[trial] * (1.0 - (stai_val * stai_dampening))
        
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Structural Uncertainty
This model suggests that anxiety degrades the participant's internal model of the environment's structure. While the true transition probabilities are 0.7/0.3, anxious participants may perceive the world as more chaotic or uncertain, effectively "flattening" their internal transition matrix toward 0.5/0.5.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Structural Uncertainty Model.
    
    Hypothesis: Anxiety degrades the precision of the model-based system.
    While the true transition matrix is 70/30, high anxiety participants operate 
    under a "fuzzier" model where transitions are perceived as more random.
    The internal transition probability 'p' shifts from 0.7 towards 0.5 based on STAI.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    stai_entropy: [0, 0.4] (Degree to which STAI flattens the transition matrix)
    """
    learning_rate, beta, w, stai_entropy = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    # Modulate the internal model of transitions based on anxiety
    # True p is 0.7. Anxiety reduces this p towards 0.5 (maximum entropy).
    # We clip the reduction to ensure p doesn't go below 0.5.
    p_internal = 0.7 - (stai_val * stai_entropy)
    p_internal = max(p_internal, 0.5) 
    
    transition_matrix = np.array([[p_internal, 1-p_internal], [1-p_internal, p_internal]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # The MB calculation now uses the anxiety-flattened matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_hybrid)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # policy for the second choice
        qs_stage2 = q_stage2_mf[state_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```