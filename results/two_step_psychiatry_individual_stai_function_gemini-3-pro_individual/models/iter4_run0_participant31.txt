Here are 3 new cognitive models exploring different mechanisms by which anxiety (STAI) might influence two-step decision-making.

### Model 1: Anxiety-Enhanced Learning Rate (Hyper-arousal)
This model hypothesizes that high anxiety leads to a state of hyper-arousal or over-sensitivity to prediction errors. Instead of affecting the balance of control (MB vs MF), anxiety here modulates the learning rate directly. Highly anxious individuals might update their value estimates more aggressively in response to rewards or omissions, potentially leading to "chasing" noise.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Enhanced Learning Rate.
    
    Hypothesis: Anxiety is associated with hyper-arousal and increased sensitivity 
    to feedback. Higher STAI scores increase the learning rate (alpha), causing 
    participants to over-adjust their value expectations based on recent outcomes.
    
    Mechanism: 
    effective_lr = lr_base + (stai * lr_boost)
    The learning rate is clipped to [0, 1].
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate at zero anxiety.
    - lr_boost: [0, 1] Additional learning rate magnitude scaled by STAI.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for Model-Based control (fixed across anxiety levels).
    """
    lr_base, lr_boost, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective learning rate based on anxiety
    learning_rate = lr_base + (stai_val * lr_boost)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update stage 1 values (Model-Free)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Exploration (Temperature Modulation)
This model posits that anxiety increases choice stochasticity (randomness). This could be interpreted as "panic" or difficulty settling on a deterministic strategy when anxious. High STAI scores lower the inverse temperature `beta` (or conversely, modify it such that choices become more uniform), reflecting less exploitation of learned values.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Driven Exploration (Temperature Modulation).
    
    Hypothesis: High anxiety interferes with the ability to consistently exploit 
    learned values, leading to more stochastic (noisier) choices.
    
    Mechanism: 
    effective_beta = beta_max * (1 - anxiety_noise * stai)
    Higher anxiety reduces beta, flattening the softmax distribution (more random).
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta_max: [0, 10] Maximum inverse temperature (consistency) at zero anxiety.
    - anxiety_noise: [0, 1] Factor by which anxiety reduces beta.
    - w: [0, 1] Weight for Model-Based control.
    """
    learning_rate, beta_max, anxiety_noise, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective beta based on anxiety
    # If anxiety_noise is high, beta drops significantly for anxious people
    beta = beta_max * (1.0 - (anxiety_noise * stai_val))
    beta = np.clip(beta, 0.0, 10.0) # Ensure it stays non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update stage 1 values
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Specific Punishment Sensitivity
This model suggests that anxiety specifically alters how individuals process negative outcomes (omission of reward). While standard RL uses a single learning rate for positive and negative prediction errors, this model introduces a specific learning rate for negative prediction errors (`lr_neg`) that scales with anxiety, hypothesizing that anxious individuals over-learn from failure.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Specific Punishment Sensitivity.
    
    Hypothesis: Anxious individuals are hypersensitive to negative outcomes 
    (reward omissions). They update their values more strongly when the outcome 
    is worse than expected (negative prediction error) compared to when it is better.
    
    Mechanism: 
    We use two learning rates: lr_pos (fixed) and lr_neg (modulated by anxiety).
    lr_neg = lr_base_neg + (stai * sensitivity)
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - lr_base_neg: [0, 1] Baseline learning rate for negative RPEs.
    - neg_sensitivity: [0, 1] Boost to negative learning rate based on STAI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    """
    lr_pos, lr_base_neg, neg_sensitivity, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate negative learning rate based on anxiety
    lr_neg_effective = lr_base_neg + (stai_val * neg_sensitivity)
    lr_neg_effective = np.clip(lr_neg_effective, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update stage 1 values
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Apply asymmetric learning rate
        lr_1 = lr_pos if delta_stage1 > 0 else lr_neg_effective
        q_stage1_mf[a1] += lr_1 * delta_stage1
        
        # Update stage 2 values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        # Apply asymmetric learning rate
        lr_2 = lr_pos if delta_stage2 > 0 else lr_neg_effective
        q_stage2_mf[state_idx, a2] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```