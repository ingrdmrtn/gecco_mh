Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process in distinct ways.

### Model 1: Anxiety-Driven Model-Based Suppression
**Hypothesis:** High anxiety consumes cognitive resources (working memory), reducing the ability to perform complex model-based planning. Therefore, higher STAI scores will lead to a lower mixing weight $w$ (less reliance on the model-based system) and a greater reliance on the simpler model-free system.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Model-Based Suppression.
    
    Hypothesis: Higher anxiety (STAI) reduces the weight (w) placed on 
    Model-Based planning due to cognitive load/worry, pushing the participant 
    towards Model-Free habits.
    
    Parameters:
    learning_rate: [0, 1] Standard learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w_max: [0, 1] Maximum possible model-based weight (for low anxiety).
    stai_suppression: [0, 1] How strongly STAI reduces the model-based weight.
    """
    learning_rate, beta, w_max, stai_suppression = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate effective w: High anxiety reduces w from w_max
    # If stai_suppression is high, anxiety heavily penalizes MB planning.
    w = np.clip(w_max - (stai_suppression * stai_score), 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values using the anxiety-modulated weight
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # Stage 1 MF update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Choice Randomness (Noise)
**Hypothesis:** Anxiety acts as a distractor or noise generator. Instead of altering learning or planning strategy specifically, high anxiety reduces the consistency of choices (lowers the inverse temperature $\beta$). This reflects "choking under pressure" or erratic behavior where the participant fails to exploit their knowledge effectively.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Choice Randomness.
    
    Hypothesis: Higher anxiety decreases the inverse temperature (beta), 
    leading to more noisy/random choices, reflecting a 'choking' effect
    or inability to consistently select the best option.
    
    Parameters:
    learning_rate: [0, 1] Update rate for values.
    beta_base: [0, 10] Base inverse temperature (consistency) for low anxiety.
    w: [0, 1] Mixing weight for MB vs MF.
    stai_noise: [0, 5] Factor by which STAI reduces beta (increases noise).
    """
    learning_rate, beta_base, w, stai_noise = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate effective beta: Anxiety reduces beta (increases randomness)
    # We clip at 0 because negative beta implies choosing the WORST option.
    beta = np.maximum(beta_base - (stai_noise * stai_score), 0.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Stickiness (Perseveration)
**Hypothesis:** Anxious individuals may exhibit safety behaviors or rigidity. This model posits that higher STAI scores lead to higher "choice stickiness" (perseveration), where the participant is more likely to repeat their previous Stage 1 choice regardless of the outcome, as a way to avoid the cognitive effort of re-decision or fear of switching.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Stickiness (Perseveration).
    
    Hypothesis: High anxiety leads to rigid behavior (perseveration). 
    The model includes a 'stickiness' parameter that biases the choice 
    towards the option selected in the previous trial. This stickiness 
    increases with STAI score.
    
    Parameters:
    learning_rate: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] MB/MF mixing weight.
    stai_stickiness: [0, 5] How much STAI increases the stickiness bonus.
    """
    learning_rate, beta, w, stai_stickiness = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Stickiness bonus scales with anxiety
    stick_weight = stai_stickiness * stai_score
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Policy for the first choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-values before softmax
        # If it's the first trial, no stickiness is applied
        q_net_sticky = q_net.copy()
        if last_action_1 != -1:
            q_net_sticky[last_action_1] += stick_weight
        
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Policy for the second choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```