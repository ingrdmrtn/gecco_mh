Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process.

### Model 1: Anxiety-Modulated Model-Based Weighting
This model tests the hypothesis that higher anxiety impairs the cognitive resources required for Model-Based (planning) control, leading to a greater reliance on Model-Free (habitual) control.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Model-Based Weighting.
    
    Hypothesis: High anxiety (STAI) reduces the weight (w) of the Model-Based 
    system, causing a shift towards Model-Free strategies.
    
    Parameters:
    learning_rate: [0, 1] - Rate of Q-value updating.
    beta: [0, 10] - Inverse temperature for softmax.
    w_base: [0, 1] - Baseline weighting for Model-Based values (low anxiety).
    w_stai_penalty: [0, 1] - Reduction in MB weight per unit of STAI.
    """
    learning_rate, beta, w_base, w_stai_penalty = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]

    # Calculate anxiety-modulated weight w. 
    # High anxiety reduces w, shifting control to Model-Free.
    w = w_base - (participant_stai * w_stai_penalty)
    w = np.clip(w, 0.0, 1.0) # Ensure w stays valid

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values based on anxiety-modulated w
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updating ---
        # Update Stage 2 MF values
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        # Update Stage 1 MF values (TD learning)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Learning Rate
This model tests the hypothesis that anxiety increases the learning rate, making participants more "reactive" or volatile in their value estimation, updating beliefs more drastically after each outcome.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Learning Rate.
    
    Hypothesis: Higher anxiety (STAI) leads to a higher learning rate, 
    making value updating more reactive to recent outcomes (higher volatility).
    
    Parameters:
    lr_base: [0, 1] - Baseline learning rate.
    lr_stai_slope: [0, 1] - Increase in learning rate per unit of STAI.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting between MB and MF values.
    """
    lr_base, lr_stai_slope, beta, w = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]

    # Calculate anxiety-modulated learning rate
    learning_rate = lr_base + (participant_stai * lr_stai_slope)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updating ---
        # Use the anxiety-modulated learning_rate here
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Degraded Transition Belief
This model tests the hypothesis that anxiety degrades the internal model of the environment. While the participant may still attempt Model-Based planning (controlled by `w`), their belief about the transition probabilities becomes "flatter" (closer to random/0.5) as anxiety increases, rendering the planning less effective.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Degraded Transition Belief.
    
    Hypothesis: Anxiety degrades the accuracy of the internal model used for planning.
    High anxiety participants perceive the transitions as more random (closer to 0.5/0.5),
    even if they attempt to use a Model-Based strategy.
    
    Parameters:
    learning_rate: [0, 1] - Rate of Q-value updating.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Weighting between MB and MF values.
    trans_degrade_slope: [0, 1] - How much STAI degrades the transition belief from 0.7 towards 0.5.
    """
    learning_rate, beta, w, trans_degrade_slope = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]

    # Calculate subjective transition probability
    # Base is 0.7. Anxiety reduces this towards 0.5 (randomness).
    # If degradation is max, belief becomes 0.5, making MB planning useless.
    subjective_p = 0.7 - (participant_stai * trans_degrade_slope * 0.2) 
    # The 0.2 scaling ensures that if slope=1 and stai=1, 0.7 - 0.2 = 0.5.
    subjective_p = np.clip(subjective_p, 0.5, 0.7)
    
    # Construct the subjective transition matrix
    subj_trans_matrix = np.array([
        [subjective_p, 1 - subjective_p], 
        [1 - subjective_p, subjective_p]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the ANXIETY-MODULATED subjective matrix for planning
        q_stage1_mb = subj_trans_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updating ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```