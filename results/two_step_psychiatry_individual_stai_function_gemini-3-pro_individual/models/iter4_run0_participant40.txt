Here are three new cognitive models that explore different mechanisms by which anxiety (STAI) might influence decision-making in the two-step task. These models focus on reward sensitivity, learning rates for negative outcomes, and the balance between model-based and model-free control.

### Model 1: STAI-Modulated Reward Sensitivity
This model hypothesizes that anxiety alters the subjective valuation of rewards. High anxiety might lead to a blunted response to positive rewards (anhedonia-like) or hypersensitivity to outcomes. Here, STAI scales the effective reward magnitude before it enters the prediction error calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Reward Sensitivity Model.
    
    This model proposes that anxiety (STAI) modulates the subjective value of the 
    reward received. The effective reward is scaled by a factor determined by STAI.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Mixing weight (0=Model-Free, 1=Model-Based).
    reward_sens_base: [0,2] - Baseline reward sensitivity.
    reward_sens_slope: [-2,2] - How STAI changes reward sensitivity.
                                Effective Reward = Reward * (Base + Slope * STAI)
    """
    learning_rate, beta, w, reward_sens_base, reward_sens_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate effective reward scaling factor based on STAI
    # We clip to ensure the multiplier doesn't flip the sign of the reward arbitrarily or explode
    reward_multiplier = reward_sens_base + (reward_sens_slope * stai_val)
    reward_multiplier = np.maximum(0.0, reward_multiplier) # Ensure non-negative scaling

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # Apply STAI modulation to the reward
        effective_reward = r * reward_multiplier

        # --- Updates ---
        # Update Stage 1 MF Q-value (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF Q-value
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: STAI-Dependent Asymmetric Learning Rates
This model suggests that anxiety creates a bias in how people learn from positive versus negative outcomes. Specifically, anxious individuals might learn more rapidly from punishments (negative prediction errors) than from rewards (positive prediction errors).

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Dependent Asymmetric Learning Rate Model.
    
    This model allows for different learning rates for positive and negative 
    prediction errors (PE). The learning rate for negative PEs is modulated 
    by STAI, hypothesizing that anxiety increases learning from bad outcomes.
    
    Parameters:
    lr_pos: [0,1] - Learning rate for positive prediction errors.
    lr_neg_base: [0,1] - Base learning rate for negative prediction errors.
    lr_neg_slope: [0,1] - Sensitivity of negative learning rate to STAI.
                          lr_neg = lr_neg_base + (lr_neg_slope * STAI)
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    """
    lr_pos, lr_neg_base, lr_neg_slope, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate negative learning rate based on STAI
    lr_neg = lr_neg_base + (lr_neg_slope * stai_val)
    # Clip to valid range [0, 1]
    lr_neg = np.clip(lr_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        current_lr_s1 = lr_pos if delta_stage1 > 0 else lr_neg
        q_stage1_mf[a1] += current_lr_s1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        current_lr_s2 = lr_pos if delta_stage2 > 0 else lr_neg
        q_stage2_mf[state_idx, a2] += current_lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: STAI-Based Transition Learning (State Inference)
Standard models assume the transition matrix (0.7/0.3) is fixed and known. This model hypothesizes that anxiety affects how quickly participants update their internal model of the environment's structure. High anxiety might lead to "over-updating" the transition probabilities after rare transitions, reflecting a volatile belief about the world structure.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Based Transition Learning Model.
    
    Unlike standard models with a fixed 0.7/0.3 transition matrix, this model 
    learns the transition probabilities trial-by-trial. STAI modulates the 
    learning rate of this transition model (lr_trans), reflecting how anxiety 
    might lead to instability in the participant's map of the task structure.
    
    Parameters:
    learning_rate: [0,1] - Value learning rate (for Q-values).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight.
    lr_trans_base: [0,1] - Base learning rate for transition probabilities.
    lr_trans_slope: [0,1] - Effect of STAI on transition learning rate.
    """
    learning_rate, beta, w, lr_trans_base, lr_trans_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate transition learning rate
    lr_trans = lr_trans_base + (lr_trans_slope * stai_val)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)

    # Initialize dynamic transition matrix (start with uniform prior or standard assumption)
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    # Initialized to the 'true' common/rare structure but allowed to drift
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the CURRENT learned transition matrix for Model-Based calc
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        
        # 1. Update Transition Matrix (State Learning)
        # Create a one-hot vector for the observed state
        observed_transition = np.zeros(2)
        observed_transition[state_idx] = 1.0
        
        # Update the row corresponding to the chosen action
        trans_probs[a1] = (1 - lr_trans) * trans_probs[a1] + lr_trans * observed_transition
        
        # 2. Update Q-Values (Value Learning)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```