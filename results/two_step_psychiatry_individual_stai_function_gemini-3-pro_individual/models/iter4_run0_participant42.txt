Here are three new cognitive models that incorporate the participant's STAI (anxiety) score into the decision-making process. These models explore different mechanisms than those previously tested, focusing on how anxiety might modulate the balance between model-based and model-free control, or how it might affect the learning rate for negative outcomes specifically.

### Model 1: Anxiety-Modulated MB/MF Weighting (Linear Shift)
This model hypothesizes that anxiety directly influences the trade-off between Model-Based (planning) and Model-Free (habitual) control. High anxiety is often associated with reduced cognitive flexibility or a retreat to habitual strategies. Here, the weighting parameter `w` is not static but shifts linearly based on the STAI score.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated MB/MF Weighting.
    
    Hypothesis: Anxiety alters the balance between Model-Based (MB) and Model-Free (MF) control.
    Specifically, the mixing weight 'w' is a linear function of the STAI score.
    Higher anxiety might lead to lower 'w' (more Model-Free) or higher 'w' depending on the fit.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature for softmax.
    - w_intercept: [0, 1] Baseline mixing weight for a theoretical STAI of 0.
    - w_slope: [-1, 1] How much STAI changes the weight 'w'. 
                 (Positive = anxiety increases MB; Negative = anxiety decreases MB).
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective w based on STAI, clamped between 0 and 1
    w_effective = w_intercept + (w_slope * participant_stai)
    w_effective = np.clip(w_effective, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Q-value using the anxiety-modulated weight
        q_net = (w_effective * q_stage1_mb) + ((1 - w_effective) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Loss Aversion (Asymmetric Learning)
This model tests the hypothesis that anxious individuals are hypersensitive to negative outcomes (omission of reward). Instead of a single learning rate, there are separate rates for positive prediction errors (reward) and negative prediction errors (no reward). The STAI score specifically amplifies the learning rate for negative prediction errors, reflecting a "fear of failure" or enhanced punishment learning.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Driven Loss Sensitivity (Asymmetric Learning).
    
    Hypothesis: Anxiety specifically increases the learning rate for negative prediction errors 
    (when reward is 0), making the participant abandon non-rewarding options faster.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors (RPE < 0).
    - anxiety_neg_boost: [0, 1] Additional boost to alpha_neg proportional to STAI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for Model-Based vs Model-Free.
    """
    alpha_pos, alpha_neg_base, anxiety_neg_boost, beta, w = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective negative learning rate
    alpha_neg_effective = alpha_neg_base + (anxiety_neg_boost * participant_stai)
    alpha_neg_effective = np.clip(alpha_neg_effective, 0.0, 1.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Use simple learning rate for stage 1 transfer or assume same asymmetry applies
        # Here we apply the asymmetry logic to the second stage outcome primarily
        q_stage1_mf[a1] += alpha_pos * delta_stage1 # Simplified for stage 1 transfer

        # Update Stage 2 MF (Where the reward actually happens)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        if delta_stage2 >= 0:
            current_alpha = alpha_pos
        else:
            current_alpha = alpha_neg_effective
            
        q_stage2_mf[state_idx, a2] += current_alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Randomness (Noise)
This model posits that high anxiety acts as a distracting factor, increasing decision noise (inverse of consistency). Instead of modulating the strategy (MB vs MF) or learning, anxiety lowers the effective `beta` (inverse temperature), making choices more random or "noisy" as anxiety increases. This reflects the "choking under pressure" or cognitive interference account of anxiety.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Induced Decision Noise.
    
    Hypothesis: Higher anxiety interferes with decision consistency, effectively lowering 
    the beta parameter (making choices more random).
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta_base: [0, 10] Baseline inverse temperature (consistency) for low anxiety.
    - anxiety_noise: [0, 10] Amount by which STAI reduces beta.
    - w: [0, 1] Mixing weight for Model-Based vs Model-Free.
    """
    learning_rate, beta_base, anxiety_noise, w = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective beta: Base - (Noise * STAI)
    # Ensure beta doesn't go below 0
    beta_effective = beta_base - (anxiety_noise * participant_stai)
    beta_effective = np.maximum(beta_effective, 0.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```