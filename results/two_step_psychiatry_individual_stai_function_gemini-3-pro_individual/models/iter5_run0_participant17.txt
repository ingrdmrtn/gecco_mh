Here are 3 new cognitive models based on the provided template and participant data.

### Model 1: Anxiety-Modulated Choice Stickiness
**Hypothesis:** High anxiety induces "safety-seeking" or repetitive behaviors (perseveration). This model posits that higher STAI scores increase the "stickiness" parameter, making the participant more likely to repeat their previous first-stage choice regardless of the model-based or model-free values.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated Choice Stickiness.
    
    Hypothesis: Anxiety (stai) drives perseveration (stickiness). The participant
    receives an additional 'bonus' to the action value of the choice made in the 
    previous trial, scaled by their anxiety level.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - stickiness_sensitivity: [0, 5] How strongly STAI scales the perseveration bonus.
    """
    learning_rate, beta, w, stickiness_sensitivity = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate the effective stickiness bonus based on anxiety
    # High anxiety -> high tendency to repeat previous choice
    stickiness_bonus = stickiness_sensitivity * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_action_1 = -1 # No previous action on trial 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value mixing MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness to the logits (not the Q-values themselves)
        logits = beta * q_net
        if prev_action_1 != -1:
            logits[prev_action_1] += stickiness_bonus
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        prev_action_1 = a1 # Store for next trial

        # --- Stage 2 Policy ---
        # Standard Softmax on Stage 2 Q-values
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
  
        # --- Updating ---
        # TD(0) update for Stage 2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # TD(1) / SARSA-like update for Stage 1 MF values
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Pessimism (Reward Valuation)
**Hypothesis:** Anxiety is associated with anhedonia or a negativity bias. Instead of modulating the learning *rate*, this model suggests anxiety modulates the subjective valuation of the *outcome*. High anxiety participants perceive the "0" coin outcome as a punishment (negative value) rather than neutral, or they perceive the "1" coin outcome as less valuable.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Induced Pessimism (Reward Valuation).
    
    Hypothesis: Anxiety alters the subjective utility of the reward.
    Normally Reward is {0, 1}. Here, Effective Reward = Reward - (STAI * pessimism).
    If pessimism is high, a reward of 0 becomes a negative value (punishment),
    and a reward of 1 becomes smaller (dampened positive affect).
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - pessimism_factor: [0, 1] Scales how much STAI subtracts from the objective reward.
    """
    learning_rate, beta, w, pessimism_factor = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        
        # --- Subjective Reward Transformation ---
        # If pessimism_factor is high and STAI is high, this shifts values downward.
        # 0 coins -> negative value (feeling of loss)
        # 1 coin -> reduced positive value
        r_objective = reward[trial]
        r_effective = r_objective - (stai_score * pessimism_factor)
  
        # --- Updating ---
        delta_stage2 = r_effective - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Transition Uncertainty
**Hypothesis:** High anxiety degrades the fidelity of the internal model of the environment. While the true transition probability is 0.7, highly anxious individuals may distrust this structure, perceiving the environment as more chaotic (higher entropy). Their internal model blurs the transition probabilities toward 0.5 (randomness) based on their STAI score.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Driven Transition Uncertainty.
    
    Hypothesis: Anxiety degrades the Model-Based component by introducing entropy 
    into the believed transition matrix. The participant understands the structure 
    less clearly as anxiety increases, pushing the believed transition probability 
    from 0.7 towards 0.5.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - uncertainty_bias: [0, 0.4] How much STAI reduces the believed probability from 0.7.
      (Max 0.4 ensures we don't flip the model to < 0.3).
    """
    learning_rate, beta, w, uncertainty_bias = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Distort the transition matrix based on anxiety
    # True prob is 0.7. Anxious prob approaches 0.5.
    # We clip it so it doesn't go below 0.5 (reversing the model).
    believed_prob = 0.7 - (stai_score * uncertainty_bias)
    believed_prob = max(0.5, believed_prob) 
    
    # Construct the anxious internal model
    # If believed_prob is 0.5, MB planning becomes useless (random walk assumption)
    transition_matrix = np.array([
        [believed_prob, 1.0 - believed_prob], 
        [1.0 - believed_prob, believed_prob]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # MB calculation uses the DISTORTED transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
  
        # --- Updating ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```