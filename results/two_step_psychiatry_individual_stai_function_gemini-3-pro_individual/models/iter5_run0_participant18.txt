Here are three new cognitive models that introduce mechanisms distinct from the previously attempted parameter combinations.

### Model 1: STAI-Modulated Eligibility Traces (Lambda)
This model introduces an eligibility trace parameter ($\lambda$). In standard Model-Free RL, the Stage 1 value is updated only by the Stage 2 value (TD(0)). With eligibility traces (TD($\lambda$)), the Stage 1 value is also directly updated by the final Reward. This model hypothesizes that anxiety (STAI) affects the "credit assignment" window—how strongly the final outcome is attributed back to the initial spaceship choice, bypassing the state transition.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with STAI-modulated Eligibility Traces (Lambda).
    
    Hypothesis: Anxiety affects the extent to which the final reward directly 
    updates the first-stage value, effectively bridging the time gap. 
    High anxiety might lead to broader credit assignment (higher lambda).

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - lam_base: [0, 1] Baseline eligibility trace decay.
    - lam_mod: [-1, 1] Modulation of lambda by STAI.
    """
    lr, beta, w, lam_base, lam_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective lambda based on STAI
    # Lambda determines how much the Stage 2 RPE affects Stage 1 Q-values
    lam = lam_base + (lam_mod * stai_score)
    lam = np.clip(lam, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted combination of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning Updates ---
        # Prediction error at stage 1 (Transition driven)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Prediction error at stage 2 (Reward driven)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Update Stage 1 MF: Includes standard update + eligibility trace from Stage 2
        q_stage1_mf[a1] += lr * delta_stage1 + (lam * lr * delta_stage2)
        
        # Update Stage 2 MF: Standard Q-learning
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: STAI-Modulated Choice Stickiness (Perseveration)
This model adds a "stickiness" parameter. This is a heuristic often found in decision-making where subjects tend to repeat their previous choice regardless of reward history. The model hypothesizes that STAI modulates this behavioral rigidity—anxious individuals may be more prone to repetitive behaviors (high stickiness) or more erratic (negative stickiness).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with STAI-modulated Choice Stickiness.
    
    Hypothesis: Anxiety influences behavioral rigidity. The model adds a 
    'stickiness' bonus to the Q-value of the previously chosen spaceship.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stick_base: [0, 5] Baseline stickiness bonus.
    - stick_mod: [-5, 5] Modulation of stickiness by STAI.
    """
    lr, beta, w, stick_base, stick_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective stickiness
    stickiness = stick_base + (stick_mod * stai_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1 # Initialize previous action placeholder

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness bonus to the previously chosen action
        if prev_a1 != -1:
            q_net[prev_a1] += stickiness

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])
        
        # Update previous action for next trial
        prev_a1 = a1

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: STAI-Modulated Stage 2 Learning Rate (Differential Learning)
Previous models tried a global learning rate. This model separates learning into two stages: learning about transitions (Stage 1) and learning about rewards (Stage 2). It hypothesizes that anxiety specifically impacts how reactive participants are to the immediate alien feedback (Stage 2), while the learning of the spaceship-planet transitions (Stage 1) remains stable or distinct.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Differential Learning Rates.
    
    Hypothesis: Anxiety specifically modulates the learning rate for the 
    second stage (immediate reward outcomes), while the first stage 
    (transition structure) uses a fixed learning rate.
    
    Parameters:
    - lr1: [0, 1] Fixed learning rate for Stage 1 (Spaceship choice).
    - lr2_base: [0, 1] Baseline learning rate for Stage 2 (Alien reward).
    - lr2_mod: [-1, 1] Modulation of Stage 2 learning rate by STAI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    """
    lr1, lr2_base, lr2_mod, beta, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate specific Stage 2 learning rate
    lr2 = lr2_base + (lr2_mod * stai_score)
    lr2 = np.clip(lr2, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Learning Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # Update Stage 1 using lr1
        q_stage1_mf[a1] += lr1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        # Update Stage 2 using the STAI-modulated lr2
        q_stage2_mf[state_idx, a2] += lr2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```