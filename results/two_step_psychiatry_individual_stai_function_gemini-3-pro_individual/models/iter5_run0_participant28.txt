Here are the 3 proposed cognitive models.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF with Anxiety-Modulated Subjective Loss (Loss Aversion).
    
    Hypothesis: Anxious individuals exhibit a negativity bias, perceiving the 
    omission of reward (0 coins) as an explicit loss or punishment rather than 
    a neutral outcome. The magnitude of this subjective loss is scaled by the 
    STAI score.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    loss_aversion_scale: [0, 10] (Scales the negative utility of 0 reward)
    """
    learning_rate, beta, w, loss_aversion_scale = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Subjective utility of '0' reward
    # If reward is 0, effective reward is -1 * scale * stai
    # If reward is 1, effective reward is 1
    loss_penalty = loss_aversion_scale * stai_val

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2)) 

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        qs_stage2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Subjective Reward Transformation ---
        r_eff = r if r == 1 else -loss_penalty

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r_eff - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF with Anxiety-Driven Memory Decay.
    
    Hypothesis: Anxiety consumes cognitive resources (working memory), leading 
    to a passive decay of learned values (Q-values) towards zero over time.
    The rate of decay is proportional to the STAI score.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    decay_scale: [0,1] (Base decay rate scaled by STAI)
    """
    learning_rate, beta, w, decay_scale = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate effective decay rate per trial
    # decay must be in [0, 1]. 
    decay_rate = np.clip(decay_scale * stai_val, 0.0, 1.0)
    retention = 1.0 - decay_rate

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        s_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        qs_stage2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # --- Decay ---
        # All Q-values decay towards 0 (forgetting)
        q_stage1_mf *= retention
        q_stage2_mf *= retention

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF with Anxiety-Dependent Stickiness (Perseveration).
    
    Hypothesis: High anxiety leads to repetitive "safety" behaviors or hesitation 
    to switch. This model includes a choice stickiness bonus (perseveration) 
    that is directly determined by the STAI score.
    
    Bounds:
    learning_rate: [0,1]
    beta: [0,10]
    w: [0,1]
    stickiness_sensitivity: [0, 5] (How strongly STAI drives stickiness)
    """
    learning_rate, beta, w, stickiness_sensitivity = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Choice stickiness bonus
    # Positive values -> repeat choice.
    stick_bonus = stickiness_sensitivity * stai_val

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        # This bonus is purely a function of STAI, not a general bias
        q_net_s1_mod = q_net_s1.copy()
        if last_action_1 != -1:
            q_net_s1_mod[last_action_1] += stick_bonus
        
        exp_q1 = np.exp(beta * q_net_s1_mod)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        last_action_1 = a1 # Update for next trial
        s_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        qs_stage2 = q_stage2_mf[s_idx]
        exp_q2 = np.exp(beta * qs_stage2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```