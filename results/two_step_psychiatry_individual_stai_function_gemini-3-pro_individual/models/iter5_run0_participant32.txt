Here are three new cognitive models. These models explore different mechanistic hypotheses about how anxiety (STAI) impacts decision-making: through perseveration (stickiness), through the degradation of model-based planning resources, and through the distortion of the internal model of the environment.

### Model 1: Anxiety-Modulated Perseveration (Stickiness)
This model hypothesizes that anxiety increases "stickiness" or perseveration. High-anxiety individuals may be more likely to repeat their previous Stage 1 choice regardless of the reward outcome, representing a "safety behavior" or avoidance of switching costs.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Perseveration (Stickiness).
    
    Hypothesis: Anxiety increases the tendency to repeat the previous Stage 1 choice 
    (perseveration), regardless of reward history. This is modeled as a 'stickiness' 
    bonus added to the Q-values of the previously chosen action.
    
    Parameters:
    - learning_rate: Update rate for value estimation [0, 1]
    - beta: Inverse temperature for softmax choice [0, 10]
    - w: Weighting between model-based and model-free control [0, 1]
    - stick_base: Baseline tendency to repeat choices [0, 5]
    - stick_anxiety_slope: How much STAI increases stickiness [0, 5]
    """
    learning_rate, beta, w, stick_base, stick_anxiety_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate effective stickiness based on anxiety
    stickiness = stick_base + (stick_anxiety_slope * stai_val)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    # Track the previous action (initialize to None or handles via logic)
    last_action_1 = -1 

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value mixing MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update last action for next trial
        last_action_1 = action_1[trial]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Stage 2 Q-values (TD Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Update Stage 1 Q-values (TD Error using Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Model-Based Suppression
This model hypothesizes that anxiety consumes working memory resources, specifically impairing Model-Based (planning) control. Instead of a generic decay, this model posits that higher STAI scores linearly reduce the mixing weight `w`, forcing the participant to rely more on Model-Free (habitual) control.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Model-Based Suppression.
    
    Hypothesis: High anxiety consumes cognitive resources required for planning 
    (Model-Based control). As STAI increases, the weight 'w' shifts away from 
    Model-Based (planning) towards Model-Free (habit).
    
    Parameters:
    - learning_rate: Update rate for value estimation [0, 1]
    - beta: Inverse temperature [0, 10]
    - w_base: Baseline Model-Based weight for a hypothetical 0-anxiety person [0, 1]
    - w_anxiety_slope: The rate at which anxiety reduces 'w' [0, 1]
    """
    learning_rate, beta, w_base, w_anxiety_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate effective w: Higher anxiety subtracts from the baseline w
    w = w_base - (w_anxiety_slope * stai_val)
    w = np.clip(w, 0.0, 1.0) # Ensure w stays valid

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Distorted Transition Beliefs
This model hypothesizes that anxiety distorts the participant's internal model of the environment. While the objective transition probability is 0.7, anxious individuals might perceive the world as more unpredictable (entropy maximization) or simply underestimate the reliability of the "common" transition. This affects the Model-Based calculation specifically.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Distorted Transition Beliefs.
    
    Hypothesis: Anxiety distorts the internal model of the environment. 
    The participant's belief about the transition probability (usually 0.7) 
    is dampened by anxiety, making the mental model of the world seem more 
    random (closer to 0.5) as STAI increases.
    
    Parameters:
    - learning_rate: Update rate for value estimation [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Weighting between model-based and model-free control [0, 1]
    - distortion_factor: How much anxiety reduces the perceived transition probability [0, 0.5]
    """
    learning_rate, beta, w, distortion_factor = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Objective probability is 0.7. Anxiety reduces this belief towards 0.5 (randomness)
    # If distortion is high, they trust the spaceship structure less.
    perceived_prob = 0.7 - (distortion_factor * stai_val)
    perceived_prob = np.clip(perceived_prob, 0.5, 0.99) # Bound between random and certain
    
    # Construct the distorted internal model
    internal_transition_matrix = np.array([
        [perceived_prob, 1 - perceived_prob], 
        [1 - perceived_prob, perceived_prob]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the DISTORTED matrix for the MB calculation
        q_stage1_mb = internal_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```