Here are 3 new cognitive models based on the provided template and participant data.

### Model 1: Anxiety-Driven Perseveration (Stickiness)
This model hypothesizes that high anxiety leads to "stickiness" or perseveration in decision-making. High STAI scores increase the probability of repeating the previous Stage 1 choice, regardless of the model-based or model-free values. This aligns with the participant's data showing repetitive choices (Spaceship 1.0 every time).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Perseveration.
    
    Hypothesis: Anxiety increases 'stickiness' to the previously chosen option.
    Anxious individuals may default to repeating choices to reduce cognitive load 
    or avoid the uncertainty of switching.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    - pers_base: [0, 5] Base level of perseveration (choice autocorrelation).
    - anxiety_stickiness: [0, 5] How much STAI amplifies perseveration.
    """
    learning_rate, beta, w, pers_base, anxiety_stickiness = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective perseveration bonus based on anxiety
    # High anxiety -> higher bonus for the previously chosen action
    pers_bonus = pers_base + (anxiety_stickiness * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Perseveration Bonus
        if last_action_1 != -1:
            q_net[int(last_action_1)] += pers_bonus
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        chosen_a1 = int(action_1[trial])
        
        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        chosen_a2 = int(action_2[trial])

        # --- Updates ---
        # Stage 1 Update (Model-Free)
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1
        
        # Stage 2 Update (Model-Free)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2
        
        # Track previous choice for next trial
        last_action_1 = chosen_a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Rigidity (Beta Scaling)
This model posits that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety increases the inverse temperature (`beta`), causing the participant to become more rigid and deterministic in their choices (exploiting small perceived value differences) rather than exploring.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Rigidity (Beta Scaling).
    
    Hypothesis: Anxiety reduces exploration. High STAI scores linearly increase
    the inverse temperature (beta), making choices more deterministic (rigid)
    and less sensitive to random exploration noise.
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - anxiety_rigidity: [0, 10] Scaling factor: how much STAI increases Beta.
    """
    learning_rate, beta_base, w, anxiety_rigidity = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Effective Beta is modulated by anxiety
    # If STAI is high, beta becomes very large -> deterministic behavior
    beta_eff = beta_base + (anxiety_rigidity * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use anxiety-modulated beta
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        chosen_a1 = int(action_1[trial])

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        chosen_a2 = int(action_2[trial])

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Terminal Reactivity
This model separates learning into two stages. It hypothesizes that anxiety specifically amplifies the reaction to immediate, terminal rewards (Stage 2 outcomes) while leaving the learning of the transition structure (Stage 1) relatively standard. High anxiety makes the participant "over-update" based on whether they got gold or not, reflecting hyper-sensitivity to final outcomes.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Terminal Reactivity.
    
    Hypothesis: Anxiety causes hyper-reactivity to terminal outcomes (Stage 2).
    The learning rate for the final reward (Stage 2) is boosted by STAI,
    while the learning rate for the transition step (Stage 1) is fixed.
    
    Parameters:
    - lr_stage1: [0, 1] Learning rate for the first stage (transition prediction).
    - lr_stage2_base: [0, 1] Base learning rate for the second stage (reward).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - stai_stage2_boost: [0, 1] How much STAI increases the Stage 2 learning rate.
    """
    lr_stage1, lr_stage2_base, beta, w, stai_stage2_boost = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate Stage 2 specific learning rate
    lr_stage2 = lr_stage2_base + (stai_stage2_boost * stai_score)
    lr_stage2 = np.clip(lr_stage2, 0.0, 1.0) # Ensure bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        chosen_a1 = int(action_1[trial])

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        chosen_a2 = int(action_2[trial])

        # --- Updates ---
        # Update Stage 1 using the fixed lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, chosen_a2] - q_stage1_mf[chosen_a1]
        q_stage1_mf[chosen_a1] += lr_stage1 * delta_stage1

        # Update Stage 2 using the anxiety-boosted lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, chosen_a2]
        q_stage2_mf[state_idx, chosen_a2] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```