Here are three new cognitive models that incorporate the STAI score into the decision-making process using different mechanisms than the previous "best model".

### Model 1: Anxiety-Driven Negativity Bias (Asymmetric Learning)
This model hypothesizes that anxiety creates a "negativity bias" in learning. High-anxiety individuals may update their value estimates more drastically when outcomes are worse than expected (negative prediction errors) compared to when they are better than expected.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Negativity Bias Model.
    
    Hypothesis: High anxiety (STAI) amplifies the learning rate specifically 
    for negative prediction errors (disappointments), leading to faster 
    avoidance learning but normal reward learning.
    
    Parameters:
    alpha_pos: [0, 1] Base learning rate for positive prediction errors.
    beta: [0, 10] Inverse temperature (choice consistency).
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    stai_neg_boost: [0, 1] Additional learning rate added for negative prediction errors, scaled by STAI.
    """
    alpha_pos, beta, w, stai_neg_boost = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Determine learning rate based on sign of prediction error
        if delta_stage1 < 0:
            # Boost learning rate if outcome was worse than expected (Negativity Bias)
            eff_alpha = alpha_pos + (current_stai * stai_neg_boost)
            eff_alpha = np.clip(eff_alpha, 0, 1)
        else:
            eff_alpha = alpha_pos
            
        q_stage1_mf[a1] += eff_alpha * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        if delta_stage2 < 0:
            eff_alpha_2 = alpha_pos + (current_stai * stai_neg_boost)
            eff_alpha_2 = np.clip(eff_alpha_2, 0, 1)
        else:
            eff_alpha_2 = alpha_pos

        q_stage2_mf[state_idx, a2] += eff_alpha_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Decision Noise
This model hypothesizes that anxiety acts as a cognitive load or distractor, reducing the precision of decision-making. Instead of altering how values are learned (alpha) or mixed (w), anxiety lowers the inverse temperature ($\beta$), making choices more stochastic (noisier) regardless of the learned values.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Decision Noise Model.
    
    Hypothesis: High anxiety consumes cognitive resources, resulting in 
    noisier decision-making. Higher STAI reduces the effective beta 
    (inverse temperature), making choices more random.
    
    Parameters:
    alpha: [0, 1] Learning rate.
    base_beta: [0, 10] Maximum inverse temperature (for 0 anxiety).
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    stai_noise: [0, 1] Factor by which STAI reduces the effective beta.
    """
    alpha, base_beta, w, stai_noise = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective beta based on STAI
    # As STAI increases, effective_beta decreases (more noise)
    effective_beta = base_beta * (1.0 - (current_stai * stai_noise))
    effective_beta = np.maximum(effective_beta, 0.0) # Ensure non-negative

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Use effective_beta
        exp_q1 = np.exp(effective_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(effective_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Perseveration (Stickiness)
This model hypothesizes that anxiety leads to "safety behaviors" or perseveration. Anxious individuals may be more likely to repeat their previous choice regardless of whether it was rewarded, simply to avoid the uncertainty of switching.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Perseveration (Stickiness) Model.
    
    Hypothesis: Anxiety increases the tendency to repeat the previous choice 
    (perseveration), regardless of reward history. This is modeled as a 
    'stickiness' bonus added to the Q-value of the previously chosen action, 
    scaled by STAI.
    
    Parameters:
    alpha: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    stai_stickiness: [0, 5] Strength of the perseveration bonus scaled by STAI.
    """
    alpha, beta, w, stai_stickiness = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus if not the first trial
        if last_action_1 != -1:
            q_net[last_action_1] += (stai_stickiness * current_stai)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])
        
        # Store for next trial
        last_action_1 = a1

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```