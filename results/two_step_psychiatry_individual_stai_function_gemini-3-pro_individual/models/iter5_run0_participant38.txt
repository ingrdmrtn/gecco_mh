Here are three new cognitive models that introduce different mechanisms for how anxiety (STAI) might influence decision-making in the two-step task, specifically exploring perseveration, asymmetric learning rates, and noise modulation.

### Model 1: Anxiety-Modulated Perseveration
This model hypothesizes that anxiety influences the tendency to repeat the previous choice (perseveration), regardless of the reward outcome. High anxiety might lead to "sticking" to a choice due to rigidity or fear of change.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Perseveration.
    
    Anxiety (stai) modulates the 'perseveration' parameter 'p', which adds a bonus
    to the previously chosen action at stage 1.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - p_base: [-1, 1] Baseline perseveration bonus.
    - p_mod: [-1, 1] Modulation of perseveration by anxiety.
    """
    learning_rate, beta, w, p_base, p_mod = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0] / 80.0 # Normalize STAI roughly to 0-1 range

    # Calculate effective perseveration bonus
    perseveration = p_base + (p_mod * stai_val)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous action for perseveration (initialize to -1 or handle first trial)
    prev_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combined Q-value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the previous choice
        q_net_biased = q_net.copy()
        if prev_action_1 != -1:
            q_net_biased[int(prev_action_1)] += perseveration

        exp_q1 = np.exp(beta * q_net_biased)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        
        # Update previous action for next trial
        prev_action_1 = act1
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]
        
        # --- Learning ---
        # Stage 2 update (TD)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Stage 1 update (TD using stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Asymmetric Learning Rates
This model posits that anxiety affects how participants learn from positive versus negative prediction errors. Anxious individuals might be more sensitive to negative outcomes (losses) or less sensitive to gains.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Asymmetric Learning Rates.
    
    Anxiety modulates the ratio between learning from positive prediction errors (alpha_pos)
    and negative prediction errors (alpha_neg).
    
    Parameters:
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - alpha_base: [0, 1] Baseline learning rate.
    - alpha_bias: [-1, 1] Bias towards positive (positive val) or negative (negative val) errors driven by anxiety.
    """
    beta, w, alpha_base, alpha_bias = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0] / 80.0

    # Anxiety shifts the balance. 
    # If alpha_bias > 0, anxiety increases learning from positive errors.
    # If alpha_bias < 0, anxiety increases learning from negative errors.
    # We use a sigmoid-like transformation or clipping to keep rates in [0,1].
    
    # Simple modulation:
    # alpha_pos = base + bias * stai
    # alpha_neg = base - bias * stai
    
    eff_bias = alpha_bias * stai_val
    alpha_pos = np.clip(alpha_base + eff_bias, 0.0, 1.0)
    alpha_neg = np.clip(alpha_base - eff_bias, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]
  
        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        lr_2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[state_idx, act2] += lr_2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        lr_1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[act1] += lr_1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Decision Noise (Lapse Rate)
This model suggests that anxiety acts as a distractor, introducing random noise into the decision process. Higher anxiety leads to a higher "lapse rate" (epsilon), where the participant chooses randomly instead of following their value-based policy.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Randomness (Epsilon-Greedy mixture).
    
    Instead of pure softmax, the policy is a mixture of Softmax and Uniform random.
    The weight of the uniform random component (epsilon) is modulated by STAI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature for the softmax component.
    - w: [0, 1] Mixing weight (MB vs MF).
    - eps_base: [0, 1] Baseline random noise (lapse rate).
    - eps_mod: [0, 1] Modulation of noise by anxiety.
    """
    learning_rate, beta, w, eps_base, eps_mod = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0] / 80.0

    # Calculate epsilon (probability of random choice)
    epsilon = eps_base + (eps_mod * stai_val)
    epsilon = np.clip(epsilon, 0.0, 1.0) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax probability
        exp_q1 = np.exp(beta * q_net)
        softmax_probs_1 = exp_q1 / np.sum(exp_q1)
        
        # Mixture policy: (1-eps)*Softmax + eps*Random
        probs_1 = (1 - epsilon) * softmax_probs_1 + epsilon * 0.5
        
        act1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[act1]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        softmax_probs_2 = exp_q2 / np.sum(exp_q2)
        
        probs_2 = (1 - epsilon) * softmax_probs_2 + epsilon * 0.5
        
        act2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[act2]
  
        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        

    eps_safe = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps_safe)) + np.sum(np.log(p_choice_2 + eps_safe)))
    return log_loss
```