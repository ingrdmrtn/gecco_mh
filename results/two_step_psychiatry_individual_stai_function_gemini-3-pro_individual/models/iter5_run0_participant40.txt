Here are three new cognitive models that incorporate the STAI anxiety score to explain individual variability in the two-step task.

### Model 1: STAI-Modulated Loss Aversion

This model hypothesizes that individuals with higher anxiety are more sensitive to negative outcomes (punishment). While they value positive rewards similarly to low-anxiety individuals, negative rewards (losses) are subjectively amplified, leading to stronger avoidance learning.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Loss Aversion Model.
    
    This model posits that anxiety (STAI) amplifies the subjective perception of 
    negative rewards. A 'loss_sensitivity' parameter scales negative rewards 
    based on the STAI score, making them effectively more painful.
    
    Parameters:
    learning_rate: [0,1] - Update rate for Q-values.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Mixing weight (0=Model-Free, 1=Model-Based).
    loss_sens: [0,10] - Sensitivity to negative rewards. 
                        Effective Reward = Reward * (1 + loss_sens * STAI) if Reward < 0.
    """
    learning_rate, beta, w, loss_sens = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # --- Value Updating ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Apply Loss Aversion modulation based on STAI
        if r < 0:
            eff_reward = r * (1.0 + loss_sens * stai_val)
        else:
            eff_reward = r

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = eff_reward - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: STAI-Modulated Memory Decay

This model suggests that anxiety interferes with memory maintenance. High anxiety consumes cognitive resources (e.g., through worry), causing the values of unchosen options to decay (forget) faster than in low-anxiety individuals. This results in a "use it or lose it" effect on value representations.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Memory Decay Model.
    
    This model assumes that anxiety accelerates the decay of value representations 
    for unchosen options. On every trial, the Q-values of actions NOT chosen 
    decay towards zero at a rate determined by STAI.
    
    Parameters:
    learning_rate: [0,1] - Update rate for chosen options.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Mixing weight (Model-Based vs Model-Free).
    decay_factor: [0,1] - Base scalar for decay. 
                          Decay Rate = decay_factor * STAI.
    """
    learning_rate, beta, w, decay_factor = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate decay rate specific to this participant
    current_decay = decay_factor * stai_val
    # Clamp to ensure stability
    if current_decay > 1.0: current_decay = 1.0
    if current_decay < 0.0: current_decay = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Choice 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Policy for Choice 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Value Updating ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        # Standard update for chosen actions
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay for unchosen actions
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - current_decay)
        
        unchosen_a2 = 1 - a2
        # Decay unchosen alien in current state
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - current_decay)
        # Note: Aliens in the other state are not decayed here to preserve
        # structure, focusing on local working memory interference.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: STAI-Modulated Planning Noise

This model proposes that anxiety specifically degrades the precision of high-level planning (Stage 1) without necessarily affecting the simpler, reactive decision-making at Stage 2. High anxiety introduces "planning noise," effectively lowering the inverse temperature ($\beta$) for the first stage choice relative to the second stage.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Planning Noise Model.
    
    This model separates exploration/noise parameters for Stage 1 (Planning) and 
    Stage 2 (Bandit). It posits that anxiety specifically degrades Stage 1 
    precision.
    
    Beta_Stage2 is the base precision.
    Beta_Stage1 = Beta_Stage2 / (1 + noise_param * STAI).
    
    Parameters:
    learning_rate: [0,1] - Update rate.
    beta_base: [0,10] - Base inverse temperature (used for Stage 2).
    w: [0,1] - Mixing weight.
    planning_noise: [0,10] - Factor by which STAI reduces Stage 1 precision.
    """
    learning_rate, beta_base, w, planning_noise = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate stage-specific betas
    # High STAI reduces beta_1 (more random/noisy planning choice)
    beta_1 = beta_base / (1.0 + planning_noise * stai_val)
    beta_2 = beta_base

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Policy for Choice 1 (Modulated Beta) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Policy for Choice 2 (Base Beta) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # --- Value Updating ---
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```