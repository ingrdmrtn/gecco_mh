Here are 3 new cognitive models based on the provided template and constraints.

### Model 1: STAI-Modulated Learning Rate
This model hypothesizes that anxiety (STAI) directly alters cognitive plasticity (learning rate). High anxiety might lead to hyper-plasticity (over-adjusting to recent feedback) or rigidity (under-adjusting). The learning rate is modeled as a linear function of the STAI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with STAI-modulated Learning Rate.
    
    Hypothesis: Anxiety levels determine the rate at which participants update 
    their value expectations. 
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate.
    - lr_stai_slope: [-1, 1] How STAI modulates learning rate. 
                     (lr = lr_base + slope * stai).
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    """
    lr_base, lr_stai_slope, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    # Calculate effective learning rate based on STAI
    # Clip to ensure it stays within valid bounds [0, 1]
    learning_rate = lr_base + (lr_stai_slope * stai_val)
    learning_rate = np.clip(learning_rate, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Weighted mixture of MB and MF
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1 (TD-0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: STAI-Modulated Reward Sensitivity
This model hypothesizes that anxiety alters the subjective magnitude of rewards. Anxious individuals might perceive rewards as more salient (hypersensitivity) or less salient (blunting) than non-anxious individuals. The STAI score scales the reward input before prediction errors are calculated.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model where STAI modulates subjective reward sensitivity.
    
    Hypothesis: Anxiety affects how strongly outcomes are perceived. 
    Effective Reward = Reward * (Base + Slope * STAI).
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    - sens_base: [0, 2] Baseline reward sensitivity multiplier.
    - sens_stai: [-2, 2] Effect of STAI on sensitivity.
    """
    learning_rate, beta, w, sens_base, sens_stai = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate subjective sensitivity multiplier
    sensitivity = sens_base + (sens_stai * stai_val)
    # Ensure sensitivity is non-negative to prevent sign flipping of rewards
    sensitivity = np.maximum(sensitivity, 0.0)

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 using Subjective Reward
        subjective_reward = reward[trial] * sensitivity
        delta_stage2 = subjective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Noise Modulation
This model separates decision noise (beta) into Stage 1 (planning) and Stage 2 (immediate outcome). It hypothesizes that anxiety specifically degrades decision consistency at the second stage—when facing the alien directly—while leaving the more abstract planning stage (spaceship choice) relatively unaffected (or affected differently).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Stage-Specific Beta modulation by STAI.
    
    Hypothesis: Anxiety impacts decision noise differently at the abstract 
    planning stage (Stage 1) vs the immediate reward stage (Stage 2).
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - beta_2_base: [0, 10] Baseline Inverse temperature for Stage 2.
    - beta_2_stai_slope: [-5, 5] Effect of STAI on Stage 2 noise.
    - w: [0, 1] Weight for Model-Based control.
    """
    learning_rate, beta_1, beta_2_base, beta_2_stai_slope, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate Stage 2 beta based on STAI
    beta_2 = beta_2_base + (beta_2_stai_slope * stai_val)
    beta_2 = np.maximum(beta_2, 0.0) # Ensure beta is non-negative

    for trial in range(n_trials):

        # policy for the first choice (Uses beta_1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice (Uses STAI-modulated beta_2)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```