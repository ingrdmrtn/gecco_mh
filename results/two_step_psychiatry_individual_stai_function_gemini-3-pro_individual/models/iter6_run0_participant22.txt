Here are three new cognitive models exploring different mechanisms by which anxiety (STAI) might influence decision-making in the two-step task.

### Model 1: Anxiety-Driven Learning Rate Asymmetry
This model hypothesizes that anxiety biases how individuals learn from positive versus negative prediction errors. High anxiety is often associated with a "negativity bias," where individuals may update their beliefs more drastically after a disappointment (negative prediction error) than after a success, or conversely, be hyper-vigilant to errors.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Driven Learning Rate Asymmetry.
    
    This model posits that anxiety (STAI) differentially affects learning from positive 
    vs. negative prediction errors. Specifically, it scales the learning rate for 
    negative prediction errors based on the STAI score, testing the hypothesis of 
    negativity bias or hyper-sensitivity to worse-than-expected outcomes.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - lr_neg_base: [0, 1] Baseline learning rate for negative prediction errors (RPE < 0).
    - lr_neg_stai_sens: [0, 1] Sensitivity of the negative learning rate to STAI.
      (Effective lr_neg = lr_neg_base + lr_neg_stai_sens * stai).
    - beta: [0, 10] Inverse temperature (softmax).
    - w: [0, 1] Weight of Model-Based system (0=MF, 1=MB).
    """
    lr_pos, lr_neg_base, lr_neg_stai_sens, beta, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective negative learning rate, bounded at 1
    lr_neg = lr_neg_base + (lr_neg_stai_sens * stai_score)
    if lr_neg > 1.0: lr_neg = 1.0
    if lr_neg < 0.0: lr_neg = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Stage 1 Update (SARSA-style for MF)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Apply asymmetric learning rate
        curr_lr1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += curr_lr1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Apply asymmetric learning rate
        curr_lr2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += curr_lr2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Eligibility Trace (Lambda)
This model explores the idea that anxiety affects how credit is assigned to past actions. In reinforcement learning, the eligibility trace parameter ($\lambda$) controls how much the second-stage reward updates the first-stage value directly. A lower $\lambda$ implies the agent treats the two stages as disconnected, while a higher $\lambda$ connects the outcome back to the initial choice. Anxiety might disrupt this integration (low $\lambda$) or cause over-attribution (high $\lambda$).

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Modulated Eligibility Trace (Lambda).
    
    This model investigates if anxiety affects the eligibility trace (lambda), which 
    controls the efficiency of credit assignment from Stage 2 rewards back to Stage 1 
    choices in the Model-Free system. 

    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lambda_base: [0, 1] Baseline eligibility trace parameter.
    - lambda_stai_slope: [-1, 1] How STAI modulates lambda. 
      (Effective lambda = lambda_base + lambda_stai_slope * stai).
    """
    learning_rate, beta, w, lambda_base, lambda_stai_slope = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective lambda, bounded [0, 1]
    eff_lambda = lambda_base + (lambda_stai_slope * stai_score)
    if eff_lambda > 1.0: eff_lambda = 1.0
    if eff_lambda < 0.0: eff_lambda = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        # Calculate RPEs
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Stage 1 MF update:
        # Standard TD(0) part: learning_rate * delta_stage1
        # Eligibility trace part: learning_rate * lambda * delta_stage2
        q_stage1_mf[a1] += learning_rate * delta_stage1 + learning_rate * eff_lambda * delta_stage2

        # Stage 2 MF update
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Exploration (Inverse Temperature Modulation)
This model posits that anxiety fundamentally alters the exploration-exploitation trade-off. High anxiety might lead to "random" behavior due to difficulty concentrating (low beta), or rigid, risk-averse behavior (high beta). This model allows the softmax temperature (inverse temperature $\beta$) to vary linearly with the STAI score.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Induced Exploration (Inverse Temperature Modulation).
    
    This model tests whether anxiety impacts decision noise (exploration/exploitation).
    It separates the baseline randomness from the anxiety-induced component. 
    It assumes anxiety modulates the inverse temperature (beta) for both stages.

    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta_base: [0, 10] Baseline inverse temperature (for low anxiety).
    - beta_stai_slope: [-5, 5] Effect of STAI on beta. 
       (Effective beta = beta_base + beta_stai_slope * stai).
    - w: [0, 1] Weight of Model-Based system.
    - stickiness: [0, 5] Choice stickiness for Stage 1.
    """
    learning_rate, beta_base, beta_stai_slope, w, stickiness = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective beta, lower bounded at 0
    beta_eff = beta_base + (beta_stai_slope * stai_score)
    if beta_eff < 0: beta_eff = 0.0
    if beta_eff > 20: beta_eff = 20.0 # Upper safety bound

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness

        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        last_action_1 = int(action_1[trial])
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```