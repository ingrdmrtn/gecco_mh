Here are 3 new cognitive models based on the provided template and specifications.

### Model 1: Anxiety-Induced Cognitive Map Distortion
This model hypothesizes that high anxiety degrades the accuracy of the internal model (the cognitive map). While the actual transition probability is 0.7, anxious participants may perceive the world as more chaotic (closer to 0.5/0.5), reducing the effectiveness of Model-Based planning.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    RL Model where STAI distorts the Model-Based Transition Matrix.
    
    Hypothesis: High anxiety blurs the participant's internal model of the 
    spaceship-planet transitions. While the real probability is 0.7, high STAI 
    scores push the internal belief towards 0.5 (maximum entropy/uncertainty), 
    weakening the Model-Based contribution.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weight for model-based control (0=MF, 1=MB).
    distortion_rate: [0, 0.4] - How much STAI reduces the perceived transition probability 
                               from 0.7. Max distortion 0.4 would flip it, but practically 
                               we limit impact to flattening towards 0.5.
    """
    learning_rate, beta, w, distortion_rate = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # STAI modifies the perceived transition matrix
    # Base is 0.7. Anxiety subtracts from this confidence.
    # We clip to ensure it doesn't go below 0.5 (randomness) or invert.
    perceived_prob = 0.7 - (distortion_rate * current_stai)
    perceived_prob = np.maximum(perceived_prob, 0.5) 
    
    # We define the matrix used for MB planning based on STAI
    transition_matrix = np.array([[perceived_prob, 1-perceived_prob], 
                                  [1-perceived_prob, perceived_prob]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB calculation uses the ANXIETY-DISTORTED matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        act1 = int(action_1[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Subjective Reward Valuation
This model hypothesizes that anxiety alters the subjective value of the reward itself, rather than the learning process. Anxious individuals might experience "relief" (amplified reward) or "anhedonia" (dampened reward) when finding gold.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    RL Model where STAI scales the Subjective Utility of the Reward.
    
    Hypothesis: The objective reward (1 coin) is perceived differently based on anxiety.
    This model allows STAI to scale the reward magnitude entered into the prediction 
    error calculation. 
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weight for model-based control.
    stai_reward_scaling: [0, 2] - Factor by which STAI amplifies (or dampens) reward perception.
                                  val = 1.0 + (stai * scaling).
    """
    learning_rate, beta, w, stai_reward_scaling = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
    
    # Calculate subjective reward multiplier based on STAI
    # If scaling is positive, anxiety increases reward sensitivity (hypersensitivity)
    # If we wanted dampening, we'd use subtraction, but here we model sensitivity magnitude.
    subjective_multiplier = 1.0 + (current_stai * stai_reward_scaling)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        act1 = int(action_1[trial])
        act2 = int(action_2[trial])
        
        # Apply STAI modulation to the reward
        r_subjective = reward[trial] * subjective_multiplier

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        # Update Stage 2 using the subjective reward
        delta_stage2 = r_subjective - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Stage 2 Noise (Dual Beta)
This model separates the exploration/exploitation trade-off into two stages. It hypothesizes that anxiety specifically degrades decision precision at the second stage (confronting the alien), leading to more stochastic behavior (lower beta) at the final step, while Stage 1 planning remains stable.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    RL Model with separate Stage 1 and Stage 2 Betas, where STAI modualtes Stage 2.
    
    Hypothesis: Anxiety acts as a distractor specifically at the moment of outcome 
    interaction (Stage 2). While the planning stage (Stage 1) uses a base beta, 
    Stage 2 decision making becomes noisier (lower beta) as STAI increases.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    w: [0, 1] - Weight for model-based control.
    beta1: [0, 10] - Fixed inverse temperature for Stage 1 (Spaceship choice).
    beta2_base: [0, 10] - Baseline inverse temperature for Stage 2 (Alien choice).
    stai_beta2_decay: [0, 5] - How strongly STAI reduces Beta 2. 
                               beta2 = beta2_base / (1 + stai * decay).
    """
    learning_rate, w, beta1, beta2_base, stai_beta2_decay = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
    
    # Calculate effective Beta for Stage 2 based on STAI
    # Higher anxiety -> Higher denominator -> Lower Beta -> More random Stage 2 choices
    beta2_eff = beta2_base / (1.0 + current_stai * stai_beta2_decay)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Uses fixed Beta1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice (Uses STAI-modulated Beta2)
        exp_q2 = np.exp(beta2_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        act1 = int(action_1[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```