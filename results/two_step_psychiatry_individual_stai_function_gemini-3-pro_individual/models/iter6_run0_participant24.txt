Here are 3 new cognitive models exploring different mechanisms by which anxiety (STAI) might influence decision-making in the two-step task.

### Model 1: Anxiety-Modulated MB/MF Trade-off
This model hypothesizes that anxiety consumes cognitive resources (working memory), reducing the ability to use Model-Based (planning) strategies. Therefore, higher anxiety (STAI) reduces the weight parameter `w`, biasing the participant towards Model-Free (habitual) control.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated MB/MF Trade-off Model.
    
    Hypothesis: Anxiety acts as a cognitive load, reducing working memory capacity 
    required for model-based planning. Higher STAI scores lead to a lower weighting (w)
    of the model-based system, pushing behavior towards model-free habits.
    
    Parameters:
    learning_rate: [0, 1] - Standard learning rate for value updates.
    beta: [0, 10] - Inverse temperature for softmax choice.
    w_max: [0, 1] - The maximum possible model-based weight (for a person with 0 anxiety).
    stai_burden: [0, 1] - The degree to which STAI reduces the model-based weight.
    """
    learning_rate, beta, w_max, stai_burden = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate the effective weight w. As STAI increases, w decreases.
    # w is clipped to be at least 0.
    w = np.clip(w_max - (stai_burden * stai_val), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values based on the anxiety-modulated weight
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Value updating
        # Stage 2 update (standard TD)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 update (standard TD)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Choice Perseveration
This model posits that anxiety leads to behavioral rigidity or "stickiness." High anxiety individuals might be more likely to repeat their previous choice regardless of the outcome, as a safety behavior or due to cognitive inflexibility. The STAI score modulates the `stickiness` parameter.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Choice Perseveration Model.
    
    Hypothesis: Anxiety increases behavioral rigidity. Higher STAI scores increase 
    choice perseveration (stickiness), making participants more likely to repeat 
    the previous Stage 1 action regardless of reward history.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-based weight.
    stick_base: [0, 5] - Baseline tendency to repeat the previous choice.
    stai_stick_boost: [0, 5] - Amplification of stickiness by STAI score.
    """
    learning_rate, beta, w, stick_base, stai_stick_boost = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate effective stickiness
    eff_stickiness = stick_base + (stai_stick_boost * stai_val)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous choice

    for trial in range(n_trials):
        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_net_stage1_stick = q_net_stage1.copy()
        if last_action_1 != -1:
            q_net_stage1_stick[last_action_1] += eff_stickiness
        
        exp_q1 = np.exp(beta * q_net_stage1_stick)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        last_action_1 = a1 # Update last action

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Value updating
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Reward Sensitivity Dampening
This model suggests that anxiety creates a blunted response to rewards (anhedonia-like component often comorbid with anxiety) or a pessimistic evaluation of outcomes. Here, STAI scales down the effective magnitude of the reward received, making the agent learn less aggressively from successes while failures (0 reward) remain unchanged.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Reward Sensitivity Dampening Model.
    
    Hypothesis: High anxiety blunts the subjective valuation of positive outcomes.
    Instead of perceiving a reward of 1.0, anxious individuals perceive it as 
    1.0 - (stai * dampening_factor). This reduces the drive to pursue rewards.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-based weight.
    stai_dampening: [0, 1] - Factor by which STAI reduces perceived reward magnitude.
    """
    learning_rate, beta, w, stai_dampening = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # Modulate reward perception based on STAI
        # If reward is 1, effective reward is reduced. If 0, stays 0.
        r = reward[trial]
        if r > 0:
            effective_reward = r * (1.0 - (stai_dampening * stai_val))
            # Ensure reward doesn't flip to negative due to high params
            effective_reward = max(effective_reward, 0.0)
        else:
            effective_reward = 0.0

        # Value updating using effective_reward
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```