Here are the 3 proposed cognitive models.

### Model 1: Anxiety-Modulated Eligibility Trace
```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated Eligibility Trace.
    
    Hypothesis: High anxiety increases the eligibility trace (lambda), causing participants 
    to attribute second-stage outcomes directly to the first-stage choice. This blurs 
    the distinction between the two stages, effectively making the agent more "model-free" 
    by reinforcing the first choice based on the final reward, bypassing the transition structure.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for Model-Based control (0=Pure MF, 1=Pure MB).
    - lambda_sensitivity: [0, 5] How strongly STAI increases the eligibility trace lambda.
      lambda = Clip(lambda_sensitivity * stai, 0, 1).
    """
    learning_rate, beta, w, lambda_sensitivity = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    # Calculate anxiety-dependent eligibility trace
    lambda_val = np.clip(lambda_sensitivity * stai_val, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # ACTION VALUE UPDATING FOR CHOICE 1
        # Standard TD update + Eligibility Trace update (controlled by anxiety)
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Apply eligibility trace: Propagate stage 2 error back to stage 1 choice
        q_stage1_mf[a1] += learning_rate * lambda_val * delta_stage2

        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Perseveration
```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Driven Perseveration.
    
    Hypothesis: Anxiety increases behavioral rigidity or "stickiness" (perseveration).
    Anxious participants are more likely to repeat their previous Stage 1 choice 
    regardless of the reward history, as a form of safety behavior or avoidance of switching costs.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for Model-Based control.
    - stick_sensitivity: [0, 5] Scaling factor for STAI-driven perseveration.
      stickiness_bonus = stick_sensitivity * stai.
    """
    learning_rate, beta, w, stick_sensitivity = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate anxiety-dependent stickiness bonus
    stickiness_bonus = stick_sensitivity * stai_val
    prev_a1 = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Calculate logits (unnormalized log probabilities)
        logits = beta * q_net
        
        # Add stickiness bonus to the previously chosen action
        if prev_a1 != -1:
            logits[prev_a1] += stickiness_bonus
            
        exp_q1 = np.exp(logits - np.max(logits)) # sub max for stability
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        prev_a1 = a1 # Store for next trial

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        # ACTION VALUE UPDATING FOR CHOICE 1
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        # ACTION VALUE UPDATING FOR CHOICE 2
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Enhanced Negativity Bias
```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Enhanced Negativity Bias.
    
    Hypothesis: Anxiety creates an asymmetry in learning from prediction errors.
    Specifically, anxiety amplifies the learning rate for negative prediction errors 
    (disappointments/losses), causing participants to over-update when outcomes are worse than expected.
    
    Parameters:
    - lr_pos: [0, 1] Base learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - anxiety_neg_boost: [0, 5] Multiplier for STAI to boost the learning rate for negative errors.
      lr_neg = lr_pos * (1 + anxiety_neg_boost * stai).
    """
    lr_pos, beta, w, anxiety_neg_boost = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Define asymmetric learning rates
    lr_neg = lr_pos * (1.0 + anxiety_neg_boost * stai_val)
    lr_neg = np.clip(lr_neg, 0.0, 1.0) # Ensure it stays within bounds

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # ACTION VALUE UPDATING FOR CHOICE 1
        # Select learning rate based on sign of prediction error
        current_lr_1 = lr_neg if delta_stage1 < 0 else lr_pos
        q_stage1_mf[a1] += current_lr_1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # ACTION VALUE UPDATING FOR CHOICE 2
        # Select learning rate based on sign of prediction error
        current_lr_2 = lr_neg if delta_stage2 < 0 else lr_pos
        q_stage2_mf[state_idx, a2] += current_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```