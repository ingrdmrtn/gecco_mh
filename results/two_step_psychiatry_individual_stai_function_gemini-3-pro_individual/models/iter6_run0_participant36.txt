Here are 3 new cognitive models that incorporate STAI scores into the decision-making process using mechanisms distinct from simple learning rate or mixing weight modulation.

### Model 1: Anxiety-Induced Decision Noise
**Hypothesis:** High anxiety acts as cognitive "noise" or distraction. Instead of altering how values are learned (alpha) or how strategies are mixed (w), anxiety reduces the precision of the choice rule itself (Beta). Anxious participants are less consistent in converting value differences into choices, effectively having a higher "temperature" (lower Beta) in the softmax function.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model where STAI increases decision noise (decreases inverse temperature Beta).
    
    Hypothesis: Anxiety consumes working memory resources, leading to less deterministic 
    selection (more randomness) for a given set of values.
    
    Parameters:
    alpha: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature for a low-anxiety individual.
    w: [0, 1] Weighting between Model-Based (1) and Model-Free (0).
    stai_noise: [0, 5] Scaling factor. Higher STAI reduces the effective Beta.
    """
    alpha, beta_base, w, stai_noise = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Effective beta decreases as anxiety increases (more noise)
    # Using division ensures it doesn't go negative and scales appropriately.
    beta_effective = beta_base / (1.0 + (stai_noise * current_stai))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Combine MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Use anxiety-modulated beta
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += alpha * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Perseveration (Stickiness)
**Hypothesis:** Anxiety leads to "safety behaviors" or rigidity. Anxious participants are more likely to repeat their previous Stage 1 choice regardless of the outcome, simply to avoid the uncertainty of switching. This adds a "stickiness" bonus to the previously chosen option, scaled by the STAI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model where STAI increases Choice Stickiness (Perseveration).
    
    Hypothesis: High anxiety increases the tendency to repeat the previous Stage 1 
    action (rigidity/safety-seeking), regardless of reward history.
    
    Parameters:
    alpha: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stai_stickiness: [0, 5] Magnitude of the stickiness bonus per unit of STAI.
    """
    alpha, beta, w, stai_stickiness = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Calculate the specific stickiness bonus for this subject
    stick_bonus = stai_stickiness * current_stai

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize as no previous action

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the Q-value of the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stick_bonus

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        curr_a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[curr_a1]
        
        # Update tracker for next trial
        last_action_1 = curr_a1
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[curr_a1]
        q_stage1_mf[curr_a1] += alpha * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Blunted Reward Sensitivity
**Hypothesis:** Anxiety is often comorbid with depressive symptoms or general pessimism. This model proposes that high anxiety dampens the subjective valuation of the "Gold Coin" reward. While the objective reward is 1.0, an anxious participant might perceive it as effectively less (e.g., 0.8), leading to smaller prediction errors and slower value accumulation.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model where STAI dampens Reward Sensitivity (Subjective Value).
    
    Hypothesis: Anxiety reduces the subjective impact of positive outcomes 
    (blunted reward processing), reducing the magnitude of value updates.
    
    Parameters:
    alpha: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Model-Based weight.
    stai_dampening: [0, 1] Percentage reduction in reward value per unit of STAI.
    """
    alpha, beta, w, stai_dampening = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Calculate Effective Reward based on STAI
        # If stai_dampening is high, high anxiety makes reward feel closer to 0.
        # We clip to ensure it doesn't invert the reward.
        dampener = np.clip(1.0 - (stai_dampening * current_stai), 0.0, 1.0)
        effective_reward = reward[trial] * dampener

        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += alpha * delta_stage1
        
        # Update Stage 2 using Effective Reward
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += alpha * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```