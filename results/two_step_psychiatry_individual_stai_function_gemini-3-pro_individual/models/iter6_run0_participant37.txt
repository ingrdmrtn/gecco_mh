Here are three new cognitive models based on the provided template and task description.

### Model 1: Anxiety-Induced Rigidity (Beta Modulation)
This model hypothesizes that anxiety reduces behavioral flexibility, leading to more "rigid" or deterministic choices (exploitation) rather than exploration. It models this by scaling the inverse temperature parameter (`beta`) with the STAI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Induced Rigidity.
    
    Hypothesis: Higher anxiety leads to reduced exploration and more rigid, 
    deterministic choice patterns. This is modeled by scaling the softmax 
    inverse temperature (beta) by the STAI score. Higher STAI results in 
    a higher effective beta (steeper softmax curve).

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta_base: [0, 10] Baseline inverse temperature for softmax.
    w: [0, 1] Mixing weight (1=Pure MB, 0=Pure MF).
    stai_rigidity: [0, 5] Scaling factor for beta based on anxiety.
    """
    learning_rate, beta_base, w, stai_rigidity = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective beta: Anxiety increases rigidity (higher beta)
    beta_effective = beta_base * (1.0 + stai_rigidity * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated values
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_effective * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta_effective * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update stage 1 MF values
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update stage 2 MF values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Transition Uncertainty
This model hypothesizes that anxiety degrades the accuracy of the internal model of the environment. Instead of trusting the true transition probabilities (0.7/0.3), anxious individuals perceive the transitions as more uncertain or random (closer to 0.5/0.5), reducing the effectiveness of Model-Based planning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Driven Transition Uncertainty.
    
    Hypothesis: Anxiety introduces noise into the internal model of the task structure.
    High anxiety individuals rely on a 'corrupted' transition matrix that is a 
    mixture of the true matrix and a uniform (random) matrix, effectively 
    blunting the Model-Based advantage without strictly changing the weight 'w'.

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Mixing weight (1=Pure MB, 0=Pure MF).
    stai_noise: [0, 1] Degree of transition matrix corruption by anxiety.
    """
    learning_rate, beta, w, stai_noise = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Define the true transition matrix and a uniform 'uncertainty' matrix
    true_transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    flat_transition_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    # Mix them based on STAI. 
    # High STAI -> More weight on the flat matrix.
    noise_level = np.clip(stai_noise * stai_score, 0, 1)
    subjective_transition_matrix = (1 - noise_level) * true_transition_matrix + noise_level * flat_transition_matrix

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the subjective (possibly corrupted) matrix for MB calculation
        q_stage1_mb = subjective_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update stage 1 MF values
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update stage 2 MF values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Dependent Value Decay
This model hypothesizes that anxiety interferes with working memory and value maintenance. It posits that anxious individuals have a "leaky" value representation, where learned Q-values decay towards zero more rapidly over time compared to low-anxiety individuals.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Dependent Value Decay.
    
    Hypothesis: Anxiety impairs the maintenance of learned values over time 
    (working memory deficit). This is modeled by introducing a decay rate 
    applied to all Q-values at the end of each trial. The decay rate is 
    proportional to the STAI score.

    Parameters:
    learning_rate: [0, 1] Learning rate for value updates.
    beta: [0, 10] Inverse temperature for softmax.
    w: [0, 1] Mixing weight (1=Pure MB, 0=Pure MF).
    stai_decay: [0, 1] Decay rate scaling factor (per trial) based on anxiety.
    """
    learning_rate, beta, w, stai_decay = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate decay factor. 
    # If stai_decay is high, factor is smaller (<1), causing value loss.
    decay_rate = np.clip(stai_decay * stai_score, 0, 1)
    retention_factor = 1.0 - decay_rate

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update stage 1 MF values
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        # Update stage 2 MF values
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Apply Anxiety-Dependent Decay to all MF values
        # This simulates 'forgetting' or difficulty maintaining precise values
        q_stage1_mf *= retention_factor
        q_stage2_mf *= retention_factor

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```