Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process.

### Model 1: STAI-Modulated Eligibility Traces
This model hypothesizes that anxiety affects **temporal credit assignment**. High anxiety (rumination) might increase the "eligibility trace" ($\lambda$), causing the final reward to be more strongly associated with the first-stage choice, effectively bridging the time gap in the model-free system. Conversely, it could represent a deficit where anxiety prevents this link. Here, we model $\lambda$ as a function of STAI.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Eligibility Trace (TD-Lambda) Model.
    
    This model introduces an eligibility trace parameter (lambda) that is modulated 
    by the participant's anxiety (STAI). The eligibility trace controls how much 
    the second-stage reward prediction error (RPE) updates the first-stage Q-value.
    
    Hypothesis: Anxiety influences the extent to which distal rewards reinforce 
    proximal choices (rumination or cognitive bridging).
    
    Parameters:
    learning_rate: [0,1] - Base learning rate for Q-value updates.
    beta: [0,10] - Inverse temperature for softmax choice.
    w: [0,1] - Weighting between Model-Based (1) and Model-Free (0) control.
    lambda_stai: [0,1] - Sensitivity of the eligibility trace to STAI.
                         lambda = lambda_stai * STAI.
    """
    learning_rate, beta, w, lambda_stai = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate eligibility trace lambda based on STAI
    # We clip to ensure it stays within valid bounds [0, 1]
    lambda_val = np.clip(lambda_stai * stai_val, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 1]
        # We calculate the stage 2 RPE early to implement the eligibility trace update
        # Q1(a1) <- Q1(a1) + lr * delta1 + lr * lambda * delta2
        rpe_stage2_anticipatory = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * (delta_stage1 + lambda_val * rpe_stage2_anticipatory)
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 2]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: STAI-Modulated Differential Learning Rates
This model hypothesizes that anxiety affects learning differently at the abstract planning stage (Stage 1) versus the concrete outcome stage (Stage 2). High anxiety might impair the ability to learn the value of the spaceship (Stage 1) while leaving the simpler stimulus-response learning of the aliens (Stage 2) intact, or vice versa.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Differential Learning Rate Model.
    
    This model allows for different learning rates in Stage 1 and Stage 2,
    where the ratio or magnitude of the Stage 1 learning rate is modulated by STAI.
    Anxiety may selectively impair the update of higher-level choices.
    
    Hypothesis: Anxiety creates a disparity between abstract (Stage 1) and 
    concrete (Stage 2) value learning.
    
    Parameters:
    lr_base: [0,1] - Base learning rate (applied to Stage 2).
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Model-based weight.
    lr_attenuation_stai: [0,1] - Factor by which STAI attenuates Stage 1 learning.
                                 lr_stage1 = lr_base * (1 - lr_attenuation_stai * STAI).
    """
    lr_base, beta, w, lr_attenuation_stai = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    # Define learning rates
    # Stage 2 uses the base rate. Stage 1 is attenuated by anxiety.
    lr_stage2 = lr_base
    attenuation = np.clip(lr_attenuation_stai * stai_val, 0.0, 1.0)
    lr_stage1 = lr_base * (1.0 - attenuation)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 1]
        # Use Stage 1 specific learning rate
        q_stage1_mf[int(action_1[trial])] += lr_stage1 * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 2]
        # Use Stage 2 specific learning rate
        q_stage2_mf[state_idx, int(action_2[trial])] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: STAI-Modulated Value Decay (Forgetting)
This model hypothesizes that anxiety consumes working memory resources, leading to faster **decay (forgetting)** of learned values for options that were *not* chosen.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Value Decay (Forgetting) Model.
    
    This model posits that anxiety increases the rate at which Q-values decay 
    towards zero for unchosen options. This represents a "use-it-or-lose-it" 
    memory failure exacerbated by anxious cognitive load.
    
    Hypothesis: Higher STAI scores lead to faster forgetting of action values 
    that are not currently being updated.
    
    Parameters:
    learning_rate: [0,1] - Learning rate for chosen options.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - Model-based weight.
    decay_sensitivity: [0,1] - Sensitivity of the decay rate to STAI.
                               decay_rate = decay_sensitivity * STAI.
    """
    learning_rate, beta, w, decay_sensitivity = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
  
    # Calculate decay rate based on STAI
    decay_rate = np.clip(decay_sensitivity * stai_val, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 1]
        a1 = int(action_1[trial])
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen option in Stage 1
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # [FILL IN ACTION VALUE UPDATING FOR CHOICE 2]
        a2 = int(action_2[trial])
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen option in Stage 2 (for the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```