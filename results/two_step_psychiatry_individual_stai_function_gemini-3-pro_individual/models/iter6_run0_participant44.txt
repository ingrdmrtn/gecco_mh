Here are three new cognitive models exploring different mechanisms for how anxiety (STAI) influences decision-making in the two-step task.

### Model 1: Anxiety-Modulated Inverse Temperature (Exploration/Exploitation Balance)
This model hypothesizes that anxiety affects the exploration-exploitation trade-off. Specifically, it tests if higher anxiety leads to more deterministic choices (higher beta, less exploration) or more stochastic choices (lower beta, "noise" or panic). It modifies the standard hybrid model by scaling the inverse temperature `beta` based on the STAI score.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Inverse Temperature.
    
    Hypothesis: Anxiety (STAI) alters the randomness of choice (beta).
    High anxiety might lead to more erratic behavior (lower beta) or 
    more rigid exploitation (higher beta).
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_mod: [-5, 5] Modulation of beta by STAI score.
    - w: [0, 1] Weight for Model-Based control.
    """
    learning_rate, beta_base, beta_mod, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    # Effective beta is modulated by STAI. 
    # We clip to ensure it stays non-negative and within reasonable bounds.
    beta_eff = max(0.0, beta_base + beta_mod * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # policy for the second choice
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # Update Q-values
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Punishment Sensitivity
This model investigates if anxious individuals are more sensitive to the lack of reward (punishment). Instead of a single learning rate, it uses separate learning rates for positive (reward=1) and negative (reward=0) outcomes. The learning rate for negative outcomes is specifically scaled by the STAI score, positing that anxiety amplifies learning from failure.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Pure MF model with Anxiety-Modulated Punishment Sensitivity.
    
    Hypothesis: Anxiety increases the learning rate specifically for 
    negative outcomes (reward = 0), making the agent flee non-rewarding 
    options faster.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for rewards (positive prediction errors).
    - lr_neg_base: [0, 1] Baseline learning rate for non-rewards.
    - lr_neg_mod: [0, 1] Additional sensitivity to non-rewards based on STAI.
    - beta: [0, 10] Inverse temperature.
    """
    lr_pos, lr_neg_base, lr_neg_mod, beta = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate effective negative learning rate
    lr_neg_eff = min(1.0, lr_neg_base + lr_neg_mod * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])

        # policy for the first choice (Pure MF here for simplicity of mechanism isolation)
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # Action value updating
        # Stage 1 update
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        # Note: Stage 1 RPE is driven by Stage 2 Q-value, usually treated with standard LR
        # We apply the pos/neg logic to the final outcome RPE (Stage 2) primarily.
        q_stage1_mf[act1] += lr_pos * delta_stage1 

        # Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        
        if delta_stage2 >= 0:
            current_lr = lr_pos
        else:
            current_lr = lr_neg_eff
            
        q_stage2_mf[state_idx, act2] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter `lambda` which controls how much the Stage 1 choice is credited for the Stage 2 outcome. It hypothesizes that anxiety affects this credit assignment. High anxiety might lead to "over-attribution" (high lambda), connecting the final outcome strongly to the first choice, or conversely, a disconnect.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    TD(lambda) Model with Anxiety-Modulated Eligibility Trace.
    
    Hypothesis: Anxiety modulates the eligibility trace (lambda), 
    affecting how strongly the Stage 1 choice is updated based on 
    the final Stage 2 reward (direct reinforcement of Stage 1).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - lambda_base: [0, 1] Baseline eligibility trace decay.
    - lambda_mod: [-1, 1] Modulation of lambda by STAI.
    """
    learning_rate, beta, lambda_base, lambda_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    # Effective lambda
    lambda_eff = np.clip(lambda_base + lambda_mod * stai_score, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])

        # policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # TD(lambda) updates
        # 1. Prediction error at Stage 1 (State 1 -> State 2)
        delta1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        
        # 2. Prediction error at Stage 2 (State 2 -> Reward)
        delta2 = reward[trial] - q_stage2_mf[state_idx, act2]
        
        # Update Stage 2 Q-value (standard TD)
        q_stage2_mf[state_idx, act2] += learning_rate * delta2
        
        # Update Stage 1 Q-value
        # It gets the immediate Step 1 error, PLUS a portion of the Step 2 error
        # scaled by lambda. This allows the reward to propagate back to Stage 1 directly.
        q_stage1_mf[act1] += learning_rate * (delta1 + lambda_eff * delta2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```