Here are 3 new cognitive models exploring different mechanisms by which anxiety (STAI) might influence decision-making in the two-step task.

### Model 1: Anxiety-Driven Learning Rate Asymmetry
This model hypothesizes that anxiety biases how individuals learn from positive versus negative prediction errors. High anxiety individuals might over-attend to negative outcomes (punishment sensitivity) or fail to integrate positive outcomes effectively. Here, STAI modulates the learning rate specifically for negative prediction errors.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Dual Learning Rate Model with STAI-modulated Negative Learning.
    
    Hypothesis: Anxiety (STAI) specifically amplifies learning from negative prediction errors 
    (disappointments/omissions of reward), reflecting a negativity bias common in anxiety disorders.
    
    Parameters:
    alpha_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    stai_neg_amp: [0, 1] How much STAI amplifies the negative learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control (fixed, not modulated by STAI in this model).
    """
    alpha_pos, alpha_neg_base, stai_neg_amp, beta, w = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Calculate the effective negative learning rate based on anxiety
    # Higher anxiety -> higher learning rate from negative outcomes
    alpha_neg = alpha_neg_base + (stai_neg_amp * current_stai)
    alpha_neg = np.clip(alpha_neg, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        effective_alpha2 = alpha_pos if delta_stage2 > 0 else alpha_neg
        q_stage2_mf[s_idx, a2] += effective_alpha2 * delta_stage2
        
        # Stage 1 update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        effective_alpha1 = alpha_pos if delta_stage1 > 0 else alpha_neg
        q_stage1_mf[a1] += effective_alpha1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Exploratory Noise
This model tests the "anxiety as noise" hypothesis. Instead of affecting learning rates or planning weights directly, high anxiety might simply disrupt decision stability. Here, STAI increases the "temperature" of the softmax function (lowering `beta`), making choices more random or erratic, representing a difficulty in selecting the optimal action despite knowing values.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Inverse Temperature Modulation Model.
    
    Hypothesis: Higher anxiety reduces decision consistency (increases exploration/noise).
    Instead of a single beta, the effective beta is reduced by the STAI score.
    High anxiety -> Lower Beta -> More random choices.
    
    Parameters:
    alpha: [0, 1] Learning rate.
    beta_base: [0, 10] Baseline inverse temperature (for low anxiety).
    stai_noise: [0, 5] Factor by which STAI reduces beta (adds noise).
    w: [0, 1] Weight for Model-Based control.
    """
    alpha, beta_base, stai_noise, w = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Calculate effective beta. 
    # We subtract a term proportional to STAI to lower beta (increase noise)
    # We ensure beta doesn't go below 0.
    beta = beta_base - (stai_noise * current_stai)
    beta = np.maximum(beta, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        
        # Stage 1 update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Perseveration (Choice Stickiness)
This model investigates if anxiety leads to repetitive behavior or "stickiness." Anxious individuals might feel safer repeating previous choices regardless of the outcome (safety behaviors). Here, STAI modulates a perseveration parameter `p` which adds a bonus to the previously chosen action in the softmax calculation.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Perseveration Bonus Model with STAI modulation.
    
    Hypothesis: Anxiety increases 'stickiness' or perseveration. Anxious individuals
    may prefer repeating the last choice (safety behavior) to avoid uncertainty.
    STAI modulates the strength of the choice autocorrelation (stickiness).
    
    Parameters:
    alpha: [0, 1] Learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weight for Model-Based control.
    stai_stickiness: [0, 5] How much STAI adds a 'bonus' to the previously chosen option.
    """
    alpha, beta, w, stai_stickiness = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
    
    # Calculate perseveration bonus strength
    perseveration_weight = stai_stickiness * current_stai

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for stage 1 (initialize as None or handle first trial)
    last_action_1 = -1 

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add perseveration bonus to Q-values before softmax
        q_net_perseveration = q_net.copy()
        if last_action_1 != -1:
            q_net_perseveration[last_action_1] += perseveration_weight
        
        exp_q1 = np.exp(beta * q_net_perseveration)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        last_action_1 = a1 # Update for next trial

        # --- Stage 2 Choice ---
        # Note: We typically model stickiness primarily at the first stage choice in this task
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += alpha * delta_stage2
        
        # Stage 1 update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```