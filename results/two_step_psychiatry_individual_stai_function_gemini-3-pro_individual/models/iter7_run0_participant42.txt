Here are three new cognitive models implemented as Python functions, designed to explore mechanisms of anxiety distinct from those previously tested.

### Model 1: Anxiety-Modulated Reward Sensitivity (Anhedonia)
This model tests the hypothesis that anxiety reduces the subjective valuation of positive outcomes (anhedonia). Instead of affecting how *fast* the participant learns (learning rate) or how *random* they are (beta), this model proposes that anxiety scales down the effective magnitude of the reward signal itself.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated Reward Sensitivity (Anhedonia).
    
    Hypothesis: Anxiety reduces the subjective consumption value of the reward.
    While the objective reward is 0 or 1, anxious participants perceive the 
    magnitude of a '1' as smaller, leading to lower Q-values and potentially 
    weaker differentiation between options.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for value updates.
    - beta: [0, 10] Inverse temperature for choice consistency.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - reward_sens_base: [0, 2] Baseline sensitivity to reward (subjective value of 1 coin).
    - anxiety_dampening: [0, 2] The degree to which STAI reduces reward sensitivity.
    """
    learning_rate, beta, w, reward_sens_base, anxiety_dampening = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective subjective reward magnitude based on anxiety
    # Higher anxiety reduces the perceived value of the reward.
    # We clip to ensure value doesn't become negative.
    effective_reward_val = max(0.0, reward_sens_base - (participant_stai * anxiety_dampening))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        a2 = int(action_2[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        a1 = int(action_1[trial])
        
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        # Here we use the Subjective Reward Value instead of the raw binary reward
        perceived_reward = reward[trial] * effective_reward_val
        
        delta_stage2 = perceived_reward - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Anxiety Interference (Stage 1 Noise)
This model hypothesizes that anxiety specifically disrupts complex, planning-based decisions (Stage 1) while leaving simple stimulus-response decisions (Stage 2) relatively intact. This creates a "Stage 1 specific" noise parameter, rather than a global temperature.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Stage-Specific Anxiety Interference.
    
    Hypothesis: Anxiety acts as a cognitive load that specifically disrupts 
    the higher-level planning stage (Stage 1), increasing randomness (noise) 
    at that step, while the simpler Stage 2 choice remains stable.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - beta_stage2: [0, 10] Inverse temperature for Stage 2 (constant across anxiety).
    - beta_stage1_base: [0, 10] Baseline inverse temperature for Stage 1.
    - anxiety_beta1_penalty: [0, 5] Reduction in Stage 1 beta per unit of STAI.
    """
    learning_rate, w, beta_stage2, beta_stage1_base, anxiety_beta1_penalty = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Anxiety specifically degrades the precision of the first choice
    beta_stage1 = max(0.0, beta_stage1_base - (participant_stai * anxiety_beta1_penalty))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Use the anxiety-modulated beta for Stage 1
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        # Use the standard beta for Stage 2
        a2 = int(action_2[trial])
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        a1 = int(action_1[trial])
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Stage 1 Reactivity
Previous models explored modulating the Stage 2 learning rate. This model hypothesizes that anxiety increases the learning rate specifically for the *Stage 1 Model-Free* system. This reflects a "reactive" phenotype where anxious individuals rapidly update their top-level preferences based on the most recent outcome, ignoring the stability that usually comes from slow MF learning or MB planning.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Driven Stage 1 Reactivity.
    
    Hypothesis: Anxiety increases the learning rate specifically for the 
    Stage 1 Model-Free values. This causes the agent to be hyper-reactive 
    to the most recent trial outcome when making their next spaceship choice, 
    regardless of the transition structure.
    
    Parameters:
    - lr_stage2: [0, 1] Learning rate for the second stage (Aliens).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - lr_stage1_base: [0, 1] Baseline learning rate for Stage 1 MF values.
    - anxiety_lr1_slope: [0, 1] Increase in Stage 1 learning rate per unit of STAI.
    """
    lr_stage2, beta, w, lr_stage1_base, anxiety_lr1_slope = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate anxiety-modulated Stage 1 learning rate
    lr_stage1 = lr_stage1_base + (participant_stai * anxiety_lr1_slope)
    lr_stage1 = min(1.0, max(0.0, lr_stage1)) # Ensure bounds [0, 1]

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        a2 = int(action_2[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        a1 = int(action_1[trial])
        
        # Update Stage 1 using the modulated rate
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Update Stage 2 using the base rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```