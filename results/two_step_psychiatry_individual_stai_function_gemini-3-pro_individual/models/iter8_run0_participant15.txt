Here are 3 new cognitive models based on the provided template and constraints.

### Model 1: Anxiety-Distorted Transition Beliefs
This model hypothesizes that high anxiety leads to a "flattening" of the internal model of the world. While the real transition probabilities are 0.7/0.3, an anxious participant may perceive the world as more unpredictable (higher entropy), effectively distorting their Model-Based transition matrix towards 0.5/0.5.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Distorted Transition Model.
    
    Hypothesis: Higher anxiety creates uncertainty about the environment structure.
    The participant's internal model of the spaceship->planet transition probabilities
    becomes flatter (closer to 50/50) as anxiety increases, reducing the effectiveness
    of the Model-Based component.

    Parameters:
    learning_rate: [0, 1] Standard Q-learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between MB (1) and MF (0).
    transition_distortion: [0, 1] How much anxiety flattens the transition matrix.
                              0 = Participant believes true 0.7/0.3.
                              1 = High anxiety shifts belief towards 0.5/0.5.
    """
    learning_rate, beta, w, transition_distortion = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Normalize STAI to 0-1 range
    norm_stai = (stai_val - 20) / 60.0
    norm_stai = np.clip(norm_stai, 0, 1)

    # Calculate subjective transition probability
    # True probability is 0.7. Maximum distortion (0.5) implies subtracting 0.2.
    # distortion_amount scales with STAI and the parameter.
    distortion_magnitude = 0.2 * transition_distortion * norm_stai
    subjective_p_common = 0.7 - distortion_magnitude
    subjective_p_rare = 1.0 - subjective_p_common
    
    # Subjective transition matrix based on anxiety
    transition_matrix = np.array([
        [subjective_p_common, subjective_p_rare], 
        [subjective_p_rare, subjective_p_common]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based Value Calculation using the DISTORTED matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updating ---
        r = reward[trial]
        
        # Update Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Dampened Subjective Reward
This model proposes that anxiety induces a form of anhedonia or reward dampening. Instead of modulating the *learning rate* (how fast they update), this model modulates the *asymptotic value* of the reward itself. An anxious person perceives a coin worth "1.0" as being worth less, leading to systematically lower Q-values.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Subjective Reward Valuation Model.
    
    Hypothesis: Anxiety reduces the subjective valuation of the reward (anhedonia).
    The 'effective' reward received is scaled down by the STAI score.
    This differs from learning rate modulation as it affects the magnitude 
    of the Q-values, not just the speed of acquisition.

    Parameters:
    learning_rate: [0, 1] Standard Q-learning rate.
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between MB (1) and MF (0).
    reward_dampening: [0, 1] Sensitivity of reward devaluation to anxiety.
                             If 0, reward is 1.0. 
                             If 1 and high anxiety, reward approaches 0.5.
    """
    learning_rate, beta, w, reward_dampening = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    norm_stai = (stai_val - 20) / 60.0
    norm_stai = np.clip(norm_stai, 0, 1)

    # Transition matrix is fixed in this model
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updating ---
        # Calculate Subjective Reward
        # We cap the dampening so the reward doesn't go to 0 or negative.
        # Max dampening reduces reward to 50% of objective value.
        subjective_r = reward[trial] * (1.0 - (0.5 * reward_dampening * norm_stai))
        
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Standard temporal difference updates, but using subjective_r
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = subjective_r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Stage-Specific Reactivity (Proximal vs Distal)
This model separates learning rates for Stage 1 (Spaceships) and Stage 2 (Aliens). It hypothesizes that anxiety specifically increases reactivity to immediate outcomes (Stage 2 learning), while leaving the abstract Stage 1 learning baseline. This reflects a "hyper-reactivity" to the most recent, concrete stimulus.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Stage-Specific Reactivity Model.
    
    Hypothesis: Anxiety increases the learning rate specifically for the 
    immediate, proximal outcome (Stage 2/Aliens), reflecting hyper-reactivity 
    to concrete feedback. Stage 1 (Spaceships) learning remains at a baseline.
    
    Parameters:
    lr_stage1: [0, 1] Fixed learning rate for the first stage (Spaceships).
    beta: [0, 10] Inverse temperature.
    w: [0, 1] Weighting between MB (1) and MF (0).
    lr_stage2_base: [0, 1] Baseline learning rate for Stage 2.
    stai_s2_reactivity: [0, 1] How much STAI increases Stage 2 learning rate.
    """
    lr_stage1, beta, w, lr_stage2_base, stai_s2_reactivity = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    norm_stai = (stai_val - 20) / 60.0
    norm_stai = np.clip(norm_stai, 0, 1)

    # Calculate Anxiety-Modulated Stage 2 Learning Rate
    # We clip the result to ensure it stays valid [0,1]
    lr_stage2 = lr_stage2_base + (stai_s2_reactivity * norm_stai)
    lr_stage2 = np.clip(lr_stage2, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        # --- Updating ---
        r = reward[trial]
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])

        # Update Stage 1 using fixed lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        
        # Update Stage 2 using anxiety-modulated lr_stage2
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```