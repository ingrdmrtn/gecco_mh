Here are 3 new cognitive models that introduce mechanisms distinct from the previously tried combinations.

### Model 1: Asymmetric Valence Learning (Anxiety-Driven Punishment Sensitivity)
This model hypothesizes that STAI (anxiety) specifically modulates how strongly participants learn from negative prediction errors (punishments/omissions) compared to positive ones. High anxiety is often associated with a hypersensitivity to negative feedback.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Asymmetric Valence Learning model.
    
    Hypothesis: Anxiety (STAI) specifically increases the learning rate for 
    negative prediction errors (disappointments), while the learning rate for 
    positive errors is fixed. This reflects a 'negativity bias' in updating values.

    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Baseline learning rate for negative prediction errors.
    - lr_neg_stai_slope: [0, 1] How much STAI increases the negative learning rate.
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w: [0, 1] Weighting parameter (0=Model-Free, 1=Model-Based).
    """
    lr_pos, lr_neg_base, lr_neg_stai_slope, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate the anxiety-modulated negative learning rate
    # Clip to ensure it doesn't exceed 1.0
    lr_neg = min(1.0, lr_neg_base + (lr_neg_stai_slope * stai_val))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        current_lr_s1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        current_lr_s2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Eligibility Trace (TD-Lambda)
This model focuses on credit assignment. The standard "Model-Free" logic updates Stage 1 values based on the Stage 2 value (TD-0). A "Monte Carlo" approach updates Stage 1 based on the final Reward (TD-1). This model proposes that anxiety affects $\lambda$ (lambda), the eligibility trace parameter. High anxiety might cause "ruminative" credit assignment (higher lambda, linking distal rewards directly to choices) or cognitive fragmentation (lower lambda).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    TD-Lambda Model with Anxiety-Modulated Eligibility Trace.
    
    Hypothesis: STAI modulates the eligibility trace parameter (lambda). 
    This controls how much the final reward directly updates the Stage 1 choice, 
    bypassing the Stage 2 value estimate.
    
    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - lambda_base: [0, 1] Baseline eligibility trace.
    - lambda_stai_slope: [-1, 1] Effect of STAI on lambda. 
      (If positive, anxiety increases direct reward association; if negative, decreases it).
    """
    learning_rate, beta, w, lambda_base, lambda_stai_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate Lambda specific to this participant
    # We use a sigmoid-like clipping to keep it between 0 and 1
    lambda_param = lambda_base + (lambda_stai_slope * stai_val)
    lambda_param = np.clip(lambda_param, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Standard Stage 2 update
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 update with Eligibility Trace (Lambda)
        # Update = alpha * (delta1 + lambda * delta2)
        combined_update = learning_rate * (delta_stage1 + (lambda_param * delta_stage2))
        q_stage1_mf[action_1[trial]] += combined_update

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Distorted Model-Based Transition (Anxiety-Induced Uncertainty)
This model assumes the "Model-Free" system is intact, but the "Model-Based" system is corrupted by anxiety. Specifically, anxious individuals may have a distorted internal model of the transition probabilities. Instead of trusting the 70/30 split, high anxiety leads to a "flatter" (higher entropy) belief about transitions, effectively reducing the utility of Model-Based planning.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Distorted Transition Model.
    
    Hypothesis: Anxiety (STAI) degrades the accuracy of the internal model used 
    for Model-Based planning. While the true transition probability is 0.7, 
    anxious participants perceive it as closer to 0.5 (random), reflecting 
    uncertainty or lack of confidence in the environment's structure.

    Parameters:
    - learning_rate: [0, 1] Value update rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - distortion_stai: [0, 0.4] How much STAI flattens the transition matrix.
      (0 = perfect knowledge [0.7], high = max uncertainty [towards 0.5]).
    """
    learning_rate, beta, w, distortion_stai = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate perceived transition probability
    # True is 0.7. distortion reduces this towards 0.5.
    # We clip distortion effect so p_trans doesn't drop below 0.5 (reversal).
    distortion_effect = distortion_stai * stai_val
    p_trans = 0.7 - distortion_effect
    p_trans = max(0.5, p_trans) 
    
    # The anxious participant's internal model
    perceived_transition_matrix = np.array([[p_trans, 1-p_trans], [1-p_trans, p_trans]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the distorted matrix for MB calculation
        q_stage1_mb = perceived_transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```