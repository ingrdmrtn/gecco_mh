Here are three cognitive models expressed as Python functions that incorporate STAI scores into the decision-making process.

### Model 1: Counterfactual Rumination
This model hypothesizes that high anxiety (STAI) leads to increased "counterfactual updating" or rumination. Anxious participants do not just learn from what happened; they also obsessively update the value of the option they *didn't* choose, assuming the opposite outcome occurred (e.g., "If I failed here, the other one must have been a winner").

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    RL Model with STAI-modulated Counterfactual Updating (Rumination).
    
    Hypothesis: High anxiety increases the weight of counterfactual learning.
    When a participant receives a reward (or lack thereof), they update the 
    unchosen Stage 1 option towards the opposite outcome, reflecting a 
    "grass is greener" or regret-driven mindset common in anxiety.

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated for chosen actions.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weight for model-based control (0=MF, 1=MB).
    cf_base: [0, 1] - Baseline weight for counterfactual updating.
    stai_cf_slope: [0, 2] - How strongly STAI increases counterfactual updating.
    """
    learning_rate, beta, w, cf_base, stai_cf_slope = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Calculate effective counterfactual weight based on STAI
    # We clip to ensure it doesn't exceed reasonable bounds (e.g., 1.0)
    cf_weight = cf_base + (stai_cf_slope * current_stai)
    if cf_weight > 1.0: cf_weight = 1.0
    if cf_weight < 0.0: cf_weight = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2) # Values for Spaceships A and U
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens on Planet X and Y

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        act1 = int(action_1[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        act2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # 1. Standard TD Update for Chosen Path
        # Note: Standard 2-step usually updates Stage 1 MF based on Stage 2 value (TD(0)) 
        # or Stage 2 Reward (TD(1)/Lambda). Here we use a simple TD(1)-like structure 
        # consistent with the template's implication of direct reward propagation.
        
        # Update Stage 1 MF (Chosen)
        # Using the direct reward for simplicity in this specific template logic
        delta_stage1_chosen = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1_chosen
        
        # 2. Counterfactual Update for Unchosen Path
        # If I got r, assume unchosen would have given (1-r)
        act1_unchosen = 1 - act1
        r_cf = 1.0 - r
        # We assume the unchosen path leads to the 'expected' value of the unchosen spaceship
        # simplified here as updating the Q-value directly towards the counterfactual reward.
        delta_stage1_unchosen = r_cf - q_stage1_mf[act1_unchosen]
        q_stage1_mf[act1_unchosen] += learning_rate * cf_weight * delta_stage1_unchosen

        # Update Stage 2 MF
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Distorted Transition Beliefs
This model proposes that anxiety distorts the internal model of the environment. High STAI participants perceive the transition probabilities as more uncertain (higher entropy) than they really are, effectively "flattening" the Model-Based transition matrix from 70/30 towards 50/50.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    RL Model with STAI-modulated Transition Beliefs (Uncertainty).
    
    Hypothesis: High anxiety participants distrust the stability of the 
    transition structure. They perceive the 'common' transition as less 
    likely and the 'rare' transition as more likely, flattening the 
    Model-Based transition matrix towards chance (0.5/0.5).

    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice rule.
    w: [0, 1] - Weight for model-based control.
    distortion_factor: [0, 1] - How much STAI flattens the transition matrix.
    """
    learning_rate, beta, w, distortion_factor = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Calculate subjective transition probabilities
    # Base is 0.7. Max distortion (factor=1, stai=1) reduces it to 0.5.
    # The 0.2 scalar ensures that at max distortion, 0.7 - 0.2*1 = 0.5.
    p_common = 0.7 - (0.2 * distortion_factor * current_stai)
    if p_common < 0.5: p_common = 0.5 # Floor at random chance
    p_rare = 1.0 - p_common
    
    # Subjective Transition Matrix used for MB planning
    # Note: The *actual* world doesn't change, only the agent's MB calculation.
    subj_transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use SUBJECTIVE matrix for MB valuation
        q_stage1_mb = subj_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        act1 = int(action_1[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hyper-vigilance (Stage 2 Learning Rate)
This model posits that anxiety manifests as hyper-vigilance to immediate outcomes. While the overall planning strategy (beta, w) might be stable, the rate at which participants update their beliefs about the immediate reward sources (the aliens in Stage 2) is accelerated by anxiety.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    RL Model with STAI-modulated Stage 2 Learning Rate (Hyper-vigilance).
    
    Hypothesis: Anxiety causes hyper-vigilance to local, immediate outcomes.
    Participants with high STAI have a higher learning rate specifically for 
    the Second Stage (Aliens), making their value estimation for the aliens 
    more volatile and sensitive to recent rewards, distinct from their 
    Stage 1 learning.

    Parameters:
    lr_stage1: [0, 1] - Learning rate for Stage 1 (Spaceships).
    lr_stage2_base: [0, 1] - Baseline learning rate for Stage 2 (Aliens).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Model-based weight.
    stai_lr2_slope: [0, 1] - How much STAI increases the Stage 2 learning rate.
    """
    lr_stage1, lr_stage2_base, beta, w, stai_lr2_slope = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Calculate Stage 2 specific learning rate
    lr_stage2 = lr_stage2_base + (stai_lr2_slope * current_stai)
    if lr_stage2 > 1.0: lr_stage2 = 1.0
    if lr_stage2 < 0.0: lr_stage2 = 0.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]

        act1 = int(action_1[trial])
        act2 = int(action_2[trial])
        r = reward[trial]

        # --- Updates ---
        # Update Stage 1 using lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += lr_stage1 * delta_stage1

        # Update Stage 2 using STAI-modulated lr_stage2
        delta_stage2 = r - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```