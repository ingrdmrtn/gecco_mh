Here are three new cognitive models, expressed as Python functions, that incorporate the STAI anxiety score into the decision-making process.

### Model 1: STAI-Modulated Asymmetric Learning Rates
This model hypothesizes that anxiety creates a bias in how prediction errors are processed. Specifically, it proposes that anxious individuals learn more rapidly from negative outcomes (punishments/omissions) than from positive ones. The model splits the learning rate into a positive learning rate (base) and a negative learning rate, where the latter is scaled by the STAI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Asymmetric Learning Rates.

    This model hypothesizes that anxiety (STAI) specifically modulates learning 
    from negative prediction errors (punishment/loss) differently than positive ones.
    High anxiety individuals may update their value estimates more drastically 
    following worse-than-expected outcomes.

    Parameters:
    lr_pos: [0,1] - Base learning rate for positive prediction errors.
    beta: [0,10] - Inverse temperature (exploration/exploitation).
    w: [0,1] - Mixing weight (0=Model-Free, 1=Model-Based).
    lr_neg_scale: [0,5] - Scaling factor for negative learning rate based on STAI.
                          lr_neg = lr_pos * (1 + lr_neg_scale * STAI).
    """
    lr_pos, beta, w, lr_neg_scale = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    # Calculate negative learning rate based on STAI
    # If scale is 0, symmetric learning. If scale > 0, anxious people learn faster from neg PEs.
    lr_neg = lr_pos * (1.0 + lr_neg_scale * stai_val)
    # Clip to ensure stability within typical bounds
    if lr_neg > 1.0: lr_neg = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        # Use asymmetric learning rate
        alpha2 = lr_pos if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, a2] += alpha2 * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        alpha1 = lr_pos if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[a1] += alpha1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: STAI-Modulated Value Decay
This model proposes that anxiety leads to "active forgetting" or signal decay. High anxiety imposes a cognitive load that makes it harder to maintain value representations for options that are not currently being attended to (unchosen options). In this model, STAI controls the rate at which unchosen Q-values decay toward zero on every trial.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Value Decay (Forgetting).

    This model proposes that anxiety leads to faster decay (forgetting) of 
    values for unchosen options. This reflects the cognitive load or 
    attentional narrowing associated with anxiety, where representations 
    of non-current options degrade over time.

    Parameters:
    learning_rate: [0,1] - Learning rate for chosen options.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weight.
    decay_slope: [0,1] - Sensitivity of decay rate to STAI.
                         decay_rate = decay_slope * STAI.
    """
    learning_rate, beta, w, decay_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    decay_rate = decay_slope * stai_val
    if decay_rate > 1.0: decay_rate = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates and Decay ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay unchosen Stage 2 action in the current state
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay unchosen Stage 1 action
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: STAI-Modulated Model-Based Pessimism
This model incorporates anxiety into the planning phase (Model-Based control). Standard MB models assume the agent will make the optimal choice at the second stage (using the `max` operator). This model suggests anxious agents are pessimistic: when evaluating a planet, they consider a mixture of the best-case (`max`) and worst-case (`min`) outcomes. The weight of the worst-case scenario increases with STAI, essentially modeling risk aversion in the planning tree.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    STAI-Modulated Model-Based Pessimism.

    This model hypothesizes that anxiety influences the Model-Based valuation 
    of future states. Instead of assuming the agent will take the best action 
    at the second stage (max Q), anxious agents mix the max Q and min Q 
    weighted by their anxiety (pessimism). They worry they might make the wrong choice
    or that the environment is malevolent.

    Parameters:
    learning_rate: [0,1] - Learning rate.
    beta: [0,10] - Inverse temperature.
    w: [0,1] - MB/MF weight.
    pess_slope: [0,2] - Scaling of pessimism by STAI. 
                        theta = pess_slope * STAI (clipped [0,1]).
                        V(s') = (1-theta)*max(Q) + theta*min(Q).
    """
    learning_rate, beta, w, pess_slope = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]
    
    theta = pess_slope * stai_val
    if theta > 1.0: theta = 1.0
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 ---
        # Calculate Pessimistic Values for Stage 2 states
        max_vals = np.max(q_stage2_mf, axis=1)
        min_vals = np.min(q_stage2_mf, axis=1)
        
        # Valuation of the state is a mixture of best and worst case based on STAI
        v_stage2 = (1.0 - theta) * max_vals + theta * min_vals
        
        # MB calculation uses the pessimistic state values
        q_stage1_mb = transition_matrix @ v_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])
        
        # --- Stage 2 ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        a2 = int(action_2[trial])
        r = reward[trial]
        
        # --- Updates ---
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```