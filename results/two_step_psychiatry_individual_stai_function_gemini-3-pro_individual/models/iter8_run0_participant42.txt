Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process.

### Model 1: Anxiety-Modulated Eligibility Trace (Lambda)
This model hypothesizes that anxiety affects how efficiently credit is assigned to the first-stage choice based on the final outcome. A higher "eligibility trace" (lambda) allows the second-stage reward prediction error to directly update the first-stage values, effectively bridging the gap between action and distal reward. Anxiety is modeled to modulate this parameter, potentially representing a "reactive" mode (high lambda) where distant outcomes are immediately attributed to initial choices, bypassing the state-step structure.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Modulated Eligibility Trace (Lambda).
    
    Hypothesis: Anxiety modulates the 'eligibility trace' (lambda) parameter. 
    This parameter controls how much the Stage 2 Reward Prediction Error (RPE) 
    directly updates the Stage 1 Q-value. High anxiety might lead to 
    stronger direct reinforcement (higher lambda), bypassing the model-based 
    state structure.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for Q-updating.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight for Model-Based (1) vs Model-Free (0).
    - lambda_base: [0, 1] Baseline eligibility trace.
    - anxiety_lambda_mod: [0, 1] How much STAI modifies lambda (can be positive or negative).
      (We assume lambda stays within [0,1] via clipping).
    """
    learning_rate, beta, w, lambda_base, anxiety_lambda_mod = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate effective lambda based on anxiety
    lambda_eff = lambda_base + (participant_stai * anxiety_lambda_mod)
    # Clip lambda to valid range [0, 1]
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updating ---
        # Calculate Stage 2 RPE (needed for eligibility trace update of Stage 1)
        rpe_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Stage 1 Update: Includes standard TD(0) part + Eligibility Trace part
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_eff * rpe_stage2)

        # Stage 2 Update
        delta_stage2 = rpe_stage2 # same as calculated above
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Transition Uncertainty
This model proposes that high anxiety degrades the precision of the Model-Based system. Instead of using the objective transition matrix (70/30), anxious participants may perceive the world as more uncertain or volatile. The model blends the true transition matrix with a "flat" (maximum entropy) matrix (50/50), where the degree of blending is determined by the STAI score. This reflects an "intolerance of uncertainty" or a failure to learn the structure confidently.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Induced Transition Uncertainty.
    
    Hypothesis: Anxiety degrades the internal model of the environment structure.
    Highly anxious individuals may act as if the transition probabilities are 
    flatter (closer to 50/50) than they really are, representing structural 
    uncertainty.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for MB vs MF.
    - uncertainty_base: [0, 1] Baseline uncertainty about transitions.
    - anxiety_unc_slope: [0, 1] How much STAI increases transition uncertainty.
    """
    learning_rate, beta, w, uncertainty_base, anxiety_unc_slope = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    # True transition matrix
    t_true = np.array([[0.7, 0.3], [0.3, 0.7]])
    # Flat / Uncertain matrix
    t_flat = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    # Calculate subjective uncertainty
    uncertainty = uncertainty_base + (participant_stai * anxiety_unc_slope)
    uncertainty = np.clip(uncertainty, 0.0, 1.0)
    
    # The transition matrix used for MB calculations is a blend
    t_subjective = (1.0 - uncertainty) * t_true + (uncertainty * t_flat)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use subjective matrix for MB valuation
        q_stage1_mb = t_subjective @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Win-Stay Heuristic
This model posits that anxiety increases reliance on simple, heuristic decision strategies like "Win-Stay" (repeating a rewarded choice), which operate independently of the reinforcement learning value calculations. This is modeled as a value bonus added to the previously chosen option if it resulted in a reward. This differs from simple stickiness (which repeats regardless of reward) and standard RL (which integrates history).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Modulated Win-Stay Heuristic.
    
    Hypothesis: Anxiety promotes the use of simple heuristics over complex 
    value integration. Specifically, anxiety increases the 'Win-Stay' bias: 
    a tendency to immediately repeat the previous Stage 1 choice if it was 
    rewarded, implemented as a temporary bonus to the Q-value.
    
    Parameters:
    - learning_rate: [0, 1] Rate of updating Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight for MB vs MF.
    - wsls_base: [0, 5] Baseline Win-Stay bonus.
    - anxiety_wsls_mod: [0, 5] How much STAI increases the Win-Stay bonus.
    """
    learning_rate, beta, w, wsls_base, anxiety_wsls_mod = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective Win-Stay bonus
    wsls_bonus = wsls_base + (participant_stai * anxiety_wsls_mod)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Win-Stay Bonus
        if trial > 0:
            prev_reward = reward[trial-1]
            if prev_reward == 1.0:
                prev_a1 = int(action_1[trial-1])
                q_net[prev_a1] += wsls_bonus
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```