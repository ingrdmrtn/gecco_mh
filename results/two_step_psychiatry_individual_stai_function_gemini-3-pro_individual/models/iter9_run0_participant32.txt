Here are three new cognitive models based on the two-step task, incorporating the STAI anxiety score into the learning and decision-making mechanisms.

### Model 1: Anxiety-Modulated Transition Belief (Distorted MB)
This model hypothesizes that anxiety distorts the internal model of the world. While the true transition probability is 0.7, anxious individuals may perceive the world as more unpredictable (higher entropy) or pessimistic, effectively "flattening" the transition matrix used for Model-Based calculations.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Transition Belief.
    
    Hypothesis: High anxiety distorts the agent's internal model of the transition 
    probabilities. While the real world is 70/30, anxious participants may effectively 
    operate as if the transitions are more random (closer to 50/50) due to uncertainty 
    or catastrophic worry, reducing the efficacy of the Model-Based system.
    
    Parameters:
    - learning_rate: Standard Q-learning update rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Weighting for model-based control [0, 1]
    - belief_distortion: How much anxiety reduces the perceived common transition probability [0, 0.4]
      (If 0, belief is 0.7. If high, belief approaches 0.5 or lower).
    """
    learning_rate, beta, w, belief_distortion = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Modulate belief: The perceived probability of the common transition
    # drops from 0.7 as anxiety increases.
    # We clip to ensure it doesn't go below 0 (though 0.3 is the flip point).
    perceived_common_prob = 0.7 - (belief_distortion * stai_val)
    perceived_common_prob = np.maximum(perceived_common_prob, 0.0)
    perceived_rare_prob = 1.0 - perceived_common_prob
    
    # Internal model used for MB calculation
    transition_matrix = np.array([
        [perceived_common_prob, perceived_rare_prob], 
        [perceived_rare_prob, perceived_common_prob]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Standard Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Standard Stage 1 update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Dependent Temporal Credit Assignment
This model separates the learning rates for Stage 1 and Stage 2. It hypothesizes that anxiety specifically disrupts the ability to back-propagate value from the outcome (Stage 2) to the initial choice (Stage 1). Anxious individuals may react strongly to the immediate reward (Stage 2) but fail to update the planning stage (Stage 1) effectively.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Dependent Temporal Credit Assignment.
    
    Hypothesis: Anxiety affects temporal credit assignment. While the agent learns 
    about the immediate aliens (Stage 2) normally, anxiety acts as a distraction 
    or noise that suppresses the back-propagation of this information to the 
    spaceships (Stage 1).
    
    Parameters:
    - lr_stage2: Base learning rate for the second stage (aliens) [0, 1]
    - lr_stage1_suppression: How much anxiety reduces the learning rate for Stage 1 [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Weighting for model-based control [0, 1]
    """
    lr_stage2, lr_stage1_suppression, beta, w = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Calculate anxiety-modulated Stage 1 learning rate
    # High anxiety reduces lr_stage1 relative to lr_stage2
    lr_stage1 = lr_stage2 * (1.0 - (lr_stage1_suppression * stai_val))
    lr_stage1 = np.maximum(lr_stage1, 0.0) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Update Stage 2 using the base rate
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2

        # Update Stage 1 using the anxiety-suppressed rate
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Stage 2 Perseveration
Previous models looked at stickiness (perseveration) at the first stage (spaceship choice). This model hypothesizes that anxiety drives rigid behavior at the *second* stage (alien choice). Anxious individuals may find a "safe" alien and stick to it, regardless of the fluctuating reward probabilities.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Stage 2 Perseveration.
    
    Hypothesis: Anxiety increases 'stickiness' specifically at the second stage 
    (alien choice). Anxious participants may latch onto a specific alien 
    within a planet and repeat that choice to minimize decision conflict, 
    independent of the reward received.
    
    Parameters:
    - learning_rate: Update rate [0, 1]
    - beta: Inverse temperature [0, 10]
    - w: Weighting for model-based control [0, 1]
    - stick_s2_base: Baseline tendency to repeat Stage 2 choice [0, 5]
    - stick_s2_anxiety: Additional Stage 2 stickiness due to STAI [0, 5]
    """
    learning_rate, beta, w, stick_s2_base, stick_s2_anxiety = model_parameters
    n_trials = len(action_1)
    stai_val = stai[0]

    # Total stickiness bonus for Stage 2
    stickiness_s2 = stick_s2_base + (stick_s2_anxiety * stai_val)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track last action taken in each state (Planet X and Planet Y)
    # Initialize with -1 (no previous action)
    last_action_s2 = np.array([-1, -1]) 

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        # Apply stickiness bonus to Q-values temporarily for choice calculation
        current_q_s2 = q_stage2_mf[state_idx].copy()
        
        if last_action_s2[state_idx] != -1:
            current_q_s2[last_action_s2[state_idx]] += stickiness_s2

        exp_q2 = np.exp(beta * current_q_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # Update tracker
        last_action_s2[state_idx] = action_2[trial]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```