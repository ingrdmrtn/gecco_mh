Here are three new cognitive models implemented as Python functions.

### Model 1: Anxiety-Driven Safety Bias
This model hypothesizes that anxiety drives a preference for the "safe" or "default" option (Spaceship A / Action 0), which has the most reliable transition to Planet X. This bias is independent of the actual reward value and represents a desire for predictability. The bias is added directly to the net Q-value of Action 0, scaled by the participant's STAI score.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-Driven Safety Bias.
    
    Hypothesis: Higher anxiety leads to a non-reward-based preference for the 
    "Common" spaceship (Action 0), representing a bias towards perceived safety 
    or predictability.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Weight for Model-Based (1) vs Model-Free (0).
    - safety_bias_scale: [0, 5] Magnitude of bias towards Action 0, scaled by STAI.
    """
    learning_rate, beta, w, safety_bias_scale = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate the effective bias for this participant
    safety_bias = participant_stai * safety_bias_scale

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Anxiety-Driven Safety Bias to Action 0
        q_net[0] += safety_bias

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Stage 1 "Choking"
This model hypothesizes that anxiety specifically degrades decision-making quality during the complex planning phase (Stage 1), but not the simpler reaction phase (Stage 2). High anxiety acts as a "choking" mechanism that increases noise (lowers beta) specifically for the first choice, while the second choice remains consistent.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Anxiety-Induced Stage 1 "Choking".
    
    Hypothesis: Anxiety consumes cognitive resources required for the complex 
    Stage 1 decision, resulting in noisier choices (lower beta) at Stage 1, 
    while Stage 2 (simple bandit) remains unaffected.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weight for Model-Based (1) vs Model-Free (0).
    - anxiety_s1_dampening: [0, 5] How much STAI reduces beta for Stage 1.
    """
    learning_rate, beta_base, w, anxiety_s1_dampening = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate stage-specific betas
    # Stage 1 beta is dampened by anxiety (choking effect)
    beta_s1 = beta_base / (1.0 + (participant_stai * anxiety_s1_dampening))
    # Stage 2 beta is the baseline
    beta_s2 = beta_base

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        exp_q1 = np.exp(beta_s1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta_s2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Rare Transition Avoidance
This model suggests that anxious individuals have a specific sensitivity to "Rare" transitions (prediction errors in the transition structure). If a Rare transition occurs, they perceive it as a failure or a threat, leading to an immediate "startle" penalty applied to that action in the subsequent trial, discouraging them from repeating the choice that led to the unexpected state.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Anxiety-Driven Rare Transition Avoidance.
    
    Hypothesis: Anxious individuals interpret Rare transitions as negative events 
    or 'errors' regardless of the reward outcome. If a trial involved a Rare 
    transition, a penalty is applied to that action in the next trial.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based (1) vs Model-Free (0).
    - rare_penalty_scale: [0, 5] Magnitude of penalty applied after rare transition.
    """
    learning_rate, beta, w, rare_penalty_scale = model_parameters
    n_trials = len(action_1)
    participant_stai = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    effective_penalty = participant_stai * rare_penalty_scale
    
    # Track previous trial info for the penalty logic
    prev_a1 = -1
    prev_state = -1

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)

        # Apply penalty if previous transition was Rare
        # Common transitions: (Action 0 -> State 0) or (Action 1 -> State 1)
        # Rare transitions: (Action 0 -> State 1) or (Action 1 -> State 0)
        if trial > 0:
            is_rare = (prev_a1 == 0 and prev_state == 1) or (prev_a1 == 1 and prev_state == 0)
            if is_rare:
                q_net[prev_a1] -= effective_penalty

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Action value updating
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        
        # Store for next trial
        prev_a1 = int(action_1[trial])
        prev_state = state_idx

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```