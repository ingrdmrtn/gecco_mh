Here are 3 new cognitive models exploring different mechanisms for how anxiety (STAI) might influence decision-making in the two-step task.

### Model 1: Anxiety-Modulated Learning Rate Asymmetry
This model tests the hypothesis that anxiety alters how participants learn from positive versus negative prediction errors. Specifically, high anxiety might lead to increased sensitivity to negative outcomes (punishment sensitivity) or decreased sensitivity to positive outcomes (anhedonia).

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Learning Asymmetry.
    
    Hypothesis: Anxiety (STAI) modulates the learning rate specifically for 
    negative prediction errors. High anxiety individuals might over-weight 
    worse-than-expected outcomes (negative RPEs) compared to better-than-expected ones.
    
    Parameters:
    - lr_base: [0, 1] Base learning rate for positive prediction errors.
    - lr_neg_mod: [0, 5] Multiplier for negative prediction errors, scaled by STAI.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for Model-Based control.
    """
    lr_base, lr_neg_mod, beta, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        # Calculate effective learning rates based on RPE sign
        # We calculate the RPE first to decide which LR to use
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        
        # If RPE is negative, boost learning rate by anxiety score
        if delta_stage2 < 0:
            current_lr = lr_base * (1 + lr_neg_mod * stai_score)
            # Clip to ensure valid range [0, 1]
            current_lr = min(max(current_lr, 0), 1)
        else:
            current_lr = lr_base

        # Update Stage 2
        q_stage2_mf[state_idx, act2] += current_lr * delta_stage2
        
        # Update Stage 1 (using the same logic for the stage 1 update)
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        
        if delta_stage1 < 0:
            current_lr_s1 = lr_base * (1 + lr_neg_mod * stai_score)
            current_lr_s1 = min(max(current_lr_s1, 0), 1)
        else:
            current_lr_s1 = lr_base
            
        q_stage1_mf[act1] += current_lr_s1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Forgetting
This model investigates whether anxiety affects memory retention. High anxiety might be associated with faster decay of learned values (forgetting) due to cognitive load or interference, leading to less stable value representations over time.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Forgetting (Decay).
    
    Hypothesis: High anxiety increases the rate at which Q-values decay 
    toward zero (forgetting) on unchosen options.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - decay_base: [0, 1] Base decay rate.
    - decay_stai: [0, 1] Additional decay scaled by STAI score.
    """
    learning_rate, beta, w, decay_base, decay_stai = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate total decay rate
    decay_rate = decay_base + (decay_stai * stai_score)
    # Ensure decay rate is within [0, 1]
    decay_rate = min(max(decay_rate, 0.0), 1.0)

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1
        
        # Decay unchosen stage 1 option
        unchosen_1 = 1 - act1
        q_stage1_mf[unchosen_1] *= (1 - decay_rate)

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        
        # Decay unchosen stage 2 option (in the current state)
        unchosen_2 = 1 - act2
        q_stage2_mf[state_idx, unchosen_2] *= (1 - decay_rate)
        # Note: We typically don't decay the unvisited state in standard models, 
        # but could decay all unchosen if desired. Here we decay unchosen in current state.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Exploration (Inverse Temperature Modulation)
This model posits that anxiety disrupts the precision of choices (decision noise). Instead of changing how values are learned or weighted (w), anxiety changes the `beta` parameter, making choices more random (high exploration/noise) or more deterministic (low exploration).

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF model with Anxiety-Modulated Inverse Temperature (Beta).
    
    Hypothesis: Anxiety affects the exploration-exploitation balance. 
    High anxiety might lead to more erratic (noisier) choices (lower beta)
    or more rigid choices (higher beta).
    
    Parameters:
    - learning_rate: [0, 1] Update rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_stai_slope: [0, 10] How much STAI modifies beta.
    - w: [0, 1] Weight for Model-Based control.
    """
    learning_rate, beta_base, beta_stai_slope, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Calculate effective beta
    # We allow beta to decrease (more noise) or increase (more rigid) based on the sign of the slope.
    # However, since bounds are usually positive in optimization, let's assume a subtractive model 
    # to test the "anxiety causes noise" hypothesis, or additive. 
    # Let's model it as: beta = base / (1 + slope * stai) 
    # This implies high anxiety -> lower beta (more noise/exploration).
    
    effective_beta = beta_base / (1.0 + beta_stai_slope * stai_score)
    # Clip to avoid extremely small betas
    effective_beta = max(effective_beta, 0.01)

    for trial in range(n_trials):
        act1 = int(action_1[trial])
        state_idx = int(state[trial])
        act2 = int(action_2[trial])

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(effective_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[act1]

        exp_q2 = np.exp(effective_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[act2]
  
        delta_stage1 = q_stage2_mf[state_idx, act2] - q_stage1_mf[act1]
        q_stage1_mf[act1] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, act2]
        q_stage2_mf[state_idx, act2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```