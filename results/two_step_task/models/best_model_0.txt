def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with dynamic associability and choice kernels.
    
    This model learns solely from experienced outcomes, but adapts its learning
    rates via Pearceâ€“Hall-like associability that tracks recent surprise (|TD error|).
    It also includes stage-specific choice kernels that capture short-term
    choice recency effects.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha0, phi, beta, k1, k2]
        - alpha0 ([0,1]): Baseline learning-rate gain.
        - phi ([0,1]): Associability update rate (higher -> faster tracking of surprise).
                        Also used as decay for choice kernels.
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - k1 ([0,1]): Strength of stage-1 choice kernel bias in logits.
        - k2 ([0,1]): Strength of stage-2 choice kernel bias in logits.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha0, phi, beta, k1, k2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    A1 = np.zeros(2)
    A2 = np.zeros((2, 2))

    CK1 = np.zeros(2)
    CK2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]

        logits1 = beta * q1 + k1 * CK1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        probs_1 = soft1 / np.sum(soft1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        logits2 = beta * q2[s] + k2 * CK2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        probs_2 = soft2 / np.sum(soft2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]


        delta2 = r - q2[s, a2]
        A2[s, a2] = (1.0 - phi) * A2[s, a2] + phi * abs(delta2)
        alpha2_eff = np.clip(alpha0 * A2[s, a2], 0.0, 1.0)
        q2[s, a2] += alpha2_eff * delta2

        td_target1 = q2[s, a2]
        delta1 = td_target1 - q1[a1]
        A1[a1] = (1.0 - phi) * A1[a1] + phi * abs(delta1)
        alpha1_eff = np.clip(alpha0 * A1[a1], 0.0, 1.0)
        q1[a1] += alpha1_eff * delta1

        CK1 *= (1.0 - phi)
        CK1[a1] += 1.0
        CK2[s] *= (1.0 - phi)
        CK2[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll