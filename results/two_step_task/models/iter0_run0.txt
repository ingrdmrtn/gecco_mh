def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free (MB-MF) with eligibility trace and first-stage stickiness.
    
    This model blends a model-based (MB) planner with a model-free (MF) learner at stage 1, 
    uses MF learning at stage 2, propagates reward prediction errors via an eligibility trace, 
    and includes a perseverative stickiness bias on the first-stage choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state indices reached (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices in the reached state (each state has two aliens).
    reward : array-like of float (0 or 1)
        Obtained reward on each trial.
    model_parameters : list or tuple
        [alpha, beta, lam, w, kappa]
        - alpha (learning rate, [0,1]): RW learning for MF values.
        - beta (inverse temperature, [0,10]): Softmax choice sensitivity for both stages.
        - lam (eligibility trace, [0,1]): Weight of stage-2 RPE bootstrapping into stage-1 MF update.
        - w (MB weight, [0,1]): Mixture weight between MB and MF values at stage 1.
        - kappa (stickiness, [0,1]): Additive bias for repeating the previous first-stage action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, lam, w, kappa = model_parameters
    n_trials = len(action_1)

    # Known transition structure: rows are first-stage actions (A,U), cols are states (X,Y).
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)         # MF action values at stage 1 (for A, U)
    q_stage2_mf = np.zeros((2, 2))    # MF action values at stage 2: for each state (X,Y) two aliens

    # Stickiness memory (previous first-stage choice)
    prev_a1 = None

    for t in range(n_trials):
        # Stage-1 MB action values: expected max over next-state MF values via transition model
        max_q_stage2 = np.max(q_stage2_mf, axis=1)                 # max within each state (X,Y)
        q_stage1_mb = transition_matrix @ max_q_stage2             # MB plan for (A,U)

        # Combine MB and MF at stage 1
        q1_mix = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Add stickiness bias toward repeating last first-stage action
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = 1.0
        q1_pref = q1_mix + kappa * bias

        # Stage-1 policy and likelihood
        exp_q1 = np.exp(beta * (q1_pref - np.max(q1_pref)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy and likelihood in the realized state
        s = state[t]
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD errors
        delta2 = r - q_stage2_mf[s, a2]                 # stage-2 RPE (terminal)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]   # stage-1 RPE using chosen second-stage action

        # MF updates with eligibility trace
        q_stage2_mf[s, a2] += alpha * delta2
        q_stage1_mf[a1] += alpha * (delta1 + lam * delta2)

        # Update stickiness memory
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA with valence-dependent learning, perseveration, and lapses.
    
    This model learns stage-1 and stage-2 Q-values via SARSA-like TD updates, 
    uses separate learning rates for positive vs. negative outcomes, includes 
    a choice perseveration bias at both stages, and a lapse rate that mixes 
    softmax with uniform random choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within the visited state).
    reward : array-like of float (0 or 1)
        Reward outcomes.
    model_parameters : list or tuple
        [alpha_pos, alpha_neg, beta, rho, epsilon]
        - alpha_pos ([0,1]): Learning rate when reward = 1.
        - alpha_neg ([0,1]): Learning rate when reward = 0.
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - rho ([0,1]): Perseveration strength added to the previously chosen action at each stage.
        - epsilon ([0,1]): Lapse probability mixing with uniform random choice.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, rho, epsilon = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)          # stage-1 actions (A,U)
    q2 = np.zeros((2, 2))     # per state (X,Y), two aliens

    # Perseveration memories
    prev_a1 = None
    prev_a2 = [None, None]    # separate memory per state for stage-2

    for t in range(n_trials):
        s = state[t]

        # Stage-1 choice with perseveration and lapse
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        logits1 = beta * (q1 + rho * bias1)
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 /= np.sum(soft1)
        probs_1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 choice with perseveration and lapse
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] = 1.0
        logits2 = beta * (q2[s] + rho * bias2)
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 /= np.sum(soft2)
        probs_2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        # Select learning rate by outcome valence
        alpha = alpha_pos if r > 0.0 else alpha_neg

        # TD errors and SARSA updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        # Update perseveration memories
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with leaky reward tracking and surprise-modulated exploration.
    
    The agent plans at stage 1 using the known transition structure and the current estimates
    of second-stage (alien) values. Second-stage values are learned by leaky integration of reward.
    The first-stage inverse temperature is modulated by surprise from the previous trial's transition
    (greater surprise -> increased/decreased choice sensitivity depending on sensitivity parameter).
    A lapse term mixes the softmax with uniform random choice at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Observed second-stage states (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices in the visited state.
    reward : array-like of float (0 or 1)
        Reward outcomes.
    model_parameters : list or tuple
        [beta1, beta2, tau, surprise_sens, epsilon]
        - beta1 ([0,10]): Base inverse temperature at stage 1.
        - beta2 ([0,10]): Inverse temperature at stage 2.
        - tau ([0,1]): Leaky RW learning rate for second-stage values.
        - surprise_sens ([0,1]): Modulation strength of stage-1 beta by previous trial surprise.
                                  Effective beta1_t = beta1 * (1 + surprise_sens * prev_surprise),
                                  where prev_surprise = 1 - P(prev_transition).
        - epsilon ([0,1]): Lapse probability mixed with uniform random choice at both stages.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    beta1, beta2, tau, surprise_sens, epsilon = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage values (estimated reward probabilities for aliens in each state)
    q2 = 0.5 * np.ones((2, 2))  # initialize to neutral prior

    # Surprise memory from previous trial
    prev_surprise = 0.0  # first trial has no prior surprise

    for t in range(n_trials):
        # Stage-1 MB action values: expected max over states using current q2
        max_q_by_state = np.max(q2, axis=1)  # two states
        q1_mb = transition_matrix @ max_q_by_state

        # Surprise-modulated stage-1 inverse temperature (from previous trial)
        beta1_eff = beta1 * (1.0 + surprise_sens * prev_surprise)

        # Stage-1 policy
        logits1 = beta1_eff * (q1_mb - np.max(q1_mb))
        soft1 = np.exp(logits1)
        soft1 /= np.sum(soft1)
        probs_1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in realized state
        s = state[t]
        logits2 = beta2 * (q2[s] - np.max(q2[s]))
        soft2 = np.exp(logits2)
        soft2 /= np.sum(soft2)
        probs_2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update second-stage values (leaky RW on chosen alien)
        r = reward[t]
        q2[s, a2] += tau * (r - q2[s, a2])

        # Compute surprise from the current transition to use on next trial
        # Surprise = 1 - P(observed transition | chosen action)
        p_trans = transition_matrix[a1, s]
        prev_surprise = 1.0 - p_trans

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll