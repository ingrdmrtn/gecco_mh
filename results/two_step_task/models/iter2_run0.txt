def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Adaptive arbitration between model-based and model-free control with learned transitions.
    
    This model learns both:
      - model-free cached Q-values at both stages, and
      - a model of first-stage transition probabilities.
    A trial-by-trial arbitration weight blends model-based (MB) and model-free (MF) values
    for first-stage choice based on relative uncertainty: higher transition uncertainty
    increases MB reliance, while higher outcome volatility (surprise) increases MF reliance.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y) reached.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [lr_q, lr_t, beta, arb_bias, arb_slope]
        - lr_q ([0,1]): Learning rate for MF Q-values at both stages.
        - lr_t ([0,1]): Learning rate for updating the transition model P(s | a1).
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - arb_bias ([0,1]): Baseline arbitration bias toward MB (0) vs MF (1) after sigmoid transform.
                             Implemented as a bias in the arbitration logistic function.
        - arb_slope ([0,1]): Sensitivity of arbitration to the uncertainty difference (MB minus MF).
                              Larger values yield stronger trial-by-trial shifts in control.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_q, lr_t, beta, arb_bias, arb_slope = model_parameters
    n_trials = len(action_1)

    # Probabilities of first- and second-stage observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Learned transition model P(s | a1) initialized near-uninformative (0.5/0.5)
    T = np.full((2, 2), 0.5)

    # Tracks MF outcome volatility via an EMA of absolute second-stage TD error per action1
    vol_mf = np.zeros(2)

    # Helper to map action1 and observed state to a transition indicator
    def is_common(a1, s):
        # A (0) commonly -> X (0); U (1) commonly -> Y (1)
        return int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # MODEL-BASED Q1 from learned transitions and current MF Q2 (one-step lookahead)
        max_q2 = np.max(q2_mf, axis=1)  # value of each state
        q1_mb = T @ max_q2  # expected value per first-stage action

        # Arbitration weight w_t in [0,1], computed via logistic transform
        # Uncertainty_MB: entropy of transition distribution for each chosen action (higher -> more uncertain)
        p_s = T[a1]  # distribution over states for chosen a1
        # Clip to avoid log(0)
        p_s_clipped = np.clip(p_s, 1e-8, 1 - 1e-8)
        H = -(p_s_clipped * np.log(p_s_clipped) + (1 - p_s_clipped) * np.log(1 - p_s_clipped)).sum()
        # Normalize entropy to [0,1] for binary distribution; max entropy = ln(2)
        unc_mb = H / np.log(2.0)

        # Uncertainty_MF: recent absolute TD error magnitude at stage-2, tracked per a1
        unc_mf = vol_mf[a1]

        # Arbitration signal: MB uncertainty minus MF uncertainty
        signal = unc_mb - unc_mf
        # Map arb_bias in [0,1] to centered bias in [-5,5], slope in [0,1] to [0,10] for flexibility
        bias_term = (arb_bias * 10.0) - 5.0
        slope_term = arb_slope * 10.0
        w = 1.0 / (1.0 + np.exp(-(bias_term + slope_term * signal)))  # weight on MB

        # Blend MB and MF at stage 1
        q1_blend = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1_blend
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure MF)
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: Stage-2 MF update
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += lr_q * delta2

        # Update volatility tracker for MF using EMA of |delta2| tied to the taken a1
        vol_mf[a1] = (1.0 - lr_q) * vol_mf[a1] + lr_q * abs(delta2)

        # Learning: Stage-1 MF bootstrapping from experienced state-action value
        # TD target uses the realized state-action value, consistent with MF SARSA
        target1 = q2_mf[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += lr_q * delta1

        # Learning: Transition model update toward observed state (one-hot target)
        # Binary outcome for s given a1: increment probability toward observed s
        # Update both probabilities to remain a valid distribution
        # For binary case: T[a1, s] moves toward 1, T[a1, 1-s] toward 0
        T[a1, s] += lr_t * (1.0 - T[a1, s])
        T[a1, 1 - s] += lr_t * (0.0 - T[a1, 1 - s])

        # Keep probabilities in [eps, 1-eps]
        T[a1] = np.clip(T[a1], 1e-6, 1.0 - 1e-6)
        # Renormalize row
        T[a1] /= np.sum(T[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with value leak, within-planet generalization, and choice perseveration.
    
    This model captures habits and generalization across similar options:
      - Stage-2 MF Q-values are updated with a leak (forgetting) toward 0.
      - Learning generalizes partially to the unchosen alien on the same planet.
      - Stage-1 MF values bootstrap from the experienced stage-2 value with leak.
      - A single perseveration bias (choice kernel) applies to both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [lr, beta, leak, gen, psi]
        - lr ([0,1]): Learning rate for value updates.
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - leak ([0,1]): Proportion of value decay toward 0 each trial for all Q-values.
                        Higher means faster forgetting.
        - gen ([0,1]): Within-planet generalization strength. Fraction of the TD-error
                       applied to the unchosen alien on the same planet.
        - psi ([0,1]): Strength of choice perseveration kernels added to logits at both stages.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, leak, gen, psi = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels (perseveration) for both stages
    ck1 = np.zeros(2)
    ck2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply global leak to all values before decision
        q1 *= (1.0 - leak)
        q2 *= (1.0 - leak)

        # Stage-1 policy with perseveration
        logits1 = beta * q1 + psi * ck1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration
        logits2 = beta * q2[s] + psi * ck2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 with generalization
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr * delta2
        # Generalize a fraction of delta2 to the unchosen alien on same planet
        other_a2 = 1 - a2
        q2[s, other_a2] += lr * gen * (r - q2[s, other_a2])

        # Stage-1 bootstrapped update from realized stage-2 value
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += lr * delta1

        # Update choice kernels with mild decay (reuse leak to avoid extra parameter)
        ck1 *= (1.0 - leak)
        ck2[s] *= (1.0 - leak)
        ck1[a1] += 1.0
        ck2[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with adjustable commonness, eligibility propagation, and reward-contingent stickiness.
    
    The agent plans using a fixed-form transition structure whose commonness can be tuned,
    integrates model-free values via an eligibility trace, and exhibits a first-stage bias
    to repeat after rewarded-common transitions versus switch after unrewarded-rare transitions.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha, beta, common_strength, stickiness, elig]
        - alpha ([0,1]): Learning rate for MF Q-value updates.
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - common_strength ([0,1]): Scales the asymmetry of transitions. Effective common
                                   probability = 0.5 + 0.2 * common_strength (in [0.5, 0.7]).
        - stickiness ([0,1]): Magnitude of reward-contingent perseveration bias at stage 1.
                              Applied to the previous action's logit based on prior trial's
                              common/rare and reward outcome.
        - elig ([0,1]): Eligibility trace strength propagating stage-2 TD error to stage-1 MF value.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, common_strength, stickiness, elig = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Transition matrix with adjustable commonness
    p_common = 0.5 + 0.2 * common_strength
    T = np.array([[p_common, 1.0 - p_common],  # A -> X (common), Y (rare)
                  [1.0 - p_common, p_common]])  # U -> X (rare), Y (common)

    # Track previous trial info for stickiness
    prev_a1 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q1 from current Q2 and transition model
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Reward-contingent stickiness bias from previous trial
        bias = np.zeros(2)
        if prev_a1 is not None:
            was_common = (prev_a1 == 0 and prev_s == 0) or (prev_a1 == 1 and prev_s == 1)
            if was_common and prev_r > 0.5:
                bias[prev_a1] += stickiness
            elif (not was_common) and prev_r < 0.5:
                # Encourage switching by penalizing repetition after unrewarded rare transitions
                bias[prev_a1] -= stickiness

        # Blend MB with MF via simple additive combination (eligibility handles MF propagation)
        q1_total = q1_mb + q1_mf

        # Stage-1 policy
        logits1 = beta * q1_total + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 learning (MF)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility propagation from stage 2
        # Two components: direct bootstrap and additional eligibility trace component
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1 + elig * alpha * delta2

        # Update previous trial info
        prev_a1 = a1
        prev_s = s
        prev_r = r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll