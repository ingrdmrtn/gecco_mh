def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid arbitration via learned transition uncertainty (single-beta).
    
    This model learns both second-stage rewards and the first-stage transition
    model. It arbitrates between model-based and model-free control at stage 1
    using the uncertainty (entropy) of the learned transition matrix: when
    transitions are uncertain, it relies more on model-free values; when they
    are reliable, it relies more on model-based planning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Observed second-stage states (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha_r, alpha_t, beta, w0, u_gain]
        - alpha_r ([0,1]): Reward learning rate for second-stage Q-values and
                           TD backup to first stage.
        - alpha_t ([0,1]): Transition learning rate (per observed transition).
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - w0 ([0,1]): Baseline weight on model-free control at stage 1.
        - u_gain ([0,1]): Gain with which transition entropy increases MF weight.
                          Effective MF weight: w = clip((1-u_gain)*w0 + u_gain*Hbar, 0, 1),
                          where Hbar is mean normalized entropy of transition rows.

    Returns
    -------
    float
        Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, alpha_t, beta, w0, u_gain = model_parameters
    n_trials = len(action_1)

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free action values
    q1_mf = np.zeros(2)          # stage-1 MF values for [A, U]
    q2 = np.zeros((2, 2))        # stage-2 MF values: state X/Y x alien action

    # Learned transition model: rows are actions (A/U), cols are states (X/Y)
    T = np.full((2, 2), 0.5)     # start agnostic; rows will be normalized

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute model-based action values for stage 1 using current T and q2
        max_q2 = np.max(q2, axis=1)          # max over aliens in each state
        q1_mb = T @ max_q2                   # expected max value via learned transitions

        # Arbitration weight based on transition uncertainty (entropy)
        # Compute normalized entropy per row (base-2; max entropy for binary is 1)
        eps = 1e-12
        p = np.clip(T, eps, 1.0 - eps)
        H_rows = -(p * np.log2(p) + (1 - p) * np.log2(1 - p))  # shape (2,2), entropy per column
        # For binary distributions, entropy in each row equals H(p_row[0]) == H(p_row[1]); take mean per row then mean across rows
        H_row_mean = np.mean(H_rows, axis=1)
        Hbar = np.mean(H_row_mean)  # global uncertainty scalar in [0,1]
        w_mf = np.clip((1.0 - u_gain) * w0 + u_gain * Hbar, 0.0, 1.0)

        # Hybrid action values for stage 1
        q1_hyb = (1.0 - w_mf) * q1_mb + w_mf * q1_mf

        # Stage-1 policy
        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (purely MF)
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: reward and transition models
        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Stage-1 MF update with TD target from realized second-stage value
        td_target1 = q2[s, a2]
        delta1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * delta1

        # Transition learning for chosen action toward observed state
        # Row-wise update then renormalize
        one_hot = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = (1.0 - alpha_t) * T[a1] + alpha_t * one_hot
        # numeric safety renormalization
        T[a1] = T[a1] / np.sum(T[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Asymmetric model-free learning with dual temperatures and perseveration.
    
    This model eschews planning and learns from experienced outcomes only.
    It uses different learning rates for positive vs. negative prediction errors
    at stage 2, propagates credit to stage 1 via a direct TD backup, and
    includes a recency-based perseveration bias at both stages. It allows
    separate inverse temperatures for stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Observed second-stage states (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [eta_plus, eta_minus, b1, b2, stick]
        - eta_plus ([0,1]): Learning rate for positive TD errors (r - Q > 0).
        - eta_minus ([0,1]): Learning rate for negative TD errors (r - Q < 0).
        - b1 ([0,10]): Softmax inverse temperature at stage 1.
        - b2 ([0,10]): Softmax inverse temperature at stage 2.
        - stick ([0,1]): Strength of perseveration bias added to logits
                         (recency kernel incremented for chosen action and
                          decayed otherwise).

    Returns
    -------
    float
        Negative log-likelihood of observed choices across both stages.
    """
    eta_plus, eta_minus, b1, b2, stick = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Perseveration kernels
    ker1 = np.zeros(2)
    ker2 = np.zeros((2, 2))

    # Decay factor for kernels derived from stick (stronger stick -> slower decay)
    # Map stick in [0,1] to decay in [0.0, 1.0): decay = 1 - stick
    decay = 1.0 - stick

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Policies with perseveration
        logits1 = b1 * q1 + stick * ker1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        logits2 = b2 * q2[s] + stick * ker2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning with asymmetric rates at stage 2
        delta2 = r - q2[s, a2]
        alpha2 = eta_plus if delta2 >= 0.0 else eta_minus
        q2[s, a2] += alpha2 * delta2

        # TD backup to stage 1 from realized second-stage value (no planning)
        td_target1 = q2[s, a2]
        delta1 = td_target1 - q1[a1]
        alpha1 = eta_plus if delta1 >= 0.0 else eta_minus
        q1[a1] += alpha1 * delta1

        # Update perseveration kernels
        ker1 *= decay
        ker1[a1] += 1.0
        ker2[s] *= decay
        ker2[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based control with volatility-adaptive learning and uncertainty bonus.
    
    This model plans at stage 1 using the known common transitions (A->X, U->Y),
    while learning second-stage values with an adaptive step size that tracks
    local volatility (uncertainty) via a simple running absolute-error estimate.
    At stage 2, it adds a directed exploration bonus proportional to uncertainty,
    encouraging sampling of uncertain aliens. The same uncertainty-modulated
    values shape the model-based evaluation at stage 1.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Observed second-stage states (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [lr0, nu, b1, b2, bonus]
        - lr0 ([0,1]): Baseline learning-rate gain for rewards.
        - nu ([0,1]): Volatility update rate for uncertainty tracker (higher -> faster).
        - b1 ([0,10]): Softmax inverse temperature at stage 1.
        - b2 ([0,10]): Softmax inverse temperature at stage 2.
        - bonus ([0,1]): Scale of directed exploration bonus added to logits,
                         proportional to current uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of observed choices across both stages.
    """
    lr0, nu, b1, b2, bonus = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 MF values and uncertainty trackers per state-action
    q2 = np.zeros((2, 2))
    unc = np.zeros((2, 2))  # tracks mean absolute TD error |delta| (proxy for volatility)

    # Known transition structure (common ~0.7): A->X, U->Y
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy: add directed exploration bonus proportional to uncertainty
        logits2 = b2 * q2[s] + bonus * unc[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # For stage-1, compute MB values using uncertainty-augmented targets
        # i.e., assume the explo/explore bonus also influences expected downstream value
        augmented_q2 = q2 + (bonus / max(b2, 1e-8)) * unc  # scale so stage-1 sees comparable effect
        max_aug = np.max(augmented_q2, axis=1)
        q1_mb = transition_matrix @ max_aug

        logits1 = b1 * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Learning with volatility-adaptive step size at stage 2
        delta2 = r - q2[s, a2]
        # Update uncertainty as an exponential moving average of |delta|
        unc[s, a2] = (1.0 - nu) * unc[s, a2] + nu * abs(delta2)
        # Effective learning rate increases with estimated uncertainty, capped to [0,1]
        alpha_eff = np.clip(lr0 * (0.5 + unc[s, a2]), 0.0, 1.0)
        q2[s, a2] += alpha_eff * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll