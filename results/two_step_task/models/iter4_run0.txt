def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with learned transition beliefs and perseveration.

    This model combines a model-free (MF) learner with a model-based (MB) planner.
    The MB component uses a learned transition model T_hat that is updated from
    experienced transitions. The MF component learns action values via TD. A
    first-stage perseveration bias captures choice stickiness.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state visited (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1).
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [av, at, beta, w_mb, pers]
        - av ([0,1]): Value learning rate for MF stage-1 and stage-2 values.
        - at ([0,1]): Learning rate for the transition model T_hat.
        - beta ([0,10]): Softmax inverse temperature (both stages).
        - w_mb ([0,1]): Weight on model-based stage-1 values (1-w_mb on MF).
        - pers ([0,1]): First-stage perseveration strength added to logits
                        for the previously chosen ship.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    av, at, beta, w_mb, pers = model_parameters
    n_trials = len(action_1)

    # Initialize likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Learned transition model T_hat (rows: actions A/U -> columns: states X/Y)
    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Perseveration kernel (first-stage)
    prev_choice1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based stage-1 values via current transition beliefs
        max_q2 = np.max(q2, axis=1)  # value of each state
        q1_mb = T_hat @ max_q2

        # Hybrid value with perseveration bias
        logits1 = beta * (w_mb * q1_mb + (1.0 - w_mb) * q1_mf)
        if prev_choice1 is not None:
            stick_vec = np.zeros(2)
            stick_vec[prev_choice1] = 1.0
            logits1 += pers * stick_vec
        # Softmax for stage 1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: stage-2 MF TD
        delta2 = r - q2[s, a2]
        q2[s, a2] += av * delta2

        # Learning: stage-1 MF TD bootstrapping from obtained second-stage action value
        td_target1 = q2[s, a2]
        delta1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += av * delta1

        # Learning: transition model from observed transition (a1 -> s)
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T_hat[a1] = (1.0 - at) * T_hat[a1] + at * target_row
        # Keep rows normalized (numerically)
        T_hat[a1] = T_hat[a1] / (np.sum(T_hat[a1]) + 1e-12)

        # Update perseveration memory
        prev_choice1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Successor-representation stage-1 with novelty-seeking at stage-2.

    Stage-1 uses a learned successor representation (SR) that estimates
    discounted occupancy of second-stage states given each first-stage action.
    The SR is combined with the current state values to produce stage-1 values.
    Stage-2 uses model-free Q-learning, augmented with a novelty bonus for
    less-visited state-action pairs.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state visited (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [eta_q, eta_sr, beta, gamma_sr, novelty]
        - eta_q ([0,1]): Learning rate for stage-2 Q-values.
        - eta_sr ([0,1]): Learning rate for the successor representation rows.
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - gamma_sr ([0,1]): SR discount controlling how strongly SR retains past estimate.
        - novelty ([0,1]): Strength of novelty bonus added to stage-2 logits,
                            scaling 1/sqrt(1 + visit_count).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eta_q, eta_sr, beta, gamma_sr, novelty = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # SR matrix: rows are first-stage actions, columns are states X/Y
    M = np.zeros((2, 2))

    # Stage-2 Q-values
    q2 = np.zeros((2, 2))

    # Visit counters for novelty
    visits = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute state values as value of best action in each state
        V_state = np.max(q2, axis=1)  # shape (2,)

        # Stage-1 SR-based values: Q1[a] = M[a] dot V_state
        q1_sr = M @ V_state

        # Softmax for stage 1
        logits1 = beta * q1_sr
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with novelty bonus (in logits)
        novelty_bonus = novelty / np.sqrt(1.0 + visits[s])
        logits2 = beta * q2[s] + novelty_bonus
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: stage-2 Q-learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += eta_q * delta2

        # Update visits for novelty after observing the action
        visits[s, a2] += 1.0

        # SR update for the chosen first-stage action.
        # TD on SR rows toward observed state occupancy with discount gamma_sr.
        # Target: one-hot(s) + gamma_sr * M[a1] (bootstraps its own estimate)
        e_s = np.array([0.0, 0.0])
        e_s[s] = 1.0
        target_M_row = e_s + gamma_sr * M[a1]
        M[a1] += eta_sr * (target_M_row - M[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model with transition reliability, value decay, and eligibility-like trace.

    Stage-1 combines a model-based plan using a reliability-tempered transition
    model with a model-free component updated via an eligibility-like trace from
    stage-2 outcomes. Both stage-1 and stage-2 values are subject to decay,
    capturing forgetting or non-stationarity.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state visited (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha, beta, conf, decay, trace]
        - alpha ([0,1]): Learning rate for stage-2 Q-values and traced credit to stage-1.
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - conf ([0,1]): Confidence in the canonical transition structure (0.7 common).
                         conf=1 uses the canonical matrix; conf=0 uses a uniform matrix.
        - decay ([0,1]): Per-trial exponential decay applied to all Q-values before updates.
        - trace ([0,1]): Eligibility-like weight: fraction of stage-2 TD error credited
                         directly to the chosen first-stage action (MF).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, conf, decay, trace = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Canonical transition structure (rows: A/U -> cols: X/Y)
    T_canon = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)
    T_uniform = np.array([[0.5, 0.5],
                          [0.5, 0.5]], dtype=float)
    T_eff = conf * T_canon + (1.0 - conf) * T_uniform

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply decay before policy and learning (forgetting/drift)
        q1_mf *= (1.0 - decay)
        q2 *= (1.0 - decay)

        # Model-based Q for stage 1 using tempered transition matrix
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_eff @ max_q2

        # Combine MB and MF by simple summation (they are on same scale)
        logits1 = beta * (q1_mb + q1_mf)
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # TD at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Eligibility-like credit from stage 2 to stage 1 MF
        q1_mf[a1] += trace * alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll