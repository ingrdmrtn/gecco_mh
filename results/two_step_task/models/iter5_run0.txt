def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with learned transitions, uncertainty bonus, and MF eligibility.

    This model maintains:
      - Q2(s,a): model-free second-stage action values updated from reward.
      - T(a,:): learned first-stage transition probabilities to second-stage states (Dirichlet-like via
                exponential averaging).
      - Q1_MF(a): model-free first-stage values updated via an eligibility trace from experienced Q2 values.
    First-stage choice values are a convex combination of:
      - a model-based evaluation: T(a) dot max_a' Q2(s,a')
      - a model-free first-stage value:
      - plus an uncertainty bonus equal to the entropy of the transition row for each action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage planet (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within visited planet).
    reward : array-like of float (0 or 1)
        Reward outcome (gold coin).
    model_parameters : list or tuple
        [alpha, beta, tau, u, trace]
        - alpha ([0,1]): Learning rate for second-stage rewards and first-stage MF backup.
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - tau ([0,1]): Transition learning rate toward observed transition (state given action_1).
        - u ([0,1]): Weight of first-stage uncertainty bonus (transition entropy).
        - trace ([0,1]): Eligibility trace strength for MF backpropagation to Q1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, tau, u, trace = model_parameters
    n_trials = len(action_1)

    # Initialize containers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q2 = np.zeros((2, 2))  # second-stage action values per state
    Q1_MF = np.zeros(2)    # first-stage model-free values

    # Initialize transition model close to neutral but slightly biased to the canonical mapping
    # A->X, U->Y. Start at [0.7,0.3] and [0.3,0.7] but allow learning via tau.
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Helper to compute entropy of a Bernoulli row [p, 1-p]
    def row_entropy(p_row):
        p = np.clip(p_row[0], 1e-6, 1 - 1e-6)
        # Since rows are binary, entropy = -p log p - (1-p) log(1-p)
        return -(p * np.log(p) + (1 - p) * np.log(1 - p))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 policy: combine MB, MF and uncertainty bonus
        mb_values = T @ np.max(Q2, axis=1)  # expected best Q2 under learned transitions
        # Uncertainty bonus via transition entropy for each action
        U_bonus = np.array([row_entropy(T[0]), row_entropy(T[1])])
        logits1 = beta * (0.5 * mb_values + 0.5 * Q1_MF) + u * U_bonus
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: standard softmax on Q2 within visited state
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 (reward prediction error)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Model-free backpropagation to stage 1 via eligibility
        td_target1 = Q2[s, a2]
        delta1 = td_target1 - Q1_MF[a1]
        Q1_MF[a1] += (alpha * trace) * delta1

        # Transition learning: update T row for chosen action toward observed state
        # One-hot target
        target = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1] = (1 - tau) * T[a1] + tau * target
        # Ensure proper normalization (numerical safety)
        T[a1] /= np.sum(T[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with variance-gated learning and first-stage bias.

    This model assumes purely model-free control but adapts learning rates based on
    a running estimate of reward variance for each second-stage action. Higher recent
    variability increases effective learning rate (Kalman-like heuristic).
    First-stage values are updated by bootstrapping the experienced Q2 value, and
    a decision bias favors spaceship A.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage planet (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited planet.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha0, beta, tau_var, gain, bA]
        - alpha0 ([0,1]): Baseline learning-rate floor.
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - tau_var ([0,1]): Exponential averaging rate for updating reward variance estimates.
        - gain ([0,1]): Strength with which variance boosts learning rate.
        - bA ([0,1]): Additive logit bias toward spaceship A (action 0). Converted to signed bias.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha0, beta, tau_var, gain, bA = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    Q2 = np.zeros((2, 2))
    Q1 = np.zeros(2)

    # Running estimates of mean and variance of rewards per (state, action)
    mu = np.zeros((2, 2))
    var = np.ones((2, 2)) * 0.25  # start with moderate uncertainty

    # Convert bA in [0,1] to signed bias in logits: map 0.5 -> 0, extremes +/- with scale pi
    # We'll center at 0.5 and scale by 2*pi to span roughly +/- pi.
    signed_bias_A = (bA - 0.5) * (2.0 * np.pi)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Policies
        logits1 = beta * Q1 + np.array([signed_bias_A, -signed_bias_A])
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Update variance estimates (exponential moving variance)
        # First update mean
        prev_mu = mu[s, a2]
        mu[s, a2] = (1 - tau_var) * mu[s, a2] + tau_var * r
        # Then update variance using squared deviation from previous mean for stability
        dev2 = (r - prev_mu) ** 2
        var[s, a2] = (1 - tau_var) * var[s, a2] + tau_var * dev2

        # Effective learning rate bounded to [0,1]
        alpha_eff = alpha0 + gain * np.clip(var[s, a2], 0.0, 1.0)
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Stage-2 MF learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_eff * delta2

        # Stage-1 bootstrapping from experienced Q2
        td_target1 = Q2[s, a2]
        delta1 = td_target1 - Q1[a1]
        # Use the same adaptive rate but diluted since first-stage is further from reward
        Q1[a1] += (0.5 * alpha_eff) * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Surprise-gated arbitration between model-based and model-free control.

    The controller mixes model-based (MB) and model-free (MF) first-stage values on each trial.
    The MB weight is not fixed; it is modulated by recent transition surprise:
      - After a rare transition (given the known 0.7/0.3 structure), MB control increases.
      - After common transitions, MB control relaxes toward a baseline.
    Second-stage choice is purely MF.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage planet (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited planet.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha, beta, pi0, sens, bias_prev]
        - alpha ([0,1]): Learning rate for MF Q-updates (both stages).
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - pi0 ([0,1]): Baseline MB weight (when surprise is zero).
        - sens ([0,1]): Sensitivity of MB weight to transition surprise (Shannon surprise).
        - bias_prev ([0,1]): Strength of one-step perseveration bias at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, pi0, sens, bias_prev = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    Q1_MF = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Known transition structure (A common->X, U common->Y)
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Perseveration biases (one-step memory)
    prev_a1 = None
    prev_a2 = [None, None]  # per state

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB value for first-stage using known transitions
        mb_values = T_known @ np.max(Q2, axis=1)

        # Compute transition probability given the chosen action and observed state
        p_trans = T_known[a1, s]
        # Surprise (Shannon): -log p
        surprise = -np.log(np.clip(p_trans, 1e-6, 1.0))
        # Normalize surprise to [0,1] relative to max surprise -log(0.3)
        surprise_norm = surprise / (-np.log(0.3))
        # Dynamic MB weight
        w_mb = np.clip(pi0 + sens * (surprise_norm - 0.5), 0.0, 1.0)

        # Perseveration logits
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias_strength = (bias_prev - 0.5) * 2.0  # center at 0; span [-1,1]
            bias1[prev_a1] += bias_strength

        # First-stage policy: mix MB and MF values in logits
        q1_mix = w_mb * mb_values + (1.0 - w_mb) * Q1_MF
        logits1 = beta * q1_mix + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage perseveration bias (state-dependent)
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias_strength2 = (bias_prev - 0.5) * 2.0
            bias2[prev_a2[s]] += bias_strength2

        # Second-stage policy: MF only with perseveration
        logits2 = beta * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 MF update from reward
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF temporal-difference using realized Q2 value
        td_target1 = Q2[s, a2]
        delta1 = td_target1 - Q1_MF[a1]
        Q1_MF[a1] += alpha * delta1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll