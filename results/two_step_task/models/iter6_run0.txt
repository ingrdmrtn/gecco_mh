def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-arbitrated hybrid with learned transitions and perseveration.
    
    This model blends model-based and model-free action values at stage 1, with an
    arbitration weight that depends on the learned transition uncertainty (entropy).
    Stage-2 values are learned via TD. Transition probabilities are learned online.
    A first-stage perseveration bias captures tendency to repeat the previous choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome for the chosen alien.
    model_parameters : list or tuple
        [alpha2, alpha_tr, beta, theta_u, kappa1]
        - alpha2 ([0,1]): Learning rate for stage-2 Q-values and for backing up to stage-1 MF.
        - alpha_tr ([0,1]): Learning rate for updating transition probabilities T(a -> s).
        - beta ([0,10]): Inverse temperature for both stages.
        - theta_u ([0,1]): Arbitration sensitivity to transition uncertainty; higher means
                           more model-based control when uncertainty is low (low entropy).
        - kappa1 ([0,1]): Strength of first-stage perseveration bias (added to logits for
                          repeating the previous spaceship).

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha2, alpha_tr, beta, theta_u, kappa1 = model_parameters
    n_trials = len(action_1)

    # Learned transition matrix T[a, s]; initialize with plausible common/rare structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q2 = np.zeros((2, 2), dtype=float)     # Stage-2 MF values: Q(s, a2)
    q1_mf = np.zeros(2, dtype=float)       # Stage-1 MF cache: Q(a1)

    # For likelihood
    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Perseveration memory
    last_a1 = -1

    ln2 = np.log(2.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based plan: for each action, expected value over states via learned T
        max_q2 = np.max(q2, axis=1)  # V(s) = max_a2 Q(s,a2)
        q1_mb = T @ max_q2

        # Compute transition uncertainty per action as normalized entropy and arbitration weight
        H = -np.sum(T * (np.log(T + 1e-12)), axis=1) / ln2  # in [0,1]
        w_mb = 1.0 - theta_u * H                            # higher when entropy low

        # Blend MB and MF action values action-wise
        q1_blend = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add perseveration bias to logits (bias to repeat last action)
        logits1 = beta * q1_blend
        if last_a1 in (0, 1):
            logits1[last_a1] += kappa1

        # Softmax for stage 1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: Stage-2 TD
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Back up to stage-1 MF value for the chosen action using obtained Q2 target
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * delta1

        # Update transition model for the chosen action towards observed state
        # Simple delta rule on row a1 towards one-hot of observed s
        T[a1, :] = (1.0 - alpha_tr) * T[a1, :]
        T[a1, s] += alpha_tr

        # Renormalize to guard against drift
        T[a1, :] /= np.sum(T[a1, :])

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Successor representation planning with confirmation-tilted commonness and dual betas.
    
    This model plans at stage 1 using a one-step successor representation (SR) over
    latent states, combined with a confirmation tilt that assumes a tunable "common"
    transition strength. Stage-2 values are learned model-free via TD. The two stages
    have separate inverse temperatures.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha2, beta_s1, beta_s2, lam_sr, bias_c]
        - alpha2 ([0,1]): Stage-2 value learning rate.
        - beta_s1 ([0,10]): Stage-1 inverse temperature.
        - beta_s2 ([0,10]): Stage-2 inverse temperature.
        - lam_sr ([0,1]): SR discount/backup strength mapping state values to stage-1.
        - bias_c ([0,1]): Confirmation tilt of the assumed common transition probability.
                          Effective common prob is p_common = 0.6 + 0.2*bias_c in [0.6, 0.8].

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha2, beta_s1, beta_s2, lam_sr, bias_c = model_parameters
    n_trials = len(action_1)

    # Fixed-but-tilted transition structure via confirmation bias
    p_common = 0.6 + 0.2 * bias_c  # in [0.6, 0.8]
    p_rare = 1.0 - p_common
    # A commonly -> X; U commonly -> Y
    T = np.array([[p_common, p_rare],
                  [p_rare,  p_common]], dtype=float)

    # Stage-2 values
    q2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # SR planning: V(s) = max_a2 Q(s,a2); Q_stage1(a) = lam_sr * sum_s T[a,s] * V(s)
        V_s = np.max(q2, axis=1)
        q1_sr = lam_sr * (T @ V_s)

        # Stage-1 softmax
        logits1 = beta_s1 * q1_sr
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax
        logits2 = beta_s2 * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Exploration-by-uncertainty bonus and dynamic temperature at stage 1; stickiness at stage 2.
    
    This model plans model-based at stage 1 using the fixed task structure but augments
    action logits with a state-uncertainty bonus derived from a running estimate of
    reward variance at stage 2. Stage-2 learning is model-free. Stage 2 also includes
    choice stickiness within each state.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha2, beta0, gain_h, tau_u, stick2]
        - alpha2 ([0,1]): Stage-2 TD learning rate.
        - beta0 ([0,10]): Baseline inverse temperature used for both stages.
        - gain_h ([0,1]): Weight of the uncertainty bonus added to stage-1 logits.
        - tau_u ([0,1]): Learning rate for tracking reward uncertainty (second-moment).
        - stick2 ([0,1]): Stage-2 within-state stickiness (bias to repeat previous alien).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha2, beta0, gain_h, tau_u, stick2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 values, and uncertainty trackers (mean and second moment -> variance)
    q2 = np.zeros((2, 2), dtype=float)
    m1 = np.zeros((2, 2), dtype=float)      # running mean of reward per (s,a2)
    m2 = np.zeros((2, 2), dtype=float)      # running second moment E[r^2]

    # For stickiness at stage 2
    last_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute state values and MB planning for stage 1
        V_s = np.max(q2, axis=1)  # value of states X, Y
        q1_mb = T @ V_s

        # Uncertainty estimate per state via running variance across aliens
        var_sa = np.clip(m2 - m1 * m1, 0.0, None)  # variance per (s,a2)
        var_mean = 0.5 * (var_sa[:, 0] + var_sa[:, 1])  # average variance per state
        # Uncertainty bonus for each first-stage action = expected sqrt(variance) over next states
        u_bonus = T @ np.sqrt(var_mean + 1e-12)

        # Stage-1 logits: value plus uncertainty-driven exploration bonus
        logits1 = beta0 * q1_mb + gain_h * u_bonus
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 logits with within-state stickiness
        logits2 = beta0 * q2[s]
        if last_a2[s] in (0, 1):
            logits2[last_a2[s]] += stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 TD learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Update reward uncertainty trackers (mean and second moment)
        m1[s, a2] += tau_u * (r - m1[s, a2])
        m2[s, a2] += tau_u * (r * r - m2[s, a2])

        # Update stickiness memory
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll