def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MBâ€“MF with learned transitions and volatility-gated arbitration.

    This model learns second-stage values and the first-stage transition model.
    It arbitrates between model-based (MB) and model-free (MF) control at stage 1
    using a dynamic weight that increases with recent transition volatility for
    the chosen spaceship. Volatility is estimated as the exponentially weighted
    magnitude of recent changes in the learned transition probabilities.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y) reached after the first-stage choice.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome on each trial.
    model_parameters : list or tuple
        [lr_q, lr_tr, beta, w0, v_sens]
        - lr_q ([0,1]): Learning rate for Q-values at both stages.
        - lr_tr ([0,1]): Learning rate for transition model T(a -> s).
        - beta ([0,10]): Softmax inverse temperature (both stages).
        - w0 ([0,1]): Baseline MB weight for stage-1 arbitration.
        - v_sens ([0,1]): Sensitivity scaling from volatility to MB weight
                          (higher -> more MB when transitions are volatile).

    Returns
    -------
    float
        Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """
    lr_q, lr_tr, beta, w0, v_sens = model_parameters
    n_trials = len(action_1)

    # Initialize choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 MF values: Q2[state, action]
    Q2 = np.zeros((2, 2))
    # Stage-1 MF values: Q1_MF[action]
    Q1_MF = np.zeros(2)

    # Learned transition model T[action, state] (rows sum to 1)
    # Start near the canonical two-step structure (A->X common; U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Volatility estimate per action (EWMA of absolute transition change)
    Vol = np.zeros(2)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q at stage 1: expected value via learned transitions
        mb_q1 = T @ np.max(Q2, axis=1)

        # Arbitration weight for current trial based on volatility for each action.
        # Compute a per-action weight and then use it in logits (action-dependent mixing).
        # w_a = clip( w0 + v_sens * (Vol[a] - 0.5), 0, 1 )
        w = np.clip(w0 + v_sens * (Vol - 0.5), 0.0, 1.0)
        # Mixed Q for stage 1 is action-dependent linear blend
        mixed_q1 = w * mb_q1 + (1.0 - w) * Q1_MF

        # Stage-1 softmax
        logits1 = beta * mixed_q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax (pure MF values at visited state)
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates

        # 1) Stage-2 value update (MF)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += lr_q * delta2

        # 2) Stage-1 MF update from realized second-stage value (one-step backup)
        td_target1 = Q2[s, a2]
        delta1 = td_target1 - Q1_MF[a1]
        Q1_MF[a1] += lr_q * delta1

        # 3) Transition learning for chosen action (row-normalized update)
        # Previous row for volatility computation
        prev_row = T[a1].copy()
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T[a1] = (1.0 - lr_tr) * T[a1] + lr_tr * target_row
        # Ensure normalization (numerical safety)
        T[a1] = T[a1] / np.maximum(np.sum(T[a1]), 1e-12)

        # 4) Volatility update: EWMA of absolute change magnitude
        # Use L1 change in the chosen row as a compact measure of surprise/volatility
        change_mag = 0.5 * np.sum(np.abs(T[a1] - prev_row))  # in [0,1]
        Vol[a1] = (1.0 - lr_tr) * Vol[a1] + lr_tr * change_mag

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with directed exploration via decaying uncertainty bonus and stage-2 stickiness.

    This model is purely model-free but includes a directed exploration component
    at stage 2: actions with lower (recency-weighted) visit counts receive a bonus.
    Counts decay over time to adapt to nonstationarity. A stage-2 stickiness bias
    captures a tendency to repeat the last action taken in the same state.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha, beta, bonus, decay, stick2]
        - alpha ([0,1]): Learning rate for Q-values at both stages (MF).
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - bonus ([0,1]): Strength of the uncertainty bonus at stage 2.
        - decay ([0,1]): Decay rate for visit counts per trial (higher -> faster forgetting).
        - stick2 ([0,1]): Strength of stage-2 stickiness bias in logits.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, bonus, decay, stick2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Decaying visit counts for stage-2 actions: higher counts -> lower uncertainty
    C2 = np.zeros((2, 2))

    # Stage-2 stickiness kernel: boosted for last chosen action in each state
    K2 = np.zeros((2, 2))

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Directed exploration at stage 2 via uncertainty bonus
        # Convert counts to uncertainty: U ~ 1/sqrt(count+1)
        U = 1.0 / np.sqrt(C2[s] + 1.0)
        logits2 = beta * Q2[s] + bonus * U + stick2 * K2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy (pure MF over Q1)
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Learning updates (MF)
        # Stage-2 update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 update (one-step backup from realized Q2 value)
        td_target1 = Q2[s, a2]
        delta1 = td_target1 - Q1[a1]
        Q1[a1] += alpha * delta1

        # Update decaying counts: global decay each trial, then increment visited
        C2 *= (1.0 - decay)
        C2[s, a2] += 1.0

        # Update stickiness kernels: decay, then add to chosen action
        K2 *= (1.0 - decay)
        K2[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Latent stability arbitration with rare-transition bias at stage 1.

    This model mixes model-based (fixed transition structure) and model-free
    values at stage 1 according to a running belief about transition stability.
    Stability is the exponentially weighted average of whether the last transition
    was common. After rare transitions, the model applies a transient aversive bias
    against repeating the same stage-1 action; after common transitions, it applies
    a transient approach bias. These biases decay over time. Stage 2 is model-free.

    Transitions assumed:
    - Spaceship A (0) commonly -> Planet X (0); rare -> Planet Y (1)
    - Spaceship U (1) commonly -> Planet Y (1); rare -> Planet X (0)

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within visited state.
    reward : array-like of float (0 or 1)
        Reward outcome.
    model_parameters : list or tuple
        [alpha, beta, mix, rare_bias, memory]
        - alpha ([0,1]): Learning rate for MF Q-values at both stages.
        - beta ([0,10]): Softmax inverse temperature for both stages.
        - mix ([0,1]): Base weight on model-based control (scaled by stability).
        - rare_bias ([0,1]): Magnitude of transient bias: negative after rare,
                              positive after common, applied to repeating the same
                              stage-1 action next time (implemented via a kernel).
        - memory ([0,1]): Decay/update rate for stability and the bias kernel
                           (higher -> faster adaptation/decay).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, mix, rare_bias, memory = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Fixed known transition structure (common=0.7, rare=0.3)
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Running stability: EWMA of common-transition indicator
    stability = 0.5

    # Stage-1 transient bias kernel (decays over time)
    B1 = np.zeros(2)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q1 from fixed transitions and current Q2
        mb_q1 = T_known @ np.max(Q2, axis=1)

        # Effective MB weight increases with stability
        w_eff = np.clip(mix * stability, 0.0, 1.0)
        q1_mix = w_eff * mb_q1 + (1.0 - w_eff) * Q1_mf

        # Stage-1 softmax with transient bias kernel
        logits1 = beta * q1_mix + B1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax (MF)
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 MF update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF update from realized Q2 value
        td_target1 = Q2[s, a2]
        delta1 = td_target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Determine whether the observed transition was common for the selected spaceship
        common = 1 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0

        # Update stability (EWMA of common indicator)
        stability = (1.0 - memory) * stability + memory * common

        # Update transient bias kernel:
        # Decay previous bias, then add approach/avoid term to the chosen action.
        # After common: bias toward repeating (positive). After rare: bias against (negative).
        B1 *= (1.0 - memory)
        B1[a1] += rare_bias if common == 1 else -rare_bias

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll