def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Dynamic arbitration via transition surprise with learned transitions.
    
    This hybrid model learns second-stage values model-freely, and separately
    learns first-stage transition probabilities. The first-stage policy mixes
    model-free and model-based values with a time-varying weight driven by
    current transition surprise. Surprise increases reliance on model-based control.
    A stickiness term captures first-stage choice persistence.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcomes.
    model_parameters : list or tuple
        [alpha_q, alpha_tr, beta, zeta, stick]
        - alpha_q ([0,1]): Learning rate for second-stage Q-values.
        - alpha_tr ([0,1]): Learning rate for first-stage transition probabilities.
        - beta ([0,10]): Softmax inverse temperature (both stages).
        - zeta ([0,1]): Surprise-to-arbitration gain; higher makes arbitration weight
                        more sensitive to transition surprise.
        - stick ([0,1]): First-stage choice stickiness strength added to logits.
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_q, alpha_tr, beta, zeta, stick = model_parameters
    n_trials = len(action_1)
    
    # Initialize estimates
    q2 = np.zeros((2, 2))           # Second-stage Q-values
    q1_mf = np.zeros(2)             # First-stage model-free values
    T = np.full((2, 2), 0.5)        # Learned transition matrix rows sum to ~1
    prev_a1 = None                  # For stickiness
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Model-based first-stage values from learned transitions
        max_q2_by_state = np.max(q2, axis=1)
        q1_mb = T @ max_q2_by_state  # shape (2,)
        
        # Surprise-based arbitration weight in [0,1]
        # Surprise for taken action equals prediction error on the observed transition
        pe_tr = 1.0 - T[a1, s]  # Observed transition indicator is 1 for visited state
        surprise = abs(pe_tr)
        # Map surprise to arbitration weight; zeta modulates sensitivity.
        # w = surprise^zeta, with edge-cases handled: if zeta=0 -> w=1; if zeta=1 -> w=surprise
        if zeta == 0.0:
            w = 1.0
        else:
            w = surprise ** zeta
        w = np.clip(w, 0.0, 1.0)
        
        # First-stage decision: mixture of MF and MB with stickiness
        q1_mix = (1.0 - w) * q1_mf + w * q1_mb
        logits1 = beta * q1_mix
        if prev_a1 is not None:
            # Add stickiness to the previously chosen action
            logits1[prev_a1] += stick
            logits1[1 - prev_a1] -= stick
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]
        
        # Second-stage decision
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]
        
        # Second-stage learning (model-free TD)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * delta2
        
        # First-stage model-free update (back up the obtained second-stage value)
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * delta1  # tie MF learning to same alpha for parsimony
        
        # Update transition estimates for the taken first-stage action toward observed state
        # Row-wise normalization by symmetric updates
        for sp in (0, 1):
            indicator = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_tr * (indicator - T[a1, sp])
        # Keep T in [0,1] numerically
        T[a1] = np.clip(T[a1], 1e-6, 1.0 - 1e-6)
        # Not updating the unchosen action's transitions this trial
        
        prev_a1 = a1
    
    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Confirmation-biased credit assignment with dual learning rates and stage-2 recency.
    
    This model is purely model-free but implements a confirmation bias in how
    first-stage values are updated based on whether the transition-outcome
    combination confirms the chosen first-stage action (common+reward or rare+no reward)
    versus disconfirms it (rare+reward or common+no reward). It also includes a
    stage-2 choice kernel with recency controlled by the stage-1 alpha.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcomes.
    model_parameters : list or tuple
        [alpha2, alpha1, beta, confirm, rep2]
        - alpha2 ([0,1]): Second-stage learning rate.
        - alpha1 ([0,1]): First-stage learning rate (also kernel decay).
        - beta ([0,10]): Softmax inverse temperature (both stages).
        - confirm ([0,1]): Magnitude of confirmation bias on first-stage learning.
                           Increases learning when confirmed, decreases when disconfirmed.
        - rep2 ([0,1]): Strength of stage-2 choice recency kernel added to logits.
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha2, alpha1, beta, confirm, rep2 = model_parameters
    n_trials = len(action_1)
    
    # Value functions
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    # Stage-2 choice kernel (recency with decay tied to alpha1)
    CK2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]
        
        # First-stage policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]
        
        # Second-stage policy with choice kernel
        logits2 = beta * q2[s] + rep2 * CK2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]
        
        # Stage-2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2
        
        # Stage-1 confirmation bias in learning
        # Determine whether transition was common or rare under fixed task mapping
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        # Confirmation if (common and reward) or (rare and no reward)
        confirmed = (is_common and r > 0.5) or ((not is_common) and r <= 0.5)
        gamma = 1.0 if confirmed else -1.0
        alpha1_eff = np.clip(alpha1 * (1.0 + gamma * confirm), 0.0, 1.0)
        
        # Back up obtained second-stage value to stage-1
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha1_eff * delta1
        
        # Update stage-2 choice kernel (recency)
        CK2 *= (1.0 - alpha1)
        CK2[s, a2] += 1.0
    
    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Volatility-adaptive learning with post-rare switching bias.
    
    This model adapts its learning rate to an online estimate of reward volatility.
    A low anchor keeps learning from collapsing when volatility is small. Additionally,
    rare first-stage transitions induce a transient bias to switch spaceships next trial.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward outcomes.
    model_parameters : list or tuple
        [alpha0, beta, kappa, anchor, rare_bias]
        - alpha0 ([0,1]): Base learning-rate gain.
        - beta ([0,10]): Softmax inverse temperature (both stages).
        - kappa ([0,1]): Volatility update rate; also decay for rare-bias trace.
        - anchor ([0,1]): Minimal fraction of alpha0 preserved regardless of volatility.
        - rare_bias ([0,1]): Strength of post-rare switching bias on stage-1 logits.
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha0, beta, kappa, anchor, rare_bias = model_parameters
    n_trials = len(action_1)
    
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))
    
    # Online volatility estimate based on squared reward prediction error at stage 2
    vol = 0.0
    # Post-rare switch bias trace (+ favors switching away from last a1; - favors staying)
    rb_trace = 0.0
    last_a1 = None
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]
        
        # Stage-1 policy with post-rare switching bias on previous action
        logits1 = beta * q1.copy()
        if last_a1 is not None:
            # Implement as differential bias on repeating vs switching
            logits1[last_a1] -= rare_bias * rb_trace
            logits1[1 - last_a1] += rare_bias * rb_trace
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]
        
        # Stage-2 policy (no extra bias)
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]
        
        # Compute current TD errors
        delta2 = r - q2[s, a2]
        # Update volatility (EWMA of squared TD error)
        vol = (1.0 - kappa) * vol + kappa * (delta2 ** 2)
        # Effective learning rates bounded by alpha0 and anchor
        alpha_eff = np.clip(alpha0 * (anchor + (1.0 - anchor) * vol), 0.0, 1.0)
        
        # Update second-stage values with volatility-adaptive LR
        q2[s, a2] += alpha_eff * delta2
        
        # Back up to stage-1 with same adaptive LR (bootstrapping on obtained Q2)
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha_eff * delta1
        
        # Update post-rare switching bias trace based on current trial's transition
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        # Rare transitions push rb_trace up toward 1, common pull it down toward 0
        rb_trace = (1.0 - kappa) * rb_trace + kappa * (0.0 if is_common else 1.0)
        
        last_a1 = a1
    
    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll