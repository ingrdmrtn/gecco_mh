Below are three standalone Python functions implementing distinct cognitive models for the two-step task. Each function follows the required signature and returns the negative log-likelihood of the observed choices. All parameters are used meaningfully, respect the specified bounds, and each model introduces a different computational mechanism.

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid SR/MB with learned transitions and volatility-adaptive learning.

    This model combines a model-based planner (using a learned transition model)
    with a successor-representation-like planner that leverages the learned
    transition to compute expected second-stage values. Second-stage learning
    is volatility-adaptive: the effective learning rate scales with recent
    outcome variability. The first-stage policy mixes MF and SR/MB components.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state encountered (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered planet.
    reward : array-like of float (0 or 1)
        Received coins.
    model_parameters : list or tuple
        [alpha_q, alpha_tr, beta, vol, w_sr]
        - alpha_q ([0,1]): Base learning rate for second-stage Q-values (MF component).
        - alpha_tr ([0,1]): Learning rate for the transition model.
        - beta ([0,10]): Inverse temperature for both stages.
        - vol ([0,1]): Volatility sensitivity; scales the learning rate using recent TD variability.
        - w_sr ([0,1]): Weight on SR/MB action values vs MF at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_q, alpha_tr, beta, vol, w_sr = model_parameters
    n_trials = len(action_1)

    # Initialize likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free action values
    q1_mf = np.zeros(2)        # Stage-1 MF Q-values
    q2 = np.zeros((2, 2))      # Stage-2 MF Q-values for each state and action

    # Learnable transition model T[a1, s2]
    T = np.array([[0.5, 0.5], [0.5, 0.5]], dtype=float)

    # Volatility proxy: running squared TD error for each second-stage state-action
    var2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # SR/MB action values at stage 1: expected max Q2 under learned transitions
        max_q2 = np.max(q2, axis=1)             # [v(X), v(Y)]
        q1_sr = T @ max_q2                      # expected value for choosing each spaceship

        # Mixed stage-1 values
        q1_mix = w_sr * q1_sr + (1.0 - w_sr) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1_mix
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Update transition model with simple delta rule
        # Target vector is one-hot on observed state s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        # Move current row T[a1] toward the target
        T[a1] = (1.0 - alpha_tr) * T[a1] + alpha_tr * target
        # Ensure row-normalization (numerical safety)
        T[a1] /= np.sum(T[a1])

        # Second-stage MF update with volatility-adaptive learning
        delta2 = r - q2[s, a2]
        # Update volatility proxy (running squared error)
        var2[s, a2] = (1.0 - vol) * var2[s, a2] + vol * (delta2 ** 2)
        # Adaptive learning rate, clipped to [0,1]
        alpha2_eff = np.clip(alpha_q * (1.0 + var2[s, a2]), 0.0, 1.0)
        q2[s, a2] += alpha2_eff * delta2

        # Back up to stage-1 MF via observed second-stage value (no explicit eligibility)
        td_target1 = q2[s, a2]
        delta1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha2_eff * delta1  # Couple the MF update to the same adaptive rate

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with confirmation-sensitive transition-tagged choice kernels.

    This model learns second-stage Q-values model-free and plans at stage 1 using
    a flexible value aggregator. It includes a transition-sensitive choice kernel:
    recency bias is modulated by whether the previous transition was common vs rare
    and whether the outcome confirmed the current transition model (reward after common,
    non-reward after rare) vs disconfirmed it.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state encountered (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered planet.
    reward : array-like of float (0 or 1)
        Received coins.
    model_parameters : list or tuple
        [alpha, beta, lambda_plan, rare_sens, bias0]
        - alpha ([0,1]): Learning rate for second-stage Q-values.
        - beta ([0,10]): Inverse temperature for both stages.
        - lambda_plan ([0,1]): Planning aggregator weight; 1 uses max(Q2), 0 uses mean(Q2).
        - rare_sens ([0,1]): Strength/decay for transition-tagged choice kernel; also governs decay.
        - bias0 ([0,1]): Fixed bias toward spaceship A (added to A and subtracted from U in logits).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, lambda_plan, rare_sens, bias0 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    T_fixed = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage Q-values
    q2 = np.zeros((2, 2))

    # Transition-tagged choice kernel for stage 1
    # Captures recency bias modulated by common vs rare and confirmation
    K1 = np.zeros(2)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Planning aggregator for stage 1:
        # value(s2) = lambda_plan * max_a Q2(s2,a) + (1 - lambda_plan) * mean_a Q2(s2,a)
        v_s2 = lambda_plan * np.max(q2, axis=1) + (1.0 - lambda_plan) * np.mean(q2, axis=1)
        q1_mb = T_fixed @ v_s2

        # Stage-1 logits: MB values + bias toward A + transition-tagged kernel
        bias_vec = np.array([bias0, -bias0])
        logits1 = beta * q1_mb + K1 + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Second-stage MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update transition-tagged kernel
        # Determine whether the observed transition was common vs rare
        # Common if (A->X) or (U->Y)
        was_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

        # Confirmation signal: reward after common or no reward after rare
        confirm = (r > 0 and was_common) or (r <= 0 and (not was_common))

        # Decay kernel and apply signed increment to the chosen action
        # Larger increment for confirmed common transitions; smaller or opposite for rare
        K1 *= (1.0 - rare_sens)  # decay governed by rare_sens
        inc = (1.0 if confirm else -1.0) * (1.0 if was_common else (1.0 - rare_sens))
        K1[a1] += rare_sens * inc

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Meta-control: uncertainty-gated MB/MF arbitration with eligibility and lapses.

    The model arbitrates between model-based (MB) and model-free (MF) control at stage 1
    using a dynamic weight that scales with the current uncertainty in second-stage action
    preferences. Uncertainty is measured via entropy of softmaxed Q2 distributions.
    It also includes an eligibility-like update at stage 1 and a small lapse rate.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state encountered (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered planet.
    reward : array-like of float (0 or 1)
        Received coins.
    model_parameters : list or tuple
        [alpha, beta, theta, mix0, epsilon]
        - alpha ([0,1]): Learning rate for MF Q-values.
        - beta ([0,10]): Inverse temperature for both stages (before lapse).
        - theta ([0,1]): Uncertainty sensitivity; also used as eligibility strength.
        - mix0 ([0,1]): Baseline MB weight in arbitration.
        - epsilon ([0,1]): Lapse rate (with probability epsilon choose uniformly at random).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, theta, mix0, epsilon = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    def softmax_probs(vec, beta_val):
        lg = beta_val * vec
        lg -= np.max(lg)
        pr = np.exp(lg)
        pr /= np.sum(pr)
        return pr

    def entropy_of_probs(pr):
        # small epsilon for stability
        p = np.clip(pr, 1e-12, 1.0)
        return -np.sum(p * np.log(p))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute second-stage softmax distributions for both states (for uncertainty)
        probs_X = softmax_probs(q2[0], beta)
        probs_Y = softmax_probs(q2[1], beta)
        H_X = entropy_of_probs(probs_X)
        H_Y = entropy_of_probs(probs_Y)
        # Normalize entropy to [0,1] for a binary choice: max entropy = log(2)
        H_X /= np.log(2.0)
        H_Y /= np.log(2.0)

        # MB values for stage 1: expected max over second stage
        v_s2 = np.max(q2, axis=1)  # [v(X), v(Y)]
        q1_mb = T @ v_s2

        # Compute action-specific uncertainty by expected entropy of next state
        exp_H_for_a0 = T[0, 0] * H_X + T[0, 1] * H_Y
        exp_H_for_a1 = T[1, 0] * H_X + T[1, 1] * H_Y
        # Arbitration weight per action
        w0 = np.clip(mix0 + theta * exp_H_for_a0, 0.0, 1.0)
        w1 = np.clip(mix0 + theta * exp_H_for_a1, 0.0, 1.0)
        w_vec = np.array([w0, w1])

        # Mixed stage-1 action values (elementwise arbitration)
        q1_mix = w_vec * q1_mb + (1.0 - w_vec) * q1_mf

        # Stage-1 policy with lapse
        pr1 = softmax_probs(q1_mix, beta)
        pr1 = (1.0 - epsilon) * pr1 + epsilon * 0.5
        p_choice_1[t] = pr1[a1]

        # Stage-2 policy with lapse
        pr2 = softmax_probs(q2[s], beta)
        pr2 = (1.0 - epsilon) * pr2 + epsilon * 0.5
        p_choice_2[t] = pr2[a2]

        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility governed by theta
        # Use the observed second-stage action value as bootstrap target
        td_target1 = q2[s, a2]
        delta1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += (alpha * theta) * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll