def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the model-free 2-step model with
    dynamic associability (Pearceâ€“Hall-like) and stage-specific choice kernels.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list): [alpha0, phi, beta, k1, k2]
            - alpha0: Baseline learning-rate gain.
            - phi: Associability update rate and kernel decay (0..1).
            - beta: Inverse temperature (applied to both stages).
            - k1: Stage-1 choice-kernel weight.
            - k2: Stage-2 choice-kernel weight.
        drift1, drift2, drift3, drift4 (np.ndarray):
            Trial-wise reward probabilities for state-action pairs:
            reward_probs = [[drift1[t], drift2[t]],   # state 0, actions 0/1
                            [drift3[t], drift4[t]]]   # state 1, actions 0/1

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np
    rng = np.random.default_rng()

    alpha0, phi, beta, k1, k2 = parameters

    # Standard two-step transition matrix (common = 0.7, rare = 0.3)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize value functions
    q1 = np.zeros(2)        # Stage-1 action values
    q2 = np.zeros((2, 2))   # Stage-2 state-action values

    # Associability traces
    A1 = np.zeros(2)
    A2 = np.zeros((2, 2))

    # Choice kernels (recency biases)
    CK1 = np.zeros(2)
    CK2 = np.zeros((2, 2))

    # Storage
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Build trial-wise reward probability matrix by state and action
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Stage 1: softmax over q1 plus choice-kernel bias
        logits1 = beta * q1 + k1 * CK1
        logits1 = logits1 - np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / p1.sum()
        a1 = rng.choice([0, 1], p=p1)

        # Transition to second-stage state
        s2 = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage 2: softmax over q2[s2] plus state-specific choice-kernel bias
        logits2 = beta * q2[s2] + k2 * CK2[s2]
        logits2 = logits2 - np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / p2.sum()
        a2 = rng.choice([0, 1], p=p2)

        # Outcome
        r = int(rng.random() < reward_probs[s2][a2])

        # Learning updates with dynamic associability
        # Stage 2 TD error and associability
        delta2 = r - q2[s2, a2]
        A2[s2, a2] = (1.0 - phi) * A2[s2, a2] + phi * abs(delta2)
        alpha2_eff = np.clip(alpha0 * A2[s2, a2], 0.0, 1.0)
        q2[s2, a2] += alpha2_eff * delta2

        # Stage 1 TD error and associability (bootstraps on updated q2[s2, a2])
        td_target1 = q2[s2, a2]
        delta1 = td_target1 - q1[a1]
        A1[a1] = (1.0 - phi) * A1[a1] + phi * abs(delta1)
        alpha1_eff = np.clip(alpha0 * A1[a1], 0.0, 1.0)
        q1[a1] += alpha1_eff * delta1

        # Choice-kernel updates with decay phi
        CK1 *= (1.0 - phi)
        CK1[a1] += 1.0
        CK2[s2] *= (1.0 - phi)
        CK2[s2, a2] += 1.0

        # Store trial data
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward