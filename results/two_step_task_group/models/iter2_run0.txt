Here are 3 new cognitive models that incorporate STAI (anxiety) into the decision-making process using mechanisms distinct from the previous iterations.

### Model 1: Anxiety-Driven Choice Stickiness
**Hypothesis:** Anxiety increases "perseveration" or "stickiness." High-anxiety individuals may prefer to repeat their previous Stage 1 choice to reduce decision-making load or avoid the uncertainty of switching, regardless of the model-based or model-free value.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Choice Stickiness Model.
    
    This model hypothesizes that anxiety increases choice perseveration (stickiness).
    A 'stickiness' bonus is added to the Q-value of the previously chosen action 
    at Stage 1. The magnitude of this bonus is scaled by the participant's STAI score.
    
    Parameters:
    - learning_rate: [0, 1] Rate of value updating.
    - beta: [0, 10] Inverse temperature (exploration/exploitation trade-off).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stai_stickiness: [0, 5] Scaling factor for how much STAI adds to the stickiness bonus.
      (Note: A slightly higher bound allows for stronger perseveration effects).
    
    """
    learning_rate, beta, w, stai_stickiness = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
    
    # Normalize STAI to 0-1 range (assuming range 20-80)
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate the specific stickiness bonus for this subject
    stickiness_bonus = stai_stickiness * norm_stai

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous choice for stickiness (initialize with -1 so no bonus on trial 0)
    last_action_1 = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += stickiness_bonus

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker
        last_action_1 = action_1[trial]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 2 Update
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * rpe_2
        
        # Stage 1 MF Update
        value_stage2_chosen = q_stage2[state_idx, action_2[trial]]
        rpe_1 = value_stage2_chosen - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Reward Blunting
**Hypothesis:** High anxiety is often comorbid with anhedonia or a focus on safety over gain. This model posits that STAI dampens the learning rate specifically for *positive* prediction errors (rewards), while learning from lack of reward (negative RPE) remains at baseline.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Reward Blunting Model.
    
    This model posits that anxiety dampens the sensitivity to positive outcomes.
    While the baseline learning rate applies to negative prediction errors,
    the learning rate for positive prediction errors is reduced as STAI increases.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate (applied fully to negative RPEs).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stai_dampening: [0, 1] Factor by which STAI reduces LR for positive RPEs.
      LR_pos = lr_base * (1 - (stai_dampening * norm_stai)).
    
    """
    lr_base, beta, w, stai_dampening = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
    
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate the dampened learning rate for positive events
    # If stai_dampening is high and anxiety is high, lr_pos becomes very small.
    lr_pos = lr_base * (1.0 - (stai_dampening * norm_stai))
    lr_pos = np.clip(lr_pos, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 2
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        # Use dampened LR if RPE is positive, base LR if negative
        current_lr_2 = lr_pos if rpe_2 > 0 else lr_base
        q_stage2[state_idx, action_2[trial]] += current_lr_2 * rpe_2
        
        # Stage 1
        value_stage2_chosen = q_stage2[state_idx, action_2[trial]]
        rpe_1 = value_stage2_chosen - q_stage1_mf[action_1[trial]]
        current_lr_1 = lr_pos if rpe_1 > 0 else lr_base
        q_stage1_mf[action_1[trial]] += current_lr_1 * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Rigidity (Beta Modulation)
**Hypothesis:** Anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety leads to "rigidity" or risk-aversion, causing participants to exploit perceived high-value options deterministically rather than exploring. This is modeled by STAI increasing the inverse temperature (`beta`).

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Rigidity Model.
    
    This model assumes that anxiety reduces exploration (stochasticity).
    The inverse temperature (beta) is not static but is a function of the 
    participant's STAI score. Higher anxiety leads to higher beta (more rigid/deterministic choices).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - w: [0, 1] Mixing weight.
    - beta_base: [0, 5] The baseline inverse temperature for a calm person.
    - stai_rigidity: [0, 5] The amount added to beta based on STAI.
      Effective Beta = beta_base + (stai_rigidity * norm_stai).
    
    """
    learning_rate, w, beta_base, stai_rigidity = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
    
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate effective beta
    # If stai_rigidity is high, anxious people will have a very high beta (deterministic)
    effective_beta = beta_base + (stai_rigidity * norm_stai)
    # Cap beta at 10 to prevent numerical overflow in exp() and adhere to general bounds
    effective_beta = np.clip(effective_beta, 0.0, 10.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Use effective_beta instead of a static parameter
        exp_q1 = np.exp(effective_beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(effective_beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * rpe_2
        
        value_stage2_chosen = q_stage2[state_idx, action_2[trial]]
        rpe_1 = value_stage2_chosen - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```