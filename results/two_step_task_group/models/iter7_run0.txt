Here are 3 new cognitive models that incorporate STAI (anxiety) into the decision-making process using mechanisms distinct from those previously attempted.

### Model 1: Anxiety-Driven Transition Distortion
This model hypothesizes that high anxiety leads to a "determinism bias" or a distortion of the internal model of the world. While the true transition probability is 0.7, anxious individuals may perceive the world as more deterministic (closer to 1.0) to reduce cognitive uncertainty, or conversely, more chaotic (closer to 0.5). This model tests if STAI exaggerates the perceived probability of the common transition, making the Model-Based system overconfident in the transition structure.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Transition Distortion Model.
    
    This model posits that anxiety distorts the Model-Based (MB) transition matrix.
    High anxiety causes the participant to perceive the 'common' transition (usually 0.7)
    as more certain (closer to 1.0) than it actually is. This reflects an intolerance 
    of uncertainty where the brain simplifies probabilistic maps into deterministic ones.

    Parameters:
    - lr: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stai_distortion: [0, 1] The degree to which STAI pushes the perceived transition 
      probability from 0.7 towards 1.0.
    
    Bounds:
    lr: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stai_distortion: [0, 1]
    """
    lr, beta, w, stai_distortion = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Normalize STAI (approx range 20-80) to 0-1
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Calculate perceived transition probability
    # Base is 0.7. Max distortion pushes it towards 1.0.
    # If stai_distortion is high and anxiety is high, prob approaches 1.0.
    true_common_prob = 0.7
    distortion_magnitude = (1.0 - true_common_prob) * stai_distortion * norm_stai
    perceived_common = true_common_prob + distortion_magnitude
    perceived_rare = 1.0 - perceived_common
    
    # Transition matrix: Row 0=Action A (to X), Row 1=Action U (to Y)
    # A -> X is common, U -> Y is common
    transition_matrix = np.array([
        [perceived_common, perceived_rare], 
        [perceived_rare, perceived_common]
    ])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (X,Y) x Action (Alien 1, Alien 2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value: Uses the distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Net Q-value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 2 Update (Standard Q-learning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Stage 1 Update (TD-learning)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Blunted Reward Sensitivity
This model draws on the literature linking anxiety and anhedonia (reduced ability to experience pleasure). Instead of affecting how *fast* participants learn (learning rate), STAI affects the *magnitude* of the reward signal itself. High anxiety dampens the subjective value of the gold coin, making the difference between "reward" and "no reward" smaller, which flattens the value landscape.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Blunted Reward Sensitivity Model.
    
    This model posits that anxiety dampens the subjective valuation of rewards.
    While the objective reward is 1.0, highly anxious participants perceive it 
    as less valuable. This reduces the driving force (prediction error) for both 
    Model-Free and Model-Based value accumulation.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stai_blunt: [0, 1] The fraction by which max anxiety reduces reward value.
      Effective Reward = Reward * (1 - (stai_blunt * normalized_stai)).
    
    Bounds:
    lr: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stai_blunt: [0, 1]
    """
    lr, beta, w, stai_blunt = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Reward Processing ---
        # Calculate subjective reward based on anxiety
        # If stai_blunt is 0.5 and anxiety is max, a reward of 1.0 feels like 0.5.
        subjective_reward = reward[trial] * (1.0 - (stai_blunt * norm_stai))

        # --- Updates ---
        # Update Stage 2 using subjective reward
        delta_stage2 = subjective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Mediated Direct Reinforcement
Standard Model-Free learning updates the first-stage value based on the value of the second stage ($Q_{stage2}$). However, a simpler, more primitive mechanism is "Direct Reinforcement," where the final outcome (Reward) directly updates the first-stage choice, bypassing the second-stage state entirely. This model proposes that anxiety causes a regression to this simpler strategy: anxious people ignore the intermediate state (Planet) and update their spaceship choice based solely on whether they got a coin or not.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Mediated Direct Reinforcement Model.
    
    This model introduces a 'Direct Reinforcement' path for the first stage update.
    Normally, MF updates Stage 1 based on Stage 2 values (TD learning).
    Here, STAI controls a mixing parameter 'lambda_dr'. 
    High anxiety increases reliance on the raw outcome (Reward) to update Stage 1,
    ignoring the intermediate state value. This represents a reactive, 
    outcome-focused heuristic common in stress.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (MB vs MF).
    - stai_direct: [0, 1] Weight of Direct Reinforcement modulated by STAI.
      The target for Stage 1 update becomes a mix of Q_stage2 (standard) 
      and Reward (direct).
    
    Bounds:
    lr: [0, 1]
    beta: [0, 10]
    w: [0, 1]
    stai_direct: [0, 1]
    """
    lr, beta, w, stai_direct = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate the mixing weight for the update target
    # If lambda_dr is high, we update based on Reward, not Q_stage2
    lambda_dr = stai_direct * norm_stai

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 2 Update (Standard)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Stage 1 Update (Hybrid TD / Direct Reinforcement)
        # Standard TD target: q_stage2_mf[state_idx, action_2[trial]]
        # Direct Reinforcement target: reward[trial]
        
        td_target = q_stage2_mf[state_idx, action_2[trial]]
        dr_target = reward[trial]
        
        # The effective target is a weighted average based on anxiety
        combined_target = (1.0 - lambda_dr) * td_target + lambda_dr * dr_target
        
        delta_stage1 = combined_target - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```