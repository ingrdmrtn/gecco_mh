Here are 3 new cognitive models based on the two-step task, incorporating STAI (anxiety) into the decision-making mechanisms in novel ways.

### Model 1: Anxious Transition Aversion
This model posits that high-anxiety participants experience a loss of control (a rare transition) as intrinsically aversive, effectively a "negative reward," regardless of whether they eventually get a coin. This "transition punishment" affects the Model-Free system, causing them to devalue spaceships that produce unexpected outcomes.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    The Anxious Transition Aversion Model.
    
    Hypothesis: High anxiety participants find the loss of control associated 
    with 'Rare' transitions intrinsically aversive. Even if they receive a coin, 
    the rare transition itself acts as a penalty to the Model-Free value of the 
    chosen spaceship.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature (softmax).
    - w: [0, 1] Weighting between MB and MF (0=MF, 1=MB).
    - stai_aversion: [0, 1] The magnitude of the penalty applied to the reward 
      signal when a rare transition occurs, scaled by STAI.
    """
    lr, beta, w, stai_aversion = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
    
    # Normalize STAI to 0-1 range (assuming range 20-80)
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Define transition structure for reference (A->X, U->Y are common)
    # 0=A, 1=U; 0=X, 1=Y
    # Common: (0,0) and (1,1). Rare: (0,1) and (1,0).
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        
        # Determine if transition was rare
        # Action 0 (A) -> State 0 (X) is Common
        # Action 1 (U) -> State 1 (Y) is Common
        is_common = (action_1[trial] == state[trial])
        
        # Calculate effective reward for Stage 1 update
        # If rare transition, subtract aversion penalty scaled by anxiety
        effective_reward = reward[trial]
        if not is_common:
            penalty = stai_aversion * norm_stai
            effective_reward -= penalty
            
        # Stage 2 Update (Standard TD)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Stage 1 Update (Using Effective Reward for MF)
        # Note: In standard Daw task, Stage 1 MF is updated using Stage 2 Q-value (TD-0) 
        # or Reward (TD-1). Here we use the standard TD-1 like approach (direct reward) 
        # but modified by the aversion.
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # We apply the penalty to the value update directly
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # If it was a rare transition, we apply an additional "shock" to the MF value
        if not is_common:
             q_stage1_mf[action_1[trial]] -= (lr * stai_aversion * norm_stai)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxious Stage-2 Overload
This model suggests that anxiety consumes cognitive resources (working memory), but this deficit is most pronounced at the second stage of the task. While Stage 1 relies on a cached policy or model, Stage 2 requires specific discrimination of aliens. High anxiety leads to a "noisier" (more random) choice policy specifically at Stage 2.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    The Anxious Stage-2 Overload Model.
    
    Hypothesis: Anxiety acts as a cognitive load that degrades decision precision 
    specifically at the second stage (the bandit task). While the first stage 
    (planning) might remain intact, the second stage beta (inverse temperature) 
    is suppressed by high STAI, leading to more random alien choices.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - w: [0, 1] Weighting between MB and MF.
    - stai_noise_s2: [0, 1] The degree to which STAI reduces Beta at Stage 2.
      If 0, Beta_1 == Beta_2. If 1, Beta_2 is significantly reduced by anxiety.
    """
    lr, beta_1, w, stai_noise_s2 = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
    
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate Stage 2 Beta
    # As anxiety increases, beta_2 decreases (more noise/exploration)
    # We model this as a divisive suppression
    beta_2 = beta_1 / (1.0 + (stai_noise_s2 * norm_stai * 5.0)) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice (Uses beta_2 - affected by Anxiety) ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxious Tunnel Vision (Simplification)
This model proposes that anxiety causes a "sharpening" or simplification of the internal model. Unlike "entropy" which flattens probabilities to 50/50, "tunnel vision" pushes the perceived transition probabilities toward determinism (100/0). Anxious participants ignore the complexity of probabilistic transitions and assume the common transition is guaranteed, leading to rigid Model-Based planning.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    The Anxious Tunnel Vision Model.
    
    Hypothesis: Under high anxiety, participants simplify the world model to reduce 
    computational uncertainty. They perceive the 'Common' transition (0.7) as 
    more certain (closer to 1.0) and the 'Rare' transition as negligible. 
    This 'Tunnel Vision' distorts the Model-Based planning by ignoring the 
    risk of rare transitions.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - stai_tunnel: [0, 1] The degree of model sharpening. 
      0 = Standard (0.7/0.3). 
      1 = Fully Deterministic (1.0/0.0) for max anxiety.
    """
    lr, beta, w, stai_tunnel = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]
    
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate Subjective Transition Matrix
    # Standard is 0.7. Max Tunnel Vision pushes this to 1.0.
    base_prob = 0.7
    # The boost is the remaining distance to 1.0 (which is 0.3) scaled by parameters
    prob_boost = 0.3 * stai_tunnel * norm_stai
    
    subjective_common = base_prob + prob_boost
    subjective_rare = 1.0 - subjective_common
    
    # Construct the distorted MB matrix
    # If stai_tunnel is high, this approaches Identity matrix logic
    mb_transitions = np.array([
        [subjective_common, subjective_rare], 
        [subjective_rare, subjective_common]
    ])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # The MB calculation uses the distorted "Tunnel Vision" matrix
        q_stage1_mb = mb_transitions @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```