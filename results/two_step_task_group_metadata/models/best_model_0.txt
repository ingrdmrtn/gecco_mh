def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    The Pessimistic Planner Model.
    
    Anxiety (STAI) distorts the Model-Based transition matrix. 
    Instead of trusting the 70/30 structure, high anxiety participants 
    perceive the transitions as closer to 50/50 (random), degrading 
    the effectiveness of the Model-Based component.

    Parameters:
    - lr: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between MB and MF (0=MF, 1=MB).
    - stai_entropy: [0, 1] The degree to which anxiety flattens the transition matrix.
      At 0, the agent uses [0.7, 0.3]. At 1 (with max anxiety), the agent uses [0.5, 0.5].

    """
    lr, beta, w, stai_entropy = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)



    subjective_common_prob = 0.7 - (0.2 * stai_entropy * norm_stai)
    subjective_rare_prob = 1.0 - subjective_common_prob



    mb_transitions = np.array([
        [subjective_common_prob, subjective_rare_prob], 
        [subjective_rare_prob, subjective_common_prob]
    ])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (X,Y) x Action (Alien 1, Alien 2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):


        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = mb_transitions @ max_q_stage2

        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]


        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss