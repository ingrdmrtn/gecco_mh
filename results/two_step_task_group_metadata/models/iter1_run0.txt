Here are 3 new cognitive models based on the two-step task, incorporating the STAI anxiety score into different mechanisms of learning and decision-making.

### Model 1: Anxiety-Modulated Loss Aversion
This model hypothesizes that anxiety affects how participants process negative feedback. High anxiety is often associated with increased sensitivity to punishment or "loss aversion." Here, STAI modulates the learning rate specifically when the prediction error is negative (i.e., when the outcome is worse than expected).

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Loss Aversion Model.
    
    This model posits that anxiety (STAI) amplifies the learning rate specifically 
    for negative prediction errors (disappointments). Anxious individuals may 
    over-correct their value estimates after a lack of reward.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate for positive outcomes.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stai_loss_amp: [0, 1] Amplification factor for learning rate when RPE < 0.
      Effective LR_neg = lr_base + (stai_loss_amp * normalized_stai).
      (Capped at 1.0).
    
    Inputs: Standard two-step task arrays and STAI score.
    """
    lr_base, beta, w, stai_loss_amp = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Normalize STAI (approx range 20-80) to 0-1
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Calculate the specific learning rate for negative prediction errors
    lr_neg = np.clip(lr_base + (stai_loss_amp * norm_stai), 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # State (X,Y) x Action (Alien 1, Alien 2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax Stage 1
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Decision ---
        state_idx = state[trial]
        
        # Softmax Stage 2
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 2 RPE
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        
        # Determine LR based on sign of RPE
        current_lr = lr_neg if rpe_2 < 0 else lr_base
        q_stage2[state_idx, action_2[trial]] += current_lr * rpe_2
        
        # Stage 1 RPE (TD(0) update for MF)
        value_stage2_chosen = q_stage2[state_idx, action_2[trial]]
        rpe_1 = value_stage2_chosen - q_stage1_mf[action_1[trial]]
        
        # Apply same logic to Stage 1 update? 
        # Usually loss aversion applies to the final outcome, so we use the same logic.
        current_lr_1 = lr_neg if rpe_1 < 0 else lr_base
        q_stage1_mf[action_1[trial]] += current_lr_1 * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Perseveration
This model hypothesizes that anxiety leads to rigid, repetitive behavior ("stickiness") regardless of reward outcomes. This is often observed as a safety behavior or freezing response in anxious phenotypes.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Perseveration Model.
    
    This model assumes that anxiety increases 'choice stickiness' (perseveration).
    A bonus is added to the Q-value of the action chosen in the previous trial.
    The magnitude of this bonus is scaled by the STAI score.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stai_sticky: [0, 5] The magnitude of the stickiness bias contributed by anxiety.
      Stickiness Bonus = stai_sticky * normalized_stai.
    
    Inputs: Standard two-step task arrays and STAI score.
    """
    learning_rate, beta, w, stai_sticky = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate stickiness magnitude
    sticky_bonus = stai_sticky * norm_stai

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Indicator for no previous action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the net Q-values before softmax
        if last_action_1 != -1:
            q_net_stage1[last_action_1] += sticky_bonus
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker
        last_action_1 = action_1[trial]

        # --- Stage 2 Decision ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * rpe_2
        
        value_stage2_chosen = q_stage2[state_idx, action_2[trial]]
        rpe_1 = value_stage2_chosen - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Model Degradation
This model suggests that anxiety degrades the internal model of the environment. While a standard Model-Based agent knows the transition probabilities are 0.7/0.3, an anxious agent might perceive the world as more volatile or unpredictable, effectively "flattening" the transition matrix toward 0.5/0.5. This reduces the efficacy of planning without abandoning the planning system entirely.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Model Degradation.
    
    This model assumes anxiety degrades the accuracy of the internal model used for planning.
    Instead of using the true transition probability (0.7), the agent uses a 
    subjective probability that degrades toward 0.5 (randomness) as anxiety increases.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Mixing weight.
    - stai_noise: [0, 1] Factor reducing the perceived transition certainty.
      Perceived_Prob = 0.7 - (stai_noise * normalized_stai * 0.2).
      If stai_noise is high and anxiety is high, prob -> 0.5.
    
    Inputs: Standard two-step task arrays and STAI score.
    """
    learning_rate, beta, w, stai_noise = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Calculate subjective transition probability
    # True is 0.7. Max degradation is 0.7 - 0.2 = 0.5.
    subjective_p = 0.7 - (stai_noise * norm_stai * 0.2)
    subjective_p = np.clip(subjective_p, 0.5, 0.7)
    
    # Construct the degraded internal model
    # If subjective_p is 0.5, the agent believes transitions are random.
    subjective_trans_matrix = np.array([
        [subjective_p, 1 - subjective_p], 
        [1 - subjective_p, subjective_p]
    ])

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Use the SUBJECTIVE matrix for planning
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = subjective_trans_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Decision ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        rpe_2 = reward[trial] - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * rpe_2
        
        value_stage2_chosen = q_stage2[state_idx, action_2[trial]]
        rpe_1 = value_stage2_chosen - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * rpe_1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```