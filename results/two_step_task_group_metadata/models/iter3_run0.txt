Here are three new cognitive models that incorporate STAI (anxiety) into the decision-making process using mechanisms distinct from the previous iterations (which focused on learning rates, mixing weights, and stickiness).

### Model 1: Anxiety-Induced Decision Noise
This model hypothesizes that anxiety acts as a cognitive load that interferes with value-based selection. Instead of altering how values are *learned*, anxiety alters how strictly values are *followed*. High anxiety increases decision noise (lowers the inverse temperature $\beta$), making behavior more stochastic and less driven by the learned Q-values.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Decision Noise Model.
    
    This model posits that anxiety (STAI) functions as a distractor or noise generator.
    High anxiety reduces the precision of the softmax choice rule (lowers beta),
    leading to more random behavior regardless of the learned values.
    
    Parameters:
    - lr: [0, 1] Learning rate for Q-value updates.
    - beta_max: [0, 10] The maximum inverse temperature (precision) at lowest anxiety.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stai_noise: [0, 1] Factor by which anxiety reduces beta. 
      Effective Beta = beta_max * (1 - (stai_noise * normalized_stai)).
    
    Inputs: Standard two-step task arrays and STAI score.
    """
    lr, beta_max, w, stai_noise = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Normalize STAI to 0-1 range (assuming range 20-80)
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Calculate effective beta: Anxiety reduces precision
    # If stai_noise is 0, beta is constant. If 1, high anxiety drives beta toward 0.
    beta_eff = beta_max * (1.0 - (stai_noise * norm_stai))
    beta_eff = np.maximum(beta_eff, 0.0) # Ensure non-negative

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (X,Y) x Action (Alien 1, Alien 2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta_eff * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Working Memory Decay
This model relies on the theory that anxiety consumes working memory resources. In a reinforcement learning context, this is modeled as "forgetting" or value decay. While the participant learns about the chosen option, the values of *unchosen* options decay back to neutral (0.0) more rapidly for anxious individuals.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Working Memory Decay Model.
    
    This model posits that anxiety consumes working memory resources, leading to 
    faster forgetting of value estimates for options that are NOT currently chosen.
    
    Parameters:
    - lr: [0, 1] Learning rate for chosen options.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stai_decay: [0, 1] The rate at which unchosen options decay toward 0, scaled by STAI.
      Decay factor = 1 - (stai_decay * normalized_stai).
    
    Inputs: Standard two-step task arrays and STAI score.
    """
    lr, beta, w, stai_decay = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Calculate decay rate based on anxiety
    # High anxiety -> higher decay -> retention factor is smaller
    retention_factor = 1.0 - (stai_decay * norm_stai)
    retention_factor = np.clip(retention_factor, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning & Decay ---
        
        # 1. Update CHOSEN Stage 2 option
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # 2. Decay UNCHOSEN Stage 2 option (in the current state)
        unchosen_a2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_a2] *= retention_factor

        # 3. Update CHOSEN Stage 1 option
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # 4. Decay UNCHOSEN Stage 1 option
        unchosen_a1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_a1] *= retention_factor

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Transition Uncertainty
This model suggests that anxiety distorts the Model-Based (MB) system's internal model of the world. While the true transition probability is 0.7, anxious individuals may distrust this structure, perceiving the environment as more chaotic or unpredictable. This "flattens" the transition matrix they use for planning, effectively diluting the MB contribution even if the mixing weight `w` is high.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Transition Uncertainty Model.
    
    This model posits that anxiety degrades the internal model of the environment structure.
    Anxious participants perceive the transition probabilities as closer to chance (0.5),
    reducing the effectiveness of model-based planning.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stai_uncertainty: [0, 1] Degree of structural distortion.
      If 0, participant uses true 0.7/0.3 matrix.
      If 1 (and high anxiety), participant uses closer to 0.5/0.5 matrix.
    
    Inputs: Standard two-step task arrays and STAI score.
    """
    lr, beta, w, stai_uncertainty = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Distort the transition matrix based on anxiety
    # True common probability is 0.7. Max distortion moves it to 0.5.
    # Range of shift is 0.2 (from 0.7 down to 0.5).
    perceived_common_prob = 0.7 - (0.2 * stai_uncertainty * norm_stai)
    perceived_rare_prob = 1.0 - perceived_common_prob
    
    # The distorted internal model used for MB planning
    subjective_transition_matrix = np.array([
        [perceived_common_prob, perceived_rare_prob], 
        [perceived_rare_prob, perceived_common_prob]
    ])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the SUBJECTIVE (distorted) matrix for planning
        q_stage1_mb = subjective_transition_matrix @ max_q_stage2

        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```