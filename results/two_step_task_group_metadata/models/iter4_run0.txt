Here are three new cognitive models that incorporate the STAI (anxiety) score into the decision-making process using mechanisms distinct from those previously explored.

### Model 1: Anxiety-Enhanced Negative Learning
This model hypothesizes that anxiety creates a "negativity bias" in learning. While the learning rate for positive outcomes (finding gold) remains constant, the learning rate for negative outcomes (omission of reward) is amplified by the participant's anxiety level. High anxiety individuals update their value estimates more drastically when they fail to find gold.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Enhanced Negative Learning Model.
    
    This model posits that anxiety increases the learning rate specifically for 
    negative outcomes (reward = 0). High anxiety individuals are hypersensitive 
    to failure/omission, updating their Q-values more aggressively after losses 
    than after wins.

    Parameters:
    - lr_base: [0, 1] Base learning rate for positive outcomes (reward=1).
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stai_neg_boost: [0, 1] Scaling factor for negative learning. 
      LR for reward=0 is: lr_base + (stai_neg_boost * normalized_stai * (1 - lr_base)).

    Inputs: Standard two-step task arrays and STAI score.
    """
    lr_base, beta, w, stai_neg_boost = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Normalize STAI to 0-1 range (assuming range 20-80)
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Calculate the boosted learning rate for negative outcomes
    # We ensure it doesn't exceed 1.0
    lr_neg = lr_base + (stai_neg_boost * norm_stai * (1.0 - lr_base))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (X,Y) x Action (Alien 1, Alien 2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Determine which learning rate to use based on outcome
        current_lr = lr_base if reward[trial] == 1.0 else lr_neg

        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2

        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Dampened Eligibility Trace
This model focuses on the credit assignment problem. In Reinforcement Learning, an "eligibility trace" ($\lambda$) determines how much the final reward updates the first-stage choice directly. This model suggests that anxiety acts as a cognitive load that disrupts working memory, making it harder to link the final reward back to the initial spaceship choice. High anxiety reduces $\lambda$, making the agent more reliant on simple one-step updates (TD-0) rather than full trajectory updates.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Dampened Eligibility Trace Model.
    
    This model posits that anxiety interferes with the ability to maintain an 
    eligibility trace (lambda) between the first stage choice and the final reward.
    High anxiety reduces lambda, meaning the first stage is updated less by the 
    final reward and more by the immediate state transition value.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stai_lambda_damp: [0, 1] Factor by which anxiety reduces the eligibility trace.
      Lambda = 1.0 - (stai_lambda_damp * normalized_stai).

    Inputs: Standard two-step task arrays and STAI score.
    """
    lr, beta, w, stai_lambda_damp = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Calculate eligibility trace lambda
    # If stai_lambda_damp is 0, lambda is 1 (full credit assignment).
    # If stai_lambda_damp is 1 and anxiety is high, lambda approaches 0.
    eligibility_lambda = 1.0 - (stai_lambda_damp * norm_stai)
    eligibility_lambda = np.maximum(eligibility_lambda, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning with Eligibility Trace ---
        
        # 1. Calculate Stage 2 RPE (Reward Prediction Error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # 2. Calculate Stage 1 RPE (Transition Prediction Error)
        # Note: We use the Q-value of the chosen stage 2 action as the target
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]

        # 3. Update Stage 2
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # 4. Update Stage 1
        # The update includes the immediate transition error (delta_stage1)
        # PLUS the stage 2 error scaled by lambda (eligibility trace)
        q_stage1_mf[action_1[trial]] += lr * delta_stage1 + (eligibility_lambda * lr * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Transition Uncertainty
This model proposes that anxiety degrades the internal model of the world structure. Even if an anxious participant attempts to use Model-Based (MB) planning, their internal representation of the transition probabilities (which spaceship goes to which planet) becomes "fuzzier" or flatter. High anxiety flattens the transition matrix toward 50/50, rendering the MB component less effective/informative, even if the weight `w` remains high.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Transition Uncertainty Model.
    
    This model posits that anxiety degrades the precision of the internal model 
    of the environment. High anxiety causes the participant to doubt the 
    transition structure (70/30), effectively flattening the transition matrix 
    used for Model-Based planning towards uniform randomness (50/50).

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stai_uncertainty: [0, 1] Degree to which anxiety flattens the transition matrix.
      0 = Perfect knowledge (70/30), 1 = Max anxiety leads to (50/50).

    Inputs: Standard two-step task arrays and STAI score.
    """
    lr, beta, w, stai_uncertainty = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Define the true transition matrix
    true_transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Define a uniform (uncertain) matrix
    uniform_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])

    # Calculate the subjective transition matrix based on anxiety
    # As anxiety increases, the matrix shifts from true to uniform
    uncertainty_factor = stai_uncertainty * norm_stai
    subjective_transition_matrix = (1 - uncertainty_factor) * true_transition_matrix + uncertainty_factor * uniform_matrix

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the subjective (anxiety-flattened) matrix for MB calculation
        q_stage1_mb = subjective_transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```