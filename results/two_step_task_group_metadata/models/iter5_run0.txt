Here are 3 new cognitive models that explore mechanisms distinct from those previously tested (such as simple learning rate boosting or noise injection).

### Model 1: Anxiety-Driven Transition Uncertainty
This model hypothesizes that anxiety degrades the participant's internal model of the environment. While the "Model-Based" system usually relies on the known transition probabilities (70% / 30%), high anxiety causes the participant to doubt this structure, flattening their belief toward randomness (50% / 50%). This makes the Model-Based component less predictive without necessarily switching to Model-Free control.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Transition Uncertainty Model.
    
    Hypothesis: Anxiety degrades the precision of the internal model (the transition matrix).
    High STAI scores flatten the transition matrix belief from [0.7, 0.3] toward [0.5, 0.5].
    This reduces the effectiveness of Model-Based planning by introducing structural uncertainty.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB (1) and MF (0).
    - stai_uncertainty: [0, 1] How much STAI flattens the transition matrix.
      At 0, agent assumes 0.7/0.3. At 1 (with max STAI), agent assumes 0.5/0.5.
      
    """
    lr, beta, w, stai_uncertainty = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Normalize STAI (approx range 20-80) to 0-1
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Calculate effective transition probability
    # Base is 0.7. Max uncertainty pushes it to 0.5.
    # Range of reduction is 0.2 (0.7 -> 0.5).
    p_common = 0.7 - (stai_uncertainty * norm_stai * 0.2)
    p_rare = 1.0 - p_common
    
    # Dynamic transition matrix based on anxiety
    transition_matrix = np.array([[p_common, p_rare], [p_rare, p_common]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB calculation uses the anxiety-distorted transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Stage 1 Update (TD-0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Anhedonia (Reward Devaluation)
This model tests the "Anhedonia" hypothesis often associated with anxiety and depression. Instead of modulating how *fast* the agent learns (learning rate), this model modulates *what* the agent learns (the asymptote). High anxiety dampens the subjective value of a positive outcome. A coin worth "1.0" to a low-anxiety person might be worth "0.7" to a high-anxiety person, leading to lower Q-values and less deterministic choices over time.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Anhedonia Model.
    
    Hypothesis: Anxiety dampens the subjective valuation of rewards (anhedonia).
    High STAI reduces the effective reward value of a 'win' (1.0).
    This prevents Q-values from reaching their maximum, keeping the agent 
    in a state of higher exploration/uncertainty even after success.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - stai_dampening: [0, 1] The fraction by which STAI reduces reward value.
      Effective Reward = Reward * (1 - (stai_dampening * norm_stai)).
      
    """
    lr, beta, w, stai_dampening = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate subjective reward scalar
    # If stai_dampening is high and anxiety is high, reward 1.0 becomes e.g. 0.6
    reward_scalar = 1.0 - (stai_dampening * norm_stai)
    reward_scalar = np.maximum(reward_scalar, 0.0) # Prevent negative reward

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Apply anhedonia: scale the observed reward
        subjective_reward = reward[trial] * reward_scalar
        
        delta_stage2 = subjective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Accelerated Memory Decay
This model posits that anxiety acts as a cognitive load that consumes working memory resources. Consequently, anxious participants struggle to maintain value representations over time. This is modeled as a decay rate applied to all Q-values at the end of every trial, where the speed of decay is proportional to the STAI score.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Accelerated Memory Decay Model.
    
    Hypothesis: Anxiety consumes cognitive resources, leading to faster forgetting 
    of learned values. Q-values decay toward 0 (neutral/ignorance) on every trial.
    The rate of this decay is determined by STAI.
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - stai_decay: [0, 1] The magnitude of memory decay induced by anxiety.
      Q_new = Q_old * (1 - (stai_decay * norm_stai)).
      
    """
    lr, beta, w, stai_decay = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate decay factor
    decay_rate = stai_decay * norm_stai
    retention_factor = 1.0 - decay_rate

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # --- Memory Decay ---
        # All Q-values decay slightly due to anxiety/cognitive load
        q_stage1_mf *= retention_factor
        q_stage2_mf *= retention_factor

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```