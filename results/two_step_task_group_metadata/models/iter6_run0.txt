Here are 3 new cognitive models that incorporate STAI (anxiety) into the decision-making process using mechanisms distinct from previous iterations.

### Model 1: Anxiety-Modulated Memory Decay
This model hypothesizes that anxiety consumes cognitive resources (working memory), leading to faster forgetting of learned values. High anxiety causes Q-values to decay toward a neutral prior (0.5) more rapidly, representing the inability to maintain precise value representations over time.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Memory Decay Model.
    
    Hypothesis: Anxiety acts as a cognitive load, accelerating the decay of 
    learned Q-values in working memory. High STAI leads to faster regression 
    to the mean (0.5), making behavior less history-dependent.
    
    Parameters:
    - lr: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature (choice consistency).
    - w: [0, 1] Mixing weight (0=MF, 1=MB).
    - stai_decay: [0, 1] Scaling factor for memory decay.
      Effective decay rate = stai_decay * normalized_stai.
      
    """
    lr, beta, w, stai_decay = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Normalize STAI to 0-1 range (assuming range 20-80)
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate effective decay rate based on anxiety
    decay_rate = stai_decay * norm_stai

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values at neutral 0.5 (assuming rewards are 0 or 1)
    q_stage1_mf = np.full(2, 0.5)
    q_stage2_mf = np.full((2, 2), 0.5) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Decay Step ---
        # Before making a choice, values decay towards neutral (0.5)
        # This simulates anxiety-induced forgetting
        q_stage1_mf = q_stage1_mf * (1 - decay_rate) + (0.5 * decay_rate)
        q_stage2_mf = q_stage2_mf * (1 - decay_rate) + (0.5 * decay_rate)

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Subjective Utility of Loss (Anxiety Framing)
This model posits that anxiety alters the subjective valuation of outcomes. While the task provides 0 or 1 coin, anxious individuals may perceive the absence of a reward (0 coins) as a painful punishment rather than a neutral non-event. STAI scales how negative the "0 coin" outcome feels, driving avoidance behavior.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Subjective Utility of Loss Model.
    
    Hypothesis: Anxiety increases sensitivity to negative outcomes (loss aversion).
    Instead of treating 0 coins as neutral, anxious participants treat it as a 
    punishment (negative reward).
    
    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stai_punishment: [0, 5] Magnitude of subjective punishment for 0 coins.
      Effective Reward = Reward - (stai_punishment * norm_stai * (1 - Reward))
      If Reward is 1, value is 1. If Reward is 0, value is negative.
    """
    lr, beta, w, stai_punishment = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Reward Transformation ---
        # If reward is 1, term becomes 0 -> Effective reward = 1
        # If reward is 0, term becomes (stai_punishment * norm_stai) -> Effective reward = negative
        r_eff = reward[trial] - (stai_punishment * norm_stai * (1.0 - reward[trial]))

        # --- Learning ---
        delta_stage2 = r_eff - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Structural Volatility
Standard models assume the transition probabilities (70/30) are fixed. This model suggests anxious individuals perceive the environment as volatile. They actively learn the transition matrix, and STAI determines the learning rate of this structural knowledge. High anxiety leads to "over-updating" beliefs about spaceship transitions after rare events.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Structural Volatility Model.
    
    Hypothesis: Anxious individuals do not trust the stability of the environment.
    They dynamically update the transition matrix (Model-Based component).
    High STAI increases the learning rate for transitions, causing the agent 
    to drastically change its world-model after rare transitions.
    
    Parameters:
    - lr_val: [0, 1] Learning rate for Q-values (reward learning).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    - stai_trans_lr: [0, 1] Learning rate for the transition matrix.
      Scales with STAI.
    """
    lr_val, beta, w, stai_trans_lr = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Effective learning rate for the transition matrix
    lr_struct = stai_trans_lr * norm_stai

    # Initialize dynamic transition probability
    # p_A_to_X represents prob that Action 0 goes to State 0
    # We assume symmetry: p_U_to_Y = p_A_to_X
    p_A_to_X = 0.7 
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # Construct current dynamic transition matrix
        # Row 0 (Action A): [p, 1-p]
        # Row 1 (Action U): [1-p, p]
        tm = np.array([[p_A_to_X, 1.0 - p_A_to_X], 
                       [1.0 - p_A_to_X, p_A_to_X]])

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = tm @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Learning ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_val * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_val * delta_stage1
        
        # --- Structural Learning (Transition Update) ---
        # Did we observe the "common" transition structure?
        # Common: (Action A -> State X) OR (Action U -> State Y)
        is_common_structure = (action_1[trial] == 0 and state[trial] == 0) or \
                              (action_1[trial] == 1 and state[trial] == 1)
        
        outcome = 1.0 if is_common_structure else 0.0
        
        # Update belief about p(Common)
        p_A_to_X += lr_struct * (outcome - p_A_to_X)
        
        # Keep probabilities valid
        p_A_to_X = np.clip(p_A_to_X, 0.01, 0.99)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```