Here are three new cognitive models that incorporate the STAI (anxiety) score into the decision-making process using mechanisms distinct from those previously listed.

### Model 1: The "Pessimistic Planner" Model
**Cognitive Theory:** This model hypothesizes that anxiety degrades the internal representation of the environment's structure (the Model-Based component). While the actual transition probabilities are fixed (0.7/0.3), anxious individuals may perceive the world as more chaotic or unpredictable. High STAI scores flatten the transition matrix used for planning, effectively reducing the confidence of the Model-Based system in predicting which planet a spaceship will lead to.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    The Pessimistic Planner Model.
    
    Anxiety (STAI) distorts the Model-Based transition matrix. 
    Instead of trusting the 70/30 structure, high anxiety participants 
    perceive the transitions as closer to 50/50 (random), degrading 
    the effectiveness of the Model-Based component.

    Parameters:
    - lr: [0, 1] Learning rate for Q-value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weighting between MB and MF (0=MF, 1=MB).
    - stai_entropy: [0, 1] The degree to which anxiety flattens the transition matrix.
      At 0, the agent uses [0.7, 0.3]. At 1 (with max anxiety), the agent uses [0.5, 0.5].

    """
    lr, beta, w, stai_entropy = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    # Normalize STAI (approx range 20-80) to 0-1
    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)

    # Calculate the subjective transition probability for the "common" path
    # Standard is 0.7. Anxiety reduces this towards 0.5.
    # Max reduction is 0.2 (resulting in 0.5).
    subjective_common_prob = 0.7 - (0.2 * stai_entropy * norm_stai)
    subjective_rare_prob = 1.0 - subjective_common_prob
    
    # Subjective transition matrix used for MB planning
    # Rows: Spaceship A, Spaceship U. Cols: Planet X, Planet Y
    # A -> X is common, U -> Y is common
    mb_transitions = np.array([
        [subjective_common_prob, subjective_rare_prob], 
        [subjective_rare_prob, subjective_common_prob]
    ])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (X,Y) x Action (Alien 1, Alien 2)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation using the ANXIETY-DISTORTED matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = mb_transitions @ max_q_stage2

        # Net Q-value
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 2 Update (Standard Q-learning)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # Stage 1 Update (Standard TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: The "Anxious Rumination" (Direct Reinforcement) Model
**Cognitive Theory:** This model posits that anxiety causes a breakdown in the separation of task stages. Anxious participants may "ruminate" on the final outcome (coin or no coin) and directly attribute it to their first choice (the spaceship), ignoring the intermediate state (the planet). This is modeled as a direct reinforcement (eligibility trace) from the reward to the Stage 1 choice, bypassing the standard TD-chain. High anxiety increases the weight of this direct, "dumb" model-free update.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    The Anxious Rumination Model.
    
    Anxiety increases the strength of a direct "eligibility trace" from the 
    final reward to the Stage 1 choice. This bypasses the two-step structure, 
    reinforcing the spaceship choice based purely on the outcome, regardless 
    of which planet was actually visited.

    Parameters:
    - lr: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - stai_trace: [0, 1] The magnitude of the direct reinforcement caused by anxiety.
      If high, the agent updates Stage 1 values directly from the reward.

    """
    lr, beta, w, stai_trace = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate the anxiety-dependent direct learning weight
    direct_lr = lr * (stai_trace * norm_stai)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # 1. Standard Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        # 2. Standard Stage 1 Update (TD-learning from Stage 2 Q-value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # 3. Anxiety-Driven Direct Update (Rumination)
        # Directly updates Stage 1 Q-value based on the final Reward
        # This reinforces "Spaceship A -> Coin" even if the path was rare.
        direct_delta = reward[trial] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += direct_lr * direct_delta

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: The "Heuristic Override" Model
**Cognitive Theory:** Under high cognitive load or stress (anxiety), individuals often revert to simple, low-effort heuristics rather than computationally expensive Reinforcement Learning. This model proposes that anxiety triggers a "Win-Stay, Lose-Shift" (WSLS) heuristic that competes with the RL system. The final choice probability is a mixture of the RL policy (Softmax) and the WSLS rule, with the mixture weight determined by STAI.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    The Heuristic Override Model.
    
    Anxiety triggers a reversion to a primitive "Win-Stay, Lose-Shift" (WSLS) 
    strategy for the first-stage choice. The model assumes the agent follows 
    standard RL, but anxiety mixes in a probability of simply repeating the 
    last spaceship if it won, or switching if it lost, ignoring Q-values.

    Parameters:
    - lr: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between MB and MF.
    - stai_heuristic: [0, 1] The weight of the WSLS heuristic in the final decision.
      If 0, pure RL. If 1, pure WSLS (driven by anxiety).

    """
    lr, beta, w, stai_heuristic = model_parameters
    n_trials = len(action_1)
    current_stai = stai[0]

    norm_stai = (current_stai - 20.0) / 60.0
    norm_stai = np.clip(norm_stai, 0.0, 1.0)
    
    # Calculate mixture weight for the heuristic
    rho = stai_heuristic * norm_stai

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Track previous trial info for WSLS
    prev_a1 = -1
    prev_reward = -1

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        
        # 1. Calculate RL Probabilities (Standard Hybrid)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_rl = exp_q1 / np.sum(exp_q1)
        
        # 2. Calculate Heuristic Probabilities (WSLS)
        probs_wsls = np.array([0.5, 0.5]) # Default random if no history
        
        if prev_a1 != -1:
            probs_wsls = np.zeros(2)
            if prev_reward == 1.0:
                # Win-Stay: 100% prob to choose same action
                probs_wsls[prev_a1] = 1.0 
            else:
                # Lose-Shift: 100% prob to choose other action
                probs_wsls[1 - prev_a1] = 1.0
        
        # 3. Mix Policies based on Anxiety
        final_probs_1 = (1 - rho) * probs_rl + rho * probs_wsls
        
        # Safety clip to prevent log(0) if heuristic is too strong
        final_probs_1 = np.clip(final_probs_1, 1e-5, 1.0 - 1e-5)
        final_probs_1 /= np.sum(final_probs_1)
        
        p_choice_1[trial] = final_probs_1[action_1[trial]]

        # --- Stage 2 Choice (Pure RL, heuristic only affects Stage 1) ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr * delta_stage1
        
        # Store history
        prev_a1 = action_1[trial]
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```