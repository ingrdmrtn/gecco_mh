Here are three cognitive models designed to explain the participant data in the two-step decision-making task. Each model incorporates the OCI (Obsessive-Compulsive Inventory) score in a different way to hypothesize how compulsivity might affect learning and decision-making.

### Model 1: Hybrid Learner with OCI-Modulated Mixing
This model assumes participants use a mixture of Model-Based (planning) and Model-Free (habitual) strategies. The key hypothesis here is that the OCI score modulates the balance between these two systems. Specifically, higher OCI scores might bias the participant towards more habitual (Model-Free) control, making them less sensitive to the transition structure.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the mixing weight (w) is modulated by OCI.
    
    Hypothesis: High OCI scores (compulsivity) lead to a stronger reliance on Model-Free 
    (habitual) learning over Model-Based (goal-directed) planning.
    
    Parameters:
    learning_rate: [0, 1] - Rate at which Q-values are updated.
    beta: [0, 10] - Inverse temperature for softmax choice (exploration/exploitation).
    w_base: [0, 1] - Baseline weight for Model-Based control (0 = pure MF, 1 = pure MB).
    oci_sensitivity: [0, 1] - How strongly OCI reduces the Model-Based weight.
    """
    learning_rate, beta, w_base, oci_sensitivity = model_parameters
    n_trials = len(action_1)
    # Normalize OCI to be roughly 0-1 scale if raw score is large, or assume pre-normalized.
    # Here we take the first value as OCI is constant per subject.
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0] 

    # Calculate the effective mixing weight. Higher OCI reduces 'w', pushing towards MF.
    # We clip to ensure it stays in [0, 1].
    w = np.clip(w_base - (current_oci * oci_sensitivity), 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # MF values for Step 1
    q_stage2_mf = np.zeros((2, 2)) # MF values for Step 2 (State x Action)

    for trial in range(n_trials):
        # --- Step 1 Choice ---
        # Model-Based Value: Transition * Max(Stage 2 Values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Step 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Update Stage 2 (TD(0))
        # delta_2 = r - Q2(s, a2)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Update Stage 1 (TD(1) / SARSA-like logic for MF)
        # delta_1 = Q2(s, a2) - Q1(a1)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        # Note: Often a lambda parameter links stage 1 update to stage 2 RPE, 
        # but here we use a simple direct update structure for parsimony.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Perseveration Model with OCI-Driven Stickiness
This model focuses on "stickiness" or choice perseveration (repeating the previous choice regardless of reward). The hypothesis is that OCI relates to a rigid repetition of motor responses. High OCI leads to higher "stickiness" (positive perseveration) at the first stage of decision-making.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Choice Perseveration (Stickiness) modulated by OCI.
    
    Hypothesis: OCI scores correlate with 'stickiness', causing participants to 
    repeat their previous Stage 1 choice regardless of the outcome.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    pers_base: [0, 5] - Base level of choice perseveration bonus.
    oci_pers_boost: [0, 5] - Additional perseveration added per unit of OCI.
    """
    learning_rate, beta, pers_base, oci_pers_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]

    # Calculate perseveration weight
    k_stick = pers_base + (current_oci * oci_pers_boost)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Indicator for previous action

    for trial in range(n_trials):
        # --- Step 1 Choice ---
        # Add stickiness bonus to the Q-value of the previously chosen action
        q_stage1_augmented = q_stage1_mf.copy()
        if last_action_1 != -1:
            q_stage1_augmented[last_action_1] += k_stick
            
        exp_q1 = np.exp(beta * q_stage1_augmented)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Record action for next trial's stickiness
        last_action_1 = a1
        
        # --- Step 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Standard TD updates
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Dependent Learning Rates
This model suggests that OCI affects the *speed* of updating beliefs rather than the decision policy itself. Specifically, it posits that high compulsivity might lead to "over-learning" from negative outcomes or general rigidity. Here, we model OCI as scaling the learning rate: high OCI participants might have a blunted learning rate, making them slow to adapt to changing probabilities.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Based learner where the Learning Rate is modulated by OCI.
    
    Hypothesis: High OCI scores result in reduced cognitive flexibility, modeled here
    as a dampening of the learning rate. High OCI = Slower updating of alien reward probabilities.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    oci_dampening: [0, 1] - Factor by which OCI reduces the learning rate.
    """
    lr_base, beta, oci_dampening = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]

    # Calculate effective learning rate
    # As OCI increases, LR decreases.
    learning_rate = lr_base * (1.0 - (current_oci * oci_dampening))
    # Ensure LR doesn't go negative or stay 0
    learning_rate = np.clip(learning_rate, 0.01, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-Based requires learning the values of the second stage states
    q_stage2_mf = np.zeros((2, 2)) # Values of aliens at Stage 2

    for trial in range(n_trials):
        # --- Step 1 Choice (Model-Based) ---
        # Calculate expected value of each spaceship based on transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1) # Best value available on Planet X and Planet Y
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Step 2 Choice ---
        s_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning ---
        r = reward[trial]
        
        # Only update Stage 2 values based on reward. 
        # In a pure MB model, Stage 1 values are derived, not stored/updated directly.
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```