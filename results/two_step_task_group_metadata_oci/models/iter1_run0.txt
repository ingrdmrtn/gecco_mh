Here are three cognitive models implemented as Python functions. They explore different mechanisms by which Obsessive-Compulsive Inventory (OCI) scores might influence reinforcement learning: decision rigidity (Beta modulation), causal attribution (Eligibility Trace modulation), and punishment sensitivity (Asymmetric Learning Rates).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with OCI-modulated Decision Rigidity.
    
    Hypothesis: High OCI scores are associated with behavioral rigidity and intolerance 
    of uncertainty. This model posits that OCI increases the inverse temperature (beta),
    making choices more deterministic (exploiting differences in Q-values more aggressively) 
    regardless of whether the system is Model-Based or Model-Free.

    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    w: [0, 1] - Weighting between Model-Based (1) and Model-Free (0) systems.
    beta_base: [0, 10] - Baseline inverse temperature.
    oci_beta_slope: [0, 10] - Increase in beta per unit of normalized OCI.
    """
    learning_rate, w, beta_base, oci_beta_slope = model_parameters
    n_trials = len(action_1)
    
    # Normalize OCI to roughly [0, 1] range for stability
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]
    
    # OCI modulates Beta: Higher OCI -> Higher Beta (More rigid/deterministic)
    beta = beta_base + (current_oci * oci_beta_slope)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        
        # --- Stage 1 Policy (Hybrid) ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]
        
        # --- Updates ---
        # Stage 2 RPE
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated Eligibility Trace (Lambda).
    
    Hypothesis: OCI scores correlate with how participants attribute outcomes to distal causes. 
    A higher OCI might lead to stronger 'magical thinking' or hyper-connectivity (high lambda), 
    or conversely, a focus on immediate steps (low lambda). This model allows OCI to scale 
    the eligibility trace parameter lambda.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_base: [0, 1] - Baseline eligibility trace.
    oci_lambda_mod: [-1, 1] - Modulation of lambda by OCI (can be positive or negative).
    """
    learning_rate, beta, lambda_base, oci_lambda_mod = model_parameters
    n_trials = len(action_1)
    
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]
    
    # Calculate effective lambda, clipped to valid [0, 1] range
    lam = lambda_base + (current_oci * oci_lambda_mod)
    lam = np.clip(lam, 0.0, 1.0)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Pure Model-Free) ---
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Stage 1 Update with Eligibility Trace
        # Q1 is updated by the immediate prediction error (Q2 - Q1)
        # PLUS the stage 2 prediction error scaled by lambda (r - Q2)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # TD(lambda) style update for 2-step task
        q_stage1_mf[a1] += learning_rate * (delta_stage1 + lam * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with OCI-modulated Punishment Sensitivity.
    
    Hypothesis: High OCI is associated with anxiety and fear of failure. This model 
    suggests that OCI specifically amplifies the learning rate when the outcome is 
    negative (0 coins), leading to stronger avoidance learning compared to reward learning.
    
    Parameters:
    lr_base: [0, 1] - Base learning rate for positive outcomes.
    w: [0, 1] - MB/MF mixing weight.
    beta: [0, 10]
    oci_punish_boost: [0, 10] - Multiplier for learning rate when reward is 0, scaled by OCI.
    """
    lr_base, w, beta, oci_punish_boost = model_parameters
    n_trials = len(action_1)
    
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Hybrid) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        r = reward[trial]

        # --- Updates with Asymmetric Learning Rate ---
        # Determine effective learning rate for this trial
        if r == 0:
            # Punishment/No Reward: Boost LR based on OCI
            # Higher OCI = Faster learning from negative outcomes
            current_lr = lr_base * (1.0 + current_oci * oci_punish_boost)
            # Clip to prevent instability
            current_lr = min(current_lr, 1.0)
        else:
            current_lr = lr_base

        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```