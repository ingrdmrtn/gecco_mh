Here are three new cognitive models exploring different mechanisms for how OCI scores might influence decision-making in the two-step task, distinct from the previously tested parameter combinations.

### Model 1: Hybrid Learner with OCI-Modulated Model-Based Weight (`w`)
This model assumes that participants use a hybrid of Model-Free (MF) and Model-Based (MB) strategies. The core hypothesis is that OCI scores specifically impair or enhance the ability to rely on the Model-Based system (planning using the transition matrix). High OCI might lead to cognitive rigidity or reliance on habit (MF), or conversely, hyper-vigilance (MB). Here, we model `w` (the mixing weight) as a function of OCI.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner where the weighting parameter 'w'
    is modulated by OCI.
    
    Hypothesis: OCI affects the balance between goal-directed (MB) and habitual (MF)
    control. Higher OCI might shift the balance towards one system.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    w_intercept: [0, 1] - Base weight for Model-Based control.
    w_slope: [-1, 1] - How OCI changes the weight (can be negative or positive).
    """
    learning_rate, beta, w_intercept, w_slope = model_parameters
    n_trials = len(action_1)
    # Normalize OCI roughly to 0-1 range for stability if it's large
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]

    # Calculate mixing weight w, constrained to [0, 1]
    w = w_intercept + (w_slope * current_oci)
    w = np.clip(w, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # Calculate Model-Based values: V(state) = max(Q_stage2(state))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update stage 2 values
        r = reward[trial]
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Update stage 1 MF values (SARSA-like logic using stage 2 value)
        # Note: Standard two-step usually updates MF stage 1 based on Q(s2, a2) or Reward
        # Here we use the standard TD(0) update towards the value of the state reached
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Second-Stage Learning Rate (`lr_stage2`)
This model hypothesizes that OCI affects how sensitive participants are to the immediate feedback received at the second stage. Individuals with high OCI might be more perfectionistic or sensitive to errors/rewards, leading to a higher learning rate specifically for the alien-choice stage (where the reward actually happens), effectively "over-updating" on recent outcomes.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Pure Model-Free learner where the Stage 2 learning rate is modulated by OCI.
    
    Hypothesis: OCI scores relate to sensitivity to immediate feedback. 
    High OCI subjects might update their valuation of the aliens (Stage 2) 
    more aggressively than low OCI subjects.
    
    Bounds:
    lr_base: [0, 1] - Base learning rate.
    beta: [0, 10]
    oci_lr_scale: [0, 2] - Multiplier for learning rate based on OCI.
    """
    lr_base, beta, oci_lr_scale = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]
  
    # Calculate specific learning rate for stage 2 based on OCI
    # We clip it to ensure it stays valid [0, 1]
    lr_stage2 = np.clip(lr_base * (1.0 + current_oci * oci_lr_scale), 0.0, 1.0)
    # Stage 1 uses the base rate
    lr_stage1 = lr_base

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        # Pure MF for stage 1 in this model formulation
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        r = reward[trial]
        
        # Action value updating for choice 2 using OCI-modulated rate
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += lr_stage2 * delta_stage2
        
        # Action value updating for choice 1 using base rate
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_stage1 * delta_stage1
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (`lambda`)
This model introduces an eligibility trace parameter ($\lambda$) for Temporal Difference learning, where the decay rate is modulated by OCI. An eligibility trace determines how much credit the Stage 1 choice gets for the Stage 2 reward directly. A high $\lambda$ (near 1) makes the update look like Monte Carlo (direct reinforcement of Stage 1 by reward), while $\lambda=0$ is pure TD learning. This tests if OCI affects the temporal credit assignment window.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with Eligibility Traces (TD(lambda)), where lambda
    is modulated by OCI.
    
    Hypothesis: OCI affects the 'credit assignment' window. High OCI might lead 
    to stronger direct association between the final reward and the initial choice
    (higher lambda), bypassing the state transition structure.
    
    Bounds:
    learning_rate: [0, 1]
    beta: [0, 10]
    lambda_intercept: [0, 1] - Base eligibility trace decay.
    lambda_slope: [-1, 1] - Effect of OCI on lambda.
    """
    learning_rate, beta, lambda_intercept, lambda_slope = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]
  
    # Calculate lambda, constrained to [0, 1]
    lam = lambda_intercept + (lambda_slope * current_oci)
    lam = np.clip(lam, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        r = reward[trial]
        
        # Calculate Prediction Errors
        # PE2: The surprise at the second stage outcome
        pe2 = r - q_stage2_mf[state_idx, a2]
        
        # PE1: The surprise at the transition to state 2
        pe1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Update Stage 2 Q-values (standard TD)
        q_stage2_mf[state_idx, a2] += learning_rate * pe2
        
        # Update Stage 1 Q-values (TD(lambda))
        # The update combines the immediate transition error (pe1)
        # plus a portion (lambda) of the second stage error (pe2)
        q_stage1_mf[a1] += learning_rate * (pe1 + lam * pe2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```