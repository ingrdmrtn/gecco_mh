Here are 3 new cognitive models based on the two-step task, incorporating OCI in novel ways distinct from the previous feedback.

### Model 1: Model-Free with OCI-Modulated Memory Decay
This model hypothesizes that OCI scores relate to the persistence of memory (or inability to let go of old values). While the chosen option is updated via a learning rate, the *unchosen* option decays. OCI modulates how quickly the participant "forgets" the value of the spaceship or alien they didn't choose.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated Memory Decay.
    
    Hypothesis: OCI affects the rate at which value traces for UNCHOSEN options 
    decay. High OCI might lead to rigid retention of old values (low decay), 
    or anxiety-driven forgetting (high decay).
    
    Parameters:
    learning_rate: [0, 1] - Standard update rate for chosen options.
    beta: [0, 10] - Inverse temperature.
    decay_base: [0, 1] - Baseline decay rate for unchosen options.
    oci_decay_slope: [-1, 1] - How OCI modulates the decay rate.
    """
    learning_rate, beta, decay_base, oci_decay_slope = model_parameters
    n_trials = len(action_1)
    
    # Normalize OCI roughly to 0-1 range for calculation stability
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]
    
    # Calculate effective decay rate, bounded [0, 1]
    decay_rate = decay_base + (current_oci * oci_decay_slope)
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
  
    # Pure MF model, so we ignore the transition matrix logic for Q-value calculation,
    # but we must retain the variable for the template structure if needed.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2) # Values for Spaceships A, U
    q_stage2_mf = np.zeros((2, 2)) # Values for Aliens (Planet X: W,S; Planet Y: P,H)

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Pure Model-Free choice for Stage 1 in this model variant
        exp_q1 = np.exp(beta * q_stage1_mf)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        
        # 1. Update Stage 1 (TD(1) style - direct reinforcement from outcome)
        # Note: Standard TD(1) often used in simple MF analyses of this task
        delta_stage1 = reward[trial] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay the unchosen Stage 1 option
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_rate)

        # 2. Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay the unchosen Stage 2 option (on the current planet)
        unchosen_a2 = 1 - a2
        q_stage2_mf[state_idx, unchosen_a2] *= (1.0 - decay_rate)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Hybrid Model with OCI-Distorted Transition Beliefs
This model proposes that OCI affects the Model-Based component not by changing the weight $w$, but by distorting the internal model of the world. High OCI participants may exhibit "black-and-white" thinking (overestimating the certainty of the common transition) or "catastrophizing" (underestimating it), changing how they value Stage 1 options.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid learner where OCI distorts the Model-Based Transition Matrix.
    
    Hypothesis: OCI scores correlate with a distortion of probability estimation.
    Participants may view the 70/30 transition as more deterministic (e.g., 90/10)
    or more random (e.g., 50/50), affecting their Model-Based value calculation.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Weighting between MB and MF (0=MF, 1=MB).
    distortion: [-0.5, 0.3] - Shift applied to the 0.7 probability based on OCI.
    """
    learning_rate, beta, w, distortion = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]
  
    # Modulate the perceived transition probability
    # Base is 0.7. OCI adds/subtracts from this belief.
    # We clip to ensure probabilities stay valid (e.g., between 0.01 and 0.99)
    p_common_belief = 0.7 + (current_oci * distortion)
    p_common_belief = np.clip(p_common_belief, 0.01, 0.99)
    
    # Internal model of transitions used by the participant
    transition_matrix = np.array([
        [p_common_belief, 1.0 - p_common_belief], 
        [1.0 - p_common_belief, p_common_belief]
    ])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Calculate Model-Based values using the distorted transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # --- Updates ---
        # MF Update for Stage 1 (SARSA-style or TD(1), using Q-value of state 2)
        # Using the value of the chosen option in stage 2 to update stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # MF Update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hybrid Model with OCI-Driven Stage 2 Perseveration
Previous models looked at Stage 1 stickiness. This model hypothesizes that OCI-related compulsivity manifests as "checking" or sticking to the specific *Alien* (Stage 2) previously chosen, regardless of the planet reached or the spaceship used.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Learner with OCI-modulated Stage 2 (Alien) Perseveration.
    
    Hypothesis: OCI drives stickiness specifically at the level of the final 
    action (Alien choice). High OCI participants will repeat their alien choice 
    from the previous trial if they encounter the same planet, effectively 
    adding a 'habit' bonus to the Q-values in Stage 2.
    
    Parameters:
    learning_rate: [0, 1]
    beta: [0, 10]
    w: [0, 1] - Hybrid weight.
    st2_pers_base: [0, 5] - Base perseveration bonus for Stage 2.
    oci_pers_mod: [0, 5] - OCI modulation of the perseveration bonus.
    """
    learning_rate, beta, w, st2_pers_base, oci_pers_mod = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] / 100.0 if oci[0] > 1 else oci[0]
  
    # Calculate the Stickiness Bonus
    k_stick = st2_pers_base + (current_oci * oci_pers_mod)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track the last alien chosen on each planet to apply specific stickiness
    # Initialize with -1 (no previous choice)
    last_action_2_on_planet = [-1, -1] 

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        a1 = int(action_1[trial])
        state_idx = int(state[trial])

        # --- Stage 2 Policy ---
        # Copy Q-values to add temporary stickiness bonus
        q_stage2_aug = q_stage2_mf[state_idx].copy()
        
        # If we have chosen an alien on this planet before, add stickiness bonus
        prev_a2 = last_action_2_on_planet[state_idx]
        if prev_a2 != -1:
            q_stage2_aug[prev_a2] += k_stick
            
        exp_q2 = np.exp(beta * q_stage2_aug)
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # Update history
        last_action_2_on_planet[state_idx] = a2
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```