Here are three new cognitive models that explore different mechanisms for how OCI might influence decision-making in this two-step task, distinct from previous attempts.

### Model 1: Hybrid Learner with OCI-modulated Model-Based Weight
This model tests the hypothesis that higher OCI scores lead to a reliance on habitual (model-free) control over goal-directed (model-based) planning. It implements a hybrid reinforcement learning agent where the weighting parameter `w` (which balances model-based and model-free values) is modulated by the participant's OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid Model-Based/Model-Free learner with OCI-modulated mixing weight.
    
    Hypothesis: OCI scores modulate the balance between goal-directed (Model-Based)
    and habitual (Model-Free) control. Specifically, higher OCI reduces the 
    weight (w) given to Model-Based values.
    
    Parameters:
    learning_rate: [0,1] Value update rate for both MB and MF values.
    beta: [0,10] Inverse temperature for softmax choice.
    w_base: [0,1] Baseline mixing weight (0=Pure MF, 1=Pure MB).
    oci_w_penalty: [0,1] Reduction in MB weighting per unit of normalized OCI.
    """
    learning_rate, beta, w_base, oci_w_penalty = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] / 20.0 # Normalize OCI roughly to [0,1] range

    # Calculate effective w, bounded between 0 and 1
    # Higher OCI reduces w, making behavior more Model-Free
    w = w_base - (oci_w_penalty * current_oci)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2)) # Used for both MB and MF calculation

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation: V_MB = T * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value: Q_net = w * Q_MB + (1-w) * Q_MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        
        if a2 == -1: # Missing data handling
            p_choice_2[trial] = 1.0
            r = 0
        else:
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]

            # --- Learning ---
            # Update Stage 2 values (common to both systems)
            # PE2 = Reward - Q_stage2
            delta_stage2 = r - q_stage2[state_idx, a2]
            q_stage2[state_idx, a2] += learning_rate * delta_stage2
            
            # Update Stage 1 MF values (SARSA-like update)
            # PE1 = Q_stage2(chosen) - Q_stage1_MF
            # Note: In standard Daw 2011, MF update uses Q(s2, a2) not max Q(s2)
            delta_stage1 = q_stage2[state_idx, a2] - q_stage1_mf[a1]
            q_stage1_mf[a1] += learning_rate * delta_stage1
            
            # Eligibility trace: stage 1 also learns from stage 2 RPE
            q_stage1_mf[a1] += learning_rate * 0.5 * delta_stage2 # Lambda usually fixed or param, using 0.5 here for simplicity

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Model-Free Learner with OCI-modulated Learning Rate Asymmetry
This model investigates if OCI affects how participants learn from positive versus negative outcomes. It proposes that higher OCI scores might lead to a "negativity bias" or hypersensitivity to punishment (or lack of reward), modeled by increasing the learning rate specifically for negative prediction errors.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Free learner with OCI-modulated asymmetric learning rates.
    
    Hypothesis: OCI is associated with altered sensitivity to prediction errors.
    Here, OCI specifically boosts the learning rate for negative prediction errors
    (disappointments), reflecting a potential negativity bias or perfectionism.
    
    Parameters:
    alpha_base: [0,1] Baseline learning rate for positive prediction errors.
    beta: [0,10] Inverse temperature.
    oci_neg_alpha_boost: [0,1] Additional learning rate added for negative PEs based on OCI.
    """
    alpha_base, beta, oci_neg_alpha_boost = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] / 20.0

    # Define effective learning rates
    # Alpha positive is the base rate
    alpha_pos = alpha_base
    
    # Alpha negative is base + boost from OCI
    # We clip to ensure it stays valid [0,1]
    alpha_neg = np.clip(alpha_base + (current_oci * oci_neg_alpha_boost), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        exp_q1 = np.exp(beta * q_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        
        if a2 == -1:
            p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]

            # --- Learning ---
            
            # Stage 2 Update
            delta_stage2 = r - q_stage2[state_idx, a2]
            
            # Apply asymmetric learning rate
            lr_s2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
            q_stage2[state_idx, a2] += lr_s2 * delta_stage2
            
            # Stage 1 Update (TD(1) / Sarsa-like)
            # Using the stage 2 value to update stage 1
            delta_stage1 = q_stage2[state_idx, a2] - q_stage1[a1]
            
            # Apply asymmetric learning rate
            lr_s1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
            q_stage1[a1] += lr_s1 * delta_stage1
            
            # Note: For simplicity in this specific variant, we update Stage 1 
            # based on the transition to stage 2 value, not the final reward directly 
            # via eligibility trace, to isolate the PE asymmetry effect at each step.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Model-Based Learner with OCI-modulated Transition Learning
This model posits that OCI affects the *state estimation* process rather than the value function directly. High OCI might be associated with a rigidity in beliefs about the world structure. This model allows the transition matrix (the map of which spaceship goes to which planet) to be updated, but OCI dampens this learning rate, representing a resistance to updating structural beliefs (rigidity).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model-Based learner with dynamic transition learning modulated by OCI.
    
    Hypothesis: Participants learn the transition structure (Spaceship -> Planet)
    over time rather than assuming it is fixed at 70/30. However, high OCI is 
    associated with cognitive rigidity, reducing the rate at which they update 
    their internal model of these transitions.
    
    Parameters:
    value_lr: [0,1] Learning rate for reward values (Stage 2 Q-values).
    beta: [0,10] Inverse temperature.
    trans_lr_base: [0,1] Base learning rate for the transition matrix.
    oci_rigidity: [0,1] How much OCI dampens transition learning.
    """
    value_lr, beta, trans_lr_base, oci_rigidity = model_parameters
    n_trials = len(action_1)
    current_oci = oci[0] / 20.0
    
    # Effective transition learning rate
    # High OCI reduces learning rate (increases rigidity)
    trans_lr = trans_lr_base * (1.0 - (current_oci * oci_rigidity))
    trans_lr = np.clip(trans_lr, 0.0, 1.0)

    # Initialize internal model of transitions (starts uniform or with slight prior)
    # Rows: Action 1 (0 or 1), Cols: State (Planet 0 or 1)
    # Initializing at 0.5 means they have no initial bias about structure
    transition_counts = np.array([[0.5, 0.5], [0.5, 0.5]]) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage2 = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # Normalize counts to get probabilities for the current model
        row_sums = transition_counts.sum(axis=1, keepdims=True)
        est_transition_matrix = transition_counts / row_sums

        # --- Stage 1 Choice (Pure Model-Based) ---
        # V_MB = Estimated_Transition_Matrix * max(Q_stage2)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = est_transition_matrix @ max_q_stage2
        
        exp_q1 = np.exp(beta * q_stage1_mb)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        
        if a2 == -1:
            p_choice_2[trial] = 1.0
        else:
            p_choice_2[trial] = probs_2[a2]
            r = reward[trial]

            # --- Value Learning ---
            delta_stage2 = r - q_stage2[state_idx, a2]
            q_stage2[state_idx, a2] += value_lr * delta_stage2
            
            # --- Structure Learning ---
            # Update the count for the observed transition (a1 -> state_idx)
            # We implement this as a leaky integration / learning rate update on probabilities
            # Instead of raw counts, we nudge the row towards the observed outcome
            
            # Create a one-hot vector for the observed state
            observed_transition = np.zeros(2)
            observed_transition[state_idx] = 1.0
            
            # Update the row corresponding to the chosen action
            # New_probs = Old_probs + lr * (Observed - Old_probs)
            # We map this back to counts conceptually or just update the matrix directly
            # For this implementation, we update the matrix row directly using the LR
            current_row = est_transition_matrix[a1]
            updated_row = current_row + trans_lr * (observed_transition - current_row)
            
            # To maintain the "counts" logic for the next loop (though we just used direct update)
            # we can just set the matrix row, as the loop recalculates from 'transition_counts'.
            # To make it compatible with the loop structure:
            transition_counts[a1] = updated_row 
            # Note: Since we are updating probabilities directly, the row_sums logic at start of loop
            # will just divide by 1.0, which is fine.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```