Here are the three cognitive models based on the two-step task structure, incorporating the OCI (Obsessive-Compulsive Inventory) score to explain variations in Model-Based/Model-Free balance, Perseveration, and Learning Rates.

### Model 1: OCI-Modulated Hybrid Model (Mixing Weight)
This model hypothesizes that the balance between Model-Based (goal-directed) and Model-Free (habitual) control is modulated by the participant's OCI score. A higher OCI score may shift the participant towards more habitual (Model-Free) or more rigid (Model-Based) strategies.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the mixing weight (w) between Model-Based and 
    Model-Free values is modulated by the OCI score.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w_base: [0, 1] Baseline mixing weight (0=Pure MF, 1=Pure MB).
    - w_oci_slope: [-1, 1] Effect of OCI on mixing weight.
    """
    learning_rate, beta, w_base, w_oci_slope = model_parameters
    
    # Data preprocessing
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    
    # Extract scalar OCI and normalize effect
    subject_oci = oci[0]
    
    # Calculate mixing weight w (bounded 0 to 1 via sigmoid-like clipping or transformation)
    # Here we use a linear combination clamped to [0,1]
    w = w_base + (w_oci_slope * subject_oci)
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Initialize Q-values
    # Stage 1: 2 options (Spaceships)
    q_stage1_mf = np.zeros(2) 
    # Stage 2: 2 states (Planets) x 2 options (Aliens)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for trial in range(n_trials):
        # Skip invalid trials (missing data marked as -1)
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 1.0
            p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        # Model-Based Value: Transition * Max(Stage2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Integrated Value: Weighted sum of MB and MF
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        
        # Softmax Choice 2 (Pure Model-Free at Stage 2)
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # TD(0) update for Stage 1 (SARSA-style using Stage 2 value)
        # Note: We use the value of the chosen stage 2 action
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # TD update for Stage 2 (Reward prediction error)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    # Return Negative Log Likelihood
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Driven Perseveration Model
This model hypothesizes that OCI relates to "stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of the outcome. The model adds a "stickiness" bonus to the Q-value of the previously chosen spaceship, with the magnitude of this bonus determined by the OCI score.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model with a perseveration (stickiness) bonus modulated by OCI.
    High OCI may lead to higher repetition of previous choices.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1] Fixed mixing weight.
    - stick_base: [0, 5] Baseline stickiness bonus.
    - stick_oci: [-1, 1] Effect of OCI on stickiness.
    """
    learning_rate, beta, w, stick_base, stick_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate Stickiness Bonus
    stickiness_bonus = stick_base + (stick_oci * subject_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10
    
    # Track previous choice (initialize as -1 or random)
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 1.0; p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Base Q-values
        q_vals = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add Stickiness Bonus to the previously chosen action
        if last_action_1 != -1:
            q_vals[last_action_1] += stickiness_bonus
            
        exp_q1 = np.exp(beta * q_vals)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update tracker
        last_action_1 = action_1[trial]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Learning Rate Model
This model hypothesizes that OCI affects the speed at which participants update their beliefs (learning rate). High OCI might correlate with hyper-correction (high learning rate) or rigidity (low learning rate). The learning rate is dynamically calculated based on the OCI score.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where the Learning Rate (alpha) is modulated by OCI.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate.
    - lr_oci: [-1, 1] Effect of OCI on learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    """
    lr_base, lr_oci, beta, w = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate Dynamic Learning Rate
    # Using a sigmoid-style transformation or clipping to keep it in [0,1]
    # Here we use simple linear scaling with clipping
    current_lr = lr_base + (lr_oci * subject_oci)
    current_lr = np.clip(current_lr, 0.01, 0.99)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 1.0; p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates (Using OCI-modulated Learning Rate) ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```