Here are three new cognitive models that incorporate the OCI score to explain participant behavior in the two-step task.

### Model 1: OCI-Modulated Decision Rigidity (Beta)
This model hypothesizes that OCI scores correlate with "cognitive rigidity" or a desire for determinism. High OCI individuals may exhibit less exploration and higher exploitation, resulting in a higher inverse temperature ($\beta$) in the softmax decision rule.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Decision Rigidity.
    
    This model posits that OCI modulates the inverse temperature (beta) of the 
    softmax choice function. High OCI scores lead to higher beta values, 
    resulting in more deterministic (rigid) choices and reduced exploration.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta_base: [0, 10] - Baseline inverse temperature.
    beta_oci: [0, 1] - Slope of OCI effect on beta.
    w: [0, 1] - Mixing weight between Model-Based (1) and Model-Free (0).
    """
    learning_rate, beta_base, beta_oci, w = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective beta based on OCI
    # We assume OCI increases rigidity (higher beta)
    beta_eff = beta_base + (beta_oci * oci_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        state_idx = int(state[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Use effective beta modulated by OCI
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Punishment Sensitivity
This model hypothesizes that OCI relates to a "fear of failure" or hypersensitivity to negative outcomes. In this model, OCI modulates the learning rate specifically when the prediction error is negative (i.e., when the outcome is worse than expected, such as receiving 0 coins). High OCI leads to faster learning from "punishments" (omission of reward).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Punishment Sensitivity.
    
    This model posits that OCI increases the learning rate specifically for 
    negative prediction errors (disappointments/punishments). High OCI 
    individuals update their values more drastically when they receive 
    outcomes worse than expected (e.g., 0 coins).
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate for positive updates.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (MB vs MF).
    neg_bias_oci: [0, 1] - Scaling factor for learning rate during negative errors.
    """
    learning_rate, beta, w, neg_bias_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        state_idx = int(state[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Determine effective learning rate for Stage 2 update
        # If prediction error is negative (disappointment), boost LR by OCI factor
        if delta_stage2 < 0:
            eff_lr = learning_rate * (1.0 + (neg_bias_oci * oci_score))
            # Clamp eff_lr to reasonable bound (e.g., 1.0) to prevent instability
            if eff_lr > 1.0: eff_lr = 1.0
        else:
            eff_lr = learning_rate
            
        q_stage2_mf[state_idx, a2] += eff_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Eligibility Trace (Habit Strength)
This model hypothesizes that OCI correlates with stronger habit formation, represented by a stronger eligibility trace ($\lambda$). A higher trace means the final reward (Stage 2 outcome) reinforces the initial choice (Stage 1 spaceship) directly, bypassing the two-step structure. This leads to behavior that is more "model-free" in a direct stimulus-response sense, ignoring the transition structure.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Eligibility Trace (Habit Strength).
    
    This model posits that OCI modulates the eligibility trace parameter (lambda).
    High OCI scores increase lambda, meaning the Stage 1 choice is more strongly 
    reinforced directly by the Stage 2 reward (ignoring the transition structure).
    This mimics strong habit formation.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (MB vs MF).
    lambda_oci: [0, 1] - Scaling factor for eligibility trace strength based on OCI.
    """
    learning_rate, beta, w, lambda_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        state_idx = int(state[trial])

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[a1]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[a2]
  
        # --- Value Updating ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        
        # Calculate Stage 2 PE temporarily for the trace update
        delta_stage2_curr = reward[trial] - q_stage2_mf[state_idx, a2]
        
        # Calculate effective lambda based on OCI
        # lambda_val determines how much the final reward updates the first choice directly
        lambda_val = lambda_oci * oci_score
        if lambda_val > 1.0: lambda_val = 1.0
        
        # Update Stage 1: Standard SARSA + Eligibility Trace term
        q_stage1_mf[a1] += learning_rate * delta_stage1 + (learning_rate * lambda_val * delta_stage2_curr)
        
        delta_stage2 = delta_stage2_curr # Use the calculated value
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```