Here are three new cognitive models for the two-step task. These models explore how the OCI score might modulate decision noise (beta), the balance between model-based and model-free control (w), and the specific learning rate for reward outcomes.

### Model 1: OCI-Modulated Decision Noise (Beta) with Stickiness
This model hypothesizes that OCI levels influence the "temperature" of the decision process. High OCI might lead to more rigid, deterministic choices (higher beta) or, conversely, more erratic behavior due to anxiety (lower beta). It includes a fixed stickiness parameter as the baseline behavior suggests perseveration is important.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where OCI modulates the inverse temperature (beta).
    Includes constant stickiness and mixing weight.
    
    Parameters:
    - learning_rate: [0, 1] Alpha for value updates.
    - w: [0, 1] Weighting between MB (1) and MF (0).
    - stick: [0, 5] Constant choice perseverance bonus.
    - beta_base: [0, 10] Baseline inverse temperature.
    - beta_oci: [-5, 5] Effect of OCI on beta.
    """
    learning_rate, w, stick, beta_base, beta_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]

    # Calculate subject-specific beta based on OCI
    # Clip to ensure beta remains within valid positive bounds [0, 20] for stability
    beta_val = beta_base + (beta_oci * subject_oci)
    beta_val = np.clip(beta_val, 0.0, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 1.0; p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add stickiness bonus to the previously chosen action
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        exp_q1 = np.exp(beta_val * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta_val * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 MF update (TD-error based on stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF update (Prediction error based on reward)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Model-Based Weight (w) with Constant Stickiness
This model builds on the previous findings that stickiness is important, but hypothesizes that the core deficit in high OCI is the balance between habitual (MF) and goal-directed (MB) control. It modulates the mixing weight $w$ by OCI, while keeping stickiness constant.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where OCI modulates the Model-Based weight (w).
    Includes a constant stickiness parameter.
    
    Parameters:
    - learning_rate: [0, 1] Alpha for value updates.
    - beta: [0, 10] Inverse temperature.
    - stick: [0, 5] Constant stickiness bonus.
    - w_base: [0, 1] Baseline mixing weight (0=MF, 1=MB).
    - w_oci: [-1, 1] Slope of OCI effect on w.
    """
    learning_rate, beta, stick, w_base, w_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]

    # Calculate subject-specific mixing weight w
    w_val = w_base + (w_oci * subject_oci)
    w_val = np.clip(w_val, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10
    
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 1.0; p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values using OCI-modulated weight
        q_net = (w_val * q_stage1_mb) + ((1 - w_val) * q_stage1_mf)
        
        # Add constant stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 2 Learning Rate
This model hypothesizes that OCI specifically affects how participants learn from direct outcomes (Stage 2 reward prediction errors), distinct from how they update Stage 1 values. It splits the learning rate into `lr_stage1` (fixed) and `lr_stage2` (modulated by OCI).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model with separate learning rates for Stage 1 and Stage 2.
    OCI modulates the Stage 2 learning rate (learning from direct reward).
    
    Parameters:
    - lr_stage1: [0, 1] Fixed learning rate for Stage 1.
    - lr_stage2_base: [0, 1] Baseline learning rate for Stage 2.
    - lr_stage2_oci: [-1, 1] Effect of OCI on Stage 2 learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Mixing weight.
    """
    lr_stage1, lr_stage2_base, lr_stage2_oci, beta, w = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]

    # Calculate Stage 2 specific learning rate
    lr_stage2 = lr_stage2_base + (lr_stage2_oci * subject_oci)
    lr_stage2 = np.clip(lr_stage2, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 1.0; p_choice_2[trial] = 1.0
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Update Stage 1 using lr_stage1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Update Stage 2 using OCI-modulated lr_stage2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```