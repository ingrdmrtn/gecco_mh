Here are the 3 cognitive models implemented as Python functions.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where OCI modulates the Eligibility Trace (lambda).
    
    This model tests the hypothesis that OCI scores influence the degree to which 
    outcomes at the second stage reinforce the first-stage choice directly (Model-Free credit assignment).
    A higher lambda implies stronger reinforcement of the initial choice by the final reward.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    - stick: [0, 5] Stickiness parameter (choice perseveration).
    - lambda_base: [0, 1] Baseline eligibility trace parameter.
    - lambda_oci: [-1, 1] Slope of OCI effect on lambda.
    """
    learning_rate, beta, w, stick, lambda_base, lambda_oci = model_parameters
    
    # Data preprocessing
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Parameter transformation with bounds
    lambda_val = lambda_base + (lambda_oci * subject_oci)
    lambda_val = np.clip(lambda_val, 0.0, 1.0)
    
    # Initialization
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2) 
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid value construction
        q_net_1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        if last_action_1 != -1:
            q_net_1[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # 1. Update Stage 1 MF value based on Stage 2 value (TD)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Update Stage 2 MF value based on Reward
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # 3. Eligibility Trace: Update Stage 1 MF value based on Reward (Monte Carlo-like)
        q_stage1_mf[action_1[trial]] += learning_rate * lambda_val * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model with Asymmetric Learning Rates modulated by OCI.
    
    This model tests if OCI scores differentially affect learning from negative outcomes (losses)
    versus positive outcomes (wins). It uses separate learning rates for positive and negative 
    prediction errors, with OCI modulating the sensitivity to losses.
    
    Parameters:
    - lr_win: [0, 1] Learning rate for positive prediction errors.
    - lr_loss_base: [0, 1] Baseline learning rate for negative prediction errors.
    - lr_loss_oci: [-1, 1] Slope of OCI effect on loss learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stick: [0, 5] Stickiness parameter.
    """
    lr_win, lr_loss_base, lr_loss_oci, beta, w, stick = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate OCI-modulated loss learning rate
    lr_loss = lr_loss_base + (lr_loss_oci * subject_oci)
    lr_loss = np.clip(lr_loss, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        if last_action_1 != -1:
            q_net_1[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        alpha_1 = lr_win if delta_stage1 >= 0 else lr_loss
        q_stage1_mf[action_1[trial]] += alpha_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        alpha_2 = lr_win if delta_stage2 >= 0 else lr_loss
        q_stage2_mf[state_idx, action_2[trial]] += alpha_2 * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Hybrid RL model where OCI modulates the Discount Factor (gamma).
    
    This model tests if OCI scores relate to temporal myopia or farsightedness. 
    The discount factor gamma controls how much the value of the second stage 
    contributes to the value of the first stage.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-Based weight.
    - stick: [0, 5] Stickiness parameter.
    - gamma_base: [0, 1] Baseline discount factor.
    - gamma_oci: [-1, 1] Slope of OCI effect on gamma.
    """
    learning_rate, beta, w, stick, gamma_base, gamma_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Parameter transformation
    gamma = gamma_base + (gamma_oci * subject_oci)
    gamma = np.clip(gamma, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # MB Calculation: Includes discounting of the future stage value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ (gamma * max_q_stage2)
        
        q_net_1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        if last_action_1 != -1:
            q_net_1[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        
        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        # MF1 Update: TD target is discounted Q2 value
        delta_stage1 = (gamma * q_stage2_mf[state_idx, action_2[trial]]) - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # MF2 Update: Standard
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```