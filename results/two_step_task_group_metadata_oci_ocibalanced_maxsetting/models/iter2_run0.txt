Here are three new cognitive models based on the participant data and the provided OCI scores.

### Model 1: OCI-Modulated Punishment Sensitivity
This model hypothesizes that OCI scores correlate with an asymmetric learning rate for negative outcomes. Individuals with higher OCI scores may react more strongly to "disappointment" (omission of reward) than to receiving a reward. This is implemented by boosting the learning rate specifically when the prediction error is negative (i.e., when the outcome is worse than expected).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Punishment Sensitivity.
    
    This model assumes that OCI modulates how strongly participants learn from 
    negative prediction errors (punishments/omission of rewards). High OCI 
    increases the learning rate specifically when the reward is less than expected.

    Parameters:
    learning_rate: [0, 1] - Base learning rate for positive updates.
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    w: [0, 1] - Weighting parameter (0 = MF, 1 = MB).
    oci_punish_boost: [0, 1] - Additional learning rate added when prediction error is negative, scaled by OCI.
    """
    learning_rate, beta, w, oci_punish_boost = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        
        # Apply punishment boost if delta is negative (worse than expected)
        lr_1 = learning_rate
        if delta_stage1 < 0:
            lr_1 = learning_rate + (oci_punish_boost * oci_score)
            # Clip to ensure valid learning rate range [0, 1]
            if lr_1 > 1.0: lr_1 = 1.0
            
        q_stage1_mf[int(action_1[trial])] += lr_1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        
        # Apply punishment boost if delta is negative
        lr_2 = learning_rate
        if delta_stage2 < 0:
            lr_2 = learning_rate + (oci_punish_boost * oci_score)
            if lr_2 > 1.0: lr_2 = 1.0

        q_stage2_mf[state_idx, int(action_2[trial])] += lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Transition Doubt
This model proposes that OCI relates to the "doubting" aspect of obsessive-compulsive traits. While the Model-Based (MB) system relies on a transition matrix (knowing A usually goes to X), high OCI might degrade the certainty of this internal model. Here, OCI flattens the subjective transition matrix towards uniformity (0.5/0.5), reducing the effectiveness of Model-Based planning without changing the mixing weight $w$ directly.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Transition Doubt.
    
    This model posits that high OCI scores degrade the structural knowledge 
    (the transition matrix) used by the Model-Based system. High OCI introduces 
    "doubt," flattening the transition probabilities toward 0.5, making the 
    MB system less decisive even if the weight 'w' is high.

    Parameters:
    learning_rate: [0, 1] - Learning rate for MF values.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter.
    oci_doubt: [0, 1] - Scaling factor for how much OCI flattens the transition matrix.
    """
    learning_rate, beta, w, oci_doubt = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    # Calculate subjective transition probability
    # Base is 0.7. OCI reduces this towards 0.5.
    # Max reduction is 0.2 (0.7 -> 0.5).
    distortion = 0.2 * oci_doubt * oci_score
    p_trans = 0.7 - distortion
    if p_trans < 0.5: p_trans = 0.5 # Lower bound at chance
    
    transition_matrix = np.array([[p_trans, 1-p_trans], [1-p_trans, p_trans]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB values derived from distorted transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 2 Stickiness
While previous feedback indicated that Stage 1 stickiness was effective, this model tests if the perseveration is specific to the *proximal* decision (the aliens in Stage 2). Since Stage 2 choices are directly followed by rewards, compulsive repetition (stickiness) might be more pronounced here for high-OCI individuals, reflecting a tendency to repeatedly check or select the same immediate source.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Stickiness.
    
    This model applies perseveration (stickiness) specifically to the second stage 
    decisions (choosing the alien). It hypothesizes that OCI correlates with a 
    tendency to repeat the immediate pre-reward action within a specific state, 
    regardless of the outcome.

    Parameters:
    learning_rate: [0, 1] - Learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Weighting parameter.
    stick_s2_oci: [0, 5] - Stickiness bonus applied to the previously chosen alien, scaled by OCI.
    """
    learning_rate, beta, w, stick_s2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track previous choice for each state (planet) independently
    # Initialize with -1 (no previous choice)
    prev_choice_s2 = np.array([-1, -1]) 

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        state_idx = int(state[trial])

        # policy for the second choice
        q_net_2 = q_stage2_mf[state_idx].copy()
        
        # Apply stickiness if there was a previous choice in this state
        last_a2 = prev_choice_s2[state_idx]
        if last_a2 != -1:
            q_net_2[last_a2] += (stick_s2_oci * oci_score)

        exp_q2 = np.exp(beta * q_net_2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
        
        # Update history
        prev_choice_s2[state_idx] = int(action_2[trial])
  
        delta_stage1 = q_stage2_mf[state_idx, int(action_2[trial])] - q_stage1_mf[int(action_1[trial])]
        q_stage1_mf[int(action_1[trial])] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, int(action_2[trial])]
        q_stage2_mf[state_idx, int(action_2[trial])] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```