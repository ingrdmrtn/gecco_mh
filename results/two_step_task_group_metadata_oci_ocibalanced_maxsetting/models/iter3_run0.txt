Here are three new cognitive models that incorporate OCI scores into the two-step decision-making process.

### Model 1: OCI-Modulated Win-Stay Bias
This model hypothesizes that OCI scores correlate with a specific type of perseveration: "Win-Stay." While general stickiness (repeating the last action) was tried previously, this model suggests that high OCI individuals are specifically more rigid in maintaining a strategy that was **immediately rewarded** in the previous trial, reflecting a compulsion to repeat "successful" rituals to prevent negative outcomes.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Win-Stay Bias.
    
    This model posits that OCI modulates 'Win-Stay' behavior. Unlike simple 
    stickiness (repeating any choice), this adds a bonus to the previous 
    Stage 1 choice ONLY if it resulted in a reward. High OCI implies 
    stronger adherence to rewarded paths (rigidity/ritualization).

    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    win_stay_oci: [0, 5] - Bonus added to previously rewarded action, scaled by OCI.
    """
    learning_rate, beta, w, win_stay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_a1 = -1
    prev_reward = 0.0

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Win-Stay bonus modulated by OCI
        if prev_a1 != -1 and prev_reward > 0:
            q_net[prev_a1] += (win_stay_oci * oci_score)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        a1 = int(action_1[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Store history
        prev_a1 = a1
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Memory Decay
This model hypothesizes that OCI relates to memory confidence or maintenance of value estimates. Specifically, it introduces a decay parameter for **unchosen** options. High OCI scores increase the decay rate (values of unchosen options revert to 0 faster), reflecting an intolerance of uncertainty regarding unobserved states or a "checking" phenotype where information is considered stale quickly if not recently verified.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Memory Decay.
    
    This model assumes that values of UNCHOSEN options decay over time.
    The rate of decay is modulated by OCI. High OCI leads to faster decay 
    (lower retention) of unchosen option values, possibly reflecting 
    uncertainty intolerance or a need to 'refresh' information (checking).
    
    Decay formula: Q_unchosen *= (1 / (1 + decay_oci * oci))
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating for chosen options.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight.
    decay_oci: [0, 10] - Factor scaling the decay rate based on OCI.
    """
    learning_rate, beta, w, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate decay rate: If decay_oci is 0, multiplier is 1 (no decay).
    # As decay_oci * oci increases, multiplier approaches 0 (full decay).
    decay_multiplier = 1.0 / (1.0 + (decay_oci * oci_score))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update Chosen Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Decay Unchosen Stage 1
        q_stage1_mf[1 - a1] *= decay_multiplier

        # Update Chosen Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # Decay Unchosen Stage 2 (on the current planet)
        q_stage2_mf[state_idx, 1 - a2] *= decay_multiplier
        
        # Note: We do not decay the unvisited planet's values in this model
        # to keep the mechanism focused on immediate counterfactuals.

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Stage 2 Temperature
This model suggests that OCI affects decision noise (exploration/exploitation) differently in the second stage compared to the first. While Stage 1 involves abstract planning (spaceships), Stage 2 is a direct reward fetch (aliens). This model proposes that OCI modulates the inverse temperature ($\beta$) specifically for Stage 2. A high OCI might lead to more deterministic behavior (higher $\beta$) at the final step to minimize "local" error, or more erratic behavior (lower $\beta$) due to anxiety.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Stage 2 Temperature.
    
    This model separates the inverse temperature (beta) for Stage 1 and Stage 2.
    It posits that OCI specifically modulates the decision noise in Stage 2 
    (the direct reward stage). 
    
    Beta_Stage2 = Beta_Base * (1 + beta_slope_oci * OCI)
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta_base: [0, 10] - Base inverse temperature (used for Stage 1).
    w: [0, 1] - Mixing weight.
    beta_slope_oci: [-1, 1] - How OCI scales Stage 2 beta relative to Stage 1. 
                              (Positive = OCI makes S2 more deterministic).
    """
    learning_rate, beta_base, w, beta_slope_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 Beta based on OCI
    # We clip to ensure beta stays non-negative and within reasonable bounds
    beta_stage2 = beta_base * (1.0 + (beta_slope_oci * oci_score))
    beta_stage2 = np.clip(beta_stage2, 0.0, 20.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Uses Beta Base)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_base * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        state_idx = int(state[trial])

        # policy for the second choice (Uses OCI-modulated Beta)
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
  
        # Update Action Values
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```