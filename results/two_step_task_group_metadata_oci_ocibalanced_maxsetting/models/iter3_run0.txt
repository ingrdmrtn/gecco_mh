Here are three new cognitive models that introduce mechanisms distinct from the previously explored parameter combinations.

### Model 1: OCI-Modulated Transition Belief
This model posits that OCI scores affect the participant's internal model of the spaceship transitions. While the true transition probability is 0.7, participants with high OCI (associated with rigidity or perfectionism) might perceive the transitions as more deterministic (closer to 1.0) or, conversely, more chaotic (closer to 0.5), affecting their Model-Based value calculation.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates the subjective belief of the transition matrix probabilities used 
    in the Model-Based calculation.
    
    Parameters:
    - learning_rate: [0, 1] Value updating rate.
    - beta: [0, 10] Inverse temperature (softmax).
    - w: [0, 1] MB/MF weighting.
    - stick: [0, 5] Choice stickiness bonus.
    - trans_belief_base: [0, 1] Baseline belief of the common transition probability (e.g., 0.7).
    - trans_belief_oci: [-1, 1] Effect of OCI on transition belief.
    """
    learning_rate, beta, w, stick, trans_belief_base, trans_belief_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]

    # Calculate subjective transition probability
    # We clip between 0.5 (random) and 1.0 (deterministic) to maintain the 'common' direction definition
    p_common = trans_belief_base + (trans_belief_oci * subject_oci)
    p_common = np.clip(p_common, 0.5, 0.99)
    
    # Subjective Transition Matrix constructed from belief
    # T[0,0] is Prob(Planet 0 | Ship 0), etc.
    transition_matrix = np.array([[p_common, 1 - p_common], 
                                  [1 - p_common, p_common]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # MB values derived from SUBJECTIVE transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Subjective Reward Valuation
This model proposes that OCI modulates the *subjective magnitude* of the reward (coin). High OCI might correlate with anhedonia (reduced sensitivity to reward) or hypersensitivity to feedback. Instead of modifying the learning rate (how fast one learns), this modifies the asymptote of the value function (how much the reward is "worth"), which interacts distinctively with the softmax temperature.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates the subjective valuation of the reward outcome.
    Effective Reward = Reward * (Base + Slope * OCI).
    
    Parameters:
    - learning_rate: [0, 1] Alpha.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick: [0, 5] Stickiness.
    - val_base: [0, 2] Baseline valuation of a coin.
    - val_oci: [-1, 1] Effect of OCI on valuation.
    """
    learning_rate, beta, w, stick, val_base, val_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate subjective reward magnitude
    # A value of 1.0 is neutral. <1 implies dampening, >1 implies amplification.
    valuation = val_base + (val_oci * subject_oci)
    valuation = np.clip(valuation, 0.1, 5.0) 

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Calculate effective subjective reward
        effective_reward = reward[trial] * valuation
        
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update Stage 2 using the subjective effective reward
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated State Generalization
This model introduces a "Generalization" parameter. Normally, learning about an alien on Planet X does not affect the value of aliens on Planet Y. However, high OCI might be associated with over-generalization of fear or reward (e.g., "Aliens are generous" vs "This specific alien is generous"). OCI modulates how much the prediction error from the current state leaks into the Q-values of the *other* state.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates State Generalization. When updating the value of an action in the 
    visited state, a fraction of the prediction error is also applied to the 
    same action in the unvisited state.
    
    Parameters:
    - learning_rate: [0, 1] Alpha.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick: [0, 5] Stickiness.
    - gen_base: [0, 1] Baseline generalization rate.
    - gen_oci: [-1, 1] Effect of OCI on generalization.
    """
    learning_rate, beta, w, stick, gen_base, gen_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate generalization factor (eta)
    eta = gen_base + (gen_oci * subject_oci)
    eta = np.clip(eta, 0.0, 1.0) # 0 = no generalization, 1 = full generalization

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State (Planet) x Action (Alien)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10
    last_action_1 = -1

    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5
            continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]
        other_state_idx = 1 - state_idx

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Standard update for the visited state
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Generalized update for the UNVISITED state (same action)
        # We apply a fraction (eta) of the learning to the other planet
        if eta > 0:
            q_stage2_mf[other_state_idx, action_2[trial]] += learning_rate * eta * delta_stage2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```