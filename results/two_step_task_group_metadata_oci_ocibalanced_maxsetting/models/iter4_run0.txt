Here are the 3 proposed cognitive models.

### Model 1: OCI Modulates Memory Decay
This model hypothesizes that OCI scores affect the **retention of value for unchosen options**. High OCI might be associated with a "cluttered" memory (low decay/inability to forget) or conversely, an instability in maintaining values for options not currently attended to (high decay). This mechanism modifies the standard Q-learning update by actively decaying the Q-values of unchosen actions on every trial.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates Memory Decay (Forgetting) of unchosen options.
    While the chosen action is updated via prediction error, the unchosen 
    action's value decays toward 0. The rate of decay is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weighting between Model-Based and Model-Free values.
    - stick: [0, 5] Choice stickiness (perseveration).
    - decay_base: [0, 1] Base decay rate for unchosen options (0=no decay, 1=full reset).
    - decay_oci: [-1, 1] Effect of OCI on the decay rate.
    """
    learning_rate, beta, w, stick, decay_base, decay_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate effective decay rate constrained to [0, 1]
    decay = decay_base + (decay_oci * subject_oci)
    decay = np.clip(decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5; continue
            
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 1 Decay (Unchosen)
        q_stage1_mf[1 - a1] *= (1.0 - decay)
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 2 Decay (Unchosen in current state)
        q_stage2_mf[s_idx, 1 - a2] *= (1.0 - decay)
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI Modulates Outcome-Specific Stickiness
This model distinguishes between **Win-Stay** and **Loss-Stay** behavior. While standard stickiness applies a bonus to the repeated action regardless of the previous outcome, this model hypothesizes that OCI specifically modulates the tendency to persist after a **loss** (compulsive persistence or frustration-based repetition), separate from the tendency to repeat a winning action.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates Outcome-Specific Stickiness.
    This model separates stickiness into 'Win-Stickiness' and 'Loss-Stickiness'.
    OCI modulates the magnitude of stickiness specifically after a loss (stick_loss),
    capturing potential compulsive repetition of unrewarded actions.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stick_win: [0, 5] Stickiness bonus applied if the previous trial was a win.
    - stick_loss_base: [0, 5] Base stickiness bonus if the previous trial was a loss.
    - stick_loss_oci: [-5, 5] Effect of OCI on loss-stickiness.
    """
    learning_rate, beta, w, stick_win, stick_loss_base, stick_loss_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate OCI-modulated loss stickiness
    stick_loss = stick_loss_base + (stick_loss_oci * subject_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    last_action_1 = -1
    last_reward = -1 # Initialize with no reward history
    
    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5; continue

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply outcome-dependent stickiness
        if last_action_1 != -1:
            if last_reward > 0.5: # Win
                q_net[last_action_1] += stick_win
            else: # Loss
                q_net[last_action_1] += stick_loss

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI Modulates Fictive (Counterfactual) Learning
This model proposes that OCI modulates **Fictive Learning** in Stage 2. When a participant receives a reward (or lack thereof) for a chosen alien, they may also update the value of the *unchosen* alien, assuming the rewards are anti-correlated (i.e., if chosen=0, unchosen=1). High OCI might drive excessive counterfactual processing ("I should have chosen the other one").

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates Fictive (Counterfactual) Learning in Stage 2.
    When the chosen action is rewarded (r), the unchosen action is updated 
    as if it received the opposite reward (1-r). The weight of this 
    counterfactual update is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick: [0, 5] General stickiness.
    - fictive_base: [0, 1] Base weight for updating the unchosen option.
    - fictive_oci: [-1, 1] Effect of OCI on the fictive update weight.
    """
    learning_rate, beta, w, stick, fictive_base, fictive_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate fictive weight
    fictive_weight = fictive_base + (fictive_oci * subject_oci)
    fictive_weight = np.clip(fictive_weight, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5; continue

        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 Chosen Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # Stage 2 Fictive Update (Unchosen)
        # We assume the participant infers the unchosen option had reward (1-r)
        r_fictive = 1.0 - r
        delta_fictive = r_fictive - q_stage2_mf[s_idx, 1 - a2]
        # Update unchosen with scaled learning rate
        q_stage2_mf[s_idx, 1 - a2] += learning_rate * fictive_weight * delta_fictive
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```