Here are three new cognitive models based on the Obsessive-Compulsive Inventory (OCI) scores.

### Model 1: OCI-Modulated Asymmetric Learning
This model posits that OCI scores influence how differently participants learn from positive outcomes (wins) versus negative outcomes (losses). High OCI is often associated with perfectionism and an increased sensitivity to errors. In this model, the learning rate for losses (0 coins) is scaled by the OCI score relative to the learning rate for wins.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Asymmetric Learning (Loss Sensitivity).
    
    This model splits the learning rate into positive (win) and negative (loss)
    components. The base learning rate applies to wins. The learning rate for
    losses is modulated by the OCI score, testing the hypothesis that high-OCI
    individuals update their beliefs more aggressively after failures (0 coins).

    Parameters:
    lr_win: [0, 1] - Base learning rate for rewarded trials (1 coin).
    beta: [0, 10] - Inverse temperature for softmax choice.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    oci_loss_scale: [0, 5] - Multiplier for learning rate on loss trials based on OCI. 
                             LR_loss = lr_win * (1 + oci_loss_scale * OCI).
    """
    lr_win, beta, w, oci_loss_scale = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        # Determine current learning rate based on outcome
        r = reward[trial]
        if r == 1:
            current_lr = lr_win
        else:
            # Scale learning rate for losses based on OCI
            # We clip to 1.0 to ensure stability
            raw_lr_loss = lr_win * (1.0 + oci_loss_scale * oci_score)
            current_lr = min(1.0, raw_lr_loss)

        # Stage 1 MF Update (TD-0)
        # Note: Using max_q_stage2 (SARSA-max/Q-learning) for the TD target
        # or simply the value of the state chosen. Standard 2-step uses Q(S2, A2).
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Rare Transition Gating
This model hypothesizes that OCI relates to a rigidity or mistrust of "noisy" events. In the two-step task, "Rare" transitions (e.g., choosing Spaceship A but landing on Planet Y) are statistical outliers. This model proposes that high-OCI participants dampen their Model-Free learning updates when a Rare transition occurs, effectively treating these trials as "glitches" that should not influence habit formation.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Rare Transition Gating.
    
    This model posits that high OCI scores lead to reduced Model-Free updating 
    during 'Rare' transitions. High OCI individuals may view rare transitions 
    as noise or errors to be ignored rather than learning signals. 
    
    Parameters:
    learning_rate: [0, 1] - Base learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    oci_rare_damp: [0, 1] - Factor by which OCI reduces learning on rare trials.
                            Effective LR = LR * (1 - oci_rare_damp * OCI).
    """
    learning_rate, beta, w, oci_rare_damp = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        # Check if transition was Common or Rare
        # Common: (a1=0 -> s=0) or (a1=1 -> s=1)
        # Rare:   (a1=0 -> s=1) or (a1=1 -> s=0)
        is_common = (a1 == state_idx)
        
        if is_common:
            current_lr = learning_rate
        else:
            # Dampen learning on rare transitions based on OCI
            # If oci_rare_damp is high and OCI is high, learning -> 0
            dampening_factor = 1.0 - (oci_rare_damp * oci_score)
            dampening_factor = max(0.0, dampening_factor) # Ensure non-negative
            current_lr = learning_rate * dampening_factor
            
        # Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr * delta_stage1
        
        # Stage 2 MF Update (always uses full learning rate, as state is known)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Unchosen Value Decay
This model introduces a memory decay mechanism for options that were *not* chosen. It hypothesizes that high OCI scores correlate with "tunnel vision" or an obsessive focus on the current path, resulting in a faster decay (forgetting) of the values associated with the alternative spaceship.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Unchosen Value Decay.
    
    This model assumes that the values of unchosen actions in Stage 1 decay 
    over time, and the rate of this decay is governed by OCI. High OCI 
    represents a narrower attentional focus, causing the value of the 
    alternative option to fade faster from working memory.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for chosen actions.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    oci_decay: [0, 1] - Rate at which unchosen Q-values decay per trial, scaled by OCI.
                        Q_unchosen *= (1 - oci_decay * OCI).
    """
    learning_rate, beta, w, oci_decay = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Learning Updates ---
        
        # Standard updates for chosen actions
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # --- Decay for Unchosen Stage 1 Action ---
        unchosen_a1 = 1 - a1
        
        # Calculate decay factor. Clip OCI effect to [0, 1] to prevent sign flip.
        decay_factor = oci_decay * oci_score
        decay_factor = min(1.0, max(0.0, decay_factor))
        
        # Apply decay to the unchosen MF value
        q_stage1_mf[unchosen_a1] *= (1.0 - decay_factor)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```