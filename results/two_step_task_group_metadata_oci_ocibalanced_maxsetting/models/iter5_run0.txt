Here are three cognitive models expressed as Python functions. They introduce new mechanisms for how OCI might influence decision-making, specifically focusing on **Stage 2 perseveration**, **sensitivity to punishment**, and **passive forgetting** of unchosen options.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI Modulates Stage 2 Stickiness.
    
    While standard models apply stickiness (perseveration) to the first choice (Spaceship),
    this model posits that OCI specifically drives repetitive behavior in the second stage (Alien choice).
    This captures the idea that compulsive behavior might be more pronounced in the concrete, 
    immediate action (choosing an alien) rather than the abstract planning stage.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stick_s1: [0, 5] Fixed stickiness for Stage 1 (Spaceship).
    - stick_s2_base: [0, 5] Base stickiness for Stage 2 (Alien).
    - stick_s2_oci: [-5, 5] Modulation of Stage 2 stickiness by OCI.
    """
    learning_rate, beta, w, stick_s1, stick_s2_base, stick_s2_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate effective stickiness for Stage 2 based on OCI
    stick_s2 = stick_s2_base + (stick_s2_oci * subject_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # state x action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2 = -1
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Stage 1 stickiness
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stick_s1
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Policy ---
        q_net_s2 = q_stage2_mf[s_idx].copy()
        
        # Apply OCI-modulated Stage 2 stickiness
        # Note: We track last_action_2 irrespective of state to capture general motor perseveration
        # or we could track it per state. Here we assume general repetition bias.
        if last_action_2 != -1:
            q_net_s2[last_action_2] += stick_s2
            
        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        # Stage 1 MF Update (TD)
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_action_2 = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI Modulates Punishment Sensitivity.
    
    The data contains negative rewards (-1.0). This model hypothesizes that OCI 
    specifically modulates the sensitivity to these punishments (negative outcomes),
    reflecting anxiety-driven avoidance or hypersensitivity to failure.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stick: [0, 5] General choice stickiness (Stage 1).
    - punish_sens_base: [0, 5] Base scaling factor for negative rewards.
    - punish_sens_oci: [-5, 5] Effect of OCI on punishment sensitivity.
    """
    learning_rate, beta, w, stick, punish_sens_base, punish_sens_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate punishment multiplier
    # We clip to ensure the multiplier doesn't become negative (which would invert the punishment to a reward)
    punish_mult = np.maximum(0.0, punish_sens_base + (punish_sens_oci * subject_oci))
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # Apply Punishment Sensitivity
        effective_r = r
        if r < 0:
            effective_r = r * punish_mult
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = effective_r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI Modulates Passive Forgetting of Unchosen Options.
    
    This model introduces a decay parameter for *unchosen* actions. 
    High OCI might lead to either rigid maintenance of values (low forgetting/rumination)
    or rapid loss of information about unvisited states (high forgetting/interference).
    This is distinct from eligibility trace decay; it is a passive decay of Q-values towards 0.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stick: [0, 5] General stickiness.
    - forget_base: [0, 1] Base rate at which unchosen Q-values decay per trial.
    - forget_oci: [-1, 1] Modulation of forgetting rate by OCI.
    """
    learning_rate, beta, w, stick, forget_base, forget_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate forgetting rate, clipped between 0 and 1
    forget_rate = np.clip(forget_base + (forget_oci * subject_oci), 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # --- Passive Forgetting of Unchosen Options ---
        # Decay Stage 1 unchosen
        unchosen_a1 = 1 - a1
        q_stage1_mf[unchosen_a1] *= (1.0 - forget_rate)
        
        # Decay Stage 2 unchosen (in the current state)
        unchosen_a2 = 1 - a2
        q_stage2_mf[s_idx, unchosen_a2] *= (1.0 - forget_rate)
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```