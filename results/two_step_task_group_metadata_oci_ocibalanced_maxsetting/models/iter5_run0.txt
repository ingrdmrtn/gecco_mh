Here are the 3 proposed cognitive models.

### Model 1: OCI-Modulated Transition Belief
This model posits that OCI scores distort the participant's internal model of the environment's transition structure. While the true transition probability is 0.7, high OCI participants may perceive the world as either more chaotic (probability closer to 0.5) or more deterministic (probability closer to 1.0) due to anxiety or rigid thinking patterns. This distortion affects the Model-Based value calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Transition Belief.
    
    This model assumes that a participant's OCI score distorts their belief about the 
    transition matrix used in Model-Based planning. The parameter 'belief_distortion' 
    shifts the perceived common transition probability away from the objective 0.7 
    based on the OCI score.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating for Model-Free values.
    beta: [0, 10] - Inverse temperature for softmax choice policy.
    w: [0, 1] - Weighting between Model-Based and Model-Free values.
    belief_distortion: [0, 1] - Controls direction and magnitude of OCI-induced belief shift.
                                < 0.5 implies OCI reduces belief in stability (towards 0.5).
                                > 0.5 implies OCI increases belief in stability (towards 1.0).
    """
    learning_rate, beta, w, belief_distortion = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate subjective transition probability
    # Map belief_distortion from [0, 1] to a shift direction
    # We use a scaling factor (e.g., 0.1) to keep the shift reasonable relative to OCI magnitude
    distortion_effect = (belief_distortion - 0.5) * 0.2 * oci_score
    
    # Base probability is 0.7. Apply distortion and clip to valid probability range [0, 1]
    p_common = np.clip(0.7 + distortion_effect, 0.0, 1.0)
    
    # Construct the subjective transition matrix
    transition_matrix = np.array([[p_common, 1 - p_common], 
                                  [1 - p_common, p_common]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Model-Based value calculation using the distorted transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net value mixing MB and MF
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Counterfactual Updating
This model introduces a "counterfactual" learning mechanism where participants update the value of the *unchosen* option in the opposite direction of the chosen option's prediction error. OCI modulates the strength of this effect, representing a tendency to engage in "what if" thinking or regret-based adjustments (e.g., "I won, so the other one must have been a loser").

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Counterfactual Updating.
    
    This model assumes that OCI leads to counterfactual updating: adjusting the 
    value of unchosen actions inversely to the prediction error of the chosen action.
    High OCI scores increase the weight of this fictitious update.
    
    Parameters:
    learning_rate: [0, 1] - Standard learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    cf_oci: [0, 1] - Factor scaling the counterfactual update based on OCI.
    """
    learning_rate, beta, w, cf_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates with Counterfactual Logic ---
        # Calculate counterfactual learning rate modulated by OCI
        # We assume the effect is proportional to OCI but capped by the base learning rate logic
        lr_cf = learning_rate * cf_oci * oci_score
        # Clip to prevent instability
        lr_cf = np.clip(lr_cf, 0.0, 1.0)

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        # Update unchosen action (1-a1) in the opposite direction
        q_stage1_mf[1 - a1] -= lr_cf * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        # Update unchosen alien (1-a2) in the opposite direction
        q_stage2_mf[state_idx, 1 - a2] -= lr_cf * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Memory Decay
This model posits that OCI affects the retention of learned values for unchosen options. While standard Q-learning keeps unchosen values static, this model decays them over time. OCI modulates this decay rate: high OCI might lead to "obsessive" retention (low decay) or, conversely, cognitive interference (high decay).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Memory Decay.
    
    This model incorporates a decay mechanism for unchosen actions. The rate of 
    decay is modulated by the OCI score. This tests whether OCI relates to 
    how quickly information about unvisited states/actions is lost (or retained).
    
    Parameters:
    learning_rate: [0, 1] - Standard learning rate.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF mixing weight.
    decay_base: [0, 1] - Baseline decay rate for unchosen options.
    decay_oci: [0, 1] - Modulation of decay by OCI. 
                        Formula: decay = decay_base / (1 + decay_oci * oci).
                        Higher OCI reduces decay (increases retention/obsession).
    """
    learning_rate, beta, w, decay_base, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate
    # Hypothesis: High OCI leads to 'stickier' memory (lower decay)
    effective_decay = decay_base / (1.0 + decay_oci * oci_score)
    # Ensure bounds
    effective_decay = np.clip(effective_decay, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_stage1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_stage1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial])

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]

        # --- Updates and Decay ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        # Decay unchosen action at stage 1
        q_stage1_mf[1 - a1] *= (1.0 - effective_decay)
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        # Decay unchosen action at stage 2 (the alien not asked)
        q_stage2_mf[state_idx, 1 - a2] *= (1.0 - effective_decay)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```