Here are three new cognitive models based on the two-step task and OCI modulation.

### Model 1: Stage-Specific Habit Formation (OCI Modulates Stage 1 Learning)
**Concept:** This model hypothesizes that OCI specifically affects the formation of "habits" (Model-Free values) at the first stage of the decision tree (choosing the spaceship), while the learning of simple reward probabilities (Stage 2 aliens) remains distinct. High OCI might correlate with either rigid habit formation (high learning rate for Stage 1 MF) or an inability to form stable habits (low learning rate). This separates the learning rates for the complex choice (Stage 1) and the simple bandit choice (Stage 2).

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI Modulates Stage 1 Model-Free Learning Rate.
    
    This model separates the learning rates for Stage 1 (Spaceship choice) and 
    Stage 2 (Alien choice). OCI specifically modulates the Stage 1 learning rate,
    reflecting the hypothesis that compulsive traits affect the formation of 
    higher-level habits/caching of values differently than simple reward associations.

    Parameters:
    - lr_s2: [0, 1] Learning rate for Stage 2 (Aliens).
    - beta: [0, 10] Inverse temperature (softmax randomness).
    - w: [0, 1] Mixing weight (0=Pure MF, 1=Pure MB).
    - stick: [0, 5] General choice stickiness for Stage 1.
    - lr_s1_base: [0, 1] Base learning rate for Stage 1 Model-Free values.
    - lr_s1_oci: [-1, 1] Effect of OCI on Stage 1 learning rate.
    """
    lr_s2, beta, w, stick, lr_s1_base, lr_s1_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]

    # Calculate effective Stage 1 learning rate and clip to valid bounds
    lr_s1 = lr_s1_base + (lr_s1_oci * subject_oci)
    lr_s1 = np.clip(lr_s1, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    last_action_1 = -1
    
    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial] # 0 or 1 (Planet X or Y)

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Learning ---
        # Stage 1 MF Update (using OCI modulated rate)
        # TD(0) update: Q1(a1) <- Q1(a1) + lr * (Q2(s, a2) - Q1(a1))
        # Note: Using Q2 value as the target for Stage 1 MF
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr_s1 * delta_stage1
        
        # Stage 2 MF Update (using standard rate)
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_s2 * delta_stage2
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Surprise-Dependent Exploration (Rare Transition Beta Modulation)
**Concept:** This model posits that "Rare" transitions (unexpected state changes) act as a specific cognitive stressor. High OCI participants may react to these model violations differently than low OCI participants. Specifically, OCI modulates the *inverse temperature* (beta) on the trial *immediately following* a rare transition. This captures whether OCI leads to a "collapse" of deterministic behavior (increased randomness/anxiety) or a "rigidification" (ignoring the error) after the world model is violated.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI Modulates Beta (Exploration) specifically after Rare Transitions.
    
    Standard models use a fixed beta. This model allows beta to fluctuate
    based on the 'surprise' of the previous trial's transition.
    If the previous transition was Rare (e.g., A -> Y), the beta for the 
    current trial is modulated by OCI.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta_common: [0, 10] Beta used after a Common transition (or first trial).
    - w: [0, 1] Mixing weight.
    - stick: [0, 5] Choice stickiness.
    - beta_rare_base: [0, 10] Base beta used after a Rare transition.
    - beta_rare_oci: [-5, 5] Effect of OCI on beta after a Rare transition.
    """
    learning_rate, beta_common, w, stick, beta_rare_base, beta_rare_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    last_action_1 = -1
    was_last_transition_rare = False # Initialize false for trial 1
    
    for trial in range(n_trials):
        # Determine current Beta based on previous surprise
        if was_last_transition_rare:
            current_beta = beta_rare_base + (beta_rare_oci * subject_oci)
            # Ensure beta remains positive
            current_beta = np.maximum(current_beta, 0.0)
        else:
            current_beta = beta_common

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Choice ---
        # Using common beta or current beta? Usually exploration states persist.
        # We use current_beta for consistency within the trial.
        exp_q2 = np.exp(current_beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        # --- Track Context for Next Trial ---
        last_action_1 = a1
        # Determine if this transition was rare
        # Common: (A->X / 0->0) or (U->Y / 1->1). Rare: (0->1) or (1->0).
        if a1 == s_idx:
            was_last_transition_rare = False
        else:
            was_last_transition_rare = True

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Post-Loss Strategy Shift (Dynamic Weighting)
**Concept:** This model introduces a dynamic mixing weight ($w$). Rather than $w$ being a static trait, it fluctuates based on recent success. Specifically, OCI modulates how much the participant shifts their strategy (towards Model-Based or Model-Free) *after a loss*. A high OCI participant might lose confidence in their Model-Based planning after a failure and revert to Model-Free habits (or vice versa), whereas a low OCI participant might maintain a stable strategy regardless of a single loss.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI Modulates Strategy Mixing Weight (w) specifically after Losses.
    
    The mixing weight 'w' determines the balance between Goal-Directed (MB)
    and Habitual (MF) control. This model assumes 'w' is dynamic.
    If the previous trial resulted in a loss (0 coins), the 'w' for the 
    current trial is adjusted by an OCI-dependent factor. 
    This captures 'reactive' shifts in control strategy triggered by failure.

    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w_base: [0, 1] Baseline mixing weight (after wins or at start).
    - stick: [0, 5]
    - w_loss_base: [-1, 1] Base adjustment to w after a loss.
    - w_loss_oci: [-1, 1] Effect of OCI on the post-loss w adjustment.
    """
    learning_rate, beta, w_base, stick, w_loss_base, w_loss_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    last_action_1 = -1
    last_reward = 1 # Assume start with a 'win' state or neutral
    
    for trial in range(n_trials):
        
        # Calculate dynamic w based on previous outcome
        if last_reward == 0:
            # Shift w if last trial was a loss
            w_shift = w_loss_base + (w_loss_oci * subject_oci)
            current_w = w_base + w_shift
        else:
            current_w = w_base
            
        # Clip w to valid range [0, 1]
        current_w = np.clip(current_w, 0.0, 1.0)

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (current_w * q_stage1_mb) + ((1 - current_w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_reward = r

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```