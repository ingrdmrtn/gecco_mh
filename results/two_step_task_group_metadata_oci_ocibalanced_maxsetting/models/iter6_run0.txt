Here are three cognitive models that incorporate the OCI score into the decision-making process in the two-step task.

### Model 1: OCI-Modulated Counterfactual Updating
This model hypothesizes that individuals with high OCI scores engage in increased "counterfactual" thinking (i.e., "If I had chosen the other alien, what would have happened?"). In this task, since the two aliens on a planet often have negatively correlated reward probabilities (if one is good, the other is likely bad), a participant can update the value of the *unchosen* alien based on the outcome of the *chosen* alien. This model scales the rate of this counterfactual updating by the OCI score.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Counterfactual Updating.
    
    This model assumes that when a participant receives a reward (or lack thereof) 
    from a chosen alien, they also update the value of the unchosen alien 
    in the opposite direction. The strength of this 'fictitious' update 
    is modulated by OCI, reflecting a tendency to obsess over the unchosen option.
    
    Parameters:
    learning_rate: [0, 1] - Direct learning rate for chosen actions.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight (0=MF, 1=MB).
    cf_base: [0, 1] - Baseline counterfactual learning rate ratio.
    cf_oci: [0, 1] - OCI modulation of counterfactual updating.
    """
    learning_rate, beta, w, cf_base, cf_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate the effective counterfactual learning rate
    # Bounded between 0 and 1
    alpha_cf = learning_rate * (cf_base + (cf_oci * oci_score))
    alpha_cf = np.clip(alpha_cf, 0.0, 1.0)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        r = reward[trial]
        
        # 1. Stage 1 MF Update
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # 2. Stage 2 MF Update (Chosen)
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 3. Stage 2 Counterfactual Update (Unchosen)
        # If we got Reward=1, we assume unchosen would have given 0.
        # If we got Reward=0, we assume unchosen would have given 1.
        # Implied counterfactual reward = 1 - r
        unc_a2 = 1 - a2
        delta_cf = (1.0 - r) - q_stage2_mf[state_idx, unc_a2]
        q_stage2_mf[state_idx, unc_a2] += alpha_cf * delta_cf

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Stage-Specific Precision
This model separates the inverse temperature ($\beta$) for Stage 1 (Spaceship choice) and Stage 2 (Alien choice). It hypothesizes that OCI specifically impacts the rigidity/determinism of the second stage. High OCI participants might be more "compulsive" or rigid in their exploitation of the immediate reward state (Stage 2), while their planning (Stage 1) remains more exploratory or distinct.

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Stage-Specific Precision.
    
    This model separates exploration parameters for the two stages.
    It proposes that OCI drives rigidity (higher beta) specifically in the 
    second stage (harvesting), creating a disparity between planning noise 
    and execution noise.
    
    Parameters:
    learning_rate: [0, 1] - Value updating rate.
    w: [0, 1] - MB/MF mixing weight.
    beta_1: [0, 10] - Inverse temperature for Stage 1 (Spaceships).
    beta_2_base: [0, 10] - Baseline inverse temperature for Stage 2 (Aliens).
    beta_2_oci: [0, 5] - Slope of OCI effect on Stage 2 precision.
    """
    learning_rate, w, beta_1, beta_2_base, beta_2_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Stage 2 Beta based on OCI
    beta_2 = beta_2_base + (beta_2_oci * oci_score)
    # Cap beta to prevent numerical overflow in exp()
    beta_2 = np.minimum(beta_2, 20.0) 
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Uses beta_1) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta_1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])
        
        # --- Stage 2 Policy (Uses OCI-modulated beta_2) ---
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        a1 = int(action_1[trial])
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Dynamic Transition Learning
Standard Model-Based RL assumes the participant knows the fixed 70/30 transition matrix. This model proposes that participants *learn* the transition matrix over time, and OCI affects the rate of this structural learning. High OCI might be associated with hyper-vigilance to structural changes (high learning rate for transitions) or, conversely, structural rigidity (low learning rate).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Dynamic Transition Learning.
    
    Instead of a fixed 70/30 transition matrix, the participant maintains 
    a dynamic estimate of the transition probabilities. OCI modulates the 
    learning rate for these transitions (lr_trans). This reflects how 
    obsessive traits might influence the monitoring of environmental statistics.
    
    Parameters:
    learning_rate: [0, 1] - Learning rate for Reward values (Q-values).
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight.
    lr_trans_base: [0, 1] - Baseline learning rate for transition matrix.
    lr_trans_oci: [0, 1] - OCI modulation of transition learning rate.
    """
    learning_rate, beta, w, lr_trans_base, lr_trans_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate Transition Learning Rate
    lr_trans = lr_trans_base + (lr_trans_oci * oci_score)
    lr_trans = np.clip(lr_trans, 0.0, 1.0)
    
    # Initialize subjective transition matrix (starts at neutral 0.5 or theoretical 0.7)
    # Here we initialize at the true prior to see if they deviate or maintain it
    trans_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy (Uses Dynamic Transition Matrix) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # MB calculation uses current subjective matrix
        q_stage1_mb = trans_matrix @ max_q_stage2
        
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        state_idx = int(state[trial]) # 0 for X, 1 for Y
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Updates ---
        
        # 1. Update Value Estimates (Standard)
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        
        # 2. Update Transition Matrix Estimate
        # If a1=0 (Spaceship A) and state=0 (Planet X), this is index 0.
        # If a1=0 and state=1 (Planet Y), this is index 1.
        # Update row a1.
        
        # Create a one-hot vector for the observed state
        observed_transition = np.zeros(2)
        observed_transition[state_idx] = 1.0
        
        # Update the row corresponding to the chosen spaceship
        # row = row + alpha * (observed - row)
        trans_matrix[a1] += lr_trans * (observed_transition - trans_matrix[a1])
        
        # Ensure row sums to 1 (numerical stability)
        trans_matrix[a1] /= np.sum(trans_matrix[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```