Here are three new cognitive models that incorporate the OCI score into the decision-making process, distinct from the previously tried combinations.

### Model 1: OCI-Modulated Memory Decay
This model hypothesizes that high OCI scores correlate with a difficulty in maintaining action values in working memory over time (intrusive thoughts or distraction). Instead of Q-values for unchosen options remaining static, they decay towards zero. The rate of this decay is modulated by the OCI score.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Memory Decay.
    
    This model posits that OCI affects the retention of learned Q-values.
    High OCI scores increase the rate at which unchosen action values decay 
    (forgetting), representing cognitive interference or difficulty maintaining 
    value representations over time.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating from prediction errors.
    beta: [0, 10] - Inverse temperature (exploration/exploitation).
    w: [0, 1] - Weighting between Model-Based and Model-Free values.
    decay_oci: [0, 1] - Scaling factor for OCI-dependent value decay.
    """
    learning_rate, beta, w, decay_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Calculate effective decay rate based on OCI (bounded [0, 1])
    # If OCI is high, decay is stronger.
    decay_rate = decay_oci * oci_score
    # Ensure decay doesn't exceed 1 or drop below 0 due to OCI scaling
    decay_rate = np.clip(decay_rate, 0.0, 1.0)
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Apply Memory Decay to MF values before decision
        q_stage1_mf *= (1.0 - decay_rate)
        q_stage2_mf *= (1.0 - decay_rate)

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        # Updates
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # TD(0) update for Stage 1
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # TD(0) update for Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI-Modulated Reward Sensitivity (Anhedonia/Valuation)
This model suggests that OCI relates to altered subjective valuation of outcomes (e.g., anhedonia or blunted reward sensitivity). The objective reward (coins) is transformed into a subjective utility based on the OCI score. A high parameter value indicates that high OCI dampens the perceived magnitude of the reward, leading to slower learning and weaker differentiation between options.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Reward Sensitivity.
    
    This model posits that OCI modulates the subjective valuation of rewards.
    High OCI scores scale the effective reward signal used for updating Q-values.
    This captures potential anhedonia or altered reward processing, where 
    the difference between 0 and 1 coin feels smaller (or larger) depending on OCI.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    sens_oci: [0, 1] - Sensitivity modulation. 
                       If sens_oci > 0, high OCI reduces effective reward magnitude (dampening).
    """
    learning_rate, beta, w, sens_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        a1 = int(action_1[trial])
        a2 = int(action_2[trial])
        
        # Calculate Subjective Reward
        # We model a dampening effect: Effective Reward = Reward * (1 - k * OCI)
        # Bounded so it doesn't invert the sign of positive rewards unexpectedly
        dampening_factor = np.clip(sens_oci * oci_score, 0.0, 0.9)
        effective_reward = reward[trial] * (1.0 - dampening_factor)

        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 using effective (subjective) reward
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Uncertainty Bonus (Exploration)
This model links OCI to "Intolerance of Uncertainty." It tracks how long it has been since an option was last chosen (uncertainty). The model adds a bias to the Q-values proportional to this time count, scaled by OCI. This is distinct from simple stickiness (which only cares about the *last* trial). A negative effect implies "neophobia" (avoiding the unknown/unchosen), while a positive effect implies hyper-checking.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Uncertainty Bonus (Exploration).
    
    This model adds an exploration/uncertainty term to the decision policy.
    It tracks 'days since last chosen' for each spaceship. OCI modulates 
    how this uncertainty influences choice. High OCI might lead to 
    avoidance of uncertain options (neophobia) or compulsive checking of them.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - MB/MF weighting.
    expl_oci: [0, 1] - Exploration modulation. 
                       Mapped to [-0.5, 0.5] internally to allow both 
                       uncertainty seeking (checking) or avoidance.
    """
    learning_rate, beta, w, expl_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Map expl_oci from [0, 1] to [-0.5, 0.5] range to allow positive/negative bias
    # If negative: Uncertainty Avoidance. If positive: Uncertainty Seeking.
    exploration_weight = (expl_oci - 0.5) * oci_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track trials since last selection for Stage 1 actions
    time_since_chosen = np.zeros(2)

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add Exploration/Uncertainty Bonus
        # Bonus = weight * time_since_chosen
        exploration_bonus = exploration_weight * time_since_chosen
        q_net_total = q_net_s1 + exploration_bonus
        
        exp_q1 = np.exp(beta * q_net_total)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[int(action_1[trial])]
        
        # Update counters
        a1 = int(action_1[trial])
        time_since_chosen += 1 # Increment for all
        time_since_chosen[a1] = 0 # Reset for chosen
        
        state_idx = int(state[trial])

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[int(action_2[trial])]
  
        a2 = int(action_2[trial])
        
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```