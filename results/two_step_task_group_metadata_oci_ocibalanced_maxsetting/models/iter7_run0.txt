Here are 3 new cognitive models based on the provided participant data and task structure.

### Model 1: OCI Modulated Stage 2 Exploration
**Hypothesis:** OCI symptoms may specifically impact decision noise (exploration vs. exploitation) in the immediate reward stage (Stage 2, Aliens), distinct from the planning stage (Stage 1, Spaceships). High OCI may lead to more rigid (lower temperature) or more chaotic (higher temperature) behavior when facing the immediate bandit task, while Stage 1 planning remains governed by a baseline temperature.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI Modulated Stage 2 Exploration (Split Beta).
    
    This model separates the inverse temperature (beta) for Stage 1 (planning)
    and Stage 2 (harvesting). OCI modulates the beta specifically for Stage 2,
    capturing how obsessive-compulsive traits affect immediate reward-based selection
    independently of the model-based planning process.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate for Q-values.
    - beta_s1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - w: [0, 1] Model-based / Model-free weighting.
    - stickiness: [0, 5] Choice stickiness for Stage 1.
    - beta_s2_base: [0, 10] Baseline inverse temperature for Stage 2 (Alien choice).
    - beta_s2_oci: [-5, 5] Effect of OCI on Stage 2 beta.
    """
    learning_rate, beta_s1, w, stickiness, beta_s2_base, beta_s2_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate OCI-modulated beta for stage 2
    # Clip to ensure beta remains non-negative and within reasonable bounds
    beta_s2 = beta_s2_base + (beta_s2_oci * subject_oci)
    beta_s2 = np.clip(beta_s2, 0, 20)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5; continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stickiness
            
        exp_q1 = np.exp(beta_s1 * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Policy (OCI Modulated Beta) ---
        # Using beta_s2 derived from OCI
        exp_q2 = np.exp(beta_s2 * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: OCI Modulated Rare-Transition Learning
**Hypothesis:** OCI is linked to difficulty processing uncertainty and prediction errors. When a "Rare" transition occurs (e.g., Spaceship A goes to Planet Y), it generates a conflict between the model (expecting X) and reality (Y). This model suggests OCI modulates the *learning rate* specifically for the Stage 1 Model-Free update following these rare events. High OCI might lead to discounting these events (learning less) or over-reacting to them (learning more).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI Modulated Rare-Transition Learning Rate.
    
    This model posits that OCI affects how much the agent learns from 
    'Rare' transitions (outliers) compared to 'Common' transitions.
    The learning rate for the Stage 1 MF update is scaled if the transition
    was rare.

    Parameters:
    - lr_base: [0, 1] Base learning rate for common transitions and Stage 2.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weight.
    - stick: [0, 5] Choice stickiness.
    - rare_mod_base: [0, 2] Base modifier for learning rate on rare transitions (1 = same as base).
    - rare_mod_oci: [-1, 1] OCI slope affecting the rare transition modifier.
    """
    lr_base, beta, w, stick, rare_mod_base, rare_mod_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate the modifier for rare transitions
    rare_modifier = rare_mod_base + (rare_mod_oci * subject_oci)
    # Ensure the resulting multiplier is non-negative
    rare_modifier = np.maximum(rare_modifier, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5; continue

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        # Determine if transition was Common or Rare
        # 0->0 and 1->1 are Common. 0->1 and 1->0 are Rare.
        is_common = (a1 == s_idx)
        
        # Select learning rate for Stage 1 update
        if is_common:
            current_lr_s1 = lr_base
        else:
            current_lr_s1 = lr_base * rare_modifier
            
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += current_lr_s1 * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += lr_base * delta_stage2
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI Modulated Probability Distortion (Risk)
**Hypothesis:** The Model-Based system relies on calculating expected values based on the probability of reward at the second stage. OCI may introduce a distortion in how these probabilities are perceived (e.g., pessimism or risk aversion). This model applies a non-linear transformation to the Stage 2 Q-values (which represent reward probabilities) *before* they are used for the Model-Based calculation.

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI Modulated Probability Distortion.
    
    This model assumes that while the agent learns the objective probability 
    of reward (Q-values) correctly, the Model-Based system distorts these 
    probabilities when planning. OCI modulates the exponent of this distortion, 
    representing risk aversion or pessimism (underweighting high probabilities).
    
    Q_used_for_MB = Q_learned ^ (gamma)
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stick: [0, 5]
    - gamma_base: [0, 5] Base distortion exponent (1.0 = linear/rational).
    - gamma_oci: [-2, 2] OCI effect on distortion.
    """
    learning_rate, beta, w, stick, gamma_base, gamma_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate distortion exponent gamma
    gamma = gamma_base + (gamma_oci * subject_oci)
    gamma = np.maximum(gamma, 0.01) # Avoid negative or zero powers

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    eps = 1e-10
    last_action_1 = -1
    
    for trial in range(n_trials):
        if action_1[trial] < 0 or action_2[trial] < 0:
            p_choice_1[trial] = 0.5; p_choice_2[trial] = 0.5; continue

        # --- Stage 1 Policy ---
        # Apply distortion to Stage 2 values specifically for the MB calculation
        # Q-values are approx [0,1], so power law works as probability weighting
        q_stage2_distorted = np.power(np.clip(q_stage2_mf, 0, 1), gamma)
        
        max_q_stage2_mb = np.max(q_stage2_distorted, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2_mb
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        a1 = action_1[trial]
        s_idx = state[trial]

        # --- Stage 2 Policy ---
        # Stage 2 choice uses the raw learned values (or could use distorted, 
        # but usually distortion affects planning/prospects more than direct bandits)
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        a2 = action_2[trial]
        r = reward[trial]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```