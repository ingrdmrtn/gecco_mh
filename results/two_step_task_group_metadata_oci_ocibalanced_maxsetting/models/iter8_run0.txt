Here are three cognitive models that propose different mechanisms for how OCI might influence decision-making in the two-step task.

### Model 1: Transition-Dependent Stickiness
**Hypothesis:** Participants with varying OCI scores may respond differently to "structural surprises" (Rare transitions). While standard stickiness applies a bonus to the previously chosen action regardless of the outcome, this model posits that the stickiness magnitude depends on whether the previous transition was **Common** or **Rare**. High OCI might lead to rigid perseveration even after rare transitions (ignoring the model structure), or conversely, a "reset" of stickiness upon surprise.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates Transition-Dependent Stickiness.
    Stickiness is applied differently depending on whether the previous trial's 
    transition was Common (Expected) or Rare (Unexpected).
    OCI modulates the stickiness specifically following Rare transitions.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based (1) vs Model-free (0) weighting.
    - stick_common: [0, 5] Stickiness bonus after a Common transition.
    - stick_rare_base: [0, 5] Base stickiness bonus after a Rare transition.
    - stick_rare_oci: [-5, 5] Effect of OCI on stickiness after Rare transitions.
    """
    learning_rate, beta, w, stick_common, stick_rare_base, stick_rare_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate effective stickiness for rare transitions based on OCI
    stick_rare = stick_rare_base + (stick_rare_oci * subject_oci)

    # Fixed transition matrix as per template
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_transition_type = -1 # 0 for Common, 1 for Rare
    
    eps = 1e-10
    
    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply transition-dependent stickiness
        if last_action_1 != -1:
            if last_transition_type == 0: # Common
                q_net[last_action_1] += stick_common
            else: # Rare
                q_net[last_action_1] += stick_rare

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Current trial details
        a1 = action_1[trial]
        s_idx = state[trial]
        
        # Determine transition type for next trial logic
        # Action 0 -> State 0 (Common), Action 1 -> State 1 (Common)
        if (a1 == 0 and s_idx == 0) or (a1 == 1 and s_idx == 1):
            current_transition_type = 0 # Common
        else:
            current_transition_type = 1 # Rare

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a2 = action_2[trial]
        r = reward[trial]
        
        # SARSA / TD(1) for Stage 1
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Q-learning for Stage 2
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1
        last_transition_type = current_transition_type

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Uncertainty-Driven Exploration
**Hypothesis:** OCI is often linked to "Intolerance of Uncertainty." This model introduces an exploration bonus based on the time since an option was last chosen (uncertainty). High OCI participants may have a higher "uncertainty bonus" (leading to checking behaviors) or a negative one (avoidance), modulating their exploration/exploitation balance distinct from simple random noise ($\beta$).

```python
def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates Uncertainty-Driven Exploration (Exploration Bonus).
    Adds a bonus to the Q-values proportional to the number of trials 
    since an option was last chosen.
    
    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stick: [0, 5] Standard choice stickiness.
    - kappa_base: [0, 2] Base exploration bonus parameter (weight of uncertainty).
    - kappa_oci: [-2, 2] Effect of OCI on the exploration bonus weight.
    """
    learning_rate, beta, w, stick, kappa_base, kappa_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate effective exploration weight
    kappa = kappa_base + (kappa_oci * subject_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track trials since last chosen (Uncertainty counter)
    trials_since_chosen = np.zeros(2) 
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    eps = 1e-10

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add Stickiness
        if last_action_1 != -1:
            q_net[last_action_1] += stick
            
        # Add Exploration Bonus (Kappa * Uncertainty)
        q_net += kappa * trials_since_chosen

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Update uncertainty counters
        a1 = action_1[trial]
        trials_since_chosen += 1 # Increment for all
        trials_since_chosen[a1] = 0 # Reset for chosen

        # --- Stage 2 Policy ---
        s_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a2 = action_2[trial]
        r = reward[trial]
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Dynamic Reward-Dependent Weighting
**Hypothesis:** The balance between Model-Based (planning) and Model-Free (habit) control ($w$) is not static. Stress or low reward rates might induce a shift toward Model-Free habits. This model proposes that $w$ changes dynamically based on the recent average reward rate, and OCI modulates how sensitive the participant's strategy is to their recent performance history (e.g., high OCI might abandon planning faster when rewards drop).

```python
def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates Dynamic Reward-Dependent Weighting (w).
    The weighting parameter 'w' (MB vs MF) is dynamic, adjusting based on 
    the smoothed average reward history.
    OCI modulates the slope of this relationship: how much the reward history 
    impacts the arbitration between MB and MF systems.

    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - stick: [0, 5]
    - w_base: [0, 1] Baseline mixing weight at average performance (0.5).
    - w_slope_base: [-2, 2] Base sensitivity of w to reward history.
    - w_slope_oci: [-2, 2] Effect of OCI on the sensitivity to reward history.
    """
    learning_rate, beta, stick, w_base, w_slope_base, w_slope_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Effective slope determined by OCI
    w_slope = w_slope_base + (w_slope_oci * subject_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    avg_reward = 0.5 # Initialize average reward expectation
    reward_smoothing_alpha = 0.2 # Fixed smoothing factor for reward history
    
    eps = 1e-10
    
    for trial in range(n_trials):
        # --- Calculate Dynamic w ---
        # w adjusts based on how avg_reward deviates from 0.5
        # If slope is positive: High reward -> More MB (High w).
        # If slope is negative: High reward -> More MF (Low w).
        w_dynamic = w_base + w_slope * (avg_reward - 0.5)
        
        # Clip w to valid range [0, 1]
        w_curr = np.clip(w_dynamic, 0.0, 1.0)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w_curr * q_stage1_mb) + ((1 - w_curr) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net[last_action_1] += stick

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # --- Stage 2 Policy ---
        s_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[s_idx])
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        a1 = action_1[trial]
        a2 = action_2[trial]
        r = reward[trial]
        
        # Update smoothed average reward
        avg_reward = (1 - reward_smoothing_alpha) * avg_reward + reward_smoothing_alpha * r
        
        delta_stage1 = q_stage2_mf[s_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = r - q_stage2_mf[s_idx, a2]
        q_stage2_mf[s_idx, a2] += learning_rate * delta_stage2
        
        last_action_1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```