Here are three new cognitive models for the two-step decision task, incorporating OCI scores to explain specific behavioral patterns like habit formation, uncertainty avoidance, and goal perseveration.

```python
def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 1: OCI-Modulated Habit Trace (Accumulating Stickiness).
    
    Unlike simple 1-back stickiness, this model posits that OCI relates to the 
    strength of a 'Habit Trace' that accumulates with repeated choices and 
    decays slowly. High OCI participants form deeper 'ruts' or habits that 
    are harder to break, explaining long blocks of repeated choices.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating for MB and MF values.
    beta: [0, 10] - Inverse temperature for softmax.
    w: [0, 1] - Mixing weight (1 = pure MB, 0 = pure MF).
    habit_oci: [0, 5] - Scaling factor for the influence of the habit trace.
    """
    learning_rate, beta, w, habit_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    # Fixed parameters for structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    trace_decay = 0.5 # Fixed decay rate for the habit trace
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Habit trace vector for the two Stage 1 actions
    habit_trace = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Add Habit Trace bonus modulated by OCI
        # High OCI -> Stronger influence of accumulated habits
        q_net += (habit_oci * oci_score * habit_trace)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update Habit Trace: Decay existing, increment chosen
        habit_trace *= trace_decay
        habit_trace[a1] += 1.0
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        # TD(0) update for Stage 1 MF
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        # Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 2: OCI-Modulated Uncertainty Avoidance.
    
    This model assumes that OCI correlates with an aversion to uncertainty.
    Instead of an exploration bonus (seeking novel options), high OCI leads 
    to an 'Avoidance Penalty' for options that haven't been chosen recently.
    The longer an option remains unchosen, the more negative its value becomes
    for high-OCI agents, driving them to stick to the familiar.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight.
    avoid_oci: [0, 5] - Scaling factor for the uncertainty penalty.
    """
    learning_rate, beta, w, avoid_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track trials since last chosen for Stage 1 actions
    trials_since_chosen = np.zeros(2)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Uncertainty Avoidance Penalty
        # Penalty increases logarithmically with time since last choice to prevent explosion
        # High OCI -> Larger penalty for the "unknown" (unchosen) option
        penalty = avoid_oci * oci_score * np.log(trials_since_chosen + 1)
        q_net -= penalty
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # Update counts
        trials_since_chosen += 1
        trials_since_chosen[a1] = 0
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    Model 3: OCI-Modulated Goal Stickiness (State Perseveration).
    
    This model proposes that high OCI participants perseverate not just on the 
    motor action (Spaceship), but on the *goal state* (Planet). They prefer 
    actions that are likely to return them to the planet they visited previously,
    regardless of reward. This reflects a compulsion to re-enter the same context.
    
    Parameters:
    learning_rate: [0, 1] - Rate of value updating.
    beta: [0, 10] - Inverse temperature.
    w: [0, 1] - Mixing weight.
    goal_stick_oci: [0, 5] - Strength of the preference to return to the previous planet.
    """
    learning_rate, beta, w, goal_stick_oci = model_parameters
    n_trials = len(action_1)
    oci_score = oci[0]
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_planet = -1

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Goal Stickiness Bonus
        # If a planet was visited last trial, boost actions that lead to it
        if prev_planet != -1:
            # Probability of reaching prev_planet given action 0 or 1
            prob_reach_prev = transition_matrix[:, prev_planet]
            q_net += (goal_stick_oci * oci_score * prob_reach_prev)
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        
        a1 = int(action_1[trial])
        p_choice_1[trial] = probs_1[a1]
        
        # --- Stage 2 Policy ---
        state_idx = int(state[trial])
        prev_planet = state_idx # Update previous planet
        
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        
        a2 = int(action_2[trial])
        p_choice_2[trial] = probs_2[a2]
        
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[state_idx, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, a2]
        q_stage2_mf[state_idx, a2] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```