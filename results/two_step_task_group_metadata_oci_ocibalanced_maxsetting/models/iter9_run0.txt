Here are three new cognitive models for the two-step task, incorporating OCI scores to modulate specific decision-making mechanisms.

### Model 1: Outcome-Specific Stage 2 Stickiness
This model hypothesizes that OCI affects "compulsive checking" behavior in the second stage (Alien choice). Specifically, it proposes that OCI modulates the tendency to repeat a choice in Stage 2 (stickiness) differently depending on whether the previous outcome was a win or a loss. High OCI might lead to "chasing" or persistently checking an unrewarded alien (loss-stickiness).

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates Outcome-Specific Stickiness in Stage 2.
    Separates Stage 2 stickiness into 'Win-Stickiness' and 'Loss-Stickiness'.
    OCI modulates the magnitude of stickiness specifically after a loss in Stage 2,
    capturing compulsive repetition of unrewarded actions (checking).

    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stick_s1: [0, 5] Stickiness for Stage 1 choices.
    - stick_s2_win: [0, 5] Stickiness for Stage 2 if previous outcome was a win.
    - stick_s2_loss_base: [0, 5] Base stickiness for Stage 2 if previous outcome was a loss.
    - stick_s2_loss_oci: [-5, 5] Effect of OCI on Stage 2 loss-stickiness.
    """
    learning_rate, beta, w, stick_s1, stick_s2_win, stick_s2_loss_base, stick_s2_loss_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate OCI-modulated loss stickiness
    stick_s2_loss = stick_s2_loss_base + (stick_s2_loss_oci * subject_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_action_2_in_state = np.full(2, -1) # Track last action per state (0 or 1)
    last_reward_in_state = np.full(2, -1.0) # Track last reward per state
    
    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Stage 1 Stickiness
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stick_s1
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        q_net_s2 = q_stage2_mf[state_idx].copy()
        
        # Apply Outcome-Specific Stage 2 Stickiness
        prev_a2 = last_action_2_in_state[state_idx]
        prev_r = last_reward_in_state[state_idx]
        
        if prev_a2 != -1:
            if prev_r > 0.5: # Win
                q_net_s2[prev_a2] += stick_s2_win
            else: # Loss
                q_net_s2[prev_a2] += stick_s2_loss

        exp_q2 = np.exp(beta * q_net_s2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update history
        last_action_1 = action_1[trial]
        last_action_2_in_state[state_idx] = action_2[trial]
        last_reward_in_state[state_idx] = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Outcome-Dependent Exploration (Beta)
This model proposes that OCI modulates the "rigidity" of behavior (inverse temperature, beta) specifically after a negative outcome. High OCI participants may exhibit higher beta (less exploration, more freezing/deterministic choice) after a loss compared to a win.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates Inverse Temperature (Beta) based on previous reward.
    Distinguishes between 'Beta-Win' (exploration after success) and 
    'Beta-Loss' (exploration after failure).
    OCI modulates Beta-Loss, capturing potential rigidity/freezing after negative feedback.

    Parameters:
    - learning_rate: [0, 1]
    - beta_win: [0, 10] Beta applied if previous trial was a win.
    - beta_loss_base: [0, 10] Base Beta applied if previous trial was a loss.
    - beta_loss_oci: [-5, 5] Effect of OCI on Beta-Loss.
    - w: [0, 1] Model-based weight.
    - stick: [0, 5] General stickiness parameter (Stage 1).
    """
    learning_rate, beta_win, beta_loss_base, beta_loss_oci, w, stick = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate OCI-modulated Beta for losses
    beta_loss = beta_loss_base + (beta_loss_oci * subject_oci)
    # Ensure beta stays non-negative
    beta_loss = max(0.0, beta_loss)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    last_reward = 1.0 # Initialize assuming a 'win' state or neutral for first trial
    
    for trial in range(n_trials):
        
        # Determine current beta based on previous reward
        if last_reward > 0.5:
            current_beta = beta_win
        else:
            current_beta = beta_loss

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stick
            
        exp_q1 = np.exp(current_beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        # Apply same beta to stage 2 (outcome-dependent state of the organism)
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: OCI-Modulated Side Bias
This model introduces a fixed "Side Bias" (preference for Spaceship A vs U) in Stage 1, which is modulated by OCI. This accounts for intrinsic, non-learning-based preferences or rigidities (e.g., "I always prefer the left option") that might be stronger in individuals with higher OCI scores.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, oci, model_parameters):
    """
    OCI modulates a fixed Side Bias in Stage 1.
    This captures an intrinsic preference for one spaceship over the other 
    (e.g., spatial bias or color preference) that persists regardless of value.
    OCI modulates the strength and direction of this bias.

    Parameters:
    - learning_rate: [0, 1]
    - beta: [0, 10]
    - w: [0, 1]
    - stick: [0, 5] General stickiness.
    - bias_base: [-5, 5] Base bias towards Action 0 (Spaceship A). Positive = prefer A.
    - bias_oci: [-5, 5] Effect of OCI on the bias.
    """
    learning_rate, beta, w, stick, bias_base, bias_oci = model_parameters
    
    action_1 = action_1.astype(int)
    state = state.astype(int)
    action_2 = action_2.astype(int)
    
    n_trials = len(action_1)
    subject_oci = oci[0]
    
    # Calculate Side Bias
    # Bias is added to the value of Action 0 relative to Action 1
    side_bias = bias_base + (bias_oci * subject_oci)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1
    
    for trial in range(n_trials):
        
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net_s1 = (w * q_stage1_mb) + ((1 - w) * q_stage1_mf)
        
        # Apply Side Bias to Action 0
        q_net_s1[0] += side_bias
        
        # Apply Stickiness
        if last_action_1 != -1:
            q_net_s1[last_action_1] += stick
            
        exp_q1 = np.exp(beta * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        
        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
        
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```