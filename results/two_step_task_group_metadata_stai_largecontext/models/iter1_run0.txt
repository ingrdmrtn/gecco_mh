Here are three new cognitive models that explore different mechanisms by which anxiety (STAI) might influence decision-making in the two-step task.

### Model 1: Anxiety-Modulated Learning Rate Asymmetry
This model hypothesizes that anxiety affects how participants learn from positive versus negative outcomes. High anxiety is often associated with a negativity bias or increased sensitivity to punishment. Here, the STAI score modulates the learning rate specifically for negative prediction errors (disappointments), potentially making anxious individuals update their values more drastically after a loss (or lack of reward) compared to a gain.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Learning Rate Asymmetry (Pos/Neg Bias).
    
    Hypothesis: Anxiety (STAI) modulates the learning rate for negative prediction errors.
    High anxiety individuals may over-weight negative outcomes (losses/omissions) 
    relative to positive ones.
    
    Parameters:
    - alpha_pos: [0, 1] Learning rate for positive prediction errors (RPE > 0).
    - alpha_neg_base: [0, 1] Baseline learning rate for negative prediction errors (RPE < 0).
    - alpha_stai_mod: [-1, 1] Modulation of negative learning rate by STAI.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w: [0, 1] Mixing weight (fixed parameter for MB/MF balance).
    """
    alpha_pos, alpha_neg_base, alpha_stai_mod, beta, w = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the effective negative learning rate based on anxiety
    # We clip to ensure it stays within valid bounds [0, 1]
    alpha_neg = alpha_neg_base + (alpha_stai_mod * stai_score)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)      # Model-free values for Stage 1
    q_stage2 = np.zeros((2, 2))    # Values for Stage 2 (State X/Y, Aliens)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        # Model-Based Value Calculation
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Softmax
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial] # 0 for X, 1 for Y
        
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 Update
        delta_stage2 = r - q_stage2[state_idx, action_2[trial]]
        lr_2 = alpha_pos if delta_stage2 >= 0 else alpha_neg
        q_stage2[state_idx, action_2[trial]] += lr_2 * delta_stage2
        
        # Stage 1 Update (TD(0) - updating MF value based on Stage 2 value)
        # Note: Using the value of the chosen stage 2 state/action as the target
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_1 = alpha_pos if delta_stage1 >= 0 else alpha_neg
        q_stage1_mf[action_1[trial]] += lr_1 * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Choice Randomness (Noise)
This model posits that anxiety acts as a distractor or a source of neural noise, reducing the precision of value-based choices. Instead of altering how values are learned or the balance between systems, anxiety here degrades the ability to select the optimal action given the values. This is modeled by modulating the inverse temperature parameter ($\beta$) with the STAI score. Higher anxiety leads to a lower effective $\beta$ (more exploration/randomness).

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Choice Randomness (Inverse Temperature Modulation).
    
    Hypothesis: Anxiety acts as cognitive noise, reducing the precision of choices.
    Higher STAI scores lower the effective beta (inverse temperature), leading to 
    more random/exploratory behavior regardless of the learned values.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta_base: [0, 10] Baseline inverse temperature (for low anxiety).
    - beta_stai_decay: [0, 10] Amount beta decreases as STAI increases.
    - w: [0, 1] Mixing weight (fixed parameter for MB/MF balance).
    - eligibility: [0, 1] Eligibility trace parameter (lambda) for Stage 1 updates.
    """
    learning_rate, beta_base, beta_stai_decay, w, eligibility = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective beta: High anxiety reduces beta (more noise)
    # We ensure beta doesn't drop below 0
    beta = beta_base - (beta_stai_decay * stai_score)
    beta = np.max([beta, 0.0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        r = reward[trial]
        
        # Stage 2 RPE
        delta_stage2 = r - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Stage 1 RPE (using eligibility trace logic common in these tasks)
        # The update is driven by the Stage 2 RPE, scaled by eligibility
        q_stage1_mf[action_1[trial]] += learning_rate * eligibility * delta_stage2
        
        # Also standard TD(0) update for stage 1 based on stage 2 value onset
        # (Simplified here to just the eligibility update from reward for clarity 
        # and parameter economy, as is common in some 2-step variants, 
        # or we can add the immediate stage 1 update).
        # Let's add the immediate transition update for robustness:
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Perseveration
This model suggests that anxiety increases "stickiness" or perseveration. Anxious individuals might be more risk-averse or rigid, preferring to repeat the last chosen action regardless of the reward outcome, as a safety behavior. The STAI score modulates the `perseveration` parameter, which adds a bonus to the value of the previously chosen action in the softmax step.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Perseveration (Choice Stickiness).
    
    Hypothesis: Anxiety increases behavioral rigidity. High STAI scores increase 
    the tendency to repeat the previous Stage 1 choice (perseveration), 
    independent of reward history.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF mixing weight.
    - pers_base: [0, 5] Baseline perseveration bonus.
    - pers_stai_mod: [0, 5] Additional perseveration bonus scaled by STAI.
    """
    learning_rate, beta, w, pers_base, pers_stai_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate total perseveration bonus
    perseveration_weight = pers_base + (pers_stai_mod * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    last_action_1 = -1 # Initialize with no previous action

    for trial in range(n_trials):
        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add perseveration bonus to the logits (before exponentiation)
        logits = beta * q_net
        if last_action_1 != -1:
            logits[last_action_1] += perseveration_weight
            
        exp_q1 = np.exp(logits)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Store current action for next trial
        last_action_1 = action_1[trial]

        # --- Stage 2 Choice ---
        state_idx = state[trial]
        
        exp_q2 = np.exp(beta * q_stage2[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        r = reward[trial]
        
        delta_stage2 = r - q_stage2[state_idx, action_2[trial]]
        q_stage2[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        delta_stage1 = q_stage2[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```