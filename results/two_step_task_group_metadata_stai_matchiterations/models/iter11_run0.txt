Here are 3 new cognitive models based on the two-step task, incorporating STAI anxiety scores into the decision-making and learning processes.

### Model 1: Anxiety-Modulated Reward Devaluation ("Worry Cost")
This model hypothesizes that anxiety acts as a constant psychological "tax" or "worry cost" on outcomes. Anxious participants may perceive rewards as less valuable (dampened positive affect) or lack of rewards as more painful (heightened negative affect). This is implemented by subtracting a penalty scaled by the STAI score from the objective reward before calculating prediction errors.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Reward Devaluation Model.
    
    Hypothesis: Anxiety introduces a subjective 'worry cost' that devalues rewards.
    High anxiety individuals perceive the effective reward as lower than the objective
    reward, effectively operating in a domain of lower value, which alters 
    Model-Free value estimation.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w: [0, 1] Weight for model-based control (0=MF, 1=MB).
    - worry_cost: [0, 1] The magnitude of reward devaluation scaled by STAI.
    """
    learning_rate, beta, w, worry_cost = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the subjective penalty applied to rewards based on anxiety
    subjective_penalty = worry_cost * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Calculate effective reward (Objective Reward - Anxiety Penalty)
        # This shifts the baseline of what is considered "good" or "bad"
        effective_reward = reward[trial] - subjective_penalty

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Counterfactual Rumination
This model hypothesizes that anxious individuals engage in "rumination" or counterfactual thinking. When they experience a prediction error (positive or negative) on the chosen option, they also update the value of the *unchosen* option in the opposite direction. For example, if the chosen path yields a negative surprise, an anxious agent assumes the unchosen path would have been better ("grass is greener").

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Counterfactual Rumination Model.
    
    Hypothesis: Anxiety drives counterfactual updating ('rumination'). 
    When an anxious participant updates the value of a chosen action based on a 
    prediction error (RPE), they also update the UNCHOSEN action using the 
    inverse of that RPE. E.g., a disappointment (negative RPE) on the chosen 
    option increases the value of the unchosen option (regret).
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - rumination_rate: [0, 1] Scaling factor for updating the unchosen option based on STAI.
    """
    learning_rate, beta, w, rumination_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # The strength of the counterfactual update depends on anxiety
    cf_learning_rate = learning_rate * rumination_rate * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Stage 1 Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Counterfactual update for Stage 1 unchosen option
        # If delta is negative (bad outcome), we assume unchosen was better (add positive value)
        unchosen_action_1 = 1 - action_1[trial]
        q_stage1_mf[unchosen_action_1] -= cf_learning_rate * delta_stage1

        # Stage 2 Updates
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Counterfactual update for Stage 2 unchosen option
        unchosen_action_2 = 1 - action_2[trial]
        q_stage2_mf[state_idx, unchosen_action_2] -= cf_learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Transition Uncertainty
This model hypothesizes that anxiety degrades the precision of the Model-Based system. While the standard model assumes the agent knows the transition matrix is exactly `[[0.7, 0.3], [0.3, 0.7]]`, anxious individuals may be less confident in this structure, perceiving the world as more chaotic (higher entropy). This is modeled by mixing the true transition matrix with a uniform (random) matrix, weighted by STAI.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Transition Uncertainty Model.
    
    Hypothesis: Anxiety reduces confidence in the Model-Based transition structure.
    Instead of using the objective transition matrix (0.7/0.3), anxious participants
    plan using a 'noisier' matrix that is closer to uniform (0.5/0.5). This dilutes
    the effectiveness of Model-Based planning without turning it off completely.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - uncertainty_bias: [0, 1] How much STAI mixes the true matrix with a uniform one.
    """
    learning_rate, beta, w, uncertainty_bias = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Define the objective transition matrix
    true_transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    # Define a maximum entropy (uniform) matrix representing total uncertainty
    uniform_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    # Calculate the subjective transition matrix used for planning
    # High anxiety pulls the matrix towards uniform (0.5/0.5)
    mixing_param = uncertainty_bias * stai_score
    # Ensure mixing param doesn't exceed 1
    if mixing_param > 1.0:
        mixing_param = 1.0
        
    subjective_transition_matrix = (1 - mixing_param) * true_transition_matrix + mixing_param * uniform_matrix

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # The MB calculation uses the anxiety-distorted matrix
        q_stage1_mb = subjective_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```