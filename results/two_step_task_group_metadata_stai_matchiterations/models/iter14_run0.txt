Here are 3 new cognitive models based on the two-step task, incorporating STAI scores into the decision-making and learning processes in novel ways.

### Model 1: Anxiety-Scaled Reward Valuation
This model hypothesizes that anxiety alters the subjective valuation of the reward itself. Rather than changing how *fast* participants learn (learning rate) or how they balance systems (w), this model suggests anxious participants perceive the magnitude of the "gold coin" differently—potentially as a "hyper-reward" due to relief, or dampened due to anhedonia. This scaling factor is applied directly to the reward signal before it enters the prediction error calculation.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Scaled Reward Valuation Model.
    
    Hypothesis: Anxiety alters the subjective magnitude of the reward outcome. 
    High anxiety might lead to hypersensitivity to the positive outcome (relief) 
    or dampening (anhedonia). This scales the reward input to the prediction error.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - reward_sens: [0, 2] Scaling factor for reward valuation. 
      (1.0 = normal, >1.0 = hypersensitive, <1.0 = dampened).
      The effective reward is reward * (1 + (reward_sens - 1) * stai).
    """
    learning_rate, beta, w, reward_sens = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective reward scaling based on anxiety
    # If reward_sens is 1, anxiety has no effect. 
    # If reward_sens > 1, anxiety amplifies reward.
    # If reward_sens < 1, anxiety dampens reward.
    # We center the modulation around 1.0
    scaling_factor = 1.0 + (reward_sens - 1.0) * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (TD-0)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 Update with Anxiety-Scaled Reward
        effective_reward = reward[trial] * scaling_factor
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Anxiety Dampening
This model posits that anxiety specifically degrades decision-making in the first stage (planning under ambiguity) while leaving the second stage (reaction to concrete stimuli) relatively intact. High anxiety introduces "noise" or "indecision" specifically when choosing between spaceships, modeled by reducing the inverse temperature ($\beta$) for Stage 1 based on the STAI score.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Stage-Specific Anxiety Dampening Model.
    
    Hypothesis: Anxiety specifically degrades decision consistency in the 
    planning stage (Stage 1) due to ambiguity, while Stage 2 (concrete choice) 
    remains unaffected. Anxiety lowers beta specifically for the first choice.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Base inverse temperature (for Stage 2).
    - w: [0, 1] Weight for model-based control.
    - s1_damp: [0, 1] Factor by which anxiety reduces Stage 1 beta.
      beta_stage_1 = beta * (1 - s1_damp * stai).
    """
    learning_rate, beta, w, s1_damp = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate Stage 1 specific beta
    # If s1_damp is high and anxiety is high, beta_1 becomes very small (random choice)
    beta_stage_1 = beta * (1.0 - (s1_damp * stai_score))
    # Ensure beta doesn't go negative
    if beta_stage_1 < 0:
        beta_stage_1 = 0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy (Anxiety Modulated) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use the dampened beta for stage 1
        exp_q1 = np.exp(beta_stage_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (Standard Beta) ---
        # Use the base beta for stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Hyper-Vigilant Surprise Learning
This model suggests that anxious individuals are hyper-vigilant to "surprising" events—specifically, the rare transitions (e.g., Spaceship A going to Planet Y). When a rare transition occurs, anxious participants boost their learning rate for that specific trial, updating their beliefs more aggressively than they would after a common transition.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hyper-Vigilant Surprise Learning Model.
    
    Hypothesis: Anxious individuals are hyper-vigilant to unexpected structural 
    events (rare transitions). When a rare transition occurs, the learning rate 
    is boosted by anxiety, leading to stronger updates on 'glitch' trials.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - surprise_boost: [0, 5] Multiplier for learning rate on rare transitions.
      lr_effective = lr * (1 + surprise_boost * stai) if transition was rare.
    """
    learning_rate, beta, w, surprise_boost = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]
        chosen_action_1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Determine if transition was Rare or Common ---
        # Common: (Action 0 -> State 0) or (Action 1 -> State 1)
        # Rare:   (Action 0 -> State 1) or (Action 1 -> State 0)
        is_rare = False
        if chosen_action_1 == 0 and state_idx == 1:
            is_rare = True
        elif chosen_action_1 == 1 and state_idx == 0:
            is_rare = True

        # Calculate effective learning rate for this trial
        current_lr = learning_rate
        if is_rare:
            current_lr = learning_rate * (1.0 + surprise_boost * stai_score)
            # Cap LR at 1.0 to prevent instability
            if current_lr > 1.0:
                current_lr = 1.0

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```