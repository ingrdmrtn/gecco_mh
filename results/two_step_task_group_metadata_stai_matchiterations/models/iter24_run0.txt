Here are 3 new cognitive models based on the two-step task structure and anxiety literature.

### Model 1: Anxiety-Dependent Default Bias
This model hypothesizes that anxiety increases a static preference for a "default" or "safe" option (Spaceship A), regardless of reward history. This differs from perseveration (stickiness), which repeats the *previous* choice. Here, anxiety acts as a constant bias towards Action 0 (Spaceship A), reflecting a tendency to stick to the status quo or the most common transition path (A->X) under stress.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Dependent Default Bias Model.
    
    Hypothesis: Anxiety induces a static bias towards Spaceship A (Action 0), 
    treating it as a 'default' or 'safe' option regardless of history.
    
    Parameters:
    - learning_rate: [0, 1] Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - default_bias: [0, 5] Magnitude of the static bias towards Action 0, scaled by STAI.
    """
    learning_rate, beta, w, default_bias = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the static bias for this participant
    bias_value = default_bias * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply static bias to Action 0 (Spaceship A)
        q_net[0] += bias_value

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Counterfactual Rumination
This model hypothesizes that anxious individuals engage in "rumination" or counterfactual thinking. When they observe an outcome for a chosen alien, they update the value of the *unchosen* alien in the opposite direction (assuming anticorrelation: if chosen was bad, unchosen was likely good). The strength of this counterfactual update is modulated by STAI.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Counterfactual Rumination Model.
    
    Hypothesis: Anxious participants update the value of the unchosen alien (Stage 2)
    based on the outcome of the chosen alien ('If I lost, the other one was probably a winner').
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - rumination_rate: [0, 1] Fraction of the learning rate applied to the unchosen option, scaled by STAI.
    """
    learning_rate, beta, w, rumination_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Standard update for chosen option
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Counterfactual update for unchosen option (Rumination)
        # If reward was 1, assume unchosen was 0. If reward was 0, assume unchosen was 1.
        unchosen_action_2 = 1 - action_2[trial]
        assumed_reward_unchosen = 1.0 - reward[trial]
        
        delta_cf = assumed_reward_unchosen - q_stage2_mf[state_idx, unchosen_action_2]
        
        # Apply update scaled by STAI and rumination parameter
        effective_rumination = rumination_rate * stai_score
        q_stage2_mf[state_idx, unchosen_action_2] += learning_rate * effective_rumination * delta_cf

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Post-Error Panic
This model hypothesizes that anxiety affects exploration dynamics specifically after negative outcomes. If the previous trial resulted in no reward (0 coins), anxious participants experience a "panic" response that increases decision noise (lowers beta) for the subsequent Stage 1 choice. If the previous trial was rewarded, they maintain a baseline level of focus.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Post-Error Panic Model.
    
    Hypothesis: Anxiety causes a temporary increase in decision noise (reduction in beta)
    immediately following a lack of reward (0 coins), representing a 'panic' response.
    
    Parameters:
    - learning_rate: [0, 1] Q-learning rate.
    - beta_base: [0, 10] Baseline inverse temperature (used after rewards).
    - w: [0, 1] Model-based weight.
    - panic_damp: [0, 1] Percentage reduction in beta after a loss, scaled by STAI.
    """
    learning_rate, beta_base, w, panic_damp = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_reward = 1.0 # Initialize assuming calm state (as if rewarded)

    for trial in range(n_trials):

        # Determine current beta based on previous outcome
        current_beta = beta_base
        if last_reward == 0.0:
            # Reduce beta (increase noise) proportional to anxiety
            current_beta = beta_base * (1.0 - (panic_damp * stai_score))
            # Ensure beta doesn't go negative
            current_beta = max(0.0, current_beta)

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice (using baseline beta, assuming panic is primarily about the next planning step)
        exp_q2 = np.exp(beta_base * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```