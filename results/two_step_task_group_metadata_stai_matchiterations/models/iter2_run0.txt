Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process in distinct ways.

### Model 1: Anxiety-Modulated Exploration (Inverse Temperature)
This model hypothesizes that anxiety affects the exploration-exploitation trade-off. Specifically, high anxiety might lead to more rigid, exploitative behavior (higher beta), reducing the tendency to explore uncertain options, or conversely, it might lead to more random behavior due to stress. Here, we model anxiety as scaling the inverse temperature `beta`.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Exploration (Inverse Temperature).
    
    Hypothesis: Anxiety (STAI) modulates the inverse temperature (beta), affecting
    the exploration-exploitation trade-off. High anxiety might lead to more 
    deterministic choices (higher beta) or more noise (lower beta), depending on the sign of the modulation.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate for Q-values.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - beta_mod: [0, 5] Magnitude of anxiety's effect on beta.
    """
    learning_rate, beta_base, w, beta_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Anxiety modulates beta. We allow it to increase beta (more exploitation/rigidity).
    # Effective beta = beta_base + (beta_mod * stai_score)
    # Note: If beta_mod is large, high anxiety leads to very deterministic choices.
    beta_eff = beta_base + (beta_mod * stai_score)
    
    # Ensure beta doesn't exceed reasonable bounds for numerical stability
    beta_eff = np.clip(beta_eff, 0.0, 20.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_eff * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta_eff * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Stage 1 Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Perseveration (Stickiness)
This model tests the hypothesis that anxiety increases "stickiness" or perseverationâ€”the tendency to repeat the previous choice regardless of the outcome. High anxiety individuals might rely on repetitive motor habits as a coping mechanism or due to reduced cognitive flexibility.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Perseveration (Stickiness).
    
    Hypothesis: Anxiety increases the tendency to repeat the previous choice 
    (perseveration), independent of reward history. This is modeled as a 'stickiness' 
    bonus added to the Q-value of the previously chosen action, scaled by STAI.
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - stick_base: [0, 5] Baseline tendency to repeat choices.
    - stick_anx: [0, 5] Additional stickiness driven by anxiety score.
    """
    learning_rate, beta, w, stick_base, stick_anx = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective stickiness
    stickiness = stick_base + (stick_anx * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1 # Initialize with an impossible action

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Add stickiness bonus to the Q-values before softmax
        q_net_sticky = q_net.copy()
        if last_action_1 != -1:
            q_net_sticky[last_action_1] += stickiness
        
        exp_q1 = np.exp(beta * q_net_sticky)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Decision ---
        # Note: Stickiness is usually modeled primarily on the first stage choice in this task
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Update history
        last_action_1 = action_1[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Eligibility Trace (Lambda)
This model introduces an eligibility trace parameter ($\lambda$) that controls how much the second-stage outcome updates the first-stage value directly (TD($\lambda$)). The hypothesis is that anxiety affects the credit assignment process. High anxiety might impair the ability to link distal rewards to initial choices (lower $\lambda$), or conversely, heighten sensitivity to the chain of events.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Eligibility Trace (Lambda).
    
    Hypothesis: Anxiety modulates the eligibility trace parameter (lambda), which 
    controls the efficiency of credit assignment from the second stage reward back 
    to the first stage choice. 
    
    Parameters:
    - learning_rate: [0, 1] Learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - lambda_base: [0, 1] Baseline eligibility trace.
    - lambda_mod: [-1, 1] Modulation of lambda by STAI.
    """
    learning_rate, beta, w, lambda_base, lambda_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective lambda
    # We use a sigmoid-like transformation or clipping to keep lambda in [0, 1]
    # Here we use simple linear modulation with clipping.
    lambda_eff = lambda_base + (lambda_mod * stai_score)
    lambda_eff = np.clip(lambda_eff, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        # Stage 2 Prediction Error
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 Q-value
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

        # Stage 1 Prediction Error (Standard TD(0) part)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1 Q-value using Lambda
        # Q1 = Q1 + alpha * delta1 + alpha * lambda * delta2
        # This allows the stage 2 RPE to directly influence stage 1 Q-values
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1 + (learning_rate * lambda_eff * delta_stage2)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```