Here are three new cognitive models that incorporate STAI anxiety scores into the decision-making process in distinct ways.

### Model 1: Anxiety-Modulated Transition Learning
This model hypothesizes that high anxiety impairs the ability to update the internal model of the environment (the transition matrix). While standard models often assume a fixed transition matrix (70/30), this model allows the agent to learn the transition matrix, but anxiety dampens the learning rate for these state transitions, reflecting a rigidity or difficulty in updating structural knowledge under stress.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Transition Learning Model.
    
    Hypothesis: High anxiety reduces the learning rate for state transitions.
    Anxious individuals may rely on prior beliefs about structure and fail to 
    update their internal model of the world (transition matrix) as effectively 
    as low-anxiety individuals when transitions occur.
    
    Parameters:
    - learning_rate_reward: [0, 1] Learning rate for reward values (Q-values).
    - beta: [0, 10] Inverse temperature for softmax.
    - w: [0, 1] Weight for model-based control.
    - lr_transition_base: [0, 1] Base learning rate for the transition matrix.
    - anxiety_damp: [0, 1] How much STAI reduces transition learning (scaling factor).
    """
    learning_rate_reward, beta, w, lr_transition_base, anxiety_damp = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Effective transition learning rate is reduced by anxiety
    # If anxiety_damp is high and STAI is high, lr_transition becomes small.
    lr_transition = lr_transition_base * (1.0 - (anxiety_damp * stai_score))
    lr_transition = max(0.0, lr_transition) # Ensure non-negative

    # Initialize transition matrix (start with uniform or slight bias, here uniform for learning)
    # Rows: Action 1 (0 or 1), Cols: State (0 or 1)
    # We track counts or probabilities. Here we do incremental probability updating.
    # Initial belief is the true structure to start, but it can drift.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Model-Based Value: Uses the dynamic transition matrix
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 or 1 (Planet X or Y)
        a1 = action_1[trial]     # 0 or 1 (Spaceship A or B)

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # 1. Update Reward Q-values (Standard TD)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[a1]
        q_stage1_mf[a1] += learning_rate_reward * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate_reward * delta_stage2
        
        # 2. Update Transition Matrix
        # We observed transition: a1 -> state_idx.
        # We increase prob of a1->state_idx and decrease a1->other_state
        # Update row a1:
        # T(a1, state_idx) = T(a1, state_idx) + lr * (1 - T(a1, state_idx))
        # T(a1, other)     = T(a1, other)     + lr * (0 - T(a1, other))
        
        transition_matrix[a1, state_idx] += lr_transition * (1 - transition_matrix[a1, state_idx])
        transition_matrix[a1, 1-state_idx] += lr_transition * (0 - transition_matrix[a1, 1-state_idx])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced "Safe Bet" Heuristic
This model posits that anxiety introduces a bias toward a specific "safe" option (e.g., Spaceship A) regardless of value. This is distinct from perseveration (repeating the *last* choice). Instead, it represents a heuristic avoidance of uncertainty by defaulting to a known or preferred baseline option when anxiety is high.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced 'Safe Bet' Heuristic Model.
    
    Hypothesis: Anxiety increases the reliance on a fixed heuristic or 'default' 
    choice (e.g., always choosing option A), effectively acting as a bias term 
    added to the Q-value of Option 0 (Spaceship A). This reflects a desire to 
    reduce cognitive load or uncertainty by sticking to a 'safe bet'.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - heuristic_weight: [0, 2] Base magnitude of the heuristic bias.
    - anxiety_scale: [0, 2] How strongly STAI amplifies this heuristic bias.
    """
    learning_rate, beta, w, heuristic_weight, anxiety_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # The bias added to Option 0 (Spaceship A)
    # If anxiety is high, the bias towards the 'safe' option increases.
    safe_bet_bias = heuristic_weight + (anxiety_scale * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply the Safe Bet Bias to Option 0
        q_net[0] += safe_bet_bias

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Outcome Sensitivity Asymmetry
This model suggests that anxious individuals process positive and negative prediction errors differently. Specifically, high anxiety might amplify the learning rate for negative prediction errors (punishment sensitivity) or dampen the learning rate for positive prediction errors (anhedonia/dampened reward processing).

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Outcome Sensitivity Asymmetry Model.
    
    Hypothesis: Anxiety alters how prediction errors are processed. 
    Specifically, this model splits the learning rate into positive (better than expected)
    and negative (worse than expected) components. Anxiety is hypothesized to 
    amplify the learning rate for negative outcomes (loss sensitivity).
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors.
    - lr_neg_base: [0, 1] Base learning rate for negative prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - anxiety_sens: [0, 2] Scaling factor: how much STAI boosts negative learning rate.
    """
    lr_pos, lr_neg_base, beta, w, anxiety_sens = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the effective negative learning rate
    # High anxiety increases the weight given to negative surprises.
    # We clip at 1.0 to maintain valid learning rate bounds.
    lr_neg = min(1.0, lr_neg_base * (1.0 + (anxiety_sens * stai_score)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        if delta_stage1 >= 0:
            q_stage1_mf[action_1[trial]] += lr_pos * delta_stage1
        else:
            q_stage1_mf[action_1[trial]] += lr_neg * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 >= 0:
            q_stage2_mf[state_idx, action_2[trial]] += lr_pos * delta_stage2
        else:
            q_stage2_mf[state_idx, action_2[trial]] += lr_neg * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```