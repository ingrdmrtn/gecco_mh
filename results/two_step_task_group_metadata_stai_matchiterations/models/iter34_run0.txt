Here are 3 new cognitive models implemented as Python functions.

### Model 1: Fictitious Update (Counterfactual) Model
This model hypothesizes that anxious individuals engage in "counterfactual updating." When they receive a reward (or lack thereof) for their chosen action, they also update the value of the *unchosen* action, simulating what would have happened if they had chosen differently. High anxiety increases the weight of this fictitious learning, effectively "over-thinking" the road not taken.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Fictitious Update (Counterfactual) Model.
    
    Hypothesis: Anxiety drives learning about the unchosen path (counterfactuals).
    While standard RL only updates the chosen action, this model updates the 
    unchosen Stage 1 action towards the *opposite* of the received reward 
    (assuming the other path would have yielded the opposite outcome), 
    scaled by STAI.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - cf_lr_scale: [0, 1] Scaling factor for the counterfactual learning rate.
      The effective learning rate for the unchosen option is:
      alpha_cf = learning_rate * cf_lr_scale * stai
    """
    learning_rate, beta, w, cf_lr_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Standard Update for Chosen Action
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Fictitious Update for Unchosen Action
        # Hypothesis: If I got 0, I assume the other path had 1. If I got 1, I assume other had 0.
        # This is a simplification of counterfactual reasoning often seen in anxiety.
        unchosen_action = 1 - action_1[trial]
        fictitious_outcome = 1.0 - reward[trial] # The "grass is greener" assumption
        
        # Note: We use the stored Q-value of the unchosen action and update it 
        # towards the fictitious outcome.
        # We use the expected value of the *other* state (approximate) or just the raw fictitious reward.
        # Here we update Stage 1 MF value directly towards the fictitious reward.
        delta_cf = fictitious_outcome - q_stage1_mf[unchosen_action]
        effective_cf_lr = learning_rate * cf_lr_scale * stai_score
        q_stage1_mf[unchosen_action] += effective_cf_lr * delta_cf

        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Surprise-Modulated Learning Model
This model posits that anxious individuals are hyper-sensitive to "prediction errors" regarding the environment structure. When a "Rare" transition occurs (e.g., choosing Spaceship A but landing on Planet Y), it triggers a spike in arousal that boosts the learning rate for that specific trial. Anxious agents learn significantly more from these surprising events than from common ones.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Surprise-Modulated Learning Model.
    
    Hypothesis: Anxiety increases the learning rate specifically following 
    'Rare' transitions (prediction errors in the transition structure).
    Anxious individuals may over-interpret the significance of rare events,
    updating their Model-Free values more aggressively when the transition 
    violates the common rule.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - rare_boost: [0, 5] Multiplier for the learning rate when a Rare transition occurs.
      If transition is Rare: alpha = learning_rate * (1 + rare_boost * stai)
      If transition is Common: alpha = learning_rate
    """
    learning_rate, beta, w, rare_boost = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Determine if transition was Common or Rare
        # Action 0 (A) -> State 0 (X) is Common. Action 1 (U) -> State 1 (Y) is Common.
        # If action == state, it's Common. If action != state, it's Rare.
        is_rare = (action_1[trial] != state[trial])
        
        # Calculate dynamic learning rate
        if is_rare:
            current_lr = learning_rate * (1.0 + rare_boost * stai_score)
            # Cap LR at 1.0 to prevent instability
            current_lr = min(current_lr, 1.0)
        else:
            current_lr = learning_rate

        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Pessimistic Planning Integration Model
Standard Model-Based RL assumes the agent will choose the *best* option at Stage 2 (Max operator). This model suggests that anxious individuals are pessimistic about their future agency or the environment's reliability. Instead of planning based solely on the maximum possible value of the next stage, they incorporate the *minimum* value (the worst-case scenario), weighted by their anxiety level.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Pessimistic Planning Integration Model.
    
    Hypothesis: Anxiety alters the Model-Based planning computation.
    Standard MB calculates the value of a state as the transition-weighted 
    MAX value of the next stage (optimistic planning). 
    This model proposes that anxiety introduces pessimism: the agent plans 
    as if they might end up with the WORST outcome at Stage 2.
    
    The MB value is a weighted average of Max(Q_s2) and Min(Q_s2), 
    controlled by STAI.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] MB/MF weighting.
    - pessimism_weight: [0, 1] How strongly STAI drives the agent towards Min(Q).
      Planning Value = (1 - rho)*Max(Q) + rho*Min(Q)
      where rho = pessimism_weight * stai
    """
    learning_rate, beta, w, pessimism_weight = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
  
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        
        # Standard Optimistic Planning (Max)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Pessimistic Planning (Min) - assuming worst case at next stage
        min_q_stage2 = np.min(q_stage2_mf, axis=1)
        
        # Calculate Pessimism Factor (rho)
        rho = pessimism_weight * stai_score
        
        # Blended Value for Planning
        integrated_q_stage2 = (1 - rho) * max_q_stage2 + rho * min_q_stage2
        
        q_stage1_mb = transition_matrix @ integrated_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```