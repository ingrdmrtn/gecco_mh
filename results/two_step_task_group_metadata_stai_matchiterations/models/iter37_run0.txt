Here are 3 new cognitive models based on the two-step task and anxiety (STAI) data.

### Model 1: Transition-Shock Arbitration
**Hypothesis:** Anxious individuals have a lower tolerance for "surprise." When a rare transition occurs (e.g., choosing Spaceship A but landing on Planet Y), it acts as a "shock" that undermines their confidence in their internal model of the world (Model-Based control). Consequently, on the *subsequent* trial, anxious participants retreat to Model-Free (habitual) control, reducing the weight `w`.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Transition-Shock Arbitration Model.
    
    Hypothesis: Rare transitions (surprises) undermine confidence in the Model-Based 
    system. Anxious individuals react to these shocks by reducing 'w' (the weight 
    of MB control) on the subsequent trial, reverting to Model-Free habits.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_base: [0, 1] Baseline weight for model-based control.
    - shock_sens: [0, 1] How much STAI reduces 'w' after a rare transition.
    """
    learning_rate, beta, w_base, shock_sens = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Transition matrix: A(0)->X(0) is 0.7, A(0)->Y(1) is 0.3
    # U(1)->Y(1) is 0.7, U(1)->X(0) is 0.3
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track if the previous trial was a rare transition
    prev_was_rare = False

    for trial in range(n_trials):
        # 1. Calculate Stage 1 Values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # 2. Dynamic Arbitration (w)
        # If previous trial was rare, anxiety reduces w (retreat to habit)
        if prev_was_rare:
            # shock_sens * stai determines the magnitude of the drop
            # We clip to ensure w stays non-negative
            current_w = max(0.0, w_base - (shock_sens * stai_score))
        else:
            current_w = w_base

        q_net = current_w * q_stage1_mb + (1 - current_w) * q_stage1_mf

        # 3. Stage 1 Choice Probability
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # 0 for X, 1 for Y
        chosen_action_1 = action_1[trial] # 0 for A, 1 for U

        # Determine if THIS trial is rare for the next loop
        # Common: (A->X) or (U->Y). Rare: (A->Y) or (U->X)
        # A=0, X=0 -> Common. A=0, Y=1 -> Rare.
        if chosen_action_1 == state_idx:
            prev_was_rare = False
        else:
            prev_was_rare = True

        # 4. Stage 2 Choice Probability
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # 5. Learning Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_action_1]
        q_stage1_mf[chosen_action_1] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage-Specific Rigidity
**Hypothesis:** Anxiety does not affect decision noise uniformly. While Stage 1 involves abstract planning (choosing a spaceship), Stage 2 involves immediate reward consumption (choosing an alien). This model posits that anxiety increases "rigidity" (higher inverse temperature $\beta$) specifically at Stage 2, making anxious participants more likely to greedily exploit the best-looking alien, while remaining more exploratory at Stage 1.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Stage-Specific Rigidity Model.
    
    Hypothesis: Anxiety increases exploitation (rigidity) specifically at the 
    second stage (proximal reward), while Stage 1 remains more exploratory.
    This is modeled by a separate beta boost for Stage 2 scaled by STAI.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta_1: [0, 10] Inverse temperature for Stage 1 (Spaceship choice).
    - w: [0, 1] Weight for model-based control.
    - stg2_rigidity: [0, 5] Additional beta added to Stage 2 for anxious agents.
    """
    learning_rate, beta_1, w, stg2_rigidity = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate Stage 2 beta: Base beta + anxiety boost
    # Anxious people are more deterministic/rigid at the alien choice
    beta_2 = beta_1 + (stg2_rigidity * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice (Uses beta_1)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # Policy for the second choice (Uses beta_2)
        exp_q2 = np.exp(beta_2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Structural Confusion (Entropy Injection)
**Hypothesis:** Anxious individuals may struggle to maintain a precise mental map of the environment's structure. Instead of using the clear `[0.7, 0.3]` transition matrix for Model-Based planning, their internal model is "confused" or entropic. This model mixes the true transition matrix with a uniform distribution `[0.5, 0.5]`, where the degree of mixing is determined by anxiety. This blurs the distinction between "common" and "rare" paths in their planning.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Structural Confusion Model.
    
    Hypothesis: Anxiety introduces entropy into the mental model of the environment.
    The agent's internal transition matrix is a mix of the true matrix and a 
    uniform (random) matrix. High anxiety flattens the probabilities, making 
    Model-Based planning less effective/distinct.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - confusion: [0, 1] Weight of the uniform matrix (noise) added by anxiety.
    """
    learning_rate, beta, w, confusion = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # True structure
    true_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    # Uniform structure (maximum entropy/confusion)
    uniform_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    # Calculate the subjective transition matrix
    # confusion_factor scales with STAI. 
    # If confusion_factor * stai = 1, the agent sees the world as totally random.
    mix_param = min(1.0, confusion * stai_score)
    
    subjective_matrix = (1 - mix_param) * true_matrix + mix_param * uniform_matrix

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        # Uses the SUBJECTIVE matrix for MB calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```