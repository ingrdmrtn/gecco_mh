Here are 3 new cognitive models implemented as Python functions.

### Model 1: Anxiety-Induced Transition Entropy
This model hypothesizes that anxiety consumes working memory resources required to maintain an accurate map of the task structure. As anxiety increases, the participant's internal representation of the transition matrix "blurs" towards randomness (50/50), making Model-Based planning less effective and effectively forcing a reliance on Model-Free habits, not by choice (low `w`), but by degraded structural knowledge.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Transition Entropy Model.
    
    Hypothesis: Anxiety degrades the precision of the internal model of the environment.
    High anxiety 'blurs' the transition matrix towards uniform probability (0.5/0.5),
    making Model-Based predictions noisy and less distinct, effectively degrading 
    planning capability.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - entropy_scale: [0, 1] How much STAI blurs the transition matrix. 
      (0 = perfect matrix, 1 = max blurring based on STAI).
    """
    learning_rate, beta, w, entropy_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # True transition matrix: Row 0=Act A, Row 1=Act U. Col 0=State X, Col 1=State Y
    true_transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Calculate the subjective transition matrix based on anxiety
    # As blur approaches 1, matrix approaches [[0.5, 0.5], [0.5, 0.5]]
    blur_amount = entropy_scale * stai_score
    # Ensure blur doesn't exceed 1
    blur_amount = min(blur_amount, 1.0) 
    
    subjective_transition_matrix = true_transition_matrix * (1 - blur_amount) + 0.5 * blur_amount

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the ANXIETY-BLURRED matrix for planning
        q_stage1_mb = subjective_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # MF Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # MF Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Outcome Clinging
This model proposes that anxious individuals exhibit a specific form of "safety seeking" behavior. Instead of just repeating an action (perseveration), they try to return to the *State* (Planet) where they last felt safe (received a reward). If they got gold at Planet X, they bias their next choice toward the spaceship most likely to take them back to Planet X, regardless of the actual calculated value.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Outcome Clinging Model.
    
    Hypothesis: Anxiety drives a heuristic to return to the last 'safe' (rewarded) 
    environment state. If a reward was received at Planet X, the agent receives a 
    bonus for the action most likely to lead to Planet X in the next trial, 
    bypassing value calculation.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - cling_bias: [0, 5] Magnitude of the bias to return to the rewarded state, scaled by STAI.
    """
    learning_rate, beta, w, cling_bias = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_rewarded_state = -1 # -1 indicates no recent reward to cling to

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply Clinging Bias
        if last_rewarded_state != -1:
            # Determine which action leads to the last rewarded state most often
            # transition_matrix[:, state] gives prob of reaching 'state' for Action 0 and 1
            # We add a bonus proportional to the probability of reaching that safe state
            
            safety_bonus_vector = transition_matrix[:, last_rewarded_state] * (cling_bias * stai_score)
            q_net += safety_bonus_vector

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update tracking for next trial
        if reward[trial] == 1:
            last_rewarded_state = state_idx
        else:
            last_rewarded_state = -1 # Reset if no reward found

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Future Discounting
This model posits that anxiety creates a "myopic" view of value. While the Model-Based system calculates the expected value of the second stage (the aliens), anxious participants discount this future value because it feels uncertain or distant. They rely more on the immediate "feeling" of the spaceship (MF) because the MB value is dampened by a discount factor proportional to their anxiety.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Future Discounting Model.
    
    Hypothesis: Anxiety causes temporal myopia or discounting of downstream values.
    When planning (Model-Based), anxious participants dampen the estimated value 
    of the second stage (the planets/aliens) before integrating it into the 
    first-stage decision. This reduces the effective 'signal' of the MB system 
    without changing the weight 'w' directly.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - discount_scale: [0, 1] How much STAI reduces the value of Stage 2 estimates.
    """
    learning_rate, beta, w, discount_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate the discount factor based on anxiety
    # If discount_scale is high and STAI is high, future_discount becomes small (<1)
    future_discount = 1.0 - (discount_scale * stai_score)
    # Ensure it doesn't go below 0
    future_discount = max(future_discount, 0.0)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # Get max values from stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Apply Anxiety Discount: The value of the planets looks "smaller" to anxious agents
        discounted_stage2_values = max_q_stage2 * future_discount
        
        # MB calculation uses these discounted values
        q_stage1_mb = transition_matrix @ discounted_stage2_values
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```