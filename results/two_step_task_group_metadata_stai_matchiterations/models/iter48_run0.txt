Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process in distinct ways.

### Model 1: Anxiety-Induced Model-Based Suppression
This model hypothesizes that high anxiety consumes cognitive resources (working memory), thereby suppressing the computationally expensive Model-Based (MB) system. Instead of a fixed weight `w`, the weight given to the MB system is dynamically reduced as anxiety increases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Model-Based Suppression.

    Hypothesis: High anxiety consumes working memory resources required for 
    model-based planning. Therefore, the weight (w) assigned to the model-based 
    controller is inversely proportional to the STAI score.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate for value updates.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w_max: [0, 1] The maximum possible model-based weight (at 0 anxiety).
    - suppression_rate: [0, 5] How strongly anxiety suppresses the w parameter.
    """
    learning_rate, beta, w_max, suppression_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the effective weight w based on anxiety
    # As anxiety (stai_score) increases, w decreases from w_max
    w = w_max * np.exp(-suppression_rate * stai_score)
    
    # Ensure w stays within bounds [0, 1]
    w = np.clip(w, 0, 1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: V(S') * T
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Negative Learning Bias
This model posits that anxious individuals are hypersensitive to negative outcomes (omission of reward). While they learn from rewards normally, they update their value estimates more aggressively when a reward is *not* received, reflecting a "pessimism bias" or fear of failure.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Negative Learning Bias.

    Hypothesis: Anxiety creates an asymmetry in learning. Anxious individuals 
    learn more intensely from negative prediction errors (lack of reward) than 
    positive ones. The learning rate for negative errors is boosted by the STAI score.

    Parameters:
    - lr_base: [0, 1] Base learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - neg_lr_boost: [0, 5] Multiplier for STAI to increase learning rate on negative errors.
    """
    lr_base, beta, w, neg_lr_boost = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the specific learning rate for negative prediction errors
    # It is the base rate plus a boost proportional to anxiety
    lr_neg = lr_base + (neg_lr_boost * stai_score)
    lr_neg = np.clip(lr_neg, 0, 1) # Ensure it doesn't exceed 1

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Determine which learning rate to use based on the sign of the error
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        current_lr_1 = lr_base if delta_stage1 >= 0 else lr_neg
        q_stage1_mf[action_1[trial]] += current_lr_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        current_lr_2 = lr_base if delta_stage2 >= 0 else lr_neg
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Transition Learning
This model suggests that anxiety affects how participants learn the structure of the environment (the transition matrix). High anxiety might lead to "over-updating" the transition probabilities after rare events (believing the world has changed when it hasn't), or conversely, rigidity. Here, we model anxiety as increasing the learning rate for the *transition matrix* itself, making the internal model unstable.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Transition Learning.

    Hypothesis: Anxiety destabilizes the internal model of the environment. 
    Instead of a fixed transition matrix, the participant updates their belief 
    about transition probabilities trial-by-trial. Anxiety increases the 
    learning rate of these transitions (lr_trans), causing the participant to 
    over-react to rare transitions (stochastic volatility).

    Parameters:
    - learning_rate_reward: [0, 1] Learning rate for reward values (Q-values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - lr_trans_mod: [0, 1] How much STAI scales the transition learning rate.
    """
    learning_rate_reward, beta, w, lr_trans_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Transition learning rate scales with anxiety
    lr_trans = lr_trans_mod * stai_score
    lr_trans = np.clip(lr_trans, 0, 1)

    # Initialize dynamic transition matrix (starts at true probabilities)
    # Row 0: Action A -> [Planet X, Planet Y]
    # Row 1: Action U -> [Planet X, Planet Y]
    # Note: In the data, Action A=0, U=1. Planet X=0, Y=1.
    # A usually goes to X (0->0 is 0.7), U usually goes to Y (1->1 is 0.7)
    est_transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Use the DYNAMIC estimated transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = est_transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial] # The actual planet arrived at (0 or 1)
        chosen_action_1 = action_1[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Value Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[chosen_action_1]
        q_stage1_mf[chosen_action_1] += learning_rate_reward * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate_reward * delta_stage2

        # --- Transition Matrix Update ---
        # Create a one-hot vector for the state we actually arrived at
        # If we arrived at state 0, observed is [1, 0]. If state 1, [0, 1].
        observed_transition = np.zeros(2)
        observed_transition[state_idx] = 1.0
        
        # Update the row corresponding to the chosen action
        # New_Estimate = Old_Estimate + lr * (Observed - Old_Estimate)
        est_transition_matrix[chosen_action_1] += lr_trans * (observed_transition - est_transition_matrix[chosen_action_1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```