Here are three new cognitive models that incorporate STAI anxiety scores into the decision-making process in distinct ways.

### Model 1: Anxiety-Induced Model-Based Suppression
This model hypothesizes that high anxiety consumes cognitive resources (working memory), thereby suppressing the ability to use complex Model-Based (MB) planning. Instead of a fixed weight `w`, the weight given to the model-based system is dynamically reduced as anxiety increases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Model-Based Suppression Model.

    Hypothesis: High anxiety consumes working memory resources required for 
    model-based planning. Therefore, the weight (w) assigned to the model-based 
    controller is suppressed as STAI score increases.
    
    Parameters:
    - learning_rate: [0, 1] Update rate for Q-values.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w_max: [0, 1] The maximum possible model-based weight (at 0 anxiety).
    - suppression_rate: [0, 1] How strongly anxiety reduces w_max.
    """
    learning_rate, beta, w_max, suppression_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective w based on anxiety
    # If anxiety is 0, w = w_max. If anxiety is 1, w is reduced by suppression_rate.
    w = w_max * (1 - (suppression_rate * stai_score))
    
    # Ensure w stays within bounds [0, 1]
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid Value
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF Update (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Negative Learning Bias
This model posits that anxious individuals are hypersensitive to negative outcomes (omission of reward). While they learn from rewards normally, they update their value estimates more drastically when a reward is *not* received, effectively having a separate, boosted learning rate for losses scaled by their anxiety.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Negative Learning Bias Model.

    Hypothesis: Anxiety creates a hypersensitivity to negative outcomes (0 reward).
    The learning rate for unrewarded trials is boosted by the STAI score, causing
    anxious participants to abandon non-rewarding options faster than rewarding ones.

    Parameters:
    - lr_pos: [0, 1] Base learning rate for positive outcomes (reward=1).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - neg_bias: [0, 1] Additional boost to learning rate for negative outcomes (reward=0), scaled by STAI.
    """
    lr_pos, beta, w, neg_bias = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Determine Learning Rate ---
        # If reward is 0, learning rate is boosted by anxiety
        current_reward = reward[trial]
        if current_reward == 1:
            current_lr = lr_pos
        else:
            # Boost LR for negative outcomes based on anxiety
            # Bounded at 1.0
            current_lr = min(1.0, lr_pos + (neg_bias * stai_score))

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        delta_stage2 = current_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Transition Learning
This model suggests that anxiety affects how participants learn the structure of the environment (the transition matrix). High anxiety might lead to "over-updating" the transition probabilities after rare events (a "jumpy" belief system), making the internal model of the world unstable.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Transition Learning Model.

    Hypothesis: Instead of a fixed transition matrix (70/30), participants learn 
    transitions dynamically. Anxiety increases the learning rate for these transitions 
    (lr_trans), causing anxious individuals to over-react to rare transitions 
    and perceive the environment as more volatile than it is.

    Parameters:
    - learning_rate_reward: [0, 1] Learning rate for reward values (Q-values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - lr_trans_base: [0, 1] Base learning rate for the transition matrix.
    - anxiety_volatility: [0, 1] How much STAI increases the transition learning rate.
    """
    learning_rate_reward, beta, w, lr_trans_base, anxiety_volatility = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Initialize dynamic transition matrix (start with true priors or flat)
    # Rows: Action 1 (A or U), Cols: State (X or Y)
    # Initialize near the true 0.7/0.3 to represent instruction knowledge, but allow drift
    trans_probs = np.array([[0.7, 0.3], [0.3, 0.7]]) 
    
    # Calculate effective transition learning rate
    lr_trans = min(1.0, lr_trans_base + (anxiety_volatility * stai_score))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Use dynamic trans_probs for MB calculation
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = trans_probs @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        # Observe state transition
        chosen_action = action_1[trial]
        actual_state = state[trial] # 0 for X, 1 for Y
        
        # --- Update Transition Matrix ---
        # Create a one-hot vector for the state that actually occurred
        state_one_hot = np.zeros(2)
        state_one_hot[actual_state] = 1.0
        
        # Update the row corresponding to the chosen action
        # New_Prob = Old_Prob + lr * (Outcome - Old_Prob)
        trans_probs[chosen_action] += lr_trans * (state_one_hot - trans_probs[chosen_action])
        
        # Normalize to ensure probabilities sum to 1 (numerical stability)
        trans_probs[chosen_action] /= np.sum(trans_probs[chosen_action])

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[actual_state])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Reward Updates ---
        delta_stage1 = q_stage2_mf[actual_state, action_2[trial]] - q_stage1_mf[chosen_action]
        q_stage1_mf[chosen_action] += learning_rate_reward * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[actual_state, action_2[trial]]
        q_stage2_mf[actual_state, action_2[trial]] += learning_rate_reward * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```