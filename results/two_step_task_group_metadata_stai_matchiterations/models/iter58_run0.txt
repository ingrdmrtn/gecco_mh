Here are three new cognitive models that incorporate STAI scores into the decision-making process in distinct ways.

### Model 1: Anxiety-Induced Model-Based Suppression
This model hypothesizes that high anxiety consumes cognitive resources (working memory), thereby suppressing the ability to use complex Model-Based (MB) planning. Instead of a fixed weight `w`, the weight given to the model-based system is dynamically reduced as anxiety increases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Model-Based Suppression.

    Hypothesis: High anxiety consumes working memory resources required for 
    model-based planning. Therefore, higher STAI scores reduce the weight (w) 
    assigned to the model-based system, forcing a reliance on the simpler 
    model-free system.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate for value updates.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w_max: [0, 1] The maximum possible weight for model-based control (at 0 anxiety).
    - suppression_rate: [0, 5] How strongly anxiety reduces w_max.
    """
    learning_rate, beta, w_max, suppression_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective w based on anxiety
    # As STAI increases, w decreases from w_max towards 0.
    w = w_max * np.exp(-suppression_rate * stai_score)
    
    # Ensure w stays within bounds [0, 1]
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based calculation: Bellman equation using transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Value: Weighted sum of MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Softmax choice probability for Stage 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning / Updates ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Transition Learning Impairment
This model posits that anxiety specifically interferes with learning the structure of the environment (the transition matrix). While the standard model assumes a fixed 70/30 transition matrix, real participants might try to learn it. This model assumes participants update their belief about transition probabilities, but anxiety acts as "noise" or a dampener, making high-anxiety individuals slower to update their internal model of the world, leading to inaccurate Model-Based planning.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Transition Learning Impairment.

    Hypothesis: Anxiety impairs the ability to update internal models of 
    environmental structure (state transitions). High anxiety individuals 
    have a lower learning rate for the transition matrix, causing their 
    Model-Based system to rely on outdated or static priors.

    Parameters:
    - learning_rate_reward: [0, 1] Learning rate for reward values (Q-values).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - lr_trans_base: [0, 1] Base learning rate for state transitions.
    - anxiety_damp: [0, 5] How much anxiety reduces the transition learning rate.
    """
    learning_rate_reward, beta, w, lr_trans_base, anxiety_damp = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective transition learning rate
    # High anxiety reduces the ability to learn transitions
    lr_transition = lr_trans_base / (1 + anxiety_damp * stai_score)
    lr_transition = np.clip(lr_transition, 0.0, 1.0)

    # Initialize estimated transition matrix (start with uniform or slight bias)
    # Rows: Action 1 (A or U), Cols: State (X or Y)
    # Let's assume they start with the true prior but update it
    est_transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]]) 

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the DYNAMICALLY learned transition matrix for MB calculation
        q_stage1_mb = est_transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial] # 0 for X, 1 for Y
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Value Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate_reward * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate_reward * delta_stage2

        # --- Structure Learning (Transition Matrix Update) ---
        # Update the row corresponding to the chosen action (0 or 1)
        # Target is 1 for the state we landed in, 0 for the other
        chosen_a1 = action_1[trial]
        actual_state = state[trial]
        
        # Create a one-hot vector for the state outcome
        outcome_vector = np.zeros(2)
        outcome_vector[actual_state] = 1.0
        
        # Update belief: New = Old + lr * (Outcome - Old)
        est_transition_matrix[chosen_a1] += lr_transition * (outcome_vector - est_transition_matrix[chosen_a1])
        
        # Normalize to ensure probabilities sum to 1
        est_transition_matrix[chosen_a1] /= np.sum(est_transition_matrix[chosen_a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Loss Rigidity
This model suggests that anxiety affects how participants respond to *negative* outcomes (lack of reward). Specifically, high anxiety leads to "loss rigidity" or "freezing," where the inverse temperature (`beta`) effectively increases after a loss (0 reward). This makes the participant more deterministic and less exploratory immediately following a failure, preventing them from exploring potentially better options.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Loss Rigidity.

    Hypothesis: Anxiety induces a 'freezing' or rigid response to negative outcomes.
    After receiving no reward (0), the inverse temperature (beta) temporarily 
    increases for the next trial, reducing exploration. The magnitude of this 
    rigidity is scaled by the STAI score.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta_base: [0, 10] Baseline inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - loss_rigidity: [0, 5] How much beta increases after a loss, scaled by STAI.
    """
    learning_rate, beta_base, w, loss_rigidity = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_reward = 1 # Assume success on trial -1 to start with baseline beta

    for trial in range(n_trials):
        
        # Determine current beta
        # If previous reward was 0, beta increases (less exploration)
        # The increase is proportional to anxiety (loss_rigidity * stai)
        if prev_reward == 0:
            current_beta = beta_base + (loss_rigidity * stai_score)
        else:
            current_beta = beta_base
            
        # Cap beta to prevent overflow
        current_beta = min(current_beta, 20.0)

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # --- Stage 2 Policy ---
        state_idx = state[trial]
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store reward for next trial's beta calculation
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```