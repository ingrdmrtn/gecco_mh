Here are 3 new cognitive models based on the provided template and data.

### Model 1: Anxiety-Modulated Negative Learning Bias
This model hypothesizes that anxiety creates a "negativity bias" in learning. High-anxiety participants may update their value estimates more aggressively when expectations are violated negatively (receiving 0 coins) compared to positive outcomes.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Negative Learning Bias.
    
    Hypothesis: Anxiety increases the learning rate specifically for negative prediction errors 
    (when outcomes are worse than expected). This reflects a hyper-sensitivity to failure 
    or lack of reward common in high anxiety.
    
    Parameters:
    - lr_base: [0, 1] Baseline learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - neg_lr_boost: [0, 1] Additional learning rate added to lr_base when delta < 0, scaled by STAI.
    """
    lr_base, beta, w, neg_lr_boost = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Determine LR based on sign of prediction error
        current_lr_1 = lr_base
        if delta_stage1 < 0:
            current_lr_1 += (neg_lr_boost * stai_score)
            if current_lr_1 > 1.0: current_lr_1 = 1.0
            
        q_stage1_mf[action_1[trial]] += current_lr_1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Determine LR based on sign of prediction error
        current_lr_2 = lr_base
        if delta_stage2 < 0:
            current_lr_2 += (neg_lr_boost * stai_score)
            if current_lr_2 > 1.0: current_lr_2 = 1.0

        q_stage2_mf[state_idx, action_2[trial]] += current_lr_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Accelerated Forgetting
This model hypothesizes that anxiety consumes cognitive resources (working memory load), leading to faster decay of learned values. High anxiety participants "forget" the value of options they didn't just choose (or all options) faster than low anxiety participants.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Accelerated Forgetting Model.
    
    Hypothesis: Anxiety acts as a cognitive load that accelerates the decay (forgetting) 
    of Q-values. On every trial, Q-values decay towards 0, with the rate of decay 
    proportional to the STAI score.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - decay_base: [0, 0.5] Base decay rate for everyone.
    - decay_stai_slope: [0, 0.5] Additional decay rate scaled by STAI.
    """
    learning_rate, beta, w, decay_base, decay_stai_slope = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective decay rate
    decay_rate = decay_base + (decay_stai_slope * stai_score)
    if decay_rate > 1.0: decay_rate = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Standard Updating
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Forgetting Step: Decay all Q-values towards 0
        # (Simulating memory loss between trials)
        q_stage1_mf *= (1 - decay_rate)
        q_stage2_mf *= (1 - decay_rate)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Eligibility Trace
This model introduces an eligibility trace parameter ($\lambda$) which controls how much the Stage 2 reward updates the Stage 1 choice directly. The hypothesis is that anxiety affects this credit assignmentâ€”either by making people focus only on the immediate step (low $\lambda$) or by over-linking distal outcomes (high $\lambda$).

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Eligibility Trace (Lambda).
    
    Hypothesis: Anxiety affects the efficiency of credit assignment between Stage 1 choices 
    and Stage 2 rewards. This uses a Q(lambda) approach where the Stage 1 value is updated 
    not just by the Stage 2 value (TD(0)), but also directly by the Stage 2 prediction error.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - lambda_base: [0, 1] Base eligibility trace parameter.
    - lambda_stai_mod: [-1, 1] Modulation of lambda by STAI. 
      (Note: code clamps total lambda between 0 and 1).
    """
    learning_rate, beta, w, lambda_base, lambda_stai_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective lambda
    lambda_val = lambda_base + (lambda_stai_mod * stai_score)
    # Clamp lambda to [0, 1]
    if lambda_val < 0: lambda_val = 0.0
    if lambda_val > 1: lambda_val = 1.0

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Calculate prediction errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 (Standard TD)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 with Eligibility Trace
        # Q1 updates by its own error, PLUS a fraction (lambda) of the subsequent error
        q_stage1_mf[action_1[trial]] += learning_rate * (delta_stage1 + lambda_val * delta_stage2)
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```