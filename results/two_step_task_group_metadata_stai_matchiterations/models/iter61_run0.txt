Here are three new cognitive models that incorporate STAI anxiety scores into the decision-making process in distinct ways.

### Model 1: Anxiety-Induced Model-Based Suppression
This model hypothesizes that high anxiety consumes cognitive resources (working memory), thereby suppressing the ability to use complex Model-Based (MB) planning. Instead of a fixed weight `w`, the weight given to the model-based system is dynamically reduced as anxiety increases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Model-Based Suppression.

    Hypothesis: High anxiety consumes cognitive resources (working memory), 
    making it harder to compute or rely on Model-Based (MB) planning. 
    The weight `w` (mixing parameter) is not fixed but is a maximum potential 
    that is suppressed by the STAI score.
    
    Mechanism: w_effective = w_max * (1 - (suppression_rate * stai))
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_max: [0, 1] The maximum possible reliance on Model-Based planning (at 0 anxiety).
    - suppression_rate: [0, 1] How strongly anxiety reduces MB usage.
    """
    learning_rate, beta, w_max, suppression_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective w based on anxiety
    # We clip at 0 to ensure w doesn't become negative
    w_effective = max(0.0, w_max * (1.0 - (suppression_rate * stai_score)))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-Free values for Stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-Free values for Stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        # Model-Based calculation: V(S') = max_a Q(S', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Value: Mix MB and MF using anxiety-modulated weight
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf

        # Softmax choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] # 0 for Planet X, 1 for Planet Y

        # --- Stage 2 Policy ---
        # Standard MF choice at stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # 1. Update Stage 1 MF (TD(0))
        # Note: In standard Daw task, Stage 1 MF is often updated by Stage 2 Q-value (SARSA-like)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        # 2. Update Stage 2 MF
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Loss Rigidity
This model posits that anxiety affects how participants update their beliefs after a lack of reward (a loss). Specifically, anxious individuals may "freeze" or fail to learn from negative outcomes effectively, or conversely, over-react to them. Here, we model "Loss Rigidity," where high anxiety reduces the learning rate specifically when the reward is 0.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Loss Rigidity.

    Hypothesis: Anxiety interferes with error-driven learning specifically 
    in the context of negative outcomes (omission of reward). High anxiety 
    causes 'rigidity' or a reduced ability to update values when receiving 0 coins.
    
    Mechanism: 
    If reward == 1: lr = learning_rate_base
    If reward == 0: lr = learning_rate_base * (1 - rigidity * stai)
    
    Parameters:
    - learning_rate_base: [0, 1] Base learning rate for positive outcomes.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - rigidity: [0, 1] How much anxiety dampens learning from losses.
    """
    learning_rate_base, beta, w, rigidity = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Determine effective learning rate based on outcome and anxiety
        current_reward = reward[trial]
        
        if current_reward == 1:
            lr_eff = learning_rate_base
        else:
            # Dampen learning rate for losses based on anxiety
            lr_eff = learning_rate_base * (1.0 - (rigidity * stai_score))
            lr_eff = max(0.0, lr_eff) # Ensure non-negative

        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_eff * delta_stage1

        # Update Stage 2
        delta_stage2 = current_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_eff * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Rare Transition Discounting
This model suggests that anxious individuals are skeptical of "rare" events. When a rare transition occurs (e.g., Spaceship A goes to Planet Y), a purely Model-Based agent uses this to update the value of Spaceship A significantly. However, an anxious agent might treat this rare transition as "noise" or an unreliable signal, discounting the Model-Based value update or the Model-Based valuation itself when the transition structure feels uncertain.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Rare Transition Discounting.

    Hypothesis: Anxious individuals distrust the stability of the environment. 
    While they may use a Model-Based strategy, they discount the value of the 
    'rare' transition path in their planning calculations, effectively assuming 
    the rare path is less likely or less valuable than the transition matrix suggests.
    
    Mechanism: The transition matrix used for planning is distorted by anxiety.
    The probability of the rare transition (0.3) is reduced by a factor of anxiety.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - rare_discount: [0, 1] How much anxiety reduces the perceived probability of rare transitions.
    """
    learning_rate, beta, w, rare_discount = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Standard transition matrix: [[0.7, 0.3], [0.3, 0.7]]
    # We distort this based on anxiety.
    # We reduce the 0.3 (rare) component and normalize.
    
    # Calculate perceived rare probability
    # If rare_discount is high and anxiety is high, 0.3 becomes smaller.
    p_rare_perceived = 0.3 * (1.0 - (rare_discount * stai_score))
    p_common_perceived = 1.0 - p_rare_perceived # Normalize so rows sum to 1
    
    # Subjective transition matrix for planning
    subjective_tm = np.array([
        [p_common_perceived, p_rare_perceived], 
        [p_rare_perceived, p_common_perceived]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        
        # Model-Based Step: Use the SUBJECTIVE transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = subjective_tm @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]

        # --- Learning ---
        # Standard updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1

        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```