Here are three new cognitive models that incorporate STAI anxiety scores into the decision-making process of the two-step task.

### Model 1: Anxiety-Induced Model-Based Suppression
This model hypothesizes that high anxiety consumes cognitive resources (working memory), thereby suppressing the ability to use Model-Based (planning) strategies. Instead of a fixed weight `w`, the weight given to the model-based system is dynamically reduced as anxiety increases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Model-Based Suppression.

    Hypothesis: High anxiety consumes working memory resources required for 
    model-based planning. Therefore, the weight (w) assigned to the model-based 
    controller is not fixed but is suppressed by the STAI score.
    
    w_effective = w_max * (1 - suppression_rate * stai)

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate for value updates.
    - beta: [0, 10] Inverse temperature for softmax choice.
    - w_max: [0, 1] The maximum possible model-based weight (at 0 anxiety).
    - suppression_rate: [0, 1] How strongly anxiety reduces model-based control.
    """
    learning_rate, beta, w_max, suppression_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate effective model-based weight based on anxiety
    # We clip at 0 to ensure w doesn't become negative
    w_effective = np.maximum(0, w_max * (1 - suppression_rate * stai_score))

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Q-values
    q_stage1_mf = np.zeros(2)      # Model-free values for stage 1
    q_stage2_mf = np.zeros((2, 2)) # Model-free values for stage 2 (State x Action)

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        # Model-Based Valuation: V(S') = max_a Q(S', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Valuation
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf

        # Softmax Choice 1
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial] # 0 for Planet X, 1 for Planet Y

        # --- Stage 2 Decision ---
        # Standard Model-Free choice at Stage 2
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Stage 1 MF Update (TD(0))
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Loss Rigidity
This model posits that anxiety affects how agents update their beliefs after a lack of reward (a loss). Specifically, anxious individuals may over-react to losses (0 coins) by "freezing" or reducing their learning rate specifically for negative outcomes, making them slower to update values when things go wrong, or conversely, they might learn *more* from failure. Here, we model it as a modulation of the learning rate specifically for unrewarded trials.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Loss Rigidity.

    Hypothesis: Anxiety modulates learning specifically from negative outcomes (0 reward).
    While positive learning remains constant, the learning rate for losses is scaled
    by anxiety. High anxiety might lead to 'loss rigidity' (ignoring losses) or 
    hypersensitivity.
    
    lr_loss = lr_base * (1 + rigidity_param * stai * direction)
    (Implemented as a direct modifier to the learning rate on 0-reward trials).

    Parameters:
    - lr_base: [0, 1] Base learning rate for rewarded trials.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - loss_mod: [0, 5] Multiplier for STAI to modify learning rate on unrewarded trials.
                 If > 0, anxiety increases learning from failure.
    """
    lr_base, beta, w, loss_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        # Determine effective learning rate for this trial
        current_reward = reward[trial]
        
        if current_reward == 1:
            current_lr = lr_base
        else:
            # On loss trials, modify LR based on anxiety
            # We clip to [0, 1] to ensure stability
            mod_factor = 1.0 + (loss_mod * stai_score) 
            # Note: If loss_mod is high, this implies hypersensitivity to failure
            current_lr = np.clip(lr_base * mod_factor, 0.0, 1.0)

        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += current_lr * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = current_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += current_lr * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Induced Transition Uncertainty
This model suggests that anxiety degrades the internal representation of the transition matrix. While the true transitions are 70/30, an anxious agent might perceive the world as more chaotic or uncertain (entropy), effectively "flattening" the transition probabilities used in the Model-Based calculation towards 50/50.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Transition Uncertainty (Entropy).

    Hypothesis: Anxiety degrades the precision of the internal model of the environment.
    Instead of using the perfect [0.7, 0.3] transition matrix, anxious participants
    perceive the transitions as flatter (closer to 0.5/0.5), reducing the effectiveness
    of model-based planning without changing the weight 'w'.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - entropy_scale: [0, 1] How much anxiety flattens the transition matrix.
                      0 = use perfect matrix, 1 = use flat [0.5, 0.5] matrix.
    """
    learning_rate, beta, w, entropy_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Define the true transition matrix
    true_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    
    # Define a maximum entropy matrix (random chance)
    flat_matrix = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    # Calculate the subjective matrix based on anxiety
    # High anxiety mixes in more of the flat matrix
    mix_param = entropy_scale * stai_score
    # Clip mix_param to [0,1] to prevent matrix inversion logic errors
    mix_param = np.clip(mix_param, 0.0, 1.0)
    
    subjective_matrix = (1 - mix_param) * true_matrix + mix_param * flat_matrix

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Decision ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the SUBJECTIVE matrix for planning
        q_stage1_mb = subjective_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        state_idx = state[trial]

        # --- Stage 2 Decision ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Learning ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```