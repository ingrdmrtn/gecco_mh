Here are 3 new cognitive models based on the two-step task and anxiety (STAI) modulation.

### Model 1: Anxiety-Modulated Stage-Specific Learning
This model hypothesizes that anxiety differentially affects learning at different levels of abstraction. Stage 1 (Spaceships) is abstract and probabilistic, while Stage 2 (Aliens) is concrete and immediately rewarding. High anxiety may cause hyper-focus on the immediate reward predictors (Aliens) at the expense of, or distinct from, the abstract transition predictors.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Stage-Specific Learning.

    Hypothesis: Anxiety alters the balance of learning speeds between the two stages.
    Anxious participants may learn about immediate reward sources (Stage 2 Aliens) 
    at a different rate than abstract transition predictors (Stage 1 Spaceships).
    
    Parameters:
    - lr_stage1: [0, 1] Base learning rate for Stage 1 (Spaceships).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - lr_s2_mod: [0, 5] Multiplier for Stage 2 learning rate based on STAI.
      lr_stage2 = lr_stage1 * (1 + lr_s2_mod * STAI).
    """
    lr_stage1, beta, w, lr_s2_mod = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate Stage 2 learning rate based on anxiety
    # If lr_s2_mod is positive, anxiety accelerates learning about aliens relative to spaceships.
    lr_stage2 = lr_stage1 * (1.0 + lr_s2_mod * stai_score)
    # Clip to ensure valid learning rate
    lr_stage2 = np.clip(lr_stage2, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # 2 states (Planets), 2 actions (Aliens)

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 Update (using lr_stage1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += lr_stage1 * delta_stage1
        
        # Stage 2 Update (using anxiety-modulated lr_stage2)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += lr_stage2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Modulated Win-Clinging
This model proposes that anxiety drives a specific type of perseveration: "Win-Clinging." Unlike general stickiness (repeating any choice), this mechanism posits that anxious individuals are specifically afraid to switch away from a choice that just provided safety (reward). They cling to "winners" to avoid the uncertainty of exploration.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Win-Clinging.

    Hypothesis: Anxiety increases 'stickiness' specifically after a positive outcome (Win).
    Anxious participants are hyper-conservative and refuse to switch strategies 
    after receiving a reward, effectively adding a bonus to the previous winner.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - win_stickiness: [0, 5] Magnitude of the bonus added to the previously chosen 
      action ONLY if it resulted in a reward, scaled by STAI.
    """
    learning_rate, beta, w, win_stickiness = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_action_1 = -1
    last_reward = 0

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Apply stickiness bonus ONLY if the previous trial was a "Win" (reward == 1)
        if last_action_1 != -1 and last_reward == 1.0:
            q_net[last_action_1] += (win_stickiness * stai_score)

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        last_action_1 = action_1[trial]
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Fictive Regret
This model incorporates "fictive learning" (counterfactual updating). It hypothesizes that when anxious participants receive no coins (loss), they ruminate on the unchosen option, assuming it might have been the better choice. They update the value of the *unchosen* alien as if it had been rewarding, driven by regret/worry.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Fictive Regret.

    Hypothesis: When receiving a loss (0 coins), anxious participants engage in 
    counterfactual thinking ("The other alien probably had the coin"). They update 
    the value of the UNCHOSEN option in Stage 2 positively, proportional to anxiety.
    
    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - fictive_rate: [0, 1] The rate at which the unchosen option is updated 
      towards 1.0 (assumed reward) upon a loss, scaled by STAI.
    """
    learning_rate, beta, w, fictive_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Standard Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Fictive (Regret) Update
        # If reward was 0 (Loss), update the unchosen alien as if it might have been a Win.
        if reward[trial] == 0.0:
            unchosen_action = 1 - action_2[trial]
            # Calculate effective fictive learning rate based on STAI
            alpha_fictive = learning_rate * fictive_rate * stai_score
            # Update unchosen towards 1.0
            q_stage2_mf[state_idx, unchosen_action] += alpha_fictive * (1.0 - q_stage2_mf[state_idx, unchosen_action])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```