Here are three new cognitive models that incorporate STAI anxiety scores into the decision-making process in distinct ways.

### Model 1: Anxiety-Induced Model-Based Suppression
This model hypothesizes that high anxiety consumes cognitive resources (working memory), thereby suppressing the ability to use complex Model-Based (planning) strategies. Instead of a fixed weight `w`, the weight given to the Model-Based system is dynamically reduced as anxiety increases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Model-Based Suppression.

    Hypothesis: High anxiety consumes cognitive resources (working memory), 
    making it harder to maintain and use the transition map (Model-Based control).
    Therefore, the weight `w` (mixing parameter) is not fixed but is a function 
    of a maximum capacity `w_max` reduced by the STAI score.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate for value updates.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w_max: [0, 1] The maximum possible model-based weight a participant could have at 0 anxiety.
    - suppression_rate: [0, 1] How strongly STAI reduces w_max. 
      Effective w = w_max * (1 - suppression_rate * stai).
    """
    learning_rate, beta, w_max, suppression_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the effective weight w based on anxiety
    # If suppression_rate is high and stai is high, w approaches 0 (pure Model-Free).
    w = w_max * (1.0 - (suppression_rate * stai_score))
    # Ensure w stays within bounds [0, 1]
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        # Model-Based Value: V(S') = max(Q(S', a'))
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        # Q_MB(S, a) = T(S, a, S') * V(S')
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Value: w * MB + (1-w) * MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        # Stage 1 MF Update (TD(1) style or simple SARSA-like for stage 1)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Stage 2 MF Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Negative Bias (Asymmetric Learning)
This model posits that anxious individuals are more sensitive to negative outcomes (lack of reward) than positive ones. While they learn from rewards normally, they "over-learn" from punishments (zeros), causing them to abandon choices faster than non-anxious participants.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Negative Bias (Asymmetric Learning).

    Hypothesis: Anxiety creates a hypersensitivity to negative outcomes (0 reward).
    The learning rate for negative prediction errors is boosted by the STAI score,
    while the learning rate for positive errors remains baseline.

    Parameters:
    - learning_rate: [0, 1] Base learning rate for positive prediction errors.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - neg_bias: [0, 5] Multiplier for the learning rate when the prediction error is negative.
      Effective LR_neg = learning_rate * (1 + neg_bias * stai).
    """
    learning_rate, beta, w, neg_bias = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Determine LR based on sign of error
        if delta_stage1 < 0:
            lr_eff_1 = learning_rate * (1.0 + neg_bias * stai_score)
        else:
            lr_eff_1 = learning_rate
        lr_eff_1 = np.clip(lr_eff_1, 0.0, 1.0)
        q_stage1_mf[action_1[trial]] += lr_eff_1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        if delta_stage2 < 0:
            lr_eff_2 = learning_rate * (1.0 + neg_bias * stai_score)
        else:
            lr_eff_2 = learning_rate
        lr_eff_2 = np.clip(lr_eff_2, 0.0, 1.0)
        q_stage2_mf[state_idx, action_2[trial]] += lr_eff_2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Modulated Uncertainty Aversion
This model suggests that anxiety drives an aversion to uncertainty. In the context of the two-step task, "uncertainty" is interpreted as the randomness of the state transitions. Anxious participants may discount the value of the Model-Based system because the transitions (70/30) feel unreliable or risky to them, effectively "flattening" the transition matrix in their internal model.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Uncertainty Aversion (Transition Distortion).

    Hypothesis: Anxious individuals are intolerant of uncertainty. They may distrust 
    the probabilistic transition structure (70/30). This model proposes that anxiety 
    causes the participant's internal model of the transition matrix to become 
    more uniform (closer to 50/50), reducing the effectiveness of planning.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - uncertainty_scale: [0, 1] How much anxiety flattens the transition matrix.
      If 0, matrix is [0.7, 0.3]. If 1 (and high anxiety), matrix approaches [0.5, 0.5].
    """
    learning_rate, beta, w, uncertainty_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Define the objective transition matrix
    true_prob = 0.7
    
    # Distort the matrix based on anxiety
    # distortion moves the probability from 0.7 towards 0.5
    # distortion magnitude = uncertainty_scale * stai
    distortion = uncertainty_scale * stai_score
    # Ensure we don't invert the matrix (cap distortion so prob doesn't go below 0.5)
    distortion = np.clip(distortion, 0.0, 0.2) 
    
    perceived_prob = true_prob - distortion
    complement_prob = 1.0 - perceived_prob
    
    # The internal model used for planning
    internal_transition_matrix = np.array([
        [perceived_prob, complement_prob], 
        [complement_prob, perceived_prob]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the DISTORTED matrix for planning
        q_stage1_mb = internal_transition_matrix @ max_q_stage2

        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf

        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```