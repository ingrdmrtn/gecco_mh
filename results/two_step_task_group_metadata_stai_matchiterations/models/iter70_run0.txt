Here are three new cognitive models that incorporate STAI anxiety scores into the decision-making process in distinct ways.

### Model 1: Anxiety-Modulated Model-Based Suppression
This model hypothesizes that high anxiety consumes cognitive resources (working memory), thereby suppressing the ability to use Model-Based (planning) strategies. Instead of a fixed weight `w`, the weight given to the model-based system is dynamically reduced as anxiety increases.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Model-Based Suppression.
    
    Hypothesis: High anxiety consumes working memory resources, reducing the 
    capacity for model-based planning. The weight `w` (mixing parameter) is 
    defined as a maximum potential `w_max` which is suppressed by the STAI score.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature (exploration/exploitation).
    - w_max: [0, 1] The maximum possible model-based weight (at 0 anxiety).
    - suppression_rate: [0, 1] How strongly anxiety reduces w_max.
    """
    learning_rate, beta, w_max, suppression_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the effective weight w based on anxiety
    # If suppression_rate is high and stai is high, w approaches 0 (pure Model-Free)
    w = w_max * (1.0 - (suppression_rate * stai_score))
    # Ensure w stays within bounds [0, 1]
    w = np.clip(w, 0.0, 1.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Mix MB and MF values using the anxiety-adjusted w
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Negative Bias (Asymmetric Learning)
This model posits that anxious individuals are more sensitive to negative outcomes (omission of reward) than positive ones. It implements separate learning rates for positive and negative prediction errors, where the learning rate for negative errors is amplified by the STAI score.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Negative Bias (Asymmetric Learning).
    
    Hypothesis: Anxiety increases sensitivity to negative outcomes (loss/no reward).
    The model uses a base learning rate for positive prediction errors, but 
    scales the learning rate for negative prediction errors based on STAI.
    
    Parameters:
    - lr_pos: [0, 1] Learning rate for positive prediction errors (reward > expectation).
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - neg_bias: [0, 5] Multiplier for STAI to boost learning from negative errors.
    """
    lr_pos, beta, w, neg_bias = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate learning rate for negative prediction errors
    # High anxiety results in faster updating (stronger reaction) to disappointments
    lr_neg_scale = 1.0 + (neg_bias * stai_score)
    
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Update Stage 1
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        current_lr_s1 = lr_pos if delta_stage1 >= 0 else np.clip(lr_pos * lr_neg_scale, 0, 1)
        q_stage1_mf[action_1[trial]] += current_lr_s1 * delta_stage1
        
        # Update Stage 2
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        current_lr_s2 = lr_pos if delta_stage2 >= 0 else np.clip(lr_pos * lr_neg_scale, 0, 1)
        q_stage2_mf[state_idx, action_2[trial]] += current_lr_s2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Uncertainty Avoidance
This model suggests that anxiety drives a preference for certainty. It tracks the uncertainty (variance) of the value estimates for the second-stage states. Anxious participants will penalize options where the outcome is highly uncertain, effectively subtracting a "risk premium" from the Q-values.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Uncertainty Avoidance.
    
    Hypothesis: Anxiety correlates with intolerance of uncertainty. This model 
    tracks the variance (uncertainty) of rewards at the second stage. 
    High anxiety participants subtract a penalty proportional to this uncertainty 
    from their value estimates.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - uncertainty_scale: [0, 5] How strongly STAI penalizes variance.
    """
    learning_rate, beta, w, uncertainty_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Mean estimate
    
    # Initialize variance estimate (uncertainty)
    # We start with some uncertainty (e.g., 0.25, max variance of a Bernoulli)
    v_stage2_mf = np.ones((2, 2)) * 0.25 

    for trial in range(n_trials):

        # Calculate penalty based on uncertainty and anxiety
        penalty = uncertainty_scale * stai_score * v_stage2_mf
        
        # Effective Q-values are Mean - Penalty
        q_stage2_effective = q_stage2_mf - penalty

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_effective, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        # Use effective Q (penalized) for choice
        exp_q2 = np.exp(beta * q_stage2_effective[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Standard TD updates for means
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update variance estimate: V_new = V_old + lr * ((R - Q)^2 - V_old)
        # This approximates the running variance of the prediction error
        prediction_error_sq = delta_stage2 ** 2
        v_stage2_mf[state_idx, action_2[trial]] += learning_rate * (prediction_error_sq - v_stage2_mf[state_idx, action_2[trial]])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```