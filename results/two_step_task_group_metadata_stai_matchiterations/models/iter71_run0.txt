def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Direct Reward Projection (Model-Free Bypass).

    Hypothesis: Anxious individuals may struggle to maintain the two-step causal chain 
    (Spaceship -> Planet -> Alien -> Reward). Instead, they may form a direct, 
    heuristic association between the Stage 1 choice (Spaceship) and the final Reward, 
    bypassing the state transition structure. This represents a "hyper-model-free" 
    mechanism where anxiety collapses the temporal structure of the task.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - bypass_scale: [0, 1] The strength of the direct association update, scaled by STAI.
      If high, the agent updates Stage 1 values directly from the final reward.
    """
    learning_rate, beta, w, bypass_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the anxiety-dependent bypass weight
    bypass_weight = bypass_scale * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Net Q-value mixes MB and MF
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Standard TD Error (Stage 1 -> Stage 2 value)
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        
        # Direct Bypass Error (Stage 1 -> Reward)
        # This error ignores the intermediate state and updates based on the final outcome
        delta_bypass = reward[trial] - q_stage1_mf[action_1[trial]]
        
        # Update Stage 1: Standard TD update + Anxiety-driven direct update
        q_stage1_mf[action_1[trial]] += (learning_rate * delta_stage1) + (bypass_weight * delta_bypass)
        
        # Standard Stage 2 update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Amplified Regret Learning.

    Hypothesis: Anxious individuals exhibit a "negativity bias" or heightened sensitivity 
    to missed rewards (regret). When they receive 0 coins, they perceive this as a 
    stronger error signal than non-anxious participants, effectively increasing the 
    learning rate specifically for unrewarded trials. This leads to faster abandonment 
    of non-rewarding options.

    Parameters:
    - learning_rate: [0, 1] Base learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - regret_scale: [0, 5] Multiplier for the learning rate on 0-reward trials, scaled by STAI.
    """
    learning_rate, beta, w, regret_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Determine effective learning rate for Stage 2
        # If reward is 0 (failure), anxiety boosts the learning rate (regret/avoidance)
        if reward[trial] == 0:
            effective_lr = learning_rate * (1.0 + (regret_scale * stai_score))
            # Cap LR at 1.0 to prevent instability
            effective_lr = min(effective_lr, 1.0)
        else:
            effective_lr = learning_rate
            
        q_stage2_mf[state_idx, action_2[trial]] += effective_lr * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Stage-Specific Anxiety Chaos (Precision Collapse).

    Hypothesis: Anxiety affects decision stages differently. While Stage 1 (Spaceship) 
    allows for deliberation, Stage 2 (Alien choice) is more immediate and reactive. 
    High anxiety causes a "panic" or loss of precision specifically at Stage 2, 
    modeled as a reduction in the inverse temperature (beta) for the second choice only.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta_base: [0, 10] Base inverse temperature (used for Stage 1).
    - w: [0, 1] Weight for model-based control.
    - s2_chaos: [0, 1] Factor reducing beta at Stage 2 based on STAI. 
      (1 = complete randomness at high anxiety, 0 = no effect).
    """
    learning_rate, beta_base, w, s2_chaos = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate Stage 2 specific beta
    # High anxiety reduces beta_stage2, leading to more random/chaotic choices at the alien stage
    beta_stage2 = beta_base * (1.0 - (s2_chaos * stai_score))
    # Ensure beta doesn't go negative
    beta_stage2 = max(beta_stage2, 0.0)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice (Uses beta_base)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_base * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice (Uses anxiety-modulated beta_stage2)
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss