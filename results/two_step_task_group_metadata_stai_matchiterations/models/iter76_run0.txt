Here are three cognitive models implemented as Python functions.

### Model 1: Anxiety-Driven RPE-Scaled Learning
This model hypothesizes that anxiety amplifies the reaction to large surprises (Prediction Errors). Instead of a fixed learning rate, the effective learning rate scales with the magnitude of the Reward Prediction Error (RPE). Anxious participants are "shocked" into learning more from outcomes that deviate significantly from expectations.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven RPE-Scaled Learning Model.

    Hypothesis: Anxiety amplifies learning when prediction errors are large.
    The learning rate is not constant; it is boosted by the magnitude of the 
    Reward Prediction Error (RPE), scaled by the participant's anxiety (STAI).
    High anxiety leads to hyper-plasticity in the face of surprise.

    Parameters:
    - base_lr: [0, 1] Baseline learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Model-based weight.
    - shock_sensitivity: [0, 5] How much anxiety scales the LR based on RPE magnitude.
    """
    base_lr, beta, w, shock_sensitivity = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates with RPE Scaling ---
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        # Calculate dynamic LR based on absolute RPE and Anxiety
        # If RPE is high and Anxiety is high, learning rate increases.
        lr_s1 = base_lr * (1 + shock_sensitivity * stai_score * np.abs(delta_stage1))
        # Clip LR to keep it reasonable (though math works without clipping, good for stability)
        lr_s1 = np.clip(lr_s1, 0, 1)
        
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        # Dynamic LR for stage 2 as well
        lr_s2 = base_lr * (1 + shock_sensitivity * stai_score * np.abs(delta_stage2))
        lr_s2 = np.clip(lr_s2, 0, 1)
        
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Post-Loss Frantic Search
This model hypothesizes that anxiety triggers a "frantic search" mechanism after a negative outcome. If a participant receives no coins (loss), their exploration parameter (`beta`) decreases for the *next* trial, making their behavior more random/exploratory. High anxiety individuals become less consistent after failure.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Post-Loss Frantic Search Model.

    Hypothesis: Anxiety causes a collapse in decision consistency (low beta) 
    specifically following a loss (0 reward). This represents a "panic" or 
    "frantic search" response where the participant tries to explore randomly 
    to find a solution, rather than exploiting known values.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta_base: [0, 10] Baseline inverse temperature (consistency).
    - w: [0, 1] Model-based weight.
    - frantic_scale: [0, 5] How much beta is reduced after a loss, scaled by STAI.
    """
    learning_rate, beta_base, w, frantic_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    last_reward = 1.0 # Initialize assuming no panic at start

    for trial in range(n_trials):
        
        # Determine current beta based on PREVIOUS outcome
        if last_reward == 0.0:
            # Panic response: Beta is reduced (more random)
            current_beta = beta_base / (1 + frantic_scale * stai_score)
        else:
            # Calm response: Baseline beta
            current_beta = beta_base

        # --- Stage 1 Policy ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(current_beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy ---
        # We assume the frantic state persists through the whole trial
        exp_q2 = np.exp(current_beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Store reward for next trial's beta calculation
        last_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Proximal Rigidity
This model differentiates between the two stages of the task. It hypothesizes that anxiety causes "choking" or rigidity specifically at the second stage (proximal choice, confronting the alien), while the first stage (distal planning) remains relatively unaffected.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Proximal Rigidity Model.

    Hypothesis: Anxiety affects the two stages of decision-making differently.
    While Stage 1 (planning) uses a baseline exploration rate, Stage 2 (proximal 
    confrontation) triggers a rigidity response in anxious individuals. 
    This results in a significantly higher beta (lower exploration) for the 
    second choice compared to the first.

    Parameters:
    - learning_rate: [0, 1] Standard learning rate.
    - beta_base: [0, 10] Baseline inverse temperature (used for Stage 1).
    - w: [0, 1] Model-based weight.
    - rigidity_boost: [0, 5] Additional beta added to Stage 2, scaled by STAI.
    """
    learning_rate, beta_base, w, rigidity_boost = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Calculate Stage 2 specific beta
    # High anxiety -> High rigidity (high beta) at Stage 2
    beta_stage2 = beta_base * (1 + rigidity_boost * stai_score)

    for trial in range(n_trials):

        # --- Stage 1 Policy (Uses beta_base) ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta_base * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Policy (Uses beta_stage2) ---
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```