Here are 3 new cognitive models based on the provided template and constraints.

### Model 1: Anxiety-Induced Baseline Shift
This model hypothesizes that anxiety alters the subjective valuation of outcomes. Rather than treating "0 coins" as a neutral event, anxious participants may perceive it as a loss (a negative value), effectively shifting the baseline of the reward function downwards.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Baseline Shift Model.
    
    Hypothesis: Anxiety causes participants to perceive neutral outcomes (0 coins) 
    as losses. The subjective reward is shifted downwards by STAI score.
    This affects the Q-values, making them potentially negative, which 
    changes choice probabilities via the softmax function.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - shift_param: [0, 1] Magnitude of the subjective reward subtraction.
      Effective Reward = Reward - (shift_param * STAI).
    """
    learning_rate, beta, w, shift_param = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the subjective penalty based on anxiety
    subjective_shift = shift_param * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # Apply the anxiety-driven baseline shift to the reward
        effective_reward = reward[trial] - subjective_shift
        
        delta_stage2 = effective_reward - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Stage 1 Fatalism (Beta Decoupling)
This model suggests that anxiety specifically degrades the sense of agency over the probabilistic transition (Stage 1), leading to "fatalistic" or more random choices at the spaceship level, while preserving directed behavior at the deterministic alien level (Stage 2).

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Stage 1 Fatalism Model.
    
    Hypothesis: Anxiety reduces the inverse temperature (beta) specifically for 
    the first stage choice (Spaceship), reflecting a lack of perceived control 
    over the probabilistic transition. Stage 2 (Alien) choices remain 
    directed by the base beta.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Base inverse temperature (used for Stage 2).
    - w: [0, 1] Weight for model-based control.
    - fatalism_scale: [0, 5] Scaling factor for anxiety-driven randomness in Stage 1.
      Beta_Stage1 = Beta / (1 + fatalism_scale * STAI).
    """
    learning_rate, beta, w, fatalism_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Decouple betas: Stage 1 beta is suppressed by anxiety
    beta_stage1 = beta / (1.0 + fatalism_scale * stai_score)
    beta_stage2 = beta

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use the suppressed beta for Stage 1
        exp_q1 = np.exp(beta_stage1 * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        # Use the standard beta for Stage 2
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Rare-Transition Panic (Dynamic MB Collapse)
This model posits that anxiety makes the Model-Based system fragile. When a "rare" transition occurs (e.g., Spaceship A goes to Planet Y), it triggers a temporary "panic" state that suppresses Model-Based planning (reduces `w`) for the *subsequent* trial.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Rare-Transition Panic Model.
    
    Hypothesis: Unexpected (rare) transitions cause a temporary 'panic' in 
    anxious participants, causing them to abandon Model-Based planning 
    (reducing w) on the immediately following trial.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Baseline weight for model-based control.
    - panic_scale: [0, 1] Proportion of MB-control lost after a rare transition.
      If previous was rare: w_effective = w * (1 - panic_scale * STAI).
    """
    learning_rate, beta, w, panic_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    # Track if the previous trial was a rare transition
    last_was_rare = False

    for trial in range(n_trials):

        # Determine effective w based on previous trial outcome and anxiety
        if last_was_rare:
            w_effective = w * (1.0 - panic_scale * stai_score)
        else:
            w_effective = w

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_effective * q_stage1_mb + (1 - w_effective) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Determine if current trial is rare for the next iteration
        # Common: (A=0 -> X=0) or (U=1 -> Y=1). Rare: (0->1) or (1->0).
        if action_1[trial] == state[trial]:
            last_was_rare = False
        else:
            last_was_rare = True

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```