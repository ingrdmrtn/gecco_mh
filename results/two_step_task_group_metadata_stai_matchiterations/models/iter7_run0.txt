Here are three new cognitive models that incorporate the STAI anxiety score into the decision-making process using mechanisms distinct from those previously explored.

### Model 1: Anxiety-Modulated Eligibility Trace
This model hypothesizes that anxiety affects how deeply the participant credits the initial choice (Spaceship) for the final outcome (Coin), bypassing the intermediate state (Planet). In Reinforcement Learning, this is often handled by an eligibility trace ($\lambda$). High anxiety might lead to a higher $\lambda$, where the participant "blames" or "credits" the spaceship directly for the coin, ignoring the transition structure (a more reactive, less structured update).

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Modulated Eligibility Trace Model.
    
    Hypothesis: Anxiety modulates the eligibility trace (lambda). 
    High anxiety increases the direct association between the Stage 1 choice 
    and the final Reward, effectively bypassing the Stage 2 value estimation 
    in the Model-Free update. This represents a 'panic' or 'reactive' update 
    where the structure of the task (the two steps) is ignored in favor of 
    immediate outcome association.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - lambda_bias: [0, 1] Scales the eligibility trace based on STAI. 
                   Effective lambda = lambda_bias * STAI.
    """
    learning_rate, beta, w, lambda_bias = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate the eligibility trace factor based on anxiety
    eligibility = lambda_bias * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Calculate Prediction Errors
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        
        # Update Stage 2 (Standard Q-learning)
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # Update Stage 1 (Q-learning with Anxiety-Modulated Eligibility Trace)
        # If eligibility is high, the Stage 2 error (delta_stage2) propagates back to Stage 1
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1 + learning_rate * eligibility * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Induced Memory Decay
This model draws on the "Attentional Control Theory" of anxiety, which suggests anxiety consumes working memory resources. Here, high anxiety causes learned Q-values to decay (forget) faster towards a neutral baseline. This prevents the accumulation of robust value estimates over long periods, making behavior more erratic or reliant on very recent outcomes.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Memory Decay Model.
    
    Hypothesis: Anxiety impairs the maintenance of value representations over time.
    High STAI scores lead to a higher decay rate of Q-values toward zero (forgetting)
    on every trial. This forces anxious participants to rely only on very recent 
    outcomes as older knowledge 'fades'.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - decay_k: [0, 1] The magnitude of memory decay scaled by STAI.
    """
    learning_rate, beta, w, decay_k = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Decay rate is proportional to anxiety
    decay_rate = decay_k * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):
        
        # Apply Anxiety-Driven Decay to all Q-values before making a choice
        # Values decay toward 0.0 (neutral/ignoring previous history)
        q_stage1_mf *= (1.0 - decay_rate)
        q_stage2_mf *= (1.0 - decay_rate)

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Distorted Transition Belief
Standard Model-Based RL assumes participants know the transition matrix is `[[0.7, 0.3], [0.3, 0.7]]`. This model hypothesizes that anxiety leads to "catastrophizing" or uncertainty about the world structure, causing participants to perceive the transitions as less reliable (more random/entropic) than they actually are. High anxiety flattens the internal transition matrix toward 50/50, degrading the quality of Model-Based planning even if the weight `w` is high.

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Distorted Transition Belief Model.
    
    Hypothesis: Anxiety distorts the internal model of the environment's structure.
    While the real world has 70/30 transitions, anxious participants perceive the 
    world as more volatile or uncertain. High STAI flattens the internal transition 
    matrix toward 50/50, reducing the effectiveness of Model-Based planning.

    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - distortion: [0, 1] How much anxiety flattens the transition matrix.
                  0 = Accurate belief (0.7), 1 = Max distortion (closer to 0.5).
    """
    learning_rate, beta, w, distortion = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate subjective transition probability
    # Real prob is 0.7. Max distortion pushes it down to 0.5 (random).
    # Formula: 0.7 - (distortion * stai * 0.2)
    # If distortion=1 and stai=1, prob = 0.5. If distortion=0, prob=0.7.
    subjective_prob = 0.7 - (distortion * stai_score * 0.2)
    
    # Construct the distorted internal model
    subjective_transitions = np.array([
        [subjective_prob, 1 - subjective_prob], 
        [1 - subjective_prob, subjective_prob]
    ])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        
        # Use the ANXIETY-DISTORTED matrix for planning
        q_stage1_mb = subjective_transitions @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```