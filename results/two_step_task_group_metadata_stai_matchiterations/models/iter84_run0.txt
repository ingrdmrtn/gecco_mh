Here are 3 new cognitive models based on the two-step task and anxiety literature.

### Model 1: Failure-Induced Model-Based Suppression
This model hypothesizes that anxiety creates a fragility in high-level cognitive control. While the agent normally mixes Model-Based (MB) and Model-Free (MF) strategies, a negative outcome (failure to get a coin) triggers a "panic" response that temporarily suppresses the cognitive (MB) system for the next trial, forcing a reliance on the habitual (MF) system.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Failure-Induced Model-Based Suppression.
    
    Hypothesis: Anxiety makes model-based control fragile. After a trial with 
    no reward (failure), the weight of the model-based system (w) is suppressed 
    proportional to anxiety, causing a temporary regression to model-free habits.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w_base: [0, 1] Baseline weight for model-based control.
    - suppression_param: [0, 1] How much failure suppresses 'w' for anxious agents.
    """
    learning_rate, beta, w_base, suppression_param = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    prev_reward = 1.0 # Assume success before start to not trigger suppression immediately

    for trial in range(n_trials):
        # Calculate dynamic w based on previous outcome
        # If prev_reward was 1, suppression is 0. If 0, suppression is active.
        current_suppression = suppression_param * stai_score * (1.0 - prev_reward)
        w_trial = w_base * (1.0 - current_suppression)
        
        # Ensure w stays in bounds [0, 1]
        if w_trial < 0: w_trial = 0

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w_trial * q_stage1_mb + (1 - w_trial) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        prev_reward = reward[trial]

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Defensive Denial (Negative Learning Suppression)
This model draws on the concept of avoidance coping in anxiety. Rather than being hypersensitive to punishment (a common hypothesis), this model suggests anxious agents engage in "defensive denial." When a prediction error is negative (outcome was worse than expected), they reduce their learning rate to avoid internalizing the failure, leading to slower adaptation to deteriorating conditions.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Defensive Denial (Negative Learning Suppression).
    
    Hypothesis: Anxious individuals engage in avoidance coping. When faced with 
    negative prediction errors (outcomes worse than expected), they suppress 
    learning to avoid internalizing the 'bad news'.
    
    Parameters:
    - learning_rate: [0, 1] Base learning rate for positive updates.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - denial_param: [0, 1] Proportion of learning suppression on negative errors scaled by STAI.
    """
    learning_rate, beta, w, denial_param = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for the first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates with asymmetric learning rates
        
        # Stage 1 Update
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        lr_s1 = learning_rate
        if delta_stage1 < 0:
            lr_s1 = learning_rate * (1.0 - denial_param * stai_score)
        q_stage1_mf[action_1[trial]] += lr_s1 * delta_stage1
        
        # Stage 2 Update
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        lr_s2 = learning_rate
        if delta_stage2 < 0:
            lr_s2 = learning_rate * (1.0 - denial_param * stai_score)
        q_stage2_mf[state_idx, action_2[trial]] += lr_s2 * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Ambiguity Intolerance (Value Discounting)
This model operationalizes "Intolerance of Uncertainty." In the Model-Based planning step, the agent evaluates the value of future states (Planets X and Y). If the two aliens on a planet have very similar Q-values, the outcome is ambiguous/uncertain. Anxious agents penalize these ambiguous states, effectively discounting the value of a planet if they don't have a clear preference for an alien there.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Ambiguity Intolerance (Value Discounting).
    
    Hypothesis: Anxious agents dislike states where the best action is unclear 
    (high decision conflict/ambiguity). During planning, they discount the 
    value of Stage 2 states where the Q-values of the two aliens are similar.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - intolerance: [0, 5] Penalty magnitude for ambiguous states, scaled by STAI.
    """
    learning_rate, beta, w, intolerance = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Calculate Ambiguity Penalty for MB planning
        # Ambiguity is defined as the inverse of the absolute difference between Q-values
        # If Q-values are close, difference is small, penalty is high.
        # We use exp(-diff) to bound it smoothly.
        
        diff_planet_X = np.abs(q_stage2_mf[0, 0] - q_stage2_mf[0, 1])
        diff_planet_Y = np.abs(q_stage2_mf[1, 0] - q_stage2_mf[1, 1])
        
        penalty_X = intolerance * stai_score * np.exp(-2.0 * diff_planet_X)
        penalty_Y = intolerance * stai_score * np.exp(-2.0 * diff_planet_Y)
        
        # Effective value of states includes the max Q minus the ambiguity penalty
        val_planet_X = np.max(q_stage2_mf[0]) - penalty_X
        val_planet_Y = np.max(q_stage2_mf[1]) - penalty_Y
        
        effective_state_vals = np.array([val_planet_X, val_planet_Y])

        # Policy for the first choice
        q_stage1_mb = transition_matrix @ effective_state_vals
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # Policy for the second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```