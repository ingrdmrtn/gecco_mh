Here are three cognitive models that incorporate the STAI anxiety score into the decision-making process using mechanisms distinct from those listed in the feedback.

### Model 1: Bayesian Transition Rigidity
This model hypothesizes that anxiety increases the strength of the "prior" belief about the world's structure, making the agent more rigid and less responsive to observed transition statistics. It uses a Bayesian update (Dirichlet distribution) for the transition matrix, where STAI scales the fictitious prior counts.

```python
import numpy as np

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Bayesian Transition Rigidity Model.
    
    Hypothesis: Anxiety acts as a strong 'prior' on the transition structure. 
    High anxiety individuals hold onto a fixed belief (prior) about how spaceships 
    move, making them slower to update their internal model based on observed 
    transitions (Bayesian rigidity).
    
    Parameters:
    - learning_rate: [0, 1] Learning rate for Model-Free Q-values.
    - beta: [0, 10] Inverse temperature (softmax sensitivity).
    - w: [0, 1] Weight for Model-Based control (0=MF, 1=MB).
    - prior_strength: [0, 100] The magnitude of the Dirichlet prior count, scaled by STAI.
    """
    learning_rate, beta, w, prior_strength = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Initialize transition counts (Action -> State)
    # Rows: Action A, Action U. Cols: Planet X, Planet Y.
    # We start with 0 observed counts.
    trans_counts = np.zeros((2, 2))
    
    # The prior is uniform (0.5/0.5) but its *weight* depends on anxiety.
    # High anxiety = high prior count = hard to move from 0.5/0.5 (or hard to learn).
    # Alternatively, we can assume the prior is the "common" structure (0.7/0.3).
    # Let's assume a uniform prior representing uncertainty/entropy that resists learning.
    prior_count = prior_strength * stai_score

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):
        
        # 1. Calculate Dynamic Transition Matrix (Bayesian Mean)
        # T_est = (Counts + Prior) / (Total_Counts + 2*Prior)
        row_sums = trans_counts.sum(axis=1, keepdims=True)
        # Avoid division by zero if prior is 0 and counts are 0 (add epsilon)
        denom = row_sums + 2 * prior_count + 1e-10
        transition_matrix = (trans_counts + prior_count) / denom

        # 2. Model-Based Step
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # 3. Net Value & Choice 1
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]

        # 4. Choice 2
        state_idx = state[trial]
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # 5. Updates
        # Update Transition Counts
        trans_counts[action_1[trial], state_idx] += 1

        # Update Q-values
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Failure-Induced Value Erosion
This model proposes that anxiety leads to a specific "loss of confidence" mechanism. When a reward is *not* received (Reward = 0), the agent doesn't just update via prediction error; the value of that action actively "erodes" or shrinks, representing a pessimistic collapse of value estimation driven by anxiety.

```python
import numpy as np

def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Failure-Induced Value Erosion Model.
    
    Hypothesis: Anxiety correlates with a 'collapse' of confidence after failure.
    When a participant receives 0 coins (failure), their estimated value for that 
    alien doesn't just update normally; it actively shrinks (erodes) towards 0 
    by a multiplicative factor determined by STAI. This is distinct from a 
    learning rate, as it represents a loss of value retention.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - erosion_rate: [0, 1] Percentage of value lost upon failure, scaled by STAI.
    """
    learning_rate, beta, w, erosion_rate = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    erosion_factor = erosion_rate * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # Policy for first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        state_idx = state[trial]

        # Policy for second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        
        # ANXIETY MECHANISM: Erosion on Failure
        if reward[trial] == 0:
            # Multiplicative decay of the specific action value involved in the failure
            q_stage2_mf[state_idx, action_2[trial]] *= (1.0 - erosion_factor)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Context-Switch Reset
This model suggests that anxiety limits the cognitive resources required to maintain stable value representations across different contexts (spaceships). If the agent switches their Stage 1 choice (e.g., from Spaceship A to Spaceship U), high anxiety causes a "reset" or forgetting of the Stage 2 values, as the agent struggles to carry over context.

```python
import numpy as np

def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Context-Switch Reset Model.
    
    Hypothesis: Anxiety consumes working memory resources required to maintain 
    stable value representations across contexts. If the participant switches 
    their Stage 1 choice (changing context from Spaceship A to U or vice versa), 
    anxiety causes a partial 'reset' of the Stage 2 (Alien) values towards a 
    neutral prior (0.5), representing a failure to maintain the memory of the 
    aliens when the spaceship context changes.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - reset_strength: [0, 1] How much values reset to 0.5 upon context switch, scaled by STAI.
    """
    learning_rate, beta, w, reset_strength = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    reset_factor = reset_strength * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # Initialized at 0, but reset targets 0.5
    
    # Initialize Q-values to 0.5 (neutral probability) for this logic to make sense
    q_stage2_mf.fill(0.5)
    q_stage1_mf.fill(0.5)

    last_action_1 = -1

    for trial in range(n_trials):
        
        # ANXIETY MECHANISM: Context Switch Reset
        # If this is not the first trial, and the action changed...
        if trial > 0 and action_1[trial] != last_action_1:
            # Reset Stage 2 values towards 0.5 based on anxiety
            # New_Q = (1 - reset) * Old_Q + reset * 0.5
            q_stage2_mf = q_stage2_mf * (1.0 - reset_factor) + 0.5 * reset_factor

        # Policy for first choice
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        last_action_1 = action_1[trial]
        state_idx = state[trial]

        # Policy for second choice
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # Updates
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```