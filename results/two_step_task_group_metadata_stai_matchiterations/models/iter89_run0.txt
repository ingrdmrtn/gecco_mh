Here are 3 new cognitive models based on the two-step task, incorporating State-Trait Anxiety Inventory (STAI) scores into the decision-making process.

### Model 1: Anxiety-Induced Stage 2 Rigidity
This model hypothesizes that anxiety has a differential effect on exploration depending on the proximity to the reward. While Stage 1 is abstract (planning), Stage 2 involves immediate consumption. This model proposes that anxiety causes "choking" or extreme rigidity (high exploitation) specifically in Stage 2, reducing exploration when the stakes are visible.

```python
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Induced Stage 2 Rigidity Model.
    
    Hypothesis: Anxiety leads to a 'choking' effect or extreme rigidity specifically 
    in the second stage (proximal to reward). While Stage 1 exploration is governed 
    by a baseline beta, Stage 2 choices become more deterministic (higher beta) 
    as anxiety increases.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta_stage1: [0, 10] Inverse temperature for the first stage choice.
    - w: [0, 1] Weight for model-based control (0=MF, 1=MB).
    - beta_s2_base: [0, 10] Baseline inverse temperature for Stage 2.
    - anxiety_stiffening: [0, 5] How much STAI increases Stage 2 beta (rigidity).
    """
    learning_rate, beta_stage1, w, beta_s2_base, anxiety_stiffening = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Calculate the anxiety-modulated beta for stage 2
    # Higher anxiety -> Higher beta -> More deterministic/rigid choice in Stage 2
    beta_stage2 = beta_s2_base + (anxiety_stiffening * stai_score)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2)) # State x Action

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net_s1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Use beta_stage1 for the first decision
        exp_q1 = np.exp(beta_stage1 * q_net_s1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        # Use the anxiety-modulated beta_stage2 for the second decision
        exp_q2 = np.exp(beta_stage2 * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 2: Anxiety-Driven Model Blurring (Entropy)
This model suggests that anxiety degrades the precision of the internal model used for planning. Instead of simply reducing the weight of the model-based system (`w`), anxiety "blurs" the transition matrix itself. Anxious participants perceive the transitions as more random (closer to 50/50) than they actually are, reflecting a sense of uncontrollability or entropy.

```python
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Model Blurring (Entropy) Model.
    
    Hypothesis: Anxiety degrades the precision of the internal transition model.
    High anxiety participants blend the true transition matrix (70/30) with a 
    uniform random matrix (50/50), effectively perceiving the world as more 
    unpredictable regardless of their 'w' parameter.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - blur_base: [0, 1] Baseline amount of model blurring.
    - blur_anxiety: [0, 1] Additional blurring added by STAI score.
    """
    learning_rate, beta, w, blur_base, blur_anxiety = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]

    # Define the true structure and a maximum entropy (uniform) structure
    true_transition = np.array([[0.7, 0.3], [0.3, 0.7]])
    uniform_transition = np.array([[0.5, 0.5], [0.5, 0.5]])
    
    # Calculate mixing parameter based on anxiety
    # We clip it to ensure it stays within [0, 1]
    mixing_param = blur_base + (blur_anxiety * stai_score)
    if mixing_param > 1.0: mixing_param = 1.0
    if mixing_param < 0.0: mixing_param = 0.0
    
    # The agent uses a 'perceived' matrix that is a mix of truth and noise
    perceived_transition = (1 - mixing_param) * true_transition + mixing_param * uniform_transition

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        # MB calculation uses the ANXIETY-BLURRED transition matrix
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = perceived_transition @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```

### Model 3: Anxiety-Driven Counterfactual Rumination
This model posits that anxious individuals engage in excessive "what if" thinking (rumination). When they receive an outcome, they not only update the chosen path but also update the *unchosen* path based on a fictitious outcome (assuming the grass is greener, or conversely, that they avoided a loss).

```python
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-Driven Counterfactual Rumination Model.
    
    Hypothesis: Anxious participants engage in 'fictitious play' or rumination.
    They update the value of the unchosen Stage 1 option as if they had chosen it 
    and received the opposite of the current reward. The strength of this 
    counterfactual update is driven by their STAI score.
    
    Parameters:
    - learning_rate: [0, 1] Standard Q-learning rate for chosen actions.
    - beta: [0, 10] Inverse temperature.
    - w: [0, 1] Weight for model-based control.
    - rumination_scale: [0, 1] Scaling factor for counterfactual updating.
    """
    learning_rate, beta, w, rumination_scale = model_parameters
    n_trials = len(action_1)
    stai_score = stai[0]
    
    # Calculate the effective learning rate for the unchosen option
    # High anxiety -> stronger update of the path NOT taken
    lr_counterfactual = learning_rate * rumination_scale * stai_score

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for trial in range(n_trials):

        # --- Stage 1 Choice ---
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        q_net = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        exp_q1 = np.exp(beta * q_net)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[trial] = probs_1[action_1[trial]]
        
        state_idx = state[trial]

        # --- Stage 2 Choice ---
        exp_q2 = np.exp(beta * q_stage2_mf[state_idx])
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[trial] = probs_2[action_2[trial]]
  
        # --- Updates ---
        
        # 1. Standard update for the CHOSEN path
        delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
        q_stage1_mf[action_1[trial]] += learning_rate * delta_stage1
        
        # 2. Counterfactual update for the UNCHOSEN path
        # If I got Reward=0, I assume Unchosen would have given Reward=1 (Regret)
        # If I got Reward=1, I assume Unchosen would have given Reward=0 (Relief)
        unchosen_action = 1 - action_1[trial]
        fictitious_outcome = 1.0 - reward[trial] 
        
        # Note: We approximate the 'value' of the unchosen path's next state 
        # as the average value of the OTHER state, or simply drive it by the fictitious reward.
        # Here we use a simple MF update towards the fictitious outcome.
        delta_cf = fictitious_outcome - q_stage1_mf[unchosen_action]
        q_stage1_mf[unchosen_action] += lr_counterfactual * delta_cf
        
        # Stage 2 update (standard)
        delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
        q_stage2_mf[state_idx, action_2[trial]] += learning_rate * delta_stage2
        

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss
```