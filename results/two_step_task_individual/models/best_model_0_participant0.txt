def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Perseverative model-free SARSA(λ) with separate learning rates.
    
    A purely model-free controller updates first-stage values via SARSA(λ),
    using an eligibility trace to propagate reward prediction errors from
    stage 2 back to stage 1. A choice perseveration bias adds a bonus to
    repeating the previous action at each stage. A single softmax temperature
    governs both stages.
    
    Parameters
    ----------
    action_1 : array-like of int, shape (n_trials,)
        First-stage choices (0=A, 1=U).
    state : array-like of int, shape (n_trials,)
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int, shape (n_trials,)
        Second-stage choices (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float/int, shape (n_trials,)
        Obtained reward on each trial (e.g., 0/1 coins).
    model_parameters : iterable of floats, length 5
        [alpha1, alpha2, beta, lam, pers]
        - alpha1 in [0,1]: learning rate for stage-1 MF values.
        - alpha2 in [0,1]: learning rate for stage-2 MF values.
        - beta in [0,10]: inverse temperature used at both stages.
        - lam in [0,1]: eligibility trace; backprop from stage-2 RPE to stage-1.
        - pers in [0,1]: perseveration strength added to previously chosen action.
                         Implemented as additive bias to the chosen action's preference.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha1, alpha2, beta, lam, pers = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)       # A/U
    q_stage2 = np.zeros((2, 2))  # X/Y x actions

    prev_a1 = None
    prev_a2 = [None, None]  # one for each state

    for t in range(n_trials):
        s = state[t]

        pref1 = q_stage1.copy()
        if prev_a1 is not None:
            pref1[prev_a1] += pers

        pref1_centered = pref1 - np.max(pref1)
        probs_1 = np.exp(beta * pref1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        pref2 = q_stage2[s].copy()
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += pers

        pref2_centered = pref2 - np.max(pref2)
        probs_2 = np.exp(beta * pref2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        delta1_direct = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += alpha1 * (delta1_direct + lam * delta2)

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll