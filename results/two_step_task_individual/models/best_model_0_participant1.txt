def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Learned-transitions Model-Based planner with risk sensitivity and perseveration.
    
    This model learns the environmentâ€™s transition probabilities online and plans
    using those estimates. Second-stage values incorporate a risk penalty based on
    estimated Bernoulli outcome variance. A perseveration bias encourages repeating
    the previous stage-1 choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, alphaT, rho, phi]
        - alpha (reward learning rate, [0,1]): for updating second-stage reward expectations.
        - beta (inverse temperature, [0,10]): softmax for both stages.
        - alphaT (transition learning rate, [0,1]): for updating P(state | action_1).
        - rho (risk sensitivity, [0,1]): subtracts rho * variance from second-stage Q.
          Variance is p*(1-p), where p is the learned reward probability.
        - phi (perseveration, [0,1]): additive bias to repeat the last stage-1 action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, alphaT, rho, phi = model_parameters
    n_trials = len(action_1)


    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    q2 = np.zeros((2, 2))  # expected value
    p_hat = np.full((2, 2), 0.5)  # estimated reward probability (for variance)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        var = p_hat * (1.0 - p_hat)
        q2_risk = q2 - rho * var

        max_q2 = np.max(q2_risk, axis=1)
        q1 = T @ max_q2

        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += phi

        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * q2_risk[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]


        oh = np.array([0.0, 0.0])
        oh[s2] = 1.0
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh

        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        delta = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta
        p_hat[s2, a2] += alpha * (r - p_hat[s2, a2])

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll