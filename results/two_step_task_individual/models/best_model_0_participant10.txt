def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with asymmetric learning and PE gain to stage-1, plus first-stage stay bias.
    
    This model eschews planning and relies on model-free values only. Second-stage learning uses
    asymmetric learning rates for positive vs. negative prediction errors. The second-stage PE
    is propagated to the first-stage action via a gain factor (without an explicit eligibility trace).
    A first-stage perseveration bias favors repeating the last chosen spaceship.
    
    Parameters (all used; suggested bounds):
    - alpha_pos: [0,1] learning rate when second-stage TD error is positive
    - alpha_neg: [0,1] learning rate when second-stage TD error is negative
    - pe_gain:  [0,1] gain scaling how much second-stage PE updates first-stage MF value
    - beta:     [0,10] softmax inverse temperature shared across stages
    - stay1:    [0,1] first-stage perseveration strength added to last-chosen spaceship
    
    Inputs:
    - action_1: array (n_trials,), 0/1 for spaceship A/U
    - state:    array (n_trials,), 0/1 for planet X/Y actually visited
    - action_2: array (n_trials,), 0/1 for alien within visited planet
    - reward:   array (n_trials,), typically 0/1
    - model_parameters: iterable [alpha_pos, alpha_neg, pe_gain, beta, stay1]
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, pe_gain, beta, stay1 = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)         # model-free first-stage Q-values
    q2 = np.zeros((2, 2))    # second-stage Q-values: Q2[state, action]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        pref1 = q1.copy()
        if last_a1 is not None:
            bias = np.zeros(2)
            bias[last_a1] = stay1
            pref1 = pref1 + bias

        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        pref2 = q2[s].copy()
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]


        td2 = reward[t] - q2[s, a2]
        alpha2 = alpha_pos if td2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2 * td2

        td1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha2 * td1  # use same asymmetry as the experienced TD

        q1[a1] += pe_gain * alpha2 * td2

        last_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss