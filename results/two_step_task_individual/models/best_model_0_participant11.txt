def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Associability-modulated learning with transition-dependent credit assignment and perseveration.
    Mechanisms:
    - Stage-2 associability (Pearceâ€“Hall-like): learning rate increases with recent unsigned prediction error.
      Associability is updated per (state, action) and bounded in [0,1].
    - Transition-dependent credit assignment at stage 1: after rare transitions, some credit is shifted to the
      unchosen first-stage action to approximate model-based credit routing.
    - Standard softmax with perseveration at both stages.

    Parameters (all used; keep within bounds):
    - alpha0 in [0,1]: base learning rate scale.
    - beta in [0,10]: inverse temperature for softmax choices at both stages.
    - phi in [0,1]: associability update rate (higher -> faster changes in learning rate).
    - xi in [0,1]: fraction of TD credit shifted to the unchosen first-stage action after rare transitions.
    - stick in [0,1]: perseveration strength at both stages.

    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of second-stage states (0=X, 1=Y)
    - action_2: array of second-stage choices on the visited state (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: [alpha0, beta, phi, xi, stick]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha0, beta, phi, xi, stick = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    A = np.full((2, 2), 0.5)

    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    p_common = 0.7  # used only to determine common vs rare

    for t in range(n_trials):

        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        pref1 = beta * q1 + bias1
        pref1 -= np.max(pref1)
        probs_1 = np.exp(pref1)
        probs_1 /= np.sum(probs_1)

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick

        pref2 = beta * q2[s] + bias2
        pref2 -= np.max(pref2)
        probs_2 = np.exp(pref2)
        probs_2 /= np.sum(probs_2)

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]


        alpha2_t = alpha0 * (0.5 + 0.5 * np.clip(A[s, a2], 0.0, 1.0))
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2_t * delta2

        A[s, a2] = (1.0 - phi) * A[s, a2] + phi * np.clip(abs(delta2), 0.0, 1.0)

        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

        target = q2[s, a2]

        if is_common:

            delta_chosen = target - q1[a1]
            q1[a1] += alpha0 * delta_chosen
        else:

            other = 1 - a1
            delta_chosen = target - q1[a1]
            delta_other = target - q1[other]
            q1[a1] += alpha0 * (1.0 - xi) * delta_chosen
            q1[other] += alpha0 * xi * delta_other

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll