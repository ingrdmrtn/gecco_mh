def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with decay, risk-sensitive utility, and dual inverse temperatures.
    
    This model assumes choices are driven by model-free values only. It includes:
    - Exponential decay (forgetting) of all Q-values each trial.
    - Risk/utility curvature that transforms rewards as u = reward^rho.
    - Separate inverse temperatures for stage 1 and stage 2.
    - An eligibility-like propagation from stage 2 to stage 1 scaled by the decay parameter.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state.
    reward : array-like of float
        Reward per trial (>= 0).
    model_parameters : tuple/list of 5 floats
        (alpha, beta1, beta2, rho, d)
        - alpha in [0,1]: learning rate for value updates.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2.
        - rho in [0,1]: risk/utility curvature; rho<1 leads to diminishing sensitivity.
        - d in [0,1]: decay rate applied to Q-values each trial; also scales eligibility propagation.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, rho, d = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for t in range(n_trials):

        q_stage1_mf *= (1.0 - d)
        q_stage2_mf *= (1.0 - d)

        exp_q1 = np.exp(beta1 * (q_stage1_mf - np.max(q_stage1_mf)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2_state = q_stage2_mf[s]
        exp_q2 = np.exp(beta2 * (q2_state - np.max(q2_state)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        u = (r ** rho) if r >= 0 else -((-r) ** rho)

        delta2 = u - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + d * alpha * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss