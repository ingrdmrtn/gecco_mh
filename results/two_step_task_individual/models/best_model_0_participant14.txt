def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Surprise-modulated arbitration between model-based and model-free control.
    
    The agent learns model-free values but uses a model-based planner at stage 1
    with a dynamic arbitration weight that increases with transition surprise
    (rarer transitions induce more MB control). A first-stage stay-bias captures
    tendency to repeat the last chosen spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states (planets): 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the visited planet.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, omega0, gamma_surprise, stick1)
        - alpha in [0,1]: learning rate for model-free values at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega0 in [0,1]: baseline weight for MB control at stage 1.
        - gamma_surprise in [0,1]: gain on transition surprise to boost MB weight.
        - stick1 in [0,1]: stay-bias added to the previously chosen first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha, beta, omega0, gamma_surprise, stick1 = model_parameters
    n_trials = len(action_1)

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1

    for t in range(n_trials):

        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2

        a1 = action_1[t]
        s = state[t]
        surprise = 1.0 - T[a1, s]  # common -> 0.3, rare -> 0.7

        w = omega0 + gamma_surprise * surprise

        if w < 0.0:
            w = 0.0
        elif w > 1.0:
            w = 1.0

        q1 = (1 - w) * q1_mf + w * q1_mb

        if prev_a1 in (0, 1):
            q1[prev_a1] += stick1

        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        a2 = action_2[t]
        q2 = q2_mf[s, :]
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        pe1 = r - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss