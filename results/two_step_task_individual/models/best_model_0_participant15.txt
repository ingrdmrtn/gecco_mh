def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with dual-stage perseveration and value forgetting, plus MB guidance.

    Mechanisms
    ----------
    - Stage-2 model-free learning of alien values (Q2).
    - Stage-1 action values include both a cached MF component (Q1_MF) and MB guidance via
      current Q2 (using fixed transition matrix with common-rare structure).
    - Both stages include choice perseveration kernels (carry-over of last chosen action).
    - Value forgetting toward a neutral prior (0.5) at both stages.

    Parameters (model_parameters)
    -----------------------------
    [alpha, beta, persev1, persev2, forget]
      - alpha in [0,1]: learning rate for Q-value updates.
      - beta in [0,10]: inverse temperature at both stages.
      - persev1 in [0,1]: strength of first-stage perseveration kernel.
      - persev2 in [0,1]: strength of second-stage perseveration kernel.
      - forget in [0,1]: per-trial decay toward 0.5 for all Q-values (both stages).

    Inputs
    ------
    action_1 : array-like of int (0 or 1)
    state : array-like of int (0 or 1)
    action_2 : array-like of int (0 or 1)
    reward : array-like of float (0 or 1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, persev1, persev2, forget = model_parameters
    n_trials = len(action_1)

    T_fixed = np.array([[0.7, 0.3],  # A -> X (0.7), Y (0.3)
                        [0.3, 0.7]]) # U -> X (0.3), Y (0.7)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    K1 = np.zeros(2)
    K2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        logits1 = beta * (q1_mf + q1_mb + persev1 * K1 - np.max(q1_mf + q1_mb + persev1 * K1))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = beta * (q2[s] + persev2 * K2[s] - np.max(q2[s] + persev2 * K2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        K1 *= (1 - alpha)
        K1[a1] += alpha

        K2 *= (1 - alpha)
        K2[s, a2] += alpha

        q1_mf = (1 - forget) * q1_mf + forget * 0.5
        q2 = (1 - forget) * q2 + forget * 0.5

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll