def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with decaying choice kernels and directed exploration bonus (UCB-like).
    
    This model learns MF Q-values at both stages. It augments decision values with:
    - A decaying choice kernel that captures a graded tendency to repeat recent actions.
    - A directed exploration bonus inversely proportional to choice counts (higher for
      less tried actions), applied separately at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet: 0/1.
    reward : array-like of float
        Coins received at the end of each trial.
    model_parameters : sequence
        [alpha, beta, kappa, tau, b_unc]
        - alpha (learning rate, [0,1]): MF learning rate for Q-values.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - kappa (choice-kernel gain, [0,1]): Weight of the choice kernel added to values.
        - tau (kernel decay, [0,1]): Fractional decay of the choice kernel per trial.
        - b_unc (uncertainty bonus, [0,1]): Weight of directed exploration bonus (UCB-like).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, kappa, tau, b_unc = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    K1 = np.zeros(2)
    K2 = np.zeros((2, 2))

    N1 = np.zeros(2)
    N2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        bonus1 = b_unc / np.sqrt(N1 + 1.0)
        bonus2 = b_unc / np.sqrt(N2[s] + 1.0)

        v1 = q1 + kappa * K1 + bonus1
        exp_v1 = np.exp(beta * (v1 - np.max(v1)))
        probs1 = exp_v1 / (np.sum(exp_v1) + eps)
        p_choice_1[t] = probs1[a1]

        v2 = q2[s] + kappa * K2[s] + bonus2
        exp_v2 = np.exp(beta * (v2 - np.max(v2)))
        probs2 = exp_v2 / (np.sum(exp_v2) + eps)
        p_choice_2[t] = probs2[a2]

        K1 *= (1.0 - tau)
        K2 *= (1.0 - tau)

        K1[a1] += 1.0
        K2[s, a2] += 1.0

        N1[a1] += 1.0
        N2[s, a2] += 1.0

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll