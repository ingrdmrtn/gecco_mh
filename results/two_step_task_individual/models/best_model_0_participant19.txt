def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with uncertainty bonus (UCB) at stage 2 and value forgetting.

    The agent:
    - Uses purely model-free control at stage 1 (no model-based component).
    - At stage 2, augments MF Q-values with an uncertainty bonus to encourage exploration.
    - Applies reward sensitivity to scale outcomes before learning.
    - Applies value forgetting toward a neutral baseline at both stages each trial.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage actions per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions per trial (0/1 for the two aliens on that planet).
    reward : array-like of float
        Reward outcome per trial (e.g., -1/0/1).
    model_parameters : iterable of float
        [alpha, beta, rho, c, delta]
        - alpha in [0, 1]: learning rate for MF value updates.
        - beta  in [0, 10]: inverse temperature for softmax choice.
        - rho   in [0, 1]: reward sensitivity; scales experienced reward before updating.
        - c     in [0, 1]: uncertainty-bonus weight at stage 2 (larger favors less-visited options).
        - delta in [0, 1]: value forgetting rate toward 0 at both stages each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, rho, c, delta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)         # MF Q at stage 1 (A/U)
    q_stage2 = np.zeros((2, 2))    # MF Q at stage 2 (X/Y x two aliens)
    visit_counts = np.zeros((2, 2))  # counts for UCB at stage 2

    eps = 1e-12

    for t in range(n_trials):

        q_stage1 *= (1.0 - delta)
        q_stage2 *= (1.0 - delta)

        q1 = q_stage1
        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_shift)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        n_sa = visit_counts[s] + 1.0  # add 1 to avoid division by zero
        bonus = c / np.sqrt(n_sa)
        q2_eff = q_stage2[s] + bonus
        q2_shift = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2_shift)
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        u = rho * r  # down/up-weight outcome magnitude


        pe2 = u - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        target1 = q_stage2[s, a2]
        pe1 = target1 - q_stage1[a1]
        q_stage1[a1] += alpha * pe1

        visit_counts[s, a2] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll