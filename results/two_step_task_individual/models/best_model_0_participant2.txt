def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model-based/model-free RL with eligibility trace and perseveration at both stages.

    This model combines a model-based (MB) planner that uses the known transition structure
    with a model-free (MF) controller updated via temporal-difference learning and an eligibility
    trace to propagate second-stage outcomes back to first-stage values. It also includes
    choice perseveration (stickiness) biases at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached planet:
        at planet X (state=0): 0 = alien W, 1 = alien S;
        at planet Y (state=1): 0 = alien P, 1 = alien H.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w, lam, p_stick)
        - alpha in [0,1]: Learning rate for value updates (both stages).
        - beta in [0,10]: Softmax inverse temperature shared across stages.
        - w in [0,1]: Hybrid weight; 1 = purely model-based, 0 = purely model-free at stage 1.
        - lam in [0,1]: Eligibility trace; larger values propagate reward more to stage-1.
        - p_stick in [0,1]: Perseveration strength added to the previously chosen action at both stages.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, lam, p_stick = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)          # Q for first-stage actions (A,U)
    q_stage2_mf = np.zeros((2, 2))     # Q for second-stage actions within each state

    prev_a1 = -1
    prev_a2 = np.array([-1, -1])  # store last choice per state for second-stage; -1 = none

    for t in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # best option per state
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value by each spaceship

        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        if prev_a1 in (0, 1):
            q1[prev_a1] += p_stick

        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2 = q_stage2_mf[s].copy()

        if prev_a2[s] in (0, 1):
            q2[prev_a2[s]] += p_stick

        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        q2_sa_old = q_stage2_mf[s, a2]


        delta2 = reward[t] - q2_sa_old
        q_stage2_mf[s, a2] = q2_sa_old + alpha * delta2


        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha * (q2_sa_old - q_stage1_mf[a1]) + alpha * lam * (reward[t] - q2_sa_old)

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll