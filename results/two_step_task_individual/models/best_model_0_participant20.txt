def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-learning model-based planner with confidence-weighted planning and Stage-1 perseveration.

    The agent learns its own transition model T_hat by observing which planet follows each spaceship.
    Stage-1 planning uses T_hat and the current Stage-2 values. Planning strength is down-weighted when
    transition confidence is low (high entropy in T_hat). Stage-1 perseveration captures a tendency
    to repeat the previous first-stage choice. Stage-2 values are learned model-free from reward.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (number of coins) per trial.
    model_parameters : sequence of floats
        [alpha2, beta, alpha_T, chi_conf, rho1_stay]
        - alpha2 in [0,1]: learning rate for Stage-2 Q-values.
        - beta in [0,10]: base inverse temperature for softmax at both stages.
        - alpha_T in [0,1]: learning rate for transition matrix T_hat (per chosen first-stage action).
        - chi_conf in [0,1]: confidence-sensitivity; higher values reduce planning when transitions are uncertain.
        - rho1_stay in [0,1]: strength of Stage-1 perseveration (bias to repeat last first-stage action).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, alpha_T, chi_conf, rho1_stay = model_parameters
    n_trials = len(action_1)

    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float) + 0.5  # Stage-2 values

    last_a1 = -1  # for perseveration

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]

        row = T_hat[a1].copy()
        row = np.clip(row, eps, 1.0)  # avoid log(0)
        row = row / np.sum(row)
        H = -np.sum(row * np.log(row))  # natural log entropy in [0, ln2]
        H_norm = H / np.log(2.0)        # normalize to [0,1]
        conf_scale = 1.0 - chi_conf * H_norm  # down-weight MB when entropy is high

        max_q2 = np.max(q2, axis=1)   # value per state
        q1_mb = T_hat @ max_q2        # shape (2,)

        logits1 = conf_scale * q1_mb

        if last_a1 >= 0:
            stick = np.zeros(2, dtype=float)
            stick[last_a1] += rho1_stay
            logits1 = logits1 + stick

        l1 = beta * (logits1 - np.max(logits1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        l2 = beta * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        target_vec = np.array([0.0, 0.0], dtype=float)
        target_vec[s] = 1.0
        T_hat[a1] = (1.0 - alpha_T) * T_hat[a1] + alpha_T * target_vec

        T_hat[a1] = T_hat[a1] / (np.sum(T_hat[a1]) + eps)

        last_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll