def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free RL with eligibility trace and perseveration.
    
    This model blends model-based planning at stage 1 with a model-free value for
    first-stage actions, learned via an eligibility trace from stage-2 outcomes.
    Perseveration biases repeating the previous first-stage choice.
    
    Parameters (all must be in bounds):
    - alpha: [0,1] learning rate for value updates (both stages)
    - beta: [0,10] inverse temperature for both stages' softmax policies
    - w_mb: [0,1] weight of model-based value in the first-stage action values
    - lambda_et: [0,1] eligibility trace for propagating stage-2 TD error to stage-1 MF values
    - kappa: [0,1] perseveration strength added to the last chosen first-stage action
    
    Inputs:
    - action_1: array of length T with first-stage choices (0=A, 1=U)
    - state:    array of length T with second-stage states (0=X, 1=Y)
    - action_2: array of length T with second-stage choices (0 or 1; alien within state)
    - reward:   array of length T with outcomes (0/1 coins)
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w_mb, lambda_et, kappa = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],  # from A to [X, Y]
                                  [0.3, 0.7]]) # from U to [X, Y]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)           # model-free value for first-stage actions
    q_stage2 = np.full((2, 2), 0.5)     # second-stage Q-values initialized to 0.5

    last_a1 = None

    for t in range(n_trials):

        max_q_stage2 = np.max(q_stage2, axis=1)     # best option on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] += kappa

        q1 = (1.0 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb + bias

        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]


        delta2 = r - q_stage2[s, a2]

        q_stage2[s, a2] += alpha * delta2



        delta1_mf = (q_stage2[s, a2]) - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (lambda_et * delta2 + (1.0 - lambda_et) * delta1_mf)

        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss