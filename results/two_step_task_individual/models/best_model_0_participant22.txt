def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Successor-representation augmented hybrid model.
    
    This model learns a simple one-step successor map from first-stage actions
    to second-stage states (a 2x2 matrix), and uses it to compute a model-based
    value for stage-1 by backing up the best available second-stage values.
    It blends this SR-derived model-based value with a learned model-free
    first-stage value. Second-stage values are learned with TD. A first-stage
    choice stickiness bias captures perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (typically 0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha2: [0,1] learning rate for second-stage Q-values and for updating the model-free stage-1 value.
        - eta: [0,1] learning rate for the successor map (action->state occupancy probabilities).
        - beta: [0,10] inverse temperature for softmax at both stages.
        - omega: [0,1] weight on SR-derived model-based value in stage-1 (blend with model-free Q1).
        - stick1: [0,1] additive bias for repeating the previous first-stage action.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha2, eta, beta, omega, stick1 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2))

    q1_mf = np.zeros(2)

    M = np.full((2, 2), 0.5)  # start with uninformative mapping

    prev_a1 = None

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)      # best action value in each second-stage state
        q1_mb = M @ max_q2               # expected value under learned transitions

        q1_net = (1.0 - omega) * q1_mf + omega * q1_mb
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] += stick1
            q1_net = q1_net + bias

        q1_shift = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2_s = q2[s].copy()
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        target1 = q2[s, a2]
        q1_mf[a1] += alpha2 * (target1 - q1_mf[a1])


        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        M[a1, :] += eta * (one_hot_s - M[a1, :])

        prev_a1 = a1

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))