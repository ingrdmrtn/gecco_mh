def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free hierarchical TD with decaying choice kernels at both stages.

    This model uses model-free credit assignment from the second-stage value back to
    the first-stage action (hierarchical TD). Additionally, it includes decaying
    choice kernels (recency biases) at both stages that promote repeating recent choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1).
    reward : array-like of float
        Reward per trial (e.g., 0/1 coins).
    model_parameters : iterable
        [alpha, beta1, beta2, eta_c, zeta]
        - alpha in [0,1]: learning rate for both first- and second-stage TD updates.
        - beta1 in [0,10]: inverse temperature for first-stage softmax.
        - beta2 in [0,10]: inverse temperature for second-stage softmax.
        - eta_c in [0,1]: step size for increasing the choice kernel on chosen actions.
        - zeta in [0,1]: decay rate of choice kernels each trial (higher = faster decay).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, eta_c, zeta = model_parameters
    n_trials = len(action_1)

    q1_mf = np.zeros(2)        # values for A,U
    q2 = np.zeros((2, 2))      # values for actions at X,Y

    k1 = np.zeros(2)           # bias over A,U
    k2 = np.zeros(2)           # bias over second-stage actions (state-independent)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):

        logits1 = beta1 * q1_mf + k1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = beta2 * q2[s] + k2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        k1 *= (1.0 - zeta)
        k2 *= (1.0 - zeta)
        k1[a1] += eta_c
        k2[a2] += eta_c

    negloglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return negloglik