def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with eligibility trace and choice stickiness at both stages.
    
    This model combines model-based (MB) planning with model-free (MF) values for the first-stage decision, 
    uses a single MF learner at the second stage, and includes an eligibility trace that propagates 
    second-stage reward prediction errors back to the first stage. A choice stickiness term adds a bias 
    toward repeating the previous choice at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial; 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached; 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) within the state; 0 or 1.
    reward : array-like of float (typically 0 or 1)
        Reward obtained on each trial.
    model_parameters : sequence of floats
        [alpha, beta, w, lam, kappa]
        - alpha in [0, 1]: learning rate for MF updates at both stages.
        - beta in [0, 10]: inverse temperature (softmax) for both stages.
        - w in [0, 1]: weight balancing MB vs MF at stage 1 (w=1: pure MB; w=0: pure MF).
        - lam in [0, 1]: eligibility trace parameter propagating second-stage RPE to stage 1.
        - kappa in [0, 1]: choice stickiness strength (adds bias to repeat the previous choice).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))  # state x action

    prev_a1 = None
    prev_a2 = None  # last second-stage action (global stickiness)
    
    for t in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # for each state, best alien value
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value per first-stage action

        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        stickiness1 = np.zeros(2)
        if prev_a1 is not None:
            stickiness1[prev_a1] += kappa
        logits1 = beta * q1 + stickiness1
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        q2 = q_stage2_mf[s].copy()
        stickiness2 = np.zeros(2)
        if prev_a2 is not None:
            stickiness2[prev_a2] += kappa
        logits2 = beta * q2 + stickiness2
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]


        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        q_stage1_mf[a1] += alpha * lam * delta2

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll