def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and first-stage perseveration.
    
    This model blends model-based (MB) and model-free (MF) values at the first stage, 
    uses a standard MF learner at the second stage, propagates reward to the first stage 
    via an eligibility trace, and includes a first-stage perseveration bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; the two aliens available in the reached state) for each trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome on each trial.
    model_parameters : tuple/list
        (alpha, beta, w, lam, kappa)
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - w in [0,1]: weight of model-based control at stage 1 (1=fully MB, 0=fully MF).
        - lam in [0,1]: eligibility trace parameter scaling how much second-stage PE updates stage-1 MF.
        - kappa in [0,1]: strength of first-stage perseveration bias to repeat last first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],  # from A to (X, Y)
                                  [0.3, 0.7]]) # from U to (X, Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)        # for spaceships A,U
    q_stage2_mf = np.zeros((2, 2))   # for aliens within X and Y

    s1_prev = np.zeros(2)

    for t in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)            # best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2         # expected value of each spaceship

        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf + kappa * s1_prev

        exp_q1 = np.exp(beta * (q1_combined - np.max(q1_combined)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        exp_q2 = np.exp(beta * (q_stage2_mf[s, :] - np.max(q_stage2_mf[s, :])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        bootstrapped_value = q_stage2_mf[s, a2]
        delta1_boot = bootstrapped_value - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1_boot + alpha * lam * delta2

        s1_prev = np.zeros(2)
        s1_prev[a1] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll