def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive model-free control with transition–outcome interaction and leaky choice traces.

    This model is primarily model-free but captures the classic model-based stay/switch signature via
    a transition–outcome interaction bias on first-stage choices. It also includes leaky choice traces
    (autocorrelation) that bias repeating recent actions at both stages, and risk-sensitive utility.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received.
    model_parameters : iterable of floats
        [alpha, beta, gamma_risk, xi_interact, tau_trace]
        - alpha (0..1): Learning rate for model-free updates at both stages.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - gamma_risk (0..1): Risk sensitivity; utility u(r)=r^(gamma_risk). Lower values overweight small rewards.
        - xi_interact (0..1): Strength of transition–outcome interaction bias on first-stage stay/switch.
                              Positive values increase staying after rewarded-common or unrewarded-rare outcomes.
        - tau_trace (0..1): Leak factor for choice traces; closer to 1 retains more past choices.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, gamma_risk, xi_interact, tau_trace = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    trace1 = np.zeros(2)
    trace2 = np.zeros((2, 2))

    prev_a1 = None
    prev_common = None
    prev_r = None

    for t in range(n_trials):

        bias1 = trace1.copy()

        if prev_a1 is not None and prev_common is not None and prev_r is not None:
            signed_r = (2.0 * prev_r - 1.0)  # +1 if reward=1, -1 if reward=0
            signed_trans = 1.0 if prev_common else -1.0
            interact_bias = xi_interact * (signed_r * signed_trans)
            bias1[prev_a1] += interact_bias

        logits1 = beta * (q1 + bias1 - np.max(q1 + bias1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s2 = state[t]
        bias2 = trace2[s2].copy()
        logits2 = beta * (q2[s2] + bias2 - np.max(q2[s2] + bias2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        u = (r + 1e-12) ** gamma_risk

        td2 = u - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        target1 = q2[s2, a2]
        td1 = target1 - q1[a1]
        q1[a1] += alpha * td1

        trace1 *= tau_trace
        trace1[a1] += 1.0
        trace2[s2] *= tau_trace
        trace2[s2, a2] += 1.0

        prev_a1 = a1
        prev_r = r

        prev_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll