def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    MB-MF mixture with learned transition model and value forgetting at stage 2.
    Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 or 1).
    state : array-like, shape (n_trials,)
        Second-stage states (0 or 1).
    action_2 : array-like, shape (n_trials,)
        Second-stage actions (0 or 1).
    reward : array-like, shape (n_trials,)
        Rewards (e.g., 0/1).
    model_parameters : list or array-like, length 5
        [alpha_q, beta, w_mix, alpha_T, zeta]
        - alpha_q in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w_mix in [0,1]: mixture weight of model-based value at stage 1.
        - alpha_T in [0,1]: learning rate for transition probabilities T(s | a1).
        - zeta in [0,1]: forgetting/decay rate applied to all second-stage Q-values each trial.
    Notes
    -----
    - Transition matrix T is learned online via delta-rule with row-wise renormalization.
    - Stage-1 action values are a mixture of learned model-free values and model-based values computed from T and Q2.
    - Second-stage Q-values undergo decay toward zero controlled by zeta before each update.
    """
    alpha_q, beta, w_mix, alpha_T, zeta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    T = np.full((2, 2), 0.5)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        max_q2 = np.max(q2, axis=1)      # for each state, best second-stage value
        q1_mb = T @ max_q2               # expected value under learned transitions

        q1_mix = w_mix * q1_mb + (1.0 - w_mix) * q1_mf

        logits1 = beta * q1_mix
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]


        T[a1, :] = (1.0 - alpha_T) * T[a1, :]
        T[a1, s] += alpha_T


        q2 *= (1.0 - zeta)

        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll