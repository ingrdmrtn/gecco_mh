def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Purely model-based planner with uncertainty bonus, stage-1 stickiness, and value forgetting.

    The agent:
      - Learns second-stage action values (Q2) from rewards (learning rate alpha).
      - Adds an uncertainty-driven exploration bonus at stage 2 proportional to 1/sqrt(visit_count+1).
      - Plans at stage 1 using the fixed transition matrix and the (bonus-augmented) max Q2 values.
      - Includes a perseveration (stickiness) bias on stage-1 choices.
      - Applies forgetting (decay) to unchosen second-stage actions each trial.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward received on each trial.
    model_parameters : list or array
        [alpha, beta, stick1, upsilon, nu]
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature used at both stages.
        - stick1 in [0,1]: perseveration weight added to the previously chosen stage-1 action.
        - upsilon in [0,1]: weight of the uncertainty bonus (UCB-like) at stage 2.
        - nu in [0,1]: forgetting rate applied to all unchosen second-stage actions each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, stick1, upsilon, nu = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2))           # second-stage action values
    visits = np.zeros((2, 2))       # visit counts for uncertainty bonus

    prev_a1 = None

    for t in range(n_trials):

        bonus = upsilon / np.sqrt(visits + 1.0)  # shape (2,2)

        max_q2_bonus = np.max(q2 + bonus, axis=1)  # max over actions for each state
        q1 = transition_matrix @ max_q2_bonus

        if prev_a1 is not None:
            stick_vec = np.zeros(2)
            stick_vec[prev_a1] = 1.0
            q1 = q1 + stick1 * stick_vec

        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2_s = q2[s] + bonus[s]
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]


        q2 = q2 * (1.0 - nu)

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        visits[s, a2] += 1.0

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss