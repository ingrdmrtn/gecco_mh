def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free RL with asymmetric learning rates, forgetting, and stickiness.
    
    The agent uses only model-free values, with separate learning rates for 
    positive vs. negative outcomes, a uniform forgetting/decay on all Q-values,
    and a first-stage stickiness bias to repeat the previous first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=planet X, 1=planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the observed state per trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of floats
        [alpha_pos, alpha_neg, beta, rho, kappa]
        - alpha_pos in [0,1]: learning rate when reward prediction error >= 0.
        - alpha_neg in [0,1]: learning rate when reward prediction error < 0.
        - beta in [0,10]: inverse temperature for softmax choices at both stages.
        - rho in [0,1]: forgetting rate applied to all Q-values each trial
                       (higher rho => faster decay toward 0).
        - kappa in [0,1]: first-stage stickiness bias (added to the last chosen
                          first-stage action).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, rho, kappa = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)          # first stage actions
    q2 = np.zeros((2, 2))     # second stage (state x action)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None  # for stickiness

    for t in range(n_trials):

        q1 *= (1.0 - rho)
        q2 *= (1.0 - rho)

        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        q1_bias = q1 + bias1
        exp_q1 = np.exp(beta * (q1_bias - np.max(q1_bias)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        exp_q2 = np.exp(beta * (q2[s, :] - np.max(q2[s, :])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        delta2 = r - q2[s, a2]
        lr2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * delta2

        delta1 = q2[s, a2] - q1[a1]
        lr1 = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1[a1] += lr1 * delta1

        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll