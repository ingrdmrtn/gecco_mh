def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based RL with learned transition probabilities, model-free second-stage learning, and choice stickiness.
    
    The agent learns each spaceship's transition probabilities via a simple delta rule
    and plans using the learned transition model. Second-stage rewards are learned model-free.
    A stickiness parameter captures perseveration at both stages.
    
    Parameters (tuple):
    - alphaQ: learning rate for second-stage Q-values. Bounds [0,1].
    - alphaT: learning rate for transition probabilities T(a -> s). Bounds [0,1].
    - beta: inverse temperature for both stages. Bounds [0,10].
    - kappa1: first-stage choice stickiness weight (bias to repeat last first-stage action). Bounds [0,1].
    - kappa2: second-stage choice stickiness weight (bias to repeat last second-stage action within a state). Bounds [0,1].
    
    Inputs:
    - action_1: array-like, choices at stage 1 (0=A, 1=U).
    - state: array-like, observed second-stage state (0=X, 1=Y).
    - action_2: array-like, choices at stage 2 within the observed state (0 or 1).
    - reward: array-like, outcome (typically 0/1 coins).
    - model_parameters: tuple/list of (alphaQ, alphaT, beta, kappa1, kappa2).
    
    Returns:
    - Negative log-likelihood of the observed sequence of stage-1 and stage-2 choices.
    """
    alphaQ, alphaT, beta, kappa1, kappa2 = model_parameters
    n_trials = len(action_1)

    T = np.full((2, 2), 0.5)

    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):

        maxQ_by_state = np.max(Q2, axis=1)          # value of best action per state
        Q1_mb = T @ maxQ_by_state                   # expected value per first-stage action

        pref1 = Q1_mb.copy()
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] = 1.0
            pref1 = pref1 + kappa1 * bias

        pref1c = pref1 - np.max(pref1)
        probs1 = np.exp(beta * pref1c)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        pref2 = Q2[s].copy()
        prev_a2 = prev_a2_by_state[s]
        if prev_a2 is not None:
            bias2 = np.zeros(2)
            bias2[prev_a2] = 1.0
            pref2 = pref2 + kappa2 * bias2

        pref2c = pref2 - np.max(pref2)
        probs2 = np.exp(beta * pref2c)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        Q2[s, a2] += alphaQ * (r - Q2[s, a2])


        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alphaT * (target - T[a1, sp])

        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, :] = T[a1, :] / row_sum

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll