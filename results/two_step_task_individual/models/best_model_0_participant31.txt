def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Asymmetric TD learning with eligibility trace and value forgetting.

    This is a purely model-free controller that:
    - Uses separate learning rates for positive vs. negative TD errors (asymmetric learning).
    - Propagates second-stage TD error to the chosen first-stage action via an eligibility trace (lambda).
    - Applies value forgetting (decay toward zero) to all Q-values each trial.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage action within state (0/1).
    reward : array-like of float
        Received reward (e.g., 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha_pos, alpha_neg, beta, lambda_et, decay]
        - alpha_pos in [0,1]: Learning rate applied when TD error is >= 0.
        - alpha_neg in [0,1]: Learning rate applied when TD error is < 0.
        - beta in [0,10]: Inverse temperature for both stages.
        - lambda_et in [0,1]: Eligibility trace coefficient for bootstrapping from stage 2 to stage 1.
        - decay in [0,1]: Value forgetting rate applied to all Q-values each trial (higher = more forgetting).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, lambda_et, decay = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)          # stage-1 MF Q
    q2 = np.zeros((2, 2))     # stage-2 MF Q

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        q1c = q1 - np.max(q1)
        pi1 = np.exp(beta * q1c)
        pi1 = pi1 / np.sum(pi1)
        a1 = action_1[t]
        p_choice_1[t] = pi1[a1]

        s = state[t]
        q2s = q2[s, :]
        q2c = q2s - np.max(q2s)
        pi2 = np.exp(beta * q2c)
        pi2 = pi2 / np.sum(pi2)
        a2 = action_2[t]
        p_choice_2[t] = pi2[a2]

        r = reward[t]

        delta2 = r - q2[s, a2]
        alpha2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2 * delta2

        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        alpha1 = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1[a1] += (lambda_et * alpha1) * delta1

        q2 *= (1.0 - decay)
        q1 *= (1.0 - decay)

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik