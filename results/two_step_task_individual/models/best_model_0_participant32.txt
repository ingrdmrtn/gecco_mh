def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free with asymmetric learning, transition-dependent credit, and forgetting.
    Computes the negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of floats
        (alpha_pos, alpha_neg, beta, eta, f)
        - alpha_pos in [0,1]: learning rate when reward = 1 at stage 2.
        - alpha_neg in [0,1]: learning rate when reward = 0 at stage 2.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - eta in [0,1]: reduces stage-1 credit assignment after rare transitions
                       (0 = no reduction; 1 = no learning after rare).
        - f in [0,1]: forgetting toward priors (Q1 toward 0, Q2 toward 0.5) each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha_pos, alpha_neg, beta, eta, f = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)             # stage-1 MF, prior 0
    q2 = np.full((2, 2), 0.5)    # stage-2 MF, prior 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        exp1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        exp2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        alpha2 = alpha_pos if r > 0.0 else alpha_neg

        q2[s, a2] += alpha2 * (r - q2[s, a2])


        is_common = (a1 == s)

        credit_scale = 1.0 if is_common else (1.0 - eta)

        td1 = q2[s, a2] - q1[a1]
        q1[a1] += credit_scale * alpha2 * td1

        q1 = (1.0 - f) * q1  # toward 0
        q2 = (1.0 - f) * q2 + f * 0.5  # toward 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss