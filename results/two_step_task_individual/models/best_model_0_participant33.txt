def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free agent with eligibility trace and choice stickiness.
    
    This model blends a model-based (MB) plan computed from a known transition matrix
    with model-free (MF) action values learned from rewards. It also uses an eligibility
    trace to propagate reward prediction errors to the first-stage values and includes
    a choice stickiness bias that favors repeating the most recent choices at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=Planet X, 1=Planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state (0 or 1 for the two aliens on that planet).
    reward : array-like of float (0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha: learning rate for MF value updates and eligibility propagation. [0,1]
        - beta: inverse temperature for softmax choice at both stages. [0,10]
        - w: weight of model-based value in first-stage decision (0=MF only, 1=MB only). [0,1]
        - lam: eligibility trace parameter propagating second-stage RPE to first-stage. [0,1]
        - kappa: stickiness strength added to the last chosen action at each stage. [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of first- and second-stage choices.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],  # P(state | action A)
                                  [0.3, 0.7]]) # P(state | action U)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)        # First-stage MF Q for actions [A,U]
    q2_mf = np.zeros((2, 2))   # Second-stage MF Q for states [X,Y] and actions [0,1]

    last_a1 = -1
    last_a2 = np.array([-1, -1])  # per state

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        max_q2 = np.max(q2_mf, axis=1)  # shape (2,)
        q1_mb = transition_matrix @ max_q2  # shape (2,)

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        if last_a1 != -1:
            bias = np.zeros(2)
            bias[last_a1] += kappa
            q1 = q1 + bias


        q1_center = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_center)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        p_choice_1[t] = probs_1[a1]

        q2 = q2_mf[s].copy()
        if last_a2[s] != -1:
            q2[last_a2[s]] += kappa

        q2_center = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_center)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        p_choice_2[t] = probs_2[a2]

        q2_sa_old = q2_mf[s, a2]

        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        delta1 = q2_sa_old - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        q1_mf[a1] += alpha * lam * delta2

        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll