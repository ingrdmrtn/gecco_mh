def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Volatility-gated MB/MF arbitration with value decay and stage-2 stickiness.
    
    This model blends model-based (MB) and model-free (MF) control at the first stage,
    with the arbitration weight adapting to an online estimate of reward volatility.
    When recent reward prediction errors are large (high volatility), the model reduces
    reliance on MB planning. MF values use a decay (forgetting) term. A stage-2
    perseveration bias captures the tendency to repeat the previous second-stage action
    within the same state.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial on each planet.
    reward : array-like of float
        Outcome received (typically 0/1).
    model_parameters : sequence of floats
        [alpha_q, beta, kappa2, decay, phi_vol]
        - alpha_q in [0,1]: learning rate for MF values (stage-2) and volatility trace.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - kappa2 in [0,1]: stage-2 perseveration bias toward repeating the previous
          action on the same planet.
        - decay in [0,1]: forgetting factor applied to MF values each trial
          (1 means no decay; values move toward zero if decay<1).
        - phi_vol in [0,1]: arbitration gain; effective MB weight is phi_vol*(1 - v),
          where v is the running average of absolute RPE magnitude (volatility proxy).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha_q, beta, kappa2, decay, phi_vol = model_parameters
    n_trials = len(action_1)

    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    q_s2 = np.zeros((2, 2))      # Q(s, a2) MF
    q_s1_mf = np.zeros(2)        # First-stage MF values

    v_trace = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a2 = [None, None]

    for t in range(n_trials):

        max_q2 = np.max(q_s2, axis=1)  # per state
        q1_mb = T @ max_q2


        v_per_state = np.max(v_trace, axis=1)  # volatility of best action per state

        v_expected = T @ v_per_state  # shape (2,)
        w_mb = phi_vol * (1.0 - np.clip(v_expected, 0.0, 1.0))  # in [0,phi_vol] subset of [0,1]

        q1_blend = w_mb * q1_mb + (1.0 - w_mb) * q_s1_mf

        logits1 = beta * q1_blend
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] = 1.0

        logits2 = beta * q_s2[s, :] + kappa2 * bias2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        q_s2 *= decay
        q_s1_mf *= decay

        pe2 = r - q_s2[s, a2]
        q_s2[s, a2] += alpha_q * pe2

        v_trace[s, a2] = (1.0 - alpha_q) * v_trace[s, a2] + alpha_q * abs(pe2)

        q_s1_mf[a1] += alpha_q * pe2

        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll