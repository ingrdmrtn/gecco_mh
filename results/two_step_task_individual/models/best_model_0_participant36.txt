def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with Pearceâ€“Hall associability and one-step perseveration.

    Mechanism:
    - Second-stage Q-values are learned with an associability-modulated learning rate.
      Each state-action has an associability A(s,a) that tracks recent surprise:
        A <- (1 - assoc_tau)*A + assoc_tau*|delta|
      Effective learning rate is base_lr * A(s,a).
    - First-stage values are model-based using the known transition structure and
      the current second-stage Q-values.
    - Perseveration biases additively increase the logit of repeating the previous
      action at each stage (separately parameterized for stages 1 and 2).

    Parameters (bounds):
    - base_lr in [0, 1]: Base learning rate scaled by associability at stage 2.
    - beta in [0, 10]: Inverse temperature for softmax at both stages.
    - assoc_tau in [0, 1]: Smoothing for associability updates (higher -> more reactive).
    - stick1 in [0, 1]: Strength of first-stage perseveration bias.
    - stick2 in [0, 1]: Strength of second-stage perseveration bias.

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states per trial (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, second-stage actions within observed state.
    - reward: array-like of floats/ints (typically 0/1), reward outcome per trial.
    - model_parameters: iterable [base_lr, beta, assoc_tau, stick1, stick2].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    base_lr, beta, assoc_tau, stick1, stick2 = model_parameters
    n_trials = len(action_1)

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))
    A = np.ones((2, 2))  # start with full associability to allow early learning

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2 = [None, None]  # per state

    for t in range(n_trials):

        V = np.max(q2, axis=1)   # value of states X and Y
        q1_mb = T @ V

        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick1

        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick2

        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        delta2 = r - q2[s, a2]
        A[s, a2] = (1.0 - assoc_tau) * A[s, a2] + assoc_tau * abs(delta2)
        eff_lr = base_lr * A[s, a2]
        q2[s, a2] += eff_lr * delta2

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll