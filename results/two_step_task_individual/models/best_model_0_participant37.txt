def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model-based/model-free RL with eligibility trace and stickiness.
    Parameters (all used):
    - alpha: learning rate for both stages (bound [0,1])
    - beta: inverse temperature for softmax (bound [0,10])
    - w: weight on model-based vs model-free (0=model-free, 1=model-based) (bound [0,1])
    - lam: eligibility trace that propagates second-stage reward to first-stage MF (bound [0,1])
    - kappa: choice stickiness bias for repeating the last action at each stage (bound [0,1])

    Inputs:
    - action_1: array-like of ints in {0,1} for first-stage actions (A=0, U=1)
    - state: array-like of ints in {0,1} for second-stage states/planets (X=0, Y=1)
    - action_2: array-like of ints in {0,1} for second-stage actions/aliens (index within state)
    - reward: array-like of floats in [0,1]
    - model_parameters: iterable of the five parameters [alpha, beta, w, lam, kappa]

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],   # from A to [X, Y]
                                  [0.3, 0.7]])  # from U to [X, Y]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)           # model-free first-stage Q
    q2 = np.zeros((2, 2))         # second-stage Q: [state, action]

    last_a1 = None
    last_a2 = np.array([None, None], dtype=object)  # per-state last second-stage action

    for t in range(n_trials):
        s = state[t]

        max_q2 = np.max(q2, axis=1)  # max over actions for each state
        q1_mb = transition_matrix @ max_q2  # expected value for each first-stage action

        q1_comb = (1.0 - w) * q1_mf + w * q1_mb
        stick1 = np.zeros(2)
        if last_a1 is not None:
            stick1[last_a1] += kappa
        logits1 = beta * q1_comb + stick1

        logits1_shift = logits1 - np.max(logits1)
        exp1 = np.exp(logits1_shift)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        q2_state = q2[s].copy()
        stick2 = np.zeros(2)
        if last_a2[s] is not None:
            stick2[last_a2[s]] += kappa
        logits2 = beta * q2_state + stick2
        logits2_shift = logits2 - np.max(logits2)
        exp2 = np.exp(logits2_shift)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]


        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * (delta1 + lam * delta2)

        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll