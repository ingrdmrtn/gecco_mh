def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Model-free with a dynamic choice kernel at stage 1.

    The agent:
    - Learns MF values for both stages.
    - Augments stage-1 decision values with a recency-weighted choice kernel that captures
      inertia/exploration dynamics independent of reward.
    - No explicit model-based planning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A; 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 = first alien, 1 = second alien).
    reward : array-like of float (e.g., 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_r, alpha_ck, w_ck, beta1, beta2]
        - alpha_r: [0, 1] learning rate for model-free values
        - alpha_ck: [0, 1] learning rate for the choice kernel (recency of last choice)
        - w_ck: [0, 1] weight of the choice kernel in stage-1 decision values
        - beta1: [0, 10] inverse temperature at stage 1
        - beta2: [0, 10] inverse temperature at stage 2

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, alpha_ck, w_ck, beta1, beta2 = model_parameters
    n_trials = len(action_1)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))  # state x action

    ck = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        ck_centered = ck - np.mean(ck)
        q1 = (1.0 - w_ck) * q1_mf + w_ck * ck_centered

        a1 = action_1[t]
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        a2 = action_2[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        exp_q2 = np.exp(beta2 * q2c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        target1 = q2[s, a2]
        q1_mf[a1] += alpha_r * (target1 - q1_mf[a1])

        ck *= (1.0 - alpha_ck)
        ck[a1] += alpha_ck

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik