def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planning with learned transitions and perseveration bias.
    Learns second-stage values from rewards and learns the transition probabilities online.
    Stage-1 decisions plan using the learned transition matrix and the current second-stage values.
    Includes a perseveration bias favoring repeating the previous stage-1 action.
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state reached per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state
        (for X: 0 = W, 1 = S; for Y: 0 = P, 1 = H).
    reward : array-like of float
        Reward observed per trial (e.g., 0 or 1).
    model_parameters : iterable of float
        Tuple/list with four parameters:
        - alpha: learning rate for second-stage values, in [0,1].
        - beta: inverse temperature for both stages, in [0,10].
        - eta: transition learning rate (row-wise toward observed state), in [0,1].
        - pi: perseveration strength added to the previously chosen stage-1 action, in [0,1].
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """
    alpha, beta, eta, pi = model_parameters
    n_trials = len(action_1)

    T = np.full((2, 2), 0.5)  # rows = actions (A,U), cols = states (X,Y)

    q_stage2 = np.zeros((2, 2))  # states X,Y each with 2 aliens

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):

        max_q2 = np.max(q_stage2, axis=1)   # per state
        q1_mb = T @ max_q2                  # shape (2,)

        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += pi  # encourages repeating the last choice

        prefs1 = q1_mb + bias
        exp_q1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2 = q_stage2[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2


        a1_row = T[a1].copy()
        T[a1] = (1 - eta) * a1_row
        T[a1, s] += eta  # ensures the row sums to 1

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll