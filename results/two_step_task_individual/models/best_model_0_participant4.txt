def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Pure model-free with eligibility traces, forgetting, and reward sensitivity.

    Idea
    ----
    - No explicit transition learning or model-based planning.
    - Use eligibility traces that assign delayed credit from second-stage reward to first-stage choices.
    - Include value forgetting (decay toward zero) for all Q-values each trial.
    - Reward sensitivity scales the experienced reward before learning, dissociating it from choice stochasticity.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state visited per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward per trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, xi, phi, gamma)
        - alpha in [0,1]: learning rate for TD updates.
        - beta in [0,10]: inverse temperature for softmax policy (both stages).
        - xi in [0,1]: eligibility trace decay (higher -> longer-lasting credit).
        - phi in [0,1]: forgetting rate applied to all Q-values each trial.
        - gamma in [0,1]: reward sensitivity scaling the reward used in learning (r_eff = gamma * r).

    Notes
    -----
    - Traces e1 (size 2) and e2 (size 2x2) are decayed every trial by xi, then incremented for chosen actions.
    - A single TD error at the outcome updates both stages weighted by their traces.
    - Forgetting is applied multiplicatively to all Q-values each trial.
    """
    alpha, beta, xi, phi, gamma = model_parameters
    n_trials = len(action_1)

    eps = 1e-12

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)        # stage-1 MF action values
    q2 = np.zeros((2, 2))   # stage-2 MF action values

    e1 = np.zeros(2)
    e2 = np.zeros((2, 2))

    for t in range(n_trials):

        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        q1 *= (1.0 - phi)
        q2 *= (1.0 - phi)

        e1 *= xi
        e2 *= xi
        e1[a1] += 1.0
        e2[s, a2] += 1.0

        r_eff = gamma * reward[t]
        delta = r_eff - q2[s, a2]

        q2 += alpha * delta * e2
        q1 += alpha * delta * e1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss