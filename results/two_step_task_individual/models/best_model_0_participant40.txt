def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with eligibility trace, rare-transition misattribution, and value decay.
    
    The agent does not maintain an explicit transition model. Instead, it:
    - Learns second-stage values Q2[s, a2] from reward.
    - Updates first-stage MF values Q1_mf via an eligibility trace from the second-stage TD error.
    - Applies misattribution of credit on rare transitions: when the observed planet is rare for the
      chosen spaceship (assuming common mapping A->X, U->Y), a fraction of the credit (phi_mis)
      is assigned to the unchosen spaceship (captures state-dependent choice perseveration vs. credit assignment errors).
    - Applies value decay toward zero each trial to both Q1_mf and Q2 (captures forgetting/drift).
    
    Parameters (bounds):
    - eta_q (0-1): Learning rate for second-stage TD updates (Q2).
    - beta (0-10): Inverse temperature for softmax at both stages.
    - elig (0-1): Eligibility trace strength scaling how much second-stage TD error updates Q1_mf.
    - phi_mis (0-1): Fraction of the eligibility credit given to the unchosen first-stage action on rare transitions.
    - nu_decay (0-1): Per-trial value decay toward 0 for both Q1_mf and Q2.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_q, beta, elig, phi_mis, nu_decay]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_q, beta, elig, phi_mis, nu_decay = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        logits1 = beta * Q1_mf
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        Q1_mf *= (1.0 - nu_decay)
        Q2 *= (1.0 - nu_decay)

        delta2 = r - Q2[s, a2]
        Q2[s, a2] += eta_q * delta2


        is_common = (a1 == s)

        Q1_mf[a1] += elig * delta2

        if not is_common:
            unchosen = 1 - a1
            Q1_mf[unchosen] += phi_mis * elig * delta2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss