def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid RL with learned transition model and stage-1 stickiness.

    The agent learns second-stage (alien) values model-free, learns the A→planet
    and U→planet transition probabilities over time from experience, and blends
    model-based (via the learned transition model) and model-free values at
    stage 1. A stage-1 perseveration bias favors repeating the previous first-stage
    choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : list or array, length=5
        [alpha, beta, w, eta_T, stick]
        - alpha in [0,1]: learning rate for MF value updates.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight on model-based plan at stage 1 (1=fully MB).
        - eta_T in [0,1]: learning rate for updating the transition model.
        - stick in [0,1]: stage-1 perseveration strength.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w, eta_T, stick = model_parameters
    n_trials = len(action_1)

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # start with a reasonable prior but learn from data

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)                # first stage MF values
    q2_mf = np.ones((2, 2)) * 0.5      # second stage MF values (start at 0.5)

    last_a1 = None

    for t in range(n_trials):

        max_q2 = np.max(q2_mf, axis=1)        # best alien per planet
        q1_mb = T @ max_q2                    # MB values from learned transitions
        q1_base = (1 - w) * q1_mf + w * q1_mb

        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stick
        q1_eff = q1_base + bias1

        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        s = int(state[t])

        q2_eff = q2_mf[s].copy()
        q2c = q2_eff - np.max(q2_eff)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]


        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1 - eta_T) * T[a1] + eta_T * target

        T[a1] = T[a1] / np.sum(T[a1])

        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss