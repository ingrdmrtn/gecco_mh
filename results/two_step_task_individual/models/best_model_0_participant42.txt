def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid with learned transitions, uncertainty-sensitive arbitration, forgetting, and novelty bonus.
    Computes the negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien within observed state).
    reward : array-like of float (typically 0 or 1)
        Trial outcomes.
    model_parameters : iterable
        [alpha, beta, omega, eta, phi]
        - alpha in [0,1]: Learning rate for both second-stage values and transition probabilities.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - omega in [0,1]: Base weight on model-based values at stage 1.
        - eta in [0,1]: Forgetting/decay of second-stage Q-values per trial (higher = faster decay).
        - phi in [0,1]: Novelty/exploration bonus magnitude for less-visited second-stage actions.

    Notes
    -----
    - Transition structure is learned online from observed transitions with rate alpha.
    - Arbitration weight is reduced by transition uncertainty (entropy): when transitions are
      unpredictable, the model relies more on model-free values.
    - A novelty bonus phi/sqrt(count+1) is added to second-stage Qs for action selection.
    - Forgetting applies to all second-stage values each trial before the update.
    """
    alpha, beta, omega, eta, phi = model_parameters
    n_trials = len(action_1)

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))  # directly learned second-stage Qs

    counts = np.ones((2, 2))  # start at 1 to avoid division by zero

    for t in range(n_trials):

        q_stage2 *= (1.0 - eta)

        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = T @ max_q2


        with np.errstate(divide='ignore', invalid='ignore'):
            ent = -np.sum(T * np.log(np.clip(T, 1e-12, 1.0)), axis=1) / np.log(2.0)
        ent = np.clip(ent, 0.0, 1.0)

        w_eff = omega * (1.0 - ent)

        q1 = w_eff * q1_mb + (1.0 - w_eff) * q_stage1_mf

        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        bonus = phi / np.sqrt(counts[s] + 0.0)
        q2_aug = q_stage2[s] + bonus

        exp_q2 = np.exp(beta * (q2_aug - np.max(q2_aug)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2
        counts[s, a2] += 1.0

        delta1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        one_hot = np.array([1.0 if ss == s else 0.0 for ss in range(2)])
        T[a1] = (1.0 - alpha) * T[a1] + alpha * one_hot

        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] = T[a1] / np.sum(T[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll