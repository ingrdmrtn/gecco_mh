def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based control with cross-state generalization and value decay.
    
    This model assumes that the two aliens in state X correspond feature-wise to the two aliens
    in state Y (action index 0 corresponds across states; index 1 corresponds across states).
    Learning for a chosen alien generalizes to its counterpart in the other state with strength g.
    All second-stage Q-values also decay toward zero each trial with rate d, capturing forgetting.
    Stage 1 uses an MB planner over the decaying, generalized second-stage values.
    
    Parameters (model_parameters):
    - alpha: [0,1]
        Learning rate for second-stage Q-value updates.
    - beta: [0,10]
        Inverse temperature for softmax choices at both stages.
    - g: [0,1]
        Cross-state generalization strength applied to the corresponding alien in the other state.
    - d: [0,1]
        Per-trial decay rate applied to all second-stage Q-values (forgetting).
    
    Inputs:
    - action_1: array-like of int {0,1}
    - state: array-like of int {0,1}
    - action_2: array-like of int {0,1}
    - reward: array-like of float {0,1}
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, g, d = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2 = np.zeros((2, 2))

    for t in range(n_trials):

        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        logits1 = q_stage1_mb.copy()
        logits1 -= np.max(logits1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        logits2 = q_stage2[s].copy()
        logits2 -= np.max(logits2)
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        q_stage2 *= (1.0 - d)

        r = reward[t]
        q_old = q_stage2[s, a2]
        delta2 = r - q_old

        q_stage2[s, a2] += alpha * delta2

        other_s = 1 - s
        q_stage2[other_s, a2] += alpha * g * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll