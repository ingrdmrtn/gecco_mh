def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free two-stage TD with eligibility to stage 1 and counterfactual (opponent)
    learning at stage 2, plus a baseline bias at stage 1.

    This model learns both stages model-free. The obtained TD error at stage 2 is used
    to update the chosen second-stage action value, and a counterfactual/opponent update
    nudges the unchosen action in the opposite direction. Stage-1 values are updated
    by bootstrapping from stage-2 via an eligibility parameter. A first-stage baseline
    bias captures stable preference for one spaceship.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state: 0/1 are the two aliens.
    reward : array-like of float
        Obtained reward on each trial (e.g., 0 or 1).
    model_parameters : list/array of 5 floats
        [alpha, beta, elig, rho_cf, b0]
        - alpha in [0,1]: TD learning rate for both stage-1 and stage-2 updates.
        - beta in [0,10]: Inverse temperature used at both stages.
        - elig in [0,1]: Eligibility strength for backing up from stage 2 to stage 1.
        - rho_cf in [0,1]: Counterfactual/opponent learning strength applied to the
          unchosen second-stage action (fraction of the chosen-action TD error).
        - b0 in [0,1]: Baseline bias toward spaceship A vs. U; transformed to signed
          bias b = (b0 - 0.5) added to Q1 logits (positive favors A).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, elig, rho_cf, b0 = model_parameters
    n_trials = len(action_1)

    Q1 = np.zeros(2, dtype=float)       # stage-1: actions A/U
    Q2 = np.zeros((2, 2), dtype=float)  # stage-2: states X/Y x actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    bias = b0 - 0.5

    for t in range(n_trials):
        s = state[t]

        Q1_eff = Q1.copy()

        Q1_eff[0] += bias
        Q1_eff[1] -= bias
        Q1_eff = Q1_eff - np.max(Q1_eff)
        exp_q1 = np.exp(beta * Q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        Q2_eff = Q2[s, :].copy()
        Q2_eff = Q2_eff - np.max(Q2_eff)
        exp_q2 = np.exp(beta * Q2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        alt2 = 1 - a2

        Q2[s, alt2] -= alpha * rho_cf * delta2

        bootstrap = Q2[s, a2]
        delta1 = bootstrap - Q1[a1]
        Q1[a1] += alpha * elig * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll