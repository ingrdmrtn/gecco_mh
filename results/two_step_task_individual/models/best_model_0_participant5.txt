def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Uncertainty-adaptive temperature with forgetting and model-based stage-1.
    
    Summary
    -------
    - Stage 2: Model-free Q-learning with decay (forgetting).
    - Temperature adaptation: The inverse temperature is increased when second-stage policies
      are more certain (low entropy) and decreased when they are uncertain (high entropy).
      The same adaptive temperature is applied to both stages each trial.
    - Stage 1: Model-based planning using a fixed transition structure (common=0.7, rare=0.3),
      plus a perseveration bias toward repeating the previous spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcomes per trial.
    model_parameters : iterable of floats
        [lr2, beta_base, decay, temp_slope, stick1]
        - lr2 in [0,1]: learning rate for stage-2 Q-updates.
        - beta_base in [0,10]: base inverse temperature before uncertainty modulation.
        - decay in [0,1]: per-trial forgetting applied to all stage-2 Q-values.
        - temp_slope in [0,1]: strength of uncertainty-to-temperature modulation.
        - stick1 in [0,1]: perseveration bias added to the previously chosen stage-1 action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr2, beta_base, decay, temp_slope, stick1 = model_parameters
    n_trials = len(action_1)

    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])
    q2 = np.zeros((2, 2))  # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        q2 *= (1.0 - decay)


        entropies = np.zeros(2)
        for ss in range(2):
            logits_tmp = beta_base * q2[ss, :]
            logits_tmp -= np.max(logits_tmp)
            p_tmp = np.exp(logits_tmp) / np.sum(np.exp(logits_tmp))

            eps_h = 1e-12
            entropies[ss] = -np.sum(p_tmp * np.log(p_tmp + eps_h))
        H = np.mean(entropies)
        H_max = np.log(2.0)
        uncertainty_factor = 1.0 - (H / H_max)  # 0 when max uncertainty, 1 when deterministic
        beta_adapt = beta_base * (1.0 + temp_slope * uncertainty_factor)

        logits2 = beta_adapt * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        V_states = np.max(q2, axis=1)
        q1_mb = T_fixed @ V_states

        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick1

        logits1 = beta_adapt * q1_mb + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        pe2 = r - q2[s, a2]
        q2[s, a2] += lr2 * pe2

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss