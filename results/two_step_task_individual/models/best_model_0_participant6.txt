def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA with risk-sensitive utility, value decay, and second-stage stickiness.
    This model uses only model-free learning across both stages. Rewards are transformed
    by a concave/convex utility to capture risk sensitivity, values decay over time,
    and there is a within-planet perseveration bias at the second stage.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens on the planet).
    reward : array-like of float
        Outcome per trial (coins).
    model_parameters : tuple/list of 5 floats
        - alpha (0 to 1): learning rate for MF Q-value updates at both stages.
        - beta (0 to 10): inverse temperature for softmax at both stages.
        - rho (0 to 1): risk-sensitivity for utility; u(r) = sign(r) * |r|^rho.
        - decay (0 to 1): forgetting rate; Q-values shrink by (1 - decay) each trial.
        - sigma2 (0 to 1): second-stage choice stickiness (within visited planet).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, rho, decay, sigma2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)         # over spaceships
    q2 = np.zeros((2, 2))    # over aliens by planet

    prev_a2_for_state = [None, None]

    for t in range(n_trials):

        q1 *= (1.0 - decay)
        q2 *= (1.0 - decay)

        logits1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        stick2 = np.zeros(2)
        if prev_a2_for_state[s] is not None:
            stick2[prev_a2_for_state[s]] = sigma2

        logits2 = beta * (q2[s, :] - np.max(q2[s, :])) + stick2
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        u = np.sign(r) * (np.abs(r) ** rho)

        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        td1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * td1

        prev_a2_for_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll