def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Surprise-gated hybrid (fixed transition model), dual temperatures.
    
    Core idea:
    - First-stage action values are a weighted mix of model-based (MB) and model-free (MF) values.
    - MB uses the known transition structure (common=0.7) and the current max Q at stage 2.
    - MF credit assignment to stage-1 is gated by transition surprise: rare transitions down-weight MF eligibility.
    - Separate softmax temperatures for each stage.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int {0,1}
        Second-stage states (planets X=0, Y=1).
    action_2 : array-like of int {0,1}
        Second-stage choices (aliens within the observed planet).
    reward : array-like of float
        Rewards received each trial.
    model_parameters : iterable of 5 floats
        [theta_q, beta1, beta2, omega_mb, kappa_s]
        - theta_q:  [0,1] learning rate for both stage-2 values and stage-1 MF values
        - beta1:    [0,10] inverse temperature for first-stage softmax
        - beta2:    [0,10] inverse temperature for second-stage softmax
        - omega_mb: [0,1] weight of the MB controller at stage 1 (1=fully MB, 0=fully MF)
        - kappa_s:  [0,1] surprise gating strength; MF eligibility is multiplied by (1 - kappa_s) on rare transitions

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    theta_q, beta1, beta2, omega_mb, kappa_s = model_parameters
    n_trials = len(action_1)

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)        # model-free first-stage values
    q2 = np.zeros((2, 2))      # second-stage values: q2[state, action]

    for t in range(n_trials):
        s = state[t]

        logits2 = beta2 * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        max_q2 = np.max(q2, axis=1)        # max over actions per state
        q1_mb = T @ max_q2                 # MB action values for stage 1

        q1 = omega_mb * q1_mb + (1.0 - omega_mb) * q1_mf

        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        r = reward[t]

        delta2 = r - q2[s, a2]
        q2[s, a2] += theta_q * delta2


        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        surprise = 0.0 if is_common else 1.0

        elig = 1.0 - kappa_s * surprise
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += theta_q * elig * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll