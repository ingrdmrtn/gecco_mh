def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Uncertainty-weighted hybrid control with learned transitions and stage-1 stickiness.

    Mechanism
    - Learn a per-action transition model T (rows sum to 1) using a delta rule.
    - Learn second-stage Q-values with model-free RL.
    - Learn first-stage model-free Q-values via TD(0) bootstrapping from second-stage Q.
    - Arbitration at stage 1 is action-specific: each action's MB weight is proportional
      to the certainty (1 - entropy) of its learned transition row, modulated by mb_gate.
    - Stage 1 includes a stickiness bias toward repeating the last first-stage action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 within the reached state).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha_v, beta, trans_alpha, mb_gate, kappa_s1]
        - alpha_v in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for both stages.
        - trans_alpha in [0,1]: transition learning rate for T rows.
        - mb_gate in [0,1]: how strongly certainty boosts model-based control at stage 1.
        - kappa_s1 in [0,1]: stickiness toward the previous stage-1 action.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_v, beta, trans_alpha, mb_gate, kappa_s1 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    q1_mf = np.zeros(2)        # model-free first-stage
    q2 = np.zeros((2, 2))      # second-stage

    prev_a1 = -1

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        v2 = np.max(q2, axis=1)         # value of each second-stage state
        q1_mb = T @ v2                  # model-based value per first-stage action


        row_ent = np.zeros(2)
        for a in range(2):
            p_row = T[a]

            p_safe = np.clip(p_row, 1e-12, 1.0)
            H = -(p_safe[0] * np.log2(p_safe[0]) + p_safe[1] * np.log2(p_safe[1]))
            row_ent[a] = H  # in [0,1] for binary distribution
        certainty = 1.0 - row_ent               # in [0,1]

        w = mb_gate * certainty + (1.0 - mb_gate) * 0.5

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        pref1 = q1.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += kappa_s1

        pref1 -= np.max(pref1)
        pr1 = np.exp(beta * pref1)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        pref2 = q2[s, :].copy()
        pref2 -= np.max(pref2)
        pr2 = np.exp(beta * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]


        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_v * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_v * pe1

        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T[a1, :] = (1.0 - trans_alpha) * T[a1, :] + trans_alpha * target_row
        T[a1, :] /= (np.sum(T[a1, :]) + 1e-12)

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll