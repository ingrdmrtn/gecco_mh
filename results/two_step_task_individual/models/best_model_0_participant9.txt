def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free learner with eligibility trace (λ) and separate inverse temperatures.
    
    Uses:
    - Model-free values at both stages (Q-learning).
    - Model-based plan at stage 1 by projecting stage-2 values through the transition structure.
    - Hybrid arbitration between MF and MB at stage 1 via weight w.
    - Eligibility trace λ to propagate outcome back to stage-1 MF values.
    - Separate softmax temperatures for stage 1 and stage 2.

    Parameters (with bounds):
    - alpha in [0, 1]: learning rate for value updates.
    - beta1 in [0, 10]: inverse temperature for the first-stage softmax.
    - beta2 in [0, 10]: inverse temperature for the second-stage softmax.
    - w in [0, 1]: weight blending model-based and model-free values at stage 1.
    - lam in [0, 1]: eligibility trace controlling how much the reward directly updates stage-1 MF values.

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien on the reached planet (two per planet).
    - reward: array-like of floats (typically 0 or 1), coins received.
    - model_parameters: list/array [alpha, beta1, beta2, w, lam].

    Returns:
    - Negative log-likelihood of observed choices (float).
    """
    alpha, beta1, beta2, w, lam = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)               # for spaceships A,U
    q2_mf = np.zeros((2, 2))          # for aliens on planet X (0) and Y (1)

    for t in range(n_trials):

        max_q2 = np.max(q2_mf, axis=1)                  # best alien on each planet
        q1_mb = transition_matrix @ max_q2              # expected value for each spaceship

        q1 = w * q1_mb + (1 - w) * q1_mf

        q1_stable = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2 = q2_mf[s]
        q2_stable = q2 - np.max(q2)
        exp_q2 = np.exp(beta2 * q2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        q2_old = q2_mf[s, a2]
        q1_old = q1_mf[a1]

        delta2 = r - q2_old
        q2_mf[s, a2] += alpha * delta2




        delta1_bootstrap = q2_old - q1_old
        delta1_outcome = r - q1_old
        q1_mf[a1] += alpha * ((1 - lam) * delta1_bootstrap + lam * delta1_outcome)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss