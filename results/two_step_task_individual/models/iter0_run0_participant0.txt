def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free learner with eligibility trace.
    
    This model blends a model-based (MB) planner that knows the transition
    structure with a model-free (MF) learner at the first stage, and uses a
    model-free learner at the second stage. An eligibility trace (lambda)
    propagates reward prediction errors from the second stage back to the
    first-stage MF values. Decision policies at each stage are softmax with
    their own inverse temperatures.
    
    Parameters
    ----------
    action_1 : array-like of int, shape (n_trials,)
        First-stage choices (0=A, 1=U).
    state : array-like of int, shape (n_trials,)
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int, shape (n_trials,)
        Second-stage choices (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float/int, shape (n_trials,)
        Obtained reward on each trial (e.g., 0/1 coins).
    model_parameters : iterable of floats, length 5
        [alpha, beta1, beta2, w, lam]
        - alpha in [0,1]: learning rate for MF updates (both stages).
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2.
        - w in [0,1]: weight on model-based values at stage 1 (1=fully MB).
        - lam in [0,1]: eligibility trace; how much stage-2 RPE updates stage-1 MF.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta1, beta2, w, lam = model_parameters
    n_trials = len(action_1)

    # Known transition structure: A->X is common, U->Y is common
    transition_matrix = np.array([[0.7, 0.3],  # A to [X,Y]
                                  [0.3, 0.7]]) # U to [X,Y]

    # Choice likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)        # actions A/U
    q_stage2_mf = np.zeros((2, 2))   # states X/Y x actions (W/S or P/H)

    for t in range(n_trials):
        # Model-based action values at stage 1 from current MF estimates of stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)            # value of each planet
        q_stage1_mb = transition_matrix @ max_q_stage2         # expected value by spaceship

        # Hybrid stage-1 values
        q1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Stage-1 policy and likelihood
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta1 * q1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy and likelihood
        s = state[t]
        q2 = q_stage2_mf[s]
        q2_centered = q2 - np.max(q2)
        probs_2 = np.exp(beta2 * q2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Rewards and TD errors
        r = reward[t]
        # Stage-2 MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace:
        # direct update toward the value of the executed second-stage action
        delta1_direct = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # eligibility component further nudges by the immediate reward RPE
        q_stage1_mf[a1] += alpha * (delta1_direct + lam * delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Perseverative model-free SARSA(λ) with separate learning rates.
    
    A purely model-free controller updates first-stage values via SARSA(λ),
    using an eligibility trace to propagate reward prediction errors from
    stage 2 back to stage 1. A choice perseveration bias adds a bonus to
    repeating the previous action at each stage. A single softmax temperature
    governs both stages.
    
    Parameters
    ----------
    action_1 : array-like of int, shape (n_trials,)
        First-stage choices (0=A, 1=U).
    state : array-like of int, shape (n_trials,)
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int, shape (n_trials,)
        Second-stage choices (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float/int, shape (n_trials,)
        Obtained reward on each trial (e.g., 0/1 coins).
    model_parameters : iterable of floats, length 5
        [alpha1, alpha2, beta, lam, pers]
        - alpha1 in [0,1]: learning rate for stage-1 MF values.
        - alpha2 in [0,1]: learning rate for stage-2 MF values.
        - beta in [0,10]: inverse temperature used at both stages.
        - lam in [0,1]: eligibility trace; backprop from stage-2 RPE to stage-1.
        - pers in [0,1]: perseveration strength added to previously chosen action.
                         Implemented as additive bias to the chosen action's preference.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha1, alpha2, beta, lam, pers = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1 = np.zeros(2)       # A/U
    q_stage2 = np.zeros((2, 2))  # X/Y x actions

    # Perseveration memory (previous action per stage, None at start)
    prev_a1 = None
    prev_a2 = [None, None]  # one for each state

    for t in range(n_trials):
        s = state[t]

        # Stage-1 preferences with perseveration
        pref1 = q_stage1.copy()
        if prev_a1 is not None:
            pref1[prev_a1] += pers

        pref1_centered = pref1 - np.max(pref1)
        probs_1 = np.exp(beta * pref1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 preferences with state-specific perseveration
        pref2 = q_stage2[s].copy()
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += pers

        pref2_centered = pref2 - np.max(pref2)
        probs_2 = np.exp(beta * pref2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # Stage-2 update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Stage-1 update with SARSA(λ): move toward the chosen stage-2 action's value
        delta1_direct = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += alpha1 * (delta1_direct + lam * delta2)

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Hybrid model with learned transitions and value forgetting.
    
    This model learns the first-stage transition probabilities online and
    combines a learned model-based planner with model-free values at stage 1.
    Second-stage values are subject to forgetting (decay toward zero) on each
    trial, capturing volatility and limited memory. A single softmax temperature
    is used at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int, shape (n_trials,)
        First-stage choices (0=A, 1=U).
    state : array-like of int, shape (n_trials,)
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int, shape (n_trials,)
        Second-stage choices (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float/int, shape (n_trials,)
        Obtained reward on each trial (e.g., 0/1 coins).
    model_parameters : iterable of floats, length 5
        [alpha_r, alpha_t, beta, w, rho]
        - alpha_r in [0,1]: reward learning rate for MF value updates.
        - alpha_t in [0,1]: transition learning rate (toward observed next state).
        - beta in [0,10]: inverse temperature used at both stages.
        - w in [0,1]: weight on model-based value at stage 1 (1=fully MB).
        - rho in [0,1]: forgetting rate applied to all stage-2 Q-values each trial.
                        Q <- (1 - rho) * Q before incorporating new outcome.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_r, alpha_t, beta, w, rho = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)        # A/U
    q_stage2_mf = np.zeros((2, 2))   # X/Y x actions

    # Learned transition model P(s | a), rows sum to 1. Initialize agnostically.
    trans = np.full((2, 2), 0.5)     # A: [X,Y], U: [X,Y]

    for t in range(n_trials):
        # Apply forgetting to all second-stage values
        q_stage2_mf *= (1.0 - rho)

        # Compute model-based value from learned transitions
        max_q2 = np.max(q_stage2_mf, axis=1)   # value of each planet
        q1_mb = trans @ max_q2                 # expected value by spaceship

        # Hybrid first-stage values
        q1 = w * q1_mb + (1 - w) * q_stage1_mf

        # Stage-1 policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2 = q_stage2_mf[s]
        q2_centered = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # Update second-stage MF values with reward
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * delta2

        # Update first-stage MF toward the value of the executed stage-2 action
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Learn transitions for the chosen first-stage action toward the observed state
        # One-hot target distribution for observed next state
        target = np.array([1.0 if i == s else 0.0 for i in range(2)])
        trans[a1] = trans[a1] + alpha_t * (target - trans[a1])
        # Ensure numerical stability (row sums remain ~1 and probabilities in [0,1])
        # The above update preserves row sums, but clip tiny negatives due to eps
        trans[a1] = np.clip(trans[a1], 1e-8, 1.0)
        trans[a1] = trans[a1] / np.sum(trans[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll