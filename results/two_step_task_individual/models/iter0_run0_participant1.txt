def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with eligibility trace and choice-stickiness at both stages.
    
    Computes the negative log-likelihood of observed two-step choices under a model that:
    - Mixes model-based (MB) and model-free (MF) values at stage 1
    - Uses an eligibility trace from stage 2 reward to update stage 1 MF values
    - Adds choice stickiness biases at both stages
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, w, lam, kappa]
        - alpha (learning rate, [0,1]): MF learning rate for both stages.
        - beta (inverse temperature, [0,10]): softmax sensitivity for both stages.
        - w (MB weight, [0,1]): weight on MB values in stage-1 choice (1 = purely MB).
        - lam (eligibility trace, [0,1]): how much stage-2 reward TD error updates stage-1 MF.
        - kappa (stickiness, [0,1]): strength of repeating the last chosen action at both stages.
          Implemented as an additive bias to the chosen action’s logit; scaled by kappa.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)          # stage-1 MF Q-values for A/U
    q2_mf = np.zeros((2, 2))     # stage-2 MF Q-values for planets X/Y and aliens

    # Stickiness traces: last chosen actions
    last_a1 = None
    last_a2 = np.array([None, None], dtype=object)  # one per state

    for t in range(n_trials):
        s2 = state[t]       # 0 or 1
        a1 = action_1[t]    # 0 or 1
        a2 = action_2[t]    # 0 or 1
        r = reward[t]

        # Model-based stage-1 action values from current MF stage-2 values
        max_q2 = np.max(q2_mf, axis=1)  # value of best alien on each planet
        q1_mb = transition_matrix @ max_q2

        # Combine MB and MF for stage-1 policy; add stickiness bias
        q1_comb = w * q1_mb + (1.0 - w) * q1_mf

        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa
        logits1 = beta * q1_comb + bias1
        logits1 -= np.max(logits1)  # stabilize softmax
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within observed state; add per-state stickiness
        bias2 = np.zeros(2)
        if last_a2[s2] is not None:
            bias2[last_a2[s2]] += kappa
        logits2 = beta * q2_mf[s2] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: TD at stage 2
        delta2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * delta2

        # Learning: stage-1 MF update with eligibility trace and bootstrapping towards Q2
        # Bootstrapped target from the chosen stage-2 action
        target1 = q2_mf[s2, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1 + alpha * lam * delta2

        # Update stickiness memories
        last_a1 = a1
        last_a2[s2] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Learned-transitions Model-Based planner with risk sensitivity and perseveration.
    
    This model learns the environment’s transition probabilities online and plans
    using those estimates. Second-stage values incorporate a risk penalty based on
    estimated Bernoulli outcome variance. A perseveration bias encourages repeating
    the previous stage-1 choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, alphaT, rho, phi]
        - alpha (reward learning rate, [0,1]): for updating second-stage reward expectations.
        - beta (inverse temperature, [0,10]): softmax for both stages.
        - alphaT (transition learning rate, [0,1]): for updating P(state | action_1).
        - rho (risk sensitivity, [0,1]): subtracts rho * variance from second-stage Q.
          Variance is p*(1-p), where p is the learned reward probability.
        - phi (perseveration, [0,1]): additive bias to repeat the last stage-1 action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, alphaT, rho, phi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T[a1, s2] (rows sum to 1)
    # Start with weak prior near the canonical structure, but still learnable.
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Second-stage expected rewards and their Bernoulli means for variance estimate
    q2 = np.zeros((2, 2))  # expected value
    p_hat = np.full((2, 2), 0.5)  # estimated reward probability (for variance)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Risk-adjusted second-stage values: Q = mean - rho * variance
        var = p_hat * (1.0 - p_hat)
        q2_risk = q2 - rho * var

        # Model-based stage-1 values via current transition estimates
        max_q2 = np.max(q2_risk, axis=1)
        q1 = T @ max_q2

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += phi

        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (risk-adjusted)
        logits2 = beta * q2_risk[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Update transitions T using delta rule on the chosen row
        # Move T[a1] toward the one-hot vector of the observed state
        oh = np.array([0.0, 0.0])
        oh[s2] = 1.0
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh

        # Ensure row sums remain 1 and non-negative
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        # Reward learning at stage 2: update both mean value and probability estimate
        delta = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta
        p_hat[s2, a2] += alpha * (r - p_hat[s2, a2])

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Asymmetric MF SARSA(λ) with transition-sensitive credit assignment.
    
    Purely model-free learner with:
    - Separate learning rates for positive vs. negative outcomes
    - Eligibility trace from stage 2 to stage 1
    - Transition-based credit assignment: eligibility impact is attenuated or flipped
      on rare transitions via parameter zeta.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha_pos, alpha_neg, beta, lam, zeta]
        - alpha_pos (pos learning rate, [0,1]): used when reward >= current value (positive TD error).
        - alpha_neg (neg learning rate, [0,1]): used when reward < current value (negative TD error).
        - beta (inverse temperature, [0,10]): softmax sensitivity for both stages.
        - lam (eligibility trace, [0,1]): scales how much stage-2 TD error updates stage-1 MF.
        - zeta (transition sensitivity, [0,1]): modulates eligibility on rare transitions.
          Rare transitions reduce or invert credit: effective factor = (1 - 2*rare) * zeta + (1 - zeta),
          so common => factor = 1; rare => factor = 1 - 2*zeta (ranges from +1 to -1).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha_pos, alpha_neg, beta, lam, zeta = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure for rarity detection
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # MF Q-values
    q1 = np.zeros(2)        # stage-1 MF values
    q2 = np.zeros((2, 2))   # stage-2 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Policies
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # TD at stage 2 with asymmetric learning rate
        delta2 = r - q2[s2, a2]
        alpha2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s2, a2] += alpha2 * delta2

        # Bootstrapped target for stage 1 is Q2(s2,a2) after update
        target1 = q2[s2, a2]
        delta1 = target1 - q1[a1]

        # Determine whether the transition was rare
        # Rare if observed state is the less-probable one for chosen a1 under the fixed structure
        common_state = 0 if a1 == 0 else 1  # A commonly->X(0), U commonly->Y(1)
        is_rare = 1 if s2 != common_state else 0

        # Transition-sensitive eligibility factor:
        # common => factor = 1
        # rare   => factor = 1 - 2*zeta (in [1, -1])
        elig_factor = 1.0 - 2.0 * zeta * is_rare

        # Stage-1 update combines bootstrapping and eligibility-scaled stage-2 TD error
        alpha1 = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1[a1] += alpha1 * delta1 + lam * elig_factor * alpha2 * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll