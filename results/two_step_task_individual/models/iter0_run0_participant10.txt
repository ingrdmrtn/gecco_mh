def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and first-stage perseveration.
    
    This model blends model-free SARSA(λ) values with model-based planning at the first stage.
    Second stage is model-free. A perseveration bias encourages repeating the previous spaceship.
    
    Parameters (all used):
    - alpha: stage-2 reward learning rate in [0,1]
    - lam: eligibility trace parameter in [0,1] controlling how reward propagates back to stage 1
    - w: weight of model-based value at stage 1 in [0,1]; (1-w) weights model-free value
    - beta: inverse temperature (softmax) in [0,10]
    - kappa: first-stage perseveration strength in [0,1], added to last-chosen action's preference
    
    Inputs:
    - action_1: array of shape (n_trials,), 0/1 for spaceship A/U
    - state: array of shape (n_trials,), 0/1 for planet X/Y observed after transition
    - action_2: array of shape (n_trials,), 0/1 for alien within the visited planet
    - reward: array of shape (n_trials,), typically 0/1
    - model_parameters: iterable of [alpha, lam, w, beta, kappa]
    
    Returns:
    - Negative log-likelihood of the observed choices across both stages.
    """
    alpha, lam, w, beta, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X, U->Y common (0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])  # rows: action1 (A,U), cols: state (X,Y)

    # Probabilities of chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)          # model-free values at stage 1 (A,U)
    q_stage2 = np.zeros((2, 2))        # state-action values at stage 2: Q2[state, action]

    # Perseveration memory (last chosen spaceship)
    last_a1 = None

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute model-based first-stage values via one-step lookahead: T @ max_a Q2
        v_state = np.max(q_stage2, axis=1)  # best alien per planet
        q_stage1_mb = transition_matrix @ v_state  # value of choosing spaceship

        # Add perseveration bias to first-stage preferences
        pref1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        if last_a1 is not None:
            bias = np.zeros(2)
            bias[last_a1] = kappa
            pref1 = pref1 + bias

        # First-stage choice probability (softmax)
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs_1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice probability (softmax over Q2 at observed state)
        q2_pref = q_stage2[s].copy()
        exp2 = np.exp(beta * (q2_pref - np.max(q2_pref)))
        probs_2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs_2[a2]

        # -------- Learning --------
        # TD for stage 1 towards second-stage value before reward (SARSA(0) component)
        td1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * td1

        # TD for stage 2 from reward
        td2 = reward[t] - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * td2

        # Eligibility trace back to stage 1 from reward (SARSA(λ))
        q_stage1_mf[a1] += alpha * lam * td2

        last_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA(λ) with dual learning rates and choice kernels at both stages.
    
    This model learns action values purely from experienced transitions and reward, without
    model-based planning. It includes decaying choice kernels (perseveration) at both stages.
    
    Parameters (all used):
    - alpha1: stage-1 learning rate in [0,1]
    - alpha2: stage-2 learning rate in [0,1]
    - lam: eligibility trace parameter in [0,1] for propagating reward to stage 1
    - beta: inverse temperature (softmax) in [0,10]
    - rho: choice kernel learning/decay rate in [0,1]; higher -> stronger, slower-decaying bias
    
    Inputs:
    - action_1: array of shape (n_trials,), 0/1 for spaceship
    - state: array of shape (n_trials,), 0/1 for planet
    - action_2: array of shape (n_trials,), 0/1 for alien
    - reward: array of shape (n_trials,), typically 0/1
    - model_parameters: iterable of [alpha1, alpha2, lam, beta, rho]
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha1, alpha2, lam, beta, rho = model_parameters
    n_trials = len(action_1)

    # Probabilities of chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q1 = np.zeros(2)         # stage-1 Q-values for (A,U)
    q2 = np.zeros((2, 2))    # stage-2 Q-values for (state, alien)

    # Choice kernels (perseveration biases)
    k1 = np.zeros(2)         # for stage-1 actions
    k2 = np.zeros((2, 2))    # for stage-2 actions, per state

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Apply decayed choice kernels
        k1 = (1.0 - rho) * k1
        k2[s] = (1.0 - rho) * k2[s]

        # First-stage policy: softmax over Q1 + kernel
        pref1 = q1 + k1
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs_1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy: softmax over Q2[state] + kernel
        pref2 = q2[s] + k2[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs_2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Update choice kernels with chosen actions
        k1[a1] += rho
        k2[s, a2] += rho

        # -------- Learning --------
        # Stage-1 TD towards second-stage value (before reward)
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha1 * td1

        # Stage-2 TD from reward
        td2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha2 * td2

        # Eligibility trace: propagate reward TD back to stage 1
        q1[a1] += alpha1 * lam * td2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-based with learned transition probabilities and uncertainty bonus.
    
    The agent learns both transition probabilities and second-stage reward values.
    First-stage choices are planned using the learned transition model and an
    optimism/uncertainty bonus derived from visit counts. Second-stage choices also
    include the same bonus to encourage exploration of less-visited aliens.
    
    Parameters (all used):
    - alpha_r: reward learning rate for second-stage Q-values in [0,1]
    - alpha_t: transition learning rate for updating T(action->state) in [0,1]
    - beta: inverse temperature (softmax) in [0,10]
    - bonus: exploration bonus strength added as bonus / sqrt(N) in [0,1]
    - kappa: first-stage perseveration strength in [0,1], added to last-chosen action
    
    Inputs:
    - action_1: array of shape (n_trials,), 0/1 for spaceship
    - state: array of shape (n_trials,), 0/1 for planet reached
    - action_2: array of shape (n_trials,), 0/1 for alien
    - reward: array of shape (n_trials,), typically 0/1
    - model_parameters: iterable of [alpha_r, alpha_t, beta, bonus, kappa]
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, alpha_t, beta, bonus, kappa = model_parameters
    n_trials = len(action_1)

    # Initialize transition model close to common/rare but let it learn
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: action1, cols: state

    # Second-stage values and visit counts (for uncertainty bonus)
    Q2 = np.zeros((2, 2))
    N = np.zeros((2, 2))  # visit counts per (state, action2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute bonus-adjusted state values for planning
        bonus_vec = np.zeros_like(Q2)
        # Avoid division by zero; add small constant in denominator
        bonus_vec = bonus / (np.sqrt(N + 1e-8))
        Vmb = np.max(Q2 + bonus_vec, axis=1)  # optimistic value per state

        # First-stage preferences: expected value under learned transitions + perseveration
        pref1 = T @ Vmb
        if last_a1 is not None:
            bias = np.zeros(2)
            bias[last_a1] = kappa
            pref1 = pref1 + bias

        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs_1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy: softmax over Q2[state] plus bonus
        pref2 = Q2[s] + bonus_vec[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs_2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs_2[a2]

        # -------- Learning --------
        # Update transition model T[action1] towards observed state
        # One-step exponential moving average for the categorical transition row.
        # For chosen action a1, increase probability of observed state s and decrease the other.
        for st in (0, 1):
            target = 1.0 if st == s else 0.0
            T[a1, st] = (1.0 - alpha_t) * T[a1, st] + alpha_t * target
        # Normalize to ensure row sums to 1 (numerically)
        row_sum = T[a1, 0] + T[a1, 1] + eps
        T[a1, 0] /= row_sum
        T[a1, 1] /= row_sum

        # Update second-stage value and counts
        N[s, a2] += 1.0
        td2 = reward[t] - Q2[s, a2]
        Q2[s, a2] += alpha_r * td2

        last_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss