def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free RL with eligibility trace and first-stage perseveration.
    Parameters (all used):
    - alpha: stage-2 learning rate (bounds: [0,1])
    - beta: inverse temperature for softmax at both stages (bounds: [0,10])
    - w: weight of model-based value in first-stage decision (bounds: [0,1])
    - lam: eligibility trace strength for bootstrapping reward to stage 1 (bounds: [0,1])
    - kappa: first-stage perseveration (stickiness) bias for repeating last spaceship (bounds: [0,1])
    Inputs:
    - action_1: array of first-stage choices (0 or 1)
    - state: array of second-stage states (0 or 1)
    - action_2: array of second-stage choices (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: list/tuple [alpha, beta, w, lam, kappa]
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # rows: action1 (A=0,U=1); cols: state (X=0,Y=1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)           # for actions at stage 1
    q_stage2 = np.zeros((2, 2))         # for actions at stage 2: Q[s, a2]

    # Perseveration memory for stage 1
    last_a1 = -1  # no previous choice initially

    for t in range(n_trials):
        # Model-based first-stage action values from transition and current second-stage values
        max_q_stage2 = np.max(q_stage2, axis=1)  # value of best alien at each planet
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value for each spaceship

        # Combine MF and MB, add stickiness bias for repeating last choice
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        if last_a1 in (0, 1):
            stick_vec = np.zeros(2)
            stick_vec[last_a1] = kappa
            q1_combined = q1_combined + stick_vec

        # First-stage policy
        pref1 = beta * (q1_combined - np.max(q1_combined))
        exp_q1 = np.exp(pref1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy at realized state
        s = state[t]
        q2_s = q_stage2[s].copy()
        # No stickiness at stage 2 in this model
        pref2 = beta * (q2_s - np.max(q2_s))
        exp_q2 = np.exp(pref2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: Stage 2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Learning: Stage 1 MF update with eligibility trace
        # Bootstraps via both the immediate Q2 value (standard MF) and the final reward (eligibility)
        # First TD towards Q2 value (SARSA-like)
        td1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * td1
        # Then eligibility trace to propagate reward prediction error directly
        q_stage1_mf[a1] += alpha * lam * delta2

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-learning model-based RL with value decay and first-stage choice bias.
    Parameters (all used):
    - alpha_q: stage-2 value learning rate (bounds: [0,1])
    - alpha_t: transition learning rate for P(state|action1) (bounds: [0,1])
    - beta: inverse temperature for softmax at both stages (bounds: [0,10])
    - decay: value forgetting applied each trial to second-stage Q-values (bounds: [0,1])
    - biasA: constant bias added to spaceship A (action 0) at stage 1 (bounds: [0,1])
    Inputs:
    - action_1: array of first-stage choices (0 or 1)
    - state: array of second-stage states (0 or 1)
    - action_2: array of second-stage choices (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: list/tuple [alpha_q, alpha_t, beta, decay, biasA]
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_q, alpha_t, beta, decay, biasA = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Learn transition probabilities P(state=0|a1) per spaceship; initialize near uniform
    tprob = np.array([0.5, 0.5])  # for action 0 and 1: probability of going to state 0 (X)
    # Stage-2 Q-values
    q_stage2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Value decay (forgetting/drift compensation)
        q_stage2 = (1.0 - decay) * q_stage2

        # Model-based first-stage values from learned transitions
        max_q_stage2 = np.max(q_stage2, axis=1)  # values at each state
        q1_mb = np.zeros(2)
        # Expected values using learned P(s|a)
        q1_mb[0] = tprob[0] * max_q_stage2[0] + (1.0 - tprob[0]) * max_q_stage2[1]
        q1_mb[1] = tprob[1] * max_q_stage2[0] + (1.0 - tprob[1]) * max_q_stage2[1]

        # Add constant bias favoring spaceship A (action 0)
        bias_vec = np.array([biasA, 0.0])
        pref1 = beta * (q1_mb + bias_vec - np.max(q1_mb + bias_vec))
        exp_q1 = np.exp(pref1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy at observed state
        s = state[t]
        pref2 = beta * (q_stage2[s] - np.max(q_stage2[s]))
        exp_q2 = np.exp(pref2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second-stage Q-values
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_q * delta2

        # Update learned transition probability for the chosen first-stage action
        # Binary RW update toward indicator(state==0)
        indicator_s0 = 1.0 if s == 0 else 0.0
        tprob[a1] += alpha_t * (indicator_s0 - tprob[a1])
        # Keep within [0,1] by clipping
        tprob[a1] = min(1.0, max(0.0, tprob[a1]))

    eps = 1e-12
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with intrinsic exploration bonus and asymmetric outcome learning at stage 2.
    Parameters (all used):
    - alpha_pos: stage-2 learning rate for positive prediction errors (bounds: [0,1])
    - alpha_neg: stage-2 learning rate for negative prediction errors (bounds: [0,1])
    - beta: inverse temperature for softmax at both stages (bounds: [0,10])
    - eta: weight of intrinsic entropy bonus on first-stage action values (bounds: [0,1])
    - kappa2: stage-2 perseveration (stickiness) toward repeating the last alien at each state (bounds: [0,1])
    Inputs:
    - action_1: array of first-stage choices (0 or 1)
    - state: array of second-stage states (0 or 1)
    - action_2: array of second-stage choices (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: list/tuple [alpha_pos, alpha_neg, beta, eta, kappa2]
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, eta, kappa2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values
    q_stage2 = np.zeros((2, 2))

    # Track last chosen second-stage action per state for stickiness
    last_a2 = np.array([-1, -1])

    def entropy_of_softmax(q_vec, beta_local):
        # Compute softmax entropy for a 2-action vector
        prefs = beta_local * (q_vec - np.max(q_vec))
        exps = np.exp(prefs)
        probs = exps / np.sum(exps)
        # Add small epsilon to avoid log(0)
        eps_local = 1e-12
        return -np.sum(probs * np.log(probs + eps_local))

    for t in range(n_trials):
        # Compute model-based first-stage values (expected max over aliens at each planet)
        max_q_stage2 = np.max(q_stage2, axis=1)
        q1_mb = transition_matrix @ max_q_stage2

        # Intrinsic exploration bonus: expected entropy at next state under current second-stage policy
        ent_state0 = entropy_of_softmax(q_stage2[0], beta)
        ent_state1 = entropy_of_softmax(q_stage2[1], beta)
        ent_vec = np.array([ent_state0, ent_state1])
        intrinsic_bonus = transition_matrix @ ent_vec  # expected entropy for each spaceship
        q1_augmented = q1_mb + eta * intrinsic_bonus

        # First-stage policy
        pref1 = beta * (q1_augmented - np.max(q1_augmented))
        exp_q1 = np.exp(pref1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy at realized state with stickiness toward last chosen alien
        s = state[t]
        q2_s = q_stage2[s].copy()
        if last_a2[s] in (0, 1):
            stick_vec = np.zeros(2)
            stick_vec[last_a2[s]] = kappa2
            q2_s = q2_s + stick_vec
        pref2 = beta * (q2_s - np.max(q2_s))
        exp_q2 = np.exp(pref2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 asymmetric learning
        pe = r - q_stage2[s, a2]
        alpha_use = alpha_pos if pe >= 0.0 else alpha_neg
        q_stage2[s, a2] += alpha_use * pe

        # Update last action memory for stickiness
        last_a2[s] = a2

    eps = 1e-12
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss