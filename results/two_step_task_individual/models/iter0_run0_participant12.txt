def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility and perseveration.
    
    This model blends model-based (MB) predictions based on a fixed transition
    structure with model-free (MF) values, includes an eligibility trace that
    propagates second-stage prediction errors to the first stage, and adds
    action perseveration (stickiness) at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state: 0 or 1 (aliens W/S on X; P/H on Y).
    reward : array-like of float
        Reward per trial (e.g., 0 or 1 coins).
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w, lam, kappa)
        - alpha in [0,1]: learning rate for value updates.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight on model-based vs. model-free at stage 1.
        - lam in [0,1]: eligibility trace factor scaling propagation of stage-2 TD error to stage-1 values.
        - kappa in [0,1]: perseveration strength (adds bias to repeating prior choices at both stages).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed common-rare transition structure: A->X (0.7), U->Y (0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)           # for A/U
    q_stage2_mf = np.zeros((2, 2))      # for (state X/Y) x (alien 0/1)

    # Perseveration trackers (initialize to None -> no bias on first trial)
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)  # per state

    for t in range(n_trials):
        # Model-based evaluation at stage 1: value = E_s' [max_a2 Q2_mf(s', a2)]
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # size 2 over states
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Compose hybrid first-stage action values with perseveration bias
        q1_hybrid = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = kappa
            q1_hybrid = q1_hybrid + stick

        # First-stage policy and choice likelihood
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with perseveration within the current state
        s = state[t]
        q2_state = q_stage2_mf[s].copy()
        if prev_a2[s] is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2[s]] = kappa
            q2_state = q2_state + stick2

        exp_q2 = np.exp(beta * (q2_state - np.max(q2_state)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD error and update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 TD target bootstrapped from stage 2 (SARSA(Î»)-like)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + lam * alpha * delta2

        # Update perseveration trackers
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid model with learned transitions and valence-asymmetric learning.
    
    This model learns the transition probabilities between stage-1 actions and
    stage-2 states online, combines model-based and model-free values at stage 1,
    and uses separate learning rates for positive vs. negative second-stage
    prediction errors.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state.
    reward : array-like of float
        Reward per trial (e.g., 0 or 1 coins).
    model_parameters : tuple/list of 5 floats
        (alpha_pos, alpha_neg, beta, w, eta)
        - alpha_pos in [0,1]: learning rate when TD error at stage 2 is positive.
        - alpha_neg in [0,1]: learning rate when TD error at stage 2 is negative.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight on model-based vs. model-free at stage 1.
        - eta in [0,1]: transition learning rate for updating P(state | action_1).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, w, eta = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model as uninformative (0.5 / 0.5)
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for t in range(n_trials):
        # Model-based value from learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # size 2 over states
        q_stage1_mb = T @ max_q_stage2

        # Hybrid first-stage values
        q1_hybrid = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # First-stage policy and likelihood
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy and likelihood
        s = state[t]
        q2_state = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2_state - np.max(q2_state)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        lr2 = alpha_pos if delta2 >= 0 else alpha_neg
        q_stage2_mf[s, a2] += lr2 * delta2

        # Update model-free stage-1 via bootstrapped TD from stage 2 (valence-matched lr)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr2 * delta1

        # Learn transitions for the chosen first-stage action
        # Move T[a1] toward the observed one-hot state with learning rate eta
        one_hot_s = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1, :] = (1 - eta) * T[a1, :] + eta * one_hot_s
        # Keep the other row unchanged (no assumption linking A and U directly)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with decay, risk-sensitive utility, and dual inverse temperatures.
    
    This model assumes choices are driven by model-free values only. It includes:
    - Exponential decay (forgetting) of all Q-values each trial.
    - Risk/utility curvature that transforms rewards as u = reward^rho.
    - Separate inverse temperatures for stage 1 and stage 2.
    - An eligibility-like propagation from stage 2 to stage 1 scaled by the decay parameter.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state.
    reward : array-like of float
        Reward per trial (>= 0).
    model_parameters : tuple/list of 5 floats
        (alpha, beta1, beta2, rho, d)
        - alpha in [0,1]: learning rate for value updates.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2.
        - rho in [0,1]: risk/utility curvature; rho<1 leads to diminishing sensitivity.
        - d in [0,1]: decay rate applied to Q-values each trial; also scales eligibility propagation.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, rho, d = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for t in range(n_trials):
        # Apply decay/forgetting to all Q-values before computing policy and updating
        q_stage1_mf *= (1.0 - d)
        q_stage2_mf *= (1.0 - d)

        # Stage 1 policy (pure MF)
        exp_q1 = np.exp(beta1 * (q_stage1_mf - np.max(q_stage1_mf)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (state-dependent, pure MF)
        s = state[t]
        q2_state = q_stage2_mf[s]
        exp_q2 = np.exp(beta2 * (q2_state - np.max(q2_state)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Risk-sensitive utility transform
        r = reward[t]
        u = (r ** rho) if r >= 0 else -((-r) ** rho)

        # Learning at stage 2
        delta2 = u - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Learning at stage 1: bootstrap from stage 2 + eligibility scaled by d
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + d * alpha * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss