def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and first-stage perseveration.
    Returns negative log-likelihood of observed first- and second-stage choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship at stage 1 on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Chosen alien at stage 2 on each trial (planet-specific).
    reward : array-like of float in [0,1]
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, w, lam, kappa]
        - alpha in [0,1]: learning rate for value updates (both stages).
        - beta in [0,10]: inverse temperature for both stages.
        - w in [0,1]: weight on model-based values at stage 1 (1=fully MB, 0=fully MF).
        - lam in [0,1]: eligibility trace mixing for bootstrapping with outcome.
        - kappa in [0,1]: first-stage perseveration strength; mapped to [-1,1] internally.

    Notes
    -----
    - Transition structure is fixed and known (common=0.7).
    - Stage-1 decision uses a hybrid MB/MF action value with perseveration bias.
    - Stage-2 decision uses MF action values.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Known transition matrix: rows=actions (A,U), cols=states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)          # for actions A,U
    q_stage2_mf = np.zeros((2, 2))     # for states X,Y and 2 aliens per state

    prev_a1 = None  # for perseveration
    # Map kappa from [0,1] to [-1,1] so mid-point means no bias
    kappa_centered = 2.0 * (kappa - 0.5)

    for t in range(n_trials):
        s = state[t]        # 0 or 1
        a1 = action_1[t]    # 0 or 1
        a2 = action_2[t]    # 0 or 1
        r = reward[t]

        # Model-based stage-1 values: expected best second-stage value after transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # per state
        q_stage1_mb = transition_matrix @ max_q_stage2  # shape (2,)

        # Hybrid stage-1 values
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Add perseveration bias to the previously chosen action
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] = kappa_centered
            q1 = q1 + bias

        # Stage-1 softmax policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax policy (pure MF)
        q2 = q_stage2_mf[s, :]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Learning signals
        # Stage-2 TD error
        delta2 = r - q_stage2_mf[s, a2]
        # Stage-1 TD error (bootstraps off second-stage value)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]

        # Updates
        # Stage-2 MF value update
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF value update with eligibility trace
        q_stage1_mf[a1] += alpha * delta1 + alpha * lam * delta2

        # Update perseveration memory
        prev_a1 = a1

    eps = 1e-10
    neg_loglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_loglik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based with learned transitions, stage-2 MF values with forgetting, and first-stage perseveration.
    Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship at stage 1 on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Chosen alien at stage 2 on each trial.
    reward : array-like of float in [0,1]
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_r, beta, alpha_T, forget, kappa]
        - alpha_r in [0,1]: stage-2 reward learning rate for Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_T in [0,1]: learning rate for the state-transition probabilities.
        - forget in [0,1]: per-trial forgetting toward 0 for all unnormalized Q2 values.
        - kappa in [0,1]: first-stage perseveration strength; mapped to [-1,1] internally.

    Notes
    -----
    - Transition matrix is learned separately for each first-stage action and updated each trial.
    - Stage-1 policy uses model-based action values computed from learned transitions and current Q2.
    - Stage-2 policy uses softmax over Q2 for the reached state.
    - Q2 values decay multiplicatively by (1-forget) each trial before the update.
    """
    alpha_r, beta, alpha_T, forget, kappa = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix to neutral (0.5/0.5)
    transition_matrix = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2 = np.zeros((2, 2))  # MF second-stage values

    prev_a1 = None
    kappa_centered = 2.0 * (kappa - 0.5)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Forgetting on all Q2 before using them this trial
        q_stage2 *= (1.0 - forget)

        # Model-based stage-1 values from learned transitions
        max_q2 = np.max(q_stage2, axis=1)    # best alien per state
        q1_mb = transition_matrix @ max_q2   # shape (2,)

        # Add perseveration bias
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] = kappa_centered
            q1_mb = q1_mb + bias

        # Stage-1 softmax
        exp_q1 = np.exp(beta * (q1_mb - np.max(q1_mb)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax
        q2_vec = q_stage2[s, :]
        exp_q2 = np.exp(beta * (q2_vec - np.max(q2_vec)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Update transition model for the executed first-stage action
        # Move probability mass toward observed next state s
        for s_idx in range(2):
            target = 1.0 if s_idx == s else 0.0
            transition_matrix[a1, s_idx] = (1.0 - alpha_T) * transition_matrix[a1, s_idx] + alpha_T * target
        # Ensure row normalization (numerical safety)
        row_sum = np.sum(transition_matrix[a1, :])
        if row_sum > 0:
            transition_matrix[a1, :] /= row_sum

        # Update stage-2 Q-value
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * delta2

        prev_a1 = a1

    eps = 1e-10
    neg_loglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_loglik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA(λ) with asymmetric learning and perseveration at both stages.
    Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship at stage 1 on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Chosen alien at stage 2 on each trial.
    reward : array-like of float in [0,1]
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_pos, alpha_neg, beta, lam, kappa]
        - alpha_pos in [0,1]: learning rate used when reward prediction error is positive.
        - alpha_neg in [0,1]: learning rate used when reward prediction error is non-positive.
        - beta in [0,10]: inverse temperature for both stages.
        - lam in [0,1]: eligibility trace parameter for credit assignment from stage 2 to stage 1.
        - kappa in [0,1]: perseveration strength at both stages; mapped to [-1,1] internally.

    Notes
    -----
    - No model-based planning or transition learning; decisions rely on learned Q-values.
    - Stage-1 values bootstrap off observed stage-2 action value, plus λ-weighted outcome error.
    - Perseveration biases repeat of previous actions at each stage.
    """
    alpha_pos, alpha_neg, beta, lam, kappa = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)        # MF values for A,U
    q_stage2 = np.zeros((2, 2))   # MF values per state and alien

    prev_a1 = None
    prev_a2 = [None, None]  # track previous a2 per state to apply state-specific perseveration
    kappa_centered = 2.0 * (kappa - 0.5)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 action values with perseveration bias
        q1 = q_stage1.copy()
        if prev_a1 is not None:
            q1[prev_a1] += kappa_centered

        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 action values with within-state perseveration bias
        q2_vec = q_stage2[s, :].copy()
        if prev_a2[s] is not None:
            q2_vec[prev_a2[s]] += kappa_centered

        exp_q2 = np.exp(beta * (q2_vec - np.max(q2_vec)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # TD errors
        delta2 = r - q_stage2[s, a2]
        delta1 = q_stage2[s, a2] - q_stage1[a1]

        # Choose asymmetric learning rate based on sign of delta2 (outcome)
        alpha2 = alpha_pos if delta2 > 0 else alpha_neg
        alpha1 = alpha_pos if (delta1 + lam * delta2) > 0 else alpha_neg

        # Updates
        q_stage2[s, a2] += alpha2 * delta2
        q_stage1[a1] += alpha1 * (delta1 + lam * delta2)

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    neg_loglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_loglik