def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free learner with eligibility traces and choice perseveration.
    
    The agent blends a model-based (MB) plan computed from a known transition structure
    with a model-free (MF) cached value at the first stage. Second-stage values are MF.
    An eligibility trace propagates reward back to the first stage. A perseveration bias
    favors repeating the previous action(s).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states (planets): 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 denote the two aliens available on the visited planet.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w, lam, persev)
        - alpha in [0,1]: learning rate for MF updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w in [0,1]: weight on MB values (1 = purely MB, 0 = purely MF) at stage 1.
        - lam in [0,1]: eligibility trace strength backing up reward to stage 1.
        - persev in [0,1]: choice perseveration strength added to the previously chosen action(s).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, lam, persev = model_parameters
    n_trials = len(action_1)

    # Known transitions: A->X common (0.7), U->Y common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Previous choices for perseveration
    prev_a1 = None
    prev_a2 = None

    for t in range(n_trials):
        # Model-based plan at stage 1: transition @ best second-stage MF values
        max_q_stage2_mf = np.max(q_stage2_mf, axis=1)  # size 2 (per planet)
        q_stage1_mb = transition_matrix @ max_q_stage2_mf  # size 2 (per spaceship)

        # Combine MB and MF at stage 1
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Add perseveration bias (feature is 1 for previous action)
        if prev_a1 is not None:
            bias_vec = np.zeros(2)
            bias_vec[prev_a1] = persev
            q1 = q1 + bias_vec

        # Softmax policy at stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy based on MF values (with perseveration)
        s = state[t]
        q2 = q_stage2_mf[s, :].copy()
        if prev_a2 is not None:
            bias2 = np.zeros(2)
            bias2[prev_a2] = persev
            q2 = q2 + bias2

        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # MF learning at stage 2
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # MF learning at stage 1 with eligibility trace
        # Bootstrapping toward second-stage MF value
        q_stage1_mf[a1] += alpha * (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        # Eligibility/return component to incorporate reward
        q_stage1_mf[a1] += alpha * lam * (r - q_stage2_mf[s, a2])

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with learned transition probabilities, reward learning, and forgetting.
    
    The agent learns both the second-stage reward contingencies and the first-stage
    transition matrix from experience. Planning is purely model-based at stage 1.
    Second-stage values are learned via a delta rule. A forgetting process decays
    unvisited second-stage action values toward a neutral prior. A static bias favors
    spaceship A.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states (planets): 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the visited planet.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha_r, alpha_t, beta, forget, biasA)
        - alpha_r in [0,1]: learning rate for second-stage reward values.
        - alpha_t in [0,1]: learning rate for the transition matrix rows.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - forget in [0,1]: forgetting rate pulling unvisited second-stage values toward 0.5.
        - biasA in [0,1]: additive bias for choosing spaceship A at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_t, beta, forget, biasA = model_parameters
    n_trials = len(action_1)

    # Initialize learned transitions to uniform (uncertain)
    T = np.full((2, 2), 0.5)  # rows: actions (A/U), cols: states (X/Y)
    # Second-stage MF values
    q_stage2 = np.full((2, 2), 0.5)  # start from neutral prior

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based action values at stage 1 using learned transitions and current second-stage values
        max_q2 = np.max(q_stage2, axis=1)  # per state, pick best action
        q1_mb = T @ max_q2  # expected value per first-stage action
        # Add static bias toward spaceship A (action 0)
        q1 = q1_mb.copy()
        q1[0] += biasA

        # Softmax at stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 softmax over current q_stage2 for the visited state
        s = state[t]
        q2 = q_stage2[s, :]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Reward learning at stage 2 (delta rule)
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * delta2

        # Forgetting toward neutral prior (0.5) for all unvisited second-stage pairs
        # Apply to all entries except (s, a2)
        for ss in (0, 1):
            for aa in (0, 1):
                if not (ss == s and aa == a2):
                    q_stage2[ss, aa] = (1 - forget) * q_stage2[ss, aa] + forget * 0.5

        # Learn transition probabilities for the chosen first-stage action
        # Move the chosen row toward a one-hot vector of the observed state
        for st in (0, 1):
            target = 1.0 if st == s else 0.0
            T[a1, st] += alpha_t * (target - T[a1, st])

        # (Optional) normalize row to avoid drift (kept implicit by symmetric update)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Exploration-bonus (UCB) at stage 2 with hybrid planning and MF backup to stage 1.
    
    The agent maintains Bayesian-like estimates (Beta(1,1) priors) of second-stage
    reward probabilities and augments them with an uncertainty bonus (UCB). Stage-2
    choices use these optimism-in-the-face-of-uncertainty values. Stage-1 choices
    are a hybrid of model-based planning (using the known transition matrix and
    the exploratory second-stage values) and a model-free cached value. MF values
    are updated with an eligibility trace to back up reward to stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states (planets): 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the visited planet.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w, lam, kappa)
        - alpha in [0,1]: MF learning rate (for stage-2 MF values and stage-1 backup).
        - beta in [0,10]: inverse temperature for both stages.
        - w in [0,1]: weight on model-based values at stage 1 (hybrid with MF).
        - lam in [0,1]: eligibility trace strength backing up reward to stage 1.
        - kappa in [0,1]: weight of the uncertainty bonus in UCB values at stage 2.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Known transitions
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Beta-Bernoulli sufficient statistics for each (state, action) at stage 2
    # Prior Beta(1,1): successes=1, failures=1 => s=1, f=1
    succ = np.ones((2, 2))
    fail = np.ones((2, 2))
    counts = np.zeros((2, 2))  # total observations for uncertainty scaling

    # MF values
    q_stage2_mf = np.zeros((2, 2))
    q_stage1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute UCB-like values at stage 2 from Beta posteriors
        # Posterior mean and uncertainty proxy
        mean_p = succ / (succ + fail)  # E[p]
        # Use 1/sqrt(n+2) as a simple uncertainty scale (n starts at 0 -> matches Beta(1,1))
        u_bonus = 1.0 / np.sqrt(counts + 2.0)
        q_stage2_ucb = mean_p + kappa * u_bonus

        # Model-based plan at stage 1 uses exploratory Q (q_stage2_ucb)
        max_ucb = np.max(q_stage2_ucb, axis=1)
        q_stage1_mb = transition_matrix @ max_ucb

        # Hybrid first-stage value
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Stage 1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy uses UCB values on the visited state
        s = state[t]
        q2 = q_stage2_ucb[s, :]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update Beta posterior stats and counts
        counts[s, a2] += 1.0
        if r > 0:
            succ[s, a2] += 1.0
        else:
            fail[s, a2] += 1.0

        # MF learning at stage 2
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # MF learning at stage 1 with eligibility trace using MF bootstrap and reward return
        q_stage1_mf[a1] += alpha * (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        q_stage1_mf[a1] += alpha * lam * (r - q_stage2_mf[s, a2])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss