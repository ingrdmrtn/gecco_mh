def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and action perseveration.
    
    This model blends a model-based (MB) valuation of first-stage actions with a model-free (MF)
    valuation. It uses an eligibility trace to propagate second-stage reward back to the first-stage
    MF value and adds an action perseveration bias that encourages repeating the previous action
    at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet (0/1 for the two aliens on that planet).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : list or array-like
        [alpha, beta, w, lam, kappa]
        - alpha in [0,1]: learning rate for MF updates (both stages).
        - beta in [0,10]: inverse temperature for softmax choice (both stages).
        - w in [0,1]: weight on MB valuation at the first stage (1 = fully MB).
        - lam in [0,1]: eligibility trace strength for credit assignment from stage 2 to stage 1.
        - kappa in [0,1]: perseveration strength added to the last chosen action at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities of the chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)          # first-stage MF values for [A, U]
    q2 = np.zeros((2, 2))        # second-stage MF values: states X/Y x actions (two aliens)

    # Perseveration traces (last chosen actions)
    prev_a1 = None
    prev_a2 = [None, None]       # track previous action separately for each second-stage state

    for t in range(n_trials):
        # Model-based first-stage values: expected max value at next state under transition model
        max_q2 = np.max(q2, axis=1)                       # best alien per planet
        q1_mb = transition_matrix @ max_q2                # MB value for [A, U]

        # Combine MB and MF, add perseveration bias
        logits_1 = (w * q1_mb + (1 - w) * q1_mf).copy()
        if prev_a1 is not None:
            logits_1[prev_a1] += kappa

        # Softmax policy for first-stage
        exp_q1 = np.exp(beta * (logits_1 - np.max(logits_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy on reached state
        s = state[t]
        logits_2 = q2[s].copy()
        if prev_a2[s] is not None:
            logits_2[prev_a2[s]] += kappa

        exp_q2 = np.exp(beta * (logits_2 - np.max(logits_2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update: bootstrap plus eligibility from stage 2
        # (1 - lam): standard TD towards the second-stage action value
        # lam: direct credit assignment from the stage-2 reward prediction error
        delta1_bootstrap = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * ((1 - lam) * delta1_bootstrap + lam * delta2)

        # Update perseveration traces
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid with learned transition model (adaptive model-based) and MF with separate learning rates.
    
    This model learns the transition probabilities from experience and uses them to compute a
    model-based (MB) first-stage value that is blended with a model-free (MF) first-stage value.
    Second-stage values are learned with their own learning rate. An eligibility term helps
    propagate second-stage reward to the first stage. The same beta controls choice stochasticity
    at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet (0/1 for the two aliens on that planet).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : list or array-like
        [alpha2, alpha1, beta, tau, lam]
        - alpha2 in [0,1]: learning rate for second-stage MF values.
        - alpha1 in [0,1]: learning rate for first-stage MF values.
        - beta in [0,10]: inverse temperature for softmax choice (both stages).
        - tau in [0,1]: learning rate for updating the transition matrix from observed transitions.
        - lam in [0,1]: arbitration weight for MB vs MF at stage 1 (also scales eligibility in MF update).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha2, alpha1, beta, tau, lam = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model; start uninformative (uniform)
    T = np.full((2, 2), 0.5)

    # Probabilities of the chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q1_mf = np.zeros(2)      # first-stage MF values for [A, U]
    q2 = np.zeros((2, 2))    # second-stage MF values

    for t in range(n_trials):
        # Model-based first-stage values from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Blend MB and MF using lam
        q1_blend = lam * q1_mb + (1 - lam) * q1_mf

        # First-stage policy
        exp_q1 = np.exp(beta * (q1_blend - np.max(q1_blend)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy on reached state
        s = state[t]
        exp_q2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learn transition model from observed transition (a1 -> s)
        # Row a1 moves toward one-hot on s with learning rate tau
        T[a1] = (1 - tau) * T[a1]
        T[a1, s] += tau

        # Second-stage MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # First-stage MF update: bootstrap toward the current second-stage value,
        # plus an eligibility component that uses the stage-2 TD error
        delta1_bootstrap = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha1 * (delta1_bootstrap + lam * delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free with transition-dependent choice kernels and stage-2 choice kernel.
    
    This model learns MF values at both stages and augments the first-stage policy with
    transition-dependent choice kernels (TDCK): separate kernels track the tendency to repeat
    the previous first-stage choice following common vs rare transitions. A stage-2 choice
    kernel captures perseveration at the second stage. Kernels decay geometrically.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet (0/1 for the two aliens on that planet).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : list or array-like
        [alpha, beta, eta, kappa, zeta]
        - alpha in [0,1]: learning rate for MF Q-values (both stages).
        - beta in [0,10]: inverse temperature for softmax choice (both stages).
        - eta in [0,1]: learning rate for updating choice kernels (decay = 1 - eta).
        - kappa in [0,1]: overall strength of choice-kernel biases in both stages.
        - zeta in [0,1]: relative weight of the rare-transition kernel vs the common-transition kernel
                         in the first-stage policy (0 = ignore rare kernel; 1 = equal weighting).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, eta, kappa, zeta = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure for determining common vs rare
    # A->X and U->Y are common.
    common_map = np.array([[1, 0],  # action A: common if state X (0)
                           [0, 1]]) # action U: common if state Y (1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels
    K_common = np.zeros(2)       # first-stage kernel following common transitions
    K_rare = np.zeros(2)         # first-stage kernel following rare transitions
    K2 = np.zeros((2, 2))        # second-stage kernel per state

    # Track previous first-stage action to apply kernels after we observe transition type
    prev_a1 = None
    prev_s = None  # previous state to know whether it was common/rare for kernel update

    for t in range(n_trials):
        # First-stage policy with TDCK (kernels from previous trial)
        logits_1 = q1 + kappa * (K_common + zeta * K_rare)
        exp_q1 = np.exp(beta * (logits_1 - np.max(logits_1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with state-dependent choice kernel
        s = state[t]
        logits_2 = q2[s] + kappa * K2[s]
        exp_q2 = np.exp(beta * (logits_2 - np.max(logits_2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # MF learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2
        # Eligibility-like propagation to first stage (pure MF)
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        # Update second-stage choice kernel (decay then add to chosen)
        K2[s] *= (1 - eta)
        K2[s, a2] += eta

        # Update first-stage TDCK based on the transition experienced on THIS trial.
        # Determine whether this trial's transition was common or rare for the chosen action.
        is_common = bool(common_map[a1, s])
        # Decay both kernels
        K_common *= (1 - eta)
        K_rare *= (1 - eta)
        # Add to the chosen action in the kernel corresponding to the observed transition type
        if is_common:
            K_common[a1] += eta
        else:
            K_rare[a1] += eta

        # Track history (not needed for computation beyond what's already done)
        prev_a1 = a1
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll