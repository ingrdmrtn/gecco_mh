def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free learner with eligibility trace and perseveration.
    
    Description:
    - Stage 2: Model-free Q-learning on the encountered planet-alien pair.
    - Stage 1: Hybrid action values combining model-based planning (via fixed transition model)
      and model-free stage-1 values. An eligibility trace bootstraps the stage-1 value toward
      the stage-2 TD error.
    - Perseveration: Adds a bias to repeat the last chosen action at both stages.
    
    Parameters (model_parameters):
    - alpha: learning rate for Q-values (bounds: [0,1])
    - beta: inverse temperature for softmax at both stages (bounds: [0,10])
    - w: weight on model-based component at stage 1 (0=MF only, 1=MB only) (bounds: [0,1])
    - lam: eligibility trace parameter propagating stage-2 TD error to stage-1 (bounds: [0,1])
    - rho: perseveration bias magnitude (added to last-chosen action at each stage) (bounds: [0,1])
    
    Inputs:
    - action_1: array-like of length n_trials with first-stage choices (0=A, 1=U)
    - state: array-like of length n_trials with reached planet (0=X, 1=Y)
    - action_2: array-like of length n_trials with second-stage choices (0 or 1)
    - reward: array-like of length n_trials with received reward (typically 0/1)
    
    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    alpha, beta, w, lam, rho = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])
    
    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-free Q-values
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5  # rows: states X/Y, cols: aliens
    
    # Last choices for perseveration (initialize with -1 meaning "none")
    last_a1 = -1
    last_a2_by_state = [-1, -1]
    
    for t in range(n_trials):
        # Compute model-based action values at stage 1 from current stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # for each state, best alien
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value of actions A/U
        
        # Combine MB and MF at stage 1, add perseveration bias
        q1 = w * q_stage1_mb + (1 - w) * q_stage1_mf.copy()
        if last_a1 != -1:
            q1[last_a1] += rho
        
        # Softmax for stage 1
        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_shift)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # Stage 2 policy in the reached state
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        if last_a2_by_state[s] != -1:
            q2[last_a2_by_state[s]] += rho
        q2_shift = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_shift)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        r = reward[t]
        
        # TD errors and learning
        # Stage-2 TD error
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2
        
        # Stage-1 MF update:
        # We use eligibility trace: propagate stage-2 TD error back to stage-1 chosen action.
        q_stage1_mf[a1] += alpha * lam * delta2
        
        # Update perseveration memory
        last_a1 = a1
        last_a2_by_state[s] = a2
    
    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid model with learned transition structure (transition learning).
    
    Description:
    - Learns both the transition probabilities from stage 1 actions to planets and
      the stage-2 reward values for aliens. Planning uses the learned transition
      model rather than a fixed one.
    - Stage 1 policy uses a mixture of model-based (using learned transitions)
      and model-free action values.
    
    Parameters (model_parameters):
    - alpha_q: learning rate for Q-values at stage 2 and stage-1 MF (bounds: [0,1])
    - beta: inverse temperature for softmax at both stages (bounds: [0,10])
    - w: weight on model-based component at stage 1 (bounds: [0,1])
    - alpha_t: learning rate for transition probabilities (bounds: [0,1])
    
    Inputs:
    - action_1: array-like of length n_trials with first-stage choices (0=A, 1=U)
    - state: array-like of length n_trials with reached planet (0=X, 1=Y)
    - action_2: array-like of length n_trials with second-stage choices (0 or 1)
    - reward: array-like of length n_trials with received reward (typically 0/1)
    
    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    alpha_q, beta, w, alpha_t = model_parameters
    n_trials = len(action_1)
    
    # Initialize learned transition matrix T[a, s] (rows sum to 1)
    T = np.ones((2, 2)) * 0.5
    
    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Model-free Q-values
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    for t in range(n_trials):
        # Model-based values via learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # best alien per planet
        q_stage1_mb = T @ max_q_stage2
        
        # Stage-1 value mixture
        q1 = w * q_stage1_mb + (1 - w) * q_stage1_mf
        
        # Stage-1 softmax
        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_shift)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # Stage-2 policy in reached state
        s = state[t]
        q2 = q_stage2_mf[s]
        q2_shift = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_shift)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        r = reward[t]
        
        # Learning updates
        # Transition learning (Rescorla-Wagner on one-hot next state)
        # Increase probability of observed transition, decrease the other.
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_t * (target - T[a1, sp])
        # Ensure numerical stability (optional but safe)
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, :] /= row_sum
        
        # Stage-2 Q-learning
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_q * delta2
        
        # Stage-1 MF update toward the value of the encountered stage-2 choice
        # Use the realized value of the chosen alien as target (semi-gradient TD)
        target1 = q_stage2_mf[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_q * delta1
    
    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Risk-asymmetric learning with forgetting and separate choice temperatures.
    
    Description:
    - Stage 2: Model-free Q-learning with asymmetric sensitivity to positive vs. negative
      prediction errors (loss sensitivity eta scales negative PEs).
    - Stage 1: Model-based planning using fixed transition structure, combined with
      a model-free stage-1 value component (hybrid). Separate softmax temperatures are
      used for stage 1 and stage 2.
    - Forgetting/decay: all Q-values decay toward 0.5 each trial by a fraction 'decay'.
    
    Parameters (model_parameters):
    - alpha: base learning rate for Q-values (bounds: [0,1])
    - beta1: inverse temperature for stage-1 softmax (bounds: [0,10])
    - beta2: inverse temperature for stage-2 softmax (bounds: [0,10])
    - eta: loss sensitivity for negative prediction errors at stage 2 (0=no learning from losses, 1=symmetric) (bounds: [0,1])
    - decay: forgetting rate pulling Q-values toward 0.5 each trial (bounds: [0,1])
    
    Inputs:
    - action_1: array-like of length n_trials with first-stage choices (0=A, 1=U)
    - state: array-like of length n_trials with reached planet (0=X, 1=Y)
    - action_2: array-like of length n_trials with second-stage choices (0 or 1)
    - reward: array-like of length n_trials with received reward (typically 0/1)
    
    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    alpha, beta1, beta2, eta, decay = model_parameters
    n_trials = len(action_1)
    
    # Fixed transitions
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    
    # Initialize Q-values
    q_stage1_mf = np.zeros(2) + 0.5
    q_stage2_mf = np.zeros((2, 2)) + 0.5
    
    # Mixing weight implicitly 0.5 MB / 0.5 MF by constructing q1 as average via learned values
    # To ensure all parameters are used meaningfully, we will compute stage-1 value as
    # the sum of MB and MF divided by 2 (weight fixed), focusing manipulation on temperatures,
    # asymmetric learning, and decay.
    
    for t in range(n_trials):
        # Decay Q-values toward 0.5 (forgetting)
        q_stage1_mf = (1 - decay) * q_stage1_mf + decay * 0.5
        q_stage2_mf = (1 - decay) * q_stage2_mf + decay * 0.5
        
        # Model-based values for stage 1 from current stage-2 MF values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        
        # Hybrid stage-1 values (equal mixture)
        q1 = 0.5 * q_stage1_mb + 0.5 * q_stage1_mf
        
        # Stage-1 softmax with beta1
        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1_shift)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]
        
        # Stage-2 softmax with beta2 in reached state
        s = state[t]
        q2 = q_stage2_mf[s]
        q2_shift = q2 - np.max(q2)
        exp_q2 = np.exp(beta2 * q2_shift)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]
        
        r = reward[t]
        
        # Stage-2 learning with asymmetric PE scaling
        pe2 = r - q_stage2_mf[s, a2]
        if pe2 >= 0:
            eff_pe2 = pe2
        else:
            eff_pe2 = eta * pe2  # reduced impact of negative outcomes if eta < 1
        q_stage2_mf[s, a2] += alpha * eff_pe2
        
        # Stage-1 MF update toward the updated value of the realized second-stage choice
        target1 = q_stage2_mf[s, a2]
        pe1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * pe1
    
    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll