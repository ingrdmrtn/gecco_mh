def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and action perseveration.
    
    This model blends model-free and model-based values at stage 1, updates
    with an eligibility trace from the stage-2 reward prediction error, and
    adds a perseveration bias (choice stickiness) at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship at stage 1 (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed planet (0=X, 1=Y) reached after the stage-1 transition.
    action_2 : array-like of int (0 or 1)
        Chosen alien at stage 2 on the reached planet (0 or 1) for each trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome for each trial.
    model_parameters : tuple/list of floats
        (alpha, beta, w, lam, pers)
        - alpha in [0,1]: Learning rate for value updates.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - w in [0,1]: Model-based weight in stage-1 value mixture.
        - lam in [0,1]: Eligibility trace weighing stage-2 PEâ€™s influence on stage-1 MF update.
        - pers in [0,1]: Perseveration strength added to the previously chosen action at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, lam, pers = model_parameters
    n_trials = len(action_1)

    # Fixed transition: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)          # for spaceship A/U
    q_stage2_mf = np.zeros((2, 2))     # for each planet (X/Y) and two aliens

    # Perseveration memory
    prev_a1 = -1
    prev_a2_by_state = np.array([-1, -1])

    for t in range(n_trials):
        # Model-based stage-1 values via transition + greedy stage-2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # per planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Perseveration biases
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += pers

        s = state[t]
        bias2_vec = np.zeros(2)
        if prev_a2_by_state[s] != -1:
            bias2_vec[prev_a2_by_state[s]] += pers

        # Mixture at stage 1
        q1 = (1.0 - w) * q_stage1_mf + w * q_stage1_mb + bias1
        # Policy stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy conditioned on state with bias
        q2 = q_stage2_mf[s, :] + bias2_vec
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 PE and update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 TD target and update with eligibility trace blending
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (lam * delta2 + (1.0 - lam) * delta1)

        # Update perseveration memories
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-learning model with value forgetting and reward utility bias.
    
    This model learns transition probabilities from experience, applies
    model-based planning using the learned transitions, updates model-free
    values, forgets stage-2 values toward a neutral baseline, and transforms
    rewards with a utility bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship at stage 1 (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed planet (0=X, 1=Y) reached after the stage-1 transition.
    action_2 : array-like of int (0 or 1)
        Chosen alien at stage 2 on the reached planet (0 or 1) for each trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome for each trial.
    model_parameters : tuple/list of floats
        (alpha_mf, beta, tau_trans, phi_forget, eta_reward)
        - alpha_mf in [0,1]: Learning rate for model-free Q updates.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - tau_trans in [0,1]: Transition learning rate (row-wise towards observed state).
        - phi_forget in [0,1]: Forgetting strength; Q2 decays each trial toward 0.5 baseline.
        - eta_reward in [0,1]: Reward utility weight; r' = eta*reward + (1-eta)*0.5.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_mf, beta, tau_trans, phi_forget, eta_reward = model_parameters
    n_trials = len(action_1)

    # Initialize transition matrix with uncertainty (uniform)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.full((2, 2), 0.5)  # start at neutral baseline to match forgetting anchor

    for t in range(n_trials):
        # Forgetting of stage-2 values towards 0.5 baseline (before observing current outcome)
        q_stage2_mf = (1.0 - phi_forget) * q_stage2_mf + phi_forget * 0.5

        # Model-based planning using learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T @ max_q_stage2

        # Stage-1 policy (mixture of MB and MF via simple sum)
        q1 = q_stage1_mf + q_stage1_mb
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]

        # Stage-2 policy
        q2 = q_stage2_mf[s, :]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward utility transformation
        r = reward[t]
        r_tilde = eta_reward * r + (1.0 - eta_reward) * 0.5

        # Update transitions for chosen action towards observed state
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = T[a1, :] + tau_trans * (oh - T[a1, :])

        # Model-free learning
        delta2 = r_tilde - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_mf * delta2

        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_mf * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Dynamic arbitration model with surprise- and PE-driven MB/MF weighting.
    
    The mixing weight between model-based and model-free control at stage 1
    evolves across trials as a logistic function driven by (i) the magnitude
    of the stage-2 reward prediction error (|PE2|) and (ii) transition surprise
    (rare vs. common). Eligibility trace propagates reward information to stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship at stage 1 (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed planet (0=X, 1=Y) reached after the stage-1 transition.
    action_2 : array-like of int (0 or 1)
        Chosen alien at stage 2 on the reached planet (0 or 1) for each trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome for each trial.
    model_parameters : tuple/list of floats
        (alpha, beta, w0, kappa, lam)
        - alpha in [0,1]: Learning rate for model-free Q updates.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - w0 in [0,1]: Initial model-based weight at the start of the task.
        - kappa in [0,1]: Sensitivity scaling for arbitration updates.
        - lam in [0,1]: Eligibility trace weighting stage-2 PE in stage-1 MF update.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w0, kappa, lam = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Initialize arbitration weight
    def _clip(x, low=1e-6, high=1.0 - 1e-6):
        return np.minimum(np.maximum(x, low), high)

    def _logit(p):
        p = _clip(p)
        return np.log(p / (1.0 - p))

    def _sigmoid(z):
        return 1.0 / (1.0 + np.exp(-z))

    w_prev = _clip(w0)

    for t in range(n_trials):
        # Model-based stage-1 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Stage-1 policy with current arbitration weight
        w_t = _clip(w_prev)
        q1 = (1.0 - w_t) * q_stage1_mf + w_t * q_stage1_mb
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]

        # Stage-2 policy
        q2 = q_stage2_mf[s, :]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Compute PE at stage 2 (pre-update) for arbitration signal
        r = reward[t]
        pe2 = r - q_stage2_mf[s, a2]
        pe2_mag = np.abs(pe2)

        # Determine transition surprise (rare=1, common=0)
        # A(0)->X(0) and U(1)->Y(1) are common
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        surprise = 0.0 if is_common else 1.0

        # Update arbitration weight in logit space
        z = _logit(w_prev) + kappa * (pe2_mag - surprise)
        w_next = _sigmoid(z)

        # Learning updates
        # Stage-2 update
        q_stage2_mf[s, a2] += alpha * pe2

        # Stage-1 MF update with eligibility trace
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (lam * pe2 + (1.0 - lam) * delta1)

        # Carry arbitration forward
        w_prev = _clip(w_next)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss