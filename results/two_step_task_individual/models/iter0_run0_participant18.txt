def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with eligibility trace and choice perseveration at both stages.
    
    This model blends model-based (MB) and model-free (MF) valuation at stage 1, 
    uses MF learning at stage 2, and includes an eligibility trace to pass back
    second-stage outcomes to the first-stage MF values. A perseveration (stickiness)
    bias encourages repeating the previous actions at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 correspond to the two aliens available on that planet.
    reward : array-like of float (typically 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha, beta, w, lam, kappa]
        - alpha (learning rate, [0,1]): MF learning rate.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - w (MB/MF mixing, [0,1]): Weight on MB values at stage 1 (1 = pure MB, 0 = pure MF).
        - lam (eligibility trace, [0,1]): Strength of passing back stage-2 TD error to stage-1 MF.
        - kappa (perseveration, [0,1]): Bias magnitude to repeat previous actions at both stages.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common (0.7), U->Y common (0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices per trial
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)         # MF values at stage 1 (for A,U)
    q2 = np.zeros((2, 2))       # MF values at stage 2 (for each state, two aliens)

    # Perseveration indicators (1 for last chosen action, 0 otherwise)
    pers1 = np.zeros(2)         # for stage 1 actions
    pers2 = np.zeros((2, 2))    # for stage 2 actions, state-specific

    for t in range(n_trials):
        s = state[t]        # observed planet
        a1 = action_1[t]    # chosen spaceship
        a2 = action_2[t]    # chosen alien
        r = reward[t]

        # Model-based stage-1 value: expected max Q at stage 2 under transition matrix
        max_q2 = np.max(q2, axis=1)                 # size 2: best option at each planet
        q1_mb = transition_matrix @ max_q2          # expected value for A/U

        # Combine MB and MF at stage 1 + perseveration bias
        q1_eff = w * q1_mb + (1 - w) * q1_mf + kappa * pers1

        # Softmax for stage 1
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with perseveration bias (state-specific)
        q2_eff = q2[s].copy() + kappa * pers2[s]
        exp_q2 = np.exp(beta * (q2_eff - np.max(q2_eff)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # TD updates
        # Stage 2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage 1 MF update with eligibility trace:
        # bootstrapped TD off stage-2 value plus a direct eligibility term from outcome
        delta1_boot = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1_boot + (lam * alpha) * delta2

        # Update perseveration indicators
        pers1[:] = 0.0
        pers1[a1] = 1.0
        pers2[:, :] = 0.0
        pers2[s, a2] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Kalman-filter stage-2 learning and learned transition model at stage 1.
    
    Stage 2 values are learned via a (scalar) Kalman filter per state-action, producing
    trial-by-trial uncertainty-adaptive learning rates. Stage 1 is fully model-based,
    using a learned transition model updated from observed transitions with a Dirichlet
    prior. A secondary MF learner at stage 2 is mixed into the policy.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the reached state.
    reward : array-like of float
        Obtained outcome (0/1 coins).
    model_parameters : sequence
        [beta, tau, w_mf, alpha_mf, eta]
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - tau (process noise, [0,1]): Process variance added before each KF update (controls volatility).
        - w_mf (stage-2 MF mix, [0,1]): Weight on KF vs MF Q at stage 2 policy (0=MF-only, 1=KF-only).
        - alpha_mf (MF learning rate at stage 2, [0,1]): TD rate for the auxiliary MF learner at stage 2.
        - eta (Dirichlet prior strength, [0,1]): Scales pseudocounts for the transition model.
          Prior pseudocounts per action-state are set to c0 = 1 + 9*eta.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    beta, tau, w_mf, alpha_mf, eta = model_parameters
    n_trials = len(action_1)

    # Stage 2: Kalman filter means and variances
    m2 = np.zeros((2, 2))              # posterior means for each state-action
    s2 = np.ones((2, 2)) * 1.0         # posterior variances (start with 1.0)
    obs_var = 1.0                      # observation noise variance (fixed)

    # Auxiliary MF learner at stage 2
    q2_mf = np.zeros((2, 2))

    # Learned transition model via Dirichlet counts (per action: counts to X and Y)
    c0 = 1.0 + 9.0 * eta
    trans_counts = np.ones((2, 2)) * c0  # initialize symmetric prior

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Current transition probabilities from counts
        trans_probs = trans_counts / np.sum(trans_counts, axis=1, keepdims=True)
        # Stage 2 policy values: mixture of KF mean and MF Q
        v2_state = w_mf * m2[s] + (1.0 - w_mf) * q2_mf[s]
        # Softmax for stage 2
        exp_q2 = np.exp(beta * (v2_state - np.max(v2_state)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Stage 1 model-based values use expected max over states under learned transitions
        max_v2 = np.max(w_mf * m2 + (1.0 - w_mf) * q2_mf, axis=1)  # best value at each planet
        q1_mb = trans_probs @ max_v2  # expected values for A/U
        exp_q1 = np.exp(beta * (q1_mb - np.max(q1_mb)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Update transition counts with the realized transition
        trans_counts[a1, s] += 1.0

        # Stage 2 Kalman filter update for the chosen state-action
        # Predict step
        pred_var = s2[s, a2] + tau
        # Kalman gain
        K = pred_var / (pred_var + obs_var)
        # Update mean and variance
        m2[s, a2] = m2[s, a2] + K * (r - m2[s, a2])
        s2[s, a2] = (1.0 - K) * pred_var

        # Auxiliary MF TD update at stage 2
        q2_mf[s, a2] = q2_mf[s, a2] + alpha_mf * (r - q2_mf[s, a2])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Asymmetric learning by transition type with MB/MF mixture at stage 1.
    
    This model mixes model-based and model-free values at stage 1 and uses separate
    learning rates for outcomes following common vs rare transitions at stage 2.
    A revaluation parameter sharpens the agent's belief in common transitions at
    decision time (without changing the true transition probabilities), biasing
    model-based evaluation at stage 1 toward the most likely next state.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the reached state.
    reward : array-like of float
        Obtained outcome (0/1 coins).
    model_parameters : sequence
        [alpha_c, alpha_r, beta, omega, rho]
        - alpha_c (learning rate after common transitions, [0,1]): TD rate for stage-2 update and MF1 backup when transition is common.
        - alpha_r (learning rate after rare transitions, [0,1]): TD rate for stage-2 update and MF1 backup when transition is rare.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - omega (MB/MF mixing at stage 1, [0,1]): Weight on MB values (1 = pure MB, 0 = pure MF).
        - rho (common-transition sharpening, [0,1]): Increases assumed common transition probability toward 1.0 at choice time.
          When rho=0, the agent assumes the canonical 0.7/0.3; when rho=1, it assumes 1.0/0.0.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha_c, alpha_r, beta, omega, rho = model_parameters
    n_trials = len(action_1)

    # Canonical transition matrix (A->X common, U->Y common)
    base_T = np.array([[0.7, 0.3],
                       [0.3, 0.7]])

    # MF values
    q1_mf = np.zeros(2)     # stage-1 MF values
    q2 = np.zeros((2, 2))   # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Build a "sharpened" transition matrix toward the common transition
        # For each action, move the common prob p toward 1.0 by fraction rho.
        # A's common state is X (0); U's common state is Y (1).
        T_eff = np.zeros_like(base_T)
        # For A (row 0): common to X
        p_common_A = base_T[0, 0] + rho * (1.0 - base_T[0, 0])
        T_eff[0, 0] = p_common_A
        T_eff[0, 1] = 1.0 - p_common_A
        # For U (row 1): common to Y
        p_common_U = base_T[1, 1] + rho * (1.0 - base_T[1, 1])
        T_eff[1, 1] = p_common_U
        T_eff[1, 0] = 1.0 - p_common_U

        # Model-based stage-1 values use expected max of stage-2 Q under T_eff
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_eff @ max_q2

        # Mix MB and MF for stage 1
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage 1 softmax
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax
        q2_s = q2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Determine if the observed transition was common
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        alpha = alpha_c if is_common else alpha_r

        # Stage-2 TD update with transition-type-specific learning rate
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF backup from stage-2 value (bootstrapped)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll