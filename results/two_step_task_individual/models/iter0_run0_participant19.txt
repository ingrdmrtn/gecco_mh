def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and choice stickiness.
    
    The agent blends model-based (MB) and model-free (MF) values at the first stage,
    uses MF learning at the second stage, and propagates second-stage prediction errors
    to first-stage MF via an eligibility trace. A perseveration (stickiness) bias adds
    value to the previously chosen first-stage action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage actions per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions per trial (0/1 corresponding to the two aliens available on that planet).
    reward : array-like of float
        Reward outcome per trial (e.g., -1/0/1).
    model_parameters : iterable of float
        [alpha, beta, w, lambda_, kappa]
        - alpha   in [0, 1]: learning rate for MF updates at both stages.
        - beta    in [0, 10]: inverse temperature for softmax choice at both stages.
        - w       in [0, 1]: weight on model-based values at stage 1 (1-w on model-free).
        - lambda_ in [0, 1]: eligibility trace; scales how much stage-2 PE updates stage-1 MF.
        - kappa   in [0, 1]: choice stickiness bias added to the previously chosen stage-1 action.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, lambda_, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],  # from A to (X,Y)
                                  [0.3, 0.7]]) # from U to (X,Y)

    # Probabilities of chosen actions (for NLL)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q_stage1_mf = np.zeros(2)        # MF Q at stage 1 (for A/U)
    q_stage2_mf = np.zeros((2, 2))   # MF Q at stage 2 (for X/Y x two aliens)

    # Stickiness: remember last chosen first-stage action
    last_a1 = None

    eps = 1e-10

    for t in range(n_trials):
        # Model-based component for stage 1: expected max of stage-2 MF values via transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # max over actions for each state X,Y
        q_stage1_mb = transition_matrix @ max_q_stage2  # two entries: for A and U

        # Blend MB and MF for stage-1 policy and add stickiness
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Add perseveration bias to the previously chosen action
        if last_a1 is not None:
            stick = np.zeros(2)
            stick[last_a1] = kappa
            q1_eff = q1_combined + stick
        else:
            q1_eff = q1_combined

        # Stage-1 softmax
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy given observed state
        s = state[t]
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        r = reward[t]

        # Stage-2 TD error and update (MF)
        pe2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * pe2

        # Stage-1 MF update via eligibility trace (uses stage-2 PE)
        q_stage1_mf[a1] += alpha * lambda_ * pe2

        # Also allow a direct MF SARSA-style update towards the realized second-stage value
        target1 = q_stage2_mf[s, a2]  # bootstrapped value
        pe1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (1.0 - lambda_) * pe1

        # Update stickiness memory
        last_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based with learned transitions and directed exploration via uncertainty bonus.
    
    The agent learns both second-stage rewards and first-stage transition probabilities.
    Choices incorporate a directed exploration bonus proportional to the (learned) uncertainty
    of second-stage options. The uncertainty is tracked as a running estimate of outcome
    variance per state-action, which contributes a UCB-like bonus to choice values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage actions per trial (0: A, 1: U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions per trial (0/1: two aliens on the planet).
    reward : array-like of float
        Reward outcome per trial.
    model_parameters : iterable of float
        [alpha_r, alpha_t, beta, gamma, kappa]
        - alpha_r in [0, 1]: learning rate for second-stage reward expectations and uncertainty.
        - alpha_t in [0, 1]: learning rate for first-stage transition probabilities.
        - beta    in [0, 10]: inverse temperature for softmax choices at both stages.
        - gamma   in [0, 1]: weight for uncertainty bonus (UCB) at both stages.
        - kappa   in [0, 1]: first-stage choice perseveration bias (stickiness).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_t, beta, gamma, kappa = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T[a, s] ~ P(s | a)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Second-stage reward means and running variance per state-action
    q2 = np.zeros((2, 2))
    var2 = np.ones((2, 2)) * 0.25  # start with moderate uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        # Uncertainty bonus at stage 2: b2 = gamma * sqrt(var)
        bonus2 = gamma * np.sqrt(np.clip(var2, 0.0, None))
        q2_bonus = q2 + bonus2  # for policy

        # Stage 1 MB value uses learned transitions T and the max over action values at each state
        max_q2_bonus = np.max(q2_bonus, axis=1)  # length-2 over states
        q1_mb = T @ max_q2_bonus  # two entries for actions A/U

        # Add stickiness
        if last_a1 is not None:
            stick = np.zeros(2)
            stick[last_a1] = kappa
            q1_eff = q1_mb + stick
        else:
            q1_eff = q1_mb

        # Stage-1 softmax
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax in observed state with bonus
        s = state[t]
        exp_q2 = np.exp(beta * (q2_bonus[s] - np.max(q2_bonus[s])))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update reward mean and variance (running/EMA)
        r = reward[t]
        pe2 = r - q2[s, a2]
        # Update mean
        q2[s, a2] += alpha_r * pe2
        # Update variance proxy (EMA of squared residuals relative to pre-update mean)
        var2[s, a2] = (1.0 - alpha_r) * var2[s, a2] + alpha_r * (pe2 ** 2)

        # Update transition model T using alpha_t towards the observed state
        # One-hot target for observed state given chosen action
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        # Row update with soft learning; then renormalize to be a valid probability distribution
        T[a1] = (1.0 - alpha_t) * T[a1] + alpha_t * target
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        last_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Asymmetric valence learning, confirmation-weighted eligibility, and lapse.
    
    The agent uses separate learning rates for positive vs. negative outcomes at stage 2,
    propagates the stage-2 prediction error to stage-1 MF values via an eligibility trace
    that is stronger for common transitions (confirmation effect), and includes a lapse
    parameter that injects stimulus-independent noise into both stage-1 and stage-2 choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage actions per trial (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions per trial.
    reward : array-like of float
        Reward per trial.
    model_parameters : iterable of float
        [alpha_pos, alpha_neg, beta, lambda_, epsilon]
        - alpha_pos in [0, 1]: learning rate for positive stage-2 prediction errors (r - Q > 0).
        - alpha_neg in [0, 1]: learning rate for negative stage-2 prediction errors (r - Q < 0).
        - beta      in [0, 10]: inverse temperature for softmax choices.
        - lambda_   in [0, 1]: eligibility trace strength for common transitions; for rare transitions
                               the trace is scaled by (1 - lambda_) to capture confirmation asymmetry.
        - epsilon   in [0, 1]: lapse probability; with probability epsilon choose uniformly at random.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, lambda_, epsilon = model_parameters
    n_trials = len(action_1)

    # Fixed transitions for common/rare detection (A->X common, U->Y common)
    # We'll treat "common" as (a1 == state), "rare" otherwise under this coding.
    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        # Stage-1 softmax over MF values
        exp_q1 = np.exp(beta * (q1_mf - np.max(q1_mf)))
        probs_1_soft = exp_q1 / (np.sum(exp_q1) + eps)
        # Lapse mixture
        probs_1 = (1.0 - epsilon) * probs_1_soft + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax in observed state
        s = state[t]
        exp_q2 = np.exp(beta * (q2_mf[s] - np.max(q2_mf[s])))
        probs_2_soft = exp_q2 / (np.sum(exp_q2) + eps)
        probs_2 = (1.0 - epsilon) * probs_2_soft + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Stage-2 PE and asymmetric learning rate
        pe2 = r - q2_mf[s, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2_mf[s, a2] += alpha2 * pe2

        # Confirmation-weighted eligibility from stage 2 to stage 1
        # Common if chosen action usually leads to observed state under fixed mapping (A->X, U->Y).
        is_common = 1 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0
        trace = lambda_ if is_common else (1.0 - lambda_)
        q1_mf[a1] += alpha2 * trace * pe2  # use same valence-dependent step size

        # Optional direct bootstrap to align stage-1 MF with realized second-stage value
        target1 = q2_mf[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use a small complementary weight tied to confirmation structure to avoid extra parameters
        q1_mf[a1] += (1.0 - trace) * alpha2 * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll