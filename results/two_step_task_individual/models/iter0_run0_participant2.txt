def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model-based/model-free RL with eligibility trace and perseveration at both stages.

    This model combines a model-based (MB) planner that uses the known transition structure
    with a model-free (MF) controller updated via temporal-difference learning and an eligibility
    trace to propagate second-stage outcomes back to first-stage values. It also includes
    choice perseveration (stickiness) biases at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached planet:
        at planet X (state=0): 0 = alien W, 1 = alien S;
        at planet Y (state=1): 0 = alien P, 1 = alien H.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w, lam, p_stick)
        - alpha in [0,1]: Learning rate for value updates (both stages).
        - beta in [0,10]: Softmax inverse temperature shared across stages.
        - w in [0,1]: Hybrid weight; 1 = purely model-based, 0 = purely model-free at stage 1.
        - lam in [0,1]: Eligibility trace; larger values propagate reward more to stage-1.
        - p_stick in [0,1]: Perseveration strength added to the previously chosen action at both stages.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, lam, p_stick = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows = first-stage action (A,U), cols = next state (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Tracking choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)          # Q for first-stage actions (A,U)
    q_stage2_mf = np.zeros((2, 2))     # Q for second-stage actions within each state

    # Perseveration (stickiness) memories for both stages
    prev_a1 = -1
    prev_a2 = np.array([-1, -1])  # store last choice per state for second-stage; -1 = none

    for t in range(n_trials):

        # Model-based first-stage values: plan via transition matrix over MF second-stage values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # best option per state
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value by each spaceship

        # Hybrid first-stage value
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Add perseveration bias at stage 1
        if prev_a1 in (0, 1):
            q1[prev_a1] += p_stick

        # First-stage policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in the realized state
        s = state[t]
        q2 = q_stage2_mf[s].copy()

        # Add perseveration bias at stage 2 for this state
        if prev_a2[s] in (0, 1):
            q2[prev_a2[s]] += p_stick

        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Cache pre-update value for TD propagation
        q2_sa_old = q_stage2_mf[s, a2]

        # Model-free updates
        # Stage-2 TD update
        delta2 = reward[t] - q2_sa_old
        q_stage2_mf[s, a2] = q2_sa_old + alpha * delta2

        # Stage-1 MF update with eligibility trace:
        # First move Q1 toward the second-stage value; then add eligibility component toward the reward.
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha * (q2_sa_old - q_stage1_mf[a1]) + alpha * lam * (reward[t] - q2_sa_old)

        # Update perseveration memories
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Uncertainty-weighted hybrid with learned transitions and adaptive model-based weighting.

    This model learns the transition probabilities from experience and adapts the reliance
    on model-based planning trial-by-trial as a function of transition surprise:
    more predictable transitions increase the model-based weight, while surprising transitions
    shift weight toward model-free control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial in the reached state:
        at X: 0 = W, 1 = S; at Y: 0 = P, 1 = H.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w0, eta_w, alpha_T)
        - alpha in [0,1]: Learning rate for Q-value updates (both stages).
        - beta in [0,10]: Softmax inverse temperature shared across stages.
        - w0 in [0,1]: Initial weight on model-based values at stage 1.
        - eta_w in [0,1]: Rate of adapting the model-based weight based on transition predictability.
        - alpha_T in [0,1]: Learning rate for transition probabilities T(action -> state).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w0, eta_w, alpha_T = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix with agnostic prior (rows sum to 1)
    T = np.full((2, 2), 0.5)

    # Model-free values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Dynamic MB weight
    w = w0

    for t in range(n_trials):
        # Model-based first-stage values computed using current transition estimates
        max_q2 = np.max(q_stage2_mf, axis=1)
        q1_mb = T @ max_q2

        # Hybrid first-stage values
        q1 = w * q1_mb + (1.0 - w) * q_stage1_mf

        # First-stage policy and probability
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in realized state
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Cache old values for updates
        q2_sa_old = q_stage2_mf[s, a2]
        T_row_old_pred = T[a1, s]  # predicted probability of observed transition before update

        # Value learning
        # Stage-2 TD update
        delta2 = reward[t] - q2_sa_old
        q_stage2_mf[s, a2] = q2_sa_old + alpha * delta2

        # Stage-1 MF update toward (pre-update) second-stage value
        q_stage1_mf[a1] = q_stage1_mf[a1] + alpha * (q2_sa_old - q_stage1_mf[a1])

        # Transition learning for the taken first-stage action
        # Increase probability of the observed state and decrease the unobserved one
        T[a1, s] = T[a1, s] + alpha_T * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] = T[a1, other] + alpha_T * (0.0 - T[a1, other])

        # Adapt model-based weight based on predictability (lower surprise -> larger w)
        # surprise = 1 - predicted probability of the observed transition
        surprise = 1.0 - T_row_old_pred
        predictability = 1.0 - surprise  # equals T_row_old_pred
        w = (1.0 - eta_w) * w + eta_w * predictability
        # keep w within [0,1] numerically
        w = min(1.0, max(0.0, w))

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Model-based controller with outcome-asymmetric learning and choice kernels.

    This model plans at the first stage using the known transition structure (pure model-based),
    learns second-stage values with outcome-dependent learning rates (more/less learning from
    rewards vs. omissions), and includes choice kernels (action repetition tendencies) at both
    stages that are learned with their own timescale.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial in the reached state:
        at X: 0 = W, 1 = S; at Y: 0 = P, 1 = H.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, rho, phi, kappa)
        - alpha in [0,1]: Base learning rate for value updates.
        - beta in [0,10]: Softmax inverse temperature shared across stages.
        - rho in [0,1]: Outcome asymmetry; effective learning rate is alpha*rho for rewards
                        and alpha*(1-rho) for omissions.
        - phi in [0,1]: Choice-kernel learning rate (how fast kernels update/decay).
        - kappa in [0,1]: Strength of choice-kernel bias added to action preferences.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, rho, phi, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure used by the planner
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values and choice kernels
    q_stage2 = np.zeros((2, 2))  # second-stage action values
    K1 = np.zeros(2)             # first-stage choice kernel for (A,U)
    K2 = np.zeros((2, 2))        # second-stage choice kernels per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage values (plan over current second-stage values)
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Add choice kernel bias at stage 1
        logits1 = q1_mb + kappa * K1
        exp_q1 = np.exp(beta * (logits1 - np.max(logits1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with choice kernel bias in realized state
        s = state[t]
        logits2 = q_stage2[s] + kappa * K2[s]
        exp_q2 = np.exp(beta * (logits2 - np.max(logits2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome-dependent learning at stage 2
        r = reward[t]
        lr = alpha * (rho if r > 0.0 else (1.0 - rho))
        pe2 = r - q_stage2[s, a2]
        q_stage2[s, a2] = q_stage2[s, a2] + lr * pe2

        # Update choice kernels (stage-specific, recency-weighted)
        # Exponential recency with learning rate phi
        # First stage
        K1 = (1.0 - phi) * K1
        K1[a1] += phi
        # Second stage (only in reached state)
        K2[s] = (1.0 - phi) * K2[s]
        K2[s, a2] += phi

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll