def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and separate learning rates.
    
    This model combines model-based planning at the first stage with model-free values,
    and propagates second-stage reward prediction errors back to stage 1 via an eligibility trace.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on that planet).
    reward : array-like of float (typically 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha1, alpha2, beta, w, lam]
        - alpha1 in [0,1]: learning rate for stage-1 model-free value updates.
        - alpha2 in [0,1]: learning rate for stage-2 value updates (rewards).
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - w in [0,1]: weight on model-based values at stage 1 (1-w on model-free).
        - lam in [0,1]: eligibility trace controlling how much stage-2 PE backs up to stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha1, alpha2, beta, w, lam = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # action 0 (A): P(X), P(Y)
                                  [0.3, 0.7]]) # action 1 (U): P(X), P(Y)

    # Initialize choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2) + 0.5  # neutral prior
    q_stage2_mf = np.zeros((2, 2)) + 0.5  # for each state, two actions

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first stage: backup via transition model using current MF second-stage values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # value of each state under best second-stage action
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value for each first-stage action

        # Hybrid first-stage action values
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # First-stage policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (pure model-free at the encountered state)
        s = state[t]
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        pe2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha2 * pe2

        # Stage-1 MF update with eligibility
        # Immediate backup from second-stage value (Sutton-style) plus eligibility component from reward PE
        td_target_s1 = q_stage2_mf[s, a2]  # bootstrap target
        pe1 = td_target_s1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha1 * pe1 + alpha1 * lam * pe2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with choice stickiness, transition-sensitive credit assignment, and forgetting.
    
    This model assumes primarily model-free control but includes:
    - Perseveration (stickiness) bias that adds value to repeating the previous choice.
    - Transition-sensitive credit assignment: stage-1 learning rate depends on whether
      the observed transition was common or rare.
    - Forgetting towards a neutral value across trials.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on that planet).
    reward : array-like of float (typically 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta, kappa, eta, f]
        - alpha in [0,1]: base learning rate.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa in [0,1]: stickiness weight added to the previously chosen action(s).
        - eta in [0,1]: transition sensitivity (higher values give more credit after common transitions,
          lower values give more credit after rare transitions).
        - f in [0,1]: forgetting rate pulling Q-values toward 0.5 each trial.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, kappa, eta, f = model_parameters
    n_trials = len(action_1)

    # Known transition structure to determine common vs rare transitions
    # Common if (A->X) or (U->Y)
    def is_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize Q-values and stickiness traces
    q1 = np.zeros(2) + 0.5
    q2 = np.zeros((2, 2)) + 0.5

    prev_a1 = None
    prev_a2 = [None, None]  # keep separate previous second-stage action per state

    eps = 1e-12

    for t in range(n_trials):
        # Apply forgetting drift toward 0.5
        q1 = (1 - f) * q1 + f * 0.5
        q2 = (1 - f) * q2 + f * 0.5

        # Choice stickiness augmentations
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa

        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa

        # First-stage policy with stickiness
        val1 = q1 + bias1
        exp1 = np.exp(beta * (val1 - np.max(val1)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with stickiness
        val2 = q2[s] + bias2
        exp2 = np.exp(beta * (val2 - np.max(val2)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Learning updates
        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update with transition-sensitive credit assignment
        # Weight alpha depending on whether transition was common or rare
        common = is_common(a1, s)
        alpha1_eff = alpha * (eta if common else (1.0 - eta))
        # Backup from obtained second-stage value
        pe1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha1_eff * pe1

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Adaptive transition learning with uncertainty-driven exploration and lapse.
    
    This model learns the first-stage transition probabilities online and
    uses model-based planning. It adds an uncertainty bonus:
      - At stage 1: bonus proportional to the entropy of learned transition from each action.
      - At stage 2: bonus proportional to Q-value uncertainty proxy q*(1-q).
    A small lapse probability mixes uniform choice into the policy.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on that planet).
    reward : array-like of float (typically 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta, tau, phi, xi]
        - alpha in [0,1]: learning rate for reward-based Q updates at stage 2.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - tau in [0,1]: transition learning rate for first-stage transition probabilities.
        - phi in [0,1]: strength of uncertainty bonus (applied to both stages).
        - xi in [0,1]: lapse probability mixing uniform choice into the softmax policy.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, tau, phi, xi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model T[action, state]
    T = np.ones((2, 2)) * 0.5  # start agnostic

    # Second-stage Q-values
    Q2 = np.zeros((2, 2)) + 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Compute model-based values at stage 1 using learned transitions
        maxQ2 = np.max(Q2, axis=1)  # value per state
        Q1_mb = T @ maxQ2  # expected value per first-stage action

        # Uncertainty bonuses
        # Stage-1: entropy of transition distribution for each action
        # H(p) = -sum p log p; handle numerical stability
        p = np.clip(T, eps, 1 - eps)
        H = -(p * np.log(p) + (1 - p) * np.log(1 - p)).sum(axis=1)  # entropy per action

        Q1_aug = Q1_mb + phi * H

        # First-stage policy with lapse
        q1_centered = Q1_aug - np.max(Q1_aug)
        soft1 = np.exp(beta * q1_centered)
        soft1 /= (np.sum(soft1) + eps)
        policy1 = (1 - xi) * soft1 + xi * 0.5  # uniform mixture
        a1 = action_1[t]
        p_choice_1[t] = policy1[a1]

        # Second-stage policy with uncertainty bonus based on Q variance proxy q*(1-q)
        s = state[t]
        q2s = Q2[s]
        u2 = q2s * (1 - q2s)  # maximum at 0.5, lower near extremes
        q2_aug = q2s + phi * u2
        q2_centered = q2_aug - np.max(q2_aug)
        soft2 = np.exp(beta * q2_centered)
        soft2 /= (np.sum(soft2) + eps)
        policy2 = (1 - xi) * soft2 + xi * 0.5
        a2 = action_2[t]
        p_choice_2[t] = policy2[a2]

        # Outcome and learning
        r = reward[t]

        # Stage-2 TD learning
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update learned transitions with simple delta rule
        # For chosen first-stage action a1, move probability toward observed state s
        for st in (0, 1):
            target = 1.0 if st == s else 0.0
            T[a1, st] += tau * (target - T[a1, st])

        # Ensure numerical stability (keep inside (eps, 1-eps) and row-normalization)
        T[a1] = np.clip(T[a1], eps, 1 - eps)
        T[a1] /= np.sum(T[a1])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll