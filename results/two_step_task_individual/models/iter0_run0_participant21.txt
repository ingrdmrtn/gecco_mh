def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free RL with eligibility trace and perseveration.
    
    This model blends model-based planning at stage 1 with a model-free value for
    first-stage actions, learned via an eligibility trace from stage-2 outcomes.
    Perseveration biases repeating the previous first-stage choice.
    
    Parameters (all must be in bounds):
    - alpha: [0,1] learning rate for value updates (both stages)
    - beta: [0,10] inverse temperature for both stages' softmax policies
    - w_mb: [0,1] weight of model-based value in the first-stage action values
    - lambda_et: [0,1] eligibility trace for propagating stage-2 TD error to stage-1 MF values
    - kappa: [0,1] perseveration strength added to the last chosen first-stage action
    
    Inputs:
    - action_1: array of length T with first-stage choices (0=A, 1=U)
    - state:    array of length T with second-stage states (0=X, 1=Y)
    - action_2: array of length T with second-stage choices (0 or 1; alien within state)
    - reward:   array of length T with outcomes (0/1 coins)
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w_mb, lambda_et, kappa = model_parameters
    n_trials = len(action_1)

    # Known (fixed) transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X, Y]
                                  [0.3, 0.7]]) # from U to [X, Y]

    # Probabilities of the observed choices at each stage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q_stage1_mf = np.zeros(2)           # model-free value for first-stage actions
    q_stage2 = np.full((2, 2), 0.5)     # second-stage Q-values initialized to 0.5

    # Perseveration bias: last chosen first-stage action index; initialize to None
    last_a1 = None

    for t in range(n_trials):
        # Model-based stage-1 action values via one-step lookahead over learned stage-2 values
        max_q_stage2 = np.max(q_stage2, axis=1)     # best option on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Add perseveration to the policy as a bias term
        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] += kappa

        # Hybrid value for stage 1
        q1 = (1.0 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb + bias

        # Stage-1 softmax policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax policy in the observed state
        s = state[t]
        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcomes
        r = reward[t]

        # TD errors
        # Stage-2 TD error
        delta2 = r - q_stage2[s, a2]
        # Update stage-2 value
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 MF update via eligibility trace using the obtained stage-2 value as the target
        # The target for stage-1 MF is the stage-2 Q-value after observing the state and choice
        # delta1_mf = Q2(s,a2)_pre - Q1_MF(a1); we apply backprop of delta2 via lambda as well
        delta1_mf = (q_stage2[s, a2]) - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (lambda_et * delta2 + (1.0 - lambda_et) * delta1_mf)

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with learned transition model, value forgetting, and second-stage stickiness.
    
    This model learns the transition probabilities online and plans at stage 1 using
    the current transition estimates and stage-2 values. Second-stage values undergo
    reward learning with forgetting for unchosen actions. A stickiness bias promotes
    repeating the previous second-stage action within each state.
    
    Parameters:
    - alpha_val: [0,1] learning rate for second-stage Q-value updates
    - alpha_tr:  [0,1] learning rate for transition probability estimates
    - beta:      [0,10] inverse temperature for both stages' softmax policies
    - rho_forget:[0,1] forgetting rate toward 0.5 for unchosen second-stage actions
    - sigma_s:   [0,1] stickiness strength added to the last chosen second-stage action in each state
    
    Inputs:
    - action_1: array of length T with first-stage choices (0=A, 1=U)
    - state:    array of length T with second-stage states (0=X, 1=Y)
    - action_2: array of length T with second-stage choices (0 or 1)
    - reward:   array of length T with outcomes (0/1 coins)
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha_val, alpha_tr, beta, rho_forget, sigma_s = model_parameters
    n_trials = len(action_1)

    # Initialize transition estimates: rows are actions (A,U), cols are states (X,Y)
    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    # Second-stage Q-values initialized to 0.5
    q2 = np.full((2, 2), 0.5)

    # Track last second-stage action per state for stickiness
    last_a2 = np.array([-1, -1], dtype=int)  # -1 denotes no prior action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based stage-1 action values using learned transitions
        max_q2 = np.max(q2, axis=1)  # value of each state
        q1_mb = T_hat @ max_q2

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1_mb - np.max(q1_mb)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in observed state with stickiness
        s = state[t]
        pref2 = q2[s].copy()
        if last_a2[s] != -1:
            pref2[last_a2[s]] += sigma_s
        exp_q2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update second-stage Q-values
        r = reward[t]
        # Update chosen
        delta = r - q2[s, a2]
        q2[s, a2] += alpha_val * delta
        # Forget unchosen action in this state toward 0.5
        other = 1 - a2
        q2[s, other] += rho_forget * (0.5 - q2[s, other])

        # Update transition estimates with simple delta rule toward observed transition
        # Observed transition indicator: one-hot for reached state
        obs = np.array([0.0, 0.0])
        obs[s] = 1.0
        T_hat[a1] += alpha_tr * (obs - T_hat[a1])
        # Renormalize row to sum to 1 (to correct numeric drift)
        row_sum = np.sum(T_hat[a1])
        if row_sum > 0:
            T_hat[a1] /= row_sum

        # Update stickiness memory
        last_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Transition-aware win-stay/lose-shift with policy preferences (policy-gradient style).
    
    This heuristic model maintains action preferences rather than values.
    First-stage preference updates depend on outcome and whether the transition
    was common or rare, capturing model-based signatures without full planning.
    Second-stage preferences implement a win-stay/lose-shift rule within each state.
    
    Parameters:
    - beta:   [0,10] inverse temperature for softmax over preferences (both stages)
    - eta:    [0,1] learning rate for preference updates (both stages)
    - k_win:  [0,1] magnitude of win-driven reinforcement of chosen action
    - k_loss: [0,1] magnitude of loss-driven suppression of chosen action
    - tau:    [0,1] transition sensitivity: scales how much rare vs common affects stage-1 update
               (common scale = 1.0, rare scale = 1.0 - tau)
    
    Inputs:
    - action_1: array of length T with first-stage choices (0=A, 1=U)
    - state:    array of length T with second-stage states (0=X, 1=Y)
    - action_2: array of length T with second-stage choices (0 or 1)
    - reward:   array of length T with outcomes (0/1 coins)
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    beta, eta, k_win, k_loss, tau = model_parameters
    n_trials = len(action_1)

    # First-stage action preferences
    pref1 = np.zeros(2)
    # Second-stage action preferences per state
    pref2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Policies from preferences
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        exp2 = np.exp(beta * (pref2[s] - np.max(pref2[s])))
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Determine if transition was common or rare based on known structure:
        # A commonly -> X; U commonly -> Y
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Scale factor for transition effect on first-stage learning
        trans_scale = 1.0 if is_common else max(0.0, 1.0 - tau)

        # First-stage preference update (policy-gradient like, baseline via softmax probs)
        # Chosen action preference increases on wins, decreases on losses; unchosen moves opposite via softmax baseline.
        # Use signed "advantage" constructed from outcome and k_win/k_loss.
        signed_outcome = (k_win * r) - (k_loss * (1 - r))
        # REINFORCE-like update: Î”pref = eta * signed_outcome * (onehot(chosen) - probs)
        grad1 = -probs1
        grad1[a1] += 1.0
        pref1 += eta * trans_scale * signed_outcome * grad1

        # Second-stage preference update within reached state (no transition modulation)
        grad2 = -probs2
        grad2[a2] += 1.0
        pref2[s] += eta * signed_outcome * grad2

        # Small decay toward zero to prevent unbounded growth (implicit forgetting)
        pref1 *= (1.0 - 0.5 * eta * 0.1)
        pref2[s] *= (1.0 - 0.5 * eta * 0.1)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss