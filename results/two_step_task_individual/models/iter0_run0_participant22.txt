def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based + model-free with eligibility trace and choice stickiness.
    
    This model blends model-based action values (using a fixed transition model)
    with model-free values at stage 1, learns second-stage values with a TD rule,
    and propagates value to stage 1 using an eligibility-trace-like mixture of
    immediate reward and bootstrapped second-stage value. It also includes
    choice stickiness at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (typically 0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha: learning rate for value updates, in [0,1].
        - beta: inverse temperature for softmax (both stages), in [0,10].
        - w: weight on model-based values at stage 1 (blend with model-free), in [0,1].
        - lam: eligibility-trace mixing between immediate reward and bootstrapped value, in [0,1].
        - stick: choice stickiness weight added to the previously chosen action at each stage, in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, w, lam, stick = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # A to X/Y
                                  [0.3, 0.7]]) # U to X/Y

    # Probabilities of the chosen options on each trial (for log-likelihood)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)          # model-free Q for first-stage actions
    q_stage2 = np.zeros((2, 2))         # second-stage Q(s,a)

    # Stickiness trackers: previous action at stage 1; previous action per second-stage state
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        # Model-based value for stage 1
        max_q_stage2 = np.max(q_stage2, axis=1)                # value of best action in each second-stage state
        q_stage1_mb = transition_matrix @ max_q_stage2          # MB backup to stage 1

        # Blend MB and MF, add stickiness bias
        q1_net = (1.0 - w) * q_stage1_mf + w * q_stage1_mb
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] += stick
            q1_net = q1_net + bias

        # Softmax for stage 1
        exp_q1 = np.exp(beta * (q1_net - np.max(q1_net)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with per-state stickiness
        s = state[t]
        q2_net = q_stage2[s].copy()
        if prev_a2[s] is not None:
            bias2 = np.zeros(2)
            bias2[prev_a2[s]] += stick
            q2_net = q2_net + bias2

        exp_q2 = np.exp(beta * (q2_net - np.max(q2_net)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # Second stage TD error and update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage 1 MF update with eligibility-like mixture
        target1 = lam * r + (1.0 - lam) * q_stage2[s, a2]
        q_stage1_mf[a1] += alpha * (target1 - q_stage1_mf[a1])

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based choice at stage 1 with adaptive (Pearce–Hall-like) learning rate,
    subjective transition belief, and perseveration bias.

    Stage 1: purely model-based using the agent’s subjective belief about common transitions.
    Stage 2: model-free softmax. The learning rate adapts each trial based on unsigned PE.
    Perseveration adds a bias toward repeating the previous action at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (typically 0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha0: initial learning rate (adapted across trials), in [0,1].
        - beta: inverse temperature for softmax (both stages), in [0,10].
        - p_common: subjective probability of common transition (A->X and U->Y), in [0,1].
        - kappa: adaptation rate for the Pearce–Hall update of the learning rate, in [0,1].
        - rho: perseveration weight added to previously chosen action logits at both stages, in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha_t, beta, p_common, kappa, rho = model_parameters
    n_trials = len(action_1)

    # Subjective transition matrix constructed from p_common
    # Row 0 = A; row 1 = U. Col 0 = X; col 1 = Y.
    transition_matrix = np.array([[p_common, 1.0 - p_common],
                                  [1.0 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2 = np.zeros((2, 2))  # second-stage values
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        # Model-based value for stage 1 via subjective transitions
        max_q_stage2 = np.max(q_stage2, axis=1)
        q1_mb = transition_matrix @ max_q_stage2

        # Add perseveration at stage 1
        q1_net = q1_mb.copy()
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] += rho
            q1_net += bias

        # Softmax for stage 1
        exp_q1 = np.exp(beta * (q1_net - np.max(q1_net)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with perseveration per state
        s = state[t]
        q2_net = q_stage2[s].copy()
        if prev_a2[s] is not None:
            bias2 = np.zeros(2)
            bias2[prev_a2[s]] += rho
            q2_net += bias2

        exp_q2 = np.exp(beta * (q2_net - np.max(q2_net)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning with adaptive learning rate (Pearce–Hall style):
        # Update second-stage Q with current alpha_t
        pe = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_t * pe

        # Update alpha_t based on unsigned prediction error
        alpha_t = (1.0 - kappa) * alpha_t + kappa * np.abs(pe)
        # Bound alpha_t to [0,1] numerically
        alpha_t = max(0.0, min(1.0, alpha_t))

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free SARSA with reward-by-transition-modulated stay bias at stage 1.

    This model learns purely model-free values at both stages. The first-stage policy
    includes a dynamic bias to repeat the previous first-stage choice that depends on:
    - whether the previous transition was common vs. rare (computed from task structure)
    - and whether the previous outcome was rewarded.
    This captures a hallmark "model-based-like" stay/switch pattern without explicit planning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (typically 0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha: learning rate for value updates, in [0,1].
        - beta: inverse temperature for softmax (both stages), in [0,10].
        - psi: base strength of the stay-bias added to last first-stage choice, in [0,1].
        - xi: modulation by transition type; xi=0 ignores transition, xi=1 flips bias after rare transitions, in [0,1].
        - gamma: discount factor used when backing up second-stage value to stage 1, in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, psi, xi, gamma = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)        # stage-1 MF values
    q2 = np.zeros((2, 2))   # stage-2 MF values

    prev_a1 = None
    prev_reward = 0.0
    prev_common = True  # initialize as common; only used when prev_a1 exists

    # Helper to determine common vs rare from task structure:
    # A commonly -> X; U commonly -> Y.
    def is_common(a1, s2):
        return (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)

    for t in range(n_trials):
        # Compute dynamic stay bias for stage 1 based on previous trial
        bias_vec = np.zeros(2)
        if prev_a1 is not None:
            # Map reward to {-1, +1}
            rew_sign = 2.0 * prev_reward - 1.0
            # Transition modulation factor in [+1, -1] controlled by xi:
            # xi=0 -> factor = +1 (no dependence on transition)
            # xi=1 -> factor = +1 for common, -1 for rare
            trans_factor = (1.0 - xi) + xi * (1.0 if prev_common else -1.0)
            bias_mag = psi * rew_sign * trans_factor
            bias_vec[prev_a1] += bias_mag

        # Stage 1 policy
        q1_net = q1 + bias_vec
        exp_q1 = np.exp(beta * (q1_net - np.max(q1_net)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        q2_net = q2[s]
        exp_q2 = np.exp(beta * (q2_net - np.max(q2_net)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates (MF SARSA-like)
        # Update second stage toward reward
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Backup to stage 1 using discounted second-stage value (no explicit reward at stage 1)
        pe1 = gamma * q2[s, a2] - q1[a1]
        q1[a1] += alpha * pe1

        # Update memory for next trial's bias
        prev_common = is_common(a1, s)
        prev_a1 = a1
        prev_reward = r

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))