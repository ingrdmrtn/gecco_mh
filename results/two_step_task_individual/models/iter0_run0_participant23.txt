Below are three standalone cognitive models tailored to the two-step decision task. Each function:

- Takes arrays action_1, state, action_2, reward, and a model_parameters vector.
- Uses all parameters meaningfully within the specified bounds.
- Returns the negative log-likelihood of the observed choices across both stages.

Notes and conventions assumed by all models:
- action_1 ∈ {0,1}: 0 = spaceship A, 1 = spaceship U
- state ∈ {0,1}: 0 = planet X, 1 = planet Y
- action_2 ∈ {0,1}: indices of the two aliens on the current planet
- reward ∈ {0,1} (or bounded scalar)
- Common transitions: A→X and U→Y; Rare transitions: A→Y and U→X

Model 1: Hybrid model-based/model-free with eligibility trace and choice stickiness
- Parameters: alpha (0-1), beta (0-10), w (0-1), lambda_ (0-1), kappa (0-1)

Model 2: Learned transition model (model-based planning with online transition learning), stage-2 forgetting, and perseveration
- Parameters: alpha (0-1), beta (0-10), tau (0-1), phi (0-1), rho (0-1)

Model 3: Mixture of win-stay/lose-shift heuristic and model-free values, with transition sensitivity and lapse
- Parameters: alpha (0-1), beta (0-10), omega_h (0-1), eta (0-1), epsilon (0-1)


def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with eligibility trace and choice stickiness.
    
    Combines model-based (MB) first-stage values from the transition model with
    model-free (MF) values at both stages. A single learning rate updates both
    stages, with an eligibility trace to propagate reward back to stage 1.
    A simple perseveration (stickiness) term biases repeating the previous choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) per trial.
    reward : array-like of float
        Scalar reward per trial (e.g., 0/1 coins).
    model_parameters : iterable
        [alpha, beta, w, lambda_, kappa]
        - alpha in [0,1]: learning rate for MF values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight mixing MB and MF at stage 1 (w=1 purely MB).
        - lambda_ in [0,1]: eligibility trace to propagate reward to stage 1.
        - kappa in [0,1]: choice stickiness at stage 1 (bias to repeat last a1).

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w, lambda_, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix: rows=first-stage action, cols=state
    # A→X (0.7), A→Y (0.3); U→X (0.3), U→Y (0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize value functions
    q1_mf = np.zeros(2)          # MF values for first-stage actions
    q2 = np.zeros((2, 2))        # MF values for second-stage actions: q2[state, action2]

    # For likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None  # to implement perseveration

    eps = 1e-10

    for t in range(n_trials):
        # Model-based first-stage values via max over second-stage actions
        max_q2 = np.max(q2, axis=1)             # value of each state
        q1_mb = transition_matrix @ max_q2      # expected value of first-stage actions

        # Combine MB and MF at stage 1
        q1_combined = w * q1_mb + (1 - w) * q1_mf

        # Add stickiness for repeating previous a1
        stickiness_bias = np.zeros(2)
        if prev_a1 is not None:
            stickiness_bias[prev_a1] = kappa

        # Stage 1 policy
        logits1 = beta * q1_combined + stickiness_bias
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy for current state
        s = state[t]
        logits2 = beta * q2[s]
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update with eligibility trace
        # Standard hybrid: immediate backup from q2 plus lambda * reward PE
        backup = np.max(q2[s])  # bootstrap target for stage 1 MF component
        pe1 = backup - q1_mf[a1]
        q1_mf[a1] += alpha * pe1 + alpha * lambda_ * pe2

        # Update prev action
        prev_a1 = a1

    negloglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return negloglik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with learned transitions, stage-2 forgetting, and perseveration.

    Learns the transition function online and plans model-based at stage 1.
    Second-stage values are updated with TD and decay (forgetting).
    A perseveration term biases repeating the previous first-stage choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward per trial (e.g., 0/1 coins).
    model_parameters : iterable
        [alpha, beta, tau, phi, rho]
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature (both stages).
        - tau in [0,1]: learning rate for transition probabilities.
        - phi in [0,1]: forgetting/decay rate for second-stage Q-values.
        - rho in [0,1]: perseveration strength at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, tau, phi, rho = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T (rows=sum to ~1), start neutral
    T = np.full((2, 2), 0.5)

    # Second-stage MF values (with forgetting)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        # Decay/forget Q2 slightly each trial (toward 0)
        q2 *= (1.0 - phi)

        # Model-based value for stage 1 via learned transitions
        max_q2 = np.max(q2, axis=1)   # value of states X,Y
        q1_mb = T @ max_q2

        # Add perseveration bias
        stickiness = np.zeros(2)
        if prev_a1 is not None:
            stickiness[prev_a1] = rho

        # Stage 1 policy
        logits1 = beta * q1_mb + stickiness
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * q2[s]
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = reward[t]

        # Update Q2 with TD
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update learned transition probabilities for chosen action
        # Move chosen action's row toward the observed state (one-hot)
        # One-hot target for observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        # Row update: T[a1] <- T[a1] + tau*(target - T[a1])
        T[a1] += tau * (target - T[a1])

        prev_a1 = a1

    negloglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return negloglik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Mixture of win-stay/lose-shift heuristic and model-free values with transition sensitivity and lapse.

    Stage 1 policy is a convex combination of:
      - A softmax over MF values, and
      - A transition-sensitive win-stay/lose-shift (WSLS) heuristic.
    The heuristic increases staying after rewarded/common transitions, and reduces
    staying after rewarded/rare transitions (controlled by eta). A lapse parameter
    adds uniform noise. Stage 2 choices are softmax over learned Q2 with the same lapse.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward per trial (e.g., 0/1 coins).
    model_parameters : iterable
        [alpha, beta, omega_h, eta, epsilon]
        - alpha in [0,1]: learning rate for MF values (both stages).
        - beta in [0,10]: inverse temperature for softmax.
        - omega_h in [0,1]: weight on the WSLS heuristic at stage 1 (0=MF only, 1=heuristic only).
        - eta in [0,1]: transition sensitivity; eta=1 fully flips stay tendency after rare transitions.
        - epsilon in [0,1]: lapse rate; with prob epsilon choose uniformly at random.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, omega_h, eta, epsilon = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure for determining common vs rare
    # A(0)->X(0), U(1)->Y(1) common; otherwise rare
    def is_common(a1, s):
        return int(a1 == s)

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_r = None
    prev_common = None

    eps = 1e-10

    for t in range(n_trials):
        # Stage 1: MF softmax component
        logits_mf = beta * q1_mf
        exp_mf = np.exp(logits_mf - np.max(logits_mf))
        probs_mf = exp_mf / (np.sum(exp_mf) + eps)

        # Stage 1: WSLS heuristic component
        if prev_a1 is None:
            probs_h = np.array([0.5, 0.5])
        else:
            # Base stay tendency: higher after win, lower after loss
            base_stay = 0.5 + (prev_r - 0.5)  # equals prev_r (1.0 if win, 0.0 if loss)
            # After rare transitions, flip stay tendency proportionally to eta
            # effective_stay = (1-eta)*base_stay + eta*(1-base_stay)
            effective_stay = base_stay * (1 - eta) + (1 - base_stay) * eta if (prev_common == 0) else base_stay
            stay_prob = np.clip(effective_stay, 0.0, 1.0)
            probs_h = np.array([0.0, 0.0])
            probs_h[prev_a1] = stay_prob
            probs_h[1 - prev_a1] = 1.0 - stay_prob

        # Mixture policy for stage 1
        mix_probs = (1.0 - omega_h) * probs_mf + omega_h * probs_h
        # Apply lapse
        probs1 = (1.0 - epsilon) * mix_probs + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax with lapse
        s = state[t]
        logits2 = beta * q2[s]
        exp2 = np.exp(logits2 - np.max(logits2))
        softmax2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - epsilon) * softmax2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]
        common_flag = is_common(a1, s)

        # Learning: Stage 2 MF
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Learning: Stage 1 MF (TD with full backup: lambda≈1)
        # Combine bootstrapped value and reward PE
        backup = np.max(q2[s])
        pe1 = backup - q1_mf[a1]
        q1_mf[a1] += alpha * pe1 + alpha * pe2

        # Store for next-trial heuristic
        prev_a1 = a1
        prev_r = r
        prev_common = common_flag

    negloglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return negloglik