def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with eligibility trace and choice stickiness at both stages.
    
    This model combines model-based (MB) planning with model-free (MF) values for the first-stage decision, 
    uses a single MF learner at the second stage, and includes an eligibility trace that propagates 
    second-stage reward prediction errors back to the first stage. A choice stickiness term adds a bias 
    toward repeating the previous choice at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial; 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached; 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) within the state; 0 or 1.
    reward : array-like of float (typically 0 or 1)
        Reward obtained on each trial.
    model_parameters : sequence of floats
        [alpha, beta, w, lam, kappa]
        - alpha in [0, 1]: learning rate for MF updates at both stages.
        - beta in [0, 10]: inverse temperature (softmax) for both stages.
        - w in [0, 1]: weight balancing MB vs MF at stage 1 (w=1: pure MB; w=0: pure MF).
        - lam in [0, 1]: eligibility trace parameter propagating second-stage RPE to stage 1.
        - kappa in [0, 1]: choice stickiness strength (adds bias to repeat the previous choice).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))  # state x action

    prev_a1 = None
    prev_a2 = None  # last second-stage action (global stickiness)
    
    for t in range(n_trials):
        # Model-based action-values for stage 1 via planning on MF stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # for each state, best alien value
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value per first-stage action

        # Hybrid first-stage values and stickiness bias
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        stickiness1 = np.zeros(2)
        if prev_a1 is not None:
            stickiness1[prev_a1] += kappa
        logits1 = beta * q1 + stickiness1
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with stickiness
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        stickiness2 = np.zeros(2)
        if prev_a2 is not None:
            stickiness2[prev_a2] += kappa
        logits2 = beta * q2 + stickiness2
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Second-stage TD error and update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # First-stage MF TD error (bootstrapping from second-stage action value)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Eligibility trace: propagate reward-based RPE back to stage 1
        q_stage1_mf[a1] += alpha * lam * delta2

        # Update stickiness memory
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-learning MB + MF with asymmetric learning and transition-outcome bias.
    
    This model learns the transition probabilities from each first-stage action to the two states,
    uses them for model-based planning, and maintains model-free values at both stages.
    Second-stage learning uses asymmetric learning rates for positive vs negative prediction errors.
    A transition-outcome bias term (phi) captures the classic model-based 'stay/switch' signature:
    after a rewarded common transition (or unrewarded rare), it biases toward repeating the previous
    first-stage action; after a rewarded rare (or unrewarded common), it biases against repeating.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial; 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached; 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) within the state; 0 or 1.
    reward : array-like of float (typically 0 or 1)
        Reward obtained on each trial.
    model_parameters : sequence of floats
        [alpha_pos, alpha_neg, alpha_T, beta, phi]
        - alpha_pos in [0, 1]: learning rate when TD error is positive.
        - alpha_neg in [0, 1]: learning rate when TD error is negative or zero.
        - alpha_T in [0, 1]: learning rate for updating transition probabilities.
        - beta in [0, 10]: inverse temperature (softmax) for both stages.
        - phi in [0, 1]: strength of transition-outcome-dependent stay/switch bias at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha_pos, alpha_neg, alpha_T, beta, phi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition: p_trans[a] = P(state=0 | action=a)
    p_trans = np.array([0.7, 0.3], dtype=float)  # A->X likely, U->X unlikely (thus U->Y likely)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))  # state x action

    prev_a1 = None
    prev_state = None
    prev_reward = None

    for t in range(n_trials):
        # Model-based values via current transition estimates and MF stage-2 values
        max_q2 = np.max(q_stage2_mf, axis=1)  # [V(X), V(Y)]
        # q_MB[a] = p(s=0|a)*V(X) + (1 - p(s=0|a))*V(Y)
        q_stage1_mb = np.array([
            p_trans[0] * max_q2[0] + (1.0 - p_trans[0]) * max_q2[1],
            p_trans[1] * max_q2[0] + (1.0 - p_trans[1]) * max_q2[1]
        ])

        # Transition-outcome interaction bias toward staying or switching
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            was_common = int(prev_state == prev_a1)  # A->X or U->Y is common
            was_rewarded = int(prev_reward > 0)
            # sign = +1 for (common & rewarded) or (rare & unrewarded); else -1
            sign = 1 if (was_common and was_rewarded) or ((1 - was_common) and (1 - was_rewarded)) else -1
            bias1[prev_a1] += phi * sign

        # Combine MB and MF (unit weights) plus bias; beta scales both
        q1 = q_stage1_mb + q_stage1_mf
        logits1 = beta * q1 + bias1
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2_mf[s]
        logits2 = beta * q2
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update learned transition for chosen first-stage action (toward observed state)
        s0_indicator = 1 if s == 0 else 0
        p_trans[a1] += alpha_T * (s0_indicator - p_trans[a1])
        # Keep probabilities in [eps, 1-eps] numerically stable
        p_trans[a1] = min(max(p_trans[a1], 1e-6), 1 - 1e-6)

        # Second-stage learning with asymmetric rates
        delta2 = r - q_stage2_mf[s, a2]
        alpha2 = alpha_pos if delta2 > 0 else alpha_neg
        q_stage2_mf[s, a2] += alpha2 * delta2

        # First-stage MF learning (bootstrapping from updated second-stage value)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        alpha1 = alpha_pos if delta1 > 0 else alpha_neg
        q_stage1_mf[a1] += alpha1 * delta1

        # Memorize for next-trial bias
        prev_a1 = a1
        prev_state = s
        prev_reward = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Decomposed planet-alien learning with MB planning and first-stage perseveration.
    
    This model assumes second-stage values decompose into a planet-level value and an alien-specific
    value within that planet. The composite Q for an alien is a convex combination of the planet value
    and the alien value. First-stage decisions use model-based planning over these composite values
    with the fixed transition matrix. A perseveration bias at stage 1 encourages repeating the last action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial; 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached; 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) within the state; 0 or 1.
    reward : array-like of float (typically 0 or 1)
        Reward obtained on each trial.
    model_parameters : sequence of floats
        [alpha_planet, alpha_alien, beta, eta, rho]
        - alpha_planet in [0, 1]: learning rate for planet-level value updates.
        - alpha_alien in [0, 1]: learning rate for alien-specific value updates.
        - beta in [0, 10]: inverse temperature (softmax) for both stages.
        - eta in [0, 1]: mixing weight for composing alien Q from planet and alien components.
                         Q2(s,a) = eta * V_alien(s,a) + (1 - eta) * V_planet(s)
        - rho in [0, 1]: first-stage perseveration strength (bias to repeat previous action).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha_p, alpha_a, beta, eta, rho = model_parameters
    n_trials = len(action_1)

    # Fixed transitions: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Decomposed second-stage values
    V_planet = np.zeros(2)        # value per planet X/Y
    V_alien = np.zeros((2, 2))    # value per alien within each planet

    prev_a1 = None

    for t in range(n_trials):
        # Compose current second-stage Q-values from planet and alien components
        Q2_composite = eta * V_alien + (1.0 - eta) * V_planet[:, None]  # shape (2,2)

        # Model-based planning: expected value for each first-stage action
        max_Q2_per_state = np.max(Q2_composite, axis=1)  # [best on X, best on Y]
        q_stage1_mb = transition_matrix @ max_Q2_per_state

        # First-stage perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho

        logits1 = beta * q_stage1_mb + bias1
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy using composite Q in the reached state
        s = state[t]
        q2 = Q2_composite[s]
        logits2 = beta * q2
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates: update both planet and chosen alien values
        # Planet-level TD error
        delta_p = r - V_planet[s]
        V_planet[s] += alpha_p * delta_p

        # Alien-specific TD error
        delta_a = r - V_alien[s, a2]
        V_alien[s, a2] += alpha_a * delta_a

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll