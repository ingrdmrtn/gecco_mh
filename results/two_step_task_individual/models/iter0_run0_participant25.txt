def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and first-stage perseveration.
    
    This model blends model-based (MB) and model-free (MF) values at the first stage, 
    uses a standard MF learner at the second stage, propagates reward to the first stage 
    via an eligibility trace, and includes a first-stage perseveration bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; the two aliens available in the reached state) for each trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome on each trial.
    model_parameters : tuple/list
        (alpha, beta, w, lam, kappa)
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - w in [0,1]: weight of model-based control at stage 1 (1=fully MB, 0=fully MF).
        - lam in [0,1]: eligibility trace parameter scaling how much second-stage PE updates stage-1 MF.
        - kappa in [0,1]: strength of first-stage perseveration bias to repeat last first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to (X, Y)
                                  [0.3, 0.7]]) # from U to (X, Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q_stage1_mf = np.zeros(2)        # for spaceships A,U
    q_stage2_mf = np.zeros((2, 2))   # for aliens within X and Y

    # First-stage perseveration kernel: indicates last chosen action
    s1_prev = np.zeros(2)

    for t in range(n_trials):
        # Model-based estimate for stage 1 from second-stage MF
        max_q_stage2 = np.max(q_stage2_mf, axis=1)            # best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2         # expected value of each spaceship

        # Combine MB and MF, add perseveration bias
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf + kappa * s1_prev

        # First-stage policy
        exp_q1 = np.exp(beta * (q1_combined - np.max(q1_combined)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in reached state
        s = state[t]
        exp_q2 = np.exp(beta * (q_stage2_mf[s, :] - np.max(q_stage2_mf[s, :])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # TD updates
        r = reward[t]
        # Stage-2 update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace
        bootstrapped_value = q_stage2_mf[s, a2]
        delta1_boot = bootstrapped_value - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1_boot + alpha * lam * delta2

        # Update perseveration kernel: mark chosen first-stage action
        s1_prev = np.zeros(2)
        s1_prev[a1] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid model with learned transition probabilities and second-stage choice kernel.
    
    This model learns the transition structure online, blends model-based and model-free 
    values at the first stage using a fixed weight, learns second-stage values from reward, 
    and includes a second-stage choice kernel that biases repeating previous alien choices 
    within each state.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; the two aliens available in the reached state) for each trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome on each trial.
    model_parameters : tuple/list
        (alpha_r, beta, alpha_t, w, rho)
        - alpha_r in [0,1]: reward learning rate for MF Q-value updates.
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - alpha_t in [0,1]: learning rate for updating transition probabilities.
        - w in [0,1]: weight of model-based control at stage 1 (1=fully MB, 0=fully MF).
        - rho in [0,1]: strength/decay of the second-stage choice kernel.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_r, beta, alpha_t, w, rho = model_parameters
    n_trials = len(action_1)

    # Initialize transition beliefs; start with the known structure but allow learning
    T = np.array([[0.7, 0.3],   # P(X|A), P(Y|A)
                  [0.3, 0.7]])  # P(X|U), P(Y|U)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q_stage1_mf = np.zeros(2)        # for spaceships
    q_stage2_mf = np.zeros((2, 2))   # for aliens within each state

    # Second-stage choice kernel (state-dependent)
    K2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Model-based values from current transition belief
        max_q2 = np.max(q_stage2_mf, axis=1)          # best alien on each planet
        q1_mb = T @ max_q2

        # Combine MB and MF
        q1 = w * q1_mb + (1.0 - w) * q_stage1_mf

        # First-stage policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (with choice kernel)
        s = state[t]
        logits2 = beta * q_stage2_mf[s, :] + K2[s, :]
        logits2 = logits2 - np.max(logits2)
        exp_q2 = np.exp(logits2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Second-stage MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * delta2

        # First-stage MF update (bootstrapping to current second-stage value)
        boot_value = q_stage2_mf[s, a2]
        delta1 = boot_value - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Update transition beliefs for chosen first-stage action using alpha_t
        # Move probability mass toward the observed state s for the chosen action a1
        # T[a1, s] <- T[a1, s] + alpha_t*(1 - T[a1, s])
        # T[a1, 1-s] <- T[a1, 1-s] + alpha_t*(0 - T[a1, 1-s])
        T[a1, s] += alpha_t * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] += alpha_t * (0.0 - T[a1, other])

        # Renormalize row to guard against numeric drift
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

        # Update second-stage choice kernel: decay then add to chosen action
        K2[s, :] *= (1.0 - rho)
        K2[s, a2] += rho

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Dynamic arbitration model: PE-driven MB weight and asymmetric learning.
    
    This model learns MF values at both stages with asymmetric learning for 
    positive vs. negative prediction errors, and it arbitrates between model-based 
    and model-free control at the first stage using a running estimate of recent 
    surprise (absolute prediction error) to upweight MB control when outcomes are volatile.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; the two aliens available in the reached state) for each trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome on each trial.
    model_parameters : tuple/list
        (alpha, beta, eta, tau_u, w0)
        - alpha in [0,1]: base learning rate for MF updates.
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - eta in [0,1]: asymmetry factor for negative PEs (effective LR = alpha*eta when PE<0).
        - tau_u in [0,1]: update rate for the running surprise signal (exponential smoothing).
        - w0 in [0,1]: baseline weight on MB control; dynamic weight increases with surprise.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, eta, tau_u, w0 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Running surprise (absolute PE) used for arbitration (bounded in [0,1] given reward in [0,1])
    surprise = 0.0

    for t in range(n_trials):
        # Model-based estimate for stage 1 from second-stage MF
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2

        # Dynamic arbitration weight based on recent surprise
        w_t = w0 + (1.0 - w0) * surprise
        w_t = min(max(w_t, 0.0), 1.0)

        # Combine MB and MF values
        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # First-stage policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        exp_q2 = np.exp(beta * (q2_mf[s, :] - np.max(q2_mf[s, :])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and TD updates with asymmetric learning
        r = reward[t]
        delta2 = r - q2_mf[s, a2]
        lr2 = alpha if delta2 >= 0 else alpha * eta
        q2_mf[s, a2] += lr2 * delta2

        # Stage-1 MF update with bootstrapping and propagation of PE (lambda=1 effective)
        boot = q2_mf[s, a2]
        delta1 = boot - q1_mf[a1]
        # Use same asymmetric LR as second stage to maintain consistency with outcome valence
        q1_mf[a1] += lr2 * delta1 + lr2 * delta2

        # Update running surprise (absolute PE) with smoothing tau_u
        surprise = (1.0 - tau_u) * surprise + tau_u * abs(delta2)
        # Bound to [0,1] for numerical safety
        surprise = min(max(surprise, 0.0), 1.0)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll