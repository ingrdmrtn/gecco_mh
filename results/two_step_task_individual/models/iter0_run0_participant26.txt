def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free two-step RL with eligibility trace and first-stage stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (e.g., 0=W/P, 1=S/H) for each trial.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha, beta, w_mb, lambda_et, stickiness]
        - alpha (0..1): Learning rate for both stages.
        - beta (0..10): Inverse-temperature for softmax at both stages.
        - w_mb (0..1): Weight on model-based action values at the first stage (1=fully model-based).
        - lambda_et (0..1): Eligibility trace coupling from second-stage TD error to first-stage value update.
        - stickiness (0..1): Tendency to repeat the previous first-stage action (applied as a bias in softmax).
        
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w_mb, lambda_et, stickiness = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix: rows = first-stage action, cols = next state
    # A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)          # model-free first-stage Q(s1,a1)
    q_stage2_mf = np.zeros((2, 2))     # model-free second-stage Q(state, action2)

    # Stickiness memory (previous first-stage action)
    prev_a1 = None

    for t in range(n_trials):
        # Compute model-based first-stage values from current second-stage MF values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)            # best second-stage value per state
        q_stage1_mb = transition_matrix @ max_q_stage2         # expected value per first-stage action

        # Hybrid value with stickiness bias on first-stage choices
        q1 = (1.0 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += stickiness
        q1_biased = q1 + bias

        # First-stage policy
        q1b = q1_biased - np.max(q1_biased)
        exp_q1 = np.exp(beta * q1b)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy given observed state
        s2 = state[t]
        q2 = q_stage2_mf[s2].copy()
        q2b = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2b)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD updates
        # Second-stage update (standard TD(0))
        delta2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha * delta2

        # First-stage model-free update bootstrapping from second stage with eligibility lambda
        target1 = q_stage2_mf[s2, a2]  # after update
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * lambda_et * delta1

        # Update stickiness memory
        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid RL with learned transition probabilities and second-stage stickiness.
    
    The agent jointly learns:
      - Second-stage rewards (Q2) via TD(0).
      - First-stage model-free values (Q1-MF) via bootstrapping from Q2.
      - Transition probabilities for each spaceship, used for model-based evaluation.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (e.g., 0=W/P, 1=S/H).
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha_q, beta, alpha_t, w_mb, kappa2]
        - alpha_q (0..1): Learning rate for Q-values (both stages).
        - beta (0..10): Inverse-temperature for softmax at both stages.
        - alpha_t (0..1): Learning rate for transition probabilities per spaceship.
        - w_mb (0..1): Weight on model-based first-stage values.
        - kappa2 (0..1): Second-stage choice stickiness (state-specific repeat bias).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_q, beta, alpha_t, w_mb, kappa2 = model_parameters
    n_trials = len(action_1)

    # Initialize values
    q1_mf = np.zeros(2)            # model-free first-stage values
    q2 = np.zeros((2, 2))          # second-stage values per state and action

    # Initialize transition beliefs: p(ship -> X)
    # Prior consistent with task's common transitions
    p_to_X = np.array([0.7, 0.3], dtype=float)  # index 0=A, 1=U; p->X, then p->Y = 1 - p_to_X

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage stickiness memory: last chosen action per state
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        # Model-based evaluation using current transition beliefs
        max_q2 = np.max(q2, axis=1)  # value of best alien on each planet
        # Expected value for choosing A or U given learned transition probs
        # For action 0 (A): E = p_to_X[0]*max_q2[X] + (1-p)*max_q2[Y]
        # For action 1 (U): E = p_to_X[1]*max_q2[X] + (1-p)*max_q2[Y]
        q1_mb = np.empty(2)
        q1_mb[0] = p_to_X[0] * max_q2[0] + (1.0 - p_to_X[0]) * max_q2[1]
        q1_mb[1] = p_to_X[1] * max_q2[0] + (1.0 - p_to_X[1]) * max_q2[1]

        # Hybrid first-stage value
        q1_hyb = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # First-stage policy
        q1h = q1_hyb - np.max(q1_hyb)
        exp_q1 = np.exp(beta * q1h)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with state-specific stickiness
        s2 = state[t]
        a2 = action_2[t]
        bias2 = np.zeros(2)
        if prev_a2[s2] is not None:
            bias2[prev_a2[s2]] += kappa2
        q2_biased = q2[s2] + bias2
        q2b = q2_biased - np.max(q2_biased)
        exp_q2 = np.exp(beta * q2b)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD updates for Q-values
        # Second-stage
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_q * delta2

        # First-stage model-free bootstrapping to the just-updated second-stage value
        target1 = q2[s2, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * delta1

        # Update transition belief for the chosen ship toward the observed state
        # We learn p(ship->X) with a simple delta rule.
        observed_is_X = 1.0 if s2 == 0 else 0.0
        p_to_X[a1] += alpha_t * (observed_is_X - p_to_X[a1])
        # Keep numerical stability inside [0,1]
        p_to_X[a1] = min(1.0, max(0.0, p_to_X[a1]))

        # Update second-stage stickiness memory
        prev_a2[s2] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free RL with asymmetric learning rates and transition-sensitive eligibility.
    
    This model captures model-based-like sensitivity via an eligibility trace that depends
    on whether the transition was common or rare. Learning is purely model-free but weighs
    rare vs common transitions differently when propagating the second-stage TD error back
    to first-stage values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) per trial within observed state.
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of floats
        [alpha_pos, alpha_neg, beta, lambda_common, lambda_rare]
        - alpha_pos (0..1): Learning rate used when reward = 1.
        - alpha_neg (0..1): Learning rate used when reward = 0.
        - beta (0..10): Inverse-temperature for softmax at both stages.
        - lambda_common (0..1): Eligibility for common transitions.
        - lambda_rare (0..1): Eligibility for rare transitions.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, lambda_common, lambda_rare = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure to determine common vs. rare transitions
    # A->X common, A->Y rare; U->Y common, U->X rare.
    def is_common(a1, s2):
        if a1 == 0:  # A
            return s2 == 0  # X
        else:        # U
            return s2 == 1  # Y

    # Values
    q1 = np.zeros(2)         # model-free first-stage Q
    q2 = np.zeros((2, 2))    # second-stage Q per state, action

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage policy
        q1s = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1s)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        q2s = q2[s2] - np.max(q2[s2])
        exp_q2 = np.exp(beta * q2s)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        alpha = alpha_pos if r > 0.5 else alpha_neg

        # Second-stage TD update
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Transition-sensitive eligibility for propagating to first stage
        lam = lambda_common if is_common(a1, s2) else lambda_rare

        # First-stage TD update toward updated second-stage value, scaled by lam
        target1 = q2[s2, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * lam * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll