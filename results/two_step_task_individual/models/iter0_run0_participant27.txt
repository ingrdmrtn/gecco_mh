def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free learner with eligibility trace and perseveration.
    
    This model blends model-based planning from a known transition matrix with
    model-free values, uses an eligibility trace to propagate reward back to
    the first-stage MF values, and includes first-stage perseveration (choice
    stickiness) bias.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state/planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1, e.g., aliens on each planet).
    reward : array-like of float
        Outcome on each trial (e.g., coins; usually 0/1).
    model_parameters : iterable
        Model parameters (all in [0,1] except beta in [0,10]):
        - alpha: learning rate for MF values in both stages [0,1]
        - beta: inverse temperature for softmax at both stages [0,10]
        - w: weight of model-based relative to model-free at stage 1 [0,1]
        - lam: eligibility trace parameter λ for bootstrapping MF(1) from δ2 [0,1]
        - kappa: perseveration bias for repeating last first-stage choice [0,1]
          (added as +kappa to last chosen action and -kappa to the other)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (commonly A->X, U->Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)          # MF values for spaceships A/U
    q_stage2_mf = np.zeros((2, 2))     # MF values for aliens per planet

    prev_a1 = None  # for perseveration

    for t in range(n_trials):
        # Model-based first-stage action values via one-step planning
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF with perseveration bias
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        if prev_a1 is not None:
            stickiness = np.array([-kappa, -kappa])
            stickiness[prev_a1] += 2.0 * kappa  # +kappa to previous, -kappa to other
            q1 = q1 + stickiness

        # Policy for the first choice (softmax)
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Policy for the second choice (softmax on MF values in encountered state)
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD updates
        # Stage-2 MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update: bootstrapping toward value of chosen second-stage action
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Eligibility trace: propagate δ2 back to first-stage MF
        q_stage1_mf[a1] += alpha * lam * delta2

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with uncertainty bonus (UCB-style) and value forgetting, plus perseveration.
    
    This model is primarily model-free but directs second-stage choices with an
    exploration bonus proportional to the estimated uncertainty (standard deviation)
    of each alien's reward. It also includes forgetting (decay) of second-stage
    values to capture nonstationarity and first-stage perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state/planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Outcome on each trial (e.g., 0/1 coins).
    model_parameters : iterable
        Model parameters (all in [0,1] except beta in [0,10]):
        - alpha: learning rate for MF updates [0,1]
        - beta: inverse temperature for softmax choices [0,10]
        - eta: uncertainty bonus weight (UCB coefficient) [0,1]
        - rho: forgetting rate for second-stage Q-values [0,1]
               (applied as Q <- (1 - rho) * Q each trial before update)
        - kappa: perseveration bias at first stage [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, eta, rho, kappa = model_parameters
    n_trials = len(action_1)

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)          # MF values for spaceships
    q_stage2_mf = np.zeros((2, 2))     # MF values for aliens per planet

    # For UCB bonus: track running mean and variance of rewards per (state, action)
    # Using Welford's algorithm variables: count, mean, M2 (sum of squared deviations)
    counts = np.zeros((2, 2))
    means_r = np.zeros((2, 2))
    M2 = np.zeros((2, 2))

    prev_a1 = None

    for t in range(n_trials):
        # Apply forgetting to all second-stage Q-values to capture drift
        q_stage2_mf *= (1.0 - rho)

        # First-stage values are MF bootstrapped from second-stage values
        # (no explicit MB planning here)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)

        q1 = q_stage1_mf.copy()
        # Perseveration at first stage
        if prev_a1 is not None:
            stickiness = np.array([-kappa, -kappa])
            stickiness[prev_a1] += 2.0 * kappa
            q1 = q1 + stickiness

        # First-stage policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with UCB bonus on uncertainty (std dev of rewards)
        s = state[t]
        # Compute standard deviation estimate; if counts < 2, use a small prior variance
        var_est = np.zeros(2)
        for a in range(2):
            if counts[s, a] >= 2:
                var_est[a] = M2[s, a] / (counts[s, a] - 1.0)
            elif counts[s, a] == 1:
                var_est[a] = 0.25  # a conservative prior variance for Bernoulli-like rewards
            else:
                var_est[a] = 0.25  # prior variance before any observation
        std_est = np.sqrt(np.maximum(var_est, 1e-8))

        q2 = q_stage2_mf[s] + eta * std_est
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update running reward statistics for UCB (Welford)
        counts[s, a2] += 1.0
        delta = r - means_r[s, a2]
        means_r[s, a2] += delta / counts[s, a2]
        delta2 = r - means_r[s, a2]
        M2[s, a2] += delta * delta2

        # MF TD updates
        # Stage-2: update chosen alien
        delta2_td = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2_td

        # Stage-1 MF: update chosen spaceship toward realized second-stage opportunity value
        # Use the value of the chosen second-stage action as the bootstrapping target
        target1 = q_stage2_mf[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planning with learned transition model, plus biases.
    
    This model learns both the reward values of second-stage actions and the
    first-stage transition probabilities. The first-stage choice is fully
    model-based, computed as the expected value under the learned transition
    model. Two biases are included: first-stage perseveration and a static
    preference for spaceship A vs U.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state/planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Outcome on each trial (e.g., 0/1 coins).
    model_parameters : iterable
        Model parameters (all in [0,1] except beta in [0,10]):
        - alpha_r: learning rate for second-stage rewards [0,1]
        - alpha_t: learning rate for transition probabilities [0,1]
                   (simple delta rule towards observed state)
        - beta: inverse temperature for softmax choices [0,10]
        - kappa: perseveration bias at first stage [0,1]
        - pi_a: static bias favoring spaceship A vs U [0,1]
                (added as +pi_a to A and -pi_a to U)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_r, alpha_t, beta, kappa, pi_a = model_parameters
    n_trials = len(action_1)

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Learned transition model T[a, s], initialize neutral (uninformative)
    T = np.full((2, 2), 0.5)

    # Second-stage MF reward values
    Q2 = np.zeros((2, 2))

    prev_a1 = None

    for t in range(n_trials):
        # Compute MB first-stage values using current transition model and Q2
        max_Q2 = np.max(Q2, axis=1)  # value of best alien on each planet
        Q1_mb = T @ max_Q2

        # Add biases: perseveration and static bias toward A
        Q1 = Q1_mb.copy()
        if prev_a1 is not None:
            stickiness = np.array([-kappa, -kappa])
            stickiness[prev_a1] += 2.0 * kappa
            Q1 = Q1 + stickiness
        Q1 = Q1 + np.array([pi_a, -pi_a])

        # First-stage policy
        exp_q1 = np.exp(beta * (Q1 - np.max(Q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (greedy-softmax on Q2 in encountered state)
        s = state[t]
        exp_q2 = np.exp(beta * (Q2[s] - np.max(Q2[s])))
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second-stage reward values
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * delta2

        # Update transition model for chosen first-stage action toward observed state
        # Delta rule on categorical: move probability mass toward observed state
        # T[a1, s] <- T[a1, s] + alpha_t * (1 - T[a1, s])
        # T[a1, 1-s] <- T[a1, 1-s] + alpha_t * (0 - T[a1, 1-s])
        T[a1, s] += alpha_t * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] += alpha_t * (0.0 - T[a1, other])

        # Keep rows normalized and within [0,1] bounds (small numeric guard)
        T[a1] = np.clip(T[a1], 1e-6, 1.0 - 1e-6)
        T[a1] = T[a1] / np.sum(T[a1])

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss