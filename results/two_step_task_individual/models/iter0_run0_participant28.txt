def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid MB/MF with eligibility trace and choice stickiness.
    Returns negative log-likelihood of observed first- and second-stage choices.

    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 for spaceship A, 1 for spaceship U).
    state : array-like, shape (n_trials,)
        Second-stage state indices (0 for planet X, 1 for planet Y).
    action_2 : array-like, shape (n_trials,)
        Second-stage choices within state (0 for first alien on that planet, 1 for second alien).
    reward : array-like, shape (n_trials,)
        Received reward on each trial (e.g., 0 or 1 coins).
    model_parameters : list or array-like, length 5
        [alpha, beta, w_mb, lambda_elig, kappa]
        - alpha (learning rate) in [0,1]: TD learning rate for Q updates.
        - beta (inverse temperature) in [0,10]: softmax sensitivity at both stages.
        - w_mb (model-based weight) in [0,1]: mixture between MB and MF action values at stage 1.
        - lambda_elig (eligibility trace) in [0,1]: propagates stage-2 prediction error to stage-1.
        - kappa (stickiness) in [0,1]: bias toward repeating previous action at each stage.

    Notes
    -----
    - Transition matrix is fixed: A->X and U->Y with 0.7; rare transition 0.3.
    - Stage-1 policy uses mixture Q1 = w*MB + (1-w)*MF plus stickiness bias.
    - Stage-2 policy is standard softmax over Q2 with stickiness bias.
    """
    alpha, beta, w_mb, lambda_elig, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Likelihood tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)           # MF values for spaceships
    q_stage2_mf = np.zeros((2, 2))      # MF values for aliens per planet

    # Stickiness trackers (previous choices)
    prev_a1 = None
    prev_a2 = [None, None]  # track last action within each second-stage state

    for t in range(n_trials):
        s = state[t]

        # Model-based component for stage 1: expected max over next-stage values
        max_q2 = np.max(q_stage2_mf, axis=1)  # shape (2,)
        q_stage1_mb = transition_matrix @ max_q2  # shape (2,)

        # Mixture of MB and MF for stage 1
        q1_mix = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # Add stickiness to stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa

        # Softmax for stage-1 policy
        logits1 = beta * q1_mix + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness within state
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa
        logits2 = beta * q_stage2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Compute TD errors
        # Stage-2 TD error
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        # Stage-1 bootstrapped target: value of observed second-stage chosen action
        target1 = q_stage2_mf[s, a2]
        delta1 = target1 - q_stage1_mf[a1]

        # Update MF values
        q_stage2_mf[s, a2] += alpha * delta2
        # Stage-1 MF gets both its own delta and an eligibility-trace backprop of stage-2 delta
        q_stage1_mf[a1] += alpha * (delta1 + lambda_elig * delta2)

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Learned-transitions model-based RL with value forgetting and perseveration.
    Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 or 1).
    state : array-like, shape (n_trials,)
        Observed second-stage states (0 or 1).
    action_2 : array-like, shape (n_trials,)
        Second-stage actions (0 or 1).
    reward : array-like, shape (n_trials,)
        Reward outcomes (e.g., 0/1).
    model_parameters : list or array-like, length 5
        [alpha_q, beta, alpha_t, rho, phi]
        - alpha_q in [0,1]: learning rate for Q-value updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - alpha_t in [0,1]: learning rate for transition probabilities T.
        - rho in [0,1]: perseveration strength added to previously chosen action logits (both stages).
        - phi in [0,1]: value forgetting/decay toward 0 applied each trial to Q-values.

    Notes
    -----
    - Transition matrix is learned online (row-stochastic), initialized to [0.5,0.5] per action.
    - Stage-1 uses purely model-based Q computed from learned transitions and current Q2.
    - Perseveration adds a bias to repeat previous action at each stage.
    - Forgetting applies to both Q1-MF proxy (not used for choice) and Q2 to keep values bounded and capture drift.
    """
    alpha_q, beta, alpha_t, rho, phi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transitions T[a, s'] (rows sum to 1)
    T = np.full((2, 2), 0.5)

    # Values: we only need Q2 for MB planning; keep Q1_mf as a bootstrap target (not for choice)
    q_stage2 = np.zeros((2, 2))
    q_stage1_mf = np.zeros(2)  # internal MF trace for learning targets

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2 = [None, None]

    for t in range(n_trials):
        s = state[t]

        # Model-based Q1 from learned transitions and current Q2
        max_q2 = np.max(q_stage2, axis=1)  # value per state
        q1_mb = T @ max_q2  # expected value per first-stage action

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho

        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration within state
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += rho
        logits2 = beta * q_stage2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = reward[t]

        # TD update at stage 2
        # Apply forgetting to all Q-values before updating (decay toward 0)
        q_stage2 *= (1.0 - phi)
        q_stage1_mf *= (1.0 - phi)

        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_q * delta2

        # Update stage-1 MF trace toward the realized second-stage chosen value
        target1 = q_stage2[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_q * delta1

        # Learn transitions T with a simple delta rule toward the observed next state
        # One-hot target for s given a1
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_t * (target - T[a1, sp])
        # Re-normalize to avoid drift (kept row-stochastic)
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum
        else:
            T[a1] = np.array([0.5, 0.5])

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Surprise-gated arbitration between model-based and model-free control with eligibility trace.
    Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 or 1).
    state : array-like, shape (n_trials,)
        Second-stage states (0 or 1).
    action_2 : array-like, shape (n_trials,)
        Second-stage actions (0 or 1).
    reward : array-like, shape (n_trials,)
        Rewards (e.g., 0/1).
    model_parameters : list or array-like, length 5
        [alpha, beta, w0, eta, lambda_elig]
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w0 in [0,1]: baseline MB weight (mixture at stage 1 when surprise is neutral).
        - eta in [0,1]: sensitivity of arbitration to transition surprise (rare transitions reduce MB weight).
        - lambda_elig in [0,1]: eligibility trace for propagating stage-2 PE to stage-1 MF.

    Notes
    -----
    - Fixed transition matrix with common = 0.7, rare = 0.3.
    - Surprise for trial t is defined as 1 - P(s_t | a1_t) given the fixed transition matrix.
      This makes surprise higher for rare transitions.
    - The effective MB weight each trial is:
        w_t = sigmoid( logit(w0) + (1 - 2*I_rare) * (-eta_scale) )
      Implemented equivalently as:
        w_t = clamp( w0 * (1 - eta*surprise) + (1 - w0) * eta * (1 - surprise) ), but here we use a simple linear gate:
        w_t = clip( w0 * (1 - eta*surprise), 0, 1 )
      So higher surprise lowers the MB weight.
    """
    alpha, beta, w0, eta, lambda_elig = model_parameters
    n_trials = len(action_1)

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # No explicit stickiness here; arbitration alone explains sequential effects.

    for t in range(n_trials):
        s = state[t]

        # Compute model-based value for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Compute surprise of the realized transition given chosen action
        a1 = action_1[t]
        prob_s = T[a1, s]
        surprise = 1.0 - prob_s  # 0.3 for common, 0.7 for rare

        # Trial-specific arbitration weight (MB weight decreases with surprise)
        w_t = w0 * (1.0 - eta * surprise)
        # Clip to [0,1] to respect bounds
        if w_t < 0.0:
            w_t = 0.0
        elif w_t > 1.0:
            w_t = 1.0

        # Mixture policy for stage 1
        q1_mix = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage-1 choice probability
        logits1 = beta * q1_mix
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        a2 = action_2[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF TD update toward realized second-stage chosen value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        # Combine direct stage-1 delta with eligibility-trace backprop of stage-2 error
        q1_mf[a1] += alpha * (delta1 + lambda_elig * delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll