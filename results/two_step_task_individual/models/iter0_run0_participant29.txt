def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and choice stickiness.
    
    A standard two-step hybrid model that blends a model-based (MB) planner with a
    model-free (MF) learner, uses an eligibility trace to propagate second-stage
    prediction errors back to the first stage, and includes a perseveration (stickiness)
    bias at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1) per trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien index within each planet) per trial.
    reward : array-like of float (typically 0 or 1)
        Reward received on each trial.
    model_parameters : list or array
        [alpha, beta, w, lam, kappa]
        - alpha in [0,1]: learning rate for MF Q-value updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w in [0,1]: weight of model-based relative to model-free at stage 1.
        - lam in [0,1]: eligibility trace weighting backpropagating delta at stage 2 to stage 1.
        - kappa in [0,1]: choice stickiness weight added to the previously chosen action (both stages).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed, known transition structure: A->X and U->Y are common (0.7), rare otherwise (0.3)
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X, Y]
                                  [0.3, 0.7]]) # from U to [X, Y]

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)        # for actions A,U
    q_stage2_mf = np.zeros((2, 2))   # for states X,Y and actions within each state

    # Choice stickiness trackers (previous chosen action indices)
    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        # Compute model-based action values for stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)               # value of best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2            # expected value per spaceship

        # Blend MB and MF and add stage-1 stickiness bias
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        if prev_a1 is not None:
            stick1 = np.zeros(2)
            stick1[prev_a1] = 1.0
            q1 = q1 + kappa * stick1

        # Softmax to get probability of the observed first-stage choice
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in the reached state with stickiness
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        prev_a2 = prev_a2_by_state[s]
        if prev_a2 is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2] = 1.0
            q2 = q2 + kappa * stick2

        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD errors
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s, a2]

        # MF updates
        q_stage1_mf[a1] += alpha * (delta_stage1 + lam * delta_stage2)   # with eligibility trace
        q_stage2_mf[s, a2] += alpha * delta_stage2

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Arbitrated hybrid with transition surprise gating and stickiness.
    
    A hybrid model-based/model-free agent where the MB weight is dynamically
    increased on rare transitions (transition surprise). Includes MF learning
    with eligibility trace and stage-specific perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1) per trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (typically 0 or 1)
        Reward received on each trial.
    model_parameters : list or array
        [alpha, beta, w0, eta, kappa]
        - alpha in [0,1]: learning rate for MF Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w0 in [0,1]: baseline model-based weight at stage 1.
        - eta in [0,1]: surprise gain; MB weight increases by eta on rare transitions.
        - kappa in [0,1]: choice stickiness weight (both stages).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w0, eta, kappa = model_parameters
    n_trials = len(action_1)

    # Known transition structure (common = 0.7, rare = 0.3)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        # Model-based action values from current MF stage-2 estimates
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Surprise-gated MB weight: add eta on rare transitions from the previous choice's perspective
        # On trial t, we don't yet know transition; we will apply w_t on current trial using
        # the rarity defined by the actually observed state for the current a1.
        a1 = action_1[t]
        s = state[t]
        rare = 1.0 if s != a1 else 0.0  # A commonly goes to X (0), U to Y (1)
        w_t = w0 + eta * rare
        # Clamp to [0,1]
        if w_t < 0.0:
            w_t = 0.0
        elif w_t > 1.0:
            w_t = 1.0

        # Blend MB and MF for stage 1 plus stickiness
        q1 = w_t * q_stage1_mb + (1.0 - w_t) * q_stage1_mf
        if prev_a1 is not None:
            stick1 = np.zeros(2)
            stick1[prev_a1] = 1.0
            q1 = q1 + kappa * stick1

        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 choice probabilities in reached state with stickiness
        q2 = q_stage2_mf[s].copy()
        prev_a2 = prev_a2_by_state[s]
        if prev_a2 is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2] = 1.0
            q2 = q2 + kappa * stick2

        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD errors
        delta_stage1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        delta_stage2 = r - q_stage2_mf[s, a2]

        # MF learning with eligibility trace
        q_stage1_mf[a1] += alpha * (delta_stage1 + delta_stage2)
        q_stage2_mf[s, a2] += alpha * delta_stage2

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Asymmetric model-free with eligibility trace and perseveration.
    
    A purely model-free agent with separate learning rates for positive and
    negative outcomes, an eligibility trace to propagate reward back to the
    first stage, and perseveration bias at both stages. No model-based planning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (typically 0 or 1)
        Reward received on each trial.
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta, lam, rho]
        - alpha_pos in [0,1]: learning rate when stage-2 prediction error is positive.
        - alpha_neg in [0,1]: learning rate when stage-2 prediction error is negative.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - lam in [0,1]: eligibility trace weight to update stage-1 values from stage-2 errors.
        - rho in [0,1]: perseveration (stickiness) weight added to previously chosen actions (both stages).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, lam, rho = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values only
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        # Stage 1 policy with stickiness
        q1 = q_stage1.copy()
        if prev_a1 is not None:
            stick1 = np.zeros(2)
            stick1[prev_a1] = 1.0
            q1 = q1 + rho * stick1

        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state with stickiness
        s = state[t]
        q2 = q_stage2[s].copy()
        prev_a2 = prev_a2_by_state[s]
        if prev_a2 is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2] = 1.0
            q2 = q2 + rho * stick2

        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD errors and asymmetric learning rate
        delta2 = r - q_stage2[s, a2]
        alpha2 = alpha_pos if delta2 >= 0.0 else alpha_neg

        # Update stage 2
        q_stage2[s, a2] += alpha2 * delta2

        # Update stage 1 with eligibility trace using the same asymmetric learning rate
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += alpha2 * (delta1 + lam * (r - q_stage2[s, a2]))

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss