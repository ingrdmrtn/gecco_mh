def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free RL with eligibility trace and perseveration.
    
    The agent blends a learned model-based (MB) plan using a fixed transition 
    structure with a model-free (MF) cached value, and applies an eligibility 
    trace to propagate second-stage prediction errors to first-stage values. 
    Perseveration biases both stages toward repeating the previous action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=planet X, 1=planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the observed state per trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, w, lam, persev]
        - alpha in [0,1]: learning rate for MF updates (both stages).
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - w in [0,1]: weight on MB values at the first stage (1-w on MF).
        - lam in [0,1]: eligibility trace propagating stage-2 RPE to stage-1 MF.
        - persev in [0,1]: perseveration strength added as a bias for repeating
                           the immediately previous action at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, lam, persev = model_parameters
    n_trials = len(action_1)

    # Fixed common transition structure (rows: ships A/U; cols: planets X/Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Choice probabilities per trial
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)        # first stage: 2 actions (A,U)
    q2 = np.zeros((2, 2))      # second stage: 2 states x 2 actions

    # Perseveration biases (last actions)
    last_a1 = None
    last_a2_by_state = [None, None]

    for t in range(n_trials):

        # Model-based action values at first stage: T @ max_a Q2(s,a)
        max_q2 = np.max(q2, axis=1)                      # size 2 (states)
        q1_mb = transition_matrix @ max_q2               # size 2 (actions)

        # Perseveration bias vectors
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += persev

        # First-stage policy: softmax over w*MB + (1-w)*MF + perseveration
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf + bias1
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy at observed state with perseveration within state
        s = state[t]
        bias2 = np.zeros(2)
        if last_a2_by_state[s] is not None:
            bias2[last_a2_by_state[s]] += persev

        q2_bias = q2[s, :] + bias2
        exp_q2 = np.exp(beta * (q2_bias - np.max(q2_bias)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcomes and TD errors
        r = reward[t]
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF TD to value of reached second-stage action
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1 + alpha * lam * delta2

        # Update perseveration memory
        last_a1 = a1
        last_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free RL with asymmetric learning rates, forgetting, and stickiness.
    
    The agent uses only model-free values, with separate learning rates for 
    positive vs. negative outcomes, a uniform forgetting/decay on all Q-values,
    and a first-stage stickiness bias to repeat the previous first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=planet X, 1=planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the observed state per trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of floats
        [alpha_pos, alpha_neg, beta, rho, kappa]
        - alpha_pos in [0,1]: learning rate when reward prediction error >= 0.
        - alpha_neg in [0,1]: learning rate when reward prediction error < 0.
        - beta in [0,10]: inverse temperature for softmax choices at both stages.
        - rho in [0,1]: forgetting rate applied to all Q-values each trial
                       (higher rho => faster decay toward 0).
        - kappa in [0,1]: first-stage stickiness bias (added to the last chosen
                          first-stage action).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, rho, kappa = model_parameters
    n_trials = len(action_1)

    # Model-free Q-values
    q1 = np.zeros(2)          # first stage actions
    q2 = np.zeros((2, 2))     # second stage (state x action)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None  # for stickiness

    for t in range(n_trials):
        # Apply forgetting (decay) to all Q-values
        q1 *= (1.0 - rho)
        q2 *= (1.0 - rho)

        # First-stage policy with stickiness bias
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        q1_bias = q1 + bias1
        exp_q1 = np.exp(beta * (q1_bias - np.max(q1_bias)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        exp_q2 = np.exp(beta * (q2[s, :] - np.max(q2[s, :])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and TD errors
        r = reward[t]
        delta2 = r - q2[s, a2]
        lr2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * delta2

        # First-stage MF backup toward second-stage value; use asymmetric LR
        delta1 = q2[s, a2] - q1[a1]
        lr1 = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1[a1] += lr1 * delta1

        # Update stickiness memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Adaptive transition learning with surprise-weighted arbitration (MBâ€“MF).
    
    The agent learns the transition matrix online and adjusts the arbitration
    weight between model-based and model-free control based on trial-wise 
    transition surprise. Surprise is defined as 1 - P(observed transition).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=planet X, 1=planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the observed state per trial.
    reward : array-like of float (typically 0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, tau, w0, s_weight]
        - alpha in [0,1]: learning rate for value updates (MF, both stages).
        - beta in [0,10]: inverse temperature for softmax choices.
        - tau in [0,1]: learning rate for updating the transition matrix rows.
        - w0 in [0,1]: baseline weight on MB at first stage.
        - s_weight in [0,1]: slope scaling that increases MB weight with surprise.
                             Effective weight per trial is w_t = clip(w0 + s_weight * surprise, 0, 1).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, tau, w0, s_weight = model_parameters
    n_trials = len(action_1)

    # Initialize transition matrix (rows: actions A/U, cols: states X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]

        # Compute MB values using current transition belief
        max_q2 = np.max(q2, axis=1)     # size 2
        q1_mb = T @ max_q2              # size 2

        # Surprise from the transition actually observed
        p_obs = T[a1, s]
        surprise = 1.0 - p_obs
        w_t = np.clip(w0 + s_weight * surprise, 0.0, 1.0)

        # First-stage policy: softmax over mixture of MB and MF
        q1_mix = w_t * q1_mb + (1.0 - w_t) * q1_mf
        exp_q1 = np.exp(beta * (q1_mix - np.max(q1_mix)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy at observed state
        exp_q2 = np.exp(beta * (q2[s, :] - np.max(q2[s, :])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update transition matrix row for chosen action toward observed state
        # One-hot target for the observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        # Row-wise update keeping normalization implicitly via complementary updates
        T[a1, :] = (1.0 - tau) * T[a1, :] + tau * target
        # Ensure numerical stability/normalization
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

        # Value learning (model-free)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Propagate to first-stage MF value (SARSA-style toward the taken a2 value plus RPE)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1 + alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll