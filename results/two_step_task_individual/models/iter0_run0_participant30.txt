def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free RL with eligibility traces and separate inverse temperatures.
    
    This model blends a model-based (transition-aware) planner with a model-free learner.
    Second-stage rewards drive updates at both stages via an eligibility trace.
    Known transitions: Spaceship A commonly -> X, U commonly -> Y.
    
    Parameters (tuple):
    - alpha: learning rate for Q-updates at both stages. Bounds [0,1].
    - beta1: inverse temperature for first-stage softmax. Bounds [0,10].
    - beta2: inverse temperature for second-stage softmax. Bounds [0,10].
    - w: mixing weight between model-based and model-free values at first stage (0=model-free, 1=model-based). Bounds [0,1].
    - lam: eligibility trace parameter controlling how much second-stage RPE updates stage-1 values. Bounds [0,1].
    
    Inputs:
    - action_1: array-like, choices at stage 1 (0=A, 1=U).
    - state: array-like, observed second-stage state (0=X, 1=Y).
    - action_2: array-like, choices at stage 2 within the observed state (0 or 1).
    - reward: array-like, outcome (typically 0/1 coins).
    - model_parameters: tuple/list of (alpha, beta1, beta2, w, lam).
    
    Returns:
    - Negative log-likelihood of the observed sequence of stage-1 and stage-2 choices.
    """
    alpha, beta1, beta2, w, lam = model_parameters
    n_trials = len(action_1)

    # Known transition structure: rows = first-stage action, cols = state
    # A -> X common, U -> Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)         # Q for first-stage actions (A,U)
    q_stage2_mf = np.zeros((2, 2))    # Q for second-stage actions per state: Q[state, action2]

    for t in range(n_trials):
        # Model-based planning at stage 1 using current MF estimates of stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)            # best action value in each state
        q_stage1_mb = transition_matrix @ max_q_stage2        # expected value for A and U

        # Hybrid action values at stage 1
        q1_hybrid = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Stage-1 policy and likelihood
        q1 = q1_hybrid
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy and likelihood for the reached state
        s = state[t]
        q2 = q_stage2_mf[s]
        q2c = q2 - np.max(q2)
        exp_q2 = np.exp(beta2 * q2c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Temporal-difference errors
        # Stage-1 bootstraps off current stage-2 value of chosen action
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # Stage-2 TD error from reward
        delta2 = r - q_stage2_mf[s, a2]

        # Update model-free values
        # Stage 1: direct update toward stage-2 value + eligibility-trace from reward
        q_stage1_mf[a1] += alpha * (delta1 + lam * delta2)
        # Stage 2: standard TD update
        q_stage2_mf[s, a2] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based RL with learned transition probabilities, model-free second-stage learning, and choice stickiness.
    
    The agent learns each spaceship's transition probabilities via a simple delta rule
    and plans using the learned transition model. Second-stage rewards are learned model-free.
    A stickiness parameter captures perseveration at both stages.
    
    Parameters (tuple):
    - alphaQ: learning rate for second-stage Q-values. Bounds [0,1].
    - alphaT: learning rate for transition probabilities T(a -> s). Bounds [0,1].
    - beta: inverse temperature for both stages. Bounds [0,10].
    - kappa1: first-stage choice stickiness weight (bias to repeat last first-stage action). Bounds [0,1].
    - kappa2: second-stage choice stickiness weight (bias to repeat last second-stage action within a state). Bounds [0,1].
    
    Inputs:
    - action_1: array-like, choices at stage 1 (0=A, 1=U).
    - state: array-like, observed second-stage state (0=X, 1=Y).
    - action_2: array-like, choices at stage 2 within the observed state (0 or 1).
    - reward: array-like, outcome (typically 0/1 coins).
    - model_parameters: tuple/list of (alphaQ, alphaT, beta, kappa1, kappa2).
    
    Returns:
    - Negative log-likelihood of the observed sequence of stage-1 and stage-2 choices.
    """
    alphaQ, alphaT, beta, kappa1, kappa2 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T[a, s]; start agnostic (0.5/0.5)
    T = np.full((2, 2), 0.5)

    # Second-stage model-free values
    Q2 = np.zeros((2, 2))

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness history
    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        # Model-based action values at stage 1 from learned transitions
        maxQ_by_state = np.max(Q2, axis=1)          # value of best action per state
        Q1_mb = T @ maxQ_by_state                   # expected value per first-stage action

        # Add first-stage stickiness bias
        pref1 = Q1_mb.copy()
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] = 1.0
            pref1 = pref1 + kappa1 * bias

        # Stage-1 policy and likelihood
        pref1c = pref1 - np.max(pref1)
        probs1 = np.exp(beta * pref1c)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in the reached state with stickiness
        s = state[t]
        pref2 = Q2[s].copy()
        prev_a2 = prev_a2_by_state[s]
        if prev_a2 is not None:
            bias2 = np.zeros(2)
            bias2[prev_a2] = 1.0
            pref2 = pref2 + kappa2 * bias2

        pref2c = pref2 - np.max(pref2)
        probs2 = np.exp(beta * pref2c)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage model-free Q-values
        Q2[s, a2] += alphaQ * (r - Q2[s, a2])

        # Learn transitions: update only the row for the chosen first-stage action a1
        # Move the probability mass toward the actually observed state s
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alphaT * (target - T[a1, sp])
        # Ensure numerical sanity (row sums ~ 1); small drift may accumulate otherwise
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, :] = T[a1, :] / row_sum

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Prospect utility with entropy-adaptive exploration and value decay (forgetting).
    
    The agent transforms rewards via a concave utility (risk sensitivity),
    uses a model-based planner with known transitions, and adapts exploration
    based on current policy entropy. Values decay over time to capture forgetting.
    
    Parameters (tuple):
    - alpha: learning rate for Q-updates at both stages. Bounds [0,1].
    - beta0: baseline inverse temperature before entropy adjustment. Bounds [0,10].
    - rho: utility curvature; u(r) = r**rho (rho<=1 is risk-averse for 0/1 rewards). Bounds [0,1].
    - tau: entropy sensitivity scaling the reduction of beta by policy entropy. Bounds [0,1].
    - phi: value decay (forgetting) applied each trial to all Q-values. Bounds [0,1].
    
    Inputs:
    - action_1: array-like, choices at stage 1 (0=A, 1=U).
    - state: array-like, observed second-stage state (0=X, 1=Y).
    - action_2: array-like, choices at stage 2 within the observed state (0 or 1).
    - reward: array-like, outcome (typically 0/1 coins).
    - model_parameters: tuple/list of (alpha, beta0, rho, tau, phi).
    
    Returns:
    - Negative log-likelihood of the observed sequence of stage-1 and stage-2 choices.
    """
    alpha, beta0, rho, tau, phi = model_parameters
    n_trials = len(action_1)

    # Known transition structure (model-based)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    Q1_mf = np.zeros(2)        # model-free stage-1 values
    Q2 = np.zeros((2, 2))      # stage-2 action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    ln2 = np.log(2.0)

    for t in range(n_trials):
        # Apply global decay (forgetting) before computing policies
        Q1_mf *= (1.0 - phi)
        Q2 *= (1.0 - phi)

        # Model-based component for stage 1 from current Q2
        maxQ_by_state = np.max(Q2, axis=1)
        Q1_mb = T @ maxQ_by_state

        # Combine MB and MF by simple summation (both contribute, implicitly with equal weight)
        Q1 = Q1_mb + Q1_mf

        # Entropy-adaptive inverse temperature for stage 1
        q1c = Q1 - np.max(Q1)
        pi1_unt = np.exp(beta0 * q1c)
        pi1 = pi1_unt / np.sum(pi1_unt)
        H1 = -np.sum(pi1 * (np.log(pi1 + 1e-12)))  # natural entropy
        beta1_eff = beta0 * (1.0 - tau * (H1 / ln2))  # reduce beta in high-entropy regimes

        # Final stage-1 policy with adjusted beta
        q1c = Q1 - np.max(Q1)
        p1_unt = np.exp(beta1_eff * q1c)
        p1 = p1_unt / np.sum(p1_unt)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with its own entropy-adjusted temperature (uses state-conditional Q2)
        s = state[t]
        q2s = Q2[s]
        q2c = q2s - np.max(q2s)
        pi2_unt = np.exp(beta0 * q2c)
        pi2 = pi2_unt / np.sum(pi2_unt)
        H2 = -np.sum(pi2 * (np.log(pi2 + 1e-12)))
        beta2_eff = beta0 * (1.0 - tau * (H2 / ln2))

        q2c = q2s - np.max(q2s)
        p2_unt = np.exp(beta2_eff * q2c)
        p2 = p2_unt / np.sum(p2_unt)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Prospect-like utility transform
        r = reward[t]
        u = (r ** rho)

        # TD learning
        # Stage 2 update toward utility
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage 1 MF update toward current stage-2 chosen action value (SARSA(0)-like bootstrap)
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll