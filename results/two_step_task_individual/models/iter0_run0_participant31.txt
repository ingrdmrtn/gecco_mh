def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and first-stage perseveration.
    
    The model blends model-based (MB) action values computed from a known transition
    structure with model-free (MF) values learned via temporal-difference (TD).
    An eligibility trace propagates second-stage reward prediction errors to the
    first-stage MF values, and a perseveration term biases repeating the last
    first-stage action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (planet) reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) on each trial (state-specific alien indices 0/1).
    reward : array-like of float
        Reward (coins) received on each trial (typically 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha, beta, w, lam, kappa]
        - alpha (in [0,1]): Learning rate for MF updates at both stages.
        - beta (in [0,10]): Inverse temperature for softmax at both stages.
        - w (in [0,1]): Weight of model-based values at first stage (1=MB, 0=MF).
        - lam (in [0,1]): Eligibility trace propagating stage-2 RPE to stage-1 MF.
        - kappa (in [0,1]): First-stage perseveration bias added to repeating the last first-stage action.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Known transition structure: rows=first-stage actions, cols=states
    # A->X common (0.7), U->Y common (0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)          # MF Q at stage 1
    q_stage2_mf = np.zeros((2, 2))     # MF Q at stage 2: rows=state, cols=action

    prev_a1 = None  # for perseveration

    for t in range(n_trials):
        # Model-based Q at stage 1 from current stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)    # value of each state
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Perseveration bias (only boosts last chosen action)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa

        # Combine MB and MF for first-stage policy
        q1 = w * q_stage1_mb + (1 - w) * q_stage1_mf + bias1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2_mf[s, :]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 TD update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update via eligibility trace from stage 2 RPE
        q_stage1_mf[a1] += alpha * lam * delta2

        # Update perseveration memory
        prev_a1 = a1

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-learning hybrid with certainty-weighted arbitration and separate stage temperatures.
    
    This model learns the first-stage transition function and uses it to compute
    model-based values. Arbitration between model-based (MB) and model-free (MF)
    action values at the first stage is dynamic and depends on the certainty of
    the learned transition for each action: more certain transitions increase MB weight.
    Stage-1 and stage-2 choices have separate softmax inverse temperatures.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (planet) reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) on each trial (state-specific alien indices 0/1).
    reward : array-like of float
        Reward (coins) received on each trial (typically 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha_r, alpha_t, beta1, beta2, phi]
        - alpha_r (in [0,1]): Reward learning rate for Q-values (both stages).
        - alpha_t (in [0,1]): Transition learning rate for P(state | action_1).
        - beta1 (in [0,10]): Inverse temperature for first-stage softmax.
        - beta2 (in [0,10]): Inverse temperature for second-stage softmax.
        - phi (in [0,1]): Arbitration sensitivity; higher values weight MB more strongly when transitions are certain.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_t, beta1, beta2, phi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model to uniform uncertainty
    T = np.full((2, 2), 0.5)  # rows=first-stage action, cols=state

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Compute MB Q for stage 1 using current transition estimates
        max_q_s2 = np.max(q_stage2, axis=1)  # value of each state
        q1_mb = T @ max_q_s2

        # Dynamic arbitration weight per action based on transition certainty
        # For each action a, define certainty c_a = 1 - 4*p*(1-p), where p=max T[a,:]
        # c_a in [0,1], max when distribution is peaked (certain), min at p=0.5
        cert = np.zeros(2)
        for a in range(2):
            p_peak = max(T[a, 0], T[a, 1])
            cert[a] = 1.0 - 4.0 * p_peak * (1.0 - p_peak)

        # Map certainty into MB weight w_a in [0,1]; when phi=0, w_a=0.5 (equal arbitration)
        w_a = 0.5 * (1.0 - phi) + phi * cert  # elementwise

        # Combine MB and MF per action
        q1 = w_a * q1_mb + (1.0 - w_a) * q_stage1_mf

        # First-stage policy
        exp_q1 = np.exp(beta1 * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2[s, :]
        exp_q2 = np.exp(beta2 * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # Transition learning: update row a1 toward one-hot of observed state s
        T[a1, s] += alpha_t * (1.0 - T[a1, s])
        other_s = 1 - s
        T[a1, other_s] += alpha_t * (0.0 - T[a1, other_s])

        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * delta2

        # Stage-1 MF TD update toward observed stage-2 action value
        target1 = q_stage2[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Risk/transition-sensitive MF with forgetting and state-specific second-stage stickiness.
    
    A purely model-free (MF) learner with:
    - Global forgetting toward neutral values at both stages.
    - Credit assignment from stage-2 reward prediction errors to stage-1 MF values
      weighted by whether the transition was common or rare (controlled by rho).
    - State-specific perseveration (stickiness) at stage 2.

    Common/rare is determined by the taskâ€™s fixed mapping (A->X, U->Y). Credit
    is weighted by rho for common transitions and by (1-rho) for rare transitions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (planet) reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) on each trial (state-specific alien indices 0/1).
    reward : array-like of float
        Reward (coins) received on each trial (typically 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha, beta, rho, f, k2]
        - alpha (in [0,1]): Learning rate for MF value updates.
        - beta (in [0,10]): Inverse temperature for softmax at both stages.
        - rho (in [0,1]): Credit weighting toward common transitions (1=only common, 0=only rare).
        - f (in [0,1]): Forgetting rate toward 0.5 each trial for all Q-values.
        - k2 (in [0,1]): State-specific second-stage stickiness added to repeating the last action in that state.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, rho, f, k2 = model_parameters
    n_trials = len(action_1)

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values (MF)
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # State-specific stickiness memory for stage 2
    prev_a2_by_state = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        # Apply forgetting toward 0.5 baseline (neutral value) before acting
        q_stage1 = (1.0 - f) * q_stage1 + f * 0.5
        q_stage2 = (1.0 - f) * q_stage2 + f * 0.5

        # First-stage policy (pure MF)
        exp_q1 = np.exp(beta * (q_stage1 - np.max(q_stage1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with state-specific stickiness
        s = state[t]
        q2 = q_stage2[s, :].copy()
        if prev_a2_by_state[s] != -1:
            q2[prev_a2_by_state[s]] += k2
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 TD
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Determine common vs rare transition based on fixed mapping: A->X (0->0), U->Y (1->1)
        is_common = 1.0 if s == a1 else 0.0
        weight = rho * is_common + (1.0 - rho) * (1.0 - is_common)

        # Stage-1 MF update weighted by transition type
        q_stage1[a1] += alpha * weight * delta2

        # Update stickiness memory for stage 2
        prev_a2_by_state[s] = a2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik