def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and perseveration.
    Computes the negative log-likelihood of observed choices in a two-step task.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial: for planet X, 0=W,1=S; for planet Y, 0=P,1=H.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of floats
        (alpha, beta, w, lam, kappa)
        - alpha in [0,1]: learning rate for value updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w in [0,1]: weight of model-based vs. model-free control at stage 1.
        - lam in [0,1]: eligibility trace strength to backpropagate reward to stage-1 MF.
        - kappa in [0,1]: perseveration strength to repeat last action (both stages).

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows = action_1 (A,U), cols = state (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities for chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)           # model-free stage-1 Q
    q_stage2_mf = np.full((2, 2), 0.5)  # model-free stage-2 Q for each planet/action

    prev_a1 = None
    prev_a2 = {0: None, 1: None}  # track perseveration separately for each planet

    for t in range(n_trials):

        # Model-based plan: expected value of choosing each spaceship via transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2  # MB values for [A,U]

        # Perseveration features
        persev1 = np.zeros(2)
        if prev_a1 is not None:
            persev1[prev_a1] = kappa

        # Stage-1 action probabilities (hybrid MB/MF + perseveration)
        q1_combined = w * q_stage1_mb + (1 - w) * q_stage1_mf + persev1
        exp_q1 = np.exp(beta * (q1_combined - np.max(q1_combined)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 action probabilities (MF + perseveration on the encountered planet)
        s = state[t]
        persev2 = np.zeros(2)
        if prev_a2[s] is not None:
            persev2[prev_a2[s]] = kappa

        q2_pref = q_stage2_mf[s] + persev2
        exp_q2 = np.exp(beta * (q2_pref - np.max(q2_pref)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 MF update (chosen alien at encountered planet)
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace:
        # 1) bootstrap toward second-stage chosen-action value (TD(0) toward q2)
        td_boot = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # 2) eligibility trace to propagate actual reward prediction error
        elig = lam * delta2
        q_stage1_mf[a1] += alpha * (td_boot + elig)

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planner with learned transitions, lapses, and a stage-1 choice bias.
    Computes the negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of floats
        (alpha, beta, tau, bias, epsilon)
        - alpha in [0,1]: learning rate for stage-2 reward values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - tau in [0,1]: transition learning rate toward observed planets.
        - bias in [0,1]: additive bias favoring spaceship A (action 0) at stage 1.
        - epsilon in [0,1]: lapse rate mixing uniform choice noise into both stages.

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, tau, bias, epsilon = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T (rows: action_1; cols: state)
    T = np.full((2, 2), 0.5)

    # Stage-2 reward value estimates (MF critic used by the MB planner)
    q_stage2 = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        # Model-based evaluation at stage 1: expect max value on next states
        max_q2 = np.max(q_stage2, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        # Stage-1 softmax with bias for action 0 and lapse
        pref1 = q1_mb.copy()
        pref1[0] += bias  # bias pushes toward spaceship A
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        soft1 = exp1 / np.sum(exp1)
        probs_1 = (1 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax on encountered planet with lapse
        s = state[t]
        pref2 = q_stage2[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        soft2 = exp2 / np.sum(exp2)
        probs_2 = (1 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update transition model toward the observed state for the chosen action
        one_hot_s = np.array([1.0 if j == s else 0.0 for j in range(2)])
        T[a1] += tau * (one_hot_s - T[a1])

        # Update stage-2 reward value for the chosen alien
        q_stage2[s, a2] += alpha * (r - q_stage2[s, a2])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free with asymmetric learning, transition-dependent credit, and forgetting.
    Computes the negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of floats
        (alpha_pos, alpha_neg, beta, eta, f)
        - alpha_pos in [0,1]: learning rate when reward = 1 at stage 2.
        - alpha_neg in [0,1]: learning rate when reward = 0 at stage 2.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - eta in [0,1]: reduces stage-1 credit assignment after rare transitions
                       (0 = no reduction; 1 = no learning after rare).
        - f in [0,1]: forgetting toward priors (Q1 toward 0, Q2 toward 0.5) each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha_pos, alpha_neg, beta, eta, f = model_parameters
    n_trials = len(action_1)

    # Q-values
    q1 = np.zeros(2)             # stage-1 MF, prior 0
    q2 = np.full((2, 2), 0.5)    # stage-2 MF, prior 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        # Policies
        exp1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        exp2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning rates depend on outcome at stage 2
        alpha2 = alpha_pos if r > 0.0 else alpha_neg

        # Stage-2 update (chosen alien)
        q2[s, a2] += alpha2 * (r - q2[s, a2])

        # Determine if transition was common or rare for the chosen action
        # Common: A->X or U->Y
        is_common = (a1 == s)

        # Transition-dependent credit scaling for stage-1 update
        credit_scale = 1.0 if is_common else (1.0 - eta)

        # Stage-1 MF update toward the value of the chosen second-stage action
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += credit_scale * alpha2 * td1

        # Global forgetting toward priors
        q1 = (1.0 - f) * q1  # toward 0
        q2 = (1.0 - f) * q2 + f * 0.5  # toward 0.5

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss