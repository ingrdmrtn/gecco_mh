def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free agent with eligibility trace and choice stickiness.
    
    This model blends a model-based (MB) plan computed from a known transition matrix
    with model-free (MF) action values learned from rewards. It also uses an eligibility
    trace to propagate reward prediction errors to the first-stage values and includes
    a choice stickiness bias that favors repeating the most recent choices at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=Planet X, 1=Planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state (0 or 1 for the two aliens on that planet).
    reward : array-like of float (0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha: learning rate for MF value updates and eligibility propagation. [0,1]
        - beta: inverse temperature for softmax choice at both stages. [0,10]
        - w: weight of model-based value in first-stage decision (0=MF only, 1=MB only). [0,1]
        - lam: eligibility trace parameter propagating second-stage RPE to first-stage. [0,1]
        - kappa: stickiness strength added to the last chosen action at each stage. [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of first- and second-stage choices.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Known transition structure (A->X common; U->Y common)
    transition_matrix = np.array([[0.7, 0.3],  # P(state | action A)
                                  [0.3, 0.7]]) # P(state | action U)

    # Probabilities of the actually chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free action values
    q1_mf = np.zeros(2)        # First-stage MF Q for actions [A,U]
    q2_mf = np.zeros((2, 2))   # Second-stage MF Q for states [X,Y] and actions [0,1]

    # Stickiness: last choices (initialize to "none" = -1; bias applied only if matching)
    last_a1 = -1
    last_a2 = np.array([-1, -1])  # per state

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage action values = E_s [ max_a' Q2(s,a') ]
        max_q2 = np.max(q2_mf, axis=1)  # shape (2,)
        q1_mb = transition_matrix @ max_q2  # shape (2,)

        # Combine MB and MF with stickiness bias
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Add choice stickiness at stage 1
        if last_a1 != -1:
            bias = np.zeros(2)
            bias[last_a1] += kappa
            q1 = q1 + bias

        # Softmax policy for first-stage choice
        # Numerical stability: subtract max before exp
        q1_center = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_center)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy: MF Q with stickiness within the current state
        q2 = q2_mf[s].copy()
        if last_a2[s] != -1:
            q2[last_a2[s]] += kappa

        q2_center = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_center)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        p_choice_2[t] = probs_2[a2]

        # Learning (store pre-update value for eligibility computation)
        q2_sa_old = q2_mf[s, a2]

        # MF update at second stage
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # MF update at first stage toward observed second-stage action value (SARSA-style bootstrapping)
        delta1 = q2_sa_old - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Eligibility trace: propagate second-stage RPE to first-stage value
        q1_mf[a1] += alpha * lam * delta2

        # Update stickiness memory
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA(lambda) with separate learning rates and perseveration.
    
    This agent learns action values at both stages purely from experienced rewards.
    It uses separate learning rates for first and second stage, an eligibility trace
    to propagate reward prediction errors back to the first stage, and a perseveration
    bias that favors repeating the most recent choices at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=Planet X, 1=Planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state (0 or 1 for the two aliens on that planet).
    reward : array-like of float (0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha1: learning rate for first-stage MF values. [0,1]
        - alpha2: learning rate for second-stage MF values. [0,1]
        - beta: inverse temperature for softmax choice at both stages. [0,10]
        - lam: eligibility trace parameter propagating second-stage RPE to first-stage. [0,1]
        - rho: perseveration strength added to the last chosen action at each stage. [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha1, alpha2, beta, lam, rho = model_parameters
    n_trials = len(action_1)

    # Probabilities of the actually chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)        # First-stage Q for actions [A,U]
    q2 = np.zeros((2, 2))   # Second-stage Q for states [X,Y] and actions [0,1]

    # Perseveration memory
    last_a1 = -1
    last_a2 = np.array([-1, -1])  # per state

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 1 policy with perseveration
        q1_pref = q1.copy()
        if last_a1 != -1:
            q1_pref[last_a1] += rho

        q1c = q1_pref - np.max(q1_pref)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + 1e-12)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with state-specific perseveration
        q2_pref = q2[s].copy()
        if last_a2[s] != -1:
            q2_pref[last_a2[s]] += rho

        q2c = q2_pref - np.max(q2_pref)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + 1e-12)
        p_choice_2[t] = probs2[a2]

        # Store pre-update value for bootstrapping/eligibility calculations
        q2_old = q2[s, a2]

        # Second-stage MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # First-stage MF update towards the observed second-stage action value
        delta1 = q2_old - q1[a1]
        q1[a1] += alpha1 * delta1

        # Eligibility trace: propagate second-stage RPE back to first-stage
        q1[a1] += alpha1 * lam * delta2

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based agent with learned transitions and intrinsic uncertainty-driven exploration (UCB).
    
    The agent learns:
      - Second-stage reward values for each state-action.
      - First-stage transition probabilities from each spaceship to each planet.
    Action selection is purely model-based:
      - At stage 2, it values aliens by their learned expected reward plus an
        uncertainty bonus proportional to sqrt(q*(1-q)) (higher near 0.5).
      - At stage 1, it computes the expected value of each spaceship by
        multiplying learned transition probabilities with the best second-stage
        value available on each planet (including the uncertainty bonus).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=Planet X, 1=Planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state (0 or 1 for the two aliens on that planet).
    reward : array-like of float (0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 4 floats
        - alpha_r: learning rate for second-stage reward values. [0,1]
        - alpha_t: learning rate for transition probabilities. [0,1]
        - beta: inverse temperature for softmax choice at both stages. [0,10]
        - ucb: weight of the intrinsic uncertainty bonus (0=no exploration, 1=max bonus). [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha_r, alpha_t, beta, ucb = model_parameters
    n_trials = len(action_1)

    # Initialize learned transitions:
    # Start with the canonical structure as a prior (A->X, U->Y common), but learn from experience.
    T = np.array([[0.7, 0.3],   # P(state | action A): [X, Y]
                  [0.3, 0.7]])  # P(state | action U): [X, Y]

    # Second-stage value estimates
    q2 = np.zeros((2, 2))  # states [X,Y] x actions [0,1]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Uncertainty bonus at stage 2 based on Bernoulli variance q*(1-q)
        var2 = q2 * (1.0 - q2)                 # shape (2,2)
        bonus2 = ucb * np.sqrt(np.maximum(var2, 0.0))

        # Stage-2 action preferences for the current state
        v2_state = q2[s] + bonus2[s]
        v2c = v2_state - np.max(v2_state)
        exp_v2 = np.exp(beta * v2c)
        probs2 = exp_v2 / (np.sum(exp_v2) + 1e-12)
        p_choice_2[t] = probs2[a2]

        # Stage-1 model-based values: expectation over learned transitions of best v2 on each planet
        max_v2_per_state = np.max(q2 + bonus2, axis=1)  # shape (2,)
        v1 = T @ max_v2_per_state                       # shape (2,)
        v1c = v1 - np.max(v1)
        exp_v1 = np.exp(beta * v1c)
        probs1 = exp_v1 / (np.sum(exp_v1) + 1e-12)
        p_choice_1[t] = probs1[a1]

        # Learning updates

        # Update transitions for the chosen first-stage action a1 toward the observed state s
        # Keep the row normalized: increase prob of observed s, decrease the other.
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_t * (target - T[a1, sp])
        # (Other action's row unchanged this trial)

        # Update second-stage reward value
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll