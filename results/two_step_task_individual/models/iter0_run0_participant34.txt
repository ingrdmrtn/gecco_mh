def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and action perseveration.
    
    This model blends model-based (MB) planning using a known transition structure
    with model-free (MF) Q-learning. Stage-2 values are learned model-free; stage-1
    combines MB and MF values. An eligibility trace propagates the stage-2
    prediction error to stage-1 MF values. Perseveration biases repeat of the
    previous action at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed first-stage choices per trial: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Observed second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Observed second-stage choices per trial: 0/1 correspond to the two aliens on each planet.
    reward : array-like of float (typically 0 or 1)
        Reward received on each trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, w, lam, kappa]
        - alpha in [0,1]: learning rate for MF updates (both stages).
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight mixing MB and MF at stage 1; 1=fully MB, 0=fully MF.
        - lam in [0,1]: eligibility trace strength propagating stage-2 PE to stage-1 MF.
        - kappa in [0,1]: action perseveration strength added to the previous action's logit at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Known (stationary) transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # P(state | action1=0)
                                  [0.3, 0.7]]) # P(state | action1=1)

    # Initialize value functions
    q1_mf = np.zeros(2)           # MF values for stage-1 actions
    q2 = np.zeros((2, 2))         # MF values for stage-2 (state, action)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration tracking (previous actions)
    prev_a1 = None
    prev_a2 = np.array([None, None])  # track per-state stage-2 previous actions

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation: expected value of each stage-1 action from transition model and current q2
        max_q2 = np.max(q2, axis=1)          # value per state
        q1_mb = transition_matrix @ max_q2   # expected value per action1

        # Combine MB and MF for stage-1
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Add perseveration bias at stage-1
        logits1 = beta * q1
        if prev_a1 is not None:
            logits1[prev_a1] += kappa

        # Softmax for stage-1
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration (state-dependent)
        logits2 = beta * q2[s].copy()
        if prev_a2[s] is not None:
            logits2[prev_a2[s]] += kappa
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD error and update
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update with eligibility trace using stage-2 PE
        # First update direct SARSA-like bootstrapping to the chosen stage-2 action value
        # (using the updated q2 is acceptable; alternatives differ subtly)
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Eligibility trace component propagating the stage-2 PE to stage-1 MF
        q1_mf[a1] += alpha * lam * pe2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA(Î») with asymmetric learning rates and stickiness.
    
    This model learns solely from rewards via model-free mechanisms, with
    asymmetric learning rates for positive vs. negative prediction errors,
    an eligibility trace linking stage-2 outcomes to stage-1 values, and a
    choice stickiness bias favoring repetition of previous actions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed first-stage choices per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Observed second-stage choices per trial.
    reward : array-like of float
        Reward received on each trial.
    model_parameters : list or tuple of 5 floats
        [alpha_pos, alpha_neg, beta, lam, phi]
        - alpha_pos in [0,1]: learning rate when TD error >= 0.
        - alpha_neg in [0,1]: learning rate when TD error < 0.
        - beta in [0,10]: inverse temperature for both stages.
        - lam in [0,1]: eligibility trace weighting from stage-2 PE to stage-1 values.
        - phi in [0,1]: stickiness strength added to previous action's logit at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, lam, phi = model_parameters
    n_trials = len(action_1)

    # Initialize MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2 = np.array([None, None])

    def lr(delta):
        return alpha_pos if delta >= 0.0 else alpha_neg

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # Stage-1 policy with stickiness
        logits1 = beta * q1.copy()
        if prev_a1 is not None:
            logits1[prev_a1] += phi
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness (state-dependent)
        logits2 = beta * q2[s].copy()
        if prev_a2[s] is not None:
            logits2[prev_a2[s]] += phi
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD error and asymmetric update
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += lr(pe2) * pe2

        # Stage-1 SARSA-style update toward the chosen stage-2 value
        pe1 = q2[s, a2] - q1[a1]
        q1[a1] += lr(pe1) * pe1

        # Eligibility trace: additional credit assignment from stage-2 PE
        q1[a1] += lr(pe2) * lam * pe2

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Learning transition-aware hybrid with lapse: MB using learned transitions + MF.
    
    The agent jointly learns:
      - Stage-2 MF values from reward.
      - Stage-1 MF values from bootstrapping to stage-2.
      - The transition probabilities from each first-stage action to second-stage states.
    Planning at stage-1 uses the learned transition matrix. A lapse parameter blends
    softmax with a uniform random policy.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed first-stage choices per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Observed second-stage choices per trial.
    reward : array-like of float
        Reward received on each trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, w, xi, epsilon]
        - alpha in [0,1]: learning rate for MF value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight mixing MB and MF at stage-1.
        - xi in [0,1]: learning rate for transition matrix rows (row-stochastic updates).
        - epsilon in [0,1]: lapse probability; with probability epsilon, choose uniformly at random.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, xi, epsilon = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model (start uninformative)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Initialize values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation from learned transitions
        max_q2 = np.max(q2, axis=1)     # value per state
        q1_mb = T @ max_q2              # expected value per action1
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy with lapse
        logits1 = beta * q1
        exp1 = np.exp(logits1 - np.max(logits1))
        soft1 = exp1 / np.sum(exp1)
        probs1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapse
        logits2 = beta * q2[s]
        exp2 = np.exp(logits2 - np.max(logits2))
        soft2 = exp2 / np.sum(exp2)
        probs2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Update transition model row for chosen a1 toward observed state s
        # Row-stochastic exponential recency-weighted averaging:
        # shrink all entries, then add mass xi to observed state
        a1_idx = a1
        T[a1_idx, :] = (1.0 - xi) * T[a1_idx, :]
        T[a1_idx, s] += xi
        # Numerical guard for normalization drift
        T[a1_idx, :] /= np.sum(T[a1_idx, :])

        # Stage-2 TD learning
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF learning toward the (updated) stage-2 chosen action value
        pe1 = q2[s, a2] - q1_mf[a1_idx]
        q1_mf[a1_idx] += alpha * pe1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss