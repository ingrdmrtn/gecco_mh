def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with eligibility trace and first-stage stickiness.
    
    This model combines a model-based (MB) planner with a model-free (MF) learner.
    Stage-2 values are learned via a delta rule. Stage-1 MF values are updated via
    an eligibility trace from the obtained reward. The first-stage policy blends
    MB and MF action values and includes a perseveration bias toward repeating the
    previous first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 are the two aliens available on that planet).
    reward : array-like of float (typically 0 or 1)
        Outcome received at the end of each trial.
    model_parameters : sequence of floats
        [alpha, lambda_et, w_mb, beta, kappa_stay]
        - alpha in [0,1]: learning rate for MF updates (stage-2 and eligibility at stage-1).
        - lambda_et in [0,1]: eligibility-trace strength passing the stage-2 reward prediction error back to stage-1 MF value.
        - w_mb in [0,1]: weight of model-based values in the first-stage decision (1=fully MB).
        - beta in [0,10]: inverse temperature for softmax choice (both stages).
        - kappa_stay in [0,1]: perseveration bias to repeat previous first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, lambda_et, w_mb, beta, kappa_stay = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X and U->Y are common (0.7); rare otherwise.
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # rows: action, cols: state

    # Initialize value functions
    q_stage2 = np.zeros((2, 2))  # Q(s, a2)
    q_stage1_mf = np.zeros(2)    # MF Q(a1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None  # for perseveration

    for t in range(n_trials):
        # Model-based first-stage values: expected max over next-state Qs
        max_q2 = np.max(q_stage2, axis=1)           # max over actions for each state
        q_stage1_mb = transition_matrix @ max_q2    # expected value per action

        # Perseveration bias vector
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = 1.0

        # Blend MB and MF, add perseveration, and form policy
        q1_blend = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf + kappa_stay * bias
        logits1 = beta * q1_blend
        # softmax
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy at observed state
        s = state[t]
        logits2 = beta * q_stage2[s, :]
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Eligibility trace updates stage-1 MF for chosen a1
        q_stage1_mf[a1] += alpha * lambda_et * delta2

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-weighted MB/MF with learned transitions and stickiness.
    
    This model learns both reward values (MF) and the transition structure online.
    Model-based (MB) first-stage action values are computed using the learned transition
    matrix, while model-free (MF) stage-1 values receive eligibility updates from the
    stage-2 reward prediction error. The arbitration between MB and MF is done per action,
    weighting MB more when transition uncertainty (entropy) for that action is low.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial on each planet.
    reward : array-like of float
        Outcome received.
    model_parameters : sequence of floats
        [alpha_r, alpha_t, omega, beta, kappa_stay]
        - alpha_r in [0,1]: reward learning rate for MF (stage-2 and eligibility to stage-1).
        - alpha_t in [0,1]: transition learning rate (exponential recency for p(state|action)).
        - omega in [0,1]: base weight on MB control; effective MB weight per action scales with (1 - uncertainty).
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - kappa_stay in [0,1]: perseveration bias toward repeating previous first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_t, omega, beta, kappa_stay = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix for actions (rows) to states (cols)
    # Start at the known prior (common=0.7) to be sensible, but learning can move it.
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    q_stage2 = np.zeros((2, 2))  # Q(s, a2)
    q_stage1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Compute MB Q1 using learned transitions
        max_q2 = np.max(q_stage2, axis=1)  # for each state
        q1_mb = T @ max_q2  # expected value per action

        # Compute per-action uncertainty via entropy of transition row (normalized)
        # entropy = -sum p log p; normalize by log(2) to get [0,1]
        # Add tiny floor to avoid log(0).
        eps_p = 1e-12
        ent = -np.sum(T * np.log(T + eps_p), axis=1) / np.log(2.0)  # in [0,1]
        # Arbitration weight per action: more MB when uncertainty is low
        w_eff = omega * (1.0 - ent)  # vector of size 2

        # Perseveration bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = 1.0

        # Blend MB and MF per action using w_eff, add stickiness bias
        q1_blend = w_eff * q1_mb + (1.0 - w_eff) * q_stage1_mf + kappa_stay * bias

        # First-stage policy
        logits1 = beta * q1_blend
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta * q_stage2[s, :]
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Learn transitions from (a1 -> s)
        # Exponential recency: move row T[a1,:] toward one-hot of observed state s
        T[a1, :] = (1.0 - alpha_t) * T[a1, :]
        T[a1, s] += alpha_t

        # Reward learning
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * delta2

        # Eligibility update to stage-1 MF
        q_stage1_mf[a1] += alpha_r * delta2

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Transition-aware win-stay/lose-switch heuristic with recency and MF stage-2.
    
    This model uses a heuristic for the first stage that blends model-free and
    model-based win-stay/lose-switch (WSLS). After a rewarded common transition,
    it tends to repeat the previous first-stage action (stay); after a rewarded
    rare transition, it tends to switch (MB-WSLS). The degree of transition
    dependence is controlled by gamma. A recency parameter updates a running
    first-stage preference. Second-stage choices are standard MF with delta-rule learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Outcome per trial.
    model_parameters : sequence of floats
        [alpha_h, gamma, alpha_s2, beta, rho_ps]
        - alpha_h in [0,1]: recency/learning rate for updating first-stage preference.
        - gamma in [0,1]: degree of transition-dependent WSLS (0=pure MF WSLS, 1=fully MB WSLS).
        - alpha_s2 in [0,1]: learning rate for stage-2 MF values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - rho_ps in [0,1]: perseveration bias to repeat previous first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_h, gamma, alpha_s2, beta, rho_ps = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure for determining common vs rare
    # Common when state == action (A->X, U->Y)
    # Not used for planning here, only for the heuristic sign flip.
    # Initialize preferences and Q-values
    pref1 = np.zeros(2)           # running first-stage action preferences
    q_stage2 = np.zeros((2, 2))   # MF Q(s, a2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # First-stage policy from current preference plus perseveration
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = 1.0

        logits1 = beta * (pref1 + rho_ps * bias)
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta * q_stage2[s, :]
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        # Stage-2 MF update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_s2 * delta2

        # First-stage heuristic update (applied to chosen action preference)
        # Win-stay/lose-switch signal
        # sign_r: +1 for reward, -1 for no reward
        sign_r = 1.0 if r > 0 else -1.0
        # common indicator: 1 if common, 0 if rare
        is_common = 1.0 if s == a1 else 0.0
        # Transition-dependent mixing:
        # m = (1 - gamma) * 1  + gamma * ( +1 if common else -1 )
        # => when gamma=0 (pure MF), m=1; when gamma=1 (fully MB), m=+1 for common, -1 for rare.
        m = (1.0 - gamma) * 1.0 + gamma * (1.0 if is_common > 0.5 else -1.0)
        update = alpha_h * sign_r * m

        # Apply update to chosen action's preference; mild decay on the other to stabilize
        pref1[a1] += update
        other = 1 - a1
        pref1[other] *= (1.0 - 0.5 * alpha_h)

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll