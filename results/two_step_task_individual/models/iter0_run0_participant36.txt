def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based + model-free controller with eligibility traces and choice perseveration.
    
    This model combines a model-based first-stage planner (using the fixed transition
    structure) with a model-free learner at both stages. A single perseveration
    parameter biases repeating the most recent action (first stage) and the most recent
    action within each second-stage state.

    Parameters (with suggested bounds):
    - alpha: scalar in [0, 1]
        Learning rate for model-free value updates (both stages).
    - beta: scalar in [0, 10]
        Inverse temperature for softmax choice policy (both stages).
    - w: scalar in [0, 1]
        Weight of model-based values vs. model-free values at stage 1.
        Effective stage-1 value is: w * Q_MB + (1 - w) * Q_MF.
    - lam: scalar in [0, 1]
        Eligibility trace that propagates second-stage reward prediction error to stage-1 MF values.
    - phi: scalar in [0, 1]
        Choice perseveration strength (stickiness). Adds an action-specific bias to repeat the
        previous action (same for stage 1, and per-state for stage 2).

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states per trial (0=planet X, 1=planet Y).
    - action_2: array-like of ints in {0,1}, second-stage choices per trial (aliens on current planet).
    - reward: array-like of floats/ints (typically 0/1), outcome per trial.
    - model_parameters: iterable [alpha, beta, w, lam, phi].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, lam, phi = model_parameters
    n_trials = len(action_1)

    # Fixed (known) transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X, Y]
                                  [0.3, 0.7]]) # from U to [X, Y]

    # Probabilities of observed choices each trial
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)        # Q at stage 1 for actions [A, U]
    q_stage2_mf = np.zeros((2, 2))   # Q at stage 2 for states [X, Y] and actions [0,1]

    # Perseveration memory: last choices
    last_a1 = None                   # last first-stage action
    last_a2_by_state = [None, None]  # last second-stage action for each state

    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation for stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)   # value of best action in each state
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF for stage 1
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += phi

        # Softmax for stage 1
        logits1 = beta * (q1_combined + bias1)
        logits1 -= np.max(logits1)  # numerical stability
        exp_q1 = np.exp(logits1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (pure model-free with perseveration in the current state)
        bias2 = np.zeros(2)
        if last_a2_by_state[s] is not None:
            bias2[last_a2_by_state[s]] += phi

        logits2 = beta * (q_stage2_mf[s] + bias2)
        logits2 -= np.max(logits2)
        exp_q2 = np.exp(logits2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # TD updates
        # Stage 2 update
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage 1 model-free update with eligibility
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1 + lam * delta2)

        # Update perseveration memory
        last_a1 = a1
        last_a2_by_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid model that learns the transition structure online and combines it with model-free values.

    The agent learns both: (i) model-free action values at both stages, and
    (ii) the first-stage transition probabilities from choices to second-stage states.
    Planning uses the learned transition matrix to compute model-based values. A
    hybrid weight determines the contribution of MB vs MF at stage 1. Eligibility
    traces propagate second-stage reward PEs to stage-1 MF values.

    Parameters (with suggested bounds):
    - alpha_r: scalar in [0, 1]
        Reward learning rate for model-free Q updates (both stages).
    - beta: scalar in [0, 10]
        Inverse temperature for softmax choice policy (both stages).
    - w: scalar in [0, 1]
        Weight of model-based values vs. model-free values at stage 1.
    - lam: scalar in [0, 1]
        Eligibility trace that propagates second-stage reward prediction error to stage-1 MF values.
    - alpha_t: scalar in [0, 1]
        Transition learning rate. Updates the learned transition probabilities after each observed transition.

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial.
    - state: array-like of ints in {0,1}, second-stage states per trial.
    - action_2: array-like of ints in {0,1}, second-stage choices per trial.
    - reward: array-like of floats/ints (typically 0/1), outcome per trial.
    - model_parameters: iterable [alpha_r, beta, w, lam, alpha_t].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha_r, beta, w, lam, alpha_t = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix as uninformative (rows sum to 1)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]

        # Model-based value using learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T @ max_q_stage2

        # Hybrid combination at stage 1
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Softmax for stage 1
        logits1 = beta * q1_combined
        logits1 -= np.max(logits1)
        exp_q1 = np.exp(logits1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 softmax (MF)
        logits2 = beta * q_stage2_mf[s]
        logits2 -= np.max(logits2)
        exp_q2 = np.exp(logits2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward update (model-free)
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * delta2

        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * (delta1 + lam * delta2)

        # Transition learning update for the chosen first-stage action
        # Move the chosen row toward the one-hot of observed state
        one_hot = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = T[a1, :] + alpha_t * (one_hot - T[a1, :])
        # Ensure numerical stability (maintain row stochasticity)
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] = T[a1, :] / row_sum

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA(Î») controller with separate learning rates and temperatures per stage.

    The agent does not plan over transitions. Instead, it learns:
    - Stage-1 Q-values for first-stage actions directly.
    - Stage-2 Q-values within each second-stage state.
    Eligibility traces propagate second-stage reward prediction errors to stage-1 values.
    Separate learning rates and inverse temperatures are used for stage 1 and stage 2.

    Parameters (with suggested bounds):
    - alpha1: scalar in [0, 1]
        Learning rate for stage-1 Q-value updates.
    - alpha2: scalar in [0, 1]
        Learning rate for stage-2 Q-value updates.
    - beta1: scalar in [0, 10]
        Inverse temperature for softmax policy at stage 1.
    - beta2: scalar in [0, 10]
        Inverse temperature for softmax policy at stage 2.
    - lam: scalar in [0, 1]
        Eligibility trace parameter that blends second-stage reward PE into stage-1 updates.

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial.
    - state: array-like of ints in {0,1}, second-stage states per trial.
    - action_2: array-like of ints in {0,1}, second-stage choices per trial.
    - reward: array-like of floats/ints (typically 0/1), outcome per trial.
    - model_parameters: iterable [alpha1, alpha2, beta1, beta2, lam].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha1, alpha2, beta1, beta2, lam = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)        # MF Q values at stage 1
    q_stage2 = np.zeros((2, 2))   # MF Q values at stage 2 for states [0,1]

    for t in range(n_trials):
        s = state[t]

        # Stage 1 policy (MF only)
        logits1 = beta1 * q_stage1
        logits1 -= np.max(logits1)
        exp_q1 = np.exp(logits1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (MF only)
        logits2 = beta2 * q_stage2[s]
        logits2 -= np.max(logits2)
        exp_q2 = np.exp(logits2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # TD learning
        r = reward[t]

        # Stage 2 update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Stage 1 update with eligibility trace on reward PE
        delta1 = q_stage2[s, a2] - q_stage1[a1]
        q_stage1[a1] += alpha1 * (delta1 + lam * delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll