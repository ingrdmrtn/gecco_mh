def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace (SARSA(Î»)).
    First-stage choices are driven by a weighted combination of model-based
    (transition-informed) and model-free values. Second-stage values are
    learned via TD. The model-free first-stage values receive eligibility-trace
    credit assignment from second-stage reward prediction errors.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A-like, 1=U-like).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0=first alien on that planet, 1=second alien).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, lam, w, beta1, beta2]
        - alpha (learning rate for MF Q): [0, 1]
        - lam (eligibility trace, credit from stage2 PE to stage1 MF): [0, 1]
        - w (mixing weight of MB vs MF at stage 1; 1=pure MB): [0, 1]
        - beta1 (inverse temperature, stage 1 softmax): [0, 10]
        - beta2 (inverse temperature, stage 2 softmax): [0, 10]
    
    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, lam, w, beta1, beta2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows are first-stage actions (A, U), columns are states (X, Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Choice likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)           # model-free first-stage Q
    q2_mf = np.zeros((2, 2))      # model-free second-stage Q for each state and action

    for t in range(n_trials):
        # Model-based evaluation for stage 1 uses current second-stage MF values
        max_q2 = np.max(q2_mf, axis=1)                 # best second-stage action per state
        q1_mb = transition_matrix @ max_q2             # MB first-stage values
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf      # mixture of MB and MF

        # Stage 1 policy
        exp_q1 = np.exp(beta1 * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (conditioned on reached state)
        s = state[t]
        exp_q2 = np.exp(beta2 * (q2_mf[s] - np.max(q2_mf[s])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD error and update (model-free)
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 model-free update via eligibility trace from stage-2 PE
        # Also allow a small direct bootstrap toward observed second-stage value
        delta1_bootstrap = (q2_mf[s, a2] - q1_mf[a1])
        q1_mf[a1] += alpha * ((1.0 - lam) * delta1_bootstrap + lam * delta2)

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planning with learned transitions and choice stickiness.
    The agent learns both second-stage rewards and first-stage transition
    probabilities, and plans by propagating expected second-stage values
    through the learned transition matrix. A perseveration bias encourages
    repeating the previous action at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A-like, 1=U-like).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0=first alien, 1=second alien).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_r, alpha_t, kappa, beta1, beta2]
        - alpha_r (learning rate for second-stage rewards): [0, 1]
        - alpha_t (learning rate for transition probabilities): [0, 1]
        - kappa (perseveration/stickiness strength added to last chosen action value): [0, 1]
        - beta1 (inverse temperature, stage 1 softmax): [0, 10]
        - beta2 (inverse temperature, stage 2 softmax): [0, 10]
    
    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, alpha_t, kappa, beta1, beta2 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T[a, s]; start uninformative (0.5, 0.5)
    T = np.full((2, 2), 0.5)
    # Ensure rows sum to 1
    for a in range(2):
        T[a] /= np.sum(T[a])

    # Second-stage value function
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = np.full(2, -1)  # track previous action per state for stage-2 stickiness

    for t in range(n_trials):
        # Model-based first-stage values from learned transitions and current q2
        max_q2 = np.max(q2, axis=1)   # best second-stage action per state
        q1_mb = T @ max_q2

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa

        # Stage 1 policy
        q1_eff = q1_mb + bias1
        exp_q1 = np.exp(beta1 * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with per-state perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa

        q2_eff = q2[s] + bias2
        exp_q2 = np.exp(beta2 * (q2_eff - np.max(q2_eff)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update second-stage values
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update learned transitions for chosen first-stage action using simple delta rule
        # Move the chosen row toward a one-hot indicating the observed state s
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_t * (target - T[a1, sp])
        # Re-normalize to avoid drift from numerical error
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-weighted arbitration between model-based and model-free control.
    The agent maintains both model-based (MB) and model-free (MF) valuations.
    Arbitration weight w_t is computed online from recent uncertainty signals:
    - MF uncertainty from absolute reward TD errors at stage 2,
    - MB uncertainty from transition surprise (rare transitions).
    The weight is passed through a sigmoid scaled by gamma to yield w_t in [0,1].
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A-like, 1=U-like).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0=first alien, 1=second alien).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, lam, gamma, beta1, beta2]
        - alpha (learning rate for MF Q updates): [0, 1]
        - lam (decay/smoothing for uncertainty traces and MF eligibility): [0, 1]
        - gamma (arbitration gain; higher makes w_t more sensitive to uncertainty differences): [0, 1]
        - beta1 (inverse temperature, stage 1 softmax): [0, 10]
        - beta2 (inverse temperature, stage 2 softmax): [0, 10]
    
    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, lam, gamma, beta1, beta2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (for planning and surprise signal)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
    common_state = np.array([0, 1])  # action 0 commonly->state 0, action 1 commonly->state 1

    # Model-free value functions
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Uncertainty traces (initialized neutral/low)
    u_mf = 0.0  # tracks recent absolute reward PE magnitude
    u_mb = 0.0  # tracks recent transition surprise (rare transitions)

    for t in range(n_trials):
        # Model-based evaluation from current MF second-stage values
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Arbitration weight from uncertainties
        # w_t = sigmoid(gamma * (u_mf - u_mb))
        z = gamma * (u_mf - u_mb)
        w_t = 1.0 / (1.0 + np.exp(-z))

        # Hybrid first-stage values
        q1_hybrid = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage 1 policy
        exp_q1 = np.exp(beta1 * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy given state
        s = state[t]
        exp_q2 = np.exp(beta2 * (q2_mf[s] - np.max(q2_mf[s])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]

        # Stage-2 TD learning
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF eligibility update driven by stage-2 PE
        q1_mf[a1] += alpha * lam * delta2

        # Update uncertainty traces
        # MF uncertainty: magnitude of recent reward PE
        u_mf = (1.0 - lam) * u_mf + lam * np.abs(delta2)

        # MB uncertainty: transition surprise (1 if rare, 0 if common)
        rare = 1.0 if s != common_state[a1] else 0.0
        u_mb = (1.0 - lam) * u_mb + lam * rare

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik