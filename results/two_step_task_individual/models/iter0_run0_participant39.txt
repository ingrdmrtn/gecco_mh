def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-free/model-based with eligibility trace and separate temperatures.
    Uses a weighted combination of model-based (MB) and model-free (MF) values at stage 1,
    standard MF learning at stage 2, and an eligibility trace to propagate stage-2
    prediction errors back to stage-1 MF values.
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state reached per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state
        (for X: 0 = W, 1 = S; for Y: 0 = P, 1 = H).
    reward : array-like of float
        Reward observed per trial (e.g., 0 or 1).
    model_parameters : iterable of float
        Tuple/list with five parameters:
        - alpha: learning rate for MF values (stage 1 and 2), in [0,1].
        - beta1: inverse temperature for stage-1 softmax, in [0,10].
        - beta2: inverse temperature for stage-2 softmax, in [0,10].
        - w: weight of MB relative to MF at stage 1, in [0,1].
        - lam: eligibility trace strength for propagating stage-2 PE to stage-1 MF, in [0,1].
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """
    alpha, beta1, beta2, w, lam = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows = actions (A, U), cols = states (X, Y)
    transition_matrix = np.array([[0.7, 0.3],  # A -> X common
                                  [0.3, 0.7]]) # U -> Y common

    # Probabilities of the observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)              # for A, U
    q_stage2_mf = np.zeros((2, 2))         # for states X,Y each with 2 aliens

    for t in range(n_trials):
        # Stage-1 policy: hybrid MB + MF
        # MB component = transition * max over stage-2 MF values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # per state
        q_stage1_mb = transition_matrix @ max_q_stage2  # shape (2,)
        q1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Softmax for stage-1 choice
        exp_q1 = np.exp(beta1 * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy: softmax over MF values in reached state
        s = state[t]
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta2 * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD error and update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update via eligibility trace from stage-2 PE
        # (propagate reward-based learning to chosen stage-1 action)
        q_stage1_mf[a1] += alpha * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planning with learned transitions and perseveration bias.
    Learns second-stage values from rewards and learns the transition probabilities online.
    Stage-1 decisions plan using the learned transition matrix and the current second-stage values.
    Includes a perseveration bias favoring repeating the previous stage-1 action.
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state reached per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state
        (for X: 0 = W, 1 = S; for Y: 0 = P, 1 = H).
    reward : array-like of float
        Reward observed per trial (e.g., 0 or 1).
    model_parameters : iterable of float
        Tuple/list with four parameters:
        - alpha: learning rate for second-stage values, in [0,1].
        - beta: inverse temperature for both stages, in [0,10].
        - eta: transition learning rate (row-wise toward observed state), in [0,1].
        - pi: perseveration strength added to the previously chosen stage-1 action, in [0,1].
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """
    alpha, beta, eta, pi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix to neutral (0.5/0.5)
    T = np.full((2, 2), 0.5)  # rows = actions (A,U), cols = states (X,Y)

    # Second-stage action values
    q_stage2 = np.zeros((2, 2))  # states X,Y each with 2 aliens

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Compute MB stage-1 values via planning with learned transitions
        max_q2 = np.max(q_stage2, axis=1)   # per state
        q1_mb = T @ max_q2                  # shape (2,)

        # Add perseveration bias to the previously chosen first-stage action
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += pi  # encourages repeating the last choice

        # Stage-1 softmax
        prefs1 = q1_mb + bias
        exp_q1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax
        s = state[t]
        q2 = q_stage2[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update second-stage values
        r = reward[t]
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Update learned transitions for the chosen first-stage action toward the observed state
        # Row-wise delta rule toward one-hot of observed state
        a1_row = T[a1].copy()
        T[a1] = (1 - eta) * a1_row
        T[a1, s] += eta  # ensures the row sums to 1

        # Update perseveration memory
        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free with directed exploration bonus and rarity-sensitive credit assignment.
    Maintains MF values for both stages and uncertainty bonuses that decay with sampling.
    The first- and second-stage policies include an exploration bonus proportional to
    action-specific uncertainty. Credit assignment from stage-2 to stage-1 is down-weighted
    following rare transitions (rarity sensitivity).
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state reached per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state
        (for X: 0 = W, 1 = S; for Y: 0 = P, 1 = H).
    reward : array-like of float
        Reward observed per trial (e.g., 0 or 1).
    model_parameters : iterable of float
        Tuple/list with five parameters:
        - alpha: learning rate for MF values, in [0,1].
        - beta: inverse temperature for both stages, in [0,10].
        - kappa: uncertainty decay per visit (larger -> faster certainty), in [0,1].
        - phi: directed exploration strength multiplying uncertainty bonus, in [0,1].
        - gamma: rarity sensitivity scaling eligibility after rare transitions, in [0,1].
                 Eligibility scale = 1.0 for common; (1 - gamma) for rare.
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """
    alpha, beta, kappa, phi, gamma = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure for identifying common vs rare
    # A->X and U->Y are common (0.7), others rare (0.3)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Model-free Q-values
    q1 = np.zeros(2)            # stage-1 MF values for A,U
    q2 = np.zeros((2, 2))       # stage-2 MF values per state

    # Uncertainty (initialized high = 1); decays with sampling by factor (1 - kappa)
    u1 = np.ones(2)
    u2 = np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage-1 policy with directed exploration bonus
        v1 = q1 + phi * u1
        exp_q1 = np.exp(beta * (v1 - np.max(v1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with directed exploration bonus
        s = state[t]
        v2 = q2[s] + phi * u2[s]
        exp_q2 = np.exp(beta * (v2 - np.max(v2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # Stage-2 TD learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Determine transition rarity for eligibility scaling
        # common if transition probability >= 0.5 under the fixed matrix
        is_common = (transition_matrix[a1, s] >= 0.5)
        elig_scale = 1.0 if is_common else (1.0 - gamma)

        # Propagate stage-2 PE to stage-1 MF (no separate lambda; use elig_scale)
        q1[a1] += alpha * elig_scale * delta2

        # Uncertainty decay on chosen actions (becoming more certain when sampled)
        u1[a1] *= (1.0 - kappa)
        u2[s, a2] *= (1.0 - kappa)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll