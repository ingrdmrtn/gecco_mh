def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with eligibility trace and first-stage perseveration.
    Returns the negative log-likelihood of the observed choices across both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state observed (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1: alien choice on the visited planet) per trial.
    reward : array-like of float (e.g., 0 or 1)
        Outcome received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w, lam, stay)
        - alpha in [0,1]: learning rate for MF updates (both stages).
        - beta in [0,10]: softmax inverse temperature (both stages).
        - w in [0,1]: weight of model-based value at first stage (1=fully MB).
        - lam in [0,1]: eligibility trace weighting of the second-stage RPE on first-stage MF value.
        - stay in [0,1]: perseveration bias magnitude for repeating the previous first-stage choice.

    Notes
    -----
    - Transition structure is fixed/common: action 0 commonly to state 0, action 1 commonly to state 1.
    - First-stage decision uses a weighted combination of MB and MF values plus a stay bias.
    - Second-stage uses MF Q-values.
    """
    alpha, beta, w, lam, stay = model_parameters
    n_trials = len(action_1)

    # Fixed common transition matrix: rows actions (A,U), cols states (X,Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)          # MF values for first-stage actions
    q_stage2_mf = np.zeros((2, 2))     # MF values for second-stage choices per state

    prev_a1 = -1  # for perseveration on the first stage

    for t in range(n_trials):
        # Model-based first-stage values via Bellman backup over second stage (max over actions on each state)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)              # shape (2,)
        q_stage1_mb = transition_matrix @ max_q_stage2           # shape (2,)

        # First-stage choice policy (hybrid MB/MF + perseveration)
        # Stay bias: add +stay to the previously chosen action, -stay to the other (0-centered)
        if prev_a1 in (0, 1):
            stay_bias = np.array([stay if prev_a1 == 0 else -stay,
                                  stay if prev_a1 == 1 else -stay])
        else:
            stay_bias = np.zeros(2)

        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf + stay_bias
        # softmax
        q1c = q1_combined - np.max(q1_combined)
        exp_q1 = np.exp(beta * q1c)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy: softmax over MF values at the reached state
        s = state[t]
        q2 = q_stage2_mf[s]
        q2c = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2c)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]

        # Second-stage TD error and update (MF)
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # First-stage MF TD toward the value encountered at stage 2
        # Standard MF delta1
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Eligibility trace: propagate the outcome RPE up to first stage
        q_stage1_mf[a1] += alpha * lam * delta2

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid with learned transitions and second-stage choice-kernel perseveration.
    Returns the negative log-likelihood of the observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state observed (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien on the visited planet).
    reward : array-like of float
        Obtained reward per trial.
    model_parameters : tuple/list of 5 floats
        (alpha_r, beta, w, alpha_t, stick2)
        - alpha_r in [0,1]: reward learning rate for MF Q updates.
        - beta in [0,10]: softmax inverse temperature (both stages).
        - w in [0,1]: weight of model-based value at first stage using learned transitions.
        - alpha_t in [0,1]: transition learning rate to update P(state | action_1).
        - stick2 in [0,1]: second-stage choice perseveration (within-state repetition bias).

    Notes
    -----
    - Transition probabilities are learned online separately for each first-stage action.
    - First stage uses MB (via learned transitions) mixed with MF.
    - Second stage includes a state-specific choice kernel that biases repeating the last action on that state.
    """
    alpha_r, beta, w, alpha_t, stick2 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix; start unbiased (0.5/0.5) to avoid strong priors
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Second-stage choice kernels (per state, action), keep last action sign bias
    # We'll implement a simple "last action" kernel: add +stick2 to last chosen action in that state, -stick2 to the other.
    last_a2 = np.array([-1, -1])  # last action taken in state 0 and 1

    for t in range(n_trials):
        # MB plan with learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T @ max_q_stage2

        # First-stage policy (hybrid)
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        q1c = q1_combined - np.max(q1_combined)
        exp_q1 = np.exp(beta * q1c)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Observe state and form second-stage policy with choice kernel
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        if last_a2[s] in (0, 1):
            # Add +stick2 to last chosen action and -stick2 to the other (0-centered)
            q2 += np.array([stick2 if last_a2[s] == 0 else -stick2,
                            stick2 if last_a2[s] == 1 else -stick2])
        q2c = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2c)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Transition learning: update only the row corresponding to chosen first-stage action
        # Move probabilities toward the observed state with rate alpha_t, keep row normalized.
        # Equivalent to: T[a1, s] += alpha_t*(1 - T[a1, s]); T[a1, 1-s] += alpha_t*(0 - T[a1, 1-s])
        T[a1, s] = T[a1, s] + alpha_t * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] = T[a1, other] + alpha_t * (0.0 - T[a1, other])

        # Second-stage MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * delta2

        # First-stage MF update towards encountered second-stage value (SARSA(0)-like)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Update choice kernel memory
        last_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Hybrid with forgetting and uncertainty-driven exploration bonus at stage 2.
    Returns the negative log-likelihood of the observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state observed (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien on the visited planet).
    reward : array-like of float
        Obtained reward per trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w, kappa, eta)
        - alpha in [0,1]: reward learning rate for MF updates.
        - beta in [0,10]: softmax inverse temperature (both stages).
        - w in [0,1]: weight of model-based value at first stage.
        - kappa in [0,1]: forgetting rate pulling Q-values toward 0.5 each trial (captures non-stationarity).
        - eta in [0,1]: weight of uncertainty bonus for second-stage actions (directed exploration).

    Notes
    -----
    - Second-stage MF values are subject to leaky forgetting toward 0.5 before each update.
    - Uncertainty for each second-stage state-action is u = 1/sqrt(n+1), where n is visit count.
      This encourages sampling less-visited aliens via an additive bonus eta*u in the policy.
    - First stage uses an MB/MF hybrid; transitions are fixed/common.
    """
    alpha, beta, w, kappa, eta = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.full((2, 2), 0.5)  # initialize at 0.5 to align with forgetting anchor
    visit_counts = np.zeros((2, 2))     # counts per state-action for uncertainty

    for t in range(n_trials):
        # Apply forgetting toward 0.5 baseline at second stage (to all actions)
        q_stage2_mf = (1.0 - kappa) * q_stage2_mf + kappa * 0.5

        # MB values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # First-stage policy (hybrid)
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        q1c = q1_combined - np.max(q1_combined)
        exp_q1 = np.exp(beta * q1c)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy: uncertainty bonus
        s = state[t]
        # Compute uncertainty bonuses for the two aliens on this state
        u = 1.0 / np.sqrt(visit_counts[s] + 1.0)  # shape (2,)
        q2_bonus = q_stage2_mf[s] + eta * u
        q2c = q2_bonus - np.max(q2_bonus)
        exp_q2 = np.exp(beta * q2c)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]

        # Update visit counts after the choice for next-trial uncertainty
        visit_counts[s, a2] += 1.0

        # Second-stage MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # First-stage MF update toward encountered second-stage value (no eligibility, kept simple here)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss