def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free learner with eligibility trace and action perseveration.
    
    This model blends model-based (MB) and model-free (MF) values at the first stage,
    uses a single model-free learner at the second stage, and propagates reward back
    to the first stage via an eligibility trace. A perseveration term captures a tendency
    to repeat the previous action at both stages.
    
    Parameters (use values within bounds):
    - alpha (0-1): Learning rate for value updates at both stages.
    - beta (0-10): Inverse temperature for softmax choice policy (both stages).
    - w (0-1): Weight on model-based values at stage 1 (1 = purely MB, 0 = purely MF).
    - lam (0-1): Eligibility trace for propagating stage-2 prediction errors to stage-1 MF values.
    - kappa (0-1): Action perseveration strength (adds bias toward repeating last action).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet observed at stage 2 (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien within the observed planet.
    - reward: array-like of floats/ints, obtained outcome on each trial (e.g., -1/0/1).
    - model_parameters: [alpha, beta, w, lam, kappa]
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A -> X common, U -> Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # shape (2 actions, 2 states)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)        # MF at stage 1
    q_stage2_mf = np.zeros((2, 2))   # MF at stage 2: Q[state, action]

    # Previous actions for perseveration; initialize as "no previous choice"
    prev_a1 = -1
    prev_a2_by_state = [-1, -1]  # state-dependent perseveration at stage 2

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]

        # Stage-1 action values: compute MB from current stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)          # shape (2,)
        q_stage1_mb = transition_matrix @ max_q_stage2       # shape (2,)

        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa

        # Stage-1 policy
        logits1 = beta * q1_combined + bias1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within observed state
        q2 = q_stage2_mf[s].copy()

        # Add state-dependent perseveration at stage 2
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] in (0, 1):
            bias2[prev_a2_by_state[s]] += kappa

        logits2 = beta * q2 + bias2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD error and update
        delta2 = reward[t] - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF TD error (bootstrapping on stage-2 action value)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # Split update between direct bootstrapping and eligibility-trace from delta2
        q_stage1_mf[a1] += alpha * ((1.0 - lam) * delta1 + lam * delta2)

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based learner with learned transition structure, model-free backup,
    and rare-transition modulation of credit assignment.
    
    This model learns the transition probabilities online, blends learned MB and MF
    values at stage 1, and modulates how much reward credit is assigned back to
    the chosen stage-1 action depending on whether the transition was common or rare.
    A perseveration bias affects both stages.
    
    Parameters (use values within bounds):
    - alpha_q (0-1): Learning rate for Q-value updates (both stages).
    - beta (0-10): Inverse temperature for softmax choice policy (both stages).
    - alpha_t (0-1): Learning rate for transition probability learning.
    - kappa (0-1): Action perseveration strength (both stages).
    - phi (0-1): Rare-transition modulation of eligibility; higher phi increases
                 the influence of rare transitions on credit assignment to stage-1 MF.
                 Effective eligibility weight = lam_eff = 0.5 + (is_rare - 0.5) * 2*phi.
                 Thus lam_eff âˆˆ [0,1], shifting toward 1 on rare events and toward 0 on common events as phi increases.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet observed at stage 2 (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien within the observed planet.
    - reward: array-like of floats/ints, obtained outcome on each trial (e.g., -1/0/1).
    - model_parameters: [alpha_q, beta, alpha_t, kappa, phi]
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha_q, beta, alpha_t, kappa, phi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T[a, s], rows sum to 1
    T = np.full((2, 2), 0.5)  # start uncertain

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Perseveration memory
    prev_a1 = -1
    prev_a2_by_state = [-1, -1]

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]

        # Stage-1 MB values from learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # shape (2,)
        q_stage1_mb = T @ max_q_stage2

        # Blend MB and MF for decision policy
        q1_combined = 0.5 * (q_stage1_mb + q_stage1_mf)  # equal blend as default
        # Note: MF/MB blending here is implicit; MB vs MF influence on learning is modulated below.

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa

        # Stage-1 policy
        logits1 = beta * q1_combined + bias1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within observed state
        q2 = q_stage2_mf[s].copy()
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] in (0, 1):
            bias2[prev_a2_by_state[s]] += kappa

        logits2 = beta * q2 + bias2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Determine if transition was common or rare relative to a simple prior (A->X, U->Y)
        # Use this only for modulation; the policy uses learned T.
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        is_rare = 1 - is_common

        # Learn transitions with delta rule toward the observed state
        # For chosen action a1, move its row T[a1] toward one-hot of observed state s
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_t * (target - T[a1, sp])

        # Stage-2 TD error and update
        delta2 = reward[t] - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_q * delta2

        # Stage-1 MF TD error bootstrapping
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]

        # Rare-transition modulation of eligibility: lam_eff in [0,1]
        lam_eff = 0.5 + (is_rare - 0.5) * (2.0 * phi)  # if phi=1: lam_eff=1 for rare, 0 for common

        # Update stage-1 MF
        q_stage1_mf[a1] += alpha_q * ((1.0 - lam_eff) * delta1 + lam_eff * delta2)

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    # Normalize learned transition rows (numerical hygiene)
    T /= (np.sum(T, axis=1, keepdims=True) + eps)

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Adaptive metacontrol model: volatility-gated arbitration between MB and MF control.
    
    The model tracks a running estimate of outcome surprise (volatility) from stage-2
    prediction errors and adaptively adjusts the weight on model-based vs. model-free
    values at stage 1. Credit assignment uses an eligibility trace. No explicit perseveration.
    
    Parameters (use values within bounds):
    - alpha (0-1): Learning rate for Q-value updates (both stages).
    - beta (0-10): Inverse temperature for softmax policy (both stages).
    - lam (0-1): Eligibility trace parameter for propagating reward to stage-1 MF.
    - w0 (0-1): Baseline model-based weight when volatility is minimal.
    - gamma (0-1): Metacontrol learning rate: how quickly the MB weight adapts to surprise.
                   The MB weight w_t is updated as w_t <- (1-gamma)*w_{t-1} + gamma * surprise_t,
                   with surprise_t in [0,1] derived from the magnitude of the stage-2 TD error.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet observed at stage 2 (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien within the observed planet.
    - reward: array-like of floats/ints, obtained outcome on each trial (e.g., -1/0/1).
    - model_parameters: [alpha, beta, lam, w0, gamma]
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, lam, w0, gamma = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Adaptive MB weight
    w = float(w0)  # current arbitration weight in [0,1]

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]

        # MB action values from transition model
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF using adaptive weight
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Stage-1 policy
        logits1 = beta * q1_combined
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        q2 = q_stage2_mf[s].copy()
        logits2 = beta * q2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-2 TD error and update
        delta2 = reward[t] - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF bootstrapping and eligibility-trace update
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * ((1.0 - lam) * delta1 + lam * delta2)

        # Update adaptive arbitration weight from surprise magnitude
        # Map surprise to [0,1] by squashing |delta2| via 1 - exp(-|delta2|)
        surprise = 1.0 - np.exp(-abs(delta2))
        w = (1.0 - gamma) * w + gamma * surprise
        # Keep w in [0,1]
        if w < 0.0:
            w = 0.0
        elif w > 1.0:
            w = 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss