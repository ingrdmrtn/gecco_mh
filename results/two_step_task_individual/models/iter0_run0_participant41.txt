def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free RL with eligibility trace and perseveration.
    The agent blends a learned model-based (MB) evaluation with model-free (MF) values at stage 1,
    learns MF values at stage 2, uses an eligibility trace to propagate reward back to stage 1,
    and exhibits choice perseveration at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : list or array, length=5
        [alpha, beta, w, lam, perseveration]
        - alpha in [0,1]: learning rate for value updates.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight on model-based planning at stage 1 (1=fully MB).
        - lam in [0,1]: eligibility trace strength propagating reward to stage 1.
        - perseveration in [0,1]: strength of stickiness bias toward repeating the last choice.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, lam, perseveration = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (rows: A,U; cols: X,Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Choice probabilities per trial to compute likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)          # MF Q-values for first stage (A,U)
    q_stage2_mf = np.ones((2, 2)) * 0.5  # MF Q-values for second stage (X:[W,S], Y:[P,H]), start at 0.5

    # Perseveration biases (stickiness): last chosen actions
    last_a1 = None
    last_a2 = None

    for t in range(n_trials):
        # Model-based evaluation for stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)     # best alien per planet
        q_stage1_mb = transition_matrix @ max_q_stage2 # MB expected values for A and U

        # Combine MF and MB at stage 1
        q1_base = (1.0 - w) * q_stage1_mf + w * q_stage1_mb

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += perseveration
        q1_eff = q1_base + bias1

        # Softmax for stage 1
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        s = int(state[t])  # observed second-stage state

        # Stage 2 policy: MF values with perseveration on last stage-2 choice
        q2_base = q_stage2_mf[s].copy()
        bias2 = np.zeros(2)
        if last_a2 is not None:
            bias2[last_a2] += perseveration
        q2_eff = q2_base + bias2

        exp_q2 = np.exp(beta * (q2_eff - np.max(q2_eff)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # Second stage MF update (Q-learning)
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # First stage MF update with SARSA(Î»)-style backup using both immediate bootstrap and eligibility
        # Bootstrapped update toward the chosen second-stage value
        delta1_boot = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1_boot
        # Eligibility trace component to propagate reward prediction error
        q_stage1_mf[a1] += alpha * lam * delta2

        # Update perseveration memory
        last_a1 = a1
        last_a2 = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Asymmetric model-free RL with choice kernels and transition-modulated temperature.
    The agent learns separate learning rates for rewarded vs. unrewarded outcomes (asymmetric learning),
    uses choice kernels (stickiness) at both stages, and becomes more stochastic after rare transitions
    by reducing the inverse temperature.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : list or array, length=5
        [alpha_pos, alpha_neg, beta, kappa, rho]
        - alpha_pos in [0,1]: learning rate when the signed prediction error is positive.
        - alpha_neg in [0,1]: learning rate when the signed prediction error is negative.
        - beta in [0,10]: base inverse temperature for softmax (both stages).
        - kappa in [0,1]: strength of choice kernel (update and bias magnitude).
        - rho in [0,1]: transition-surprise sensitivity; after rare transitions, effective beta becomes beta*(1 - rho).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha_pos, alpha_neg, beta, kappa, rho = model_parameters
    n_trials = len(action_1)

    # Known common transitions: A->X, U->Y
    # We will mark a transition as rare if (A->Y) or (U->X).
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q1 = np.zeros(2)              # stage-1 MF values
    q2 = np.ones((2, 2)) * 0.5    # stage-2 MF values

    # Choice kernels (stickiness), used as additive biases
    k1 = np.zeros(2)
    k2 = np.zeros((2, 2))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])

        # Determine whether transition is common or rare for temp modulation
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        surprise = 0.0 if is_common else 1.0
        beta_eff1 = beta * (1.0 - rho * surprise)

        # Stage 1 softmax over MF values plus choice kernel bias
        q1_eff = q1 + k1
        exp_q1 = np.exp(beta_eff1 * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with same modulation of temperature after surprise
        a2 = int(action_2[t])
        beta_eff2 = beta * (1.0 - rho * surprise)
        q2_eff = q2[s] + k2[s]
        exp_q2 = np.exp(beta_eff2 * (q2_eff - np.max(q2_eff)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 learning with asymmetric learning rate
        pe2 = r - q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * pe2

        # Stage-1 learning: bootstrap toward the chosen second-stage action value (SARSA(0))
        pe1 = q2[s, a2] - q1[a1]
        lr1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        q1[a1] += lr1 * pe1

        # Update choice kernels (decay others, reinforce chosen)
        # Stage 1 kernel
        k1 *= (1.0 - kappa)
        k1[a1] += kappa
        # Stage 2 kernel for the current state
        k2[s] *= (1.0 - kappa)
        k2[s, a2] += kappa

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Hybrid RL with learned transition model and leaky value integration.
    The agent learns the transition probabilities online, plans using the learned model (MB),
    combines with MF values at stage 1, and applies leaky integration (forgetting toward 0.5)
    to second-stage values to track nonstationarity.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : list or array, length=5
        [alpha, beta, w, phi, alpha_t]
        - alpha in [0,1]: learning rate for reward-based value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight on MB evaluation at stage 1 (1=fully MB).
        - phi in [0,1]: leaky integration toward 0.5 for all second-stage values each trial.
        - alpha_t in [0,1]: learning rate for updating the transition model.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, phi, alpha_t = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix (rows: actions A,U; cols: states X,Y)
    # Start unbiased at 0.5
    T = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)            # MF first-stage values
    q2_mf = np.ones((2, 2)) * 0.5  # MF second-stage values

    for t in range(n_trials):
        # Leaky integration toward 0.5 on all second-stage values (tracks drift)
        q2_mf = (1.0 - phi) * q2_mf + phi * 0.5

        # Model-based evaluation using learned transitions
        max_q2 = np.max(q2_mf, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        # Combine MB and MF for stage 1 policy
        q1_eff = (1.0 - w) * q1_mf + w * q1_mb
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        s = int(state[t])

        # Stage 2 policy: MF softmax on current state's aliens
        q2_eff = q2_mf[s]
        exp_q2 = np.exp(beta * (q2_eff - np.max(q2_eff)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learn transition model T for the chosen first-stage action from observed state
        # Move T[a1, s] toward 1 and the alternative state toward 0
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T[a1, s_idx] += alpha_t * (target - T[a1, s_idx])
        # Ensure row remains normalized (optional but stable)
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        # Stage-2 MF update (Q-learning)
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Stage-1 MF update: bootstrap toward chosen second-stage value and incorporate reward PE
        # (Equivalent to a strong eligibility without introducing an extra parameter)
        bootstrap = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * bootstrap
        q1_mf[a1] += alpha * (r - q2_mf[s, a2])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss