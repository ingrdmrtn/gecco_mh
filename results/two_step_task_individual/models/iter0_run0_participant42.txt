def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and perseveration.
    Computes the negative log-likelihood of observed first- and second-stage choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien within observed state).
    reward : array-like of float (typically 0 or 1)
        Trial outcomes.
    model_parameters : iterable
        [alpha, beta, w, lam, kappa]
        - alpha in [0,1]: Learning rate for value updates (both stages).
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - w in [0,1]: Weight on model-based action values at stage 1 (1=MB only, 0=MF only).
        - lam in [0,1]: Eligibility trace that propagates second-stage RPE to stage-1 values.
        - kappa in [0,1]: Perseveration bias added to the last chosen action at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed (known) transition structure: rows are stage-1 actions, cols are states
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)            # Q at stage 1 for spaceships A/U
    q_stage2_mf = np.zeros((2, 2))       # Q at stage 2 for each planet and alien

    # Last-choice indices for perseveration (initialize to None -> no bias on first trial)
    last_a1 = -1
    last_a2 = -1

    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation: use transition model to back up expected max Q at stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # max over aliens for each planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid stage-1 action values
        q1 = w * q_stage1_mb + (1 - w) * q_stage1_mf

        # Add perseveration bias at stage 1
        if last_a1 >= 0:
            bias1 = np.array([kappa if a == last_a1 else 0.0 for a in range(2)])
            q1 = q1 + bias1

        # Stage-1 policy and probability of observed choice
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy for the observed state
        q2 = q_stage2_mf[s].copy()

        # Add perseveration bias at stage 2
        if last_a2 >= 0:
            bias2 = np.array([kappa if a == last_a2 else 0.0 for a in range(2)])
            q2 = q2 + bias2

        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD error and update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF TD error and update (SARSA-style using observed second-stage value)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Eligibility trace: propagate part of the stage-2 RPE back to stage-1 MF value
        q_stage1_mf[a1] += alpha * lam * delta2

        # Update perseveration trackers
        last_a1 = a1
        last_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid with learned transitions, uncertainty-sensitive arbitration, forgetting, and novelty bonus.
    Computes the negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien within observed state).
    reward : array-like of float (typically 0 or 1)
        Trial outcomes.
    model_parameters : iterable
        [alpha, beta, omega, eta, phi]
        - alpha in [0,1]: Learning rate for both second-stage values and transition probabilities.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - omega in [0,1]: Base weight on model-based values at stage 1.
        - eta in [0,1]: Forgetting/decay of second-stage Q-values per trial (higher = faster decay).
        - phi in [0,1]: Novelty/exploration bonus magnitude for less-visited second-stage actions.

    Notes
    -----
    - Transition structure is learned online from observed transitions with rate alpha.
    - Arbitration weight is reduced by transition uncertainty (entropy): when transitions are
      unpredictable, the model relies more on model-free values.
    - A novelty bonus phi/sqrt(count+1) is added to second-stage Qs for action selection.
    - Forgetting applies to all second-stage values each trial before the update.
    """
    alpha, beta, omega, eta, phi = model_parameters
    n_trials = len(action_1)

    # Initialize transition model; rows: stage-1 action, cols: state index
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value tables
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))  # directly learned second-stage Qs

    # Counts for novelty bonus
    counts = np.ones((2, 2))  # start at 1 to avoid division by zero

    for t in range(n_trials):
        # Apply forgetting to all stage-2 values
        q_stage2 *= (1.0 - eta)

        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute model-based values via current transition model
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = T @ max_q2

        # Compute uncertainty of transitions for each action (row-wise entropy)
        # Normalize entropy to [0,1] by dividing by log(2)
        with np.errstate(divide='ignore', invalid='ignore'):
            ent = -np.sum(T * np.log(np.clip(T, 1e-12, 1.0)), axis=1) / np.log(2.0)
        ent = np.clip(ent, 0.0, 1.0)
        # Uncertainty-sensitive arbitration: downweight MB when entropy is high
        w_eff = omega * (1.0 - ent)
        # Stage-1 effective Qs per action: combine MF and MB using action-specific weights
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q_stage1_mf

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 novelty bonus for the current state
        bonus = phi / np.sqrt(counts[s] + 0.0)
        q2_aug = q_stage2[s] + bonus

        # Stage-2 policy
        exp_q2 = np.exp(beta * (q2_aug - np.max(q2_aug)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2
        counts[s, a2] += 1.0

        # Update model-free stage-1 via SARSA backup from updated second-stage value
        delta1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Update transition model row for chosen action toward the observed state (one-hot)
        one_hot = np.array([1.0 if ss == s else 0.0 for ss in range(2)])
        T[a1] = (1.0 - alpha) * T[a1] + alpha * one_hot
        # Ensure normalization and numerical safety
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] = T[a1] / np.sum(T[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Dynamic arbitration via volatility-driven latent weight, with eligibility traces.
    Computes the negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien within observed state).
    reward : array-like of float (typically 0 or 1)
        Trial outcomes.
    model_parameters : iterable
        [alpha, beta, theta, gamma, lam]
        - alpha in [0,1]: Learning rate for value updates (both stages).
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - theta in [0,1]: Initial arbitration weight on model-based control at stage 1.
        - gamma in [0,1]: Volatility update rate; adjusts arbitration based on surprise |RPE|.
        - lam in [0,1]: Eligibility trace to propagate second-stage RPE to stage-1 MF.

    Notes
    -----
    - The arbitration weight w_t evolves as: w_{t+1} = w_t + gamma*(|delta2_t| - w_t),
      increasing reliance on MB when recent outcomes are surprising, and decreasing otherwise.
    - Transition structure is fixed and known (common=0.7).
    """
    alpha, beta, theta, gamma, lam = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Initialize dynamic arbitration weight
    w = float(theta)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based evaluation via transition model
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid Q at stage 1 with dynamic weight
        q1 = w * q1_mb + (1.0 - w) * q_stage1_mf

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2 = q_stage2[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 MF update from second-stage value and eligibility
        delta1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1
        q_stage1_mf[a1] += alpha * lam * delta2

        # Volatility-driven arbitration update; keep w in [0,1]
        w = w + gamma * (abs(delta2) - w)
        w = min(max(w, 0.0), 1.0)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll