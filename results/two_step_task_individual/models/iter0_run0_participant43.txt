def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free RL with eligibility trace and perseveration.
    
    This model combines a model-based (MB) planner that uses the known transition
    structure with a model-free (MF) SARSA learner. An eligibility trace propagates
    second-stage reward prediction errors back to first-stage values. A choice
    perseveration bias encourages repeating the most recent action at each stage.
    
    Parameters (model_parameters):
    - alpha: scalar in [0,1]
        Learning rate for value updates at both stages.
    - beta: scalar in [0,10]
        Inverse temperature for softmax choice at both stages.
    - w: scalar in [0,1]
        Weight of the model-based action values at stage 1; (1-w) weights MF values.
    - lambd: scalar in [0,1]
        Eligibility trace strength; fraction of the second-stage RPE credited to the
        first-stage chosen action in addition to the MF bootstrap update.
    - kappa: scalar in [0,1]
        Perseveration strength added to the previously chosen action (as a bias in
        the softmax logits) at each stage.
    
    Inputs:
    - action_1: array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    - state: array-like of int {0,1}
        Second-stage state reached per trial (0=X, 1=Y).
    - action_2: array-like of int {0,1}
        Second-stage choices per trial (0 or 1; e.g., alien on the reached planet).
    - reward: array-like of float {0,1}
        Reward outcome per trial.
    
    Returns:
    - negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, lambd, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows are first-stage actions (A,U), columns are states (X,Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Likelihood tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)          # MF action values at stage 1
    q_stage2 = np.zeros((2, 2))        # Second-stage Q-values: state x action

    # Perseveration traces (one-hot previous action)
    prev_a1 = np.zeros(2)              # last first-stage action
    prev_a2 = np.zeros((2, 2))         # last second-stage action per state

    for t in range(n_trials):
        # Model-based action values computed from second-stage values
        max_q_stage2 = np.max(q_stage2, axis=1)  # for each state, best second-stage value
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value for each first-stage action

        # Hybrid first-stage action values + perseveration bias
        logits1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf + kappa * prev_a1
        exp1 = np.exp(beta * (logits1 - np.max(logits1)))
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy: add perseveration bias within the reached state
        s = state[t]
        logits2 = q_stage2[s].copy() + kappa * prev_a2[s]
        exp2 = np.exp(beta * (logits2 - np.max(logits2)))
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Cache pre-update value for RPE computations
        q2_sa_old = q_stage2[s, a2]

        # Reward prediction errors
        r = reward[t]
        delta2 = r - q2_sa_old
        delta1 = q2_sa_old - q_stage1_mf[a1]

        # Update MF values with eligibility trace back to stage 1
        q_stage2[s, a2] += alpha * delta2
        q_stage1_mf[a1] += alpha * (delta1 + lambd * delta2)

        # Update perseveration traces (last chosen actions)
        prev_a1 = np.zeros(2)
        prev_a1[a1] = 1.0
        prev_a2[s] = np.zeros(2)
        prev_a2[s, a2] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-seeking model-based RL with learned uncertainty bonus.
    
    This model uses the fixed transition structure for model-based planning but augments
    second-stage action values with an uncertainty bonus learned from recent surprise.
    The uncertainty trace at each second-stage option is updated toward the absolute
    prediction error, and both stages' choices use these augmented values.
    
    Parameters (model_parameters):
    - alpha: scalar in [0,1]
        Learning rate for second-stage value updates.
    - beta: scalar in [0,10]
        Inverse temperature for softmax choice at both stages.
    - u_bonus: scalar in [0,1]
        Weight on the uncertainty bonus added to action values to encourage exploration.
    - eta: scalar in [0,1]
        Learning rate for updating uncertainty traces from absolute prediction errors.
    
    Inputs:
    - action_1: array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    - state: array-like of int {0,1}
        Second-stage state reached per trial (0=X, 1=Y).
    - action_2: array-like of int {0,1}
        Second-stage choices per trial (0 or 1; e.g., alien on the reached planet).
    - reward: array-like of float {0,1}
        Reward outcome per trial.
    
    Returns:
    - negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, u_bonus, eta = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage values and uncertainty traces
    q_stage2 = np.zeros((2, 2))
    u_stage2 = np.full((2, 2), 0.5)  # start moderately uncertain

    for t in range(n_trials):
        # Augmented second-stage values: value + uncertainty bonus
        aug_q2 = q_stage2 + u_bonus * u_stage2

        # Model-based first-stage values via transitions and augmented second-stage values
        max_aug_q2 = np.max(aug_q2, axis=1)
        q_stage1_mb = transition_matrix @ max_aug_q2

        # First-stage policy
        logits1 = q_stage1_mb
        exp1 = np.exp(beta * (logits1 - np.max(logits1)))
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy using augmented values at reached state
        s = state[t]
        logits2 = aug_q2[s]
        exp2 = np.exp(beta * (logits2 - np.max(logits2)))
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observed reward and prediction error
        r = reward[t]
        q2_old = q_stage2[s, a2]
        pe = r - q2_old

        # Update value and uncertainty traces
        q_stage2[s, a2] += alpha * pe
        u_stage2[s, a2] = (1.0 - eta) * u_stage2[s, a2] + eta * abs(pe)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Hybrid RL with learned transition model (transition learning) and perseveration.
    
    This model learns the first-stage transition probabilities from experience and
    uses them for model-based planning. It also learns model-free values at both
    stages and mixes MB and MF values at stage 1. A perseveration bias encourages
    repeating the last first-stage action.
    
    Parameters (model_parameters):
    - alpha_r: scalar in [0,1]
        Learning rate for reward value updates (both stages).
    - alpha_t: scalar in [0,1]
        Learning rate for transition probability updates after each observed transition.
    - beta: scalar in [0,10]
        Inverse temperature for softmax choice at both stages.
    - w: scalar in [0,1]
        Weight on model-based values at stage 1; (1-w) weights model-free values.
    - rho: scalar in [0,1]
        Perseveration strength added to the previously chosen first-stage action in logits.
    
    Inputs:
    - action_1: array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    - state: array-like of int {0,1}
        Second-stage state reached per trial (0=X, 1=Y).
    - action_2: array-like of int {0,1}
        Second-stage choices per trial (0 or 1; e.g., alien on the reached planet).
    - reward: array-like of float {0,1}
        Reward outcome per trial.
    
    Returns:
    - negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, alpha_t, beta, w, rho = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model: rows actions (A,U) -> columns states (X,Y)
    T_hat = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Perseveration (first-stage only)
    prev_a1 = np.zeros(2)

    for t in range(n_trials):
        # Model-based values from learned transitions
        max_q2 = np.max(q_stage2, axis=1)
        q_stage1_mb = T_hat @ max_q2

        # Hybrid first-stage policy with perseveration
        logits1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf + rho * prev_a1
        exp1 = np.exp(beta * (logits1 - np.max(logits1)))
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = q_stage2[s]
        exp2 = np.exp(beta * (logits2 - np.max(logits2)))
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward and TD updates
        r = reward[t]
        q2_old = q_stage2[s, a2]
        delta2 = r - q2_old
        q_stage2[s, a2] += alpha_r * delta2

        delta1 = q2_old - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Update learned transition model for the chosen action using observed state
        # Move the chosen action's transition probabilities toward the observed state s
        # Row normalization is preserved by complementary updates.
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T_hat[a1, sp] += alpha_t * (target - T_hat[a1, sp])

        # Update perseveration trace
        prev_a1 = np.zeros(2)
        prev_a1[a1] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll