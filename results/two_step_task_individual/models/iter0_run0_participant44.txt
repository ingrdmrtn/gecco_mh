def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and first-stage perseveration.
    
    This model blends a model-based (MB) controller that uses the transition matrix
    with a model-free (MF) controller learned via TD and an eligibility trace.
    A first-stage perseveration bias adds value to repeating the previous first-stage choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Observed second-stage state on each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the observed state: 
        0/1 correspond to the two aliens on that planet (X: W/S; Y: P/H).
    reward : array-like of float (typically 0 or 1)
        Obtained reward on each trial.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, w, lam, kappa]
        - alpha in [0,1]: Learning rate for MF updates at both stages.
        - beta in [0,10]: Inverse temperature for both stages' softmax policies.
        - w in [0,1]: Arbitration weight of model-based (MB) over model-free (MF) at stage 1.
                      The effective first-stage Q is: Q1 = w*Q1_MB + (1-w)*Q1_MF.
        - lam in [0,1]: Eligibility trace weighting that propagates the stage-2 TD error to stage-1.
        - kappa in [0,1]: First-stage perseveration strength added to the previously chosen ship.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common = 0.7)
    # Rows: first-stage actions [A, U], Cols: states [X, Y]
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize storage for choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize action values
    q_stage1_mf = np.zeros(2)            # MF values for first-stage actions [A, U]
    q_stage2_mf = np.zeros((2, 2))       # MF values for second-stage actions by state: [X,Y] x [0,1]

    # Perseveration memory (index of previous choice, -1 means none)
    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        s = state[t]

        # Model-based first-stage values: expectation over states of the best second-stage option
        max_q_stage2 = np.max(q_stage2_mf, axis=1)            # best attainable per state
        q_stage1_mb = transition_matrix @ max_q_stage2         # expected best by action

        # Combine MB and MF with arbitration weight w
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Add first-stage perseveration bias
        q1_bias = np.zeros(2)
        if prev_a1 >= 0:
            q1_bias[prev_a1] += kappa
        q1_eff = q1_combined + q1_bias

        # First-stage policy (softmax)
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (softmax over MF values with second-stage perseveration on last second-stage choice)
        q2_eff = q_stage2_mf[s, :].copy()
        if prev_a2 >= 0:
            # Use a small fraction of kappa at stage 2 to keep parameter meaningful but focused on stage 1
            q2_eff[prev_a2] += 0.5 * kappa
        exp_q2 = np.exp(beta * (q2_eff - np.max(q2_eff)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # TD updates
        # Stage-2 MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace: combine "bootstrapped" value and propagated delta2
        # Use the chosen second-stage action value for bootstrapping the MF stage-1 update
        bootstrap = q_stage2_mf[s, a2]
        delta1 = bootstrap - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + alpha * lam * delta2

        # Update perseveration memories
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Purely model-free controller with transition-dependent credit assignment (TDCA) and stickiness.
    
    This model ignores explicit transition probabilities at choice time (no MB planning),
    but modulates how the first-stage MF values are updated depending on whether the observed
    transition was common or rare. A single stickiness parameter biases repetition at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Observed second-stage state per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state.
    reward : array-like of float
        Reward obtained per trial.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, kappa, lam, phi]
        - alpha in [0,1]: Learning rate used at both stages.
        - beta in [0,10]: Inverse temperature for both stages' softmax policies.
        - kappa in [0,1]: Stickiness (perseveration) strength shared across both stages.
        - lam in [0,1]: Eligibility trace that propagates the stage-2 TD error to stage-1.
        - phi in [0,1]: Transition-dependent credit modulation magnitude. 
                        Stage-1 update is boosted by +phi for common transitions and reduced by -phi for rare transitions.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, kappa, lam, phi = model_parameters
    n_trials = len(action_1)

    # Fixed notion of common vs rare based on the task structure
    # Common when A->X or U->Y; i.e., common if (action_1 == state)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)            # MF values for first-stage actions
    q2 = np.zeros((2, 2))       # MF values for second-stage actions by state

    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        s = state[t]

        # First-stage policy with stickiness
        q1_eff = q1.copy()
        if prev_a1 >= 0:
            q1_eff[prev_a1] += kappa
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with stickiness
        q2_eff = q2[s, :].copy()
        if prev_a2 >= 0:
            q2_eff[prev_a2] += kappa
        exp_q2 = np.exp(beta * (q2_eff - np.max(q2_eff)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 MF TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF TD update with eligibility trace and transition-dependent credit assignment
        # Base bootstrapped update
        bootstrap = q2[s, a2]
        delta1 = bootstrap - q1[a1]

        # Determine common vs rare (based on known task structure)
        is_common = 1.0 if (a1 == s) else 0.0
        # Transition-dependent modulation: +phi for common, -phi for rare
        mod = (2.0 * is_common - 1.0) * phi  # +phi if common; -phi if rare

        # Apply update: base MF + eligibility trace + TDCA modulation (scaled by |delta2|)
        # The modulation term uses the sign of delta2 by multiplying the TD error directly.
        q1[a1] += alpha * (delta1 + lam * delta2 + mod * delta2)

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with learned transition model, reward learning, and value decay plus lapses.
    
    This model learns the state-action transition matrix online and uses it to plan at the first stage.
    Second-stage action values are learned via a delta rule with forgetting/decay. A lapse parameter
    allows occasional random responding at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Observed second-stage state per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the observed state.
    reward : array-like of float
        Reward obtained per trial.
    model_parameters : list or array-like of 5 floats
        [alpha_R, beta, alpha_T, rho, epsilon]
        - alpha_R in [0,1]: Learning rate for second-stage reward values (Q2).
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - alpha_T in [0,1]: Learning rate for the transition model P(state | first-stage action).
        - rho in [0,1]: Value decay/forgetting rate applied to all second-stage Q-values each trial.
                        Effective update: Q2 <- (1 - rho)*Q2 before applying the delta rule.
        - epsilon in [0,1]: Lapse probability; with probability epsilon, choices are random (uniform).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_R, beta, alpha_T, rho, epsilon = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model P(s | a1), start uninformative (0.5/0.5) for both actions
    T = np.full((2, 2), 0.5)  # rows: actions [A,U], cols: states [X,Y]; rows sum to 1

    # Second-stage values (MF estimates of alien reward probabilities)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # No stickiness; randomness captured via epsilon
    for t in range(n_trials):
        s = state[t]

        # First-stage MB values: expected best attainable in each state under learned transitions
        max_Q2 = np.max(Q2, axis=1)          # value of taking the best alien in each state
        Q1_MB = T @ max_Q2                   # expected value per first-stage action

        # First-stage policy: softmax with lapses
        z1 = beta * (Q1_MB - np.max(Q1_MB))
        pi1_soft = np.exp(z1) / np.sum(np.exp(z1))
        pi1 = (1.0 - epsilon) * pi1_soft + epsilon * 0.5 * np.ones(2)
        a1 = action_1[t]
        p_choice_1[t] = pi1[a1]

        # Second-stage policy: softmax with lapses within observed state
        z2 = beta * (Q2[s, :] - np.max(Q2[s, :]))
        pi2_soft = np.exp(z2) / np.sum(np.exp(z2))
        pi2 = (1.0 - epsilon) * pi2_soft + epsilon * 0.5 * np.ones(2)
        a2 = action_2[t]
        p_choice_2[t] = pi2[a2]

        r = reward[t]

        # Apply global forgetting to all second-stage values
        Q2 *= (1.0 - rho)

        # Stage-2 reward learning (delta rule)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_R * delta2

        # Update transition model T via a simple delta rule toward the observed state
        # One-hot for observed state
        target = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        # Move the chosen row T[a1, :] toward the target and renormalize (guaranteed by construction)
        T[a1, :] = (1.0 - alpha_T) * T[a1, :] + alpha_T * target
        # Ensure numerical stability (rows sum to 1 by construction, but clip to avoid extremes)
        T[a1, :] = np.clip(T[a1, :], 1e-6, 1.0 - 1e-6)
        T[a1, :] /= np.sum(T[a1, :])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll