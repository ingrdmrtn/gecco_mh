def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and first-stage perseveration.
    
    This model blends a model-based (MB) planner using the known transition structure
    with a model-free (MF) SARSA learner. An eligibility trace lambda bootstraps
    reward prediction errors back to the first-stage value. A perseveration bias
    at stage 1 promotes repeating the last chosen spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0 = planet X, 1 = planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; two available aliens on the reached planet) per trial.
    reward : array-like of float (0 or 1)
        Rewards obtained at stage 2 per trial.
    model_parameters : iterable of floats
        [alpha, beta, w_mb, lam, kappa1]
        - alpha (learning rate) in [0,1]: MF learning rate for both stages.
        - beta (inverse temperature) in [0,10]: choice stochasticity for both stages.
        - w_mb (MB weight) in [0,1]: weight of model-based Q vs model-free Q at stage 1.
        - lam (eligibility trace) in [0,1]: propagates stage-2 RPE to stage-1 action value.
        - kappa1 (stage-1 perseveration) in [0,1]: additive bias toward repeating last stage-1 choice.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, w_mb, lam, kappa1 = model_parameters
    n_trials = len(action_1)

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X, Y]
                                  [0.3, 0.7]]) # from U to [X, Y]

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)           # for A/U
    q_stage2_mf = np.zeros((2, 2))      # for planets X/Y, actions 0/1

    # Last choices for perseveration (initialize neutral: no bias on first trial)
    last_a1 = None

    for t in range(n_trials):
        # Model-based component at stage 1: expected max over second stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [value of best alien on X, best on Y]
        q_stage1_mb = transition_matrix @ max_q_stage2  # expectation over next states

        # Hybrid stage-1 action values
        q1_net = (1.0 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb

        # Add stage-1 perseveration bias
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa1

        # Stage-1 policy (softmax)
        logits1 = beta * q1_net + bias1
        logits1 -= np.max(logits1)
        exp_q1 = np.exp(logits1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (softmax over reached state's MF values)
        s = state[t]
        logits2 = beta * q_stage2_mf[s, :]
        logits2 -= np.max(logits2)
        exp_q2 = np.exp(logits2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 TD error and update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace:
        # immediate bootstrapping toward stage-2 chosen value (SARSA),
        # plus traced component of the stage-2 RPE
        target1 = q_stage2_mf[s, a2]  # post-update SARSA target
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + alpha * lam * delta2

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA with value decay and separate perseveration at both stages.
    
    This model assumes participants learn solely from reinforcement at both stages,
    without planning. Q-values are pulled toward a neutral prior (0.5) via a decay
    process, capturing forgetfulness or nonstationarity tracking. Perseveration
    biases operate independently at stage 1 and stage 2.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0 = planet X, 1 = planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; two aliens per planet) per trial.
    reward : array-like of float (0 or 1)
        Rewards obtained at stage 2 per trial.
    model_parameters : iterable of floats
        [alpha, beta, kappa1, kappa2, decay]
        - alpha (learning rate) in [0,1]: MF learning rate at both stages.
        - beta (inverse temperature) in [0,10]: choice stochasticity for both stages.
        - kappa1 (stage-1 perseveration) in [0,1]: bias to repeat last spaceship.
        - kappa2 (stage-2 perseveration) in [0,1]: bias to repeat last alien within the same planet.
        - decay in [0,1]: per-trial pull of all Q-values toward 0.5 (0=no decay; 1=full reset).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, kappa1, kappa2, decay = model_parameters
    n_trials = len(action_1)

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values initialized at neutral 0.5 (matched to decay anchor)
    q_stage1 = np.ones(2) * 0.5         # A/U
    q_stage2 = np.ones((2, 2)) * 0.5    # X/Y x two aliens

    # Perseveration memory
    last_a1 = None
    # For stage-2 perseveration, track last action separately per planet
    last_a2 = [None, None]

    for t in range(n_trials):
        # Apply decay toward 0.5 before making choices (anticipatory forgetting)
        q_stage1 += decay * (0.5 - q_stage1)
        q_stage2 += decay * (0.5 - q_stage2)

        # Stage-1 policy with perseveration
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa1
        logits1 = beta * q_stage1 + bias1
        logits1 -= np.max(logits1)
        exp_q1 = np.exp(logits1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with within-state perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += kappa2
        logits2 = beta * q_stage2[s, :] + bias2
        logits2 -= np.max(logits2)
        exp_q2 = np.exp(logits2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates (SARSA-style back-up)
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 TD update toward the chosen second-stage value (post-update SARSA target)
        target1 = q_stage2[s, a2]
        delta1 = target1 - q_stage1[a1]
        q_stage1[a1] += alpha * delta1

        # Update perseveration memories
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with learned transition model, risk-sensitive utility, and perseveration.
    
    Participants are assumed to learn the transition probabilities from each spaceship
    to each planet across trials, while learning second-stage reward values. Stage-1
    action values are computed by model-based backup using the learned transition
    model. Rewards pass through a concave utility function controlled by eta, capturing
    risk sensitivity for stochastic payoffs. A stage-1 perseveration bias promotes
    repeating the previous spaceship choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0 = planet X, 1 = planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; two aliens per planet) per trial.
    reward : array-like of float (0 or 1)
        Rewards obtained at stage 2 per trial.
    model_parameters : iterable of floats
        [alpha_r, alpha_t, beta, eta, kappa1]
        - alpha_r (reward learning rate) in [0,1]: learning rate for second-stage Q-values.
        - alpha_t (transition learning rate) in [0,1]: learning rate for the transition model T(a1 -> state).
        - beta (inverse temperature) in [0,10]: choice stochasticity for both stages.
        - eta (risk sensitivity) in [0,1]: exponent on reward utility u = r^eta (eta=1 risk-neutral; eta<1 risk-averse for binary rewards).
        - kappa1 (stage-1 perseveration) in [0,1]: additive bias to repeat last stage-1 choice.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha_r, alpha_t, beta, eta, kappa1 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model T[a1, s] (rows sum to 1)
    # Start unbiased (0.5/0.5) to allow learning of common/rare structure
    T = np.ones((2, 2)) * 0.5

    # Second-stage Q-values (utility space)
    q_stage2 = np.zeros((2, 2))  # initialized at 0 utility

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    for t in range(n_trials):
        # Stage-1 model-based action values from learned transitions and current stage-2 values
        max_q2 = np.max(q_stage2, axis=1)  # best alien utility on each planet
        q1_mb = T @ max_q2

        # Add stage-1 perseveration bias
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa1

        # Stage-1 policy
        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        exp_q1 = np.exp(logits1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (softmax over utilities at reached planet)
        s = state[t]
        logits2 = beta * q_stage2[s, :]
        logits2 -= np.max(logits2)
        exp_q2 = np.exp(logits2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and compute utility with risk sensitivity
        r = reward[t]
        if (r == 0.0) and (eta == 0.0):
            u = 0.0
        else:
            u = r ** eta

        # Update second-stage values (utility prediction)
        delta2 = u - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * delta2

        # Update transition model for the chosen first-stage action based on observed state
        # Simple delta rule toward one-hot observation
        obs = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        T[a1, :] = (1 - alpha_t) * T[a1, :] + alpha_t * obs
        # Ensure numerical stability and row normalization (should already sum to 1)
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum <= 0:
            # fallback to uniform if pathological
            T[a1, :] = np.array([0.5, 0.5])
        else:
            T[a1, :] /= row_sum

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss