def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and perseveration.
    This model blends a model-based first-stage value with a model-free value,
    uses an eligibility trace to propagate second-stage rewards to first-stage values,
    and includes choice perseveration at both stages.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens available on the visited planet).
    reward : array-like of float
        Outcome per trial (e.g., coins earned; can be negative/zero/positive).
    model_parameters : tuple/list of 5 floats
        Contains, in order:
        - alpha (0 to 1): learning rate for model-free Q-value updates (both stages).
        - beta (0 to 10): inverse temperature for softmax choice at both stages.
        - w (0 to 1): weight of model-based value in the first-stage decision (1=fully MB).
        - lam (0 to 1): eligibility trace; scales how second-stage PE updates first-stage MF values.
        - kappa (0 to 1): perseveration strength; adds bias toward repeating previous action.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)       # for spaceships A,U
    q_stage2_mf = np.zeros((2, 2))  # for aliens on planets X,Y

    prev_a1 = None
    prev_a2 = [None, None]  # track previous second-stage action per state to allow state-dependent perseveration

    for t in range(n_trials):
        # Model-based first-stage value = T @ max_a2 Q2(s,a2)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # shape (2,)
        q_stage1_mb = transition_matrix @ max_q_stage2  # shape (2,)

        # Add perseveration biases
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa

        # Hybrid first-stage value
        q1_hybrid = w * q_stage1_mb + (1.0 - w) * q_stage1_mf + bias1

        # First-stage policy
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (state-specific perseveration)
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa
        q2_biased = q_stage2_mf[s, :] + bias2
        exp_q2 = np.exp(beta * (q2_biased - np.max(q2_biased)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD error and update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update: value toward second-stage chosen value (SARSA(λ)-style)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1
        # Eligibility trace: propagate second-stage PE to first-stage MF
        q_stage1_mf[a1] += alpha * lam * delta2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid with learned transition model, reward learning, and perseveration.
    This model learns the first-stage transition probabilities online and uses them
    to compute a model-based first-stage value. It blends model-based and model-free
    values, learns rewards via model-free TD at stage 2, and includes choice perseveration.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens).
    reward : array-like of float
        Outcome per trial (coins).
    model_parameters : tuple/list of 5 floats
        Contains, in order:
        - alpha_r (0 to 1): reward learning rate for model-free Q-value updates (both stages).
        - beta (0 to 10): inverse temperature for softmax choice at both stages.
        - w (0 to 1): weight of model-based value in first-stage decision (1=fully MB).
        - alpha_t (0 to 1): transition learning rate for updating T(s'|a1).
        - kappa (0 to 1): perseveration strength; bias to repeat previous action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_r, beta, w, alpha_t, kappa = model_parameters
    n_trials = len(action_1)

    # Initialize transition model with a "common-rare" prior (reflecting task structure)
    T = np.array([[0.7, 0.3],  # P(s'|A)
                  [0.3, 0.7]]) # P(s'|U)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    prev_a1 = None
    prev_a2 = [None, None]

    for t in range(n_trials):
        # Model-based first-stage values using learned transitions
        max_q2 = np.max(q_stage2_mf, axis=1)  # (2,)
        q1_mb = T @ max_q2

        # Perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa

        # Hybrid value
        q1_hyb = w * q1_mb + (1.0 - w) * q_stage1_mf + bias1

        # First-stage policy
        exp_q1 = np.exp(beta * (q1_hyb - np.max(q1_hyb)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with state-specific perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa
        q2_biased = q_stage2_mf[s, :] + bias2
        exp_q2 = np.exp(beta * (q2_biased - np.max(q2_biased)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # Update transition model T for chosen first-stage action based on observed state
        # Move chosen-action row toward one-hot indicating observed next state
        T[a1, s] += alpha_t * (1.0 - T[a1, s])
        T[a1, 1 - s] += alpha_t * (0.0 - T[a1, 1 - s])
        # Ensure numerical stability and row-normalization
        T[a1, :] = np.maximum(T[a1, :], 1e-6)
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

        # Reward learning: stage 2 TD
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * delta2

        # Model-free stage 1 update toward second-stage value (simple SARSA(0))
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive model-free with eligibility/forgetting and perseveration.
    This model is purely model-free. It uses asymmetric learning rates for
    positive vs. negative second-stage prediction errors (risk sensitivity),
    includes a single parameter that serves as both an eligibility trace to
    propagate second-stage outcomes to first-stage values and as a mild global
    forgetting factor, and adds perseveration biases.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens).
    reward : array-like of float
        Outcome per trial (coins).
    model_parameters : tuple/list of 5 floats
        Contains, in order:
        - alpha_pos (0 to 1): learning rate when the second-stage PE is positive.
        - alpha_neg (0 to 1): learning rate when the second-stage PE is negative or zero.
        - beta (0 to 10): inverse temperature for softmax choice at both stages.
        - rho (0 to 1): dual-role parameter:
              * eligibility trace scaling for propagating second-stage PE to first-stage MF values,
              * mild global forgetting factor applied to all Q-values each trial.
        - kappa (0 to 1): perseveration strength; bias to repeat previous action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, rho, kappa = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    prev_a1 = None
    prev_a2 = [None, None]

    # Per-trial forgetting factor derived from rho (kept modest to avoid wiping values)
    forget = 1.0 - 0.5 * rho  # in [0.5, 1], larger rho -> more forgetting

    for t in range(n_trials):
        # Apply mild global forgetting to all Q-values
        q_stage1_mf *= forget
        q_stage2_mf *= forget

        # Perseveration biases
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa
        # First-stage policy (pure MF)
        q1_pol = q_stage1_mf + bias1
        exp_q1 = np.exp(beta * (q1_pol - np.max(q1_pol)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with state-dependent perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa
        q2_pol = q_stage2_mf[s, :] + bias2
        exp_q2 = np.exp(beta * (q2_pol - np.max(q2_pol)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        # Risk-sensitive learning rate selection
        alpha = alpha_pos if delta2 > 0 else alpha_neg
        # Update second-stage Q
        q_stage2_mf[s, a2] += alpha * delta2

        # Update first-stage MF with SARSA(λ)-like rule using rho as eligibility factor
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1
        # Propagate second-stage PE to first-stage value
        q_stage1_mf[a1] += alpha * rho * delta2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll