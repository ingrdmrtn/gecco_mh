def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with eligibility trace and choice stickiness.
    
    This model blends model-based action values (computed from a known transition
    structure) and model-free values learned via TD updates. The model uses an
    eligibility trace to propagate second-stage reward prediction errors back to
    the first-stage values and includes a choice stickiness bias applied at both
    stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens indexed 0 or 1 within the reached planet).
    reward : array-like of float
        Obtained reward on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, w, lam, kappa]
        - alpha (learning rate): [0, 1]
        - beta (inverse temperature): [0, 10]
        - w (model-based weight, MB/MF mixture): [0, 1]
        - lam (eligibility trace strength from stage 2 to 1): [0, 1]
        - kappa (choice stickiness bias magnitude, applied at both stages): [0, 1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Known transition structure: rows are first-stage actions (A=0,U=1), columns are states (X=0,Y=1).
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of selected actions to accumulate log-likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action-value functions
    q1_mf = np.zeros(2)          # model-free Q for stage 1 (2 actions)
    q2_mf = np.zeros((2, 2))     # model-free Q for stage 2 (2 states x 2 actions)

    # Previous choices for stickiness (initialize to None => no bias on first occurrence)
    prev_a1 = None
    prev_a2_per_state = [None, None]

    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation for stage 1 uses current second-stage MF values
        max_q2 = np.max(q2_mf, axis=1)                  # best second-stage values per state
        q1_mb = transition_matrix @ max_q2              # MB value for each first-stage action

        # Hybrid first-stage values
        q1_hyb = w * q1_mb + (1.0 - w) * q1_mf

        # Add stickiness to first-stage logits
        stick1 = np.zeros(2)
        if prev_a1 is not None:
            stick1[prev_a1] += kappa

        # First-stage policy
        logits1 = beta * q1_hyb + stick1
        logits1 -= np.max(logits1)  # numerical stability
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (within observed state s)
        stick2 = np.zeros(2)
        if prev_a2_per_state[s] is not None:
            stick2[prev_a2_per_state[s]] += kappa

        q2_row = q2_mf[s, :]
        logits2 = beta * q2_row + stick2
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        # Compute TD errors using values before updates
        q2_old = q2_mf[s, a2]
        delta2 = reward[t] - q2_old
        delta1 = q2_old - q1_mf[a1]

        # Update second-stage MF values
        q2_mf[s, a2] += alpha * delta2

        # Update first-stage MF values (direct bootstrapping + eligibility trace from stage-2 RPE)
        q1_mf[a1] += alpha * delta1 + alpha * lam * delta2

        # Update previous choices for stickiness
        prev_a1 = a1
        prev_a2_per_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planner with learned transition model, value decay, and first-stage perseveration.
    
    The agent learns the transition probabilities from first-stage actions to second-stage
    states online and plans by evaluating the expected value of first-stage actions via
    those learned transitions and the current second-stage values. Second-stage values are
    learned by TD and also subject to decay toward zero, capturing forgetting or volatility.
    A first-stage perseveration bias adds a tendency to repeat the previous first-stage choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens indexed 0 or 1 within the reached planet).
    reward : array-like of float
        Obtained reward on each trial.
    model_parameters : iterable of 5 floats
        [alpha_R, beta, alpha_T, rho, decay]
        - alpha_R (reward learning rate for second stage): [0, 1]
        - beta (inverse temperature for both stages): [0, 10]
        - alpha_T (transition learning rate): [0, 1]
        - rho (first-stage perseveration bias magnitude): [0, 1]
        - decay (per-trial decay of second-stage values toward 0): [0, 1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_R, beta, alpha_T, rho, decay = model_parameters
    n_trials = len(action_1)

    # Learned transition matrix T: rows are first-stage actions (0/1), cols are states (0/1). Initialize uniform.
    T = np.full((2, 2), 0.5)

    # Second-stage value function (2 states x 2 actions)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation for stage 1 using learned transitions
        max_Q2 = np.max(Q2, axis=1)        # best second-stage values per state
        Q1_mb = T @ max_Q2                 # expected value of each first-stage action

        # First-stage perseveration
        stick1 = np.zeros(2)
        if prev_a1 is not None:
            stick1[prev_a1] += rho

        # First-stage policy
        logits1 = beta * Q1_mb + stick1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy within observed state s
        logits2 = beta * Q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates

        # 1) Transition learning for the taken first-stage action row
        # Move T[a1, :] toward one-hot at observed state s
        T[a1, :] = (1.0 - alpha_T) * T[a1, :]
        T[a1, s] += alpha_T  # ensures row remains normalized

        # 2) Second-stage reward learning with decay toward zero
        # Apply decay to all Q2 entries to capture forgetting/volatility
        Q2 *= (1.0 - decay)
        # TD update for the selected second-stage action
        Q2[s, a2] += alpha_R * (reward[t] - Q2[s, a2])

        # Update perseveration memory
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA with asymmetric learning rates, reward sensitivity, and stage-2 stickiness.
    
    The agent learns both stages' values via model-free temporal-difference updates.
    Second-stage reward prediction errors use asymmetric learning rates depending on
    the sign of the error (positive vs. negative outcomes). The effective reward is
    scaled by a reward sensitivity parameter. The first-stage values are updated via
    bootstrapping from second-stage values plus the second-stage RPE (equivalent to
    an eligibility trace of 1). A stage-2 stickiness bias encourages repeating the
    previous action in the same second-stage state.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens indexed 0 or 1 within the reached planet).
    reward : array-like of float
        Obtained reward on each trial.
    model_parameters : iterable of 5 floats
        [alpha_pos, alpha_neg, beta, psi, eta]
        - alpha_pos (learning rate for positive RPEs): [0, 1]
        - alpha_neg (learning rate for negative RPEs): [0, 1]
        - beta (inverse temperature for both stages): [0, 10]
        - psi (stage-2 stickiness bias magnitude, per state): [0, 1]
        - eta (reward sensitivity scaling of outcomes): [0, 1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, psi, eta = model_parameters
    n_trials = len(action_1)

    # Model-free Q-values
    Q1 = np.zeros(2)        # stage-1 (2 actions)
    Q2 = np.zeros((2, 2))   # stage-2 (2 states x 2 actions)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous stage-2 action per state for stickiness
    prev_a2_per_state = [None, None]

    for t in range(n_trials):
        s = state[t]

        # First-stage policy (pure MF)
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness in state s
        stick2 = np.zeros(2)
        if prev_a2_per_state[s] is not None:
            stick2[prev_a2_per_state[s]] += psi

        logits2 = beta * Q2[s, :] + stick2
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        # Effective reward with sensitivity scaling
        r_eff = eta * reward[t]

        # Use current values to compute TD errors before updates
        q2_old = Q2[s, a2]
        delta2 = r_eff - q2_old

        # Select asymmetric learning rate based on sign of delta2
        alpha2 = alpha_pos if delta2 >= 0.0 else alpha_neg

        # Update stage-2 values
        Q2[s, a2] += alpha2 * delta2

        # Stage-1 update: bootstrap toward second-stage value and include the second-stage RPE (Î» = 1)
        delta1_boot = q2_old - Q1[a1]
        Q1[a1] += alpha2 * (delta1_boot + delta2)

        # Update stickiness memory
        prev_a2_per_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll