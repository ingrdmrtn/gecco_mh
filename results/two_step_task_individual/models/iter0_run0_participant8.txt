Below are three standalone Python functions that implement distinct cognitive models for the two-step task. Each function takes arrays of trial-by-trial observations and a parameter vector, and returns the negative log-likelihood of the observed first- and second-stage choices under the model.

Note: Assume numpy as np is already imported. Do not add imports inside the code.

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model-based/model-free with eligibility trace and perseveration (single beta).
    - Stage 1 policy: mixture of model-based (via fixed transition model) and model-free Q1.
    - Stage 2 policy: model-free Q2, with perseveration at both stages.
    - Learning: TD learning at stage 2; stage 1 MF receives direct TD(0) from Q2 plus eligibility trace lambda from reward.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 denote the two aliens on the planet).
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, w, lam, kappa]
        - alpha in [0,1]: learning rate for value updates.
        - beta in [0,10]: inverse temperature for both stages' softmax.
        - w in [0,1]: weight on model-based values at stage 1 (1-w on model-free).
        - lam in [0,1]: eligibility trace weighting from stage-2 prediction error to stage-1 MF update.
        - kappa in [0,1]: perseveration strength added to previously chosen action at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X and U->Y are common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)          # stage-1 model-free Q
    q2 = np.zeros((2, 2))        # stage-2 Q for each state and action

    # Perseveration memory (last chosen actions)
    prev_a1 = -1
    prev_a2_state = np.full(2, -1, dtype=int)  # keep last chosen action per state for stage 2

    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation for stage 1
        max_q2 = np.max(q2, axis=1)                  # best available value at each state
        q1_mb = transition_matrix @ max_q2           # expected value per first-stage action

        # Hybrid action values at stage 1 + perseveration bias
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        pref1 = q1.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += kappa

        # Softmax for stage-1 policy
        exp_q1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration
        pref2 = q2[s, :].copy()
        pa2 = prev_a2_state[s]
        if pa2 in (0, 1):
            pref2[pa2] += kappa

        exp_q2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD updates
        # Stage-2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF receives direct TD(0) from stage-2 Q plus eligibility from reward PE
        # (Daw et al. hybrid-like update)
        td0 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td0 + alpha * lam * delta2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Model-based with learned transitions, stage-wise perseveration, and transition-dependent repetition bias.
    - Learns the transition probabilities T(a -> s) online with learning rate eta.
    - Stage 1 policy is purely model-based using learned T and current Q2.
    - Stage 2 policy is softmax over Q2.
    - Includes perseveration bias (pi) at both stages and an additional bias (phi) to repeat the previous
      first-stage choice specifically after a rare transition on the previous trial.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 denote the two aliens on the planet).
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, eta, pi, phi]
        - alpha in [0,1]: learning rate for stage-2 Q values.
        - beta in [0,10]: inverse temperature for both stages' softmax.
        - eta in [0,1]: learning rate for transition probabilities T.
        - pi in [0,1]: perseveration strength to repeat previous action (both stages).
        - phi in [0,1]: extra bias to repeat the previous stage-1 action after a rare transition on the previous trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, eta, pi, phi = model_parameters
    n_trials = len(action_1)

    # Initialize transition beliefs (rows sum to 1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2))  # stage-2 Q

    # Memory for perseveration and rare/common tagging
    prev_a1 = -1
    prev_state = -1
    prev_T_for_tag = T.copy()  # store T from previous trial for rare/common tagging
    prev_a2_state = np.full(2, -1, dtype=int)

    for t in range(n_trials):
        s = state[t]

        # Model-based expected values at stage 1 using current T
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add perseveration and rare-dependent repetition bias at stage 1
        pref1 = q1_mb.copy()
        if prev_a1 in (0, 1):
            # Base perseveration
            pref1[prev_a1] += pi
            # Rare-dependent extra bias: if the last transition was rare, add phi to repeating
            if prev_state in (0, 1):
                prob_prev = prev_T_for_tag[prev_a1, prev_state]
                was_rare = 1.0 if prob_prev < 0.5 else 0.0
                if was_rare > 0.0:
                    pref1[prev_a1] += phi

        exp_q1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration in the reached state
        pref2 = q2[s, :].copy()
        pa2 = prev_a2_state[s]
        if pa2 in (0, 1):
            pref2[pa2] += pi

        exp_q2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update Q2 with reward
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learn transitions from chosen first-stage action to observed state
        # Row-wise update keeps row summing to 1
        a = a1
        s_obs = s
        for s_val in (0, 1):
            target = 1.0 if s_val == s_obs else 0.0
            T[a, s_val] += eta * (target - T[a, s_val])

        # Keep a copy of T before next trial for rare/common tagging
        prev_T_for_tag = T.copy()

        # Update perseveration memory
        prev_a1 = a1
        prev_state = s
        prev_a2_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Pure model-free SARSA(Î») with asymmetric learning rates and perseveration.
    - Stage 1 and 2 policies: softmax over model-free Q-values.
    - Learning: asymmetric learning rates for positive vs. negative prediction errors,
      plus eligibility trace from stage-2 PE to stage-1 update.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 denote the two aliens on the planet).
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_pos, alpha_neg, beta, lam, kappa]
        - alpha_pos in [0,1]: learning rate when prediction error is positive.
        - alpha_neg in [0,1]: learning rate when prediction error is negative.
        - beta in [0,10]: inverse temperature for both stages' softmax.
        - lam in [0,1]: eligibility trace mixing stage-2 PE into stage-1 update.
        - kappa in [0,1]: perseveration strength added to previously chosen action (both stages).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, lam, kappa = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)        # stage-1 Q
    q2 = np.zeros((2, 2))   # stage-2 Q

    # Perseveration memory
    prev_a1 = -1
    prev_a2_state = np.full(2, -1, dtype=int)

    for t in range(n_trials):
        s = state[t]

        # Stage-1 policy with perseveration
        pref1 = q1.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += kappa

        exp_q1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration
        pref2 = q2[s, :].copy()
        pa2 = prev_a2_state[s]
        if pa2 in (0, 1):
            pref2[pa2] += kappa

        exp_q2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 update with asymmetric learning
        pe2 = r - q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2 * pe2

        # Stage-1 update: TD towards updated Q2 plus eligibility from stage-2 PE
        pe1 = q2[s, a2] - q1[a1]
        alpha1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        q1[a1] += alpha1 * pe1 + alpha2 * lam * pe2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll