def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free learner with eligibility trace (位) and separate inverse temperatures.
    
    Uses:
    - Model-free values at both stages (Q-learning).
    - Model-based plan at stage 1 by projecting stage-2 values through the transition structure.
    - Hybrid arbitration between MF and MB at stage 1 via weight w.
    - Eligibility trace 位 to propagate outcome back to stage-1 MF values.
    - Separate softmax temperatures for stage 1 and stage 2.

    Parameters (with bounds):
    - alpha in [0, 1]: learning rate for value updates.
    - beta1 in [0, 10]: inverse temperature for the first-stage softmax.
    - beta2 in [0, 10]: inverse temperature for the second-stage softmax.
    - w in [0, 1]: weight blending model-based and model-free values at stage 1.
    - lam in [0, 1]: eligibility trace controlling how much the reward directly updates stage-1 MF values.

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien on the reached planet (two per planet).
    - reward: array-like of floats (typically 0 or 1), coins received.
    - model_parameters: list/array [alpha, beta1, beta2, w, lam].

    Returns:
    - Negative log-likelihood of observed choices (float).
    """
    alpha, beta1, beta2, w, lam = model_parameters
    n_trials = len(action_1)

    # Known transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities for each observed choice per trial
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-1 model-free Q and stage-2 model-free Q
    q1_mf = np.zeros(2)               # for spaceships A,U
    q2_mf = np.zeros((2, 2))          # for aliens on planet X (0) and Y (1)

    for t in range(n_trials):
        # Model-based computation at stage 1: project max stage-2 values through transitions
        max_q2 = np.max(q2_mf, axis=1)                  # best alien on each planet
        q1_mb = transition_matrix @ max_q2              # expected value for each spaceship

        # Hybrid arbitration
        q1 = w * q1_mb + (1 - w) * q1_mf

        # Softmax for stage 1
        q1_stable = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (planet-specific)
        s = state[t]
        q2 = q2_mf[s]
        q2_stable = q2 - np.max(q2)
        exp_q2 = np.exp(beta2 * q2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Store current values for TD errors before updating
        q2_old = q2_mf[s, a2]
        q1_old = q1_mf[a1]

        # TD error at stage 2
        delta2 = r - q2_old
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace 位
        # Two components:
        #  (1) bootstrap from stage-2 value: q2_old - q1_old
        #  (2) direct outcome propagation: r - q1_old (scaled by 位)
        delta1_bootstrap = q2_old - q1_old
        delta1_outcome = r - q1_old
        q1_mf[a1] += alpha * ((1 - lam) * delta1_bootstrap + lam * delta1_outcome)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid learner with learned transition model and first-stage perseveration bias.
    
    Uses:
    - Online learning of transition probabilities for each spaceship.
    - Model-based planning at stage 1 using learned transitions.
    - Model-free values at both stages.
    - Hybrid arbitration between MB and MF at stage 1 via w.
    - First-stage choice perseveration (stickiness) bias.

    Parameters (with bounds):
    - alpha in [0, 1]: learning rate for value updates at stage 2 and stage-1 MF.
    - alpha_t in [0, 1]: learning rate for transition matrix updates.
    - beta in [0, 10]: inverse temperature for both stages' softmax policies.
    - w in [0, 1]: weight blending MB and MF values at stage 1.
    - phi in [0, 1]: perseveration bias magnitude added to the previously chosen first-stage action.

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien on the reached planet.
    - reward: array-like of floats (typically 0 or 1), coins received.
    - model_parameters: list/array [alpha, alpha_t, beta, w, phi].

    Returns:
    - Negative log-likelihood of observed choices (float).
    """
    alpha, alpha_t, beta, w, phi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model to neutral (0.5/0.5)
    T = np.ones((2, 2)) * 0.5  # rows: actions (spaceships), cols: states (planets)

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    prev_a1 = None

    for t in range(n_trials):
        # Model-based at stage 1 with learned transitions
        max_q2 = np.max(q2_mf, axis=1)   # best alien per planet
        q1_mb = T @ max_q2

        # Hybrid arbitration
        q1 = w * q1_mb + (1 - w) * q1_mf

        # Add perseveration bias to the previously chosen action (if exists)
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += phi

        q1_biased = q1 + bias

        # Stage 1 softmax
        q1_stable = q1_biased - np.max(q1_biased)
        exp_q1 = np.exp(beta * q1_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        q2 = q2_mf[s]
        q2_stable = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update learned transitions for chosen action toward observed state
        # Binary state: s in {0,1}; ensure row sums to 1
        # Update probability of transitioning to s
        T[a1, s] += alpha_t * (1.0 - T[a1, s])
        # Complementary state gets the remaining probability
        other_s = 1 - s
        T[a1, other_s] = 1.0 - T[a1, s]

        # TD updates (model-free)
        q2_old = q2_mf[s, a2]
        q1_old = q1_mf[a1]

        delta2 = r - q2_old
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF bootstraps off stage-2 value
        delta1 = q2_old - q1_old
        q1_mf[a1] += alpha * delta1

        # Track perseveration
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Risk-/surprise-sensitive model-based policy with asymmetric learning and forgetting at stage 2.
    
    Uses:
    - Pure model-based planning at stage 1 via known transitions.
    - Stage-2 model-free values with asymmetric learning rates for rewarded vs unrewarded outcomes.
    - Global forgetting/decay of stage-2 values toward 0.5 to capture volatility tracking.
    - Surprise-modulated exploration: inverse temperature scales with absolute last prediction error.

    Parameters (with bounds):
    - alpha_pos in [0, 1]: learning rate when reward > current value (positive PE).
    - alpha_neg in [0, 1]: learning rate when reward < current value (negative PE).
    - beta0 in [0, 10]: baseline inverse temperature.
    - kappa in [0, 1]: sensitivity of inverse temperature to surprise (|last PE|).
    - gamma in [0, 1]: forgetting rate toward 0.5 applied each trial to all stage-2 values.

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien on the reached planet.
    - reward: array-like of floats (typically 0 or 1), coins received.
    - model_parameters: list/array [alpha_pos, alpha_neg, beta0, kappa, gamma].

    Returns:
    - Negative log-likelihood of observed choices (float).
    """
    alpha_pos, alpha_neg, beta0, kappa, gamma = model_parameters
    n_trials = len(action_1)

    # Known transitions (common = 0.7)
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q values
    q2 = np.zeros((2, 2))  # planet x action

    # Last absolute prediction error to modulate exploration; start at 0
    last_abs_pe = 0.0

    for t in range(n_trials):
        # Surprise-modulated inverse temperature (applied to both stages)
        beta_eff = beta0 * (1.0 + kappa * last_abs_pe)

        # Stage-1 model-based values: expected value by projecting max(q2) through T
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Softmax stage 1
        q1_stable = q1_mb - np.max(q1_mb)
        exp_q1 = np.exp(beta_eff * q1_stable)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy on reached planet
        s = state[t]
        q2_s = q2[s]
        q2_stable = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta_eff * q2_stable)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Compute PE and asymmetric update
        q_old = q2[s, a2]
        pe = r - q_old
        alpha = alpha_pos if pe >= 0 else alpha_neg
        q2[s, a2] += alpha * pe

        # Global forgetting toward 0.5 (applied after update)
        q2 = (1.0 - gamma) * q2 + gamma * 0.5

        # Update surprise for next trial
        last_abs_pe = abs(pe)

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss