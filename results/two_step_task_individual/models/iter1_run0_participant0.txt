def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Asymmetric reward learning at stage 2, model-based planning at stage 1,
    plus a learned transition-surprise bias and perseveration at stage 1.
    
    Mechanism
    - Stage 2: Q-learning with asymmetric learning rates for rewarded vs. unrewarded outcomes.
    - Stage 1: Model-based planning using the known transition structure (common=0.7).
    - Transition-surprise bias: After observing the transition (common vs. rare), the chosen
      first-stage action acquires a bias toward repeating after common transitions and away
      after rare transitions, with decay across trials.
    - Perseveration: Additional bias to repeat the last first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float/int (e.g., 0 or 1)
        Obtained outcome per trial.
    model_parameters : iterable of floats
        [alpha_pos, alpha_neg, beta, psi, rho]
        - alpha_pos in [0,1]: learning rate for stage-2 values after reward=1.
        - alpha_neg in [0,1]: learning rate for stage-2 values after reward=0.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - psi in [0,1]: surprise-bias update/decay rate (larger -> faster, stronger bias).
                         Bias drifts toward +1 after common and -1 after rare transitions for the chosen a1.
        - rho in [0,1]: perseveration strength at stage 1 (bias to repeat last a1).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, psi, rho = model_parameters
    n_trials = len(action_1)

    # Known transition structure (rows: a1, cols: states)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities per trial
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values: Q2[state, action]
    q2 = np.zeros((2, 2))

    # Transition-surprise bias stored per first-stage action; decays over time
    bias_a1 = np.zeros(2)
    last_a1 = None

    for t in range(n_trials):
        # Stage 1 policy: model-based using max over Q2 at each state
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2           # shape (2,)

        # Add perseveration bias for repeating last first-stage action
        pref1 = beta * q1_mb + bias_a1.copy()
        if last_a1 is not None:
            pref1[last_a1] += rho

        pref1 -= np.max(pref1)
        probs1 = np.exp(pref1) / np.sum(np.exp(pref1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy within reached state
        s = state[t]
        pref2 = beta * q2[s, :]
        pref2 -= np.max(pref2)
        probs2 = np.exp(pref2) / np.sum(np.exp(pref2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 learning with asymmetric learning rates
        alpha_t = alpha_pos if r > 0.5 else alpha_neg
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * delta2

        # Update transition-surprise bias for the chosen a1 based on observed transition
        # Move bias[a1] toward +1 for common, toward -1 for rare; decay by (1-psi).
        is_common = 1.0 if T[a1, s] >= 0.5 else 0.0
        target = 1.0 if is_common > 0.5 else -1.0
        bias_a1 *= (1.0 - psi)
        bias_a1[a1] += psi * target

        last_a1 = a1

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-directed exploration at stage 2 and transition-trust at stage 1,
    with perseveration.
    
    Mechanism
    - Stage 2: Softmax over Q-values plus an uncertainty bonus that decays with visits:
               bonus = tau / sqrt(N[s,a] + 1). Encourages directed exploration early on.
    - Stage 1: Model-based evaluation using an "effective" transition model that blends
               the known transition matrix with an agnostic uniform model:
               T_eff = xi * T_known + (1 - xi) * 0.5.
      Thus xi tunes reliance on the known structure (planning trust).
    - Perseveration: Bias to repeat the last choice at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float/int
        Outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, tau, xi, rho]
        - alpha in [0,1]: learning rate for stage-2 values.
        - beta in [0,10]: inverse temperature for softmax.
        - tau in [0,1]: strength of uncertainty bonus at stage 2.
        - xi in [0,1]: trust in known transition model (xi=1 uses T_known, xi=0 treats transitions as uniform).
        - rho in [0,1]: perseveration strength applied at both stages.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, tau, xi, rho = model_parameters
    n_trials = len(action_1)

    # Known transition matrix
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])
    T_eff = xi * T_known + (1.0 - xi) * 0.5  # rows automatically sum to ~1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2))
    visits = np.zeros((2, 2))  # visit counts per state-action

    last_a1 = None
    last_a2 = [None, None]

    for t in range(n_trials):
        # Stage 1 policy from model-based evaluation using T_eff
        max_q2 = np.max(q2, axis=1)      # value of each state under best a2
        q1_mb = T_eff @ max_q2           # action values at stage 1

        pref1 = beta * q1_mb
        if last_a1 is not None:
            pref1[last_a1] += rho

        pref1 -= np.max(pref1)
        probs1 = np.exp(pref1) / np.sum(np.exp(pref1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with uncertainty bonus
        s = state[t]
        bonus = tau / np.sqrt(visits[s, :] + 1.0)  # higher when less visited
        pref2 = beta * (q2[s, :] + bonus)
        if last_a2[s] is not None:
            pref2[last_a2[s]] += rho

        pref2 -= np.max(pref2)
        probs2 = np.exp(pref2) / np.sum(np.exp(pref2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update counts and Q2
        visits[s, a2] += 1.0
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-free at stage 1 with credit assignment from stage 2, gated by transition rarity,
    and lapses in both stages.
    
    Mechanism
    - Stage 2: Standard Q-learning with softmax policy; also subject to lapse (epsilon).
    - Stage 1: Model-free Q1 is updated from the stage-2 prediction error (delta2) scaled by
      a credit parameter gamma_mf and additionally gated by whether the experienced transition
      was common or rare. The 'rare_scale' parameter scales credit on rare transitions.
    - Lapse: With probability epsilon, choices are random at both stages; otherwise softmax.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float/int
        Outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, gamma_mf, rare_scale, epsilon]
        - alpha in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - gamma_mf in [0,1]: strength of credit assignment from stage 2 to stage-1 Q.
        - rare_scale in [0,1]: multiplicative scale on credit when the transition was rare
                               (0 suppresses, 1 treats rare like common).
        - epsilon in [0,1]: lapse rate; with prob epsilon, choices are random (uniform).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, gamma_mf, rare_scale, epsilon = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix to determine rarity
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 and Stage-1 Q-values
    q2 = np.zeros((2, 2))  # Q2[state, action]
    q1 = np.zeros(2)       # Q1[action]

    for t in range(n_trials):
        # Stage 1 policy: softmax over Q1, with lapse
        pref1 = beta * q1
        pref1 -= np.max(pref1)
        soft1 = np.exp(pref1) / np.sum(np.exp(pref1))
        probs1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: softmax over Q2 within reached state, with lapse
        s = state[t]
        pref2 = beta * q2[s, :]
        pref2 -= np.max(pref2)
        soft2 = np.exp(pref2) / np.sum(np.exp(pref2))
        probs2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Model-free credit assignment to stage 1, gated by transition rarity
        is_rare = 1.0 if T[a1, s] < 0.5 else 0.0
        scale = (1.0 - is_rare) + is_rare * rare_scale  # 1 for common, rare_scale for rare
        q1[a1] += gamma_mf * alpha * delta2 * scale

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))