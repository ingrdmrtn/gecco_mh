def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with eligibility trace, value forgetting, and dual perseveration.
    
    This model learns only from experienced rewards without planning. The first-stage
    values are updated by a two-step TD rule with eligibility trace, and unvisited
    action values are forgotten toward a neutral baseline. Perseveration biases
    increase the tendency to repeat the previous choice at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, lam, decay, sigma]
        - alpha (learning rate, [0,1]): TD learning for value updates.
        - beta (inverse temperature, [0,10]): softmax for both stages.
        - lam (eligibility trace, [0,1]): carries second-stage TD error back to stage 1.
        - decay (forgetting rate, [0,1]): pulls unvisited action values toward 0.5.
        - sigma (perseveration strength, [0,1]): additive bias to repeat the previous choice
          at stage 1 and to repeat the last stage-2 action within the same planet.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, lam, decay, sigma = model_parameters
    n_trials = len(action_1)

    # Model-free action values
    q1 = np.zeros(2)                # stage-1 MF Q-values (spaceships)
    q2 = np.full((2, 2), 0.5)       # stage-2 MF Q-values (aliens per planet), start neutral

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    last_a1 = None
    last_a2_for_state = [None, None]

    baseline = 0.5

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 policy with perseveration
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += sigma
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with state-specific perseveration
        bias2 = np.zeros(2)
        if last_a2_for_state[s] is not None:
            bias2[last_a2_for_state[s]] += sigma
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning: stage-1 MF with eligibility trace from stage-2
        # Standard two-step MF(Î»): back up immediate second-stage value and its TD error
        q1[a1] += alpha * (q2[s, a2] - q1[a1]) + alpha * lam * (r - q2[s, a2])

        # Forgetting: move unvisited values toward baseline
        # Stage-1: the unchosen spaceship
        unchosen_a1 = 1 - a1
        q1[unchosen_a1] = (1 - decay) * q1[unchosen_a1] + decay * baseline

        # Stage-2: all (state,action) pairs except the visited one
        for ss in (0, 1):
            for aa in (0, 1):
                if not (ss == s and aa == a2):
                    q2[ss, aa] = (1 - decay) * q2[ss, aa] + decay * baseline

        # Update perseveration memory
        last_a1 = a1
        last_a2_for_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Bayesian reward learning with directed exploration and second-stage perseveration.
    
    This model maintains Beta posteriors over second-stage reward probabilities and
    plans at stage 1 using the known transition structure and an optimism bonus
    proportional to posterior uncertainty (UCB-like). Counts decay over time to allow
    adaptation to nonstationarity. A perseveration bias encourages repeating the most
    recent second-stage action within the same planet.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [beta, kappa, xi, psi]
        - beta (inverse temperature, [0,10]): softmax for both stages.
        - kappa (count decay, [0,1]): exponential forgetting of Beta counts toward the prior.
        - xi (uncertainty bonus, [0,1]): scales directed exploration bonus from posterior std.
        - psi (perseveration, [0,1]): additive bias to repeat last second-stage action within
          the current planet.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    beta, kappa, xi, psi = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X, U->Y common (0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Beta posteriors for reward probabilities per planet-action
    a_post = np.ones((2, 2), dtype=float)  # successes
    b_post = np.ones((2, 2), dtype=float)  # failures

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a2_for_state = [None, None]

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Posterior means and uncertainty (std of the mean; Thompson/UCB proxy)
        mu = a_post / (a_post + b_post)
        std = np.sqrt(mu * (1.0 - mu) / (a_post + b_post + 1.0))

        # Directed exploration bonus at stage 2
        q2 = mu + xi * std

        # Stage-1 model-based planning using max over second-stage actions
        max_q2 = np.max(q2, axis=1)
        q1 = T @ max_q2

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration within state
        bias2 = np.zeros(2)
        if last_a2_for_state[s] is not None:
            bias2[last_a2_for_state[s]] += psi
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Decay counts toward the Beta(1,1) prior to track nonstationarity
        a_post = (1.0 - kappa) * a_post + kappa * 1.0
        b_post = (1.0 - kappa) * b_post + kappa * 1.0

        # Update posterior with observed outcome
        a_post[s, a2] += r
        b_post[s, a2] += (1.0 - r)

        # Update perseveration memory
        last_a2_for_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor-like model-based planning with transition-dependent stay bias and utility curvature.
    
    The model plans at stage 1 using a learned transition model and learned second-stage
    values. Rewards pass through a concave utility function to capture diminishing returns.
    A transition-dependent stay bias captures the classic model-based signature: participants
    tend to repeat the previous stage-1 action after rewarded common transitions and switch
    after rewarded rare transitions.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, eta, gamma, chi]
        - alpha (reward learning rate, [0,1]): updates second-stage Q-values.
        - beta (inverse temperature, [0,10]): softmax for both stages.
        - eta (transition learning rate, [0,1]): updates P(planet | spaceship).
        - gamma (utility curvature, [0,1]): subjective utility u(r) = r**gamma.
        - chi (transition-dependent stay bias, [0,1]): bias added to repeat last stage-1 action
          when the previous transition was common and rewarded; sign-flipped after rare.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, eta, gamma, chi = model_parameters
    n_trials = len(action_1)

    # Initialize transition model (rows sum to 1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage Q-values
    q2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Memory for transition-dependent bias
    last_a1 = None
    last_s = None
    last_r = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Subjective utility
        u = (r ** gamma)

        # Stage-1 model-based value from learned transitions and current q2
        max_q2 = np.max(q2, axis=1)
        q1 = T @ max_q2

        # Transition-dependent stay/switch bias for stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            # Common if A->X or U->Y (problem structure)
            was_common = ((last_a1 == 0 and last_s == 0) or (last_a1 == 1 and last_s == 1))
            sign = 1.0 if was_common else -1.0
            bias1[last_a1] += chi * sign * (last_r ** gamma)

        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Update transition model with observed planet given chosen spaceship
        oh = np.array([0.0, 0.0])
        oh[s] = 1.0
        T[a1] = (1.0 - eta) * T[a1] + eta * oh
        # Renormalize for numerical safety
        T[a1] = np.clip(T[a1], 1e-8, 1.0)
        T[a1] /= np.sum(T[a1])

        # Update second-stage values with subjective utility
        delta = u - q2[s, a2]
        q2[s, a2] += alpha * delta

        # Memory for next trial
        last_a1 = a1
        last_s = s
        last_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll