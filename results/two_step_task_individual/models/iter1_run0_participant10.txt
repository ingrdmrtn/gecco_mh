def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planner with learned transitions, stage-1 stickiness, and lapse.
    
    This model learns the transition probabilities from each spaceship to each planet,
    plans model-based values at stage 1 by multiplying learned transitions with the
    best available stage-2 values, and learns the stage-2 action values from reward.
    A first-stage stickiness bias favors repeating the previous spaceship. A lapse
    probability mixes the softmax policy with uniform random choice at both stages.
    
    Parameters (all used; bounds):
    - alpha_r: stage-2 reward learning rate in [0,1]
    - eta_tr: transition learning rate for T(s'|a1) in [0,1]
    - beta: inverse temperature (softmax) in [0,10]
    - stick1: first-stage stickiness strength in [0,1] added to last chosen spaceship's preference
    - lapse: lapse rate in [0,1]; with probability lapse, choices are uniform random
    
    Inputs:
    - action_1: array (n_trials,), 0/1 for spaceship A/U
    - state:    array (n_trials,), 0/1 for planet X/Y
    - action_2: array (n_trials,), 0/1 for alien within the visited planet
    - reward:   array (n_trials,), typically 0/1
    - model_parameters: iterable [alpha_r, eta_tr, beta, stick1, lapse]
    
    Returns:
    - Negative log-likelihood of the observed choices across both stages.
    """
    alpha_r, eta_tr, beta, stick1, lapse = model_parameters
    n_trials = len(action_1)

    # Learned transition model T[a1, s'], initialized uninformatively at 0.5
    T = np.ones((2, 2)) * 0.5

    # Stage-2 Q-values: Q2[state, action]
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage values: expected max over second stage under learned transitions
        v_state = np.max(Q2, axis=1)              # value of each planet
        q1_mb = T @ v_state                       # value of each spaceship via planning

        # Add stickiness on the previous first-stage action
        pref1 = q1_mb.copy()
        if last_a1 is not None:
            bias = np.zeros(2)
            bias[last_a1] = stick1
            pref1 += bias

        # Stage-1 policy with softmax and lapse
        pref1_centered = pref1 - np.max(pref1)
        exp1 = np.exp(beta * pref1_centered)
        soft1 = exp1 / (np.sum(exp1) + eps)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with softmax and lapse
        pref2 = Q2[s].copy()
        pref2_centered = pref2 - np.max(pref2)
        exp2 = np.exp(beta * pref2_centered)
        soft2 = exp2 / (np.sum(exp2) + eps)
        probs_2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p_choice_2[t] = probs_2[a2]

        # Learning: stage-2 Q-learning
        td2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * td2

        # Learn transitions from observed outcome: row-normalized update
        # Move probability mass toward observed state s and away from the other state
        T[a1, s] += eta_tr * (1.0 - T[a1, s])
        T[a1, 1 - s] += eta_tr * (0.0 - T[a1, 1 - s])

        # Ensure numerical stability of T rows (optional small renorm)
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, :] /= row_sum

        last_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free SARSA with value forgetting, rare-transition surprise bonus, and static side bias.
    
    This model is purely model-free: it learns Q-values for both stages with SARSA-style
    temporal-difference updates. Values (both stages) forget toward 0.5 over time.
    At the first stage, a surprise bonus boosts the preference for the chosen spaceship
    on trials in which a rare transition is observed. A side bias parameter captures a
    fixed preference for spaceship A vs U.
    
    Parameters (all used; bounds):
    - alpha: learning rate for both stages in [0,1]
    - beta: inverse temperature (softmax) in [0,10]
    - phi: forgetting rate toward 0.5 after each trial in [0,1]
    - omega: surprise bonus added to the chosen spaceship's preference when transition is rare, in [0,1]
    - bias: static side bias toward spaceship A over U in [0,1]; internally mapped to [-0.5, 0.5]
    
    Inputs:
    - action_1: array (n_trials,), 0/1 for spaceship A/U
    - state:    array (n_trials,), 0/1 for planet X/Y
    - action_2: array (n_trials,), 0/1 for alien within the visited planet
    - reward:   array (n_trials,), typically 0/1
    - model_parameters: iterable [alpha, beta, phi, omega, bias]
    
    Returns:
    - Negative log-likelihood of the observed choices across both stages.
    """
    alpha, beta, phi, omega, bias = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (for computing rarity only)
    # Common: A->X, U->Y
    def is_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    # Stage-1 and Stage-2 model-free values
    Q1 = np.zeros(2)          # for spaceships A/U
    Q2 = np.zeros((2, 2))     # for aliens within each planet

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Static side bias mapped to additive preferences for A vs U
    b = bias - 0.5  # in [-0.5, 0.5]
    side_bias = np.array([b, -b])

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Surprise bonus if observed transition is rare
        rare = 0 if is_common(a1, s) else 1
        surprise = np.zeros(2)
        surprise[a1] = omega * rare

        # Stage-1 softmax over Q1 with bias and surprise
        pref1 = Q1 + side_bias + surprise
        pref1_centered = pref1 - np.max(pref1)
        exp1 = np.exp(beta * pref1_centered)
        probs_1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax over Q2 for the visited state
        pref2 = Q2[s].copy()
        pref2_centered = pref2 - np.max(pref2)
        exp2 = np.exp(beta * pref2_centered)
        probs_2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Model-free TD learning
        td1 = Q2[s, a2] - Q1[a1]
        Q1[a1] += alpha * td1

        td2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * td2

        # Forgetting toward 0.5 baseline
        Q1 = (1.0 - phi) * Q1 + phi * 0.5
        Q2 = (1.0 - phi) * Q2 + phi * 0.5

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Adaptive arbitration between model-based and model-free control via uncertainty.
    
    This hybrid model mixes model-based (MB) and model-free (MF) values at stage 1, with
    a trial-by-trial weight determined by an uncertainty signal derived from unsigned
    reward prediction errors. When uncertainty is high, control shifts toward MF; when
    low, it shifts toward MB. MF values include an eligibility trace from stage 2 to 1.
    
    Parameters (all used; bounds):
    - alpha: learning rate for both stages in [0,1]
    - lam: eligibility trace in [0,1] scaling how much TD2 propagates to stage 1 MF
    - beta: inverse temperature (softmax) in [0,10]
    - w0: baseline MB weight lower bound in [0,1]; dynamic weight w_t ∈ [w0, 1]
    - gamma_u: uncertainty update rate in [0,1] for running unsigned TD2
    
    Inputs:
    - action_1: array (n_trials,), 0/1 for spaceship A/U
    - state:    array (n_trials,), 0/1 for planet X/Y
    - action_2: array (n_trials,), 0/1 for alien within the visited planet
    - reward:   array (n_trials,), typically 0/1
    - model_parameters: iterable [alpha, lam, beta, w0, gamma_u]
    
    Returns:
    - Negative log-likelihood of the observed choices across both stages.
    """
    alpha, lam, beta, w0, gamma_u = model_parameters
    n_trials = len(action_1)

    # Fixed transition model (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # rows: A,U; cols: X,Y

    # Model-free values
    Q1_mf = np.zeros(2)        # MF stage-1
    Q2 = np.zeros((2, 2))      # stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Uncertainty signal in [0,1], initialized high (uncertain)
    u = 1.0
    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based value via planning through T and current best stage-2 values
        v_state = np.max(Q2, axis=1)
        Q1_mb = T @ v_state

        # Adaptive arbitration weight: more MB when uncertainty is low
        w_t = w0 + (1.0 - w0) * (1.0 - u)   # u∈[0,1] -> w_t∈[w0,1]

        # Stage-1 policy from mixture of MB and MF
        pref1 = w_t * Q1_mb + (1.0 - w_t) * Q1_mf
        pref1_centered = pref1 - np.max(pref1)
        exp1 = np.exp(beta * pref1_centered)
        probs_1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy from MF values
        pref2 = Q2[s].copy()
        pref2_centered = pref2 - np.max(pref2)
        exp2 = np.exp(beta * pref2_centered)
        probs_2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Learning: MF updates with eligibility
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1

        td2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * td2

        # Eligibility trace: propagate TD2 to stage 1 MF
        Q1_mf[a1] += alpha * lam * td2

        # Update uncertainty from unsigned TD2
        u = (1.0 - gamma_u) * u + gamma_u * abs(td2)
        # Clip for safety
        u = min(max(u, 0.0), 1.0)

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss