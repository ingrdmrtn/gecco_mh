def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Perceived transition commonness, stage-2 decay, and stage-2 stickiness (pure model-based at stage 1).
    Mechanism:
    - Stage 2: standard Q-learning with learning rate and softmax.
    - Stage 1: purely model-based, using a perceived common-transition probability (not learned), combining transition
      structure with current stage-2 values.
    - Stage 2 decay: forgetting/volatility captured by a decay parameter applied each trial to all stage-2 Q-values.
    - Stage 2 stickiness: a bias to repeat the last chosen alien within each planet.

    Parameters (all used):
    - alpha2: stage-2 learning rate (bounds: [0,1])
    - beta: inverse temperature for softmax at both stages (bounds: [0,10])
    - p_common_raw: perceived common-transition strength; maps to p_common = 0.5 + 0.5*p_common_raw (bounds: [0,1])
    - decay2: per-trial multiplicative forgetting of stage-2 Q-values (0=no decay, 1=full decay to zero) (bounds: [0,1])
    - kappa2: stage-2 stickiness for repeating the previous action in the same state (bounds: [0,1])

    Inputs:
    - action_1: array-like of first-stage actions (0=A, 1=U)
    - state: array-like of second-stage states (0=X, 1=Y)
    - action_2: array-like of second-stage actions (0 or 1; e.g., W/S on X, P/H on Y)
    - reward: array-like of rewards (e.g., 0/1)
    - model_parameters: list/tuple [alpha2, beta, p_common_raw, decay2, kappa2]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha2, beta, p_common_raw, decay2, kappa2 = model_parameters
    n_trials = len(action_1)

    # Perceived (fixed) transition structure
    p_common = 0.5 + 0.5 * p_common_raw  # in [0.5,1.0]
    transition_matrix = np.array([[p_common, 1.0 - p_common],
                                  [1.0 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage2 = np.zeros((2, 2))  # Q2[s, a2]

    # Stage-2 stickiness memory: last action taken in each state
    last_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        # Apply decay to stage-2 values before decision
        q_stage2 = (1.0 - decay2) * q_stage2

        # Stage 1 policy (pure model-based): expected value of spaceships via current Q2
        max_q_stage2 = np.max(q_stage2, axis=1)  # V[s] for s in {X,Y}
        q_stage1_mb = transition_matrix @ max_q_stage2  # Q1_MB[a1]

        # Softmax for stage 1
        q1 = q_stage1_mb
        pref1 = beta * (q1 - np.max(q1))
        exp_q1 = np.exp(pref1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with stickiness on a per-state basis
        s = state[t]
        q2_s = q_stage2[s].copy()

        if last_a2[s] in (0, 1):
            stick = np.zeros(2)
            stick[last_a2[s]] = kappa2
            q2_s = q2_s + stick

        pref2 = beta * (q2_s - np.max(q2_s))
        exp_q2 = np.exp(pref2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]

        # Update stage-2 Q
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Update stickiness memory
        last_a2[s] = a2

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Pure model-free SARSA(lambda) with valence-asymmetric learning rates and separate stage temperatures.
    Mechanism:
    - Two-step episodic SARSA(lambda) with eligibility traces over both stages.
    - Separate inverse temperatures for stage 1 and stage 2.
    - Asymmetric learning rates for positive vs. non-positive outcomes at the terminal step.

    Parameters (all used):
    - alpha_pos: learning rate applied on trials with reward==1 at the terminal update (bounds: [0,1])
    - alpha_neg: learning rate applied on trials with reward==0 at the terminal update (bounds: [0,1])
    - beta1: inverse temperature for stage-1 softmax (bounds: [0,10])
    - beta2: inverse temperature for stage-2 softmax (bounds: [0,10])
    - lam: eligibility trace parameter lambda (bounds: [0,1])

    Inputs:
    - action_1: array-like first-stage actions (0 or 1)
    - state: array-like second-stage states (0 or 1)
    - action_2: array-like second-stage actions (0 or 1)
    - reward: array-like rewards (0/1)
    - model_parameters: list/tuple [alpha_pos, alpha_neg, beta1, beta2, lam]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta1, beta2, lam = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values for stage 1 (actions) and stage 2 (state-action)
    q_stage1 = np.zeros(2)         # Q1[a1]
    q_stage2 = np.zeros((2, 2))    # Q2[s, a2]

    # Eligibility traces
    e1 = np.zeros(2)
    e2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Softmax at stage 1
        pref1 = beta1 * (q_stage1 - np.max(q_stage1))
        exp_q1 = np.exp(pref1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Observe state s
        s = state[t]

        # Softmax at stage 2
        q2_s = q_stage2[s].copy()
        pref2 = beta2 * (q2_s - np.max(q2_s))
        exp_q2 = np.exp(pref2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update eligibilities (episodic two-step; gammaâ‰ˆ1)
        e1 *= lam
        e2 *= lam
        e1[a1] += 1.0
        e2[s, a2] += 1.0

        # Terminal TD error at stage 2
        delta2 = r - q_stage2[s, a2]
        alpha_term = alpha_pos if r > 0.0 else alpha_neg

        # Update Q2 with terminal TD error using eligibilities at stage 2
        q_stage2 += alpha_term * delta2 * e2

        # Bootstrapped TD error linking stages (from stage 1 to stage 2)
        # Use a single-step SARSA target: Q2[s, a2] as the next state's on-policy value
        delta1 = q_stage2[s, a2] - q_stage1[a1]

        # Use an intermediate learning rate for stage-1 bootstrapping (mean of valence-specific rates)
        alpha_boot = 0.5 * (alpha_pos + alpha_neg)
        q_stage1 += alpha_boot * delta1 * e1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Hybrid with learned transitions, successor-like planning weight, and value decay.
    Mechanism:
    - Learn transition probabilities T[a1, s] via a simple delta rule.
    - First-stage choice values are a convex combination of model-based (via learned T and current stage-2 values)
      and model-free first-stage values.
    - Stage-2 values learned via standard Q-learning.
    - Global decay (forgetting) applied to both stage-1 MF values and stage-2 values each trial.

    Parameters (all used):
    - alpha2: learning rate for stage-2 Q-values (bounds: [0,1])
    - beta: inverse temperature for softmax at both stages (bounds: [0,10])
    - w_mb: weight of model-based value in first-stage decision (0=MF only, 1=MB only) (bounds: [0,1])
    - alpha_tr: learning rate for updating the transition matrix T[a1, s] (bounds: [0,1])
    - decay: global per-trial multiplicative decay applied to Q1_MF and Q2 (bounds: [0,1])

    Inputs:
    - action_1: array-like first-stage actions (0 or 1)
    - state: array-like second-stage states (0 or 1)
    - action_2: array-like second-stage actions (0 or 1)
    - reward: array-like rewards (0/1)
    - model_parameters: list/tuple [alpha2, beta, w_mb, alpha_tr, decay]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha2, beta, w_mb, alpha_tr, decay = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize values and transition beliefs
    q_stage1_mf = np.zeros(2)         # MF Q1[a1]
    q_stage2 = np.zeros((2, 2))       # Q2[s, a2]
    T = np.full((2, 2), 0.5)          # Learned transition matrix; rows sum to 1 (start uniform)

    for t in range(n_trials):
        # Apply decay to values
        q_stage1_mf = (1.0 - decay) * q_stage1_mf
        q_stage2 = (1.0 - decay) * q_stage2

        # Model-based first-stage values using learned transitions and current Q2
        max_q_stage2 = np.max(q_stage2, axis=1)  # V[s]
        q_stage1_mb = T @ max_q_stage2

        # Hybrid combination
        q1_combined = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # Stage 1 policy
        pref1 = beta * (q1_combined - np.max(q1_combined))
        exp_q1 = np.exp(pref1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        q2_s = q_stage2[s].copy()
        pref2 = beta * (q2_s - np.max(q2_s))
        exp_q2 = np.exp(pref2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # Learn transitions: update T[a1] toward the observed state s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - alpha_tr) * T[a1] + alpha_tr * target
        # Ensure row normalization (numerical stability)
        row_sum = np.sum(T[a1])
        if row_sum > 0.0:
            T[a1] = T[a1] / row_sum

        # Update stage-2 Q
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Update stage-1 MF by bootstrapping from realized second-step value
        td1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha2 * td1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik