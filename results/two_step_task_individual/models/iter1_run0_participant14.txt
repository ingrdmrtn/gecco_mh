def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Asymmetric hybrid (MB+MF) with choice stickiness shared across stages.
    
    The agent maintains model-free (MF) values for both stages and uses a
    model-based (MB) planner at stage 1 by projecting second-stage values
    through a fixed transition model. Learning is asymmetric for positive vs.
    negative prediction errors. A single stickiness parameter adds a bias to
    repeat the last taken action at each stage (applied within the relevant
    choice set: stage 1 or the currently visited planet at stage 2).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states (planets): 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the visited planet: 0 or 1 (aliens on that planet).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha_pos, alpha_neg, beta, w, phi)
        - alpha_pos in [0,1]: learning rate for positive prediction errors.
        - alpha_neg in [0,1]: learning rate for negative prediction errors.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w in [0,1]: weight of model-based control at stage 1 (0 = pure MF, 1 = pure MB).
        - phi in [0,1]: stickiness bias added to the previously chosen action at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha_pos, alpha_neg, beta, w, phi = model_parameters
    n_trials = len(action_1)

    # Fixed, known transition structure: rows=first-stage actions (A/U), cols=states (X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Model-free values
    q1_mf = np.zeros(2)           # stage-1 MF values for A/U
    q2_mf = np.zeros((2, 2))      # stage-2 MF values per state and action

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness memory
    prev_a1 = -1
    prev_a2 = np.array([-1, -1])  # per state

    for t in range(n_trials):
        # Model-based value for stage 1: expected max second-stage value under transitions
        max_q2 = np.max(q2_mf, axis=1)   # per state (X,Y)
        q1_mb = T @ max_q2

        # Combine MB and MF at stage 1
        q1 = (1 - w) * q1_mf + w * q1_mb

        # Add stickiness to previously chosen first-stage action
        if prev_a1 in (0, 1):
            q1[prev_a1] += phi

        # Stage-1 policy
        q1_center = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_center)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (on visited state)
        s = state[t]
        q2 = q2_mf[s, :].copy()

        # Add stickiness to previously chosen second-stage action on this state
        if prev_a2[s] in (0, 1):
            q2[prev_a2[s]] += phi

        q2_center = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_center)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 learning with asymmetric alpha
        pe2 = r - q2_mf[s, a2]
        alpha2 = alpha_pos if pe2 >= 0 else alpha_neg
        q2_mf[s, a2] += alpha2 * pe2

        # Stage-1 model-free learning toward realized outcome (asymmetric)
        pe1 = r - q1_mf[a1]
        alpha1 = alpha_pos if pe1 >= 0 else alpha_neg
        q1_mf[a1] += alpha1 * pe1

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Surprise-modulated arbitration between model-based and model-free control.
    
    The agent learns model-free values but uses a model-based planner at stage 1
    with a dynamic arbitration weight that increases with transition surprise
    (rarer transitions induce more MB control). A first-stage stay-bias captures
    tendency to repeat the last chosen spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states (planets): 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the visited planet.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, omega0, gamma_surprise, stick1)
        - alpha in [0,1]: learning rate for model-free values at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega0 in [0,1]: baseline weight for MB control at stage 1.
        - gamma_surprise in [0,1]: gain on transition surprise to boost MB weight.
        - stick1 in [0,1]: stay-bias added to the previously chosen first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha, beta, omega0, gamma_surprise, stick1 = model_parameters
    n_trials = len(action_1)

    # Known transition structure (common 0.7, rare 0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Model-free values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1

    for t in range(n_trials):
        # Model-based estimate at stage 1 from current MF second-stage values
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2

        # Compute transition surprise for the observed transition on this trial
        a1 = action_1[t]
        s = state[t]
        surprise = 1.0 - T[a1, s]  # common -> 0.3, rare -> 0.7

        # Arbitration weight: increase MB control with surprise
        w = omega0 + gamma_surprise * surprise
        # Bound to [0,1]
        if w < 0.0:
            w = 0.0
        elif w > 1.0:
            w = 1.0

        # Combine MB and MF action values
        q1 = (1 - w) * q1_mf + w * q1_mb

        # Add first-stage stickiness
        if prev_a1 in (0, 1):
            q1[prev_a1] += stick1

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        a2 = action_2[t]
        q2 = q2_mf[s, :]
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # MF learning at stage 2
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # MF learning at stage 1 toward realized outcome
        pe1 = r - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Volatility-adaptive model-free control with lapse noise and asymmetric learning.
    
    The agent is purely model-free but adapts its exploration via an inverse
    temperature that decreases with estimated reward volatility (unsigned stage-2 PE).
    Learning is asymmetric for rewards vs. omissions. A lapse parameter mixes
    softmax with uniform choice noise at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states (planets): 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the visited planet.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : tuple/list of 5 floats
        (alpha_pos, alpha_neg, beta0, tau, lapse)
        - alpha_pos in [0,1]: learning rate when prediction error is positive.
        - alpha_neg in [0,1]: learning rate when prediction error is negative.
        - beta0 in [0,10]: baseline inverse temperature (upper bound on determinism).
        - tau in [0,1]: volatility update rate; higher values track volatility faster.
        - lapse in [0,1]: lapse probability; with probability 'lapse' choices are uniform.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha_pos, alpha_neg, beta0, tau, lapse = model_parameters
    n_trials = len(action_1)

    # Model-free values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Running estimate of volatility from unsigned stage-2 PE
    v = 0.0  # starts low volatility

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Volatility-adaptive inverse temperature (bounded in [0, beta0])
        beta_t = beta0 / (1.0 + v)

        # Stage-1 policy with lapse mixture
        q1c = q1 - np.max(q1)
        soft1 = np.exp(beta_t * q1c)
        soft1 = soft1 / np.sum(soft1)
        mix1 = (1.0 - lapse) * soft1 + lapse * 0.5  # uniform over 2 actions

        a1 = action_1[t]
        p_choice_1[t] = mix1[a1]

        # Stage-2 policy with lapse mixture
        s = state[t]
        q2c = q2[s, :] - np.max(q2[s, :])
        soft2 = np.exp(beta_t * q2c)
        soft2 = soft2 / np.sum(soft2)
        mix2 = (1.0 - lapse) * soft2 + lapse * 0.5

        a2 = action_2[t]
        p_choice_2[t] = mix2[a2]

        r = reward[t]

        # Stage-2 learning with asymmetric rates
        pe2 = r - q2[s, a2]
        a2_lr = alpha_pos if pe2 >= 0 else alpha_neg
        q2[s, a2] += a2_lr * pe2

        # Update volatility estimate from unsigned stage-2 PE
        v = (1.0 - tau) * v + tau * np.abs(pe2)

        # Stage-1 learning toward realized outcome (asymmetric)
        pe1 = r - q1[a1]
        a1_lr = alpha_pos if pe1 >= 0 else alpha_neg
        q1[a1] += a1_lr * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss