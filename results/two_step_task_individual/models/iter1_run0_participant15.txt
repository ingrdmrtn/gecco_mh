def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with learned transitions and eligibility trace.

    This model learns:
      - Second-stage Q-values via model-free TD learning.
      - First-stage model-free Q-values via an eligibility trace from the second-stage TD error.
      - First-stage model-based values using learned state-transition probabilities.
    Action selection at stage 1 uses a convex combination of MF and MB values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet (0/1 for the two aliens).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : list or array-like
        [alpha, beta, gamma, phi, xi]
        - alpha in [0,1]: learning rate for second-stage Q-values (and scales TD at stage 1 via xi).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - gamma in [0,1]: learning rate for transition probability estimates T(a -> s).
        - phi in [0,1]: weight of model-based value in the first-stage policy (0=MF only, 1=MB only).
        - xi in [0,1]: eligibility trace strength propagating the stage-2 TD error to stage-1 MF Q.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, gamma, phi, xi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition probabilities T[a, s] (rows sum to 1)
    T = np.ones((2, 2)) * 0.5

    # Q-values
    q1_mf = np.zeros(2)       # model-free first-stage
    q2 = np.zeros((2, 2))     # second-stage

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute model-based first-stage value from learned transitions
        max_q2 = np.max(q2, axis=1)             # value of each planet
        q1_mb = T @ max_q2                      # expected value by each spaceship
        q1_hybrid = phi * q1_mb + (1 - phi) * q1_mf

        # Stage 1 policy
        logits1 = beta * (q1_hybrid - np.max(q1_hybrid))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy for reached state
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: second stage TD
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Eligibility trace to first-stage MF values
        q1_mf[a1] += xi * alpha * delta2

        # Update learned transition probabilities for chosen first-stage action
        # Move row T[a1] toward the observed state-onehot
        onehot = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = (1 - gamma) * T[a1] + gamma * onehot
        # Ensure numerical normalization
        T[a1] = T[a1] / np.sum(T[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with asymmetric learning rates, lapse, and eligibility propagation.

    This purely model-free agent:
      - Uses separate learning rates for positive vs. negative second-stage TD errors.
      - Propagates the second-stage TD error to first-stage Q-values via an eligibility factor.
      - Includes a lapse parameter that mixes the softmax policy with uniform random choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet (0/1 for the two aliens).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : list or array-like
        [alpha_pos, alpha_neg, beta, epsilon, chi]
        - alpha_pos in [0,1]: learning rate when TD error > 0.
        - alpha_neg in [0,1]: learning rate when TD error <= 0.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - epsilon in [0,1]: lapse rate; with probability epsilon choose uniformly at random.
        - chi in [0,1]: eligibility factor scaling how strongly stage-2 TD updates stage-1 Q.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, epsilon, chi = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage 1 policy with lapse
        logits1 = beta * (q1 - np.max(q1))
        soft1 = np.exp(logits1)
        soft1 = soft1 / np.sum(soft1)
        probs1 = (1 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with lapse
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        soft2 = np.exp(logits2)
        soft2 = soft2 / np.sum(soft2)
        probs2 = (1 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second stage asymmetric TD learning
        delta2 = r - q2[s, a2]
        alpha_t = alpha_pos if delta2 > 0 else alpha_neg
        q2[s, a2] += alpha_t * delta2

        # Propagate to first-stage Q via eligibility factor
        q1[a1] += chi * alpha_t * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Hybrid MF/MB with perceptual state confusion and stage-2 perseveration.

    The agent:
      - Maintains model-free Q-values at both stages.
      - Uses a fixed transition structure (common=0.7) to compute model-based first-stage values.
      - Blends MF and MB first-stage values via a weight.
      - Suffers from perceptual confusion of the reached planet: with probability z, it
        treats the opposite planet as if it were reached, which affects both policy and learning
        via a convex blend over states.
      - Exhibits a second-stage choice kernel (perseveration) that decays proportionally to (1 - alpha).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet (0/1 for the two aliens).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    model_parameters : list or array-like
        [alpha, beta, omega, z, kappa2]
        - alpha in [0,1]: learning rate for Q-values; also governs kernel decay (decay = 1 - alpha).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega in [0,1]: weight of model-based value in first-stage policy (0=MF only, 1=MB only).
        - z in [0,1]: probability of perceptual confusion between planets; implements a blend over states.
        - kappa2 in [0,1]: strength of the second-stage perseveration kernel added to logits.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, omega, z, kappa2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure for MB planning
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Second-stage choice kernel
    K2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage MB value from fixed transitions and current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2
        q1_composite = omega * q1_mb + (1 - omega) * q1_mf

        # Stage 1 policy
        logits1 = beta * (q1_composite - np.max(q1_composite))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with perceptual confusion and perseveration
        s = state[t]
        s_alt = 1 - s

        # Blend Qs and kernels according to confusion probability z
        q2_blend = (1 - z) * q2[s] + z * q2[s_alt]
        K2_blend = (1 - z) * K2[s] + z * K2[s_alt]
        logits2 = beta * (q2_blend + kappa2 * K2_blend - np.max(q2_blend + kappa2 * K2_blend))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning at stage 2 with blended credit assignment
        # Compute TD errors for both actual and alternate states
        delta2_s = r - q2[s, a2]
        delta2_alt = r - q2[s_alt, a2]
        q2[s, a2] += alpha * (1 - z) * delta2_s
        q2[s_alt, a2] += alpha * z * delta2_alt

        # Update first-stage MF value towards the (confused) second-stage action value
        v_blend = (1 - z) * q2[s, a2] + z * q2[s_alt, a2]
        delta1 = v_blend - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update second-stage choice kernel with decay tied to alpha
        K2 *= (1 - alpha)
        K2[s, a2] += alpha

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll