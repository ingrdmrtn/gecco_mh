def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with dual learning rates and unified perseveration.

    This model mixes model-based (MB) and model-free (MF) values at stage 1,
    with separate learning rates for stage-2 and stage-1 MF updates. A single
    perseveration parameter biases repeating the previous action at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship at stage 1 (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed planet reached after the stage-1 transition (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Chosen alien at stage 2 on the reached planet (0 or 1) for each trial.
    reward : array-like of float (0 or 1)
        Reward outcome for each trial.
    model_parameters : tuple/list of floats
        (alpha2, alpha1, beta, w, kappa)
        - alpha2 in [0,1]: Learning rate for stage-2 MF value updates.
        - alpha1 in [0,1]: Learning rate for stage-1 MF value updates.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - w in [0,1]: Weight of MB value in stage-1 decision (1=fully MB).
        - kappa in [0,1]: Perseveration strength; added to the previously
          chosen actionâ€™s logit at each stage (choice stickiness).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha2, alpha1, beta, w, kappa = model_parameters
    n_trials = len(action_1)

    # Known transition structure: rows are actions (A,U), cols are states (X,Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q_stage1_mf = np.zeros(2)          # for spaceship A/U
    q_stage2_mf = np.zeros((2, 2))     # for each planet (X/Y) and two aliens

    # Perseveration trackers
    prev_a1 = -1
    prev_a2_by_state = np.array([-1, -1])

    for t in range(n_trials):
        # Model-based value for stage 1: expected max over stage-2 values via transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [q*(X), q*(Y)]
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Perseveration biases
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa

        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] != -1:
            bias2[prev_a2_by_state[s]] += kappa

        # Stage-1 policy: mix MF and MB, add bias
        q1 = (1.0 - w) * q_stage1_mf + w * q_stage1_mb + bias1
        q1 = q1 - np.max(q1)  # stabilization
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy: MF plus bias
        q2 = q_stage2_mf[s, :] + bias2
        q2 = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha2 * delta2

        # Stage-1 MF update driven by updated stage-2 value (semi-gradient TD)
        target1 = q_stage2_mf[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha1 * delta1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based with learned transitions, risk-sensitive utility, and lapse.

    Stage-1 decisions are purely model-based using a learned transition model.
    Stage-2 values are learned model-free. Rewards are transformed by a
    risk/utility parameter that shrinks outcomes toward a reference (0.5),
    and a lapse parameter mixes softmax with uniform choice at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship at stage 1 (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed planet reached after the stage-1 transition (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Chosen alien at stage 2 on the reached planet (0 or 1) for each trial.
    reward : array-like of float (0 or 1)
        Reward outcome for each trial.
    model_parameters : tuple/list of floats
        (alpha2, beta, rho, epsilon, alphaT)
        - alpha2 in [0,1]: Learning rate for stage-2 MF values.
        - beta in [0,10]: Inverse temperature for softmax.
        - rho in [0,1]: Utility shrinkage toward 0.5; u = 0.5 + rho*(r - 0.5).
                         rho=1 keeps raw reward; rho=0 maps all outcomes to 0.5.
        - epsilon in [0,1]: Lapse; probability of choosing uniformly at random.
        - alphaT in [0,1]: Learning rate for transition probabilities
                           P(state | action_1); row-wise update toward observed state.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha2, beta, rho, epsilon, alphaT = model_parameters
    n_trials = len(action_1)

    # Initialize transition model close to uninformative (or prior): start unbiased
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2 = np.zeros((2, 2))  # MF values for aliens on each planet

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Utility-transformed reward
        r = reward[t]
        u = 0.5 + rho * (r - 0.5)

        # Stage-1 MB values: expected max over stage-2 values using learned T
        max_q2 = np.max(q_stage2, axis=1)  # per planet
        q1_mb = T @ max_q2

        # Softmax with lapse
        logits1 = beta * (q1_mb - np.max(q1_mb))
        soft1 = np.exp(logits1)
        soft1 = soft1 / np.sum(soft1)
        probs_1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy from MF values with lapse
        logits2 = beta * (q_stage2[s, :] - np.max(q_stage2[s, :]))
        soft2 = np.exp(logits2)
        soft2 = soft2 / np.sum(soft2)
        probs_2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        p_choice_2[t] = probs_2[a2]

        # Stage-2 MF update with utility
        delta2 = u - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Transition learning: update row of chosen action toward observed state s
        # New row = (1 - alphaT) * old_row; then add alphaT to the observed state entry
        # Ensures row sums to 1 and stays within [0,1]
        T[a1, :] = (1.0 - alphaT) * T[a1, :]
        T[a1, s] += alphaT

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with novelty-seeking exploration bonus and decaying visit traces.

    Stage-1 values are a mixture of MF and MB. Both stages include an
    intrinsic novelty/exploration bonus that decreases with prior visits,
    controlled by a decay parameter. Visit traces decay each trial so that
    unvisited options regain novelty over time.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship at stage 1 (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed planet reached after the stage-1 transition (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Chosen alien at stage 2 on the reached planet (0 or 1) for each trial.
    reward : array-like of float (0 or 1)
        Reward outcome for each trial.
    model_parameters : tuple/list of floats
        (alpha, beta, w, zeta, gamma)
        - alpha in [0,1]: Learning rate for MF value updates (both stages).
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - w in [0,1]: Weight on MB value in stage-1 mixture.
        - zeta in [0,1]: Scale of novelty bonus added to action values.
        - gamma in [0,1]: Visit-trace decay rate; higher gamma slows decay
                          (novelty persists longer).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w, zeta, gamma = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (commonly A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q1_mf = np.zeros(2)        # stage-1 MF for A/U
    q2_mf = np.zeros((2, 2))   # stage-2 MF for aliens on planets X/Y

    # Visit traces for novelty bonuses
    v1 = np.zeros(2)           # visits for A/U
    v2 = np.zeros((2, 2))      # visits for aliens on each planet

    for t in range(n_trials):
        # Decay visit traces
        v1 *= gamma
        v2 *= gamma

        # Compute MB value at stage 1 from MF stage-2 values
        max_q2 = np.max(q2_mf, axis=1)  # [X, Y]
        q1_mb = T @ max_q2

        # Novelty bonuses: inverse with visits; add a small constant for stability
        b1 = zeta / (1.0 + v1)
        s = state[t]
        b2_state = zeta / (1.0 + v2[s, :])

        # Stage-1 mixture with novelty bonus
        q1 = (1.0 - w) * q1_mf + w * q1_mb + b1
        q1 = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with novelty bonus
        q2 = q2_mf[s, :] + b2_state
        q2 = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # MF updates
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF target: bootstrap from post-update stage-2 value
        target1 = q2_mf[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update visit traces for taken actions (after decay)
        v1[a1] += (1.0 - gamma)
        v2[s, a2] += (1.0 - gamma)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll