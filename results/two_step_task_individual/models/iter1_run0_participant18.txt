def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with directed exploration bonus and transition-based stickiness.

    This model combines model-based (MB) and model-free (MF) values at the first stage,
    uses MF learning at the second stage, adds a count-based directed exploration bonus 
    at the second stage (propagated to stage 1 through MB planning), and includes a 
    transition-based stickiness that biases the first-stage choice toward the spaceship
    likely to revisit the most recently rewarded planet.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choice per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state (planet) reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choice per trial: 0/1 = the two aliens available on that planet.
    reward : array-like of float in [0,1]
        Outcome on each trial (number of coins; typically 0 or 1).
    model_parameters : sequence
        [alpha, beta, w_mb, phi_explore, zeta_trans]
        - alpha (learning rate, [0,1]): MF learning rate for value updates.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for choices at both stages.
        - w_mb (MB weight, [0,1]): Mixing weight for MB at stage 1 (1 = pure MB, 0 = pure MF).
        - phi_explore (exploration bonus, [0,1]): Scales directed exploration bonus (1/sqrt(n)) at stage 2.
        - zeta_trans (transition stickiness, [0,1]): Bias toward the spaceship most likely to revisit
          the most recently rewarded planet; scaled by last reward.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha, beta, w_mb, phi_explore, zeta_trans = model_parameters
    n_trials = len(action_1)

    # Transition: rows = spaceship (A,U), cols = planet (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q1_mf = np.zeros(2)         # stage-1 values for A/U
    q2 = np.zeros((2, 2))       # stage-2 values for each planet's two aliens

    # Count-based exploration: visit counts per (state, action2)
    n_sa2 = np.zeros((2, 2))    # counts for uncertainty bonus

    # For transition-based stickiness after rewarded trial
    last_reward = 0.0
    last_state = 0

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Directed exploration bonus at stage 2: bonus = phi / sqrt(n+1)
        bonus2 = phi_explore / np.sqrt(n_sa2 + 1.0)
        # Effective Q at stage 2 includes exploration bonus
        q2_eff = q2 + bonus2

        # Model-based value at stage 1: expectation over transitions of the best q2_eff
        max_q2_eff = np.max(q2_eff, axis=1)               # best per planet
        q1_mb = transition_matrix @ max_q2_eff            # expected value for A/U

        # Transition-based stickiness: bias toward spaceship likely to revisit last rewarded planet
        trans_bias = np.zeros(2)
        if t > 0:
            # Probability each spaceship goes to the last visited state
            p_to_last = transition_matrix[:, last_state]  # shape (2,)
            # Centered around 0.5 so it can be positive/negative
            trans_bias = zeta_trans * last_reward * (p_to_last - 0.5)

        # Mixture policy at stage 1
        q1_mix = w_mb * q1_mb + (1 - w_mb) * q1_mf + trans_bias

        # Stage-1 choice probability
        exp_q1 = np.exp(beta * (q1_mix - np.max(q1_mix)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probability (use q2 + bonus at the reached state)
        q2_s_eff = q2_eff[s]
        exp_q2 = np.exp(beta * (q2_s_eff - np.max(q2_s_eff)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage 2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update counts AFTER using the bonus
        n_sa2[s, a2] += 1.0

        # Stage 1 MF bootstrap update from current q2 (without bonus)
        delta1_boot = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1_boot

        # Track last outcome and state for transition stickiness
        last_reward = r
        last_state = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """MF with asymmetric learning rates, diminishing reward sensitivity, and first-stage perseveration.

    This is a largely model-free learner with separate learning rates for positive vs.
    negative prediction errors at both stages. Rewards are transformed by a concavity
    parameter (gamma) to capture diminishing sensitivity. A first-stage perseveration term
    biases repeating the previous spaceship choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Second-stage state: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: planet-specific aliens.
    reward : array-like of float in [0,1]
        Outcome per trial.
    model_parameters : sequence
        [alpha_pos, alpha_neg, beta, kappa1, gamma]
        - alpha_pos (learning rate for positive PE, [0,1])
        - alpha_neg (learning rate for negative PE, [0,1])
        - beta (inverse temperature, [0,10]): For both stages.
        - kappa1 (stage-1 perseveration, [0,1]): Bias to repeat last stage-1 action.
        - gamma (utility concavity, [0,1]): Transforms reward as r_eff = r**gamma.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, kappa1, gamma = model_parameters
    n_trials = len(action_1)

    # Transition matrix used only for MB alternatives (not used here; this is MF)
    # but we keep the structure consistent; no MB planning in this model.

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)       # MF at stage 1
    q2 = np.zeros((2, 2))     # MF at stage 2

    # Perseveration traces for stage 1 only
    pers1 = np.zeros(2)

    def lr(pe):
        return alpha_pos if pe >= 0.0 else alpha_neg

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        # Diminishing sensitivity transform
        r_eff = r ** gamma

        # Stage-1 policy with perseveration
        q1_eff = q1_mf + kappa1 * pers1
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure MF)
        q2_eff = q2[s]
        exp_q2 = np.exp(beta * (q2_eff - np.max(q2_eff)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning: Stage 2
        pe2 = r_eff - q2[s, a2]
        q2[s, a2] += lr(pe2) * pe2

        # Learning: Stage 1 bootstrapping from updated q2 value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += lr(pe1) * pe1

        # Update perseveration trace
        pers1[:] = 0.0
        pers1[a1] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Arbitrated MB/MF control via reliability-based weight with eligibility trace.

    This model learns both MF values and MB transition-based expectations. The 
    arbitration weight w_t between MB and MF control at stage 1 is computed online 
    from running estimates of MB and MF unreliability:
      - MF unreliability: exponentially smoothed absolute stage-2 prediction error.
      - MB unreliability: exponentially smoothed transition surprise (1 - P(s | a1)).
    The effective MB weight is w_t = sigmoid( scale * (rel_MB - rel_MF + bias) ),
    where rel = 1 - unreliability, and bias centers the prior preference.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Second-stage state: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on reached planet.
    reward : array-like of float in [0,1]
        Outcome per trial.
    model_parameters : sequence
        [alpha, beta, lam, tau_a, bias_mb]
        - alpha (learning rate, [0,1]): MF learning rate at both stages.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - lam (eligibility trace, [0,1]): Passes stage-2 PE back to stage-1 MF.
        - tau_a (arbitration forgetting, [0,1]): Exponential averaging rate for unreliability.
        - bias_mb (MB bias, [0,1]): Prior preference toward MB control (centered at 0.5).

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha, beta, lam, tau_a, bias_mb = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)     # MF at stage 1
    q2 = np.zeros((2, 2))   # MF at stage 2

    # Running unreliability estimates (initialized moderately)
    mf_unrel = 0.5
    mb_unrel = 0.5

    scale = 5.0  # fixed gain to allow sharp arbitration with parameters in [0,1]

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # MB values: expected value of best second-stage option per planet
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Arbitration weight based on current reliabilities
        rel_mf = 1.0 - mf_unrel
        rel_mb = 1.0 - mb_unrel
        # Center bias around 0 with (bias_mb - 0.5)
        z = scale * (rel_mb - rel_mf + (bias_mb - 0.5))
        w_t = 1.0 / (1.0 + np.exp(-z))

        # Stage-1 mixture policy
        q1_eff = w_t * q1_mb + (1.0 - w_t) * q1_mf
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        q2_eff = q2[s]
        exp_q2 = np.exp(beta * (q2_eff - np.max(q2_eff)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage 2 MF update and MF unreliability
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2
        mf_unrel = (1 - tau_a) * mf_unrel + tau_a * abs(pe2)

        # Stage 1 MF update with eligibility
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1 + (lam * alpha) * pe2

        # MB unreliability via transition surprise (1 - P(s | a1))
        p_s_given_a1 = transition_matrix[a1, s]
        trans_surprise = 1.0 - p_s_given_a1  # rare transitions produce larger surprise
        mb_unrel = (1 - tau_a) * mb_unrel + tau_a * trans_surprise

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll