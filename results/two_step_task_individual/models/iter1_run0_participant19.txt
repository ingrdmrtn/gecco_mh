def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model with learned transitions and global lapse.

    The agent:
    - Learns second-stage (planet/alien) MF Q-values from reward.
    - Learns the first-stage transition model from experience (instead of assuming 0.7/0.3).
    - Chooses at stage 1 using a mixture of model-based (via the learned transitions) and
      model-free first-stage values.
    - Applies a lapse (stimulus-independent) choice noise at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage actions per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0: planet X, 1: planet Y) actually reached.
    action_2 : array-like of int (0 or 1)
        Second-stage actions per trial (0/1 corresponding to the two aliens on that planet).
    reward : array-like of float
        Reward outcome per trial (e.g., -1/0/1).
    model_parameters : iterable of float
        [alpha, beta, phi, zeta, xi]
        - alpha in [0, 1]: learning rate for model-free Q-value updates.
        - beta  in [0, 10]: inverse temperature for softmax choice at both stages.
        - phi   in [0, 1]: learning rate for updating the first-stage transition model.
        - zeta  in [0, 1]: weight on model-based value at stage 1 (1 - zeta on model-free).
        - xi    in [0, 1]: lapse rate; probability of choosing uniformly at random at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, phi, zeta, xi = model_parameters
    n_trials = len(action_1)

    # Initialize with the nominal common/rare structure but allow learning
    transition_est = np.array([[0.7, 0.3],  # P(X,Y | A)
                               [0.3, 0.7]]) # P(X,Y | U)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)        # MF Q at stage 1 (A/U)
    q_stage2_mf = np.zeros((2, 2))   # MF Q at stage 2 (X/Y x two aliens)

    eps = 1e-12

    for t in range(n_trials):
        # Stage-1 model-based values given current transition estimates
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # V2(s) = max_a Q2(s,a)
        q_stage1_mb = transition_est @ max_q_stage2

        # Hybrid valuation at stage 1
        q1 = zeta * q_stage1_mb + (1.0 - zeta) * q_stage1_mf

        # Stage-1 policy with softmax and lapse
        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_shift)
        soft1 = exp_q1 / (np.sum(exp_q1) + eps)
        probs_1 = (1.0 - xi) * soft1 + xi * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with softmax and lapse
        s = state[t]
        q2 = q_stage2_mf[s]
        q2_shift = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_shift)
        soft2 = exp_q2 / (np.sum(exp_q2) + eps)
        probs_2 = (1.0 - xi) * soft2 + xi * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update MF values
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * pe2

        # Stage-1 MF update (bootstrapping on current second-stage action value)
        target1 = q_stage2_mf[s, a2]
        pe1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * pe1

        # Update transition model for the chosen first-stage action using the observed state
        # Move the distribution for the chosen action toward the observed next state (one-hot)
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        transition_est[a1] = transition_est[a1] + phi * (one_hot_s - transition_est[a1])
        # Numerical safety: renormalize the row to sum to 1
        row_sum = np.sum(transition_est[a1])
        if row_sum > 0:
            transition_est[a1] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with uncertainty bonus (UCB) at stage 2 and value forgetting.

    The agent:
    - Uses purely model-free control at stage 1 (no model-based component).
    - At stage 2, augments MF Q-values with an uncertainty bonus to encourage exploration.
    - Applies reward sensitivity to scale outcomes before learning.
    - Applies value forgetting toward a neutral baseline at both stages each trial.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage actions per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions per trial (0/1 for the two aliens on that planet).
    reward : array-like of float
        Reward outcome per trial (e.g., -1/0/1).
    model_parameters : iterable of float
        [alpha, beta, rho, c, delta]
        - alpha in [0, 1]: learning rate for MF value updates.
        - beta  in [0, 10]: inverse temperature for softmax choice.
        - rho   in [0, 1]: reward sensitivity; scales experienced reward before updating.
        - c     in [0, 1]: uncertainty-bonus weight at stage 2 (larger favors less-visited options).
        - delta in [0, 1]: value forgetting rate toward 0 at both stages each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, rho, c, delta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)         # MF Q at stage 1 (A/U)
    q_stage2 = np.zeros((2, 2))    # MF Q at stage 2 (X/Y x two aliens)
    visit_counts = np.zeros((2, 2))  # counts for UCB at stage 2

    eps = 1e-12

    for t in range(n_trials):
        # Apply forgetting toward 0 before acting (decay past values)
        q_stage1 *= (1.0 - delta)
        q_stage2 *= (1.0 - delta)

        # Stage-1 policy: purely MF
        q1 = q_stage1
        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_shift)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy: MF + UCB bonus based on inverse sqrt of visits
        s = state[t]
        n_sa = visit_counts[s] + 1.0  # add 1 to avoid division by zero
        bonus = c / np.sqrt(n_sa)
        q2_eff = q_stage2[s] + bonus
        q2_shift = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2_shift)
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward with sensitivity scaling
        r = reward[t]
        u = rho * r  # down/up-weight outcome magnitude

        # Learning updates
        # Stage-2 MF update
        pe2 = u - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        # Stage-1 MF update (bootstrap on current second-stage chosen action value)
        target1 = q_stage2[s, a2]
        pe1 = target1 - q_stage1[a1]
        q_stage1[a1] += alpha * pe1

        # Update visit counts for UCB
        visit_counts[s, a2] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor-style model-based valuation at stage 1 with transition learning and asymmetric ship bias.

    The agent:
    - Learns second-stage MF Q-values from reward.
    - Learns first-stage transition probabilities from experience.
    - Computes stage-1 values from a successor-style combination of state values:
        V1(a) = sum_s P_hat(s|a) * V2(s), where V2(s) mixes max and mean second-stage values.
    - Includes a static asymmetry (bias) preferring one spaceship over the other.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage actions per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions per trial (0/1 for the two aliens on that planet).
    reward : array-like of float
        Reward outcome per trial (e.g., -1/0/1).
    model_parameters : iterable of float
        [alpha, beta, phi, gamma_sr, b]
        - alpha    in [0, 1]: learning rate for MF value updates at stage 2.
        - beta     in [0, 10]: inverse temperature for softmax choices at both stages.
        - phi      in [0, 1]: learning rate for updating the first-stage transition model.
        - gamma_sr in [0, 1]: weight on max vs mean in state values V2(s);
                              V2(s) = gamma_sr*max_a Q2(s,a) + (1-gamma_sr)*mean_a Q2(s,a).
        - b        in [0, 1]: asymmetric bias toward spaceship A vs U; translated to
                              (+bias, -bias) added to (A, U) values with bias = (b - 0.5).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, phi, gamma_sr, b = model_parameters
    n_trials = len(action_1)

    # Initialize transition estimates with the nominal structure
    transition_est = np.array([[0.7, 0.3],  # P(X,Y | A)
                               [0.3, 0.7]]) # P(X,Y | U)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2 = np.zeros((2, 2))  # MF Q at stage 2 (X/Y x two aliens)

    # Bias vector added to stage-1 action values
    bias_strength = b - 0.5
    bias_vec = np.array([bias_strength, -bias_strength])

    eps = 1e-12

    for t in range(n_trials):
        # Compute state values mixing max and mean
        max_q = np.max(q_stage2, axis=1)
        mean_q = np.mean(q_stage2, axis=1)
        v2 = gamma_sr * max_q + (1.0 - gamma_sr) * mean_q

        # Stage-1 model-based valuation via learned transitions (successor-like)
        q1_mb = transition_est @ v2
        q1 = q1_mb + bias_vec

        # Stage-1 policy
        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_shift)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (standard softmax on MF Q)
        s = state[t]
        q2 = q_stage2[s]
        q2_shift = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_shift)
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and updates
        r = reward[t]

        # Stage-2 MF learning
        pe2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        # Update transition model for chosen first-stage action using observed state
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        transition_est[a1] = transition_est[a1] + phi * (one_hot_s - transition_est[a1])
        # Renormalize for safety
        row_sum = np.sum(transition_est[a1])
        if row_sum > 0:
            transition_est[a1] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll