def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model-based with novelty-seeking, separate temperatures, and action stickiness.
    
    This model plans at stage 1 using a fixed transition model (common = 0.7, rare = 0.3)
    and learned second-stage values. It adds a novelty-seeking bonus that tracks the 
    recent "surprise" of each first-stage action (how often it led to rare transitions).
    Both stages also include an action stickiness bias that favors repeating the previous
    action taken at that stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on that planet).
    reward : array-like of float (typically 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta1, beta2, stick, kappa]
        - alpha in [0,1]: learning rate for second-stage values and novelty traces.
        - beta1 in [0,10]: inverse temperature for stage-1 softmax.
        - beta2 in [0,10]: inverse temperature for stage-2 softmax.
        - stick in [0,1]: stickiness strength added to the last chosen action at each stage.
        - kappa in [0,1]: weight of novelty bonus added to stage-1 action values.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, stick, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition model: rows = actions (A,U), cols = states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize values and novelty traces
    q_stage2 = np.zeros((2, 2)) + 0.5  # per state, two aliens
    novelty_trace = np.zeros(2)        # per first-stage action
    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    eps = 1e-12

    for t in range(n_trials):
        # Planning: expected value of each first-stage action under max second-stage value
        max_q2 = np.max(q_stage2, axis=1)              # value per state
        q1_mb = transition_matrix @ max_q2             # expected value per first-stage action

        # Add novelty bonus: larger when the action recently led to rare transitions
        q1_aug = q1_mb + kappa * novelty_trace

        # Add stickiness bias at stage 1
        stick_bias_1 = np.zeros(2)
        if last_a1 is not None:
            stick_bias_1[last_a1] += stick

        q1_soft = q1_aug + stick_bias_1

        # Stage-1 choice probability
        q1_soft_centered = q1_soft - np.max(q1_soft)
        probs_1 = np.exp(beta1 * q1_soft_centered)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 choice probability in observed state
        s = state[t]
        q2 = q_stage2[s].copy()

        # Add stickiness bias at stage 2 (state-specific)
        stick_bias_2 = np.zeros(2)
        if last_a2_by_state[s] is not None:
            stick_bias_2[last_a2_by_state[s]] += stick

        q2_soft = q2 + stick_bias_2
        q2_soft_centered = q2_soft - np.max(q2_soft)
        probs_2 = np.exp(beta2 * q2_soft_centered)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Update second-stage values
        pe2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        # Update novelty trace: surprise = 1 - P(observed state | chosen action)
        surprise = 1.0 - transition_matrix[a1, s]
        novelty_trace *= (1.0 - alpha)      # decay all actions
        novelty_trace[a1] += alpha * surprise

        # Update last actions for stickiness
        last_a1 = a1
        last_a2_by_state[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Purely model-free SARSA with asymmetric learning and transition-modulated lapses.
    
    This model learns both stages via model-free TD with separate learning rates for positive
    vs negative prediction errors. Choice policies at both stages include a lapse component
    that increases on rare transitions (capturing disorientation or attentional slips).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on that planet).
    reward : array-like of float (typically 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha_pos, alpha_neg, beta, lapse0, rare_boost]
        - alpha_pos in [0,1]: learning rate when PE >= 0.
        - alpha_neg in [0,1]: learning rate when PE < 0.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - lapse0 in [0,1]: baseline lapse probability mixed with uniform choice.
        - rare_boost in [0,1]: additional lapse added on trials with a rare transition.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, lapse0, rare_boost = model_parameters
    n_trials = len(action_1)

    # Transition structure to determine rarity (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2) + 0.5
    q2_mf = np.zeros((2, 2)) + 0.5

    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Determine whether the observed transition was rare for chosen a1
        is_rare = 1.0 if transition_matrix[a1, s] < 0.5 else 0.0
        lapse_t = min(1.0, lapse0 + rare_boost * is_rare)

        # Stage-1 choice policy with lapse
        q1 = q1_mf
        q1c = q1 - np.max(q1)
        probs1_soft = np.exp(beta * q1c)
        probs1_soft = probs1_soft / (np.sum(probs1_soft) + eps)
        probs_1 = (1.0 - lapse_t) * probs1_soft + lapse_t * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage-2 choice policy with lapse (state-specific)
        q2 = q2_mf[s]
        q2c = q2 - np.max(q2)
        probs2_soft = np.exp(beta * q2c)
        probs2_soft = probs2_soft / (np.sum(probs2_soft) + eps)
        probs_2 = (1.0 - lapse_t) * probs2_soft + lapse_t * 0.5
        p_choice_2[t] = probs_2[a2]

        # Stage-2 update (asymmetric learning)
        pe2 = r - q2_mf[s, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2_mf[s, a2] += alpha2 * pe2

        # Stage-1 update (bootstrapping from the second-stage action value)
        td_target = q2_mf[s, a2]
        pe1 = td_target - q1_mf[a1]
        alpha1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        q1_mf[a1] += alpha1 * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-weighted arbitration between model-based and model-free with decay and repetition bias.
    
    The model learns:
      - Model-free values at both stages with exponential decay (forgetting).
      - Model-based values using estimated second-stage outcome means and a learned uncertainty
        (via Beta-Bernoulli counts per state-action). Arbitration at stage 1 is action-specific:
        when the next-state value is uncertain for an action, the model shifts weight toward
        model-free values for that action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on that planet).
    reward : array-like of float (typically 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [beta, w0, kappa_u, phi_rep, decay]
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w0 in [0,1]: baseline weight on model-based values at stage 1 (per action, before uncertainty).
        - kappa_u in [0,1]: sensitivity of arbitration to uncertainty (higher reduces MB weight when uncertain).
        - phi_rep in [0,1]: repetition bias added to the previously chosen action at each stage.
        - decay in [0,1]: per-trial forgetting toward 0.5 for model-free values at both stages.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    beta, w0, kappa_u, phi_rep, decay = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q1_mf = np.zeros(2) + 0.5
    q2_mf = np.zeros((2, 2)) + 0.5

    # For model-based means and uncertainty: Beta-Bernoulli counts for each state-action2
    # Posterior params a,b start at 1 (uniform prior), track mean and variance analytically.
    a_post = np.ones((2, 2))
    b_post = np.ones((2, 2))

    # Repetition bias memory
    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    eps = 1e-12

    # Helper functions
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p_clip = np.clip(p, eps, 1 - eps)
        return np.log(p_clip) - np.log(1 - p_clip)

    for t in range(n_trials):
        # Current MB estimates of second-stage means and variances
        mb_mean = a_post / (a_post + b_post)
        mb_var = (a_post * b_post) / (((a_post + b_post) ** 2) * (a_post + b_post + 1.0))

        # Stage-1 MB values (expected max over aliens by state, then expectation over transitions)
        max_mean_per_state = np.max(mb_mean, axis=1)
        q1_mb = transition_matrix @ max_mean_per_state

        # Uncertainty per action: expected max variance in next state, then expectation over transitions
        max_var_per_state = np.max(mb_var, axis=1)
        uncert = transition_matrix @ max_var_per_state  # size 2, one per first-stage action

        # Arbitration weight per action: higher uncertainty -> smaller MB weight
        base_logit = logit(w0)
        w_actions = sigmoid(base_logit - kappa_u * uncert)

        # Repetition bias at stage 1
        rep1 = np.zeros(2)
        if last_a1 is not None:
            rep1[last_a1] += phi_rep

        # Combined stage-1 values, action-specific arbitration
        q1_comb = w_actions * q1_mb + (1.0 - w_actions) * q1_mf + rep1

        # Stage-1 policy
        q1c = q1_comb - np.max(q1_comb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in observed state with repetition bias
        s = state[t]
        rep2 = np.zeros(2)
        if last_a2_by_state[s] is not None:
            rep2[last_a2_by_state[s]] += phi_rep

        q2_policy = q2_mf[s] + rep2
        q2c = q2_policy - np.max(q2_policy)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Update Beta-Bernoulli posterior for MB estimates
        a_post[s, a2] += r
        b_post[s, a2] += (1.0 - r)

        # Model-free forgetting toward 0.5
        q1_mf = (1.0 - decay) * q1_mf + decay * 0.5
        q2_mf = (1.0 - decay) * q2_mf + decay * 0.5

        # Model-free updates (one-step TD with bootstrap from second stage)
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += (1.0 - decay) * pe2  # scale by (1-decay) to keep learning rate within [0,1]

        td_target_s1 = q2_mf[s, a2]
        pe1 = td_target_s1 - q1_mf[a1]
        q1_mf[a1] += (1.0 - decay) * pe1

        # Update repetition memory
        last_a1 = a1
        last_a2_by_state[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll