def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Successor-representation augmented hybrid model.
    
    This model learns a simple one-step successor map from first-stage actions
    to second-stage states (a 2x2 matrix), and uses it to compute a model-based
    value for stage-1 by backing up the best available second-stage values.
    It blends this SR-derived model-based value with a learned model-free
    first-stage value. Second-stage values are learned with TD. A first-stage
    choice stickiness bias captures perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (typically 0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha2: [0,1] learning rate for second-stage Q-values and for updating the model-free stage-1 value.
        - eta: [0,1] learning rate for the successor map (action->state occupancy probabilities).
        - beta: [0,10] inverse temperature for softmax at both stages.
        - omega: [0,1] weight on SR-derived model-based value in stage-1 (blend with model-free Q1).
        - stick1: [0,1] additive bias for repeating the previous first-stage action.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha2, eta, beta, omega, stick1 = model_parameters
    n_trials = len(action_1)

    # Probabilities of choices for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage Q(s,a)
    q2 = np.zeros((2, 2))
    # Model-free first-stage Q(a1)
    q1_mf = np.zeros(2)
    # Learned successor map M[a1, s] ~ P(s | a1)
    M = np.full((2, 2), 0.5)  # start with uninformative mapping

    prev_a1 = None

    for t in range(n_trials):
        # Compute SR-based MB value for stage 1
        max_q2 = np.max(q2, axis=1)      # best action value in each second-stage state
        q1_mb = M @ max_q2               # expected value under learned transitions

        # Blend MF and MB, add stickiness bias
        q1_net = (1.0 - omega) * q1_mf + omega * q1_mb
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] += stick1
            q1_net = q1_net + bias

        # Stage-1 choice probability via softmax
        q1_shift = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 choice probability via softmax in the reached state
        s = state[t]
        q2_s = q2[s].copy()
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Second-stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Stage-1 model-free update bootstrapping from second stage's chosen value
        target1 = q2[s, a2]
        q1_mf[a1] += alpha2 * (target1 - q1_mf[a1])

        # Successor map update toward observed state (one-step map)
        # Move the row for the chosen action toward the one-hot of the observed state
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        M[a1, :] += eta * (one_hot_s - M[a1, :])

        prev_a1 = a1

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-learning model-based controller with asymmetric outcome learning and surprise bonus.
    
    The agent learns the first-stage transition probabilities P(state | action1)
    and uses them to compute a model-based first-stage value from the current
    second-stage Q-values. Second-stage learning uses asymmetric learning rates
    for positive vs. negative TD errors. A trial-to-trial surprise signal
    (unexpectedness of the last transition) creates an action-specific bonus at
    stage 1 that biases repeating highly surprising actions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (typically 0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha_pos: [0,1] learning rate for positive TD errors at the second stage.
        - alpha_neg: [0,1] learning rate for negative TD errors at the second stage.
        - beta: [0,10] inverse temperature for softmax at both stages.
        - alpha_T: [0,1] learning rate for updating transition probabilities P(state|action1).
        - gamma: [0,1] weight of the surprise-driven first-stage action bonus.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha_pos, alpha_neg, beta, alpha_T, gamma = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage Q(s,a)
    q2 = np.zeros((2, 2))

    # Learned transition matrix T[a1, s] with rows summing to 1
    T = np.full((2, 2), 0.5)

    # Surprise trace per first-stage action, applied as an additive bonus next trial
    surprise_bonus = np.zeros(2)

    for t in range(n_trials):
        # Model-based first-stage value = E_s[ max_a2 Q2(s,a2) | a1 ]
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add action-specific surprise bonus (from previous trial)
        q1_net = q1_mb + gamma * surprise_bonus

        # Stage-1 softmax policy
        q1_shift = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax policy in the reached state
        s = state[t]
        q2_s = q2[s].copy()
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Second-stage TD with asymmetric learning rates
        delta2 = r - q2[s, a2]
        alpha = alpha_pos if delta2 >= 0 else alpha_neg
        q2[s, a2] += alpha * delta2

        # Compute surprise of this transition (before updating T)
        # Unexpectedness = 1 - P_T(observed s | chosen a1)
        unexpectedness = 1.0 - T[a1, s]

        # Update transition probabilities toward the observed state
        # Row a1 moves toward one-hot of s by alpha_T
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = (1.0 - alpha_T) * T[a1, :] + alpha_T * one_hot_s

        # Prepare surprise bonus for next trial: credit only the chosen action
        surprise_bonus = np.zeros(2)
        surprise_bonus[a1] = unexpectedness

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive model-free controller with choice kernels at both stages.
    
    The agent learns model-free Q-values at both stages. Rewards are transformed
    by a concave utility function to capture risk sensitivity. Additionally, a
    dynamic choice kernel at each stage captures short-term perseveration with
    decay over time.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (typically 0 or 1)
        Obtained reward on each trial.
    model_parameters : tuple/list of 5 floats
        - alpha: [0,1] learning rate for Q-values at both stages.
        - beta: [0,10] inverse temperature for softmax at both stages.
        - k: [0,1] weight of the choice kernel added to action values (both stages).
        - decay: [0,1] decay applied to the choice kernels each trial before reinforcement.
        - rho: [0,1] risk parameter in utility u(r) = r**(1 - rho); larger rho => more concave utility.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha, beta, k, decay, rho = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)          # stage-1 Q(a1)
    q2 = np.zeros((2, 2))     # stage-2 Q(s,a2)

    # Choice kernels (tendencies) for each stage
    K1 = np.zeros(2)
    K2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Form net preferences including choice kernels
        q1_net = q1 + k * K1

        q1_shift = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2_net = q2[s] + k * K2[s]

        q2_shift = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        # Risk-sensitive utility
        u = (r ** (1.0 - rho)) if r >= 0 else -((-r) ** (1.0 - rho))

        # Second-stage TD update
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update bootstrapping from updated second-stage value
        q1[a1] += alpha * (q2[s, a2] - q1[a1])

        # Update choice kernels with decay then reinforcement to 1 on chosen action
        K1 = (1.0 - decay) * K1
        K1[a1] += (1.0 - K1[a1])  # reinforce chosen action's kernel

        K2[s] = (1.0 - decay) * K2[s]
        K2[s, a2] += (1.0 - K2[s, a2])

    eps = 1e-10
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))