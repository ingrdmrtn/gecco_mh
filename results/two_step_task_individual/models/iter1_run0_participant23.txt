def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planning with asymmetric second-stage learning and lapse.

    On each trial, the agent plans at stage 1 using a fixed transition model
    and current second-stage values (model-based only). At stage 2, values are
    updated with asymmetric learning rates for positive vs. negative prediction
    errors. A lapse parameter mixes the softmax policy with uniform random choice
    at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1).
    reward : array-like of float
        Reward per trial (e.g., 0/1 coins).
    model_parameters : iterable
        [alpha_pos, alpha_neg, beta1, beta2, epsilon]
        - alpha_pos in [0,1]: learning rate for positive second-stage prediction errors.
        - alpha_neg in [0,1]: learning rate for negative second-stage prediction errors.
        - beta1 in [0,10]: inverse temperature for first-stage softmax.
        - beta2 in [0,10]: inverse temperature for second-stage softmax.
        - epsilon in [0,1]: lapse rate (probability of random choice at each stage).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta1, beta2, epsilon = model_parameters
    n_trials = len(action_1)

    # Fixed transition model: rows are spaceships A,U; columns are planets X,Y
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Second-stage Q-values: rows are states X,Y; columns are aliens/actions 0,1
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):

        # Stage 1: model-based action values via one-step planning
        max_q2 = np.max(q2, axis=1)           # value of states X,Y
        q1_mb = T_fixed @ max_q2              # expected value of choosing A or U

        # Softmax with lapse
        logits1 = beta1 * q1_mb
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        softmax1 = exp1 / (np.sum(exp1) + eps)
        probs1 = (1.0 - epsilon) * softmax1 + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2: softmax over current Q(s, Â·) with lapse
        s = state[t]
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        softmax2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - epsilon) * softmax2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 with asymmetric step sizes
        r = reward[t]
        pe2 = r - q2[s, a2]
        alpha = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha * pe2

        # No stage-1 MF learning (pure MB), so no q1 update

    negloglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return negloglik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with learned transitions under uncertainty and outcome sensitivity.

    The agent learns the transition matrix online and plans model-based at stage 1.
    Planning uses a convex combination (xi) between the learned transition model and
    an uninformative uniform model (0.5/0.5), capturing transition uncertainty.
    Second-stage values are updated with TD using outcome sensitivity (gamma) that
    scales the experienced reward. A single inverse temperature is used at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1).
    reward : array-like of float
        Reward per trial (e.g., 0/1 coins).
    model_parameters : iterable
        [alpha, beta, xi, tau_T, gamma]
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature (both stages).
        - xi in [0,1]: weight on the learned transition model vs. uniform (uncertainty).
                        xi=1 uses learned transitions; xi=0 ignores them (uniform).
        - tau_T in [0,1]: learning rate for transition probabilities.
        - gamma in [0,1]: outcome sensitivity scaling the reward (effective reward = gamma * r).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, xi, tau_T, gamma = model_parameters
    n_trials = len(action_1)

    # Initialize transition model as uninformative
    T_learned = np.full((2, 2), 0.5)
    T_uniform = np.full((2, 2), 0.5)

    # Second-stage Q-values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):

        # Blend learned transitions with uniform model to reflect uncertainty
        T_eff = xi * T_learned + (1.0 - xi) * T_uniform

        # Model-based values for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_eff @ max_q2

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome sensitivity: scale the experienced reward
        r = reward[t]
        r_eff = gamma * r

        # TD update at stage 2
        pe2 = r_eff - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update transition model with observed outcome (supervised update)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T_learned[a1] += tau_T * (target - T_learned[a1])

    negloglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return negloglik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free hierarchical TD with decaying choice kernels at both stages.

    This model uses model-free credit assignment from the second-stage value back to
    the first-stage action (hierarchical TD). Additionally, it includes decaying
    choice kernels (recency biases) at both stages that promote repeating recent choices.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1).
    reward : array-like of float
        Reward per trial (e.g., 0/1 coins).
    model_parameters : iterable
        [alpha, beta1, beta2, eta_c, zeta]
        - alpha in [0,1]: learning rate for both first- and second-stage TD updates.
        - beta1 in [0,10]: inverse temperature for first-stage softmax.
        - beta2 in [0,10]: inverse temperature for second-stage softmax.
        - eta_c in [0,1]: step size for increasing the choice kernel on chosen actions.
        - zeta in [0,1]: decay rate of choice kernels each trial (higher = faster decay).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, eta_c, zeta = model_parameters
    n_trials = len(action_1)

    # Model-free action values
    q1_mf = np.zeros(2)        # values for A,U
    q2 = np.zeros((2, 2))      # values for actions at X,Y

    # Choice kernels (recency biases)
    k1 = np.zeros(2)           # bias over A,U
    k2 = np.zeros(2)           # bias over second-stage actions (state-independent)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):

        # Stage 1 policy: MF values + choice kernel
        logits1 = beta1 * q1_mf + k1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: state-dependent Q plus global second-stage choice kernel
        s = state[t]
        logits2 = beta2 * q2[s] + k2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Hierarchical MF update at stage 1 bootstrapping on second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update choice kernels with decay and reinforcement of chosen actions
        k1 *= (1.0 - zeta)
        k2 *= (1.0 - zeta)
        k1[a1] += eta_c
        k2[a2] += eta_c

    negloglik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return negloglik