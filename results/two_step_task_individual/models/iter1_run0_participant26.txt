def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Adaptive arbitration with learned transitions and model-free bootstrapping.
    
    A first-stage hybrid policy blends model-based values (computed from a learned
    transition matrix) with model-free cached values. The arbitration weight
    adapts from trial to trial as a function of recent transition surprise.
    Second-stage values are learned via TD. First-stage model-free values
    bootstrap from the current estimate of second-stage values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (e.g., 0=W/P, 1=S/H) for each trial.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha, beta, w0, eta, alpha_trans]
        - alpha (0..1): Learning rate for Q-values (both stages).
        - beta (0..10): Inverse-temperature for softmax at both stages.
        - w0 (0..1): Baseline weight on model-based values at the first stage.
        - eta (0..1): Arbitration sensitivity; increases weight on model-based control after surprising transitions.
        - alpha_trans (0..1): Learning rate for the transition matrix P(state | action1).
        
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w0, eta, alpha_trans = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model P(s2 | a1) and value functions
    T = np.full((2, 2), 0.5)              # rows: a1 in {0,1}; cols: s2 in {0,1}; row-stochastic
    q1_mf = np.zeros(2)                   # model-free first-stage Q
    q2 = np.zeros((2, 2))                 # second-stage Q[s2, a2]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Arbitration state (surprise of the most recent transition, drives w_t)
    last_surprise = 0.5  # neutral initialization

    for t in range(n_trials):
        # Compute model-based first-stage values from learned transition model
        max_q2 = np.max(q2, axis=1)             # best second-stage value for each state
        q1_mb = T @ max_q2                      # expected value per first-stage action

        # Adaptive arbitration weight for this trial
        w_eff = (1.0 - eta) * w0 + eta * last_surprise
        w_eff = min(1.0, max(0.0, w_eff))       # ensure [0,1]

        # Hybrid first-stage values
        q1 = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # First-stage policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 /= (np.sum(probs_1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy within reached state
        s2 = state[t]
        q2_s = q2[s2].copy()
        q2_s_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_s_centered)
        probs_2 /= (np.sum(probs_2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD learning at second stage
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Model-free first-stage bootstrapping from updated second-stage estimate
        target1 = q2[s2, a2]  # current best estimate of the obtained branch
        q1_mf[a1] += alpha * (target1 - q1_mf[a1])

        # Transition surprise computed from pre-update transition prediction
        # Use the probability T[a1, s2] before updating T
        pred_p = T[a1, s2]
        last_surprise = 1.0 - pred_p  # larger surprise when the predicted probability was small

        # Update transition model T(a1, Â·) toward the observed state (one-hot)
        e = np.array([0.0, 1.0]) if s2 == 1 else np.array([1.0, 0.0])
        T[a1, :] = (1.0 - alpha_trans) * T[a1, :] + alpha_trans * e
        # ensure numerical row-stochasticity
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Successor-representation planning blended with model-free control and separate choice temperatures.
    
    First-stage action values are a convex combination of:
      - SR-based values: expected soft state values under a learned one-step SR (transition model)
      - Model-free values: cached first-stage values updated directly from experienced rewards
    Second-stage values are learned via TD. Separate inverse-temperatures control
    choice stochasticity at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (e.g., 0=W/P, 1=S/H) for each trial.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha, beta1, beta2, w_sr, gamma]
        - alpha (0..1): Learning rate for Q-values and SR.
        - beta1 (0..10): Inverse-temperature for first-stage softmax.
        - beta2 (0..10): Inverse-temperature for second-stage softmax.
        - w_sr (0..1): Weight on SR-based first-stage values (1=fully SR).
        - gamma (0..1): Discount scaling applied to SR value projection (controls planning depth/impact).
        
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta1, beta2, w_sr, gamma = model_parameters
    n_trials = len(action_1)

    # One-step SR over second-stage states (equivalent to learned transitions here)
    M = np.full((2, 2), 0.5)    # successor features for a1 over states s2
    q1_mf = np.zeros(2)         # model-free first-stage Q
    q2 = np.zeros((2, 2))       # second-stage Q[s2, a2]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Soft state values (log-sum-exp "softmax value") for planning
        # V_s = (1/beta2) * log sum_a exp(beta2 * Q[s,a])
        V_soft = np.zeros(2)
        for s in range(2):
            q = q2[s]
            q_center = q - np.max(q)
            Z = np.exp(beta2 * q_center).sum() + 1e-12
            V_soft[s] = (np.log(Z) / max(beta2, 1e-12)) + np.max(q)  # undo centering

        # SR-projected first-stage values
        q1_sr = gamma * (M @ V_soft)

        # Hybrid first-stage values
        q1 = (1.0 - w_sr) * q1_mf + w_sr * q1_sr

        # First-stage policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta1 * q1c)
        probs_1 /= (np.sum(probs_1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        q2s = q2[s2].copy()
        q2sc = q2s - np.max(q2s)
        probs_2 = np.exp(beta2 * q2sc)
        probs_2 /= (np.sum(probs_2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD learning at second stage
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Model-free first-stage update directly from obtained reward (cached habit)
        q1_mf[a1] += alpha * (r - q1_mf[a1])

        # SR (transition) update toward the observed state occupancy for the chosen action
        # One-step SR target equals the one-hot state vector (terminal after one step)
        target = np.array([1.0 - s2, float(s2)])  # one-hot for s2
        M[a1, :] += alpha * (target - M[a1, :])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based first stage with intrinsic novelty bonus and value forgetting at second stage.
    
    The first-stage choice is purely model-based using the fixed transition
    structure (common=0.7), projecting the current second-stage values.
    At the second stage, choices are influenced by an intrinsic novelty bonus
    that decreases with visit counts, and Q-values undergo forgetting toward
    a neutral baseline. Rewards are transformed by a utility parameter.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (e.g., 0=W/P, 1=S/H) for each trial.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha, beta, phi, decay, rho]
        - alpha (0..1): Learning rate for second-stage Q-values.
        - beta (0..10): Inverse-temperature for both stages.
        - phi (0..1): Scale of intrinsic novelty bonus at the second stage.
        - decay (0..1): Per-trial forgetting rate toward 0.5 for all second-stage Q-values.
        - rho (0..1): Reward sensitivity; effective reward r_eff = rho*r + (1-rho)*0.5.
        
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, phi, decay, rho = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    q2 = np.zeros((2, 2))       # second-stage Q[s2, a2]
    counts = np.ones((2, 2))    # initialize to 1 to define a finite initial bonus phi / sqrt(1+1)=phi/sqrt(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Apply global forgetting toward neutral value 0.5 at second stage
        q2 = (1.0 - decay) * q2 + decay * 0.5

        # Model-based first-stage value using the current second-stage values (no intrinsic bonus propagated)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # First-stage policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 /= (np.sum(probs_1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with novelty bonus
        s2 = state[t]
        # Compute count-based bonus: phi / sqrt(N)
        bonus = phi / np.sqrt(counts[s2] + 1e-12)
        q2_aug = q2[s2] + bonus

        q2c = q2_aug - np.max(q2_aug)
        probs_2 = np.exp(beta * q2c)
        probs_2 /= (np.sum(probs_2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward transformation
        r_eff = rho * reward[t] + (1.0 - rho) * 0.5

        # TD learning at second stage with effective reward
        delta2 = r_eff - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Update counts after acting
        counts[s2, a2] += 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll