def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pure model-free, asymmetric learning with stage-specific stickiness.
    Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 or 1).
    state : array-like, shape (n_trials,)
        Second-stage states (0 or 1).
    action_2 : array-like, shape (n_trials,)
        Second-stage actions (0 or 1).
    reward : array-like, shape (n_trials,)
        Rewards (e.g., 0/1).
    model_parameters : list or array-like, length 5
        [alpha_pos, alpha_neg, beta, tau_stick1, tau_stick2]
        - alpha_pos in [0,1]: learning rate used when the prediction error is positive.
        - alpha_neg in [0,1]: learning rate used when the prediction error is negative.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - tau_stick1 in [0,1]: first-stage perseveration strength (bias toward repeating previous first-stage action).
        - tau_stick2 in [0,1]: second-stage perseveration strength (bias toward repeating previous second-stage action).
    Notes
    -----
    - This is a purely model-free SARSA(1)-like learner with asymmetric learning rates.
    - First-stage Q-values are updated by propagating the second-stage reward prediction error (eligibility = 1).
    - Stickiness is implemented as an additive bias to the logits favoring the previous action at each stage.
    """
    alpha_pos, alpha_neg, beta, tau_stick1, tau_stick2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free state-action values
    q1_mf = np.zeros(2)       # first-stage actions: 0,1
    q2 = np.zeros((2, 2))     # second-stage states x actions

    prev_a1 = 0  # initialize previous actions (arbitrary)
    prev_a2 = 0

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # First-stage policy with stickiness
        logits1 = beta * q1_mf.copy()
        # add stickiness bias toward previous action
        logits1[prev_a1] += tau_stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with stickiness
        logits2 = beta * q2[s].copy()
        logits2[prev_a2] += tau_stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: second stage
        pe2 = r - q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * pe2

        # Learning: first stage (propagate second-stage PE; SARSA(1)-like)
        pe1 = pe2  # eligibility = 1
        lr1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        q1_mf[a1] += lr1 * pe1

        # Update stickiness memory
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    MB-MF mixture with learned transition model and value forgetting at stage 2.
    Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 or 1).
    state : array-like, shape (n_trials,)
        Second-stage states (0 or 1).
    action_2 : array-like, shape (n_trials,)
        Second-stage actions (0 or 1).
    reward : array-like, shape (n_trials,)
        Rewards (e.g., 0/1).
    model_parameters : list or array-like, length 5
        [alpha_q, beta, w_mix, alpha_T, zeta]
        - alpha_q in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w_mix in [0,1]: mixture weight of model-based value at stage 1.
        - alpha_T in [0,1]: learning rate for transition probabilities T(s | a1).
        - zeta in [0,1]: forgetting/decay rate applied to all second-stage Q-values each trial.
    Notes
    -----
    - Transition matrix T is learned online via delta-rule with row-wise renormalization.
    - Stage-1 action values are a mixture of learned model-free values and model-based values computed from T and Q2.
    - Second-stage Q-values undergo decay toward zero controlled by zeta before each update.
    """
    alpha_q, beta, w_mix, alpha_T, zeta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize learned transition matrix; start unbiased
    T = np.full((2, 2), 0.5)

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB values using current T and Q2
        max_q2 = np.max(q2, axis=1)      # for each state, best second-stage value
        q1_mb = T @ max_q2               # expected value under learned transitions

        # Mixture at stage 1
        q1_mix = w_mix * q1_mb + (1.0 - w_mix) * q1_mf

        # Stage 1 policy
        logits1 = beta * q1_mix
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Transition learning: update row a1 toward observed state s, keep stochastic
        # Move mass alpha_T toward one-hot at s.
        T[a1, :] = (1.0 - alpha_T) * T[a1, :]
        T[a1, s] += alpha_T
        # (Row remains normalized since (1-alpha_T)*1 + alpha_T = 1.)

        # Value forgetting at stage 2
        q2 *= (1.0 - zeta)

        # Second-stage Q update (MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # First-stage MF update toward the post-update value at second stage (bootstrapped)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Transition-sensitive choice-kernel plus model-free values.
    Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 or 1).
    state : array-like, shape (n_trials,)
        Second-stage states (0 or 1).
    action_2 : array-like, shape (n_trials,)
        Second-stage actions (0 or 1).
    reward : array-like, shape (n_trials,)
        Rewards (e.g., 0/1).
    model_parameters : list or array-like, length 5
        [alpha, beta, k_win, k_lose, theta_trans]
        - alpha in [0,1]: learning rate for Q-value updates (both stages) and kernel decay.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - k_win in [0,1]: magnitude added to the first-stage choice kernel after rewarded trials.
        - k_lose in [0,1]: magnitude subtracted (inhibitory) from the first-stage choice kernel after unrewarded trials.
        - theta_trans in [0,1]: dampening factor on kernel updates after surprising (rare) transitions
                                 based on fixed transitions (common=0.7, rare=0.3).
    Notes
    -----
    - First-stage action selection combines model-free Q1 values with a valence-dependent choice kernel K1.
    - K1 is reinforced by wins (stay bias) and suppressed by losses (shift bias).
    - The impact of outcome on K1 is modulated by transition surprise: rare transitions reduce the kernel update
      by a factor (1 - theta_trans * surprise), where surprise = 1 - P(s | a1).
    - Second-stage policy is purely model-free.
    """
    alpha, beta, k_win, k_lose, theta_trans = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure used to compute surprise only
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # First-stage choice kernel (perseveration-like but outcome- and transition-gated)
    K1 = np.zeros(2)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 1 policy: combine MF values and kernel
        logits1 = beta * q1_mf + K1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: MF softmax
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: second stage MF
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Learning: first stage MF (propagate second-stage PE)
        q1_mf[a1] += alpha * pe2

        # Update first-stage choice kernel with valence and transition gating
        # Compute surprise under fixed T
        prob_s = T_fixed[a1, s]
        surprise = 1.0 - prob_s  # 0.3 for common, 0.7 for rare
        gate = 1.0 - theta_trans * surprise
        # Decay kernel toward zero
        K1 *= (1.0 - alpha)
        # Apply win/lose increment to the chosen action
        if r > 0.0:
            K1[a1] += k_win * gate
        else:
            K1[a1] -= k_lose * gate

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll