def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with learned transitions, entropy-based arbitration, and perseveration.

    The agent learns second-stage action values (model-free) and learns the first-stage
    transition structure online. First-stage decisions combine model-based action values
    (computed from the learned transition matrix and current second-stage values) with
    cached model-free first-stage values. The mixing weight is computed per action based on
    the transition entropy of that action: lower uncertainty (entropy) increases reliance
    on model-based values. A perseveration bias adds value to the previously chosen action
    at each stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Reward received on each trial (e.g., 0 or 1).
    model_parameters : list or array
        [alpha, beta, zeta, gamma, pi]
        - alpha in [0,1]: learning rate for model-free Q updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - zeta in [0,1]: transition learning rate for first-stage transition probabilities.
        - gamma in [0,1]: arbitration sensitivity; converts transition entropy into
          a per-action MB weight: w_a = 0.5 + gamma * (0.5 - H_a), where H_a in [0,1].
        - pi in [0,1]: perseveration weight added to previously chosen actions (both stages).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, zeta, gamma, pi = model_parameters
    n_trials = len(action_1)

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Learned transition matrix T[a, s]; start uninformative
    T = np.full((2, 2), 0.5)

    # Track previous choices for perseveration
    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        # Compute model-based first-stage values from learned transitions
        max_q2_per_state = np.max(q_stage2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2_per_state  # shape (2,)

        # Per-action arbitration weight based on entropy of transition rows
        # Entropy normalized to [0,1] via division by log(2)
        H = np.zeros(2)
        for a in range(2):
            p = T[a].clip(1e-12, 1 - 1e-12)
            H[a] = -(p[0] * np.log(p[0]) + p[1] * np.log(p[1])) / np.log(2.0)  # in [0,1]
        w = 0.5 + gamma * (0.5 - H)  # higher w for lower entropy (more MB)

        # Hybrid first-stage values
        q1_hybrid = w * q1_mb + (1.0 - w) * q_stage1_mf

        # Add perseveration at stage 1
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            q1_hybrid = q1_hybrid + pi * stick

        # Softmax for first stage
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage decision values with perseveration
        s = state[t]
        q2_eff = q_stage2[s].copy()
        if prev_a2_by_state[s] is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2_by_state[s]] = 1.0
            q2_eff = q2_eff + pi * stick2

        q2c = q2_eff - np.max(q2_eff)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Model-free updates
        # Stage-2 TD
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 model-free update toward the (updated) stage-2 value
        delta1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Transition learning: update row for chosen action toward the observed state
        # RW update that preserves row normalization
        for ss in range(2):
            target = 1.0 if ss == s else 0.0
            T[a1, ss] += zeta * (target - T[a1, ss])
        # Numerical stability: renormalize the row
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with value forgetting and win-stay/lose-shift choice kernels.

    The agent plans at the first stage using the known transition structure (common=0.7),
    while second-stage values are learned model-free. Both stage-1 and stage-2 values
    undergo forgetting toward zero to capture volatility. Additionally, separate
    win-stay and lose-shift choice kernels bias choices at both stages based on the
    previous trial's outcome, with kernel states maintained separately for each second-stage state.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Reward received on each trial (e.g., 0 or 1).
    model_parameters : list or array
        [alpha, beta, decay, phi_win, phi_lose]
        - alpha in [0,1]: learning rate for model-free second-stage values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - decay in [0,1]: forgetting rate applied each trial to all Q-values and kernels.
        - phi_win in [0,1]: magnitude added to the chosen action's kernel after a win (r>0).
        - phi_lose in [0,1]: magnitude subtracted from the chosen action's kernel after a loss (r<=0),
          encouraging lose-shift.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, decay, phi_win, phi_lose = model_parameters
    n_trials = len(action_1)

    # Known transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values and kernels
    q_stage2 = np.zeros((2, 2))      # model-free second-stage values
    kernel1 = np.zeros(2)            # first-stage choice kernel
    kernel2 = np.zeros((2, 2))       # second-stage choice kernels, per state

    # Track last trial's chosen actions and outcome to update kernels
    prev_a1 = None
    prev_s = None
    prev_a2 = None
    prev_r = None

    for t in range(n_trials):
        # Apply forgetting to values and kernels
        q_stage2 *= (1.0 - decay)
        kernel1 *= (1.0 - decay)
        kernel2 *= (1.0 - decay)

        # First-stage model-based planning
        max_q2_per_state = np.max(q_stage2, axis=1)
        q1_mb = transition_matrix @ max_q2_per_state
        q1_eff = q1_mb + kernel1

        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second stage
        s = state[t]
        q2_eff = q_stage2[s] + kernel2[s]

        q2c = q2_eff - np.max(q2_eff)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Update second-stage values
        r = reward[t]
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Update kernels based on the previous trial's outcome (win-stay / lose-shift)
        if prev_r is not None and prev_a1 is not None:
            if prev_r > 0.0:
                kernel1[prev_a1] += phi_win
                if prev_s is not None and prev_a2 is not None:
                    kernel2[prev_s, prev_a2] += phi_win
            else:
                kernel1[prev_a1] -= phi_lose
                if prev_s is not None and prev_a2 is not None:
                    kernel2[prev_s, prev_a2] -= phi_lose

        # Bookkeeping for kernels
        prev_a1 = a1
        prev_s = s
        prev_a2 = a2
        prev_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based with learned transitions, UCB-style uncertainty bonus at stage-2, and transition-dependent repetition bias.

    The agent learns the first-stage transition structure and second-stage rewards online.
    At the second stage, choices include an exploration bonus inversely proportional to
    the square root of visit counts (UCB-like). First-stage values are computed model-based
    from the learned transitions and current second-stage values (including the exploration
    bonus), and a transition-dependent repetition bias encourages repeating the previous
    first-stage choice after a common transition, but switching after a rare transition.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Reward received on each trial (e.g., 0 or 1).
    model_parameters : list or array
        [alpha, beta, zeta, xi, chi]
        - alpha in [0,1]: learning rate for model-free second-stage values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - zeta in [0,1]: transition learning rate for first-stage transition probabilities.
        - xi in [0,1]: weight of the stage-2 uncertainty (exploration) bonus.
        - chi in [0,1]: transition-dependent repetition bias at stage-1:
          after a common transition, adds +chi to the previous first-stage action;
          after a rare transition, adds +chi to the alternate action (encouraging switch).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, zeta, xi, chi = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Learned transition matrix
    T = np.full((2, 2), 0.5)

    # Second-stage values and visit counts for uncertainty
    q_stage2 = np.zeros((2, 2))
    visits = np.zeros((2, 2))  # counts per state-action

    # Memory for transition-dependent repetition bias
    prev_a1 = None
    prev_state = None  # state observed on previous trial

    for t in range(n_trials):
        # Compute stage-2 uncertainty bonuses (state-dependent)
        U = 1.0 / np.sqrt(visits + 1.0)  # in (0,1]; decreases with visits

        # First-stage model-based planning uses current q2 plus exploration bonus
        q2_plus_bonus = q_stage2 + xi * U
        max_q2_per_state = np.max(q2_plus_bonus, axis=1)  # shape (2,)
        q1_mb = T @ max_q2_per_state  # expected value per first-stage action

        # Transition-dependent repetition bias
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_state is not None:
            # Identify whether last transition was common:
            # Common if (a1==0 and s==0) or (a1==1 and s==1)
            was_common = (prev_a1 == prev_state)
            if was_common:
                bias1[prev_a1] += chi
            else:
                bias1[1 - prev_a1] += chi

        q1_eff = q1_mb + bias1

        # Softmax for first stage
        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage decision with exploration bonus
        s = state[t]
        q2_eff = q_stage2[s] + xi * U[s]

        q2c = q2_eff - np.max(q2_eff)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcomes and updates
        r = reward[t]

        # Update second-stage values
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Update visit counts after choosing a2 in state s
        visits[s, a2] += 1.0

        # Update transition model for chosen first-stage action toward observed state
        for ss in range(2):
            target = 1.0 if ss == s else 0.0
            T[a1, ss] += zeta * (target - T[a1, ss])
        # Renormalize row
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        # Bookkeeping for next trial's bias
        prev_a1 = a1
        prev_state = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll