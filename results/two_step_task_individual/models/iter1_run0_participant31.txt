def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Pure model-based with second-stage uncertainty bonus (UCB-style) and action perseveration at stage 2.

    Mechanism:
    - Learns second-stage action values (Q) via TD.
    - Tracks a per-action uncertainty signal u (per state) as an exponentially weighted
      moving average of absolute RPE magnitude. Adds an exploration bonus eta * u
      to second-stage values (UCB-like).
    - First-stage policy is purely model-based by projecting bonus-augmented second-stage
      values through the known transition matrix.
    - Second-stage perseveration bias favors repeating the previous second-stage action
      within the same state.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (planet) reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) on each trial (state-specific alien indices 0/1).
    reward : array-like of float
        Reward (coins) received on each trial (typically 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha, beta, eta, tau, xi]
        - alpha in [0,1]: Learning rate for second-stage Q-values.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - eta in [0,1]: Weight of the uncertainty (exploration) bonus added to Q at stage 2.
        - tau in [0,1]: Smoothing for the uncertainty tracker u (higher = faster tracking of RPE magnitude).
        - xi in [0,1]: Second-stage action perseveration bias added to the previously chosen action in that state.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, eta, tau, xi = model_parameters
    n_trials = len(action_1)

    # Known transitions: rows = first-stage action (A,U), cols = next state (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage MF Q-values and uncertainty trackers
    q_stage2 = np.zeros((2, 2))  # rows=state (X,Y), cols=action (0,1)
    u_stage2 = np.ones((2, 2))   # initialize with 1 to encourage initial exploration

    # Perseveration memory per state
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        # Compute first-stage MB values from bonus-augmented second-stage values
        q2_bonus = q_stage2 + eta * u_stage2
        max_q2_bonus = np.max(q2_bonus, axis=1)  # value of each second-stage state under bonus
        q1 = transition_matrix @ max_q2_bonus

        # First-stage policy
        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with perseveration bias
        s = state[t]
        q2_pol = q_stage2[s, :].copy()
        # Add perseveration to repeating last action at this state
        if prev_a2_by_state[s] is not None:
            q2_pol[prev_a2_by_state[s]] += xi

        q2_centered = q2_pol - np.max(q2_pol)
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Update uncertainty as smoothed absolute RPE magnitude
        u_stage2[s, a2] = (1.0 - tau) * u_stage2[s, a2] + tau * abs(delta2)

        # Update perseveration memory
        prev_a2_by_state[s] = a2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with asymmetric learning rates and reward-contingent first-stage perseveration.

    Mechanism:
    - Second-stage Q-values learned with asymmetric learning rates for positive vs negative RPEs.
    - First-stage MF values updated directly by the second-stage RPE (no explicit lambda parameter).
    - First-stage action values are a weighted combination of MB (projected second-stage values)
      and MF values.
    - A reward-contingent perseveration bias at stage 1 increases the tendency to repeat
      the previous first-stage action after reward and decreases it after no reward.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (planet) reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) on each trial (state-specific alien indices 0/1).
    reward : array-like of float
        Reward (coins) received on each trial (typically 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha_pos, alpha_neg, beta, w_mb, psi]
        - alpha_pos in [0,1]: Learning rate when RPE >= 0.
        - alpha_neg in [0,1]: Learning rate when RPE < 0.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - w_mb in [0,1]: Weight on model-based values at stage 1 (1=fully MB, 0=fully MF).
        - psi in [0,1]: Magnitude of reward-contingent perseveration at stage 1
                        (added to repeating previous a1 after reward, subtracted after no reward).

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, w_mb, psi = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    prev_a1 = None
    prev_r = None

    for t in range(n_trials):
        # MB first-stage values from current second-stage Q
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Reward-contingent perseveration bias for first-stage
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_r is not None:
            # Add +psi to repeating previous action if previously rewarded, else -psi
            sign = 1.0 if prev_r > 0 else -1.0
            bias1[prev_a1] += sign * psi

        # Hybrid first-stage values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias1

        # First-stage policy
        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2_centered = q2[s, :] - np.max(q2[s, :])
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcomes
        r = reward[t]

        # Learning rate selection by sign of RPE
        delta2 = r - q2[s, a2]
        alpha_use = alpha_pos if delta2 >= 0 else alpha_neg

        # Update second-stage Q
        q2[s, a2] += alpha_use * delta2

        # Update first-stage MF by propagating the same RPE
        q1_mf[a1] += alpha_use * delta2

        # Update history
        prev_a1 = a1
        prev_r = r

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Arbitrated MB/MF with learned transitions and lapse noise.

    Mechanism:
    - Learns both second-stage Q-values and the first-stage transition model online.
    - Computes model-based first-stage values using the learned transition matrix.
    - Arbitration: weight on MB vs MF depends on transition certainty. For each
      first-stage action, the certainty is 1 - normalized entropy of the learned
      transition row. The effective MB weight is zeta * certainty (action-specific).
    - Lapse noise (epsilon) mixes softmax probabilities with a uniform distribution
      to capture occasional random choices at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (planet) reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) on each trial (state-specific alien indices 0/1).
    reward : array-like of float
        Reward (coins) received on each trial (typically 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha, beta, alpha_tr, zeta, epsilon]
        - alpha in [0,1]: Learning rate for second-stage Q-values (MF).
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - alpha_tr in [0,1]: Learning rate for transition probabilities per first-stage action.
        - zeta in [0,1]: Maximum arbitration weight favoring model-based control
                         as transition certainty increases.
        - epsilon in [0,1]: Lapse probability blending softmax with a uniform policy.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, alpha_tr, zeta, epsilon = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model to uninformative (0.5, 0.5) for each action
    T = np.full((2, 2), 0.5)  # rows=action (A,U), cols=state (X,Y); rows stay normalized

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Model-based values using learned transitions
        max_q2 = np.max(q2, axis=1)  # value of each state
        q1_mb = T @ max_q2  # MB values for actions A,U

        # Compute action-wise certainty from transition entropy (base-2 normalized)
        # For a binary row [p, 1-p], entropy is -sum p log2 p, max=1 bit when p=0.5.
        certainty = np.zeros(2)
        for a in range(2):
            p = T[a, 0]
            q = T[a, 1]
            # Avoid log(0) by clipping
            p_cl = max(min(p, 1 - 1e-12), 1e-12)
            q_cl = max(min(q, 1 - 1e-12), 1e-12)
            H = -(p_cl * (np.log(p_cl) / np.log(2.0)) + q_cl * (np.log(q_cl) / np.log(2.0)))  # in bits
            certainty[a] = 1.0 - H  # normalized since max H=1 for binary

        # Effective MB weights per action
        w_eff = zeta * certainty  # shape (2,)

        # Hybrid first-stage values (action-wise arbitration)
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # First-stage policy with lapse
        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_centered)
        softmax_1 = exp_q1 / np.sum(exp_q1)
        probs_1 = (1.0 - epsilon) * softmax_1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with lapse
        s = state[t]
        q2_centered = q2[s, :] - np.max(q2[s, :])
        exp_q2 = np.exp(beta * q2_centered)
        softmax_2 = exp_q2 / np.sum(exp_q2)
        probs_2 = (1.0 - epsilon) * softmax_2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]

        # Second-stage update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # First-stage MF update propagating second-stage RPE
        q1_mf[a1] += alpha * delta2

        # Transition learning: move T[a1] toward the observed next state
        # Target one-hot distribution for observed state s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1.0 - alpha_tr) * T[a1, :] + alpha_tr * target
        # Renormalize to guard against numerical drift
        T[a1, :] /= np.sum(T[a1, :])

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik