def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Asymmetric learning with learned transitions (hybrid MB/MF).

    This model learns the first-stage transition matrix from experience and
    uses a hybrid of model-based (MB) planning and model-free (MF) values to
    select the first-stage action. Second-stage values are learned with
    asymmetric learning rates for positive vs. negative prediction errors.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien indexes per state).
    reward : array-like of float (typically 0 or 1)
        Reward received on each trial.
    model_parameters : list or tuple of 5 floats
        [alpha_pos, alpha_neg, beta, w, rho]
        - alpha_pos in [0,1]: learning rate when stage-2 PE >= 0.
        - alpha_neg in [0,1]: learning rate when stage-2 PE < 0.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight on model-based values at stage 1 (1=fully MB).
        - rho in [0,1]: transition learning rate toward observed next state.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, w, rho = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix rows to be uninformative (0.5/0.5)
    T = np.full((2, 2), 0.5)
    # MF value functions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # Model-based backup using learned transitions and current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid stage-1 values
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 choice probabilities
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probabilities
        logits2 = beta * q2[s].copy()
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn transitions for the chosen first-stage action from observed state
        # Row update toward one-hot of observed state
        T[a1] = (1.0 - rho) * T[a1]
        T[a1, s] += rho
        # Numerical clamp/renormalize to ensure stochasticity
        row_sum = np.sum(T[a1])
        if row_sum <= 0:
            T[a1] = np.array([0.5, 0.5])
        else:
            T[a1] /= row_sum

        # Stage-2 update with asymmetric learning rates
        pe2 = r - q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0 else alpha_neg
        q2[s, a2] += alpha2 * pe2

        # Stage-1 MF update bootstrapping from the (updated) stage-2 value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with dual temperatures, stickiness, and value forgetting.

    Stage-2 values are learned by TD; stage-1 MF values bootstrap from stage-2.
    A stage-1 perseveration term biases repeating the previous first-stage choice.
    Both stages have separate inverse temperatures. All MF values undergo
    soft forgetting toward baselines each trial.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien indexes per state).
    reward : array-like of float (typically 0 or 1)
        Reward received on each trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta1, beta2, kappa, phi]
        - alpha in [0,1]: learning rate for MF updates.
        - beta1 in [0,10]: inverse temperature for stage-1 softmax.
        - beta2 in [0,10]: inverse temperature for stage-2 softmax.
        - kappa in [0,1]: stage-1 perseveration strength added to previous action's logit.
        - phi in [0,1]: forgetting rate toward baselines (q1->0, q2->0.5) each trial.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta1, beta2, kappa, phi = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)        # stage-1 MF values
    q2 = np.full((2, 2), 0.5)  # stage-2 MF values start neutral

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        s = state[t]

        # Stage-1 policy with perseveration
        logits1 = beta1 * q1.copy()
        if prev_a1 is not None:
            logits1[prev_a1] += kappa
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta2 * q2[s].copy()
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF TD bootstrap from current stage-2 value
        pe1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * pe1

        # Soft forgetting toward baselines
        q1 = (1.0 - phi) * q1  # toward 0
        q2 = (1.0 - phi) * q2 + phi * 0.5  # toward 0.5

        prev_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-aware model-based planner with MF first-stage bias.

    Stage-2 action values are represented by Beta-Bernoulli posteriors for each
    alien, yielding an expected value and an uncertainty bonus. The agent plans
    at stage 1 using the known transition structure and uncertainty-augmented
    stage-2 state values, mixed with a model-free first-stage value. A small
    stickiness biases repeating the previous first-stage choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien indexes per state).
    reward : array-like of float (typically 0 or 1)
        Reward received on each trial.
    model_parameters : list or tuple of 5 floats
        [beta, omega, gamma, alpha_mf, stick]
        - beta in [0,10]: inverse temperature for both stages.
        - omega in [0,1]: weight on model-based value at stage 1 (1=fully MB).
        - gamma in [0,1]: uncertainty bonus strength added to expected values at stage 2.
        - alpha_mf in [0,1]: learning rate for stage-1 MF value toward stage-2 value.
        - stick in [0,1]: stage-1 choice stickiness added to previous action's logit.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    beta, omega, gamma, alpha_mf, stick = model_parameters
    n_trials = len(action_1)

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Stage-2 Beta posteriors: a,b counts per (state,action), start at (1,1)
    a_counts = np.ones((2, 2))
    b_counts = np.ones((2, 2))

    # Stage-1 MF values
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        s = state[t]

        # Compute stage-2 expected values and uncertainty
        means = a_counts / (a_counts + b_counts)
        vars_ = (means * (1.0 - means)) / (a_counts + b_counts + 1.0)
        stds = np.sqrt(vars_)
        v2 = means + gamma * stds  # uncertainty bonus

        # Stage-2 policy in the observed state
        logits2 = beta * v2[s].copy()
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Model-based stage-1 value via one-step lookahead with v2
        max_v2 = np.max(v2, axis=1)
        q1_mb = T @ max_v2

        # Mix MB and MF, add stickiness, then compute policy
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf
        logits1 = beta * q1
        if prev_a1 is not None:
            logits1[prev_a1] += stick
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        r = reward[t]

        # Update Beta posterior for the chosen stage-2 action
        a_counts[s, a2] += r
        b_counts[s, a2] += (1.0 - r)

        # Update stage-1 MF toward current stage-2 (pre-update) expected value
        # Use the means computed at the start of the trial
        pe1 = means[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_mf * pe1

        prev_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss