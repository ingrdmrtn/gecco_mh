def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Adaptive arbitration with learned transitions (surprise-gated model-based weight) plus model-free TD.
    
    This model learns the stage-1 transition matrix online and dynamically shifts
    weight between model-based (MB) planning and model-free (MF) values at stage 1
    based on transition surprise. High surprise increases reliance on MB planning.
    
    Parameters (bounds):
    - alpha: [0, 1]
        Learning rate for model-free Q-value updates (both stages).
    - beta: [0, 10]
        Inverse temperature for softmax choice policy (both stages).
    - eta: [0, 1]
        Learning rate for updating the transition probabilities from each first-stage action.
    - omega0: [0, 1]
        Baseline MB weight at stage 1 (when surprise is neutral).
    - kappa: [0, 1]
        Sensitivity of MB weight to transition surprise (higher -> more MB after surprising transitions).
    
    Inputs:
    - action_1: array-like ints {0,1}, first-stage choices (0=A, 1=U).
    - state: array-like ints {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array-like ints {0,1}, second-stage choices within the reached state.
    - reward: array-like floats/ints (typically 0/1), outcome per trial.
    - model_parameters: iterable [alpha, beta, eta, omega0, kappa].
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, eta, omega0, kappa = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T[a, s] = P(state=s | action=a)
    T = np.full((2, 2), 0.5)  # uninformative prior

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)        # for actions at stage 1
    q2_mf = np.zeros((2, 2))   # for states x actions at stage 2

    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation via current learned transitions
        max_q2 = np.max(q2_mf, axis=1)      # best second-stage value per state
        q1_mb = T @ max_q2                  # plan by taking expectation over next states

        # Surprise of observed transition given chosen action (before updating T)
        a1 = action_1[t]
        p_obs = T[a1, s]
        surprise = 1.0 - p_obs  # high when rare under current T

        # Adaptive MB weight constrained to [0,1]
        w = omega0 + kappa * (2.0 * surprise - 1.0)  # maps surprise in [0,1] to [-1,1] scale
        w = min(1.0, max(0.0, w))

        # Stage-1 policy (mixture of MB and MF)
        q1 = w * q1_mb + (1.0 - w) * q1_mf
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure MF at the visited state)
        a2 = action_2[t]
        logits2 = beta * (q2_mf[s] - np.max(q2_mf[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Outcomes
        r = reward[t]

        # MF learning at stage 2
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # MF learning at stage 1 (bootstrap from stage-2 value)
        target1 = q2_mf[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update transition model with simple delta rule toward observed state
        # Move probability mass from non-observed to observed state.
        old_row = T[a1].copy()
        T[a1] = (1.0 - eta) * T[a1]
        T[a1, s] += eta
        # Renormalize to protect against numerical drift
        T[a1] /= np.sum(T[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free controller with asymmetric learning, forgetting, and win-stay/lose-shift bias at stage 2.
    
    This model eschews planning and learns model-free action values. It incorporates:
    - Asymmetric outcome sensitivity: separate learning gain for rewards vs. non-rewards.
    - Forgetting at stage 2 toward an uninformative baseline for unchosen actions.
    - Win-stay/lose-shift (WSLS) choice bias at stage 2 conditioned on previous outcome in that state.
    
    Parameters (bounds):
    - alpha: [0, 1]
        Base learning rate for MF value updates.
    - beta: [0, 10]
        Inverse temperature for softmax choice policy (both stages).
    - rho: [0, 1]
        Outcome asymmetry: effective learning rate is alpha*rho after reward=1
        and alpha*(1-rho) after reward=0.
    - zeta: [0, 1]
        Forgetting rate for unchosen second-stage actions in the visited state toward 0.5.
    - ws: [0, 1]
        Strength of WSLS bias at stage 2. Positive values bias repeating the last
        action after a win and switching after a loss in that state.
    
    Inputs:
    - action_1: array-like ints {0,1}, first-stage choices (0=A, 1=U).
    - state: array-like ints {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array-like ints {0,1}, second-stage choices within the reached state.
    - reward: array-like floats/ints (typically 0/1), outcome per trial.
    - model_parameters: iterable [alpha, beta, rho, zeta, ws].
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, rho, zeta, ws = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Track last action and outcome in each second-stage state for WSLS bias
    last_a2 = [None, None]
    last_r2 = [0.0, 0.0]

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 policy (pure MF)
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with WSLS bias
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            # +ws if last trial in this state was rewarded and we repeat; -ws if last trial was unrewarded and we repeat
            bias2[last_a2[s]] += ws * (1.0 if last_r2[s] > 0 else -1.0)
        logits2 = beta * ((q2[s] + bias2) - np.max(q2[s] + bias2))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Asymmetric MF learning at stage 2
        alpha_eff = alpha * (rho if r > 0 else (1.0 - rho))
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * delta2

        # Forgetting for unchosen action in the visited state toward 0.5
        other = 1 - a2
        q2[s, other] += zeta * (0.5 - q2[s, other])

        # Stage-1 MF backup toward observed stage-2 value (no planning)
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

        # Update WSLS memory
        last_a2[s] = a2
        last_r2[s] = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor representation (SR) for stage-1 evaluation combined with model-free values.
    
    This model learns an SR mapping from first-stage actions to expected discounted
    occupancies of second-stage states, enabling flexible evaluation with changing
    second-stage rewards. Stage-1 choice values are a weighted mixture of SR-based
    evaluation and model-free values. Stage-2 is learned model-free.
    
    Parameters (bounds):
    - alpha: [0, 1]
        Learning rate for model-free Q-value updates (both stages).
    - beta: [0, 10]
        Inverse temperature for softmax choice policy (both stages).
    - chi: [0, 1]
        Learning rate for the successor representation (SR) updates.
    - lam_sr: [0, 1]
        Discount/persistence parameter in SR updates (controls how strongly SR
        retains prior structure; smaller values speed adaptation).
    - psi: [0, 1]
        Mixture weight for SR vs. MF at stage 1: Q1 = psi*Q1_SR + (1-psi)*Q1_MF.
    
    Inputs:
    - action_1: array-like ints {0,1}, first-stage choices (0=A, 1=U).
    - state: array-like ints {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array-like ints {0,1}, second-stage choices within the reached state.
    - reward: array-like floats/ints (typically 0/1), outcome per trial.
    - model_parameters: iterable [alpha, beta, chi, lam_sr, psi].
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, chi, lam_sr, psi = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Successor representation: M[action, state] ~ expected discounted visitation of state after action
    M = np.full((2, 2), 0.5)  # initialize uninformatively

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Evaluate SR-based stage-1 values using current best second-stage values
        max_q2 = np.max(q2, axis=1)          # V(state) = max_a q2[state, a]
        q1_sr = M @ max_q2                   # SR projection into state values

        # Mixture for stage-1 choice
        q1 = psi * q1_sr + (1.0 - psi) * q1_mf
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF learning (bootstrap from observed stage-2 action value)
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # SR update for the chosen first-stage action toward the observed state
        # TD-style update: M[a1] <- M[a1] + chi * (one_hot(s) + lam_sr * M[a1] - M[a1])
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        M[a1] += chi * (one_hot_s + lam_sr * M[a1] - M[a1])
        # Normalize rows to sum to 1 to keep M interpretable as occupancy weights
        row_sum = np.sum(M[a1])
        if row_sum > 0:
            M[a1] /= row_sum

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll