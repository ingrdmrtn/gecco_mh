def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Asymmetric-outcome hybrid MFâ€“MB with surprise-gated arbitration.
    The agent learns model-free values for both stages with asymmetric learning
    rates for positive vs. negative outcomes, and uses a fixed transition
    structure to compute model-based first-stage values. Arbitration between
    model-based (MB) and model-free (MF) control at stage 1 is dynamically
    modulated by transition "surprise": rare transitions increase reliance
    on MB control, while common transitions decrease it.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A; 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 = first alien, 1 = second alien) in the reached state.
    reward : array-like of float (e.g., 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_pos, alpha_neg, eta, beta1, beta2]
        - alpha_pos: [0, 1] learning rate when reward > 0
        - alpha_neg: [0, 1] learning rate when reward == 0
        - eta: [0, 1] surprise-gating strength for MB arbitration.
                MB weight w_t = clip(0.5 + eta * (+1 if rare else -1), 0, 1)
        - beta1: [0, 10] inverse temperature at stage 1
        - beta2: [0, 10] inverse temperature at stage 2

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, eta, beta1, beta2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # rows: a1 (A,U); cols: state (X,Y)

    # Value tables
    q2 = np.zeros((2, 2))   # state x action
    q1_mf = np.zeros(2)     # first-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage values via expected max second-stage values
        max_q2 = np.max(q2, axis=1)     # best action per state
        q1_mb = T @ max_q2              # expected value per first-stage action

        # Surprise-gated arbitration
        a1 = action_1[t]
        s = state[t]
        # Determine whether the experienced transition was rare
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        rare = 0 if is_common else 1
        # MB weight increases on rare transitions, decreases on common
        w_t = 0.5 + eta * (1 if rare == 1 else -1)
        # clip to [0,1]
        if w_t < 0.0:
            w_t = 0.0
        elif w_t > 1.0:
            w_t = 1.0

        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage 1 policy and likelihood
        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy and likelihood
        a2 = action_2[t]
        q2_s = q2[s]
        q2_centered = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta2 * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Learning with asymmetric rates based on outcome
        r = reward[t]
        alpha = alpha_pos if r > 0.0 else alpha_neg

        # Update second-stage MF values
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update first-stage MF values toward outcome (TD(1))
        delta1 = r - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Planning with learned transitions and planet-based credit assignment bias.
    The agent learns second-stage rewards and the first-stage transition matrix
    with a volatility/learning parameter. Planning uses the learned transition
    model. A planet-based bias nudges first-stage choices toward spaceships
    that are more likely to reach the most recently rewarded planet.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A; 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 = first alien, 1 = second alien) in the reached state.
    reward : array-like of float (e.g., 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_r, tau_T, zeta, beta1, beta2]
        - alpha_r: [0, 1] reward learning rate for second-stage Q-values
        - tau_T: [0, 1] learning/volatility for transition probabilities
                 (higher -> faster adaptation toward observed transitions)
        - zeta: [0, 1] strength of planet-based bias added to first-stage values:
                 bias_a = zeta * (T[a, prev_rewarded_state] - 0.5), applied only if a recent
                 reward occurred; otherwise no bias.
        - beta1: [0, 10] inverse temperature at stage 1
        - beta2: [0, 10] inverse temperature at stage 2

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, tau_T, zeta, beta1, beta2 = model_parameters
    n_trials = len(action_1)

    # Initialize transition model to agnostic (0.5/0.5)
    T = np.full((2, 2), 0.5)  # rows: a1 (A,U), cols: state (X,Y)

    # Second-stage values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track most recently rewarded planet; -1 means none yet
    prev_rewarded_state = -1

    for t in range(n_trials):
        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Planet-based bias toward the last rewarded planet
        bias1 = np.zeros(2)
        if prev_rewarded_state != -1:
            # Add bias proportional to how much each spaceship tends to reach that planet
            # relative to chance (0.5).
            bias1 = zeta * (T[:, prev_rewarded_state] - 0.5)

        q1_eff = q1_mb + bias1

        # Stage 1 policy and likelihood
        a1 = action_1[t]
        q1c = q1_eff - np.max(q1_eff)
        exp_q1 = np.exp(beta1 * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy and likelihood
        s = state[t]
        a2 = action_2[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        exp_q2 = np.exp(beta2 * q2c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Reward learning at stage 2
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Transition learning: exponential recency-weighted update toward observed state
        # Move chosen row T[a1] toward one-hot on observed state s
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += tau_T * (target - T[a1, sp])
        # Normalize row to guard numerical issues
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        # Update last rewarded state marker
        prev_rewarded_state = s if r > 0.0 else -1

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Volatility-adaptive hybrid with meta-control over MB/MF arbitration.
    The agent maintains model-free values for both stages and uses the fixed
    transition structure for model-based planning. A running estimate of reward
    volatility (absolute prediction error) modulates arbitration: when volatility
    is low, the agent relies more on model-based control; when high, more on
    model-free control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A; 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 = first alien, 1 = second alien).
    reward : array-like of float (e.g., 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_r, nu_vol, w0, beta1, beta2]
        - alpha_r: [0, 1] learning rate for model-free values
        - nu_vol: [0, 1] adaptation rate for volatility estimate v_t
                  v_{t+1} = v_t + nu_vol * (|delta2_t| - v_t)
        - w0: [0, 1] baseline MB weight; actual weight w_t = clip(w0 + (0.5 - v_t), 0, 1)
        - beta1: [0, 10] inverse temperature at stage 1
        - beta2: [0, 10] inverse temperature at stage 2

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, nu_vol, w0, beta1, beta2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))  # state x action
    q1_mf = np.zeros(2)    # first-stage MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Volatility estimate initialized to medium level
    v = 0.5

    for t in range(n_trials):
        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Meta-control arbitration weight from volatility
        w_t = w0 + (0.5 - v)
        if w_t < 0.0:
            w_t = 0.0
        elif w_t > 1.0:
            w_t = 1.0

        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage 1 policy and likelihood
        a1 = action_1[t]
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy and likelihood
        s = state[t]
        a2 = action_2[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        exp_q2 = np.exp(beta2 * q2c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # First-stage MF learning toward realized second-stage action value
        # Using the realized second-stage action value as a bootstrap target
        target1 = q2[s, a2]
        q1_mf[a1] += alpha_r * (target1 - q1_mf[a1])

        # Update volatility estimate from absolute reward PE
        v += nu_vol * (abs(delta2) - v)

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik