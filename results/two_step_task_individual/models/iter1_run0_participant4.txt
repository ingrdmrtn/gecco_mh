def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Model-based planning with learned transitions, asymmetric outcome learning, and perseveration.
    Returns the negative log-likelihood of the observed first- and second-stage choices.

    Idea
    ----
    - Learn second-stage (alien) values model-free with valence-asymmetric learning rates (positive vs. negative outcomes).
    - Learn first-stage transition model online (from data) and use it for model-based evaluation at stage 1.
    - Add choice perseveration (stickiness) that biases repeating the previous action at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state visited (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien on visited planet) per trial.
    reward : array-like of float
        Reward (e.g., coins) obtained per trial.
    model_parameters : tuple/list of 5 floats
        (alpha_pos, alpha_neg, beta, alpha_tr, rho)
        - alpha_pos in [0,1]: learning rate for positive prediction errors at stage 2.
        - alpha_neg in [0,1]: learning rate for negative prediction errors at stage 2.
        - beta in [0,10]: softmax inverse temperature for both stages.
        - alpha_tr in [0,1]: learning rate for learning the transition model P(state | action_1).
        - rho in [0,1]: perseveration strength added to the previously chosen action at both stages.

    Notes
    -----
    - Transition model P is learned via a simple delta rule toward the observed state on each trial.
    - First-stage action values are model-based: Q1_MB[a] = sum_s P(s|a) * max_a2 Q2_MF[s, a2].
    - Second-stage values are learned via MF TD with asymmetric learning rates.
    - Perseveration is implemented as an additive bias (in value space prior to softmax) to the previously chosen action.
    """
    alpha_pos, alpha_neg, beta, alpha_tr, rho = model_parameters
    n_trials = len(action_1)

    # Initialize
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    # Learned transition probabilities P(state | action1)
    # Rows: action1 in {0,1}, Cols: state in {0,1}; initialize neutral (0.5, 0.5)
    P = np.full((2, 2), 0.5)

    prev_a1 = None
    prev_a2 = [None, None]  # we track previous second-stage action per state to define stickiness fairly

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first-stage Q via learned transitions
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = P @ max_q2  # shape (2,)

        # Add perseveration bias at stage 1 for the previously chosen action
        q1_pref = q1_mb.copy()
        if prev_a1 is not None:
            q1_pref[prev_a1] += rho

        # Softmax for stage 1
        q1c = q1_pref - np.max(q1_pref)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy within visited state
        s = state[t]
        q2_pref = q2[s].copy()
        if prev_a2[s] is not None:
            q2_pref[prev_a2[s]] += rho

        q2c = q2_pref - np.max(q2_pref)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD learning at stage 2 with asymmetric learning rates
        pe2 = r - q2[s, a2]
        alpha_eff = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha_eff * pe2

        # Update learned transition probabilities using a delta rule toward observed state
        # Target is a one-hot vector for the observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        P[a1] += alpha_tr * (target - P[a1])

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Model-free SARSA(λ) with choice stickiness and lapse.
    Returns the negative log-likelihood of the observed choices.

    Idea
    ----
    - Second-stage values are learned model-free; first-stage values are updated via an eligibility trace (λ) from the second-stage TD error.
    - Choice stickiness (ψ) biases repeating the previous action independently of reward.
    - A lapse parameter (ε) mixes the softmax policy with uniform random choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state visited (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien on visited planet) per trial.
    reward : array-like of float
        Reward (e.g., coins) obtained per trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, lam, psi, epsilon)
        - alpha in [0,1]: learning rate for MF updates.
        - beta in [0,10]: softmax inverse temperature (both stages).
        - lam in [0,1]: eligibility trace parameter; how much second-stage TD error updates first-stage values.
        - psi in [0,1]: choice stickiness strength added to the most recent action at each stage.
        - epsilon in [0,1]: lapse rate; with probability ε choose uniformly at random at each stage.

    Notes
    -----
    - First stage is purely MF via an eligibility trace from the second-stage TD error.
    - Stickiness is implemented as an additive bias to the previously chosen action at each stage.
    - Lapse mixes softmax with a 0.5/0.5 uniform policy.
    """
    alpha, beta, lam, psi, epsilon = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    prev_a1 = None
    prev_a2 = [None, None]

    eps = 1e-12

    for t in range(n_trials):
        # Stage 1 policy with stickiness
        q1_pref = q1.copy()
        if prev_a1 is not None:
            q1_pref[prev_a1] += psi

        q1c = q1_pref - np.max(q1_pref)
        soft1 = np.exp(beta * q1c)
        soft1 = soft1 / (np.sum(soft1) + eps)
        probs_1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with stickiness
        s = state[t]
        q2_pref = q2[s].copy()
        if prev_a2[s] is not None:
            q2_pref[prev_a2[s]] += psi

        q2c = q2_pref - np.max(q2_pref)
        soft2 = np.exp(beta * q2c)
        soft2 = soft2 / (np.sum(soft2) + eps)
        probs_2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # MF learning
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Eligibility trace update for stage 1
        q1[a1] += alpha * lam * delta2

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Reliability-gated MB/MF arbitration with learned transitions and stage-1 forgetting.
    Returns the negative log-likelihood of the observed choices.

    Idea
    ----
    - Second-stage values learned model-free.
    - First-stage transitions are learned; a dynamic arbitration weight w_t is computed from transition reliability.
      More reliable (less ambiguous) transitions increase model-based control.
    - First-stage MF values are subject to leaky forgetting toward 0 (captures non-stationarity at stage 1).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state visited (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien on visited planet) per trial.
    reward : array-like of float
        Reward (e.g., coins) obtained per trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, chi, kappa1, alpha_tr)
        - alpha in [0,1]: MF reward learning rate (both stages).
        - beta in [0,10]: softmax inverse temperature for both stages.
        - chi in [0,1]: arbitration sensitivity; higher values make MB weight grow faster with reliability.
        - kappa1 in [0,1]: forgetting rate on stage-1 MF values toward 0 each trial.
        - alpha_tr in [0,1]: learning rate for the transition model P(state | action_1).

    Notes
    -----
    - Transition reliability per action is r_a = |P(s=0|a) - 0.5| + |P(s=1|a) - 0.5| = 2*|P(s=0|a) - 0.5|, in [0,1].
      We use an overall reliability r = max_a r_a, then w_t = sigmoid( g ), with g = (4*chi)*(r - 0.25).
      This maps r around 0.25 (near-ambiguous) to w around 0.5 and increases with certainty.
    - First-stage action values are Q1 = w_t * Q1_MB + (1 - w_t) * Q1_MF.
    - Q1_MF is updated via TD from the realized second-stage MF value; it also decays by kappa1 toward 0 each trial.
    """
    alpha, beta, chi, kappa1, alpha_tr = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Learned transitions P(state | action1), initialized neutral
    P = np.full((2, 2), 0.5)

    eps = 1e-12

    for t in range(n_trials):
        # Forgetting on stage-1 MF values
        q1_mf = (1.0 - kappa1) * q1_mf

        # Compute MB Q1 via learned transitions
        max_q2 = np.max(q2_mf, axis=1)  # shape (2,)
        q1_mb = P @ max_q2

        # Reliability-gated arbitration weight w_t in [0,1]
        # reliability per action: r_a = 2*|P(s=0|a) - 0.5|
        r_actions = 2.0 * np.abs(P[:, 0] - 0.5)  # shape (2,)
        r_overall = np.max(r_actions)
        g = (4.0 * chi) * (r_overall - 0.25)
        w_t = 1.0 / (1.0 + np.exp(-g))

        # Combine Q1
        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage 1 softmax
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 softmax within visited state
        s = state[t]
        q2c = q2_mf[s] - np.max(q2_mf[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # MF learning at stage 2
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # MF learning at stage 1 from realized second-stage value
        target1 = q2_mf[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update learned transition probabilities toward the observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        P[a1] += alpha_tr * (target - P[a1])

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss