def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Asymmetric model-free learner with choice kernels (both stages).
    
    This model relies purely on model-free values at both stages, learns with
    asymmetric learning rates for positive vs. non-positive outcomes, and includes
    dynamic choice kernels (also known as choice traces) at both stages that
    bias toward recently chosen actions. Choice kernels are learned with their own
    learning rate and weighted additively in the softmax logits.
    
    Parameters (use values within bounds):
    - alpha_pos (0-1): Learning rate used when the outcome is positive (reward > 0).
    - alpha_neg (0-1): Learning rate used when the outcome is non-positive (reward <= 0).
    - beta (0-10): Inverse temperature for softmax at both stages.
    - w_ck (0-1): Weight of the choice kernel in the softmax logits.
    - kappa_ck (0-1): Learning rate for the choice kernel updates.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, spaceship chosen at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet observed at stage 2 (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, alien chosen at the observed planet.
    - reward: array-like of floats/ints, obtained outcome on each trial.
    - model_parameters: [alpha_pos, alpha_neg, beta, w_ck, kappa_ck]
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, w_ck, kappa_ck = model_parameters
    n_trials = len(action_1)
    
    # Model-free Q-values
    q_stage1_mf = np.zeros(2)       # for spaceships
    q_stage2_mf = np.zeros((2, 2))  # for aliens within each planet
    
    # Choice kernels (bias toward recently chosen actions)
    ck1 = np.zeros(2)
    ck2 = np.zeros((2, 2))  # state-dependent choice kernel
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10
    
    for t in range(n_trials):
        s = state[t]
        
        # Stage-1 policy (pure MF + choice kernel)
        logits1 = beta * q_stage1_mf + w_ck * ck1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]
        
        # Stage-2 policy (pure MF + choice kernel)
        logits2 = beta * q_stage2_mf[s] + w_ck * ck2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]
        
        r = reward[t]
        # Choose learning rate based on outcome valence
        alpha_r = alpha_pos if r > 0 else alpha_neg
        
        # Stage-2 MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * delta2
        
        # Stage-1 MF update via TD(0) toward the chosen second-stage action value
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1
        
        # Update choice kernels (exponentially decaying one-hot traces)
        # Stage 1
        ck1 *= (1.0 - kappa_ck)
        ck1[a1] += kappa_ck
        # Stage 2 (state-dependent)
        ck2[s] *= (1.0 - kappa_ck)
        ck2[s, a2] += kappa_ck
    
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-sensitive arbitration between learned model-based and model-free control.
    
    This model learns the transition structure online and arbitrates between model-based (MB)
    and model-free (MF) control at stage 1 based on the confidence in the learned transition
    probabilities. Confidence increases the MB weight when the chosen spaceship's transition
    is more certain (i.e., deviates further from 0.5). Stage-2 values are updated by MF learning.
    
    Parameters (use values within bounds):
    - alpha_q (0-1): Learning rate for Q-value updates (both stages MF).
    - beta (0-10): Inverse temperature for softmax at both stages.
    - alpha_t (0-1): Learning rate for learning transition probabilities T[a,s].
    - omega0 (0-1): Baseline arbitration weight toward MB control at stage 1.
    - xi (0-1): Sensitivity of arbitration to transition confidence (higher -> more MB when confident).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, spaceship chosen at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet observed at stage 2 (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, alien chosen at the observed planet.
    - reward: array-like of floats/ints, obtained outcome on each trial.
    - model_parameters: [alpha_q, beta, alpha_t, omega0, xi]
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha_q, beta, alpha_t, omega0, xi = model_parameters
    n_trials = len(action_1)
    
    # Initialize transition model as uncertain (rows will be normalized each step)
    T = np.full((2, 2), 0.5)
    
    # Model-free values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10
    
    # Helper for stable squashing of arbitration into [0,1]
    def clamp01(x):
        return max(0.0, min(1.0, x))
    
    for t in range(n_trials):
        s = state[t]
        
        # Model-based stage-1 values from current transition model
        max_q2 = np.max(q_stage2_mf, axis=1)  # best alien per planet
        q_stage1_mb = T @ max_q2             # expected values for spaceships
        
        # Arbitration weight depends on confidence in transition of each action.
        # Confidence of an action a is c[a] = |T[a,0] - 0.5| + |T[a,1] - 0.5|, 
        # but since rows sum to 1, this reduces to 2*|T[a,0]-0.5|.
        conf = 2.0 * np.abs(T[:, 0] - 0.5)  # in [0,1]
        # Convert baseline omega0 and confidence to a per-action arbitration
        omega = clamp01(omega0 + xi * (conf - np.mean(conf)))
        
        # Blend MB and MF for stage 1
        q1 = omega * q_stage1_mb + (1.0 - omega) * q_stage1_mf
        
        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]
        
        # Stage-2 policy (MF)
        logits2 = beta * q_stage2_mf[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]
        
        r = reward[t]
        
        # Learn transition model T for the chosen spaceship using delta rule
        target = np.array([1.0 if sp == s else 0.0 for sp in (0, 1)])
        T[a1] += alpha_t * (target - T[a1])
        # Normalize to guard against drift
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum
        
        # MF learning at stage 2
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_q * delta2
        
        # MF learning at stage 1: bootstrap from current second-stage value
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_q * delta1
    
    # Ensure rows are normalized (not necessary for NLL but for consistency)
    T /= (np.sum(T, axis=1, keepdims=True) + eps)
    
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive MF learner with stage-specific temperatures and model-based transition bias.
    
    This model is primarily model-free, but incorporates a model-based transition bias
    at stage 1 that favors the spaceship expected to reach the planet with higher
    current value. Rewards undergo a risk-sensitive utility transformation.
    
    Parameters (use values within bounds):
    - alpha (0-1): Learning rate for MF Q-value updates (both stages).
    - beta1 (0-10): Inverse temperature for stage-1 softmax.
    - beta2 (0-10): Inverse temperature for stage-2 softmax.
    - chi (0-1): Strength of model-based transition bias added to stage-1 logits.
    - rho (0-1): Risk-sensitivity for utility transformation u(r) = sign(r)*|r|^rho.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, spaceship chosen at stage 1 (0=A, 1=U).
    - state: array-like of ints in {0,1}, planet observed at stage 2 (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, alien chosen at the observed planet.
    - reward: array-like of floats/ints, obtained outcome on each trial.
    - model_parameters: [alpha, beta1, beta2, chi, rho]
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta1, beta2, chi, rho = model_parameters
    n_trials = len(action_1)
    
    # Fixed transition structure of the task (common transitions)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])
    
    # Model-free Q-values
    q_stage1_mf = np.zeros(2)       # spaceships
    q_stage2_mf = np.zeros((2, 2))  # aliens within each planet
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10
    
    for t in range(n_trials):
        s = state[t]
        
        # Compute MB transition-based bias toward higher-value planet
        max_q2 = np.max(q_stage2_mf, axis=1)  # best alien per planet
        expected_planet_value = T_fixed @ max_q2  # expected value per spaceship
        
        # Center the bias to keep logits well-scaled
        bias1 = chi * (expected_planet_value - np.mean(expected_planet_value))
        
        # Stage-1 policy: MF values plus MB transition bias
        logits1 = beta1 * q_stage1_mf + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]
        
        # Stage-2 policy: MF with its own temperature
        logits2 = beta2 * q_stage2_mf[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]
        
        # Risk-sensitive utility transformation
        r = reward[t]
        u = np.sign(r) * (np.abs(r) ** rho)
        
        # Stage-2 MF update
        delta2 = u - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2
        
        # Stage-1 MF update toward current second-stage value (bootstrapping)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1
    
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss