def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid RL with learned transition model and stage-1 stickiness.

    The agent learns second-stage (alien) values model-free, learns the A→planet
    and U→planet transition probabilities over time from experience, and blends
    model-based (via the learned transition model) and model-free values at
    stage 1. A stage-1 perseveration bias favors repeating the previous first-stage
    choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : list or array, length=5
        [alpha, beta, w, eta_T, stick]
        - alpha in [0,1]: learning rate for MF value updates.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: weight on model-based plan at stage 1 (1=fully MB).
        - eta_T in [0,1]: learning rate for updating the transition model.
        - stick in [0,1]: stage-1 perseveration strength.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w, eta_T, stick = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model T[a, s]: P(s | a)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # start with a reasonable prior but learn from data

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)                # first stage MF values
    q2_mf = np.ones((2, 2)) * 0.5      # second stage MF values (start at 0.5)

    last_a1 = None

    for t in range(n_trials):
        # Stage-1 evaluation: MB via learned transitions + MF blend
        max_q2 = np.max(q2_mf, axis=1)        # best alien per planet
        q1_mb = T @ max_q2                    # MB values from learned transitions
        q1_base = (1 - w) * q1_mf + w * q1_mb

        # Add stage-1 perseveration bias
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stick
        q1_eff = q1_base + bias1

        # Stage-1 policy
        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Observe state and choose second-stage action
        s = int(state[t])

        # Stage-2 policy (pure MF)
        q2_eff = q2_mf[s].copy()
        q2c = q2_eff - np.max(q2_eff)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: update transition model T using observed transition (a1 -> s)
        # Simple delta rule towards one-hot observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1 - eta_T) * T[a1] + eta_T * target
        # Ensure numerical stability (renormalize)
        T[a1] = T[a1] / np.sum(T[a1])

        # Stage-2 MF update
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF bootstrap update toward observed stage-2 chosen value
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        last_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based stage-1 with asymmetric learning and decay at stage-2, and separate betas.

    The agent evaluates first-stage actions using a fixed transition model (common=0.7),
    planning to the best alien on each planet (pure MB at stage 1). At stage 2, it learns
    alien values with asymmetric learning rates for positive vs negative outcomes and
    applies a per-trial decay toward 0.5 for all second-stage Q-values to capture forgetting.
    Choice stochasticity is governed by separate inverse temperatures for stage 1 and stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : list or array, length=5
        [alpha_pos, alpha_neg, beta1, beta2, decay]
        - alpha_pos in [0,1]: stage-2 learning rate after reward=1.
        - alpha_neg in [0,1]: stage-2 learning rate after reward=0.
        - beta1 in [0,10]: inverse temperature for stage-1 softmax.
        - beta2 in [0,10]: inverse temperature for stage-2 softmax.
        - decay in [0,1]: per-trial forgetting toward 0.5 for all stage-2 Q-values.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta1, beta2, decay = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values (aliens), initialized to 0.5
    q2 = np.ones((2, 2)) * 0.5

    for t in range(n_trials):
        # Apply forgetting/decay toward 0.5 to all aliens before choosing
        q2 = (1 - decay) * q2 + decay * 0.5

        # Stage-1 evaluation: pure MB planning with fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta1 * q1c)
        probs_1 /= np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Observe second-stage state, choose alien
        s = int(state[t])
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta2 * q2c)
        probs_2 /= np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2 with asymmetric alphas
        r = reward[t]
        alpha = alpha_pos if r > q2[s, a2] else alpha_neg
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Feature-augmented MF with model-based stay/switch bias and eligibility.

    The agent learns MF values at both stages, but the stage-1 policy includes
    two choice-bias features:
      - a perseveration bias to repeat the previous first-stage action,
      - a model-based stay/switch bias that depends on the previous trial's reward
        and whether the previous transition was common vs rare, favoring the action
        that would commonly lead back to the previously rewarded state.

    Learning uses MF bootstrapping from stage 2 and an eligibility trace that
    propagates stage-2 prediction errors back to stage 1.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : list or array, length=5
        [alpha, beta, pi, chi, gamma]
        - alpha in [0,1]: learning rate for MF Q updates.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - pi in [0,1]: perseveration strength (stage-1).
        - chi in [0,1]: model-based stay/switch bias strength (stage-1).
        - gamma in [0,1]: eligibility trace strength applied to stage-1 Q.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, pi, chi, gamma = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2_mf = np.ones((2, 2)) * 0.5

    # Track previous trial info for bias construction
    last_a1 = None
    last_s = None
    last_r = None

    for t in range(n_trials):
        # Base MF values at stage 1
        q1_base = q1_mf.copy()

        # Construct bias features for stage 1
        bias1 = np.zeros(2)

        # Perseveration toward repeating last first-stage action
        if last_a1 is not None:
            bias1[last_a1] += pi

        # Model-based stay/switch bias from previous trial
        # If last trial existed, compute which action would commonly lead to last_s
        if last_a1 is not None and last_s is not None and last_r is not None:
            # Which first-stage action is more likely to go to last_s?
            # For each action a, likelihood of reaching last_s is transition_matrix[a, last_s]
            common_pref = np.array([transition_matrix[0, last_s],
                                    transition_matrix[1, last_s]])
            # Preferred action index (the one that commonly leads to last_s)
            a_pref = int(np.argmax(common_pref))
            # If last trial was rewarded, bias toward a_pref; if unrewarded, bias away
            if last_r > 0:
                bias1[a_pref] += chi
            else:
                bias1[1 - a_pref] += chi

        q1_eff = q1_base + bias1

        # Stage-1 policy
        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 /= np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Observe state and choose second-stage action
        s = int(state[t])

        # Stage-2 policy (MF)
        q2s = q2_mf[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 /= np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 MF update
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF bootstrap toward chosen stage-2 value
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Eligibility trace: propagate some of stage-2 PE back to stage-1
        q1_mf[a1] += alpha * gamma * delta2

        # Store previous trial info
        last_a1 = a1
        last_s = s
        last_r = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss