def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with asymmetric learning and transition-contingent bias.
    
    This model blends model-based (MB) planning (using the known transition
    structure) with a model-free (MF) learner, but uses asymmetric learning
    rates for positive vs. negative prediction errors at both stages. In
    addition, it implements a transition-contingent first-stage bias: the
    previous first-stage action is favored after a common transition and
    disfavored after a rare transition.
    
    Parameters (model_parameters):
    - alpha_pos: [0,1]
        Learning rate for positive prediction errors (both stages).
    - alpha_neg: [0,1]
        Learning rate for negative prediction errors (both stages).
    - beta: [0,10]
        Inverse temperature for softmax at both stages.
    - mb_weight: [0,1]
        Mixture weight of MB values at stage 1; (1 - mb_weight) weights MF values.
    - psi: [0,1]
        Transition-contingent bias magnitude added to the previous first-stage
        action's logit: +psi after a common transition, -psi after a rare transition.
    
    Inputs:
    - action_1: array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    - state: array-like of int {0,1}
        Second-stage state per trial (0=X, 1=Y).
    - action_2: array-like of int {0,1}
        Second-stage choices per trial (0 or 1).
    - reward: array-like of float {0,1}
        Reward outcome per trial.
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, mb_weight, psi = model_parameters
    n_trials = len(action_1)

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)       # MF Q for first-stage actions
    q_stage2 = np.zeros((2, 2))     # Second-stage Q-values: state x action

    # Track last transition type to set bias
    last_a1 = None
    last_common = None

    for t in range(n_trials):
        # MB action values at stage 1 from current second-stage values
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Transition-contingent bias on stage-1 logits
        bias1 = np.zeros(2)
        if last_a1 is not None and last_common is not None:
            sign = 1.0 if last_common else -1.0
            bias1[last_a1] += psi * sign

        # Softmax policy at stage 1
        logits1 = mb_weight * q_stage1_mb + (1.0 - mb_weight) * q_stage1_mf + bias1
        logits1 = logits1 - np.max(logits1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Softmax policy at stage 2 (state-dependent)
        s = state[t]
        logits2 = q_stage2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe outcome and update values
        r = reward[t]
        q2_old = q_stage2[s, a2]

        # Stage-2 update with asymmetric learning rate
        delta2 = r - q2_old
        a2_lr = alpha_pos if delta2 >= 0.0 else alpha_neg
        q_stage2[s, a2] += a2_lr * delta2

        # Stage-1 MF update (bootstrapped) with asymmetric learning rate
        delta1 = q2_old - q_stage1_mf[a1]
        a1_lr = alpha_pos if delta1 >= 0.0 else alpha_neg
        q_stage1_mf[a1] += a1_lr * delta1

        # Update memory for next trial bias: determine if current transition was common
        # Common if (A->X) or (U->Y)
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        last_a1 = a1
        last_common = is_common

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planning with learned transitions, value forgetting, and stickiness.
    
    This model learns the transition structure online and plans using the
    learned transition matrix and second-stage values. Second-stage values
    undergo controlled forgetting toward a neutral baseline, capturing
    nonstationarity or memory decay. A choice stickiness term biases repeat
    choices at both stages.
    
    Parameters (model_parameters):
    - alpha: [0,1]
        Learning rate for second-stage value updates.
    - beta: [0,10]
        Inverse temperature for softmax at both stages.
    - tau: [0,1]
        Transition learning rate for updating the transition matrix rows toward
        the observed next state (row-wise, per chosen first-stage action).
    - f_forget: [0,1]
        Forgetting rate pulling second-stage Q-values toward 0.5 each trial
        before learning (f=0 no forgetting; f=1 full reset to 0.5).
    - kappa: [0,1]
        Choice stickiness magnitude added to the previously chosen action's
        logit at the corresponding stage (state-dependent at stage 2).
    
    Inputs:
    - action_1: array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    - state: array-like of int {0,1}
        Second-stage state per trial (0=X, 1=Y).
    - action_2: array-like of int {0,1}
        Second-stage choices per trial (0 or 1).
    - reward: array-like of float {0,1}
        Reward outcome per trial.
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, tau, f_forget, kappa = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix to uniform (agnostic)
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage Q-values; forgetting pulls toward 0.5
    q_stage2 = np.full((2, 2), 0.5)

    # Stickiness trackers
    prev_a1 = np.zeros(2)
    prev_a2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Compute MB first-stage values from learned transitions
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = T @ max_q_stage2

        # Stage-1 softmax with stickiness
        logits1 = q_stage1_mb + kappa * prev_a1
        logits1 = logits1 - np.max(logits1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax with state- and action-specific stickiness
        s = state[t]
        logits2 = q_stage2[s].copy() + kappa * prev_a2[s]
        logits2 = logits2 - np.max(logits2)
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Apply forgetting toward 0.5 prior to learning
        q_stage2 = (1.0 - f_forget) * q_stage2 + f_forget * 0.5

        # Update second-stage value
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Update learned transitions for the chosen first-stage action using a delta rule
        # Move row a1 toward one-hot indicating observed state s
        target = np.zeros(2)
        target[s] = 1.0
        T[a1] = (1.0 - tau) * T[a1] + tau * target
        # Renormalize row to be safe (should already sum to 1)
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum
        else:
            T[a1] = np.array([0.5, 0.5])

        # Update stickiness trackers
        prev_a1 = np.zeros(2)
        prev_a1[a1] = 1.0
        prev_a2[s] = np.zeros(2)
        prev_a2[s, a2] = 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free TD with transition-dependent credit assignment and dual temperatures.
    
    A purely model-free learner updates first-stage values from the second-stage
    values (bootstrapping) and additionally credits the first-stage chosen action
    with a fraction of the second-stage reward prediction error (RPE). The credit
    fraction depends on whether the experienced transition was common or rare.
    Separate inverse temperatures govern choice stochasticity at each stage.
    A single choice stickiness term biases repetition at both stages.
    
    Parameters (model_parameters):
    - alpha: [0,1]
        Learning rate for both stages (TD updates).
    - beta1: [0,10]
        Inverse temperature for first-stage softmax.
    - beta2: [0,10]
        Inverse temperature for second-stage softmax.
    - xi: [0,1]
        Transition-dependent credit assignment for the second-stage RPE to the
        first-stage chosen action: credit = xi after a common transition and
        credit = (1 - xi) after a rare transition.
    - theta: [0,1]
        Choice stickiness magnitude added to the previously chosen action's logit
        at each stage (state-dependent at stage 2).
    
    Inputs:
    - action_1: array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    - state: array-like of int {0,1}
        Second-stage state per trial (0=X, 1=Y).
    - action_2: array-like of int {0,1}
        Second-stage choices per trial (0 or 1).
    - reward: array-like of float {0,1}
        Reward outcome per trial.
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta1, beta2, xi, theta = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1 = np.zeros(2)          # first-stage MF values
    q_stage2 = np.zeros((2, 2))     # second-stage MF values

    # Stickiness trackers
    prev_a1 = np.zeros(2)
    prev_a2 = np.zeros((2, 2))

    for t in range(n_trials):
        # First-stage policy with stickiness
        logits1 = q_stage1 + theta * prev_a1
        logits1 = logits1 - np.max(logits1)
        probs_1 = np.exp(beta1 * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with stickiness
        s = state[t]
        logits2 = q_stage2[s].copy() + theta * prev_a2[s]
        logits2 = logits2 - np.max(logits2)
        probs_2 = np.exp(beta2 * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and compute TD errors
        r = reward[t]
        q2_old = q_stage2[s, a2]
        delta2 = r - q2_old

        # Update second-stage value
        q_stage2[s, a2] += alpha * delta2

        # Bootstrapped update for first-stage MF value
        delta1 = q2_old - q_stage1[a1]

        # Transition-dependent credit assignment for RPE back to stage 1
        # Common: A->X or U->Y
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        credit = xi if is_common else (1.0 - xi)

        q_stage1[a1] += alpha * (delta1 + credit * delta2)

        # Update stickiness trackers
        prev_a1 = np.zeros(2)
        prev_a1[a1] = 1.0
        prev_a2[s] = np.zeros(2)
        prev_a2[s, a2] = 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll