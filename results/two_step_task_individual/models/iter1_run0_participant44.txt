def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model with learned transitions and rarity-contingent perseveration.
    
    This model learns both second-stage model-free values and the first-stage
    transition model online. The first-stage policy blends model-based (via the
    learned transition matrix) and model-free values. Additionally, a rarity-
    contingent perseveration bias is applied based on the previous trial’s
    transition: after a common transition, the model is biased to repeat the
    previous first-stage choice; after a rare transition, it is biased to switch.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage action within the reached state: 0/1 are the two aliens.
    reward : array-like of float (e.g., 0 or 1)
        Obtained reward on each trial.
    model_parameters : list or array-like of 5 floats
        [alpha_Q, beta, omega_MB, alpha_T, zeta]
        - alpha_Q in [0,1]: Learning rate for model-free Q updates at both stages.
        - beta in [0,10]: Inverse temperature for both stages' softmax policies.
        - omega_MB in [0,1]: Weight on model-based Q at stage 1 (1-ω on model-free).
        - alpha_T in [0,1]: Learning rate for the transition matrix P(state | action_1).
        - zeta in [0,1]: Strength of rarity-contingent perseveration bias at stage 1.
                         Applied to repeat after common, to switch after rare.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_Q, beta, omega_MB, alpha_T, zeta = model_parameters
    n_trials = len(action_1)

    # Initialize a learned transition matrix; start uninformative (0.5/0.5)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Model-free Q-values
    Q1_MF = np.zeros(2)
    Q2_MF = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_common = True  # arbitrary init; used only from second trial onward

    for t in range(n_trials):
        s = state[t]

        # Model-based first-stage values via learned transitions and MF second-stage values
        max_Q2 = np.max(Q2_MF, axis=1)           # best attainable value per state
        Q1_MB = T @ max_Q2                        # expectation over next states

        # Blend MB and MF for stage 1
        Q1_blend = omega_MB * Q1_MB + (1.0 - omega_MB) * Q1_MF

        # Rarity-contingent perseveration bias based on previous trial's transition
        bias = np.zeros(2)
        if prev_a1 >= 0:
            if prev_common:
                bias[prev_a1] += zeta
            else:
                bias[1 - prev_a1] += zeta

        # Stage-1 policy
        q1_eff = Q1_blend + bias
        q1_eff = q1_eff - np.max(q1_eff)
        exp_q1 = np.exp(beta * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (pure MF)
        q2_eff = Q2_MF[s, :].copy()
        q2_eff = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # MF updates
        delta2 = r - Q2_MF[s, a2]
        Q2_MF[s, a2] += alpha_Q * delta2

        bootstrap = Q2_MF[s, a2]
        delta1 = bootstrap - Q1_MF[a1]
        Q1_MF[a1] += alpha_Q * delta1

        # Update learned transition matrix based on observed state given action a1
        # Binary state; keep row normalized by complementary assignment
        T[a1, 0] += alpha_T * ((1 if s == 0 else 0) - T[a1, 0])
        T[a1, 1] = 1.0 - T[a1, 0]

        # Determine whether the observed transition was common or rare under current T
        prev_a1 = a1
        prev_common = (s == int(np.argmax(T[a1])))

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free controller with asymmetric learning and dual perseveration.
    
    This model uses TD learning at both stages with separate learning rates for
    positive vs. negative prediction errors. It includes independent perseveration
    biases at the first and second stages that favor repeating the most recent
    action at each respective stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage action within the reached state: 0/1 are the two aliens.
    reward : array-like of float (e.g., 0 or 1)
        Obtained reward on each trial.
    model_parameters : list or array-like of 5 floats
        [alpha_pos, alpha_neg, beta, kappa1, kappa2]
        - alpha_pos in [0,1]: Learning rate when TD error is positive (both stages).
        - alpha_neg in [0,1]: Learning rate when TD error is negative (both stages).
        - beta in [0,10]: Inverse temperature for both stages' softmax policies.
        - kappa1 in [0,1]: First-stage perseveration strength (repeat previous a1).
        - kappa2 in [0,1]: Second-stage perseveration strength (repeat previous a2).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, kappa1, kappa2 = model_parameters
    n_trials = len(action_1)

    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        s = state[t]

        # First-stage action selection with perseveration bias
        q1_eff = Q1.copy()
        if prev_a1 >= 0:
            q1_eff[prev_a1] += kappa1
        q1_eff = q1_eff - np.max(q1_eff)
        exp_q1 = np.exp(beta * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage action selection with perseveration bias
        q2_eff = Q2[s, :].copy()
        if prev_a2 >= 0:
            q2_eff[prev_a2] += kappa2
        q2_eff = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update
        r = reward[t]

        # Second-stage update with asymmetric learning rates
        delta2 = r - Q2[s, a2]
        alpha2 = alpha_pos if delta2 >= 0 else alpha_neg
        Q2[s, a2] += alpha2 * delta2

        # First-stage update bootstrapping on the chosen second-stage value
        bootstrap = Q2[s, a2]
        delta1 = bootstrap - Q1[a1]
        alpha1 = alpha_pos if delta1 >= 0 else alpha_neg
        Q1[a1] += alpha1 * delta1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with novelty bonus at stage 2, MF trace, and lapse.
    
    The first-stage policy is model-based using the known transition structure,
    but it plans over novelty-augmented second-stage values. A model-free (MF)
    learner updates both stages and propagates second-stage TD error to the
    first stage via an eligibility-like trace. Choices at both stages include a
    lapse probability that mixes the softmax policy with uniform random choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage action within the reached state: 0/1 are the two aliens.
    reward : array-like of float (e.g., 0 or 1)
        Obtained reward on each trial.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, nu, trace, epsilon]
        - alpha in [0,1]: Learning rate for MF Q updates at both stages.
        - beta in [0,10]: Inverse temperature for softmax policies.
        - nu in [0,1]: Novelty bonus weight added as nu / sqrt(N[state, action_2]).
        - trace in [0,1]: Strength of propagating stage-2 TD error to stage-1 MF value.
        - epsilon in [0,1]: Lapse probability; with prob epsilon choose uniformly at random.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, nu, trace, epsilon = model_parameters
    n_trials = len(action_1)

    # Known transition matrix of the task (common = 0.7)
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # MF Q-values
    Q1_MF = np.zeros(2)
    Q2_MF = np.zeros((2, 2))

    # Novelty counts for second-stage actions
    N = np.ones((2, 2), dtype=float)  # start at 1 to avoid division by zero

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]

        # Compute novelty bonuses for stage 2
        bonus = nu / np.sqrt(N)  # shape (2,2)

        # Model-based first-stage values planning over (Q2 + bonus)
        max_aug = np.max(Q2_MF + bonus, axis=1)   # best novelty-augmented value per state
        Q1_MB = T_known @ max_aug

        # Stage-1 policy (MB only) with lapse
        q1_eff = Q1_MB - np.max(Q1_MB)
        exp_q1 = np.exp(beta * q1_eff)
        soft_1 = exp_q1 / np.sum(exp_q1)
        probs_1 = (1.0 - epsilon) * soft_1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy uses MF values plus novelty bonus with lapse
        q2_eff = (Q2_MF[s, :] + bonus[s, :]).copy()
        q2_eff = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2_eff)
        soft_2 = exp_q2 / np.sum(exp_q2)
        probs_2 = (1.0 - epsilon) * soft_2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # MF updates
        delta2 = r - Q2_MF[s, a2]
        Q2_MF[s, a2] += alpha * delta2

        bootstrap = Q2_MF[s, a2]
        delta1 = bootstrap - Q1_MF[a1]
        Q1_MF[a1] += alpha * delta1 + alpha * trace * delta2

        # Update novelty counts after acting at stage 2
        N[s, a2] += 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll