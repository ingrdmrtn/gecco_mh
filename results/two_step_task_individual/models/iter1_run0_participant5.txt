def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Model-based planning with learned transitions, reward sensitivity, and stage-2 perseveration.
    
    This model learns the first-stage transition probabilities online and performs pure
    model-based planning at stage 1 using those learned transitions. Stage 2 values are
    learned model-free. Rewards are transformed by a reward-sensitivity parameter that
    shrinks outcomes toward a neutral baseline. A stage-2 perseveration bias promotes
    repeating the last alien choice within each planet.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0 = planet X, 1 = planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) per trial (the alien chosen on that planet).
    reward : array-like of float (0 or 1)
        Binary rewards obtained at stage 2 per trial.
    model_parameters : iterable of floats
        [alpha, beta, tau, eta, kappa2]
        - alpha in [0,1]: learning rate for model-free Q updates at stage 2.
        - beta in [0,10]: inverse temperature for both stages.
        - tau in [0,1]: transition learning rate (updates p(planet | spaceship)).
        - eta in [0,1]: reward sensitivity; effective reward is eta*r + (1-eta)*0.5.
        - kappa2 in [0,1]: stage-2 perseveration bias to repeat the last alien on the visited planet.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, tau, eta, kappa2 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T[a1, s] with non-informative priors
    T = np.full((2, 2), 0.5)

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free second-stage action values Q2[s, a2]
    q2 = np.zeros((2, 2))

    # Stage-2 perseveration memory: last chosen alien per planet
    last_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        # Compute model-based first-stage values using learned transitions and current Q2
        V_states = np.max(q2, axis=1)  # best alien value on each planet
        q1_mb = T @ V_states           # expected value of each spaceship

        # Stage-1 choice policy (pure MB)
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice policy with perseveration within the visited planet
        s = state[t]
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += kappa2
        logits2 = beta * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward with sensitivity transform
        r = reward[t]
        r_eff = eta * r + (1.0 - eta) * 0.5

        # Update second-stage Q-values (model-free)
        delta2 = r_eff - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update learned transition model for the chosen spaceship using simple delta rule
        # Move row T[a1,:] toward one-hot on observed state s
        T[a1, :] = (1.0 - tau) * T[a1, :]
        T[a1, s] += tau
        # Normalize to avoid drift due to numerical errors
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

        # Update perseveration memory
        last_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Model-free SARSA with dual learning rates and value-free choice kernels at both stages.
    
    This model captures habitual tendencies via recency-based choice kernels that are
    combined with model-free Q-values. Choice kernels decay over time and increase for
    the chosen action, independently of rewards. Learning uses asymmetric learning rates
    for positive vs. negative outcomes at stage 2 and propagates to stage 1 via a SARSA target.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcomes.
    model_parameters : iterable of floats
        [alpha_pos, alpha_neg, beta, kernel_decay, kernel_gain]
        - alpha_pos in [0,1]: learning rate when reward = 1.
        - alpha_neg in [0,1]: learning rate when reward = 0.
        - beta in [0,10]: inverse temperature for both stages.
        - kernel_decay in [0,1]: decay rate for the choice kernels each trial.
        - kernel_gain in [0,1]: weight multiplying the choice kernel in decision values.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, kernel_decay, kernel_gain = model_parameters
    n_trials = len(action_1)

    # Model-free Q-values
    q1 = np.zeros(2)        # stage-1 Q for [A, U]
    q2 = np.zeros((2, 2))   # stage-2 Q for states X/Y and two aliens

    # Value-free choice kernels (recency biases)
    k1 = np.zeros(2)        # stage-1 kernel
    k2 = np.zeros((2, 2))   # stage-2 kernel per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage-1 policy: softmax over Q + kernel term
        logits1 = beta * (q1 + kernel_gain * k1)
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        logits2 = beta * (q2[s, :] + kernel_gain * k2[s, :])
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]
        alpha = alpha_pos if r > 0.5 else alpha_neg

        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 SARSA bootstrap target (post-update q2)
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

        # Update choice kernels with decay, then increment chosen actions
        k1 *= (1.0 - kernel_decay)
        k2 *= (1.0 - kernel_decay)
        k1[a1] += 1.0
        k2[s, a2] += 1.0

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: MB-MF arbitration with uncertainty bonus (UCB-like) and value forgetting.
    
    Stage-2 action values are learned model-free with a global forgetting force toward 0.5.
    An uncertainty bonus encourages exploration at stage 2 using the inverse square-root
    of visit counts. The model-based planner at stage 1 evaluates spaceships using the
    known transition matrix applied to uncertainty-augmented second-stage values. Stage 1
    integrates model-based and model-free values via a fixed arbitration weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcomes.
    model_parameters : iterable of floats
        [alpha, beta, omega, decay, zeta]
        - alpha in [0,1]: learning rate for model-free Q updates.
        - beta in [0,10]: inverse temperature for both stages.
        - omega in [0,1]: weight on model-based value at stage 1 (1=fully MB, 0=fully MF).
        - decay in [0,1]: forgetting strength pulling unvisited Q2 toward 0.5 each trial.
        - zeta in [0,1]: uncertainty bonus weight added to Q2 using 1/sqrt(visit_count).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, omega, decay, zeta = model_parameters
    n_trials = len(action_1)

    # Known transition matrix (common/rare structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Model-free values
    q1_mf = np.zeros(2)       # stage-1 MF Q
    q2 = np.full((2, 2), 0.5) # initialize around neutral due to forgetting dynamics

    # Visit counts for uncertainty bonus
    counts = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Apply global forgetting to Q2 toward 0.5
        q2 = (1.0 - decay) * q2 + decay * 0.5

        # Compute uncertainty bonuses u[s,a] = 1/sqrt(n+1)
        u = 1.0 / np.sqrt(counts + 1.0)

        # Stage-2 policy: softmax over Q2 + uncertainty bonus
        s = state[t]
        logits2 = beta * (q2[s, :] + zeta * u[s, :])
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy uses MB+MF arbitration
        # State values incorporate uncertainty bonus as in the second stage
        V_states = np.max(q2 + zeta * u, axis=1)
        q1_mb = T @ V_states
        q1_net = (1.0 - omega) * q1_mf + omega * q1_mb
        logits1 = beta * q1_net
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Observe reward and update
        r = reward[t]

        # Update counts for uncertainty after the choice
        counts[s, a2] += 1.0

        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF SARSA update using post-update Q2
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss