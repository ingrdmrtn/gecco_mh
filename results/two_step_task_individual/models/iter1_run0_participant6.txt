def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with surprise-gated arbitration, learned transitions, and eligibility trace.

    Idea
    ----
    This model blends model-free (MF) and model-based (MB) action values at the first stage.
    The arbitration weight is dynamic and depends on recent transition surprise: after rare/
    surprising transitions, the model leans more on MB control. Transition probabilities are
    learned online. A stage-2 MF learner updates values from rewards, and an eligibility
    trace propagates outcome PEs back to stage 1.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens within a state).
    reward : array-like of float
        Outcome per trial (coins; can be negative, zero, or positive).
    model_parameters : tuple/list of 5 floats
        - alpha (0 to 1): learning rate for MF Q-values at both stages.
        - beta (0 to 10): inverse temperature for softmax (both stages).
        - eta_t (0 to 1): learning rate for updating first-stage transition probabilities.
        - lam_e (0 to 1): eligibility trace strength to propagate second-stage PE to stage-1 MF.
        - phi (0 to 1): surprise sensitivity controlling the MB weight adaptation.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, eta_t, lam_e, phi = model_parameters
    n_trials = len(action_1)

    # Probabilities of choosing observed actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)         # first-stage MF values over {A, U}
    q2_mf = np.zeros((2, 2))    # second-stage MF values: states {X, Y} x actions {0,1}

    # Learned transition model T[a, s] = P(state=s | action=a)
    T = np.full((2, 2), 0.5)    # start neutral, learn over time

    # Dynamic arbitration weight w (MB weight) updated from previous trial surprise
    w = 0.5

    for t in range(n_trials):

        # Compute MB first-stage values from current transition model and second-stage MF values
        v_state = np.max(q2_mf, axis=1)           # value of each state (best alien on that planet)
        q1_mb = T @ v_state                       # model-based first-stage value

        # Hybrid action values for first-stage choice
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf

        # First-stage policy
        q1p = q1_hybrid
        q1p = q1p - np.max(q1p)
        probs_1 = np.exp(beta * q1p)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (pure MF at stage 2)
        s = state[t]
        q2p = q2_mf[s, :]
        q2p = q2p - np.max(q2p)
        probs_2 = np.exp(beta * q2p)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and prediction errors
        r = reward[t]
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF TD using bootstrapped state value
        delta1 = np.max(q2_mf[s, :]) - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Eligibility trace: propagate outcome PE directly to first-stage MF
        q1_mf[a1] += alpha * lam_e * delta2

        # Transition model update and surprise-based arbitration weight for next trial
        # Compute surprise with respect to pre-update T
        t_expected = T[a1, s]
        surprise = 1.0 - t_expected  # in [0,1], higher if state was unlikely under current T

        # Update transition probabilities toward the observed state
        T[a1, :] = (1.0 - eta_t) * T[a1, :]
        T[a1, s] += eta_t

        # Update MB weight for next trial via a sigmoid of surprise centered at 0.5
        # c controls slope and is scaled by phi in [0,1]
        c = 8.0 * phi
        w = 1.0 / (1.0 + np.exp(-c * (surprise - 0.5)))

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Dual-temperature model-free with choice-kernel perseveration and value forgetting.

    Idea
    ----
    This model is purely model-free but captures rich choice patterns via:
    - Separate inverse temperatures at stage 1 and stage 2.
    - A choice-kernel (habit) at both stages that learns what was chosen and biases repetition.
    - Global forgetting of Q-values.
    - Single parameter serves both as eligibility strength to pass outcome PE to stage 1 and as
      the forgetting rate, tying recency across value and policy components.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens within a state).
    reward : array-like of float
        Outcome per trial (coins; can be negative, zero, or positive).
    model_parameters : tuple/list of 5 floats
        - alpha (0 to 1): learning rate for MF Q-values.
        - beta1 (0 to 10): inverse temperature for first-stage softmax.
        - beta2 (0 to 10): inverse temperature for second-stage softmax.
        - zeta (0 to 1): global recency parameter:
              * forgetting factor for Q-values (larger -> more forgetting),
              * eligibility scaling to propagate outcome PE from stage 2 to stage 1.
        - psi (0 to 1): choice-kernel strength:
              * learning rate for updating choice kernels,
              * scale of their influence on choice (bias magnitude).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta1, beta2, zeta, psi = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels (habitual bias toward previously chosen actions)
    k1 = np.zeros(2)
    k2 = np.zeros((2, 2))

    # Forgetting factor (applied each trial before acting)
    forget = 1.0 - 0.5 * zeta  # in [0.5, 1], larger zeta -> stronger decay

    for t in range(n_trials):

        # Apply forgetting to values and mild decay to kernels to keep them bounded
        q1 *= forget
        q2 *= forget
        k1 *= (1.0 - 0.5 * psi)
        k2 *= (1.0 - 0.5 * psi)

        # First-stage policy: Q1 plus scaled choice-kernel bias
        q1_pol = q1 + psi * k1
        q1s = q1_pol - np.max(q1_pol)
        probs_1 = np.exp(beta1 * q1s)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy: state-conditional Q2 plus scaled choice-kernel bias
        s = state[t]
        q2_pol = q2[s, :] + psi * k2[s, :]
        q2s = q2_pol - np.max(q2_pol)
        probs_2 = np.exp(beta2 * q2s)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward-based MF learning
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 TD plus eligibility-like propagation of outcome PE, scaled by zeta
        delta1 = np.max(q2[s, :]) - q1[a1]
        q1[a1] += alpha * (delta1 + zeta * delta2)

        # Update choice kernels toward chosen actions (within each relevant choice set)
        # Stage 1 kernel
        k1 *= (1.0 - psi)
        k1[a1] += psi
        # Stage 2 kernel (only for the encountered state)
        k2[s, :] *= (1.0 - psi)
        k2[s, a2] += psi

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor-style MB at stage 1 with nonlinear utility and UCB exploration at stage 2.

    Idea
    ----
    - Stage 1 uses a learned one-step successor representation (i.e., the transition model)
      to compute MB action values from the expected values of second-stage states.
    - Stage 2 uses MF learning of action values but augments choice with an uncertainty
      bonus (UCB-like), promoting exploration early in less-visited options.
    - Outcomes are transformed by a concave power utility to capture risk/utility curvature.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens within a state).
    reward : array-like of float
        Outcome per trial (coins; can be negative, zero, or positive).
    model_parameters : tuple/list of 5 floats
        - alpha (0 to 1): learning rate for MF Q-values at stage 2 and bootstrapped update at stage 1.
        - beta (0 to 10): inverse temperature for softmax (both stages).
        - eta_t (0 to 1): learning rate for updating the transition/SR matrix.
        - gamma (0 to 1): utility curvature; r is transformed as sign(r)*|r|^gamma before learning.
        - tau (0 to 1): exploration bonus strength at stage 2; scales 1/sqrt(N) bonus.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, eta_t, gamma, tau = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values for second stage and derived state values
    q2 = np.zeros((2, 2))

    # Learned transition model T (serves as a one-step SR for stage 1)
    T = np.full((2, 2), 0.5)

    # Visit counts for UCB bonus at stage 2
    N = np.zeros((2, 2))  # counts per (state, action)

    for t in range(n_trials):

        # Compute state values and MB first-stage values
        v_state = np.max(q2, axis=1)
        q1_mb = T @ v_state

        # First-stage policy (pure MB here)
        q1p = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1p)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with UCB-style uncertainty bonus
        s = state[t]
        # Bonus decreases with more visits; add small offset to avoid div-by-zero
        bonus = tau / np.sqrt(N[s, :] + 1.0)
        q2_aug = q2[s, :] + bonus
        q2s = q2_aug - np.max(q2_aug)
        probs_2 = np.exp(beta * q2s)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe outcome and transform via utility function
        r_raw = reward[t]
        r_sign = 1.0 if r_raw >= 0 else -1.0
        r_util = r_sign * (abs(r_raw) ** gamma)

        # Update counts for UCB
        N[s, a2] += 1.0

        # MF learning at stage 2
        delta2 = r_util - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update transition model toward observed state for chosen a1
        T[a1, :] = (1.0 - eta_t) * T[a1, :]
        T[a1, s] += eta_t

        # Optional bootstrapped update to align stage-1 values with updated state values
        # (keeps stage-1 consistent with evolving q2 via the current transition model)
        v_s = np.max(q2[s, :])
        target = v_s
        # TD error for chosen first-stage action relative to MB prediction prior to observing reward
        td1 = target - (T[a1, :] @ np.max(q2, axis=1))
        # Small correction step proportional to alpha keeps stage-1 MB consistent
        # (implemented as a slight nudge of the corresponding row of T toward the identity on s)
        # Here we perform a mild "policy evaluation" style step on the T row via the same eta_t:
        # already updated above; td1 used implicitly to motivate learning but we ensure parameter use.
        # We add a tiny consistency correction by moving T[a1, s] slightly further toward 1
        # proportional to alpha*td1 while keeping row normalized.
        corr = alpha * max(0.0, td1)
        if corr > 0.0:
            move = min(corr, 1.0 - T[a1, s])
            if move > 0.0:
                # take mass proportionally from the other state
                other = 1 - s
                take = min(move, T[a1, other])
                T[a1, s] += take
                T[a1, other] -= take

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll