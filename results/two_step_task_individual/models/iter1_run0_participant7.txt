def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Model-based with learned transitions, model-free second stage, and lapse + perseveration.
    
    This model learns second-stage action values from reward (TD(0)) and simultaneously learns
    first-stage transition probabilities via a Rescorlaâ€“Wagner update. First-stage action selection
    is model-based using the learned transition matrix multiplied by the best second-stage values.
    Both stages include a choice perseveration bias, and choices have a small lapse probability
    that mixes in a uniform random policy.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens indexed 0 or 1 within the reached planet).
    reward : array-like of float
        Obtained reward on each trial (can be negative, zero, or positive).
    model_parameters : iterable of 5 floats
        [alpha, beta, tau, kappa, epsilon]
        - alpha: [0,1] learning rate for second-stage Q-values and first-stage MF backup
        - beta:  [0,10] inverse temperature for softmax at both stages
        - tau:   [0,1] learning rate for transition probabilities (RW update)
        - kappa: [0,1] perseveration bias magnitude (applied at both stages)
        - epsilon: [0,1] lapse probability; with prob epsilon, choices are uniform random
        
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, tau, kappa, epsilon = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition probabilities T[a, s'].
    # Start from mildly informative priors (near-known) to avoid zero-prob issues.
    T = np.array([[0.65, 0.35],
                  [0.35, 0.65]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values at stage 2 (for each state x action).
    q2 = np.zeros((2, 2))
    # Model-free Q-values for stage 1 used only as a bootstrap target (not for policy).
    q1_mf = np.zeros(2)

    prev_a1 = None
    prev_a2 = [None, None]

    for t in range(n_trials):
        s = state[t]

        # Compute model-based first-stage Q via learned transitions and current second-stage values
        max_q2 = np.max(q2, axis=1)          # value of best alien at each planet
        q1_mb = T @ max_q2                   # MB value for each spaceship

        # Add perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa

        # Softmax with lapse at stage 1
        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1_soft = np.exp(logits1)
        probs1_soft /= np.sum(probs1_soft)
        probs1 = (1 - epsilon) * probs1_soft + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy, with perseveration bias within the reached state
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa

        logits2 = beta * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2_soft = np.exp(logits2)
        probs2_soft /= np.sum(probs2_soft)
        probs2 = (1 - epsilon) * probs2_soft + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # 1) Transition learning for the chosen first-stage action:
        #    RW toward the observed state; keep row normalized.
        #    T[a1, s] moves toward 1, the other toward 0.
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += tau * (target - T[a1, sp])
        # Normalize to protect against drift
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

        # 2) Second-stage TD(0) update
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # 3) First-stage model-free bootstrap toward observed second-stage action value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Surprise-gated MB/MF arbitration with ship bias.
    
    This model blends model-based (MB) and model-free (MF) first-stage values using a dynamic weight
    that depends on transition surprise (rarity). After rarer transitions, the MB system is up-weighted.
    Second-stage values are learned via TD(0). A constant bias toward spaceship A (action 0) is included.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Reached second-stage state (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens indexed 0 or 1 within the reached planet).
    reward : array-like of float
        Reward obtained each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, w0, gamma, xi]
        - alpha: [0,1] learning rate for MF updates at both stages
        - beta:  [0,10] inverse temperature for both stages
        - w0:    [0,1] baseline weight on MB control at stage 1
        - gamma: [0,1] modulation strength: increases MB weight after more surprising transitions
        - xi:    [0,1] constant bias toward spaceship A (action 0); implemented as +xi to action 0 logit
        
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, w0, gamma, xi = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (known to participant)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    # Map w0 in [0,1] to logit space so gamma can act linearly before squashing
    w0_logit = np.log((w0 + 1e-8) / (1 - w0 + 1e-8))

    for t in range(n_trials):
        s = state[t]

        # Compute MB first-stage action values using known transitions and current q2_mf
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T_fixed @ max_q2

        # Surprise based on transition probability of observed state given chosen action.
        # We don't know a1 before selection; use both to compute probs for policy then select p of observed after.
        # For computing arbitration weight we will use the surprise tied to the actually chosen action.
        # First compute policy for stage 1 using provisional mixture with baseline w0 (without surprise),
        # then after observing the state we re-compute the arbitration weight for learning (not for policy).
        # However, for likelihood we need the policy before observing state:
        w_pre = w0  # use baseline for action selection

        # First-stage policy with bias xi toward action 0
        q1_hyb_pre = w_pre * q1_mb + (1.0 - w_pre) * q1_mf
        bias_vec = np.array([xi, 0.0])
        logits1 = beta * q1_hyb_pre + bias_vec
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * q2_mf[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # TD(0) at second stage
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Compute surprise for the actually chosen action a1 and reached state s
        p_trans = T_fixed[a1, s]
        surprise = 1.0 - p_trans  # higher when rarer

        # Dynamic MB weight for learning/backups (not for the already-made choice likelihood)
        w_dyn = logistic(w0_logit + gamma * (2 * surprise - 1))  # center surprise to ~[-1,1]

        # Update stage-1 MF toward second-stage value (SARSA-style using chosen a2's value)
        target1_mf = q2_mf[s, a2]
        delta1 = target1_mf - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Optional: consistency update on unchosen action to keep values bounded (no-op here)

        # No explicit update of MB component (it's computed from q2 and fixed transitions)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Hybrid SR-like MB/MF with novelty bonuses and visit decay.
    
    This model blends model-based action values (computed from known transitions) with
    model-free first-stage values. At the second stage, choices receive an intrinsic
    novelty bonus that decays with repeated visits, encouraging exploration. Visit
    counts decay over time, allowing previously familiar options to regain novelty.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage states (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens 0/1 within planet).
    reward : array-like of float
        Received reward on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, zeta, nu, decay]
        - alpha: [0,1] learning rate for Q updates (both stages)
        - beta:  [0,10] inverse temperature for both stages
        - zeta:  [0,1] weight on model-based (transition-projected) value at stage 1
                 (1 = purely MB; 0 = purely MF)
        - nu:    [0,1] novelty bonus weight added to second-stage action values
        - decay: [0,1] per-trial multiplicative decay of visit counts (higher = slower forgetting)
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, zeta, nu, decay = model_parameters
    n_trials = len(action_1)

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Visit counts for novelty (state x action)
    visits = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]

        # Decay visits each trial to allow novelty to re-emerge
        visits *= decay

        # Compute novelty bonuses for current state
        # Bonus decreases with sqrt of (1 + visits) to mimic diminishing returns
        bonus_all = nu / np.sqrt(1.0 + visits)
        bonus_s = bonus_all[s, :]

        # Stage 2 policy with novelty bonus added to Qs
        q2_bonus = q2[s, :] + bonus_s
        logits2 = beta * q2_bonus
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage 1 values: MB from transitions and max q2 per state (without bonus in learning signal),
        # and MF learned from prior backups; blend with weight zeta
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1_hyb = zeta * q1_mb + (1.0 - zeta) * q1_mf

        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning updates
        r = reward[t]
        # Update visits after choice to ensure the chosen option loses bonus next time
        visits[s, a2] += 1.0

        # Include novelty bonus in the effective outcome to capture intrinsic motivation in learning
        r_eff = r + (nu / np.sqrt(1.0 + visits[s, a2]))  # use updated count so bonus immediately diminishes

        # Second-stage TD(0)
        delta2 = r_eff - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # First-stage MF backup toward the realized second-stage value (without bonus to avoid double counting)
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll