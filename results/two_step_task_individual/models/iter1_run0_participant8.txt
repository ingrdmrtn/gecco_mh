def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Hybrid model-based/model-free controller with learned transition model and choice stickiness.

    Mechanism
    - Second-stage: model-free Q-learning toward reward.
    - Transition learning: learn the first-stage action -> state transition matrix online.
    - First-stage values: convex combination of model-free and model-based values.
      Q1 = (1 - w) * Q1_mf + w * Q1_mb, where Q1_mb = T @ max_a2 Q2(s, a2).
    - Policy: softmax with inverse temperature beta at both stages, plus choice stickiness rho
      that adds a bias to the most recently chosen action at the corresponding stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state/planet reached per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 denote the two aliens on the planet).
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_q, alpha_t, beta, w, rho]
        - alpha_q in [0,1]: learning rate for model-free Q-values (both stages).
        - alpha_t in [0,1]: learning rate for the transition model T updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w in [0,1]: weight of model-based values in the first-stage decision.
        - rho in [0,1]: choice stickiness strength added to the previously chosen action
                        at its respective stage (stage-specific perseveration).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_q, alpha_t, beta, w, rho = model_parameters
    n_trials = len(action_1)

    # Initialize
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Start with an unbiased transition model (rows sum to 1)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)  # shape (2 actions, 2 states)

    q1_mf = np.zeros(2)         # model-free first-stage Q
    q2 = np.zeros((2, 2))       # second-stage Q: states x actions

    prev_a1 = -1
    prev_a2 = np.full(2, -1, dtype=int)  # track previous action separately per state

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage values from learned transitions
        v2 = np.max(q2, axis=1)                 # value of each state
        q1_mb = T @ v2                          # shape (2,)

        # Combine MF and MB for stage 1
        q1_comb = (1.0 - w) * q1_mf + w * q1_mb

        # Add stickiness to stage-1 preferences
        pref1 = q1_comb.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += rho

        # Stage-1 softmax
        pref1 -= np.max(pref1)
        exp1 = np.exp(beta * pref1)
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 preferences with stickiness (state-specific)
        pref2 = q2[s, :].copy()
        if prev_a2[s] in (0, 1):
            pref2[prev_a2[s]] += rho

        # Stage-2 softmax
        pref2 -= np.max(pref2)
        exp2 = np.exp(beta * pref2)
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Learning updates

        # 1) Update second-stage Q toward reward
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # 2) Model-free first-stage update toward the realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # 3) Transition learning: update the row for chosen action a1 toward observed state s
        # Create one-hot target over states for this action
        target_trans = np.array([0.0, 0.0])
        target_trans[s] = 1.0
        T[a1, :] = (1.0 - alpha_t) * T[a1, :] + alpha_t * target_trans
        # Ensure row remains normalized (guard against numerical drift)
        T[a1, :] /= np.sum(T[a1, :]) + 1e-12

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Successor-style transition learning with value decay and stage-specific stickiness.

    Mechanism
    - Learn a state-successor mapping per first-stage action (row-stochastic M),
      akin to a learned transition model via a delta rule (successor-like for this 2-step task).
    - Second-stage: model-free Q-learning toward reward.
    - First-stage: hybrid of SR-derived model-based values and model-free values, with weight sr_w.
    - Value decay: all Q-values decay toward 0 baseline on each trial by a factor (1 - decay).
    - Policy: softmax with inverse temperature beta, plus stickiness tau at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choice per trial within reached state.
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, sr_w, decay, tau]
        - alpha in [0,1]: learning rate for both Q updates and SR transition learning.
        - beta in [0,10]: inverse temperature for both stages.
        - sr_w in [0,1]: weight on SR-based model-based value at stage 1.
        - decay in [0,1]: per-trial decay applied to all Q-values (0=no decay, 1=full reset).
        - tau in [0,1]: choice stickiness strength for prior action at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, sr_w, decay, tau = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Successor-like mapping from first-stage actions to second-stage states
    M = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)  # rows sum to 1

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    prev_a1 = -1
    prev_a2 = np.full(2, -1, dtype=int)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Decay all Q-values toward 0 baseline
        q1_mf *= (1.0 - decay)
        q2 *= (1.0 - decay)

        # SR-derived MB values: M @ max_a2 q2(s, a2)
        v2 = np.max(q2, axis=1)
        q1_mb = M @ v2

        # Hybrid first-stage value
        q1 = (1.0 - sr_w) * q1_mf + sr_w * q1_mb

        # Stage-1 softmax with stickiness
        pref1 = q1.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += tau
        pref1 -= np.max(pref1)
        probs1 = np.exp(beta * pref1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with stickiness (state-specific)
        pref2 = q2[s, :].copy()
        if prev_a2[s] in (0, 1):
            pref2[prev_a2[s]] += tau
        pref2 -= np.max(pref2)
        probs2 = np.exp(beta * pref2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates

        # Second-stage Q-learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First-stage model-free update toward realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update SR/transition mapping M: row for chosen action toward observed state one-hot
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        M[a1, :] = (1.0 - alpha) * M[a1, :] + alpha * target
        M[a1, :] /= np.sum(M[a1, :]) + 1e-12

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Noisy model-based planning with uncertainty bonus and transition misperception.

    Mechanism
    - Second-stage: reward means learned with a delta rule; track an uncertainty proxy per
      state-action as an exponential moving average of squared prediction errors.
    - Exploration bonus: add k * sqrt(uncertainty) to second-stage action values.
    - First-stage: purely model-based using a fixed-but-misperceived transition matrix where
      the 'common' transition probability is p_c = 0.5 + 0.3 * m (m in [0,1]).
    - Stickiness: add s to previously chosen action at both stages.
    - Policy: softmax with inverse temperature beta at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choice per trial.
    reward : array-like of float (0 or 1)
        Reward per trial.
    model_parameters : iterable of 5 floats
        [alpha_r, beta, k, m, s]
        - alpha_r in [0,1]: learning rate for second-stage reward means and uncertainty proxy.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - k in [0,1]: exploration bonus weight applied to sqrt(uncertainty).
        - m in [0,1]: transition misperception setting common prob p_c = 0.5 + 0.3*m.
        - s in [0,1]: stickiness strength added to the previously chosen action at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, beta, k, m, s_par = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Misperceived (fixed) transition matrix based on m
    p_c = 0.5 + 0.3 * m
    T = np.array([[p_c, 1.0 - p_c],
                  [1.0 - p_c, p_c]], dtype=float)

    # Second-stage: maintain mean and uncertainty proxy
    q2 = np.zeros((2, 2))            # reward means per state-action
    u2 = np.ones((2, 2)) * 0.25      # start with moderate uncertainty

    prev_a1 = -1
    prev_a2 = np.full(2, -1, dtype=int)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Construct bonus-augmented second-stage values
        bonus = k * np.sqrt(np.maximum(u2, 1e-12))
        q2_eff = q2 + bonus  # optimistic exploration

        # First-stage purely model-based using misperceived T
        v2 = np.max(q2_eff, axis=1)
        q1_mb = T @ v2

        # Stage-1 policy with stickiness
        pref1 = q1_mb.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += s_par
        pref1 -= np.max(pref1)
        probs1 = np.exp(beta * pref1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness (state-specific)
        pref2 = q2_eff[s, :].copy()
        if prev_a2[s] in (0, 1):
            pref2[prev_a2[s]] += s_par
        pref2 -= np.max(pref2)
        probs2 = np.exp(beta * pref2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning at second stage: update mean and uncertainty proxy
        pe2 = r - q2[s, a2]
        # Mean update
        q2[s, a2] += alpha_r * pe2
        # Uncertainty proxy: EMA of squared PE
        u2[s, a2] = (1.0 - alpha_r) * u2[s, a2] + alpha_r * (pe2 ** 2)

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll