def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Confidence-weighted model-based planner with value decay and lapse.
    
    This model plans at stage 1 using a fixed transition structure but allows the
    agent to attenuate how strongly they rely on the known transitions via a
    "transition confidence" parameter. Stage-2 values undergo decay (forgetting),
    are updated with a standard delta rule, and both stages include a lapse that
    blends the softmax policy with uniform random choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached state.
    reward : array-like of float/int (typically 0 or 1)
        Obtained outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, c_trans, decay, lapse]
        - alpha (Q2 learning rate) in [0,1]
        - beta  (inverse temperature) in [0,10]
        - c_trans (confidence in the known transition; 0=ignore, 1=use fully) in [0,1]
        - decay (global forgetting of Q2 each trial) in [0,1]
        - lapse (probability of random choice at each stage) in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, c_trans, decay, lapse = model_parameters
    n_trials = len(action_1)

    # Known transition structure (A->X common, U->Y common)
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])
    # Confidence-weighted transition actually used for planning
    T_eff = c_trans * T_known + (1.0 - c_trans) * 0.5  # blend with uninformative 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values: Q2[state, action]
    q2 = np.zeros((2, 2))

    for t in range(n_trials):

        # Stage-1 planning via model-based backup with confidence-weighted transitions
        max_q2 = np.max(q2, axis=1)  # value of best action in each second-stage state
        q1_mb = T_eff @ max_q2

        # Stage-1 policy (softmax with lapse)
        pref1 = beta * q1_mb
        pref1 -= np.max(pref1)
        soft1 = np.exp(pref1)
        soft1 = soft1 / np.sum(soft1)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in the reached state
        s = state[t]
        pref2 = beta * q2[s, :]
        pref2 -= np.max(pref2)
        soft2 = np.exp(pref2)
        soft2 = soft2 / np.sum(soft2)
        probs_2 = (1.0 - lapse) * soft2 + lapse * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning: decay all Q2, then update chosen state-action
        r = reward[t]
        q2 *= (1.0 - decay)

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Successor-representation planner with online transition learning and lapse.
    
    This model learns a simple successor representation (SR) over stage-1 actions
    to second-stage states. The SR row for a chosen first-stage action tracks
    the expected occupancy of second-stage states (effectively the transition
    probabilities). The model then computes stage-1 values by projecting the SR
    onto current stage-2 values. A planning-depth/discount parameter attenuates
    how strongly this SR-projected value influences choice. Both stages include
    a lapse component.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached state.
    reward : array-like of float/int (typically 0 or 1)
        Obtained outcome per trial.
    model_parameters : iterable of floats
        [alpha_r, beta, alpha_sr, gamma_sr, lapse]
        - alpha_r  (learning rate for Q2) in [0,1]
        - beta     (inverse temperature) in [0,10]
        - alpha_sr (learning rate for the SR rows) in [0,1]
        - gamma_sr (planning depth/discount scaling for Q1 from SR) in [0,1]
        - lapse    (probability of random choice at each stage) in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, alpha_sr, gamma_sr, lapse = model_parameters
    n_trials = len(action_1)

    # SR matrix from first-stage actions to second-stage states
    # M[a, s] approximates P(s | take action a) in this one-step setting.
    M = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values
    q2 = np.zeros((2, 2))

    for t in range(n_trials):

        # Compute stage-1 values by projecting SR onto current stage-2 values
        max_q2 = np.max(q2, axis=1)  # value of best action at each state
        q1_sr = M @ max_q2
        q1 = gamma_sr * q1_sr  # attenuate depth of planning

        # Stage-1 policy (softmax + lapse)
        pref1 = beta * q1
        pref1 -= np.max(pref1)
        soft1 = np.exp(pref1)
        soft1 = soft1 / np.sum(soft1)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        pref2 = beta * q2[s, :]
        pref2 -= np.max(pref2)
        soft2 = np.exp(pref2)
        soft2 = soft2 / np.sum(soft2)
        probs_2 = (1.0 - lapse) * soft2 + lapse * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcomes
        r = reward[t]

        # SR update for the chosen first-stage action: move towards one-hot of the observed state
        target = np.array([1.0 if i == s else 0.0 for i in range(2)])
        M[a1, :] = (1.0 - alpha_sr) * M[a1, :] + alpha_sr * target

        # Stage-2 value learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-bonus exploration at stage 2 with transition-surprise-driven switching bias at stage 1.
    
    Stage 2 includes an uncertainty (novelty) bonus that favors less-tried actions
    within each state. Stage 1 includes a dynamic bias that promotes repeating the
    previous first-stage action after a common transition, but switching after a
    rare transition. Both stages include a lapse parameter.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached state.
    reward : array-like of float/int (typically 0 or 1)
        Obtained outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, nu, chi, lapse]
        - alpha (Q2 learning rate) in [0,1]
        - beta  (inverse temperature) in [0,10]
        - nu    (weight of uncertainty bonus at stage 2) in [0,1]
        - chi   (magnitude of transition-surprise bias at stage 1) in [0,1]
        - lapse (probability of random choice at each stage) in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, nu, chi, lapse = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure for identifying common vs rare transitions
    # A->X common, U->Y common
    def was_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values and action counts for uncertainty bonus
    q2 = np.zeros((2, 2))
    counts = np.zeros((2, 2))  # visitation counts per state-action

    # Variables to carry transition-surprise bias across trials
    prev_a1 = None
    prev_s = None

    for t in range(n_trials):

        # Stage-1 dynamic bias based on previous trial's transition commonality
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_s is not None:
            if was_common(prev_a1, prev_s):
                # After common transition: bias to repeat previous first-stage action
                bias1[prev_a1] += chi
            else:
                # After rare transition: bias to switch first-stage action
                bias1[1 - prev_a1] += chi

        # Use a myopic model-free proxy for Q1 from current Q2 values:
        # value of each first-stage action equals best expected value of its common state.
        # This keeps the bias term as the principal driver at stage 1.
        q1_proxy = np.array([np.max(q2[0, :]), np.max(q2[1, :])])

        pref1 = beta * q1_proxy + bias1
        pref1 -= np.max(pref1)
        soft1 = np.exp(pref1)
        soft1 = soft1 / np.sum(soft1)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with uncertainty bonus
        s = state[t]
        # Uncertainty bonus: larger when counts are small
        u = 1.0 / np.sqrt(counts[s, :] + 1.0)
        q2_bonus = q2[s, :] + nu * u

        pref2 = beta * q2_bonus
        pref2 -= np.max(pref2)
        soft2 = np.exp(pref2)
        soft2 = soft2 / np.sum(soft2)
        probs_2 = (1.0 - lapse) * soft2 + lapse * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update counts then learn Q2
        counts[s, a2] += 1.0
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Store transition to define next trial's bias
        prev_a1 = a1
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll