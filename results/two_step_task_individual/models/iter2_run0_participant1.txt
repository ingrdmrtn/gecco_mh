def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based + model-free learner with separate learning rates and choice stickiness.
    
    This model combines a model-based (MB) planner using fixed transitions with
    a model-free (MF) learner at both stages. Stage-1 choices are guided by a
    convex combination of MB and MF values. A single stickiness parameter adds a
    perseveration bias to repeat the last action at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha1, alpha2, beta, theta, tau]
        - alpha1 (stage-1 MF learning rate, [0,1]): updates MF value of chosen spaceship toward realized reward.
        - alpha2 (stage-2 learning rate, [0,1]): updates expected coins for chosen alien at visited planet.
        - beta (inverse temperature, [0,10]): softmax sensitivity for both stages.
        - theta (MB weighting, [0,1]): weight on MB action values in stage-1; 1.0 = purely MB, 0.0 = purely MF.
        - tau (stickiness, [0,1]): additive perseveration bias to repeat the last action at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha1, alpha2, beta, theta, tau = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)          # model-free stage-1 values
    q2 = np.zeros((2, 2))        # stage-2 values: state x action

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness bookkeeping
    last_a1 = None
    last_a2_by_state = [None, None]

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based stage-1 values via planning with fixed transitions
        max_q2 = np.max(q2, axis=1)     # best attainable value at each planet
        q1_mb = T @ max_q2

        # Hybrid stage-1 values
        q1_hybrid = theta * q1_mb + (1.0 - theta) * q1_mf

        # Stickiness/bias for stage-1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += tau

        # Stage-1 policy
        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stickiness/bias for stage-2 (state-dependent)
        bias2 = np.zeros(2)
        if last_a2_by_state[s2] is not None:
            bias2[last_a2_by_state[s2]] += tau

        # Stage-2 policy
        logits2 = beta * q2[s2] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 MF update toward realized reward
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha2 * delta2

        # Stage-1 MF update toward realized outcome (eligibility = 1)
        delta1 = r - q1_mf[a1]
        q1_mf[a1] += alpha1 * delta1

        # Update stickiness memory
        last_a1 = a1
        last_a2_by_state[s2] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with directed exploration bonus and lapse noise.
    
    The model learns second-stage reward values and adds a state-action uncertainty
    bonus to encourage exploration (directed exploration). Uncertainty is estimated
    from the running reward probability via Bernoulli variance, and both stages
    use a small lapse that mixes the softmax policy with a uniform policy.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, zeta, epsilon]
        - alpha (reward learning rate, [0,1]): updates expected coin probability for each alien.
        - beta (inverse temperature, [0,10]): softmax sensitivity for both stages.
        - zeta (exploration bonus weight, [0,1]): scales uncertainty bonus added to second-stage values.
          Uncertainty is sqrt(p*(1-p)), where p is the learned reward probability.
        - epsilon (lapse rate, [0,1]): probability of choosing uniformly at random (applied at both stages).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, zeta, epsilon = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage value and probability trackers
    q2 = np.zeros((2, 2))           # expected value (mean reward)
    p_hat = np.full((2, 2), 0.5)    # running estimate of reward probability

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Directed exploration bonus from uncertainty (Bernoulli sd)
        var = p_hat * (1.0 - p_hat)
        bonus = zeta * np.sqrt(var)
        q2_bonus = q2 + bonus

        # Model-based planning with exploration-adjusted second-stage values
        max_q2b = np.max(q2_bonus, axis=1)
        q1 = T @ max_q2b

        # Stage-1 policy with lapse
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 /= np.sum(soft1)
        probs1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapse
        logits2 = beta * q2_bonus[s2]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 /= np.sum(soft2)
        probs2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning updates: value mean and probability estimates
        delta = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta
        p_hat[s2, a2] += alpha * (r - p_hat[s2, a2])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """MB planner with transition- and reward-dependent perseveration and value decay.
    
    This model plans using fixed transitions and second-stage MF values. Stage-1
    choices receive an additive bias to repeat the previous stage-1 action, with
    the bias magnitude modulated by the prior trialâ€™s reward (win/lose) and by
    whether the prior transition was common or rare. Second-stage values decay
    each trial (forgetting), then are updated from reward.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, psiR, psiT, decay]
        - alpha (stage-2 learning rate, [0,1]): updates expected coins for chosen alien.
        - beta (inverse temperature, [0,10]): softmax sensitivity for both stages.
        - psiR (reward-modulated perseveration strength, [0,1]): maps to a signed
          stay bias after wins vs. losses; >0.5 favors win-stay/lose-switch.
        - psiT (transition-modulated perseveration strength, [0,1]): maps to a signed
          stay bias after rare vs. common; >0.5 favors switching after rare (MB-like).
        - decay (value forgetting rate, [0,1]): multiplicative decay of all second-stage
          action values each trial before learning.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, psiR, psiT, decay = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))  # second-stage values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_state = None
    prev_reward = None

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply global decay to second-stage values (forgetting)
        q2 *= (1.0 - decay)

        # Model-based planning using current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Transition and reward dependent perseveration bias on stage-1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            # Map psiR, psiT in [0,1] to signed strengths in [-1,1]
            sR = 2.0 * psiR - 1.0
            sT = 2.0 * psiT - 1.0

            # Prior transition type: common if action==state (A->X or U->Y)
            was_common = (prev_a1 == prev_state)
            trn_term = -1.0 if was_common else 1.0  # rare -> +1, common -> -1
            rwd_term = prev_reward - 0.5            # +0.5 after win, -0.5 after loss

            bias_mag = sR * rwd_term + sT * trn_term
            bias1[prev_a1] += bias_mag

        # Stage-1 policy
        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (MF)
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: update second-stage value from reward after decay
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Cache for next trial's bias computation
        prev_a1 = a1
        prev_state = s2
        prev_reward = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll