def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model with learned transitions and second-stage perseveration.
    
    This model blends model-based planning using a learned transition matrix with a model-free
    first-stage value. The second stage is learned model-free, with a state-specific
    perseveration (choice-kernel) bias that encourages repeating the last alien chosen
    on that planet.
    
    Parameters (all used; suggested bounds):
    - alphaQ: [0,1] learning rate for stage-2 Q-values (and stage-1 MF backup)
    - alphaT: [0,1] learning rate for the transition matrix (how quickly transitions are learned)
    - w_MB:  [0,1] weight of model-based value in first-stage decision (1 - w_MB) weights MF Q1
    - beta:  [0,10] softmax inverse temperature shared across stages
    - stick2: [0,1] second-stage perseveration strength added to the last chosen alien on a planet
    
    Inputs:
    - action_1: array (n_trials,), 0/1 for spaceship A/U
    - state:    array (n_trials,), 0/1 for planet X/Y actually visited
    - action_2: array (n_trials,), 0/1 for alien within visited planet
    - reward:   array (n_trials,), typically 0/1
    - model_parameters: iterable [alphaQ, alphaT, w_MB, beta, stick2]
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alphaQ, alphaT, w_MB, beta, stick2 = model_parameters
    n_trials = len(action_1)

    # Initialize with the task's common transitions as prior, but allow learning
    T = np.array([[0.7, 0.3],   # P(state | action A)
                  [0.3, 0.7]])  # P(state | action U)

    q1_mf = np.zeros(2)          # model-free first-stage action values
    q2 = np.zeros((2, 2))        # second-stage Q-values: Q2[state, action]

    # State-specific last chosen alien for perseveration
    last_a2_by_state = [None, None]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based evaluation of first-stage via learned transitions and current Q2
        v2 = np.max(q2, axis=1)           # value of best alien on each planet
        q1_mb = T @ v2                    # MB value of each spaceship

        # Blend MB and MF for first-stage preferences
        pref1 = w_MB * q1_mb + (1.0 - w_MB) * q1_mf

        # First-stage choice probability (softmax)
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage preferences with state-specific perseveration
        pref2 = q2[s].copy()
        if last_a2_by_state[s] is not None:
            bias2 = np.zeros(2)
            bias2[last_a2_by_state[s]] = stick2
            pref2 = pref2 + bias2

        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning
        # 1) Transition learning for the chosen first-stage action using a simple delta rule
        # Move row T[a1] toward the observed state one-hot vector
        onehot_s = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * onehot_s
        # Ensure numerical stability
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # 2) Second-stage TD learning
        td2 = reward[t] - q2[s, a2]
        q2[s, a2] += alphaQ * td2

        # 3) First-stage model-free backup toward the experienced second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alphaQ * td1

        # Update perseveration memory
        last_a2_by_state[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with asymmetric learning and PE gain to stage-1, plus first-stage stay bias.
    
    This model eschews planning and relies on model-free values only. Second-stage learning uses
    asymmetric learning rates for positive vs. negative prediction errors. The second-stage PE
    is propagated to the first-stage action via a gain factor (without an explicit eligibility trace).
    A first-stage perseveration bias favors repeating the last chosen spaceship.
    
    Parameters (all used; suggested bounds):
    - alpha_pos: [0,1] learning rate when second-stage TD error is positive
    - alpha_neg: [0,1] learning rate when second-stage TD error is negative
    - pe_gain:  [0,1] gain scaling how much second-stage PE updates first-stage MF value
    - beta:     [0,10] softmax inverse temperature shared across stages
    - stay1:    [0,1] first-stage perseveration strength added to last-chosen spaceship
    
    Inputs:
    - action_1: array (n_trials,), 0/1 for spaceship A/U
    - state:    array (n_trials,), 0/1 for planet X/Y actually visited
    - action_2: array (n_trials,), 0/1 for alien within visited planet
    - reward:   array (n_trials,), typically 0/1
    - model_parameters: iterable [alpha_pos, alpha_neg, pe_gain, beta, stay1]
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, pe_gain, beta, stay1 = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)         # model-free first-stage Q-values
    q2 = np.zeros((2, 2))    # second-stage Q-values: Q2[state, action]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # First-stage preferences with perseveration bias
        pref1 = q1.copy()
        if last_a1 is not None:
            bias = np.zeros(2)
            bias[last_a1] = stay1
            pref1 = pref1 + bias

        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy is softmax over Q2 at the visited state
        pref2 = q2[s].copy()
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 TD error and asymmetric learning rate
        td2 = reward[t] - q2[s, a2]
        alpha2 = alpha_pos if td2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2 * td2

        # Stage-1 bootstrapped update towards the experienced second-stage value (SARSA-like)
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha2 * td1  # use same asymmetry as the experienced TD

        # Additional propagation of reward PE to stage-1 via gain (complements the bootstrapping)
        q1[a1] += pe_gain * alpha2 * td2

        # Update perseveration memory
        last_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor representation (SR) for stage-1 plus model-free values with forgetting.
    
    The model learns a successor map from first-stage actions to second-stage states (planets),
    akin to a learned transition distribution. First-stage choice values combine SR-predicted
    value of future states with a model-free first-stage value. Second-stage is model-free.
    All value functions are subject to decay/forgetting between trials.
    
    Parameters (all used; suggested bounds):
    - alphaQ:  [0,1] learning rate for Q-values (both stages, including MF first-stage)
    - alphaSR: [0,1] learning rate for the SR row corresponding to the chosen action
    - beta:    [0,10] softmax inverse temperature shared across stages
    - w_SR:    [0,1] weight on SR-derived first-stage values; (1 - w_SR) weights MF Q1
    - decay:   [0,1] per-trial forgetting factor applied to Q-values (0 = no decay)
    
    Inputs:
    - action_1: array (n_trials,), 0/1 for spaceship A/U
    - state:    array (n_trials,), 0/1 for planet X/Y actually visited
    - action_2: array (n_trials,), 0/1 for alien within visited planet
    - reward:   array (n_trials,), typically 0/1
    - model_parameters: iterable [alphaQ, alphaSR, beta, w_SR, decay]
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alphaQ, alphaSR, beta, w_SR, decay = model_parameters
    n_trials = len(action_1)

    # Successor map from first-stage action to states (rows: action A/U, cols: state X/Y)
    # Initialize with weak prior (uniform over states)
    M = np.ones((2, 2)) * 0.5

    q1_mf = np.zeros(2)       # model-free stage-1 action values
    q2 = np.zeros((2, 2))     # stage-2 Q-values: Q2[state, action]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Apply decay/forgetting to values each trial
        q1_mf *= (1.0 - decay)
        q2 *= (1.0 - decay)
        # Optionally normalize SR row-wise after decay
        row_sums = np.sum(M, axis=1, keepdims=True) + eps
        M = M / row_sums  # keep SR rows as distributions for interpretability

        # SR-based action values: expected value over states under M
        v2 = np.max(q2, axis=1)       # value of best alien on each planet
        q1_sr = M @ v2                # SR-implied values for A/U

        # First-stage policy mixing SR and MF values
        pref1 = w_SR * q1_sr + (1.0 - w_SR) * q1_mf
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy is softmax over Q2 in visited state
        pref2 = q2[s].copy()
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning
        # 1) Update SR row for chosen action toward the observed state one-hot vector
        onehot_s = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        M[a1] = (1.0 - alphaSR) * M[a1] + alphaSR * onehot_s
        # Keep SR row normalized
        M[a1] = M[a1] / (np.sum(M[a1]) + eps)

        # 2) Second-stage TD update
        td2 = reward[t] - q2[s, a2]
        q2[s, a2] += alphaQ * td2

        # 3) First-stage MF backup toward experienced second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alphaQ * td1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss