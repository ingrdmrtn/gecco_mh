Below are three standalone cognitive models. Each function:

- Takes inputs: action_1, state, action_2, reward, model_parameters
- Uses all parameters meaningfully, within the requested bounds
- Returns the negative log-likelihood of the observed first- and second-stage choices
- Avoids parameter combinations you tried previously

Assumptions:
- action_1, state, action_2, reward are 1D arrays of equal length
- action indices are 0/1; spaceship A=0, U=1; planet X=0, Y=1
- Common transitions: A→X, U→Y

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-modulated arbitration (MB vs MF), leaky second-stage values, and second-stage perseveration.
    Parameters (all used):
    - alpha_r: reward learning rate for stage-2 Q-values and stage-1 MF bootstrap (bounds: [0,1])
    - beta: inverse temperature for softmax at both stages (bounds: [0,10])
    - w0: baseline weight on model-based value at stage 1 (bounds: [0,1])
    - leak: leaky forgetting of stage-2 values toward 0.5 each trial (bounds: [0,1])
    - kappa2: second-stage perseveration bias toward repeating last alien in a given planet (bounds: [0,1])
    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of second-stage states (0=X, 1=Y)
    - action_2: array of second-stage choices (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: [alpha_r, beta, w0, leak, kappa2]
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, w0, leak, kappa2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (participant may learn it, but here it’s known)
    transition_matrix = np.array([[0.7, 0.3],  # A -> X common
                                  [0.3, 0.7]]) # U -> Y common

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free stage-1 Q and stage-2 Q
    q1_mf = np.zeros(2)
    q2 = np.ones((2, 2)) * 0.5  # start unbiased

    # Track last second-stage action per state for perseveration
    last_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        # Leaky forgetting of Q2 toward 0.5
        if leak > 0:
            q2 = (1.0 - leak) * q2 + leak * 0.5

        # Model-based stage-1 value from current stage-2 values
        max_q2 = np.max(q2, axis=1)                    # value of best alien on each planet
        q1_mb = transition_matrix @ max_q2             # expected value for each spaceship

        # Uncertainty-modulated arbitration:
        # If the realized transition is rare, increase MB weight to 1 on that trial; else use baseline w0.
        a1 = action_1[t]
        s = state[t]
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        is_rare = 1 - is_common
        w_t = w0 if is_common == 1 else 1.0            # rare → rely on MB more strongly

        # First-stage policy
        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf
        # Softmax with stabilization
        pref1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(pref1)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with perseveration
        a2 = action_2[t]
        q2_s = q2[s].copy()
        if last_a2[s] in (0, 1):
            stick_vec = np.zeros(2)
            stick_vec[last_a2[s]] = kappa2
            q2_s = q2_s + stick_vec

        pref2 = beta * (q2_s - np.max(q2_s))
        probs_2 = np.exp(pref2)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Stage-1 MF bootstrap from realized stage/state/action
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * td1

        # Update perseveration memory
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Purely model-free SARSA with dynamic exploration and choice kernels at both stages.
    Mechanisms:
    - Dynamic inverse temperature beta_t increases with recent reward rate (win–stay via exploration control).
    - Choice kernels (with their own learning rate) bias repeating recent actions at both stages.
    Parameters (all used):
    - alpha_r: reward learning rate for both stages (bounds: [0,1])
    - beta0: baseline inverse temperature (bounds: [0,10])
    - nu: running-average reward update rate (also scales exploration control) (bounds: [0,1])
    - chi: choice-kernel learning rate (bounds: [0,1])
    - bias0: global weight multiplying the choice-kernel biases (bounds: [0,1])
    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of second-stage states (0=X, 1=Y)
    - action_2: array of second-stage choices (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: [alpha_r, beta0, nu, chi, bias0]
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta0, nu, chi, bias0 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels: track recency of actions; centered by subtracting 0.5 when applied
    K1 = np.zeros(2)
    K2 = np.zeros((2, 2))

    # Running reward average for dynamic exploration
    r_bar = 0.0

    for t in range(n_trials):
        # Dynamic beta depends on current reward rate r_bar in [0,1], scaling beta0
        beta_t = beta0 * (0.5 + 0.5 * r_bar)

        # Stage-1 policy with choice kernel bias
        q1_eff = q1 + bias0 * (K1 - 0.5)
        pref1 = beta_t * (q1_eff - np.max(q1_eff))
        probs_1 = np.exp(pref1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with state-dependent choice kernel
        s = state[t]
        q2_eff = q2[s] + bias0 * (K2[s] - 0.5)
        pref2 = beta_t * (q2_eff - np.max(q2_eff))
        probs_2 = np.exp(pref2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and learn
        r = reward[t]

        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Stage-1 update: bootstrap from realized state-action at stage 2
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha_r * td1

        # Update choice kernels with learning rate chi (decay unchosen toward 0)
        # Stage 1 kernel
        K1 = (1.0 - chi) * K1
        K1[a1] += chi
        # Stage 2 kernel for the visited state
        K2[s] = (1.0 - chi) * K2[s]
        K2[s, a2] += chi

        # Update running average reward for exploration control
        r_bar = (1.0 - nu) * r_bar + nu * r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Transition-learning hybrid: learned model-based planning + cached model-free values with forgetting.
    Mechanisms:
    - Learns first-stage transition probabilities online.
    - Combines learned MB values with MF stage-1 values that are subject to forgetting.
    Parameters (all used):
    - alpha_r: reward learning rate (stage-2 Q-values and MF stage-1 bootstrap) (bounds: [0,1])
    - beta: inverse temperature for softmax at both stages (bounds: [0,10])
    - alpha_p: transition learning rate for P(planet | spaceship) (bounds: [0,1])
    - omega_sr: weight on model-based value at stage 1 (bounds: [0,1])
    - forget: forgetting rate for stage-1 MF values toward 0 each trial (bounds: [0,1])
    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of second-stage states (0=X, 1=Y)
    - action_2: array of second-stage choices (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: [alpha_r, beta, alpha_p, omega_sr, forget]
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, alpha_p, omega_sr, forget = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values
    q2 = np.zeros((2, 2))

    # Stage-1 MF Q-values (subject to forgetting)
    q1_mf = np.zeros(2)

    # Learn transition probabilities online; initialize neutral (0.5/0.5)
    T = np.ones((2, 2)) * 0.5  # rows: action1 (A,U), cols: states (X,Y), each row sums to ~1

    for t in range(n_trials):
        # Compute MB value from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combined value for stage 1
        q1 = omega_sr * q1_mb + (1.0 - omega_sr) * q1_mf

        # Stage-1 policy
        a1 = action_1[t]
        pref1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(pref1)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        a2 = action_2[t]
        pref2 = beta * (q2[s] - np.max(q2[s]))
        probs_2 = np.exp(pref2)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # Stage-2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Stage-1 MF learning with forgetting
        if forget > 0:
            q1_mf = (1.0 - forget) * q1_mf
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * td1

        # Transition learning: move row a1 toward the observed state s
        # Keep row normalized implicitly by symmetric push toward one-hot observation
        T[a1] = (1.0 - alpha_p) * T[a1]
        T[a1, s] += alpha_p
        # Small numerical renormalization for safety
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll