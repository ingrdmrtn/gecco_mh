def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with learned transitions, eligibility trace, and stage-1 perseveration.

    Mechanisms
    ----------
    - Stage-2 model-free learning of alien values (Q2).
    - Stage-1 model-free values (Q1_MF) updated via an eligibility trace that blends
      direct reward and bootstrapped value from stage-2.
    - Online learning of the transition model T(a1 -> state), used for model-based
      first-stage action values (Q1_MB = T @ max_a2 Q2).
    - Stage-1 perseveration kernel that decays proportionally to (1 - alpha).

    Parameters (model_parameters)
    -----------------------------
    [alpha, beta, trace, rho, stick1]
      - alpha in [0,1]: learning rate for Q-values; also governs kernel decay (decay = 1 - alpha).
      - beta in [0,10]: inverse temperature for softmax at both stages.
      - trace in [0,1]: eligibility trace weight blending reward and bootstrapped value in Q1_MF update.
      - rho in [0,1]: learning rate for the transition matrix T.
      - stick1 in [0,1]: strength of stage-1 perseveration kernel added to logits.

    Inputs
    ------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet (0/1 for the two aliens).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, trace, rho, stick1 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix: rows sum to 1
    T = np.ones((2, 2)) * 0.5

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Stage-1 perseveration kernel
    K1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        # Model-based first-stage values via learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MF and MB at stage-1 (simple additive combination)
        q1_combined = q1_mf + q1_mb

        # Stage-1 policy (with perseveration kernel)
        logits1 = beta * (q1_combined + stick1 * K1 - np.max(q1_combined + stick1 * K1))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: Stage-2 MF
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning: Stage-1 MF with eligibility trace
        # Target blends immediate reward and bootstrapped value:
        # target = (1 - trace) * max_a2 Q2[s] + trace * r
        target1 = (1 - trace) * np.max(q2[s]) + trace * r
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update learned transition model T using delta rule toward one-hot next state
        # Row a1 should approach [1,0] if s=0 else [0,1]
        T[a1, s] = (1 - rho) * T[a1, s] + rho * 1.0
        T[a1, 1 - s] = (1 - rho) * T[a1, 1 - s] + rho * 0.0

        # Perseveration kernel update (decay and add to chosen action)
        K1 *= (1 - alpha)
        K1[a1] += alpha

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """MB/MF hybrid with reward utility curvature and stage-2 UCB-style exploration.

    Mechanisms
    ----------
    - Stage-2 model-free Q-learning with a subjective utility of reward:
        u(r) = (1 - psi) * 0.5 + psi * r  (psi in [0,1]), preserving [0,1].
    - Stage-2 policy includes an exploration bonus nu / sqrt(N_sa + 1) (UCB-like),
      where N_sa is the visit count of (state, action_2).
    - Stage-1 policy blends model-based values (using fixed transitions) with model-free
      values via weight 'mix' in [0,1].

    Parameters (model_parameters)
    -----------------------------
    [alpha, beta, psi, nu, mix]
      - alpha in [0,1]: learning rate for Q-values.
      - beta in [0,10]: inverse temperature for softmax at both stages.
      - psi in [0,1]: utility curvature; psi=1 uses raw reward, psi=0 centers at 0.5.
      - nu in [0,1]: strength of stage-2 exploration bonus (added to Q2).
      - mix in [0,1]: weight on model-based value at stage-1 (0=MF only, 1=MB only).

    Inputs
    ------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet (0/1 for the two aliens).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, psi, nu, mix = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common=0.7)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    N_sa = np.zeros((2, 2))  # visit counts for UCB-like exploration

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Blend MB and MF
        q1 = mix * q1_mb + (1 - mix) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with exploration bonus
        s = state[t]
        bonus = nu / np.sqrt(N_sa[s] + 1.0)
        q2_bonus = q2[s] + bonus
        logits2 = beta * (q2_bonus - np.max(q2_bonus))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]
        u = (1 - psi) * 0.5 + psi * r  # subjective utility in [0,1]

        # Learning: Stage-2 MF with subjective utility
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2
        N_sa[s, a2] += 1.0

        # Learning: Stage-1 MF bootstraps on stage-2 maximum value
        target1 = np.max(q2[s])
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """MF with model-based win-stay/lose-switch bias, transition-contingent weighting, and decay.

    Mechanisms
    ----------
    - Pure model-free Q-learning at both stages (Q1_MF and Q2), with per-trial decay.
    - A model-based-inspired win-stay/lose-switch (WSLS) bias is added at stage-1:
        After a rewarded common transition OR an unrewarded rare transition: bias to STAY.
        After a rewarded rare transition OR an unrewarded common transition: bias to SWITCH.
      The bias magnitude is modulated by 'mb_shift' depending on whether the previous transition
      was common or rare, allowing differential sensitivity.
    - The WSLS bias is implemented as an additive bonus on the logits for the suggested action.

    Parameters (model_parameters)
    -----------------------------
    [alpha, beta, wsls, mb_shift, decay]
      - alpha in [0,1]: learning rate for Q-values.
      - beta in [0,10]: inverse temperature for softmax at both stages.
      - wsls in [0,1]: base strength of the WSLS bias at stage-1.
      - mb_shift in [0,1]: scales bias depending on transition type:
            effective magnitude = wsls * (1 - mb_shift) for common,
                                  wsls * (mb_shift) for rare.
      - decay in [0,1]: per-trial forgetting; Q-values are multiplied by (1 - decay) before updates.

    Inputs
    ------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet (0/1 for the two aliens).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, wsls, mb_shift, decay = model_parameters
    n_trials = len(action_1)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # To compute WSLS bias, store previous trial info
    prev_has_info = False
    prev_a1 = 0
    prev_s = 0
    prev_r = 0.0

    for t in range(n_trials):

        # Apply per-trial decay (forgetting) before using values
        q1_mf *= (1 - decay)
        q2 *= (1 - decay)

        # Construct WSLS bonus vector for stage-1
        bonus = np.zeros(2)
        if prev_has_info:
            # Determine if previous transition was common or rare
            prev_common = ((prev_a1 == 0 and prev_s == 0) or (prev_a1 == 1 and prev_s == 1))
            # Determine WSLS rule
            # stay if (reward and common) or (no reward and rare); else switch
            stay_suggested = (prev_r > 0 and prev_common) or (prev_r <= 0 and not prev_common)
            # Transition-contingent magnitude
            mag = wsls * ((1 - mb_shift) if prev_common else mb_shift)
            if stay_suggested:
                bonus[prev_a1] += mag
            else:
                bonus[1 - prev_a1] += mag

        # Stage-1 policy (MF + WSLS bias)
        q1 = q1_mf.copy()
        logits1 = beta * (q1 + bonus - np.max(q1 + bonus))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (MF)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: Stage-2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning: Stage-1 MF bootstraps on chosen stage-2 value (SARSA-style)
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Store info for next trial WSLS computation
        prev_has_info = True
        prev_a1 = a1
        prev_s = s
        prev_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll