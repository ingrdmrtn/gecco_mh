def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Pure model-based with learned transition matrix and asymmetric outcome learning, dual softmax temperatures.

    This model learns the first-stage transition structure online and uses it to compute
    model-based (MB) values at stage 1. At stage 2, it uses model-free (MF) learning with
    asymmetric learning rates for positive vs. negative prediction errors. Separate softmax
    temperatures govern choice stochasticity at stage 1 and stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 for the two aliens on that planet.
    reward : array-like of float (e.g., 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha_pos, alpha_neg, beta1, beta2, zeta]
        - alpha_pos (learning rate for positive PE, [0,1]): Used when r - Q2 > 0.
        - alpha_neg (learning rate for negative PE, [0,1]): Used when r - Q2 < 0.
        - beta1 (inverse temperature stage 1, [0,10]): Softmax sensitivity for first-stage choices.
        - beta2 (inverse temperature stage 2, [0,10]): Softmax sensitivity for second-stage choices.
        - zeta (transition learning rate, [0,1]): Step size for updating the transition probabilities.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha_pos, alpha_neg, beta1, beta2, zeta = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model T[action, state]
    T = np.full((2, 2), 0.5)  # start agnostic about transitions
    # Stage-2 MF values: Q2[state, action2]
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 1 policy: MB values from current learned transition T and current best Q2 per state
        max_Q2 = np.max(Q2, axis=1)  # shape (2,)
        q1_mb = T @ max_Q2           # shape (2,)

        # Softmax for stage 1
        q1_eff = q1_mb - np.max(q1_mb)
        exp_q1 = np.exp(beta1 * q1_eff)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: softmax over Q2 in the reached state
        q2_eff = Q2[s].copy() - np.max(Q2[s])
        exp_q2 = np.exp(beta2 * q2_eff)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning: update transition model T for the taken action based on observed state
        # Move row a1 towards one-hot of observed state s
        T[a1, :] = (1.0 - zeta) * T[a1, :]
        T[a1, s] += zeta
        # Ensure numerical stability (row must sum to ~1 and be non-negative)
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

        # Stage 2 MF update with asymmetric learning rates
        delta2 = r - Q2[s, a2]
        alpha_use = alpha_pos if delta2 >= 0 else alpha_neg
        Q2[s, a2] += alpha_use * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Hybrid of model-based action-to-state and state-value learning, with planet-repeat bias and second-stage perseveration.

    This model combines:
      - Model-based evaluation that uses the known transition structure to back up the
        current best second-stage values (MB-Q).
      - A state-value channel that learns the value of landing on each planet directly (V-state),
        which is also projected through the transition model to inform first-stage choices.
      - A bias to choose the first-stage action that commonly leads back to the previously visited
        planet (planet-repeat bias).
      - Second-stage perseveration (stickiness) favoring repeating the same alien within a planet.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the planet.
    reward : array-like of float (e.g., 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha, beta, omega, tau_planet, kappa2]
        - alpha (learning rate, [0,1]): Used for both Q2 and V-state updates.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - omega (state-value weight, [0,1]): Weight on the state-value channel at stage 1.
                                            0 = pure MB-Q; 1 = pure state-value.
        - tau_planet (planet repeat bias, [0,1]): Adds value to actions that commonly lead
                                                 to the previously visited planet.
        - kappa2 (second-stage perseveration, [0,1]): Bias to repeat the previous alien choice
                                                     within the same planet.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, omega, tau_planet, kappa2 = model_parameters
    n_trials = len(action_1)

    # Known transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q2 = np.zeros((2, 2))   # second-stage action values
    V_state = np.zeros(2)   # planet values
    pers2 = np.zeros((2, 2))  # second-stage perseveration traces (state-specific)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_state = None  # to compute planet-repeat bias

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 1 policy: combine MB-Q and projected state values
        mb_q = T @ np.max(Q2, axis=1)        # action values from Q2
        proj_state_val = T @ V_state         # action values from state values
        q1_base = (1.0 - omega) * mb_q + omega * proj_state_val

        # Planet-repeat bias: add tau_planet * P(going to prev_state | action)
        if prev_state is None:
            planet_bias = np.zeros(2)
        else:
            # bias each action by its probability to lead to the previously visited planet
            planet_bias = T[:, prev_state]
        q1_eff = q1_base + tau_planet * planet_bias

        # Softmax for stage 1
        q1c = q1_eff - np.max(q1_eff)
        exp_q1 = np.exp(beta * q1c)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: include perseveration bias within the current state
        q2_eff = Q2[s].copy() + kappa2 * pers2[s]
        q2c = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2c)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning
        # Update Q2 via MF TD
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Update planet value
        dv = r - V_state[s]
        V_state[s] += alpha * dv

        # Update perseveration traces: one-hot for the chosen alien in the visited planet
        pers2[:, :] = 0.0
        pers2[s, a2] = 1.0

        prev_state = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Transition-dependent eligibility traces with second-stage stickiness.

    This model is primarily model-free but allows the second-stage prediction error to
    influence the first-stage MF values via an eligibility trace whose strength depends
    on whether the observed transition was common or rare. This captures transition-based
    retrospective credit assignment. It also includes second-stage perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the planet.
    reward : array-like of float (e.g., 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha, beta, lam_common, lam_rare, rho_stay2]
        - alpha (learning rate, [0,1]): MF learning rate for both stages.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - lam_common (eligibility for common transitions, [0,1]): Strength of passing back
                                                                  stage-2 PE after common transitions.
        - lam_rare (eligibility for rare transitions, [0,1]): Strength of passing back
                                                              stage-2 PE after rare transitions.
        - rho_stay2 (second-stage perseveration, [0,1]): Bias to repeat the previous alien within the
                                                         same planet.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, lam_common, lam_rare, rho_stay2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure to determine common vs rare
    # Common: A->X, U->Y
    p_common = 0.7  # not directly used numerically, but to define common vs rare structure

    q1_mf = np.zeros(2)     # first-stage MF values for A/U
    q2 = np.zeros((2, 2))   # second-stage MF values
    pers2 = np.zeros((2, 2))  # second-stage perseveration traces

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 1 policy: MF values
        q1c = q1_mf - np.max(q1_mf)
        exp_q1 = np.exp(beta * q1c)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: MF with perseveration bias
        q2_eff = q2[s].copy() + rho_stay2 * pers2[s]
        q2c = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2c)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Determine whether the observed transition was common or rare
        # Common if (A->X) or (U->Y); Rare otherwise.
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        lam = lam_common if is_common else lam_rare

        # Update first-stage MF values with bootstrapped target plus transition-dependent eligibility from stage-2 PE
        # Bootstrapped value from the chosen second-stage action
        delta1_boot = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1_boot + (alpha * lam) * delta2

        # Update second-stage perseveration trace: one-hot in the visited state
        pers2[:, :] = 0.0
        pers2[s, a2] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll