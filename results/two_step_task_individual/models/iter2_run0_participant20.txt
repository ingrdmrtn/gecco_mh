def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free agent with eligibility trace and separate choice temperatures.

    This model combines model-based planning at stage 1 with a model-free (cached) stage-1 value.
    Stage-2 values are learned via temporal-difference learning. A backward eligibility trace
    propagates the stage-2 prediction error to stage-1 cached values. The first-stage action policy
    softmaxes a convex combination of model-based and model-free values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta1, beta2, w, lam]
        - alpha in [0,1]: learning rate for Q-value updates.
        - beta1 in [0,10]: inverse temperature for stage-1 softmax.
        - beta2 in [0,10]: inverse temperature for stage-2 softmax.
        - w in [0,1]: weight on model-based value in stage-1 (1 = fully model-based).
        - lam in [0,1]: eligibility trace strength to backpropagate stage-2 PE to stage-1 MF values.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, w, lam = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of the observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2) + 0.0          # model-free cached values for first-stage actions
    q2 = np.zeros((2, 2)) + 0.5        # second-stage Q-values (per state, two aliens)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first-stage action values from planned second-stage values
        max_q2 = np.max(q2, axis=1)           # value of best alien on each planet
        q1_mb = transition_matrix @ max_q2    # expected value per spaceship via transitions

        # Hybrid combination
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        q1c = q1_hybrid - np.max(q1_hybrid)
        p1 = np.exp(beta1 * q1c)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        s = state[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        p2 = np.exp(beta2 * q2c)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update stage-1 model-free values:
        #  - move toward currently realized second-stage action value (TD(0) toward q2[s,a2])
        #  - plus eligibility trace portion of stage-2 PE
        target_q1 = q2[s, a2]
        q1_mf[a1] += alpha * (target_q1 - q1_mf[a1]) + alpha * lam * pe2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with learned transition model and decaying perseveration kernels.

    The agent learns the state transition probabilities from experience and plans at stage 1
    using the learned transition matrix and current second-stage values. Additionally, the agent
    exhibits action perseveration at both stages, implemented as decaying choice kernels that
    bias the softmax logits toward recently chosen actions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta, eta, rho, decay]
        - alpha in [0,1]: learning rate for second-stage Q-value updates.
        - beta in [0,10]: inverse temperature shared by stage-1 and stage-2 softmax policies.
        - eta in [0,1]: learning rate for the transition matrix (row-wise update toward observed state).
        - rho in [0,1]: strength of perseveration bias added to logits (both stages use same weight).
        - decay in [0,1]: decay rate of choice kernels toward zero on each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, eta, rho, decay = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T[a, s] (rows sum to 1)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Second-stage Q-values
    q2 = np.zeros((2, 2)) + 0.5

    # Choice kernels for perseveration (decaying memory of recent choices)
    K1 = np.zeros(2)
    K2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based planning with learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add perseveration bias to logits
        logits1 = q1_mb + rho * K1
        l1c = logits1 - np.max(logits1)
        p1 = np.exp(beta * l1c)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        s = state[t]
        logits2 = q2[s] + rho * K2[s]
        l2c = logits2 - np.max(logits2)
        p2 = np.exp(beta * l2c)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Learning: second-stage TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Learning: transition matrix update for the executed first-stage action
        # Row-wise update toward the observed state s
        # Move probability mass toward observed state, and away from the other state
        T[a1, s] += eta * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] += eta * (0.0 - T[a1, other])

        # Update perseveration kernels with decay
        K1 *= (1.0 - decay)
        K2 *= (1.0 - decay)
        K1[a1] += 1.0
        K2[s, a2] += 1.0

        # Keep transition rows normalized (numerical safety)
        T[0] = T[0] / (np.sum(T[0]) + eps)
        T[1] = T[1] / (np.sum(T[1]) + eps)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with uncertainty-driven exploration and value forgetting.

    The agent plans at stage 1 using a fixed transition model but values second-stage actions
    by combining learned reward values with an uncertainty bonus that decreases with experience
    (UCB-style bonus). Second-stage Q-values also undergo gradual forgetting toward 0.5 to capture
    limited memory or volatility tracking. Stage-1 and stage-2 use separate softmax temperatures.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta1, beta2, zeta, phi]
        - alpha in [0,1]: learning rate for second-stage Q-value updates.
        - beta1 in [0,10]: inverse temperature for stage-1 softmax.
        - beta2 in [0,10]: inverse temperature for stage-2 softmax.
        - zeta in [0,1]: weight of the uncertainty bonus (higher favors less-explored aliens).
        - phi in [0,1]: forgetting rate driving Q-values toward 0.5 on each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, zeta, phi = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Second-stage Q-values and visit counts for uncertainty
    q2 = np.zeros((2, 2)) + 0.5
    counts = np.zeros((2, 2))  # number of times each alien has been queried

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Compute uncertainty bonus for each state-action
        # Bonus decreases with number of visits (initialized high)
        bonus = 1.0 / np.sqrt(counts + 1.0)

        # Stage-2 effective values include uncertainty bonus
        q2_eff = q2 + zeta * bonus

        # Model-based evaluation for stage-1 uses the best effective value on each planet
        max_q2_eff = np.max(q2_eff, axis=1)
        q1_mb = transition_matrix @ max_q2_eff

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        p1 = np.exp(beta1 * q1c)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy uses the same uncertainty-augmented values
        s = state[t]
        q2s_eff = q2_eff[s]
        q2c = q2s_eff - np.max(q2s_eff)
        p2 = np.exp(beta2 * q2c)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Apply forgetting toward 0.5 prior to learning update
        q2 = (1.0 - phi) * q2 + phi * 0.5

        # TD learning for the visited second-stage action
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update counts after the choice to reduce future bonus
        counts[s, a2] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll