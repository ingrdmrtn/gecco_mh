def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA(1) with asymmetric learning and choice perseveration at both stages.

    This model explains choices using a two-stage model-free learner that:
    - Uses different learning rates for positive vs. negative outcomes (asymmetric learning).
    - Propagates second-stage value back to the first-stage choice via a SARSA(1) target.
    - Includes perseveration (stickiness) biases at both stages that favor repeating the last chosen action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (e.g., 0=W/P, 1=S/H) for each trial.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha_pos, alpha_neg, beta, kappa1, kappa2]
        - alpha_pos (0..1): Learning rate used when reward >= current value (positive TD error).
        - alpha_neg (0..1): Learning rate used when reward < current value (negative TD error).
        - beta (0..10): Inverse temperature for softmax at both stages.
        - kappa1 (0..1): First-stage perseveration strength (bias added to previously chosen first-stage action).
        - kappa2 (0..1): Second-stage perseveration strength (bias added to previously chosen second-stage action within state).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha_pos, alpha_neg, beta, kappa1, kappa2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1 = np.zeros(2)        # Q at first stage (actions: 0=A, 1=U)
    q_stage2 = np.zeros((2, 2))   # Q at second stage (states: 0=X,1=Y; actions: 0,1)

    prev_a1 = None
    prev_a2 = [None, None]  # store last a2 for each state separately

    for t in range(n_trials):
        # First-stage policy with perseveration bias
        q1 = q_stage1.copy()
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa1
        logits1 = beta * (q1 + bias1 - np.max(q1 + bias1))
        exp1 = np.exp(logits1)
        probs_1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with state-contingent perseveration bias
        s2 = state[t]
        q2s = q_stage2[s2].copy()
        bias2 = np.zeros(2)
        if prev_a2[s2] is not None:
            bias2[prev_a2[s2]] += kappa2
        logits2 = beta * (q2s + bias2 - np.max(q2s + bias2))
        exp2 = np.exp(logits2)
        probs_2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Second-stage update with asymmetric learning rate
        td2 = r - q_stage2[s2, a2]
        alpha2 = alpha_pos if td2 >= 0 else alpha_neg
        q_stage2[s2, a2] += alpha2 * td2

        # First-stage update via SARSA(1) backup using updated second-stage value
        target1 = q_stage2[s2, a2]
        td1 = target1 - q_stage1[a1]
        alpha1 = alpha_pos if td1 >= 0 else alpha_neg
        q_stage1[a1] += alpha1 * td1

        # update perseveration memory
        prev_a1 = a1
        prev_a2[s2] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-weighted hybrid with learned transitions and value decay.

    This model blends model-based (MB) and model-free (MF) values at the first stage
    using a state-action-specific, uncertainty-driven weight. Transition probabilities
    are learned online. Second-stage values decay toward a neutral baseline to capture
    nonstationarity or memory leak.

    Mechanisms:
    - Learn transition structure p(s2 | a1) with a delta rule.
    - Compute MB Q1 as expected max Q2 under the learned transitions.
    - MF Q1 updated from second-stage value (SARSA(1)-style).
    - Blend MB and MF using a dynamic weight w(a1) proportional to transition uncertainty.
    - Apply value decay to Q2 toward 0.5 (uninformative baseline).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (e.g., 0=W/P, 1=S/H) for each trial.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha_q, beta, alpha_trans, decay, w_unc]
        - alpha_q (0..1): Learning rate for second-stage Q-values and first-stage MF backup.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - alpha_trans (0..1): Learning rate for transition probabilities.
        - decay (0..1): Per-trial decay of Q2 toward 0.5 (higher = more forgetting).
        - w_unc (0..1): Maximum contribution of MB control under high transition uncertainty.
                        The dynamic weight is w(a) = w_unc * (1 - 4*(p_a - 0.5)^2), where p_a is the
                        learned probability of the action's common transition.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha_q, beta, alpha_trans, decay, w_unc = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Learned transition parameters:
    # p_A_X: P(transition to X | choose A); p_U_Y: P(transition to Y | choose U)
    p_A_X = 0.5
    p_U_Y = 0.5

    for t in range(n_trials):
        # Apply decay to second-stage values toward 0.5
        q_stage2 = (1.0 - decay) * q_stage2 + decay * 0.5

        # Build current transition matrix from learned params
        # rows = a1 in [A(0), U(1)], cols = state [X(0), Y(1)]
        T = np.array([[p_A_X, 1.0 - p_A_X],
                      [1.0 - p_U_Y, p_U_Y]])

        # Model-based first-stage value: expected max over Q2
        max_q2 = np.max(q_stage2, axis=1)  # per state
        q1_mb = T @ max_q2

        # Dynamic uncertainty weight per action
        # uncertainty for A uses p_A_X, for U uses p_U_Y
        u_A = 1.0 - 4.0 * (p_A_X - 0.5) ** 2  # in [0,1]
        u_U = 1.0 - 4.0 * (p_U_Y - 0.5) ** 2
        wA = w_unc * u_A
        wU = w_unc * u_U
        w_vec = np.array([wA, wU])

        # Blend MF and MB
        q1_blend = (1.0 - w_vec) * q_stage1_mf + w_vec * q1_mb

        # Softmax for first-stage choice
        logits1 = beta * (q1_blend - np.max(q1_blend))
        exp1 = np.exp(logits1)
        probs_1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        q2s = q_stage2[s2].copy()
        logits2 = beta * (q2s - np.max(q2s))
        exp2 = np.exp(logits2)
        probs_2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learn transition probabilities for the chosen first-stage action
        if a1 == 0:
            target = 1.0 if s2 == 0 else 0.0
            p_A_X += alpha_trans * (target - p_A_X)
        else:
            target = 1.0 if s2 == 1 else 0.0
            p_U_Y += alpha_trans * (target - p_U_Y)

        # Second-stage value update
        td2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += alpha_q * td2

        # First-stage MF update via backup from updated second-stage value
        target1 = q_stage2[s2, a2]
        td1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_q * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free with transition-surprise bonus at stage 1 and counterfactual update at stage 2.

    This model captures a bias to favor or avoid rare transitions at the first stage
    and updates unchosen second-stage actions (counterfactual learning) within the visited state.

    Mechanisms:
    - First-stage policy is model-free but receives an additive bonus if the observed transition
      was rare given the chosen spaceship (surprise bonus).
    - Second-stage learning updates both chosen and unchosen actions: the unchosen action is
      nudged toward the obtained outcome with a reduced rate (counterfactual update).
    - First-stage MF value updated from second-stage value via eligibility trace lambda_et.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (e.g., 0=W/P, 1=S/H) for each trial.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha, beta, lambda_et, zeta_surprise, xi_cf]
        - alpha (0..1): Learning rate for value updates.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - lambda_et (0..1): Eligibility trace strength coupling second-stage value to first-stage MF update.
        - zeta_surprise (0..1): Strength of additive bonus to the chosen first-stage action after a rare transition.
                                Positive values favor repeating actions leading to rare transitions; negative penalize them.
        - xi_cf (0..1): Counterfactual update rate for the unchosen second-stage action in the visited state.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, lambda_et, zeta_surprise, xi_cf = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Stage-1 policy without surprise (surprise is applied post-transition as bias for this trial's likelihood)
        q1 = q_stage1.copy()

        # We compute probabilities using pre-transition values (decision time).
        logits1 = beta * (q1 - np.max(q1))
        exp1 = np.exp(logits1)
        probs_1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_pre = probs_1[a1]  # store pre-transition probability

        # Observe transition and apply surprise bonus to the chosen action for likelihood on this trial
        s2 = state[t]
        # Rare transition definition under fixed task structure (A->Y, U->X are rare)
        rare = (a1 == 0 and s2 == 1) or (a1 == 1 and s2 == 0)
        bonus = np.zeros(2)
        if rare:
            bonus[a1] += zeta_surprise

        # Recompute biased probabilities for likelihood accounting
        logits1_biased = beta * (q1 + bonus - np.max(q1 + bonus))
        exp1b = np.exp(logits1_biased)
        probs_1_biased = exp1b / (np.sum(exp1b) + 1e-12)
        p_choice_1[t] = probs_1_biased[a1]

        # Second-stage policy
        q2s = q_stage2[s2].copy()
        logits2 = beta * (q2s - np.max(q2s))
        exp2 = np.exp(logits2)
        probs_2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Second-stage chosen update
        td2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += alpha * td2

        # Counterfactual update for the unchosen action in the same state
        a2_other = 1 - a2
        td2_cf = r - q_stage2[s2, a2_other]
        q_stage2[s2, a2_other] += xi_cf * alpha * td2_cf

        # First-stage update via eligibility trace from the updated second-stage value
        target1 = q_stage2[s2, a2]
        td1 = target1 - q_stage1[a1]
        q_stage1[a1] += alpha * lambda_et * td1

        # Note: the surprise bonus affects policy (likelihood) but not stored Q-values,
        # modeling a transient, transition-dependent choice bias.

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll