Below are three standalone cognitive models that compute the negative log-likelihood of the observed choices in the two-step task. Each model adheres to the specified constraints and uses all parameters meaningfully.

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Adaptive arbitration (MB vs MF) with lapse noise.
    
    A hybrid controller where the model-based (MB) vs. model-free (MF) weight
    adapts online based on second-stage uncertainty (entropy of the Q-values).
    The arbitration weight is anchored by a baseline MB tendency and modulated
    by an uncertainty sensitivity parameter. Decisions at both stages include a
    lapse component that injects uniform random choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial: 0/1 (aliens on that planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome received on each trial.
    model_parameters : list or array
        [alpha, beta, mb0, omega, epsilon]
        - alpha in [0,1]: learning rate for MF Q-values (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - mb0 in [0,1]: baseline weight on model-based value at stage 1.
        - omega in [0,1]: sensitivity of MB weight to second-stage uncertainty
                          (higher uncertainty increases MB reliance).
        - epsilon in [0,1]: lapse rate; probability of random choice at both stages.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, mb0, omega, epsilon = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3],  # A -> X (common), Y (rare)
                                  [0.3, 0.7]]) # U -> X (rare),   Y (common)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)         # stage-1 MF values for A/U
    q2 = np.zeros((2, 2))       # stage-2 MF values for each state and action

    for t in range(n_trials):
        # Compute MB values at stage 1 from MF stage-2 values
        max_q2_per_state = np.max(q2, axis=1)   # [V(X), V(Y)]
        q1_mb = transition_matrix @ max_q2_per_state  # 2 actions -> expected value via transitions

        # Stage-2 uncertainty (entropy of softmax over q2 in the reached state will be used,
        # but to compute arbitration weight before observing state, we use average entropy across states)
        # Compute entropy across both states and average
        entropies = np.zeros(2)
        for s_tmp in (0, 1):
            q_tmp = q2[s_tmp]
            logits = beta * (q_tmp - np.max(q_tmp))
            probs_tmp = np.exp(logits)
            probs_tmp = probs_tmp / np.sum(probs_tmp) if np.sum(probs_tmp) > 0 else np.array([0.5, 0.5])
            # Binary entropy scaled to [0,1] by dividing by log(2)
            eps_h = 1e-12
            ent = -(probs_tmp * np.log(probs_tmp + eps_h)).sum() / np.log(2 + eps_h)
            entropies[s_tmp] = ent
        H = np.mean(entropies)           # in [0,1]
        # Arbitration weight: higher uncertainty -> more MB weight
        w_t = mb0 + omega * (H - 0.5) * 2.0  # center around 0.5, scale to [-omega, +omega]
        w_t = max(0.0, min(1.0, w_t))        # clip to [0,1]

        # Hybrid action values at stage 1
        q1_hybrid = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage-1 policy with lapse
        logits1 = beta * (q1_hybrid - np.max(q1_hybrid))
        soft1 = np.exp(logits1); soft1 = soft1 / np.sum(soft1) if np.sum(soft1) > 0 else np.array([0.5, 0.5])
        probs_1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with lapse (condition on reached state)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        soft2 = np.exp(logits2); soft2 = soft2 / np.sum(soft2) if np.sum(soft2) > 0 else np.array([0.5, 0.5])
        probs_2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates (purely MF updates; MB only used for evaluation)
        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update toward the (updated) stage-2 value of the taken path
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid with learned transition model (MB via learned T) and MF caching.
    
    The agent learns both: (i) second-stage MF action values and (ii) the
    first-stage transition probabilities from experience. First-stage choices
    combine a model-based evaluation using the learned transition matrix and a
    model-free cached value. No perseveration or lapse terms.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choice on each trial (given the state).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : list or array
        [alpha_q, beta, alpha_t, w_mb]
        - alpha_q in [0,1]: learning rate for MF Q-values at stage 2 and MF stage-1 cache.
        - beta in [0,10]: inverse temperature for softmax.
        - alpha_t in [0,1]: learning rate for transition probabilities T(action -> state).
        - w_mb in [0,1]: weight mixing MB value (from learned T) with MF stage-1 cache.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_q, beta, alpha_t, w_mb = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize learned transitions T[a, s] ~ uniform (uninformative)
    T = np.ones((2, 2)) * 0.5
    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        # MB evaluation from learned T and current MF stage-2 values
        v2 = np.max(q2, axis=1)         # [V(X), V(Y)]
        q1_mb = T @ v2                  # expected value under learned transitions
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(logits1); probs_1 = probs_1 / np.sum(probs_1) if np.sum(probs_1) > 0 else np.array([0.5, 0.5])
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (conditioned on reached state)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs_2 = np.exp(logits2); probs_2 = probs_2 / np.sum(probs_2) if np.sum(probs_2) > 0 else np.array([0.5, 0.5])
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: update transitions for the chosen action based on observed state
        # Move T[a1] toward one-hot of observed state s
        onehot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = (1.0 - alpha_t) * T[a1] + alpha_t * onehot_s
        # Ensure normalization
        T[a1] = T[a1] / np.sum(T[a1]) if np.sum(T[a1]) > 0 else np.array([0.5, 0.5])

        # MF value updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * delta2

        # Update MF stage-1 cache toward obtained stage-2 value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor Representation (SR) at stage 1 with MF stage-2 learning.
    
    The agent learns a one-step successor representation (state occupancy map)
    for each first-stage action and uses it to predict the expected future value
    by combining SR with current stage-2 MF values. A mixing weight balances SR
    evaluation with a model-free cached stage-1 value. This captures partial
    knowledge of the transition structure without explicit transition learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward outcome per trial.
    model_parameters : list or array
        [alpha_r, beta, tau_sr, zeta_sr]
        - alpha_r in [0,1]: learning rate for MF reward values (stage-2) and MF stage-1 cache.
        - beta in [0,10]: inverse temperature for softmax decisions.
        - tau_sr in [0,1]: learning rate for the successor representation M[action, state].
        - zeta_sr in [0,1]: weight on SR-based evaluation at stage 1 (vs MF cache).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, beta, tau_sr, zeta_sr = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Successor Representation M[a, s] estimates P(s | a) in this two-step task
    M = np.ones((2, 2)) * 0.5  # initialize uninformatively
    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        # SR-based evaluation: expected value of each first-stage action
        v2 = np.max(q2, axis=1)     # value of each state from best second-stage action
        q1_sr = M @ v2              # SR expectation over states
        q1 = zeta_sr * q1_sr + (1.0 - zeta_sr) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(logits1); probs_1 = probs_1 / np.sum(probs_1) if np.sum(probs_1) > 0 else np.array([0.5, 0.5])
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs_2 = np.exp(logits2); probs_2 = probs_2 / np.sum(probs_2) if np.sum(probs_2) > 0 else np.array([0.5, 0.5])
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update SR for the chosen first-stage action toward observed state occupancy
        onehot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        M[a1] = (1.0 - tau_sr) * M[a1] + tau_sr * onehot_s
        # Normalize to maintain a proper distribution
        M[a1] = M[a1] / np.sum(M[a1]) if np.sum(M[a1]) > 0 else np.array([0.5, 0.5])

        # MF learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss