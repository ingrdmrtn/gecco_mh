def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with learned transitions and second-stage choice stickiness.

    This model jointly learns:
    - Second-stage MF values via TD learning.
    - The first-stage transition probabilities via a delta-rule (participants may
      mislearn the A->X and U->Y structure).
    - First-stage action values combine MB planning using the learned transition
      matrix with MF first-stage values propagated from stage-2 via eligibility=1.
    - Second-stage choices exhibit a within-state stickiness (tendency to repeat
      the last second-stage action in the same planet).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (planet) reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien) on each trial (0/1, interpreted within each state).
    reward : array-like of float
        Reward (coins) received on each trial (e.g., 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha_q, beta, w_mb, gamma_tr, kappa2]
        - alpha_q (in [0,1]): Learning rate for MF Q-values at stage 2 (and for the
          stage-1 MF eligibility update).
        - beta (in [0,10]): Inverse temperature for softmax at both stages.
        - w_mb (in [0,1]): Weight of model-based values at the first stage.
        - gamma_tr (in [0,1]): Learning rate for updating the first-stage transition
          probabilities P(planet | spaceship) via a delta rule.
        - kappa2 (in [0,1]): Second-stage stickiness bias; added to the previously
          chosen action in the same state.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_q, beta, w_mb, gamma_tr, kappa2 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition probabilities: rows=spaceship, cols=planet
    # Start from an uninformative prior (0.5/0.5).
    T_hat = np.full((2, 2), 0.5)

    # Model-free Q-values
    q1_mf = np.zeros(2)          # stage-1 MF Q
    q2_mf = np.zeros((2, 2))     # stage-2 MF Q: rows=state, cols=action

    # For second-stage stickiness, keep last action per state
    last_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage Q from learned transitions and MF V at stage 2
        v2 = np.max(q2_mf, axis=1)              # value of each state from MF
        q1_mb = T_hat @ v2                      # MB evaluation using learned T

        # Combine MB and MF at stage 1
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Softmax for stage 1
        a1 = action_1[t]
        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with within-state stickiness
        s = state[t]
        bias2 = np.zeros(2)
        if last_a2[s] != -1:
            bias2[last_a2[s]] += kappa2

        q2 = q2_mf[s, :] + bias2
        q2_centered = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD learning at stage 2
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha_q * delta2

        # Eligibility propagation (Î»=1) to stage-1 MF value of chosen a1
        q1_mf[a1] += alpha_q * delta2

        # Update learned transitions for chosen action via delta rule
        # Move the chosen action's row toward the observed state indicator
        for sp in [0, 1]:
            if sp == a1:
                # Target distribution is one-hot at observed state
                for ss in [0, 1]:
                    target = 1.0 if ss == s else 0.0
                    T_hat[sp, ss] += gamma_tr * (target - T_hat[sp, ss])
                # Renormalize to avoid drift due to numerical issues
                row_sum = T_hat[sp, 0] + T_hat[sp, 1]
                if row_sum > 0:
                    T_hat[sp, :] /= row_sum
            else:
                # Gentle decay toward an uninformative 0.5/0.5 when unchosen
                for ss in [0, 1]:
                    T_hat[sp, ss] += gamma_tr * (0.5 - T_hat[sp, ss])
                T_hat[sp, :] /= (T_hat[sp, 0] + T_hat[sp, 1])

        # Update stickiness memory
        last_a2[s] = a2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free SARSA-style with reward sensitivity, lapse, and first-stage stickiness.

    This model uses purely model-free learning with bootstrapping from stage-2 to stage-1.
    It includes:
    - Reward sensitivity (scales objective reward before learning).
    - A lapse process that injects stimulus-independent random responding at both stages.
    - First-stage stickiness (tendency to repeat the previous first-stage action).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1) within each state.
    reward : array-like of float
        Reward received (e.g., 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha, beta, eta_r, kappa1, epsilon]
        - alpha (in [0,1]): Learning rate for MF updates at both stages.
        - beta (in [0,10]): Inverse temperature for softmax at both stages.
        - eta_r (in [0,1]): Reward sensitivity; observed reward is transformed as r' = eta_r * r
          before computing the TD error.
        - kappa1 (in [0,1]): First-stage stickiness bias added to the previously chosen first-stage action.
        - epsilon (in [0,1]): Lapse rate; with probability epsilon, choice is uniform random, otherwise softmax.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, eta_r, kappa1, epsilon = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)            # stage-1 MF Q
    q2 = np.zeros((2, 2))       # stage-2 MF Q: rows=state, cols=action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1

    for t in range(n_trials):
        # First-stage stickiness bias
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        # Stage 1 policy with lapse
        a1 = action_1[t]
        q1_eff = q1 + bias1
        q1_c = q1_eff - np.max(q1_eff)
        soft1 = np.exp(beta * q1_c)
        soft1 = soft1 / np.sum(soft1)
        probs_1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with lapse
        s = state[t]
        a2 = action_2[t]
        q2_s = q2[s, :]
        q2_c = q2_s - np.max(q2_s)
        soft2 = np.exp(beta * q2_c)
        soft2 = soft2 / np.sum(soft2)
        probs_2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        p_choice_2[t] = probs_2[a2]

        # Learning with reward sensitivity
        r = eta_r * reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 bootstrapping toward current stage-2 value of chosen a2 in reached state
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

        # Update stickiness memory
        prev_a1 = a1

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with transition-contingent credit assignment and action bias.

    This model assumes the canonical transition structure is known (A->X, U->Y common).
    It blends model-based and model-free values at stage 1, but down-weights learning
    from rare transitions via a transition-contingent gating parameter. It also includes
    a constant bias favoring spaceship A.

    Components:
    - MB values: expected value from the fixed transition matrix.
    - MF values: learned via TD; stage-1 MF updated from stage-2 RPE.
    - Transition-contingent gating: if the observed transition was rare, scale the TD
      error used for learning by (1 - chi), reducing learning from surprising outcomes.
    - Constant action bias toward spaceship A at stage 1.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1) within each state.
    reward : array-like of float
        Reward received (e.g., 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha, beta, omega, chi, bA]
        - alpha (in [0,1]): Learning rate for MF Q-values at both stages.
        - beta (in [0,10]): Inverse temperature for softmax (both stages).
        - omega (in [0,1]): Weight on MB values at stage 1 (1=MB, 0=MF).
        - chi (in [0,1]): Transition-contingent learning gate. Learning is multiplied by
          g=1 for common transitions and g=(1-chi) for rare transitions.
        - bA (in [0,1]): Constant bias added to the value of spaceship A (action 0) at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, omega, chi, bA = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common=0.7)
    T = np.array([[0.7, 0.3],   # A -> X common
                  [0.3, 0.7]])  # U -> Y common

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB values at stage 1 from fixed transitions and current stage-2 values
        v2 = np.max(q2_mf, axis=1)
        q1_mb = T @ v2

        # Add constant bias toward A
        bias = np.array([bA, 0.0])

        # Combine MB and MF
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf + bias

        # Stage 1 softmax
        a1 = action_1[t]
        q1_c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 softmax
        s = state[t]
        a2 = action_2[t]
        q2 = q2_mf[s, :]
        q2_c = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Determine if the transition on this trial was common or rare for the chosen a1
        # Common if s is the higher-probability state for the chosen action.
        is_common = (T[a1, s] >= 0.5)
        gate = 1.0 if is_common else (1.0 - chi)

        # Stage-2 TD update with transition-contingent gating
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * gate * delta2

        # Propagate gated TD error to stage-1 MF value for chosen a1
        q1_mf[a1] += alpha * gate * delta2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik