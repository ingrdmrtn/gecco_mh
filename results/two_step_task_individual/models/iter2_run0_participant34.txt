Below are three distinct cognitive models tailored to the two-step task. Each returns the negative log-likelihood of the observed choices. They explore different mechanisms than those you’ve already tried, including learned transitions, surprise-driven choice biases, transition-outcome interaction weights, and state-2 uncertainty bonuses (UCB).

Note: Assume numpy as np is already imported. All parameters are used and constrained as requested: learning/weight parameters in [0,1], beta in [0,10], and total parameters ≤ 5 per model.

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model-based with learned transitions, value decay, and surprise-driven bias.
    
    This model learns the transition structure online, performs MB planning using the learned T,
    updates stage-2 values model-free, applies leaky decay to unchosen Q-values toward a neutral prior,
    and adds a surprise-driven bias to stage-1 choice when the observed transition was unexpected.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial: 0/1 for the two aliens within a planet.
    reward : array-like of float (0/1)
        Reward outcomes per trial.
    model_parameters : list or tuple of 5 floats
        [alpha_q, beta, alpha_T, decay, surprise]
        - alpha_q in [0,1]: MF learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_T in [0,1]: learning rate for transition probabilities T(a1, s).
        - decay in [0,1]: leaky decay toward 0.5 for unchosen Q-values at stage 2.
        - surprise in [0,1]: strength of adding transition surprise to the chosen stage-1 action’s logit.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_q, beta, alpha_T, decay, surprise_w = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix (rows=actions, cols=states)
    # Start neutral at 0.5/0.5
    T = np.full((2, 2), 0.5)

    # Stage-2 model-free Q-values
    q2 = np.zeros((2, 2)) + 0.5  # start at neutral 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage action values via learned transitions
        max_q2 = np.max(q2, axis=1)  # value of each state
        q1_mb = T @ max_q2  # expected value per first-stage action

        # Surprise signal for the actually chosen a1 given observed state s
        # Surprise = 1 - P(s | a1); larger when transition is unlikely under current T.
        trans_prob = T[a1, s]
        surprise = 1.0 - trans_prob

        # Stage-1 policy: softmax over MB values + surprise bias on chosen action's logit
        logits1 = beta * q1_mb
        logits1[a1] += surprise_w * surprise  # apply surprise-driven boost to the chosen action
        # To get the choice probability of the chosen action, compute full softmax (without leakage)
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: softmax over MF Q-values
        logits2 = beta * q2[s].copy()
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 update (MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Leaky decay of unchosen Q-values at stage-2 toward 0.5
        for a in (0, 1):
            if a != a2:
                q2[s, a] = (1 - decay) * q2[s, a] + decay * 0.5

        # Transition learning via delta rule toward one-hot observed state
        # For chosen a1, move T[a1] toward [1,0] if s=0 or [0,1] if s=1
        target = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1] = (1 - alpha_T) * T[a1] + alpha_T * target
        # Keep rows normalized (should be already), safeguard:
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with transition-outcome interaction and baseline action bias.
    
    Stage-2 values are learned model-free. Stage-1 value is a weighted sum of MB and MF
    components, but the MB weight is modulated by whether the observed transition is common
    or rare (transition-outcome interaction). A baseline bias toward spaceship A is added to
    the stage-1 logits.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0/1)
        Reward outcomes per trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, w_base, delta_rare, biasA]
        - alpha in [0,1]: MF learning rate (both stages’ MF components).
        - beta in [0,10]: inverse temperature for both stages.
        - w_base in [0,1]: baseline MB weight in stage-1 value combination.
        - delta_rare in [0,1]: modulation of MB weight depending on transition type;
          increases MB weight on common transitions and decreases it on rare ones.
        - biasA in [0,1]: additive bias to the stage-1 logit of choosing action 0 (spaceship A).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, w_base, delta_rare, biasA = model_parameters
    n_trials = len(action_1)

    # Fixed known transition structure (common=0.7)
    T_fixed = np.array([[0.7, 0.3],  # P(state | action1=0)
                        [0.3, 0.7]]) # P(state | action1=1)

    # MF values
    q1_mf = np.zeros(2)            # stage-1 MF values
    q2 = np.zeros((2, 2)) + 0.5    # stage-2 MF values start neutral

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage values from fixed T and current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Determine if the observed transition (a1 -> s) was common or rare under fixed T
        is_common = 1.0 if T_fixed[a1, s] >= 0.5 else 0.0
        # Effective MB weight: raise on common, lower on rare
        w_eff = w_base + delta_rare * (1.0 if is_common > 0.5 else -1.0)
        # Clip to [0,1] to keep a valid convex combination
        w_eff = min(1.0, max(0.0, w_eff))

        # Combine MB and MF at stage-1
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage-1 policy with bias toward action 0 (spaceship A)
        logits1 = beta * q1
        logits1[0] += biasA
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s].copy()
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update via bootstrapping from the observed stage-2 value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-gated MB/MF with learned transitions and UCB exploration at stage-2.
    
    The agent learns transitions online. The MB weight at stage-1 is gated by the
    uncertainty (entropy) of the transition row for the chosen action: higher uncertainty
    reduces reliance on MB control. Stage-2 policy includes an uncertainty bonus (UCB)
    favoring less-tried actions. Stage-1 MF values are updated from stage-2 outcomes.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0/1)
        Reward outcomes per trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, alpha_T, phi, c_ucb]
        - alpha in [0,1]: MF learning rate (stage-2) and stage-1 MF bootstrapping.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_T in [0,1]: learning rate for transition probabilities T(a1, s).
        - phi in [0,1]: exponent shaping uncertainty gating of MB weight; MB weight = (1 - H)^phi.
                        H is normalized entropy of the action's transition row.
        - c_ucb in [0,1]: strength of UCB bonus added to stage-2 action values: bonus = c/sqrt(n+1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, alpha_T, phi, c_ucb = model_parameters
    n_trials = len(action_1)

    # Learned transition matrix initialized to uniform
    T = np.full((2, 2), 0.5)

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2)) + 0.5

    # Visit counts for UCB at stage-2
    n_visits = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    def normalized_entropy(p_row):
        # For binary transitions, max entropy = log(2); we normalize to [0,1]
        p = np.clip(p_row, eps, 1 - eps)
        H = -(p[0] * np.log(p[0]) + p[1] * np.log(p[1]))
        H_max = np.log(2.0)
        return H / H_max

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based stage-1 values from learned T and current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Uncertainty-gated MB weight for the chosen action a1
        H_row = normalized_entropy(T[a1])
        w_eff = (1.0 - H_row) ** max(0.0, min(1.0, phi))  # ensure exponent is within bounds

        # Combine MB and MF for all actions, but the gating is computed based on chosen action row.
        # Apply the same w_eff to both actions to form a consistent policy on this trial.
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        exp1 = np.exp(logits1 - np.max(logits1))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with UCB bonus
        bonus = c_ucb / np.sqrt(n_visits[s] + 1.0)
        logits2 = beta * (q2[s] + bonus)
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Update visit counts
        n_visits[s, a2] += 1.0

        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update via bootstrapping from stage-2
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Transition learning toward observed state
        target = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1] = (1 - alpha_T) * T[a1] + alpha_T * target
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss