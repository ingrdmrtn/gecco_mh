def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Transition-learning, uncertainty-adaptive temperature, model-based at stage 1.

    The agent:
    - Learns second-stage Q-values model-free from rewards.
    - Learns the first-stage transition probabilities online.
    - Uses model-based planning at stage 1 with the learned transition matrix.
    - Adapts the stage-1 inverse temperature to average transition uncertainty:
        beta1_t = beta0 * (1 - k_unc * mean_u), where u(a) = p(a)*(1-p(a)).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A; 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 = first alien, 1 = second alien).
    reward : array-like of float (e.g., 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_r, eta_T, k_unc, beta0, beta2]
        - alpha_r: [0, 1] learning rate for second-stage model-free values
        - eta_T: [0, 1] learning rate for first-stage transition probabilities
        - k_unc: [0, 1] strength of uncertainty-based temperature reduction
        - beta0: [0, 10] baseline inverse temperature for stage 1
        - beta2: [0, 10] inverse temperature at stage 2

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, eta_T, k_unc, beta0, beta2 = model_parameters
    n_trials = len(action_1)

    # Learned transition probabilities: pT[a] = P(state=0 | action=a)
    pT = np.array([0.5, 0.5], dtype=float)

    # Second-stage MF values
    q2 = np.zeros((2, 2))  # state x action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Build current transition matrix from learned probabilities
        T = np.array([[pT[0], 1.0 - pT[0]],
                      [pT[1], 1.0 - pT[1]]])

        # Stage-1 model-based Q from learned T and q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Uncertainty-adaptive temperature (use average of action uncertainties)
        u = pT * (1.0 - pT)               # uncertainty per action
        mean_u = 0.5 * (u[0] + u[1])
        beta1_t = beta0 * (1.0 - k_unc * mean_u)
        if beta1_t < 0.0:
            beta1_t = 0.0

        # Stage-1 policy
        a1 = action_1[t]
        q1c = q1_mb - np.max(q1_mb)
        exp_q1 = np.exp(beta1_t * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        a2 = action_2[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        exp_q2 = np.exp(beta2 * q2c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Outcomes
        r = reward[t]

        # Update learned transitions for the chosen action using indicator of s==0
        target_p = 1.0 if s == 0 else 0.0
        pT[a1] += eta_T * (target_p - pT[a1])
        # Keep within [0,1]
        if pT[a1] < 0.0:
            pT[a1] = 0.0
        elif pT[a1] > 1.0:
            pT[a1] = 1.0

        # Update second-stage values (MF)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Model-free with a dynamic choice kernel at stage 1.

    The agent:
    - Learns MF values for both stages.
    - Augments stage-1 decision values with a recency-weighted choice kernel that captures
      inertia/exploration dynamics independent of reward.
    - No explicit model-based planning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A; 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 = first alien, 1 = second alien).
    reward : array-like of float (e.g., 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_r, alpha_ck, w_ck, beta1, beta2]
        - alpha_r: [0, 1] learning rate for model-free values
        - alpha_ck: [0, 1] learning rate for the choice kernel (recency of last choice)
        - w_ck: [0, 1] weight of the choice kernel in stage-1 decision values
        - beta1: [0, 10] inverse temperature at stage 1
        - beta2: [0, 10] inverse temperature at stage 2

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, alpha_ck, w_ck, beta1, beta2 = model_parameters
    n_trials = len(action_1)

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))  # state x action

    # Choice kernel (preference for repeating recent first-stage choices)
    ck = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage-1 decision values: MF plus centered choice kernel
        ck_centered = ck - np.mean(ck)
        q1 = (1.0 - w_ck) * q1_mf + w_ck * ck_centered

        # Stage-1 policy
        a1 = action_1[t]
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        a2 = action_2[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        exp_q2 = np.exp(beta2 * q2c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Update second-stage MF values
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Propagate value to stage-1 MF
        target1 = q2[s, a2]
        q1_mf[a1] += alpha_r * (target1 - q1_mf[a1])

        # Update choice kernel: decay and reinforce chosen action
        ck *= (1.0 - alpha_ck)
        ck[a1] += alpha_ck

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Counterfactual updating with value decay (forgetting).

    The agent:
    - Is model-free at both stages.
    - Applies leaky forgetting to values at both stages each trial.
    - Updates the unchosen second-stage action in the visited state via counterfactual
      learning toward the received reward, scaled by cf.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A; 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 = first alien, 1 = second alien).
    reward : array-like of float (e.g., 0 or 1)
        Coins received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, cf, decay, beta1, beta2]
        - alpha: [0, 1] learning rate for model-free updates
        - cf: [0, 1] counterfactual strength for unchosen second-stage action
        - decay: [0, 1] per-trial forgetting applied to all Q-values
        - beta1: [0, 10] inverse temperature at stage 1
        - beta2: [0, 10] inverse temperature at stage 2

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, cf, decay, beta1, beta2 = model_parameters
    n_trials = len(action_1)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))  # state x action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Apply forgetting before computing policy (Agent's current memory)
        q1_mf *= (1.0 - decay)
        q2 *= (1.0 - decay)

        # Stage-1 policy (pure MF)
        a1 = action_1[t]
        q1c = q1_mf - np.max(q1_mf)
        exp_q1 = np.exp(beta1 * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        a2 = action_2[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        exp_q2 = np.exp(beta2 * q2c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Second-stage chosen action update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Counterfactual update for unchosen second-stage action in the visited state
        a2_unc = 1 - a2
        delta2_unc = r - q2[s, a2_unc]
        q2[s, a2_unc] += alpha * cf * delta2_unc

        # Propagate value to stage-1 MF
        target1 = q2[s, a2]
        q1_mf[a1] += alpha * (target1 - q1_mf[a1])

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik