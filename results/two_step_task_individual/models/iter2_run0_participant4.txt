def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Hybrid arbitration via transition uncertainty (MB-MF blend) with learned transitions.

    Idea
    ----
    - Learn second-stage alien values model-free (single learning rate).
    - Learn the first-stage transition model P(state | action_1) online.
    - Compute a trial-wise model-based weight w_t that decreases as the transition model becomes
      more certain (i.e., arbitration favors MF when transitions are well-known, and MB when uncertain).
    - First-stage policy uses hybrid Q1 = w_t * Q1_MB + (1 - w_t) * Q1_MF.
    - Second-stage policy is model-free over Q2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state visited per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 for the two aliens on that planet).
    reward : array-like of float
        Reward obtained per trial (e.g., number of coins).
    model_parameters : tuple/list of 5 floats
        (alpha_q, beta, alpha_tr, w0, zeta)
        - alpha_q in [0,1]: learning rate for model-free Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax choice (both stages).
        - alpha_tr in [0,1]: learning rate for transition model P(s|a).
        - w0 in [0,1]: baseline MB weight.
        - zeta in [0,1]: arbitration sensitivity to transition uncertainty (higher -> more MB when uncertain).

    Notes
    -----
    - Transition uncertainty per action is quantified by the normalized entropy H(a) of P(s|a) in [0,1].
      H(a) = -sum_s P(s|a) log2 P(s|a) / 1 (since max entropy for 2 states is 1 bit).
    - Trial MB weight: w_t = clip(w0 * (1 - zeta * H_bar), 0, 1), where H_bar is the mean entropy across actions.
    - Q1_MF is learned toward the realized stage-2 action value (one-step TD).
    """
    alpha_q, beta, alpha_tr, w0, zeta = model_parameters
    n_trials = len(action_1)

    eps = 1e-12

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q1_mf = np.zeros(2)        # stage-1 MF action values
    q2 = np.zeros((2, 2))      # stage-2 MF values per state and alien

    # Transition model P(s|a), initialized uninformative
    P = np.full((2, 2), 0.5)

    for t in range(n_trials):
        # Compute model-based Q1 from current transition model and Q2
        max_q2 = np.max(q2, axis=1)      # per state
        q1_mb = P @ max_q2               # expected value per first-stage action

        # Arbitration weight based on transition uncertainty (entropy)
        # Compute normalized entropy for each action (base-2)
        H = np.zeros(2)
        for a in range(2):
            pa = P[a].clip(eps, 1.0)
            H[a] = -np.sum(pa * (np.log(pa) / np.log(2)))
        H_bar = 0.5 * (H[0] + H[1])      # average entropy in [0,1]
        w_t = w0 * (1.0 - zeta * H_bar)
        w_t = np.minimum(1.0, np.maximum(0.0, w_t))

        # Hybrid Q1
        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: stage 2 (MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Learning: stage 1 MF (back up observed second-stage chosen action value)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # Learn transitions P(s|a1) toward observed state s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        P[a1] += alpha_tr * (target - P[a1])

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Pure model-free with eligibility traces, forgetting, and reward sensitivity.

    Idea
    ----
    - No explicit transition learning or model-based planning.
    - Use eligibility traces that assign delayed credit from second-stage reward to first-stage choices.
    - Include value forgetting (decay toward zero) for all Q-values each trial.
    - Reward sensitivity scales the experienced reward before learning, dissociating it from choice stochasticity.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state visited per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward per trial.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, xi, phi, gamma)
        - alpha in [0,1]: learning rate for TD updates.
        - beta in [0,10]: inverse temperature for softmax policy (both stages).
        - xi in [0,1]: eligibility trace decay (higher -> longer-lasting credit).
        - phi in [0,1]: forgetting rate applied to all Q-values each trial.
        - gamma in [0,1]: reward sensitivity scaling the reward used in learning (r_eff = gamma * r).

    Notes
    -----
    - Traces e1 (size 2) and e2 (size 2x2) are decayed every trial by xi, then incremented for chosen actions.
    - A single TD error at the outcome updates both stages weighted by their traces.
    - Forgetting is applied multiplicatively to all Q-values each trial.
    """
    alpha, beta, xi, phi, gamma = model_parameters
    n_trials = len(action_1)

    eps = 1e-12

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q1 = np.zeros(2)        # stage-1 MF action values
    q2 = np.zeros((2, 2))   # stage-2 MF action values

    # Eligibility traces
    e1 = np.zeros(2)
    e2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Policies
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Apply forgetting (decay toward zero) to all Q-values
        q1 *= (1.0 - phi)
        q2 *= (1.0 - phi)

        # Eligibility traces: decay then set chosen traces
        e1 *= xi
        e2 *= xi
        e1[a1] += 1.0
        e2[s, a2] += 1.0

        # Outcome and TD error at stage 2
        r_eff = gamma * reward[t]
        delta = r_eff - q2[s, a2]

        # Update values with traces
        q2 += alpha * delta * e2
        q1 += alpha * delta * e1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Bayesian-like transition learning with surprise-modulated exploration and hybrid control.

    Idea
    ----
    - Maintain Dirichlet-like counts for transitions per first-stage action; derive P(s|a) from counts.
    - Model-based evaluation at stage 1 from learned P(s|a) and MF Q2 values.
    - Hybrid between MB and MF at stage 1 with a fixed mixing weight omega.
    - Surprise-adaptive exploration: stage-1 inverse temperature is reduced on surprising (rare) transitions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state visited per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward per trial.
    model_parameters : tuple/list of 5 floats
        (alpha2, beta, alpha_tr, sigma, omega)
        - alpha2 in [0,1]: learning rate for second-stage MF Q-values.
        - beta in [0,10]: base inverse temperature.
        - alpha_tr in [0,1]: increment size for Dirichlet-like transition counts.
        - sigma in [0,1]: surprise sensitivity; higher reduces beta more on surprising transitions.
        - omega in [0,1]: model-based weight in hybrid Q1 (omega=1 purely MB, 0 purely MF).

    Notes
    -----
    - Counts initialized to [1,1] for each action (uniform prior). After action a1 and state s,
      counts[a1, s] += alpha_tr, then P = counts / counts.sum(axis=1, keepdims=True).
    - Surprise = 1 - P(prev_a1, prev_s) using P before the update on that trial.
      The stage-1 beta for that trial is beta_t = max(eps, beta * (1 - sigma * surprise)).
    - Q1_MF is learned toward Q2[s, a2] with a simple TD step.
    """
    alpha2, beta, alpha_tr, sigma, omega = model_parameters
    n_trials = len(action_1)

    eps = 1e-12

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Transition counts and probabilities
    counts = np.ones((2, 2))  # Dirichlet prior [1,1] for each action
    P = counts / counts.sum(axis=1, keepdims=True)

    for t in range(n_trials):
        # Compute model-based Q1 from current P and Q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = P @ max_q2

        # Surprise based on the action-state pair about to be experienced (using current P)
        a1 = action_1[t]
        s = state[t]
        surprise = 1.0 - P[a1, s]
        beta_t = beta * (1.0 - sigma * surprise)
        if beta_t < eps:
            beta_t = eps

        # Hybrid Q1
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta_t * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second-stage MF values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Update first-stage MF toward realized second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Update transition counts and probabilities (Bayesian-like)
        counts[a1, s] += alpha_tr
        P = counts / counts.sum(axis=1, keepdims=True)

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss