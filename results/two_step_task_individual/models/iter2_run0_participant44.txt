def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with learned transitions, confidence-weighted arbitration, and first-stage perseveration.
    
    This model learns second-stage values (model-free) and the first-stage transition
    structure. The first-stage decision blends model-based and model-free values
    with a dynamic weight that increases with transition confidence. A simple
    perseveration bias at stage 1 favors repeating the previous first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage action within the reached state: 0/1 are the two aliens.
    reward : array-like of float (e.g., 0 or 1)
        Obtained reward on each trial.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, tau, chi, psi]
        - alpha in [0,1]: Learning rate for model-free values at both stages.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - tau in [0,1]: Transition learning rate (updates P(state | action_1)).
        - chi in [0,1]: Scales arbitration toward model-based control as transition
                        confidence increases.
        - psi in [0,1]: First-stage perseveration bias added to the previously
                        chosen first-stage action's value.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, tau, chi, psi = model_parameters
    n_trials = len(action_1)

    # Model-free Q-values
    Q1_mf = np.zeros(2)        # first-stage MF values
    Q2 = np.zeros((2, 2))      # second-stage MF values: Q2[state, action_2]

    # Learned transition model: T[a, s] ~ P(s | a)
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1

    for t in range(n_trials):
        s = state[t]

        # Model-based action values at stage 1 using current transition model
        maxQ2_per_state = np.max(Q2, axis=1)  # shape (2,)
        Q1_mb = T @ maxQ2_per_state           # shape (2,)

        # Confidence of transitions per action: 2*|p-0.5| in [0,1]
        conf = np.abs(T[:, 0] - 0.5) * 2.0    # shape (2,)
        w = chi * conf                        # dynamic MB weight per action in [0,1]

        # Hybrid arbitration per action
        Q1_hybrid = (1.0 - w) * Q1_mf + w * Q1_mb

        # Add first-stage perseveration
        q1_eff = Q1_hybrid.copy()
        if prev_a1 >= 0:
            q1_eff[prev_a1] += psi

        # First-stage policy
        q1_eff = q1_eff - np.max(q1_eff)
        exp_q1 = np.exp(beta * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        q2_eff = Q2[s, :].copy()
        q2_eff = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second-stage MF values
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Back up to stage 1 MF value
        bootstrap = Q2[s, a2]
        delta1 = bootstrap - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Update transition model for the chosen first-stage action
        # Move the chosen action's row toward the observed one-hot state
        one_hot_s = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        T[a1, :] = (1.0 - tau) * T[a1, :] + tau * one_hot_s
        # Keep the other action's transition unchanged

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """MF learner with confirmation-weighted credit assignment, dual temperatures, and value forgetting.
    
    The agent uses model-free TD learning at both stages. The first-stage update
    is confirmation-weighted: learning from outcomes is amplified after common
    transitions and attenuated after rare transitions. Policies use separate
    inverse temperatures for stages. Unchosen second-stage action values in the
    visited state decay toward zero (forgetting).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage actions per trial.
    reward : array-like of float (e.g., 0 or 1)
        Obtained reward per trial.
    model_parameters : list or array-like of 5 floats
        [alpha, c, beta1, beta2, zeta]
        - alpha in [0,1]: TD learning rate (both stages).
        - c in [0,1]: Confirmation weighting. Increases stage-1 learning after
                      common transitions and decreases it after rare transitions.
        - beta1 in [0,10]: Inverse temperature at stage 1.
        - beta2 in [0,10]: Inverse temperature at stage 2.
        - zeta in [0,1]: Forgetting rate for the unchosen second-stage action
                         within the visited state (toward 0).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, c, beta1, beta2, zeta = model_parameters
    n_trials = len(action_1)

    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]

        # Stage 1 policy
        q1_eff = Q1 - np.max(Q1)
        exp_q1 = np.exp(beta1 * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        q2_eff = Q2[s, :] - np.max(Q2[s, :])
        exp_q2 = np.exp(beta2 * q2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD update at stage 2
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Forgetting for the unchosen action in the visited state
        unchosen = 1 - a2
        Q2[s, unchosen] *= (1.0 - zeta)

        # Determine whether the transition was common (A->X or U->Y)
        common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

        # Confirmation-weighted credit assignment at stage 1
        # Effective learning rate increases by factor (1+c) for common transitions
        # and decreases by factor (1-c) for rare transitions, clipped to [0,1].
        alpha1 = alpha * (1.0 + c if common else 1.0 - c)
        alpha1 = min(1.0, max(0.0, alpha1))

        # Back up to stage 1 value
        bootstrap = Q2[s, a2]
        delta1 = bootstrap - Q1[a1]
        Q1[a1] += alpha1 * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planner with learned transitions, entropy-driven exploration bonus, and stage-2 perseveration.
    
    The agent learns a transition model from each first-stage action to the two
    second-stage states and performs model-based planning using the current
    second-stage values. An exploration bonus proportional to the entropy of the
    learned transition from each first-stage action encourages information seeking.
    A perseveration bias affects second-stage choices.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage action per trial.
    reward : array-like of float (e.g., 0 or 1)
        Reward obtained per trial.
    model_parameters : list or array-like of 5 floats
        [alpha_r, beta, tau_t, u, kappa2]
        - alpha_r in [0,1]: Learning rate for second-stage Q-values.
        - beta in [0,10]: Inverse temperature for both stages' softmax policies.
        - tau_t in [0,1]: Transition learning rate for P(state | action_1).
        - u in [0,1]: Weight of the entropy-based exploration bonus added to
                      first-stage model-based values (per action).
        - kappa2 in [0,1]: Second-stage perseveration strength (repeat previous a2).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_r, beta, tau_t, u, kappa2 = model_parameters
    n_trials = len(action_1)

    # Second-stage values
    Q2 = np.zeros((2, 2))

    # Transition model initialized to ignorance
    T = np.full((2, 2), 0.5)  # rows: a1 in {0,1}; cols: state in {0,1}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a2 = -1

    for t in range(n_trials):
        s = state[t]

        # Model-based first-stage values: expected max Q2 under learned transitions
        maxQ2 = np.max(Q2, axis=1)          # shape (2,)
        Q1_mb = T @ maxQ2                   # shape (2,)

        # Entropy bonus per first-stage action (uncertainty-driven exploration)
        # H(p) = -sum p log p, normalized by log(2) to lie in [0,1]
        eps_p = 1e-12
        H = np.zeros(2)
        for a in range(2):
            p0 = max(eps_p, T[a, 0])
            p1 = max(eps_p, T[a, 1])
            ent = -(p0 * np.log(p0) + p1 * np.log(p1))
            H[a] = ent / np.log(2.0)  # normalized entropy in [~0,1]

        q1_eff = Q1_mb + u * H
        q1_eff = q1_eff - np.max(q1_eff)
        exp_q1 = np.exp(beta * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with perseveration
        q2_eff = Q2[s, :].copy()
        if prev_a2 >= 0:
            q2_eff[prev_a2] += kappa2
        q2_eff = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second-stage values
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * delta2

        # Update transition model for chosen first-stage action
        one_hot_s = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        T[a1, :] = (1.0 - tau_t) * T[a1, :] + tau_t * one_hot_s

        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll