def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Hybrid MB/MF with learned transitions and choice perseveration.

    The model learns second-stage (alien) values model-free, learns the
    first-stage transition model from experience, and arbitrates between
    model-based (via the learned transition matrix) and model-free first-stage
    values. A choice perseveration bias encourages repeating the most recent
    choice at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage spaceship choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage planet per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices per trial (0/1).
    reward : array-like of float (0 or 1)
        Binary reward outcomes per trial.
    model_parameters : iterable of floats
        [alpha_q, beta, alpha_t, omega, psi]
        - alpha_q in [0,1]: learning rate for model-free Q updates at both stages.
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - alpha_t in [0,1]: learning rate for transition model (spaceship->planet) learning.
        - omega in [0,1]: arbitration weight on model-based values at stage 1 (1=fully MB).
        - psi in [0,1]: perseveration strength added to the previously chosen action at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_q, beta, alpha_t, omega, psi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model T[action_1, state]
    # Start near the known structure but allow learning to adapt
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Model-free value functions
    q1_mf = np.zeros(2)        # stage-1 MF values for spaceships
    q2 = np.zeros((2, 2))      # stage-2 MF values: planet x action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = np.array([-1, -1])  # track previous action per state for stage-2 perseveration

    for t in range(n_trials):
        s = state[t]

        # Stage-2 policy (on reached state)
        logits2 = beta * q2[s, :].copy()
        # Add perseveration bias at stage 2 for repeating last choice in this state
        if prev_a2[s] != -1:
            logits2[prev_a2[s]] += psi
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 model-based values from learned transitions
        V_states = np.max(q2, axis=1)
        q1_mb = T @ V_states

        # Stage-1 arbitration
        q1_net = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy with perseveration
        logits1 = beta * q1_net.copy()
        if prev_a1 != -1:
            logits1[prev_a1] += psi
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning updates
        r = reward[t]

        # Update learned transition model for the chosen spaceship
        onehot = np.array([0.0, 0.0])
        onehot[s] = 1.0
        T[a1, :] = T[a1, :] + alpha_t * (onehot - T[a1, :])

        # Stage-2 TD update (MF)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * delta2

        # Stage-1 MF bootstrapped toward observed second-stage value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * delta1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: MB stage-1 with reward-contingent choice kernel and asymmetric learning at stage-2.

    The model uses a fixed transition structure for model-based planning at stage 1.
    Stage-2 alien values are learned with asymmetric learning rates for positive vs.
    negative prediction errors. A reward-contingent choice kernel at stage 1 biases
    repetition after reward and promotes switching after non-reward.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage spaceship choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcomes.
    model_parameters : iterable of floats
        [alpha_pos, alpha_neg, beta1, beta2, phi]
        - alpha_pos in [0,1]: learning rate for positive PE at stage 2.
        - alpha_neg in [0,1]: learning rate for negative PE at stage 2.
        - beta1 in [0,10]: inverse temperature for stage-1 choices.
        - beta2 in [0,10]: inverse temperature for stage-2 choices.
        - phi in [0,1]: reward-contingent choice kernel at stage 1:
            add +phi to the logit of repeating the previous spaceship after reward,
            and add -phi to repeating after non-reward (thus promoting switching).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta1, beta2, phi = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (task-known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))  # stage-2 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_r = 0.0  # prior reward (for kernel sign)

    for t in range(n_trials):
        s = state[t]

        # Stage-2 policy
        logits2 = beta2 * q2[s, :].copy()
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB values from fixed transitions
        V_states = np.max(q2, axis=1)
        q1_mb = T @ V_states

        # Reward-contingent choice kernel at stage 1
        logits1 = beta1 * q1_mb.copy()
        if prev_a1 != -1:
            # If repeating previous a1, add +phi after reward, -phi after no reward
            repeat_bias = (2.0 * prev_r - 1.0) * phi  # +phi if prev_r=1, -phi if prev_r=0
            logits1[prev_a1] += repeat_bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning at stage 2 with asymmetric LRs
        r = reward[t]
        delta2 = r - q2[s, a2]
        lr = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += lr * delta2

        # Update memories for next trial's kernel
        prev_a1 = a1
        prev_r = r

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Adaptive exploration via surprise- and PE-modulated temperatures, with hybrid values.

    This model adapts exploration dynamically:
    - Stage-1 inverse temperature is reduced on trials following surprising
      transitions (relative to the known 0.7/0.3 structure), promoting exploration.
    - Stage-2 inverse temperature is reduced following large unsigned prediction
      errors, promoting exploration after volatile outcomes.
    Values:
    - Stage-2 values are learned model-free.
    - Stage-1 combines model-based (fixed transitions) and model-free values equally.
    - Perseveration biases both stages toward repeating the previous choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcomes per trial.
    model_parameters : iterable of floats
        [alpha, beta0, chi, xi, stick]
        - alpha in [0,1]: learning rate for model-free Q updates (both stages' MF).
        - beta0 in [0,10]: base inverse temperature before modulation.
        - chi in [0,1]: sensitivity of stage-1 beta to transition surprise.
        - xi in [0,1]: sensitivity of stage-2 beta to unsigned PE magnitude.
        - stick in [0,1]: perseveration bias added to the previously chosen action at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta0, chi, xi, stick = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure used to compute transition surprise
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2)        # stage-1 model-free values
    q2 = np.zeros((2, 2))      # stage-2 model-free values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = np.array([-1, -1])
    prev_surprise = 0.0
    prev_abs_pe2 = 0.0

    for t in range(n_trials):
        s = state[t]

        # Adaptive temperatures
        beta1_t = beta0 * (1.0 - chi * prev_surprise)
        beta2_t = beta0 * (1.0 - xi * prev_abs_pe2)
        beta1_t = max(beta1_t, 0.0)
        beta2_t = max(beta2_t, 0.0)

        # Stage-2 policy with perseveration
        logits2 = beta2_t * q2[s, :].copy()
        if prev_a2[s] != -1:
            logits2[prev_a2[s]] += stick
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB values from fixed transitions
        V_states = np.max(q2, axis=1)
        q1_mb = T_fixed @ V_states

        # Hybrid stage-1 value: equal weighting of MB and MF
        q1_net = 0.5 * q1_mb + 0.5 * q1_mf

        # Stage-1 policy with perseveration
        logits1 = beta1_t * q1_net.copy()
        if prev_a1 != -1:
            logits1[prev_a1] += stick
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Outcomes and learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update toward observed second-stage value (bootstrapped)
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update adaptive signals for next trial
        # Transition surprise based on fixed structure for the chosen action
        trans_prob = T_fixed[a1, s]
        prev_surprise = 1.0 - trans_prob  # higher for rare transitions (0.7 common -> 0.3 surprise; 0.3 rare -> 0.7 surprise)
        prev_abs_pe2 = min(1.0, abs(delta2))  # keep within [0,1] for modulation

        # Update perseveration memories
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss