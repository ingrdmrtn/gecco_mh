def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Surprise-weighted MB/MF arbitration with learned transitions.
    This hybrid model blends model-free (MF) and model-based (MB) action values at stage 1.
    The arbitration weight in favor of MB is dynamically updated by the surprise of observed
    transitions, and the transition model itself is learned from experience.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens).
    reward : array-like of float
        Outcome per trial (coins).
    model_parameters : tuple/list of 5 floats
        - alpha (0 to 1): learning rate for Q-value updates (both stages).
        - beta (0 to 10): inverse temperature for softmax at both stages.
        - omega0 (0 to 1): initial weight on MB values at stage 1 (vs. MF).
        - xi (0 to 1): learning rate for updating the transition matrix from experience.
        - chi (0 to 1): surprise sensitivity controlling how strongly MB weight tracks transition surprise.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, omega0, xi, chi = model_parameters
    n_trials = len(action_1)

    # Initialize choice probability trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize MF Q-values
    q_stage1_mf = np.zeros(2)        # for spaceships A/U
    q_stage2_mf = np.zeros((2, 2))   # for aliens within each planet

    # Initialize transition model T[a, s] = P(state=s | action_1=a)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Arbitration weight
    omega = omega0

    for t in range(n_trials):
        # Compute MB action values from current transition model and MF second-stage values
        max_q2 = np.max(q_stage2_mf, axis=1)         # best alien per planet
        q_stage1_mb = T @ max_q2                     # MB value per spaceship

        # Blend MB and MF for stage 1 policy
        q1_blend = omega * q_stage1_mb + (1.0 - omega) * q_stage1_mf

        # Stage 1 policy
        q1 = q1_blend
        q1 = q1 - np.max(q1)  # numerical stability
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (conditioned on observed state)
        s = state[t]
        q2 = q_stage2_mf[s, :].copy()
        q2 = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update MF values
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Update stage-1 MF toward current second-stage value (SARSA-style bootstrapping)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Update transition model using observed transition
        # Move the chosen row toward the observed state (one-hot), renormalized by construction
        e = np.zeros(2)
        e[s] = 1.0
        T[a1, :] += xi * (e - T[a1, :])

        # Compute transition surprise and update arbitration weight
        # Surprise is high when the observed transition was unlikely under current T
        p_obs = max(1e-8, T[a1, s])
        surprise = 1.0 - p_obs  # in [0,1]
        omega = (1.0 - chi) * omega + chi * surprise
        # Keep omega within [0,1] to avoid numerical drift
        omega = min(1.0, max(0.0, omega))

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free with directed exploration (leaky count bonuses) and action inertia.
    This model is purely model-free, but choices are guided by a count-based exploration
    bonus that decays over time (leaky counts). Repetition of previous choices (inertia)
    adds a bias to repeat the most recent action at each stage.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens).
    reward : array-like of float
        Outcome per trial (coins).
    model_parameters : tuple/list of 5 floats
        - alpha (0 to 1): learning rate for Q-value updates (both stages).
        - beta (0 to 10): inverse temperature for softmax at both stages.
        - nu (0 to 1): strength of the count-based exploration bonus.
        - zeta (0 to 1): leak/decay factor for visitation counts per trial.
        - pi (0 to 1): action inertia (perseveration) strength at both stages.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, nu, zeta, pi = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Leaky visitation counts for directed exploration
    N1 = np.zeros(2)         # counts for spaceships
    N2 = np.zeros((2, 2))    # counts for aliens per planet

    prev_a1 = None
    prev_a2 = [None, None]

    for t in range(n_trials):
        # Stage 1: directed exploration bonus from leaky counts
        bonus1 = nu / np.sqrt(N1 + 1.0)

        # Action inertia bias (repeat last chosen action at stage 1)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += pi

        q1_pol = q_stage1_mf + bonus1 + bias1
        logits1 = beta * (q1_pol - np.max(q1_pol))
        probs_1 = np.exp(logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2: directed exploration within the visited planet
        s = state[t]
        bonus2 = nu / np.sqrt(N2[s, :] + 1.0)

        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += pi

        q2_pol = q_stage2_mf[s, :] + bonus2 + bias2
        logits2 = beta * (q2_pol - np.max(q2_pol))
        probs_2 = np.exp(logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update MF Q-values
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Propagate value to first-stage MF (bootstrapping via second-stage value)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Update leaky counts after choices
        N1 = (1.0 - zeta) * N1
        N1[a1] += 1.0

        N2[s, :] = (1.0 - zeta) * N2[s, :]
        N2[s, a2] += 1.0

        # Update inertia memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with dual learning rates and within-planet outcome generalization.
    This model mixes model-based (fixed transition structure) and model-free values at stage 1.
    Second-stage learning uses separate learning rates for stage 1 vs. stage 2, and outcomes
    partially generalize to the unchosen alien on the visited planet.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens).
    reward : array-like of float
        Outcome per trial (coins).
    model_parameters : tuple/list of 5 floats
        - alpha1 (0 to 1): learning rate for stage-1 MF value updates.
        - alpha2 (0 to 1): learning rate for stage-2 MF value updates.
        - beta (0 to 10): inverse temperature for softmax at both stages.
        - omega (0 to 1): mixing weight of MB vs MF for stage-1 choice values.
        - g (0 to 1): outcome generalization strength to the unchosen alien on the visited planet.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha1, alpha2, beta, omega, g = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Fixed known transition structure (common = 0.7; rare = 0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    for t in range(n_trials):
        # Model-based stage-1 values via current second-stage MF values
        max_q2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = T @ max_q2

        q1_mix = omega * q_stage1_mb + (1.0 - omega) * q_stage1_mf
        logits1 = beta * (q1_mix - np.max(q1_mix))
        probs_1 = np.exp(logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * (q_stage2_mf[s, :] - np.max(q_stage2_mf[s, :]))
        probs_2 = np.exp(logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward and learning
        r = reward[t]
        # Update chosen second-stage action
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha2 * delta2

        # Generalize outcome to the unchosen alien on the same planet
        other = 1 - a2
        q_stage2_mf[s, other] += g * alpha2 * (r - q_stage2_mf[s, other])

        # Update stage-1 MF toward current second-stage value (after q2 update)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha1 * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll