def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Purely model-free SARSA with stage-specific learning rates/temperatures and choice stickiness.
    
    This model treats the task as a two-stage SARSA process without any model-based projection.
    The first-stage value is updated toward the value of the actually selected second-stage action.
    Choice stickiness adds an intrinsic bias to repeat the most recent action at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens 0/1 within planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha1, alpha2, beta1, beta2, phi]
        - alpha1: [0,1] learning rate for first-stage Q updates (toward Q2 of chosen action)
        - alpha2: [0,1] learning rate for second-stage Q updates (toward reward)
        - beta1:  [0,10] inverse temperature for first-stage softmax
        - beta2:  [0,10] inverse temperature for second-stage softmax
        - phi:    [0,1] choice stickiness weight added to the logit of the previously chosen action
                  (applied separately at each stage)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha1, alpha2, beta1, beta2, phi = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)          # stage-1 action values
    q2 = np.zeros((2, 2))     # stage-2 action values: q2[state, action]

    last_a1 = -1  # previous first-stage action for stickiness
    last_a2 = {-1: -1, 0: -1, 1: -1}  # track last action per state for stage-2 stickiness

    for t in range(n_trials):
        s = state[t]

        # Stage-2 policy (softmax with stickiness on last action taken in this state)
        bias2 = np.zeros(2)
        prev_a2 = last_a2.get(s, -1)
        if prev_a2 in (0, 1):
            bias2[prev_a2] += phi

        logits2 = beta2 * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy (softmax with stickiness on last first-stage action)
        bias1 = np.zeros(2)
        if last_a1 in (0, 1):
            bias1[last_a1] += phi

        logits1 = beta1 * q1 + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Observe reward
        r = reward[t]

        # TD learning updates
        # Stage-2: standard TD toward reward
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Stage-1: SARSA target is value of the chosen second-stage action at the reached state
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha1 * delta1

        # Update stickiness trackers
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Hybrid MB/MF with learned transitions anchored to a prior.
    
    The agent learns second-stage rewards (Q2) and learns the transition matrix online.
    The learned transition matrix is shrunk toward a prior "common=0.7" matrix using
    a prior-strength parameter, and model-based first-stage values are combined with
    model-free first-stage values via a mixing parameter.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage states (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens 0/1 within planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, theta, tau_T, gamma]
        - alpha:  [0,1] learning rate for reward-based updates of Q2 (and MF Q1 via SARSA backup)
        - beta:   [0,10] inverse temperature for both stages
        - theta:  [0,1] weight on model-based value at stage 1 (1=fully MB, 0=fully MF)
        - tau_T:  [0,1] learning rate for updating the transition probabilities
        - gamma:  [0,1] prior strength mixing learned transitions toward the prior matrix
                          T_eff = (1-gamma)*T_hat + gamma*T_prior
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, theta, tau_T, gamma = model_parameters
    n_trials = len(action_1)

    # Prior/common transition matrix (A->X, U->Y are common)
    T_prior = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Initialize learned transitions at prior
    T_hat = T_prior.copy()

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]

        # Effective transitions after prior shrinkage
        T_eff = (1.0 - gamma) * T_hat + gamma * T_prior

        # Stage-2 policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB value via projected max Q2, and hybrid with MF Q1
        max_q2 = np.max(q2, axis=1)     # value of best alien on each planet
        q1_mb = T_eff @ max_q2
        q1_hyb = theta * q1_mb + (1.0 - theta) * q1_mf

        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Reward and learning
        r = reward[t]

        # Update learned transitions for the actually chosen first-stage action
        # Move the chosen row toward the one-hot of the observed state
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_hat[a1, :] = (1.0 - tau_T) * T_hat[a1, :] + tau_T * one_hot_s
        # Ensure numerical stability (stay normalized)
        row_sum = np.sum(T_hat[a1, :])
        if row_sum > 0:
            T_hat[a1, :] /= row_sum

        # Second-stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # First-stage MF update toward realized second-stage chosen value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Model-based first stage with outcome- and transition-contingent repetition biases.
    
    The first stage is purely model-based using the known (fixed) transition structure.
    Two repetition biases shape choice: (1) a general tendency to repeat the previous
    first-stage action; (2) a win-stay/lose-shift component proportional to the sign of
    the previous reward; and (3) a transition-contingent bias that encourages repeating
    after common transitions and discourages repeating after rare transitions.
    Second-stage values are learned model-free.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage states (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens 0/1 within planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, wsls, kappa, sigma]
        - alpha: [0,1] learning rate for Q2 updates (and hence for MB projections)
        - beta:  [0,10] inverse temperature for both stages
        - wsls:  [0,1] win-stay/lose-shift weight added to the previous first-stage action's logit
                        (+ if last reward > 0, − if last reward < 0; zero if exactly 0)
        - kappa: [0,1] general perseveration (reward-independent) to repeat the previous first-stage action
        - sigma: [0,1] transition-contingent repetition bias:
                        +sigma added to repeating after a common transition,
                        −sigma added to repeating after a rare transition (encourages switching)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, wsls, kappa, sigma = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage values (model-free)
    q2 = np.zeros((2, 2))

    # Track previous first-stage action and whether last transition was common
    prev_a1 = -1
    prev_common = None
    prev_r = 0.0

    for t in range(n_trials):
        s = state[t]

        # Stage-2 policy (softmax over Q2 at the encountered state)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 model-based values via projection through T
        max_q2 = np.max(q2, axis=1)  # best alien value on each planet
        q1_mb = T @ max_q2

        # Build first-stage repetition biases
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            # General perseveration
            bias1[prev_a1] += kappa
            # Win-stay / lose-shift (use sign of previous reward)
            if prev_r != 0:
                bias1[prev_a1] += wsls * (1.0 if prev_r > 0 else -1.0)
            # Transition-contingent repetition effect
            if prev_common is not None:
                bias1[prev_a1] += (sigma if prev_common else -sigma)

        # Stage-1 policy
        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Observe reward
        r = reward[t]

        # Second-stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Determine whether the current transition (a1 -> s) was common or rare for the next trial's bias
        # A transition is "common" if s is the more-probable planet for the chosen spaceship under T.
        common_for_a1 = (np.argmax(T[a1, :]) == s)
        prev_common = common_for_a1
        prev_a1 = a1
        prev_r = r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll