def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-gated hybrid controller with online transition learning and dynamic eligibility.
    
    The model learns the transition structure online and arbitrates between model-based
    and model-free control at stage 1 based on transition certainty. Certainty is computed
    from the entropy of the learned transition distribution for the chosen spaceship.
    Higher certainty upweights model-based planning. The model-free stage-1 values are
    updated via a TD backup from the reached second-stage value with a dynamic eligibility
    factor tied to certainty. Perseveration at stage 1 is included.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached state.
    reward : array-like of float/int (0/1)
        Outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, omega0, tau_ent, kappa1]
        - alpha in [0,1]: learning rate for Q2 and MF Q1 updates
        - beta in [0,10]: inverse temperature shared across stages
        - omega0 in [0,1]: baseline weight on model-based control at stage 1
        - tau_ent in [0,1]: entropy sensitivity; lower values make arbitration more
                             sensitive to certainty; mapped via a smooth logistic
        - kappa1 in [0,1]: perseveration strength at stage 1
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega0, tau_ent, kappa1 = model_parameters
    n_trials = len(action_1)

    # Transition model learned online (Dirichlet-like counts)
    counts = np.ones((2, 2))  # start uninformative; rows: action, cols: state
    T = counts / counts.sum(axis=1, keepdims=True)

    # Value functions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    last_a1 = None

    # Helper for stable softmax
    def softmax_logits(x):
        z = x - np.max(x)
        ez = np.exp(np.clip(z, -50, 50))
        return ez / np.sum(ez)

    for t in range(n_trials):
        # Compute model-based state values (max over second-stage actions)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 arbitration weight based on certainty of each action's transition
        # Certainty c_a = 1 - normalized entropy H(T[a,:]) / log(2)
        ent = -np.sum(T * np.where(T > 0, np.log(T), 0.0), axis=1)  # entropy per row
        ent_norm = ent / np.log(2.0)  # in [0,1]
        certainty = 1.0 - ent_norm     # in [0,1]

        # Map certainty to arbitration via a logistic with slope governed by tau_ent
        slope = 10.0 * (tau_ent + 1e-6)  # avoid zero; 0..1 -> modest to steep
        # Center around 0.5 certainty; weight closer to 1 when more certain
        w_cert = 1.0 / (1.0 + np.exp(-slope * (certainty - 0.5)))
        # Blend with baseline omega0 to allow individual bias toward MB
        w = omega0 * np.ones(2) + (1 - omega0) * w_cert

        # Combine MF and MB for each action at stage 1
        q1 = w * q1_mb + (1 - w) * q1_mf

        # Stage-1 perseveration bias
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa1

        # Stage-1 choice policy
        pref1 = beta * q1 + bias1
        probs_1 = softmax_logits(pref1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 choice policy within observed state
        s = state[t]
        pref2 = beta * q2[s, :]
        probs_2 = softmax_logits(pref2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update learned transition model for chosen action using observed state
        counts[a1, s] += 1.0
        T[a1, :] = counts[a1, :] / np.sum(counts[a1, :])

        # Stage-2 value learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF backup with dynamic eligibility tied to certainty of chosen action
        # More certain transitions yield stronger MF credit assignment.
        elig = w_cert[a1]  # in [0,1]
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * elig * delta1

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Choice-kernel augmented SARSA with bootstrapping from stage 2.
    
    This model uses purely model-free learning but augments decision-making with
    decaying choice kernels (habitual stickiness) at both stages. Stage-1 values
    are learned via SARSA-style bootstrapping from the chosen second-stage action
    value. No transition model is learned or used.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached state.
    reward : array-like of float/int (0/1)
        Outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, phi, rho_ck, gamma_boot]
        - alpha in [0,1]: learning rate for Q-values
        - beta in [0,10]: inverse temperature shared across stages
        - phi in [0,1]: strength of choice-kernel influence on preferences
        - rho_ck in [0,1]: choice-kernel decay/increment rate
        - gamma_boot in [0,1]: bootstrapping factor from Q2 to Q1 target
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, phi, rho_ck, gamma_boot = model_parameters
    n_trials = len(action_1)

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels (stickiness) for stage 1 and per-state stage 2
    k1 = np.zeros(2)
    k2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    def softmax_logits(x):
        z = x - np.max(x)
        ez = np.exp(np.clip(z, -50, 50))
        return ez / np.sum(ez)

    for t in range(n_trials):
        # Stage-1 policy: Q1 + choice kernel
        pref1 = beta * q1 + phi * k1
        probs_1 = softmax_logits(pref1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        pref2 = beta * q2[s, :] + phi * k2[s, :]
        probs_2 = softmax_logits(pref2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update choice kernels (decay + increment chosen)
        k1 *= (1.0 - rho_ck)
        k1[a1] += rho_ck

        k2[s, :] *= (1.0 - rho_ck)
        k2[s, a2] += rho_ck

        # Q-learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # SARSA-like bootstrapping for stage 1 from chosen stage-2 action value
        target1 = gamma_boot * q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Reference-dependent, risk-sensitive MF with transition-surprise modulation.
    
    Stage-2 learning uses a concave/convex utility transformation of reward relative
    to a running reference point, capturing risk sensitivity and adaptation. At stage 1,
    MF credit assignment incorporates a transition-surprise signal: rare transitions
    modulate the effective outcome used to update stage-1 values. Perseveration at both
    stages is included.
    
    The transition structure is assumed known and stationary (common=0.7); rarity is
    determined by whether the reached state is common or rare for the chosen spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached state.
    reward : array-like of float/int (0/1)
        Outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, theta, xi_surprise, kappa2]
        - alpha in [0,1]: learning rate for Q-values and reference update
        - beta in [0,10]: inverse temperature shared across stages
        - theta in [0,1]: utility curvature (r_eff = max(r - ref, 0)^theta - max(ref - r, 0)^theta)
        - xi_surprise in [0,1]: strength of transition-surprise modulation on stage-1 update
        - kappa2 in [0,1]: perseveration strength applied at both stages
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, theta, xi_surprise, kappa2 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure for rarity tagging: 0.7 common A->X, U->Y
    def is_common(a, s):
        return (a == 0 and s == 0) or (a == 1 and s == 1)

    # Values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Running reference point for outcomes
    ref = 0.5  # neutral starting point

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_a2 = [None, None]

    def softmax_logits(x):
        z = x - np.max(x)
        ez = np.exp(np.clip(z, -50, 50))
        return ez / np.sum(ez)

    for t in range(n_trials):
        # Stage-1 policy with perseveration
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa2
        pref1 = beta * q1 + bias1
        probs_1 = softmax_logits(pref1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with state-dependent perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += kappa2
        pref2 = beta * q2[s, :] + bias2
        probs_2 = softmax_logits(pref2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Utility relative to running reference (risk sensitivity)
        # Piecewise power around reference
        diff = r - ref
        pos = max(diff, 0.0)
        neg = max(-diff, 0.0)
        u = (pos ** max(theta, 1e-6)) - (neg ** max(theta, 1e-6))

        # Stage-2 learning with utility u
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Surprise-modulated MF update for stage 1
        rare = 0 if is_common(a1, s) else 1
        # Centered surprise signal: rare (+0.7) vs common (-0.3)
        surpr = (0.7 if rare == 1 else -0.3)
        r_eff1 = u + xi_surprise * surpr
        delta1 = r_eff1 - q1[a1]
        q1[a1] += alpha * delta1

        # Update running reference toward observed reward
        ref += alpha * (r - ref)

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll