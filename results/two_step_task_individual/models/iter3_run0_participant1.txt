def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-arbitrated Hybrid (learned transitions, MF-MB blend, stickiness).
    
    This model blends model-free (MF) and model-based (MB) control at stage 1 using
    a dynamic arbitration weight that depends on relative uncertainty. MB uncertainty
    is the transition entropy; MF uncertainty is low when the MF action values are
    highly differentiated. The transition model is learned online. Stage-2 values
    are learned with a simple delta rule. A stickiness term biases repeating the
    previous stage-1 action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each planet: at X (0=W,1=S); at Y (0=P,1=H).
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, alphaT, zeta, kappa]
        - alpha (reward learning rate for MF stage-2 and MF stage-1 bootstrapping, [0,1])
        - beta (inverse temperature for both stages, [0,10])
        - alphaT (transition learning rate for P(state | action_1), [0,1])
        - zeta (arbitration sensitivity, [0,1]): higher values make arbitration weight
          more sensitive to uncertainty differences.
        - kappa (stage-1 stickiness, [0,1]): additive bias to repeat the last stage-1 action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed actions under the model.
    """
    alpha, beta, alphaT, zeta, kappa = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix; rows = actions, cols = states
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Values
    q2 = np.zeros((2, 2))   # stage-2 MF values for each state-action
    q1_mf = np.zeros(2)     # stage-1 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    # Helper: binary entropy for each row of T
    def row_entropy(p_row):
        p = np.clip(p_row, 1e-8, 1 - 1e-8)
        return - (p * np.log(p) + (1 - p) * np.log(1 - p))

    for t in range(n_trials):
        a1 = action_1[t]
        s2 = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based stage-1 values via planning through learned transitions
        max_q2 = np.max(q2, axis=1)        # best attainable value on each state
        q1_mb = T @ max_q2                 # MB evaluation of each action

        # Compute arbitration weight w in [0,1]
        # - MB uncertainty: mean entropy across actions (higher entropy -> more uncertain).
        # - MF uncertainty: 1 - |q1_mf[0] - q1_mf[1]| (more separation -> less uncertainty).
        H_rows = np.array([row_entropy(T[0]), row_entropy(T[1])])
        mb_unc = 0.5 * np.sum(H_rows)  # average entropy for two actions
        mf_unc = 1.0 - min(1.0, abs(q1_mf[0] - q1_mf[1]))  # in [0,1]
        # Arbitration favors MB when MB is more certain than MF (i.e., mb_unc < mf_unc).
        # w = sigmoid(zeta * (mf_unc - mb_unc)).
        w = 1.0 / (1.0 + np.exp(-zeta * (mf_unc - mb_unc)))

        # Blend MF and MB
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 stickiness bias
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        # Stage-1 policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Transition learning: move chosen action's row toward observed state one-hot
        oh = np.array([0.0, 0.0])
        oh[s2] = 1.0
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        # Renormalize/clamp
        T[a1] = np.clip(T[a1], 1e-8, 1.0)
        T[a1] /= np.sum(T[a1])

        # Stage-2 MF learning
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Stage-1 MF bootstrapping toward realized second-stage value
        target1 = q2[s2, a2]
        q1_mf[a1] += alpha * (target1 - q1_mf[a1])

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Counterfactual-credit Hybrid with model-based WSLS bias.
    
    Stage-1 choices combine model-based (MB) planning through fixed transitions
    and model-free (MF) values. MF stage-1 credit assignment is "counterfactual":
    following rare transitions, reward is treated as evidence against repeating
    the chosen action (effective outcome is flipped). A model-based win-stay/lose-switch
    (WSLS) bias further nudges choice repetition/switching based on the previous trial's
    reward and whether the previous transition was common vs rare. Stage-2 values
    are learned via delta rule.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, eta, omega, phi]
        - alpha (stage-2 learning rate, [0,1])
        - beta (inverse temperature for both stages, [0,10])
        - eta (stage-1 MF learning rate with counterfactual credit, [0,1])
        - omega (MB weight in stage-1 value blend, [0,1])
        - phi (MB-WSLS bias magnitude on stage-1 logits, [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of the observed actions under the model.
    """
    alpha, beta, eta, omega, phi = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common = 0.7, rare = 0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))  # stage-2 MF values
    q1_mf = np.zeros(2)    # stage-1 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_r = None
    last_common = None

    def is_common_transition(chosen_a, observed_s):
        # Common transitions: A->X (0->0), U->Y (1->1)
        return (chosen_a == observed_s)

    for t in range(n_trials):
        a1 = action_1[t]
        s2 = state[t]
        a2 = action_2[t]
        r = reward[t]

        # MB planning via fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Blend MB and MF
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # MB-informed WSLS bias using previous trial's outcome and transition type
        bias1 = np.zeros(2)
        if last_a1 is not None:
            # If previous was rewarded+common OR unrewarded+rare: bias to stay
            if (last_r == 1 and last_common) or (last_r == 0 and (not last_common)):
                bias1[last_a1] += phi
            else:
                bias1[last_a1] -= phi

        # Stage-1 policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 MF update
        q2[s2, a2] += alpha * (r - q2[s2, a2])

        # Stage-1 MF with counterfactual credit based on transition type
        common = is_common_transition(a1, s2)
        # Effective outcome: treat rare transitions as inverted evidence for a1
        r_eff = r if common else (1.0 - r)
        q1_mf[a1] += eta * (r_eff - q1_mf[a1])

        # Bookkeeping for next trial
        last_a1 = a1
        last_r = r
        last_common = common

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Kalman planner with uncertainty-guided exploration and learned transitions.
    
    Second-stage reward expectations are tracked with a scalar Kalman filter per
    state-action, yielding trial-by-trial adaptive learning rates and uncertainty.
    A directed exploration bonus proportional to posterior uncertainty (UCB) is added
    at stage 2, and propagated to stage 1 via planning through learned transitions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [beta, q_var, r_var, alphaT, c]
        - beta (inverse temperature for both stages, [0,10])
        - q_var (process noise of random walk in rewards; controls volatility, [0,1])
        - r_var (observation noise variance; controls trust in observations, [0,1])
        - alphaT (transition learning rate for P(state | action_1), [0,1])
        - c (uncertainty bonus weight for UCB at stage 2, [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of the observed actions under the model.
    """
    beta, q_var, r_var, alphaT, c = model_parameters
    n_trials = len(action_1)

    # Learned transition model
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Kalman filter state per (state, action): mean m and variance s2
    m = np.zeros((2, 2))            # posterior mean reward
    s2 = np.ones((2, 2)) * 0.5      # posterior variance (start moderately uncertain)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # UCB-augmented second-stage values
        q2_ucb = m + c * np.sqrt(np.maximum(s2, 1e-12))

        # Plan stage 1 via learned transitions
        max_q2 = np.max(q2_ucb, axis=1)
        q1 = T @ max_q2

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2_ucb[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Transition learning update (row for chosen action)
        oh = np.array([0.0, 0.0])
        oh[s] = 1.0
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        T[a1] = np.clip(T[a1], 1e-8, 1.0)
        T[a1] /= np.sum(T[a1])

        # Kalman update for the experienced (state, action)
        # Time update (add process noise)
        s2[s, a2] = s2[s, a2] + q_var
        # Measurement update
        denom = s2[s, a2] + r_var + 1e-12
        K = s2[s, a2] / denom
        m[s, a2] = m[s, a2] + K * (r - m[s, a2])
        s2[s, a2] = (1.0 - K) * s2[s, a2]

        # Optional: diffuse uncertainty for unvisited pairs (process noise)
        # This helps track non-stationarity even when not chosen.
        # Add a small process noise to all entries to reflect drift.
        s2 += q_var * 0.0  # keep explicit; set to 0 to avoid double counting

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll