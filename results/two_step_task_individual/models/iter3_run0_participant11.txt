def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-weighted hybrid (learned transitions) with perseveration at both stages.
    Mechanisms:
    - Learn the action-to-state transition probabilities online (Dirichlet-like running average).
    - Compute model-based first-stage values from learned transitions and current second-stage values.
    - Weight model-based vs. model-free first-stage values by the action-specific transition uncertainty (entropy).
    - Model-free SARSA updates at both stages.
    - Perseveration (stickiness) biases repeating the most recent action at each stage.

    Parameters (all used; keep within bounds):
    - alpha_r in [0,1]: reward learning rate for Q-values (both stages).
    - beta in [0,10]: inverse temperature for softmax choice at both stages.
    - eta in [0,1]: learning rate for updating the transition matrix P(s | a1).
    - zeta in [0,1]: uncertainty weight; higher zeta down-weights MB when transitions are uncertain.
    - psi in [0,1]: perseveration strength (added to the last chosen action's preference at both stages).

    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of second-stage states (0=X, 1=Y)
    - action_2: array of second-stage choices on the visited state (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: [alpha_r, beta, eta, zeta, psi]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, eta, zeta, psi = model_parameters
    n_trials = len(action_1)

    # Prob tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Learned transition model P(s | a1)
    T = np.full((2, 2), 0.5)

    # Perseveration memory
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)  # per state

    for t in range(n_trials):
        # Compute MB first-stage values from learned transitions and stage-2 Q
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T @ max_q2  # shape (2,)

        # Entropy-based arbitration weight per action
        # Normalize entropy by log(2) to be in [0,1]
        H = np.zeros(2)
        for a in range(2):
            p = T[a]
            # numerical safety
            p_safe = np.clip(p, 1e-12, 1.0)
            H[a] = -np.sum(p_safe * np.log(p_safe)) / np.log(2.0)
        w = 1.0 - zeta * H  # higher uncertainty -> lower MB weight

        # Combine MF and MB per action
        q1_combined = w * q1_mb + (1.0 - w) * q1_mf

        # Perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += psi

        # First-stage policy
        pref1 = beta * q1_combined + bias1
        pref1 -= np.max(pref1)
        probs_1 = np.exp(pref1)
        probs_1 /= np.sum(probs_1)

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (on visited state)
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += psi

        pref2 = beta * q2[s] + bias2
        pref2 -= np.max(pref2)
        probs_2 = np.exp(pref2)
        probs_2 /= np.sum(probs_2)

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: stage 2 MF (reward prediction error)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Learning: stage 1 MF bootstrapping from stage 2
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * td1

        # Learn transitions for chosen action
        # Move chosen action's transition row towards observed state
        T[a1, :] = (1.0 - eta) * T[a1, :]
        T[a1, s] += eta
        # Renormalize row to sum to 1 (guard against numeric drift)
        T[a1, :] /= np.sum(T[a1, :])

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Risk- and surprise-sensitive model-free control with perseveration.
    Mechanisms:
    - Purely model-free SARSA values at both stages.
    - Utility transforms outcomes with asymmetric sensitivity to reward vs. no-reward.
      u(r; rho) = rho if r==1 else -(1 - rho), so rho in [0,1] trades off gain seeking vs. loss aversion.
    - Transition surprise bonus on each trial: Surprise = -log P(s|a1) using the known 0.7/0.3 structure,
      normalized to [0,1] and added to the utility with weight phi.
    - Perseveration (stickiness) biases repeating the last choice at each stage.

    Parameters (all used; keep within bounds):
    - alpha in [0,1]: learning rate for Q-values (both stages).
    - beta in [0,10]: inverse temperature for softmax choices at both stages.
    - rho in [0,1]: utility asymmetry (reward sensitivity vs. no-reward penalty).
    - phi in [0,1]: weight of transition surprise bonus added to utility.
    - stick in [0,1]: perseveration strength at both stages.

    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of second-stage states (0=X, 1=Y)
    - action_2: array of second-stage choices on the visited state (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: [alpha, beta, rho, phi, stick]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, rho, phi, stick = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Known transition structure for surprise computation
    # A (0) -> X (0) common, U (1) -> Y (1) common
    p_common = 0.7

    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        # Stage 1 policy with stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        pref1 = beta * q1 + bias1
        pref1 -= np.max(pref1)
        probs_1 = np.exp(pref1)
        probs_1 /= np.sum(probs_1)

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy on observed state with stickiness
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick

        pref2 = beta * q2[s] + bias2
        pref2 -= np.max(pref2)
        probs_2 = np.exp(pref2)
        probs_2 /= np.sum(probs_2)

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Compute asymmetric utility and surprise
        r = reward[t]
        u = rho if r == 1 else -(1.0 - rho)

        # Surprise based on whether transition was common or rare for the chosen first-stage action
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        p_trans = p_common if is_common else (1.0 - p_common)
        # Normalize surprise to [0,1] by dividing by max surprise (-log(0.3))
        surprise = -np.log(max(p_trans, 1e-12)) / (-np.log(1.0 - p_common))
        u_aug = u + phi * surprise

        # MF learning at stage 2 toward augmented utility
        delta2 = u_aug - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # MF SARSA update at stage 1 (bootstrapping from updated stage-2 value)
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * td1

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Associability-modulated learning with transition-dependent credit assignment and perseveration.
    Mechanisms:
    - Stage-2 associability (Pearceâ€“Hall-like): learning rate increases with recent unsigned prediction error.
      Associability is updated per (state, action) and bounded in [0,1].
    - Transition-dependent credit assignment at stage 1: after rare transitions, some credit is shifted to the
      unchosen first-stage action to approximate model-based credit routing.
    - Standard softmax with perseveration at both stages.

    Parameters (all used; keep within bounds):
    - alpha0 in [0,1]: base learning rate scale.
    - beta in [0,10]: inverse temperature for softmax choices at both stages.
    - phi in [0,1]: associability update rate (higher -> faster changes in learning rate).
    - xi in [0,1]: fraction of TD credit shifted to the unchosen first-stage action after rare transitions.
    - stick in [0,1]: perseveration strength at both stages.

    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of second-stage states (0=X, 1=Y)
    - action_2: array of second-stage choices on the visited state (0 or 1)
    - reward: array of rewards (e.g., 0/1)
    - model_parameters: [alpha0, beta, phi, xi, stick]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha0, beta, phi, xi, stick = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Associability per (state, action), initialized mid-range
    A = np.full((2, 2), 0.5)

    # Perseveration memory
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    p_common = 0.7  # used only to determine common vs rare

    for t in range(n_trials):
        # Stage 1 policy
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        pref1 = beta * q1 + bias1
        pref1 -= np.max(pref1)
        probs_1 = np.exp(pref1)
        probs_1 /= np.sum(probs_1)

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick

        pref2 = beta * q2[s] + bias2
        pref2 -= np.max(pref2)
        probs_2 = np.exp(pref2)
        probs_2 /= np.sum(probs_2)

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 associability-modulated learning
        # Effective learning rate: alpha2_t in [0, alpha0]
        alpha2_t = alpha0 * (0.5 + 0.5 * np.clip(A[s, a2], 0.0, 1.0))
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2_t * delta2

        # Update associability toward |delta| (kept in [0,1])
        A[s, a2] = (1.0 - phi) * A[s, a2] + phi * np.clip(abs(delta2), 0.0, 1.0)

        # Transition-dependent credit assignment at stage 1
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # TD targets from updated stage-2 value
        target = q2[s, a2]

        if is_common:
            # Standard SARSA update on chosen action
            delta_chosen = target - q1[a1]
            q1[a1] += alpha0 * delta_chosen
        else:
            # Rare transition: shift a fraction xi of credit to the unchosen action
            other = 1 - a1
            delta_chosen = target - q1[a1]
            delta_other = target - q1[other]
            q1[a1] += alpha0 * (1.0 - xi) * delta_chosen
            q1[other] += alpha0 * xi * delta_other

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll