def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB–MF with eligibility trace and value forgetting.

    The agent plans at stage 1 using a fixed transition model (common=0.7) and
    combines this model-based (MB) value with a model-free (MF) value learned
    via an eligibility trace that propagates the second-stage prediction error
    back to the chosen first-stage action. Second-stage values also undergo
    forgetting toward 0.5 to capture drift or memory decay.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta, omega, trace, forget]
        - alpha in [0,1]: learning rate for second-stage Q-value updates.
        - beta in [0,10]: inverse temperature for both stages.
        - omega in [0,1]: weight on model-based value at stage 1
                          (1 = purely MB, 0 = purely MF).
        - trace in [0,1]: eligibility trace strength propagating second-stage PE
                          to the chosen first-stage action (TD(λ)-style credit).
        - forget in [0,1]: forgetting rate driving all second-stage action values
                           toward 0.5 each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega, trace, forget = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X and U->Y are common (0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Initialize values
    q2 = np.zeros((2, 2)) + 0.5  # second-stage action values
    q1_mf = np.zeros(2)          # first-stage model-free values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Model-based stage-1 value: expected max over next-state actions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid stage-1 value
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy
        logits1 = q1 - np.max(q1)
        p1 = np.exp(beta * logits1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        p2 = np.exp(beta * logits2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Outcome and learning
        r = reward[t]

        # Stage-2 TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility trace: propagate PE back to the chosen first-stage action
        q1_mf[a1] += alpha * trace * pe2

        # Forgetting toward 0.5 for all second-stage values
        q2 = (1.0 - forget) * q2 + forget * 0.5

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-dependent credit assignment (TDCA) with lapse.

    The agent plans at stage 1 using a fixed transition model (common=0.7).
    In addition, a model-free first-stage value is learned from the second-stage
    prediction error, but the credit assigned depends on whether the observed
    transition was common or rare (separate scaling for each). A small lapse
    rate mixes the stage policies with uniform random choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta, kappa_common, kappa_rare, lapse]
        - alpha in [0,1]: learning rate for second-stage Q-value updates.
        - beta in [0,10]: inverse temperature for both stages.
        - kappa_common in [0,1]: TDCA weight applied to first-stage MF update
                                 after a common transition.
        - kappa_rare in [0,1]: TDCA weight applied after a rare transition.
        - lapse in [0,1]: probability of uniform random choice at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, kappa_common, kappa_rare, lapse = model_parameters
    n_trials = len(action_1)

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Model-based plan
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MB with MF first-stage values additively
        q1 = q1_mb + q1_mf

        # Stage-1 policy with lapse
        logits1 = q1 - np.max(q1)
        p1 = np.exp(beta * logits1)
        p1 = p1 / (np.sum(p1) + eps)
        p1 = (1.0 - lapse) * p1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with lapse
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        p2 = np.exp(beta * logits2)
        p2 = p2 / (np.sum(p2) + eps)
        p2 = (1.0 - lapse) * p2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Determine if transition was common or rare given the task structure
        # Common transitions: A->X (0->0) or U->Y (1->1)
        is_common = int((action_1[t] == 0 and s == 0) or (action_1[t] == 1 and s == 1))
        kappa = kappa_common if is_common else kappa_rare

        # Transition-dependent credit assignment to stage-1 MF value
        q1_mf[a1] += alpha * kappa * pe2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Volatility-adaptive softmax with cross-alien coupling.

    The agent plans at stage 1 using the fixed transition model (common=0.7).
    Second-stage learning is model-free, and the agent adapts its decision
    temperature to outcome volatility via a running estimate of absolute
    prediction error. Additionally, second-stage options on the same planet
    are coupled: updating one alien’s value induces an opposite adjustment
    to the unchosen alien (competitive coupling).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta, nu, decay, coupling]
        - alpha in [0,1]: learning rate for second-stage Q-value updates.
        - beta in [0,10]: base inverse temperature.
        - nu in [0,1]: volatility sensitivity; higher values reduce effective
                       temperature more strongly under high volatility.
        - decay in [0,1]: update rate for the running volatility estimate
                          (exponential moving average of |PE|).
        - coupling in [0,1]: strength of anti-correlation update to the unchosen
                             alien on the same planet.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, nu, decay, coupling = model_parameters
    n_trials = len(action_1)

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2)) + 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Running estimate of volatility via absolute PE magnitude
    vol = 0.0

    for t in range(n_trials):
        # Effective inverse temperature modulated by volatility
        beta_eff = beta / (1.0 + nu * vol)

        # Stage-1 MB planning
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        logits1 = q1_mb - np.max(q1_mb)
        p1 = np.exp(beta_eff * logits1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 choice
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        p2 = np.exp(beta_eff * logits2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Cross-alien competitive coupling on the same planet
        other_a2 = 1 - a2
        q2[s, other_a2] -= coupling * alpha * pe2

        # Update volatility estimate (EMA of absolute PE)
        vol = (1.0 - decay) * vol + decay * abs(pe2)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll