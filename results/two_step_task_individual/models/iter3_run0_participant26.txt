def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free with online transition learning and uncertainty-weighted arbitration plus lapse.

    The model learns:
    - Second-stage (state-action) values model-free via TD(0).
    - First-stage values via a hybrid of model-free and model-based evaluation.
      Model-based values use a learned transition model (Dirichlet counts).
    - Arbitration weight for model-based control depends on the uncertainty (entropy) of the learned transitions:
      less transition uncertainty -> more model-based control.
    - A small lapse parameter mixes the softmax with a uniform policy.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (0/1) per trial.
    reward : array-like of float (0 or 1)
        Reward received per trial.
    model_parameters : iterable of floats
        [alpha_mf, beta, omega_base, epsilon_lapse]
        - alpha_mf (0..1): Model-free learning rate for both stages.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - omega_base (0..1): Baseline weight on model-based control at the first stage.
        - epsilon_lapse (0..1): Lapse probability mixing softmax with a uniform random choice at both stages.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_mf, beta, omega_base, epsilon_lapse = model_parameters
    n_trials = len(action_1)

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)        # actions: 0=A,1=U
    q_stage2 = np.zeros((2, 2))      # states: 0=X,1=Y; actions: 0,1

    # Online transition counts for model-based planning (Dirichlet with init 1,1)
    trans_counts = np.ones((2, 2))   # counts[a1, s2], initialized to uniform prior

    log2 = np.log(2.0)

    for t in range(n_trials):
        # Compute learned transition probabilities and their entropies for both first-stage actions
        trans_prob = trans_counts / np.sum(trans_counts, axis=1, keepdims=True)  # shape (2,2)
        # Entropy H(a) = -sum_s p(s|a) log p(s|a); normalize by log 2 to be in [0,1]
        with np.errstate(divide='ignore', invalid='ignore'):
            h = -np.sum(trans_prob * (np.log(trans_prob + 1e-12)), axis=1) / log2  # shape (2,)
        h = np.clip(h, 0.0, 1.0)

        # Model-based action values: expected max second-stage value under learned transitions
        max_q2 = np.max(q_stage2, axis=1)  # shape (2,)
        q_stage1_mb = trans_prob @ max_q2   # shape (2,)

        # Arbitration weight depends on uncertainty (lower H => higher weight)
        # We compute an action-specific weight and blend MF and MB values elementwise.
        w = omega_base * (1.0 - h)  # shape (2,), in [0,1]
        q1_hybrid = (1.0 - w) * q_stage1_mf + w * q_stage1_mb

        # First-stage policy with softmax and lapse
        logits1 = beta * (q1_hybrid - np.max(q1_hybrid))
        exp1 = np.exp(logits1)
        softmax1 = exp1 / (np.sum(exp1) + 1e-12)
        probs_1 = (1.0 - epsilon_lapse) * softmax1 + epsilon_lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with softmax and lapse
        s2 = state[t]
        q2s = q_stage2[s2]
        logits2 = beta * (q2s - np.max(q2s))
        exp2 = np.exp(logits2)
        softmax2 = exp2 / (np.sum(exp2) + 1e-12)
        probs_2 = (1.0 - epsilon_lapse) * softmax2 + epsilon_lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update model-free values
        td2 = r - q_stage2[s2, a2]
        q_stage2[s2, a2] += alpha_mf * td2

        # Back up second-stage value to first-stage MF value (SARSA(1) target = current q2 value)
        target1 = q_stage2[s2, a2]
        td1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_mf * td1

        # Update transition counts from chosen first-stage action to observed state (learn transitions)
        trans_counts[a1, s2] += 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based first-stage with Kalman temporal-difference learning at the second stage and a common-transition bias.

    The model assumes the agent plans at the first stage using the known transition structure (0.7 common),
    but learns the second-stage reward probabilities via a scalar Kalman filter per (state, action):
    - Each second-stage Q has an uncertainty (variance) that determines an adaptive learning rate (Kalman gain).
    - Process noise parameter allows for nonstationarity in the reward environment.
    - At the first stage, we add a bias toward choosing the spaceship that commonly leads to the currently better planet.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (0/1) per trial.
    reward : array-like of float (0 or 1)
        Reward received per trial.
    model_parameters : iterable of floats
        [nu, beta, rho_bias, q0]
        - nu (0..1): Process noise added to the variance each trial (higher => faster adaptation).
        - beta (0..10): Inverse temperature for softmax at both stages.
        - rho_bias (0..1): Additive bias to the first-stage action that commonly leads to the currently better planet.
        - q0 (0..1): Initial prior mean for all second-stage Q-values.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    nu, beta, rho_bias, q0 = model_parameters
    n_trials = len(action_1)

    # Known transition matrix: rows are first-stage actions (A,U), columns are states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Kalman TD for second stage: mean and variance per (state, action)
    q2_mean = np.full((2, 2), q0, dtype=float)
    q2_var = np.full((2, 2), 0.25, dtype=float)  # initial uncertainty; bounded implicitly by dynamics

    sigma_obs = 1.0  # observation noise for binary reward

    for t in range(n_trials):
        # First-stage model-based values: expected max over second-stage actions
        max_q2 = np.max(q2_mean, axis=1)  # size 2
        q1_mb = T @ max_q2               # size 2

        # Common-transition bias: find better planet and boost the action commonly leading to it
        better_state = int(np.argmax(max_q2))  # 0 if X best else 1 if Y best
        # Action commonly leading to state 0 is A (0), to state 1 is U (1)
        bias = np.zeros(2)
        bias[better_state] = rho_bias  # because action index matches its common state

        logits1 = beta * (q1_mb + bias - np.max(q1_mb + bias))
        exp1 = np.exp(logits1)
        probs_1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy based on current means
        s2 = state[t]
        logits2 = beta * (q2_mean[s2] - np.max(q2_mean[s2]))
        exp2 = np.exp(logits2)
        probs_2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Kalman update for the chosen (state, action)
        # Predict step: add process noise
        q2_var[s2, a2] = q2_var[s2, a2] + nu
        # Kalman gain
        K = q2_var[s2, a2] / (q2_var[s2, a2] + sigma_obs)
        # Innovation
        pe = r - q2_mean[s2, a2]
        # Update mean and variance
        q2_mean[s2, a2] = q2_mean[s2, a2] + K * pe
        q2_var[s2, a2] = (1.0 - K) * q2_var[s2, a2]

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive model-free SARSA with transition-sensitive learning and partial backpropagation to stage 1.

    The model is purely model-free but includes:
    - Risk-sensitive utility: the reward is down-weighted by the squared prediction error magnitude,
      making learning more conservative after surprising outcomes.
    - Transition sensitivity: learning rate is higher after common transitions than after rare transitions.
    - Partial backpropagation: a fraction of the second-stage TD error is propagated to the first-stage value.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (0/1) per trial.
    reward : array-like of float (0 or 1)
        Reward received per trial.
    model_parameters : iterable of floats
        [alpha, beta, zeta_backprop, c_bias_common, rho_risk]
        - alpha (0..1): Base learning rate for both stages.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - zeta_backprop (0..1): Fraction of the second-stage TD error propagated to the first-stage Q.
        - c_bias_common (0..1): Degree to which learning is reduced after rare transitions.
                                 Effective learning rate is alpha after common transitions and alpha*(1-c_bias_common) after rare transitions.
        - rho_risk (0..1): Strength of risk-sensitivity; larger values penalize surprising outcomes more.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, zeta_backprop, c_bias_common, rho_risk = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)        # first-stage MF action values
    q2 = np.zeros((2, 2))   # second-stage MF state-action values

    for t in range(n_trials):
        # First-stage policy
        logits1 = beta * (q1 - np.max(q1))
        exp1 = np.exp(logits1)
        probs_1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        logits2 = beta * (q2[s2] - np.max(q2[s2]))
        exp2 = np.exp(logits2)
        probs_2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Determine if transition was common or rare given the fixed structure
        is_common = int((a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1))
        # Transition-sensitive effective learning rate
        alpha_eff = alpha if is_common == 1 else alpha * (1.0 - c_bias_common)

        # Risk-sensitive utility: penalize surprising outcomes relative to current expectation
        pe_nominal = r - q2[s2, a2]
        r_tilde = r - rho_risk * (pe_nominal ** 2)

        # Second-stage update
        td2 = r_tilde - q2[s2, a2]
        q2[s2, a2] += alpha_eff * td2

        # Partial backpropagation of second-stage TD to first-stage
        q1[a1] += alpha_eff * zeta_backprop * td2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll