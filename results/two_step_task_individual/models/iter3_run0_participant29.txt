def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free agent with learned transition model and stage-specific choice temperatures.

    The agent learns:
      - Second-stage action values (Q2) from rewards (model-free).
      - A first-stage cached value (Q1_MF) via TD from second-stage values.
      - A transition model T(s'|a1) via a simple delta rule.
    First-stage policy blends model-based values (T @ max_a Q2) with model-free Q1_MF using a mixing weight.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached on each trial (planet index).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index within state).
    reward : array-like of float (e.g., 0 or 1)
        Reward received on each trial.
    model_parameters : list or array
        [alpha, beta1, beta2, psi, phi_t]
        - alpha in [0,1]: learning rate for model-free value updates at both stages.
        - beta1 in [0,10]: inverse temperature for first-stage softmax.
        - beta2 in [0,10]: inverse temperature for second-stage softmax.
        - psi in [0,1]: weight of model-based action values at stage 1 (1=fully MB).
        - phi_t in [0,1]: transition learning rate for updating T(s'|a1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta1, beta2, psi, phi_t = model_parameters
    n_trials = len(action_1)

    # Initialize storage for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize learned quantities
    T = np.full((2, 2), 0.5)              # learned transition probabilities P(s'|a1)
    q1_mf = np.zeros(2)                   # model-free first-stage cached values
    q2 = np.zeros((2, 2))                 # second-stage Q-values

    for t in range(n_trials):
        # Compute model-based first-stage action values from current transition model
        max_q2_by_state = np.max(q2, axis=1)  # max over actions for each state
        q1_mb = T @ max_q2_by_state

        # Blend MB and MF for stage 1
        q1 = psi * q1_mb + (1.0 - psi) * q1_mf

        # Stage 1 policy
        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta1 * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (conditioned on reached state)
        s = state[t]
        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta2 * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learn second-stage Q-values
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update model-free first-stage value toward the (updated) second-stage value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update transition model toward the observed state
        onehot_s = np.zeros(2)
        onehot_s[s] = 1.0
        T[a1] = T[a1] + phi_t * (onehot_s - T[a1])

        # Keep transition rows normalized (numerical safety)
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] = T[a1] / np.sum(T[a1])

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Purely model-based planner with uncertainty bonus, stage-1 stickiness, and value forgetting.

    The agent:
      - Learns second-stage action values (Q2) from rewards (learning rate alpha).
      - Adds an uncertainty-driven exploration bonus at stage 2 proportional to 1/sqrt(visit_count+1).
      - Plans at stage 1 using the fixed transition matrix and the (bonus-augmented) max Q2 values.
      - Includes a perseveration (stickiness) bias on stage-1 choices.
      - Applies forgetting (decay) to unchosen second-stage actions each trial.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward received on each trial.
    model_parameters : list or array
        [alpha, beta, stick1, upsilon, nu]
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature used at both stages.
        - stick1 in [0,1]: perseveration weight added to the previously chosen stage-1 action.
        - upsilon in [0,1]: weight of the uncertainty bonus (UCB-like) at stage 2.
        - nu in [0,1]: forgetting rate applied to all unchosen second-stage actions each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, stick1, upsilon, nu = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2))           # second-stage action values
    visits = np.zeros((2, 2))       # visit counts for uncertainty bonus

    prev_a1 = None

    for t in range(n_trials):
        # Build uncertainty bonus for each state-action
        bonus = upsilon / np.sqrt(visits + 1.0)  # shape (2,2)

        # Stage 1 planning: compute MB Q1 via transition matrix and bonus-augmented Q2
        max_q2_bonus = np.max(q2 + bonus, axis=1)  # max over actions for each state
        q1 = transition_matrix @ max_q2_bonus

        # Add stage-1 stickiness
        if prev_a1 is not None:
            stick_vec = np.zeros(2)
            stick_vec[prev_a1] = 1.0
            q1 = q1 + stick1 * stick_vec

        # Stage 1 policy
        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy at reached state (add uncertainty bonus)
        s = state[t]
        q2_s = q2[s] + bonus[s]
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Apply forgetting to all unchosen second-stage actions
        # Decay every action, then we'll update the chosen one
        q2 = q2 * (1.0 - nu)

        # Learn chosen second-stage value
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update visit counts for uncertainty bonus
        visits[s, a2] += 1.0

        # Update stage-1 perseveration memory
        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Planet-value plus model-free hybrid with lapse noise.

    The agent maintains:
      - Planet values V(state) learned directly from reward outcomes (state-value learning).
      - Second-stage action values Q2 learned from rewards (model-free).
      - A cached first-stage model-free value Q1_MF updated from second-stage values.
    First-stage policy blends predicted planet values (via the fixed transition model)
    with cached model-free first-stage values, controlled by a mixing parameter.
    A lapse parameter mixes softmax with uniform random choice at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward received on each trial.
    model_parameters : list or array
        [alpha_planet, alpha_alien, beta, zeta_planet, eps_lapse]
        - alpha_planet in [0,1]: learning rate for planet values V(state).
        - alpha_alien in [0,1]: learning rate for second-stage Q2 and for updating Q1_MF.
        - beta in [0,10]: inverse temperature used at both stages.
        - zeta_planet in [0,1]: weight on planet-value-based planning at stage 1
                                (1=only planet values, 0=only cached MF Q1).
        - eps_lapse in [0,1]: lapse probability mixed with uniform random responding at both stages.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_planet, alpha_alien, beta, zeta_planet, eps_lapse = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    V = np.zeros(2)        # planet values V(state)
    q2 = np.zeros((2, 2))  # second-stage Q-values
    q1_mf = np.zeros(2)    # cached first-stage model-free values

    for t in range(n_trials):
        # Model-based stage-1 values from planet values
        q1_planet = transition_matrix @ V

        # Blend planet-based and cached MF first-stage values
        q1 = zeta_planet * q1_planet + (1.0 - zeta_planet) * q1_mf

        # Stage 1 policy with lapse
        q1_shift = q1 - np.max(q1)
        soft1 = np.exp(beta * q1_shift)
        soft1 = soft1 / np.sum(soft1)
        probs_1 = (1.0 - eps_lapse) * soft1 + eps_lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with lapse
        s = state[t]
        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        soft2 = np.exp(beta * q2_shift)
        soft2 = soft2 / np.sum(soft2)
        probs_2 = (1.0 - eps_lapse) * soft2 + eps_lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second-stage action value
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_alien * delta2

        # Update planet value from reward (state-value learning)
        deltaV = r - V[s]
        V[s] += alpha_planet * deltaV

        # Update cached first-stage MF value toward current second-stage value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_alien * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss