def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-adaptive hybrid (MB+MF) with learned transitions and separate stage temperatures.

    This model combines model-free (MF) and model-based (MB) control at the first stage.
    It learns the second-stage Q-values from reward, and it learns the first-stage
    transition model from experienced transitions. The MB/MF arbitration weight is
    adaptively down-weighted by transition uncertainty: when transition probabilities
    are close to 0.5 (high uncertainty), the model relies more on MF; when transitions
    are reliable (near 0 or 1), it relies more on MB. Softmax temperatures are separate
    for stage 1 and stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1).
    reward : array-like of float
        Obtained reward (e.g., 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha, beta1, beta2, tau_T, omega0]
        - alpha in [0,1]: Learning rate for MF value updates at stage 2 and bootstrapped stage 1.
        - beta1 in [0,10]: Inverse temperature for stage-1 softmax.
        - beta2 in [0,10]: Inverse temperature for stage-2 softmax.
        - tau_T in [0,1]: Learning rate for transition model updates.
        - omega0 in [0,1]: Baseline weight on MB control; effective omega is modulated by transition uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, tau_T, omega0 = model_parameters
    n_trials = len(action_1)

    # Transition model T[a, s]: P(state=s | first-stage action=a)
    # Initialize as uncertain to allow adaptive arbitration to matter.
    T = np.full((2, 2), 0.5)

    # MF Q-values
    q1_mf = np.zeros(2)        # stage-1 MF values
    q2 = np.zeros((2, 2))      # stage-2 MF values: rows=state (X=0, Y=1), cols=action (0/1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute MB values at stage 1 via learned transitions
        max_q2 = np.max(q2, axis=1)         # value of best second-stage action in each state
        q1_mb = T @ max_q2                  # expected value of each first-stage action under learned T

        # Compute transition uncertainty for each first-stage action as 2*min(p,1-p) (0= certain, 1= maximally uncertain)
        unc = 2.0 * np.minimum(T[:, 0], 1.0 - T[:, 0])  # uncertainty about reaching state 0; equals that for state 1 in 2-state case
        # Adaptive MB weight per action: reduce MB weight when transitions are uncertain
        omega = np.clip(omega0 * (1.0 - unc), 0.0, 1.0)

        # Hybrid action values at stage 1
        q1_hyb = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy
        q1c = q1_hyb - np.max(q1_hyb)
        pi1 = np.exp(beta1 * q1c)
        pi1 = pi1 / np.sum(pi1)
        a1 = action_1[t]
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy (within visited state)
        s = state[t]
        q2s = q2[s, :]
        q2c = q2s - np.max(q2s)
        pi2 = np.exp(beta2 * q2c)
        pi2 = pi2 / np.sum(pi2)
        a2 = action_2[t]
        p_choice_2[t] = pi2[a2]

        r = reward[t]

        # Learning: stage-2 MF TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Bootstrapped update to stage-1 MF value of chosen first-stage action
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Learn transition model from observed transition (a1 -> s)
        # Binary outcome per action: s==0 implies outcome 1 for state 0; update both columns to sum to 1
        # Update for state 0 probability with delta rule
        outcome_s0 = 1.0 if s == 0 else 0.0
        T[a1, 0] += tau_T * (outcome_s0 - T[a1, 0])
        # enforce normalization to 2 states
        T[a1, 1] = 1.0 - T[a1, 0]

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Asymmetric TD learning with eligibility trace and value forgetting.

    This is a purely model-free controller that:
    - Uses separate learning rates for positive vs. negative TD errors (asymmetric learning).
    - Propagates second-stage TD error to the chosen first-stage action via an eligibility trace (lambda).
    - Applies value forgetting (decay toward zero) to all Q-values each trial.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage action within state (0/1).
    reward : array-like of float
        Received reward (e.g., 0 or 1).
    model_parameters : iterable of 5 floats
        [alpha_pos, alpha_neg, beta, lambda_et, decay]
        - alpha_pos in [0,1]: Learning rate applied when TD error is >= 0.
        - alpha_neg in [0,1]: Learning rate applied when TD error is < 0.
        - beta in [0,10]: Inverse temperature for both stages.
        - lambda_et in [0,1]: Eligibility trace coefficient for bootstrapping from stage 2 to stage 1.
        - decay in [0,1]: Value forgetting rate applied to all Q-values each trial (higher = more forgetting).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, lambda_et, decay = model_parameters
    n_trials = len(action_1)

    # Initialize Q-values
    q1 = np.zeros(2)          # stage-1 MF Q
    q2 = np.zeros((2, 2))     # stage-2 MF Q

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Softmax stage 1
        q1c = q1 - np.max(q1)
        pi1 = np.exp(beta * q1c)
        pi1 = pi1 / np.sum(pi1)
        a1 = action_1[t]
        p_choice_1[t] = pi1[a1]

        # Softmax stage 2 (within observed state)
        s = state[t]
        q2s = q2[s, :]
        q2c = q2s - np.max(q2s)
        pi2 = np.exp(beta * q2c)
        pi2 = pi2 / np.sum(pi2)
        a2 = action_2[t]
        p_choice_2[t] = pi2[a2]

        r = reward[t]

        # TD update stage 2 with asymmetric learning rates
        delta2 = r - q2[s, a2]
        alpha2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2 * delta2

        # Bootstrapped target to stage 1 via eligibility trace
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        alpha1 = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1[a1] += (lambda_et * alpha1) * delta1

        # Value forgetting toward zero
        q2 *= (1.0 - decay)
        q1 *= (1.0 - decay)

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Heuristic policy: reward- and transition-dependent win-stay/lose-switch with stage-2 stickiness.

    This model does not learn Q-values. Instead, it uses a logistic policy with biases determined by:
    - Whether the previous trial was rewarded (win) or not (loss).
    - Whether the previous transition was common vs. rare.
    - An action bias for choosing spaceship U.
    - A state-conditional stickiness at stage 2 (tendency to repeat the last action within the same state).

    The first-stage utility favors repeating the previous first-stage action after wins,
    but this tendency flips sign following rare transitions (model-based signature),
    scaled by a transition-sensitivity parameter.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage action within state (0/1).
    reward : array-like of float
        Received reward (e.g., 0 or 1).
    model_parameters : iterable of 5 floats
        [beta, ws, tr_sens, biasU, kappa2]
        - beta in [0,10]: Inverse temperature for both stages.
        - ws in [0,1]: Strength of win-stay/lose-switch bias at stage 1.
        - tr_sens in [0,1]: Transition sensitivity; scales how much rare vs. common flips the WS bias.
        - biasU in [0,1]: Constant bias toward choosing spaceship U (action 1) at stage 1.
        - kappa2 in [0,1]: Stage-2 stickiness; bias to repeat the previous action within the same state.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    beta, ws, tr_sens, biasU, kappa2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_s = -1
    prev_a2_in_state = {0: -1, 1: -1}
    prev_reward = 0.0
    prev_common = True

    for t in range(n_trials):
        # -------- Stage 1 policy (logistic with biases) --------
        u1 = np.zeros(2)  # utilities for actions [A=0, U=1]

        # Constant bias toward U
        u1[1] += biasU

        if prev_a1 != -1:
            # Determine effective WS bias sign depending on reward and transition type
            # win_indicator: +1 if previous rewarded, -1 if not
            win_indicator = 1.0 if prev_reward > 0.0 else -1.0
            # transition flip: +1 for common, -1 for rare (model-based signature)
            tr_flip = 1.0 if prev_common else -1.0
            bias_strength = ws * (tr_flip * tr_sens + (1.0 - tr_sens))  # blends MB flip with pure WS

            # Apply bias to repeat previous action
            u1[prev_a1] += bias_strength * win_indicator
            # Optionally, slight counter-bias to the alternative to keep contrast balanced
            u1[1 - prev_a1] -= 0.0  # kept zero to ensure all parameters are used as specified

        # Softmax for stage 1
        u1c = u1 - np.max(u1)
        pi1 = np.exp(beta * u1c)
        pi1 = pi1 / np.sum(pi1)
        a1 = action_1[t]
        p_choice_1[t] = pi1[a1]

        # -------- Stage 2 policy (state-conditional stickiness) --------
        s = state[t]
        u2 = np.zeros(2)
        last_a2 = prev_a2_in_state[s]
        if last_a2 != -1:
            u2[last_a2] += kappa2

        u2c = u2 - np.max(u2)
        pi2 = np.exp(beta * u2c)
        pi2 = pi2 / np.sum(pi2)
        a2 = action_2[t]
        p_choice_2[t] = pi2[a2]

        # -------- Update history --------
        r = reward[t]

        # Determine if the observed transition (a1 -> s) is common or rare based on task structure:
        # A (0) commonly -> X (0); U (1) commonly -> Y (1)
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

        prev_common = is_common
        prev_reward = r
        prev_a1 = a1
        prev_s = s
        prev_a2_in_state[s] = a2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik