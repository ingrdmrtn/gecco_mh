def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Dynamic arbitration with transition trust and surprise-gated MB weight.
    
    A hybrid MB/MF model that:
    - Learns MF values at stage 2 and backs them up to stage 1 MF.
    - Computes MB values with a "trust" parameter that blends the known transition
      with uniform transitions (capturing uncertainty about the mapping).
    - Dynamically adjusts the MB/MF mixing weight based on the previous trial's
      transition surprise: after a surprising (rare) transition, the model shifts
      toward MF control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 for the two aliens within the visited planet.
    reward : array-like of float (0/1)
        Reward received on each trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, w0, tau, u]
        - alpha in [0,1]: learning rate for MF value updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w0 in [0,1]: baseline weight of model-based value at stage 1.
        - tau in [0,1]: transition trust; tau=0 fully trusts 0.7/0.3, tau=1 assumes uniform 0.5/0.5.
        - u in [0,1]: surprise sensitivity; increases reliance on MF after surprising transitions.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w0, tau, u = model_parameters
    n_trials = len(action_1)

    # Base transition and "trusted" transition (flattened toward uniform by tau)
    T_base = np.array([[0.7, 0.3],
                       [0.3, 0.7]])
    T_trust = (1.0 - tau) * T_base + tau * 0.5

    # Value functions
    q1_mf = np.zeros(2)         # MF values for A/U
    q2 = np.zeros((2, 2))       # MF values for aliens on each planet

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Surprise from previous trial; initialize neutral
    prev_trans_p = 0.5

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # MB evaluation for stage 1
        max_q2 = np.max(q2, axis=1)       # value per planet
        q1_mb = T_trust @ max_q2

        # Surprise-gated arbitration (using previous trial's observed transition probability)
        # w_t moves toward MF when prior transition was surprising (low prev_trans_p).
        w_t = w0 * (1.0 - u) + u * (1.0 - prev_trans_p)
        w_t = min(max(w_t, 0.0), 1.0)

        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage 1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Back up to stage-1 MF via TD(1) using realized second-stage action value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update transition surprise for next trial based on the actually observed transition
        prev_trans_p = T_trust[a1, s]

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid SR/MB with decaying perseveration at stage 1.

    Stage-2 values are learned model-free. Stage-1 decisions blend:
    - A model-based (MB) evaluation from the fixed transition structure.
    - A successor-like (SR) component approximated by learning, for each first-stage action,
      the expected occupancy over second-stage states (i.e., which planet is likely).
      This SR is updated by TD toward the one-hot vector of the observed planet.
    The blend between SR-derived value and MF stage-1 value is controlled by z.
    Additionally, a decaying perseveration trace biases repeating the previous stage-1 action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 for the aliens in the visited planet.
    reward : array-like of float (0/1)
        Reward per trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, gamma, z, chi]
        - alpha in [0,1]: learning rate (used for both SR and MF value updates).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - gamma in [0,1]: discount shaping SR target (softens SR toward uniform as gamma increases).
        - z in [0,1]: weight on SR-derived value at stage 1; (1-z) weights MF stage-1 value.
        - chi in [0,1]: strength of perseveration boost added to the previous stage-1 action's logit.
          The perseveration trace decays automatically across trials.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, gamma, z, chi = model_parameters
    n_trials = len(action_1)

    # Fixed transitions for MB computation
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)       # MF values for first-stage actions
    q2 = np.zeros((2, 2))     # Stage-2 MF values

    # SR: expected state occupancy per first-stage action (rows=actions, cols=states)
    # Initialize near-uniform occupancy
    M = np.ones((2, 2)) * 0.5

    # Perseveration trace over first-stage actions
    pers_trace = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # MB and SR-derived values for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # SR-derived value uses learned state occupancy M and state values
        # We softly pull SR target toward uniform with gamma to keep gamma meaningful
        # Target occupancy = (1-gamma)*one_hot(s) + gamma*[0.5, 0.5] (applied during learning below)
        q1_sr = M @ max_q2

        # Blend SR value with MF stage-1 value
        q1 = z * q1_sr + (1.0 - z) * q1_mf

        # Add decaying perseveration at stage 1
        logits1 = beta * q1 + pers_trace
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF backup
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update SR for the chosen stage-1 action toward a soft one-hot target
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        target = (1.0 - gamma) * target + gamma * 0.5
        M[a1] += alpha * (target - M[a1])

        # Update perseveration trace: decay and reinforce chosen action
        pers_trace *= (1.0 - alpha)  # decay tied to learning; larger alpha -> faster decay
        pers_trace[a1] += chi

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Valence-asymmetric learning with reward-rate-adaptive choice temperature.

    This model is primarily model-based at stage 1 (using the fixed transition),
    with stage-2 MF learning that is asymmetric for positive vs. negative prediction
    errors. The decision noise (inverse temperature) adapts online to the recent
    reward rate: when recent rewards are high, choices become more exploitative.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Visited second-stage state.
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float (0/1)
        Reward per trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta0, beta_gain, v, rho]
        - alpha in [0,1]: base learning rate.
        - beta0 in [0,10]: base inverse temperature when reward rate is 0.5.
        - beta_gain in [0,1]: sensitivity of beta to deviations of recent reward rate from 0.5.
        - v in [0,1]: valence asymmetry; alpha_plus = alpha*(1+v), alpha_minus = alpha*(1-v).
        - rho in [0,1]: learning rate for the running average reward (reward-rate estimator).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta0, beta_gain, v, rho = model_parameters
    n_trials = len(action_1)

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # Stage-2 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Running estimate of reward rate
    r_bar = 0.5

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # Adaptive inverse temperature based on current reward rate estimate
        beta_t = beta0 * (1.0 + beta_gain * (r_bar - 0.5))
        beta_t = max(beta_t, 0.0)
        beta_t = min(beta_t, 10.0)

        # Stage-1 MB value from transition and current stage-2 estimates
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        logits1 = beta_t * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta_t * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Valence-asymmetric learning at stage 2
        r = reward[t]
        pe2 = r - q2[s, a2]
        if pe2 >= 0:
            alpha_eff = alpha * (1.0 + v)
        else:
            alpha_eff = alpha * (1.0 - v)
        q2[s, a2] += alpha_eff * pe2

        # Update reward-rate estimate
        r_bar = (1.0 - rho) * r_bar + rho * r

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss