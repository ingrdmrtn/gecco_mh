def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-gated arbitration with learned transitions, model-free TD values, and value decay.

    Mechanism:
    - Learns second-stage MF Q-values via TD(0).
    - Learns the (row-stochastic) first-stage transition model online.
    - Computes a model-based (MB) first-stage value by planning through the learned transitions
      to the max second-stage MF values.
    - Blends MB and MF first-stage values using an arbitration weight that is a decreasing
      function of the current transition uncertainty (higher uncertainty => more MF).
    - Applies a per-trial value decay to both stage-1 and stage-2 Q-values (forgetting).

    Parameters (bounds):
    - a_q in [0, 1]: Learning rate for model-free Q-value updates (second stage) and stage-1 TD.
    - beta in [0, 10]: Inverse temperature for softmax policies (both stages).
    - zeta in [0, 1]: Sensitivity shaping of arbitration to transition uncertainty.
                      Arbitration weight is w = 1 - (uncertainty ** zeta).
    - a_t in [0, 1]: Learning rate for online transition probability updates.
    - tau in [0, 1]: Per-trial value decay strength applied to all Q-values before updates.

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states per trial (0=planet X, 1=planet Y).
    - action_2: array-like of ints in {0,1}, second-stage choices within the observed state.
    - reward: array-like of floats/ints (typically 0/1), outcome per trial.
    - model_parameters: iterable [a_q, beta, zeta, a_t, tau].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    a_q, beta, zeta, a_t, tau = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model rows to be agnostic (0.5/0.5), will be learned online
    T_hat = np.ones((2, 2)) * 0.5  # rows: actions [A,U], cols: states [X,Y]

    # Model-free values
    q1_mf = np.zeros(2)          # stage-1 MF values for [A, U]
    q2_mf = np.zeros((2, 2))     # stage-2 MF values, rows=state [X,Y], cols=actions [0,1]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Apply decay (forgetting) to all Q-values prior to choice and learning
        q1_mf *= (1.0 - tau)
        q2_mf *= (1.0 - tau)

        # Compute arbitration weight from current transition uncertainty
        # Uncertainty = normalized entropy averaged across rows (in [0,1])
        # For a row p, entropy H(p) / ln(2) is in [0,1] for binary distributions
        eps_u = 1e-12
        row_ent = []
        for a in range(2):
            p = T_hat[a]
            # Ensure valid probs
            p = np.clip(p, eps_u, 1.0 - eps_u)
            p /= np.sum(p)
            H = -(p[0] * np.log(p[0]) + p[1] * np.log(p[1])) / np.log(2.0)
            row_ent.append(H)
        uncertainty = 0.5 * (row_ent[0] + row_ent[1])  # average entropy
        w = 1.0 - (uncertainty ** zeta)  # higher uncertainty -> smaller w (more MF)

        # Compute model-based values at stage 1 using learned transitions and current MF Q2
        max_q2 = np.max(q2_mf, axis=1)  # length-2, value of best second-stage action per state
        q1_mb = T_hat @ max_q2          # length-2

        # Blend MB and MF for stage-1 choice
        q1_eff = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1_eff
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure MF over the observed state)
        s = state[t]
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and learn
        r = reward[t]

        # Second-stage TD update
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += a_q * delta2

        # First-stage MF TD update bootstrapping from (updated) second-stage value
        target1 = q2_mf[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += a_q * delta1

        # Update learned transition model for the chosen action row toward the observed state
        # Keep the row stochastic by complementary update on the other column
        T_hat[a1, s] += a_t * (1.0 - T_hat[a1, s])
        other = 1 - s
        T_hat[a1, other] = 1.0 - T_hat[a1, s]

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive model-based planner with volatility-tracking utilities.

    Mechanism:
    - Tracks, for each second-stage action (alien), both an EWMA mean and an EWMA variance
      of rewards over time (volatility).
    - Constructs a risk-sensitive utility for each alien: U = mean - xi * std + kappa * std,
      where xi penalizes variability (risk aversion) and kappa adds a directed exploration
      bonus for uncertain options (optimism under uncertainty).
    - Uses a fixed transition matrix to compute a purely model-based first-stage value by
      planning to the maximum utility on each planet.
    - Second-stage choices are made directly from the per-alien utilities.

    Parameters (bounds):
    - alpha_m in [0, 1]: Learning rate for updating reward means (stage-2).
    - alpha_v in [0, 1]: Learning rate for updating reward variances (stage-2).
    - beta in [0, 10]: Inverse temperature for softmax policy (both stages).
    - xi in [0, 1]: Risk sensitivity; higher values penalize variability more.
    - kappa in [0, 1]: Directed exploration weight; increases preference for uncertain options.

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states per trial (0=planet X, 1=planet Y).
    - action_2: array-like of ints in {0,1}, second-stage choices within the observed state.
    - reward: array-like of floats/ints (typically 0/1), outcome per trial.
    - model_parameters: iterable [alpha_m, alpha_v, beta, xi, kappa].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_m, alpha_v, beta, xi, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed task transitions (commonly A->X, U->Y)
    T = np.array([[0.7, 0.3],  # A to [X, Y]
                  [0.3, 0.7]]) # U to [X, Y]

    # Track per-alien mean and variance (by state, action)
    m = np.zeros((2, 2))  # means
    v = np.zeros((2, 2))  # variances (EWMA of squared deviation)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Construct utilities from current beliefs
        std = np.sqrt(v + 1e-12)
        U = m - xi * std + kappa * std  # combines risk penalty and uncertainty bonus

        # Stage-1 model-based value: expected max utility per planet
        best_U_per_state = np.max(U, axis=1)   # [U_X, U_Y]
        q1_mb = T @ best_U_per_state

        # Stage-1 policy
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy based on utilities in the encountered state
        s = state[t]
        logits2 = beta * U[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update mean/variance for the chosen alien
        r = reward[t]
        # Keep previous mean for correct EWMA variance update
        m_prev = m[s, a2]
        # Mean update
        m[s, a2] += alpha_m * (r - m[s, a2])
        # Variance (EWMA of squared deviation around the previous mean)
        dev2 = (r - m_prev) ** 2
        v[s, a2] += alpha_v * (dev2 - v[s, a2])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor-inspired planning with max-mean blending and choice kernels.

    Mechanism:
    - Second-stage values are learned model-free via TD(0).
    - First-stage planning uses the known transition model, but blends (per state)
      the max and the mean of second-stage values:
        V(state) = (1 - sr_gamma) * max(Q2[state]) + sr_gamma * mean(Q2[state]).
      This provides a smooth bridge between greedy (sr_gamma=0) and averaging (sr_gamma=1).
    - Choice kernels capture recency-driven perseveration at both stages:
      K1 for first-stage actions and K2 for second-stage actions within each state.
      Kernels are updated with a recency learning rate and weighted in the logits.

    Parameters (bounds):
    - alpha_q in [0, 1]: Learning rate for model-free Q-value updates at stage 2.
    - beta in [0, 10]: Inverse temperature for softmax policies (both stages).
    - sr_gamma in [0, 1]: Blend between max and mean second-stage values in planning.
    - kernel_alpha in [0, 1]: Learning rate for updating choice kernels (recency).
    - kernel_weight in [0, 1]: Strength of kernel influence added to action values.

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states per trial (0=planet X, 1=planet Y).
    - action_2: array-like of ints in {0,1}, second-stage choices within the observed state.
    - reward: array-like of floats/ints (typically 0/1), outcome per trial.
    - model_parameters: iterable [alpha_q, beta, sr_gamma, kernel_alpha, kernel_weight].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_q, beta, sr_gamma, kernel_alpha, kernel_weight = model_parameters
    n_trials = len(action_1)

    # Fixed task transitions
    T = np.array([[0.7, 0.3],  # A to [X, Y]
                  [0.3, 0.7]]) # U to [X, Y]

    # Model-free second-stage Q-values
    q2 = np.zeros((2, 2))

    # Choice kernels: first stage (2 actions), second stage per state
    K1 = np.zeros(2)
    K2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Build state values from q2 with max-mean blend
        max_q = np.max(q2, axis=1)
        mean_q = np.mean(q2, axis=1)
        V = (1.0 - sr_gamma) * max_q + sr_gamma * mean_q  # length-2

        # Stage-1 model-based value via known transitions
        q1 = T @ V  # length-2

        # Stage-1 policy with choice kernel bias
        logits1 = beta * (q1 + kernel_weight * K1)
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with choice kernel bias
        s = state[t]
        logits2 = beta * (q2[s] + kernel_weight * K2[s])
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and learn
        r = reward[t]

        # Second-stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * delta2

        # Update choice kernels (recency): move toward one-hot choice
        onehot_a1 = np.zeros(2); onehot_a1[a1] = 1.0
        K1 += kernel_alpha * (onehot_a1 - K1)

        onehot_a2 = np.zeros(2); onehot_a2[a2] = 1.0
        K2[s] += kernel_alpha * (onehot_a2 - K2[s])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll