def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with learned transitions, eligibility trace, and surprise-gated arbitration.
    
    The agent learns:
    - Second-stage (alien) values Q2[s, a2] from reward.
    - First-stage model-free values Q1_mf[a1] via an eligibility trace from the second-stage TD error.
    - A transition model T[a1, s] (probability of reaching planet s after spaceship a1).
    
    Action values at stage 1 are a weighted mixture of model-based (MB) and model-free (MF) values:
    Q1 = omega * Q1_mb + (1 - omega) * Q1_mf, where Q1_mb = T @ max(Q2, axis=1).
    
    Arbitration weight omega is computed online from transition surprise. Larger unexpected
    transitions (|delta_tr|) shift weight toward model-based control:
      omega_t = sigmoid(tau_arb * |delta_tr_t|), where delta_tr_t is the row-wise prediction error
      for the observed transition (a1 -> s).
    
    Parameters (bounds):
    - lr_r (0-1): Learning rate for second-stage reward values Q2.
    - lr_tr (0-1): Learning rate for the transition model T.
    - beta (0-10): Inverse temperature for softmax at both stages.
    - lambda_elig (0-1): Eligibility trace scaling; how strongly second-stage TD error updates Q1_mf.
    - tau_arb (0-1): Sensitivity of arbitration to transition surprise; higher means more MB control after surprising transitions.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [lr_r, lr_tr, beta, lambda_elig, tau_arb]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    lr_r, lr_tr, beta, lambda_elig, tau_arb = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Initialize learned structures
    T = np.ones((2, 2)) * 0.5         # transition model
    Q2 = np.zeros((2, 2))             # second-stage values: planets x aliens
    Q1_mf = np.zeros(2)               # first-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB values from current T and Q2
        max_Q2 = np.max(Q2, axis=1)            # best alien per planet
        Q1_mb = T @ max_Q2                     # MB value per spaceship

        # Arbitration based on transition surprise for chosen action
        # Transition PE vector for the chosen action's row
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        delta_tr_row = target_row - T[a1]
        # Magnitude of surprise for observed transition
        surpr = np.linalg.norm(delta_tr_row, ord=1) / 2.0  # in [0,1]
        # Smooth mapping to [0,1]; more surprise -> larger omega (more MB)
        omega = 1.0 / (1.0 + np.exp(- (tau_arb + eps) * surpr * 10.0))

        Q1 = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Stage 1 policy
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning updates

        # Transition learning
        T[a1] = (1.0 - lr_tr) * T[a1] + lr_tr * target_row
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)  # ensure normalization

        # Second-stage TD learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += lr_r * delta2

        # First-stage MF learning via eligibility trace from second-stage TD error
        Q1_mf[a1] += lambda_elig * delta2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Successor Representation (SR) for stage 1 with lapse noise and stage-2 stay bias.
    
    The agent learns:
    - An SR-like one-step predictive map M[a, s] â‰ˆ P(s | a) (learned via delta-rule).
    - Second-stage values Q2[s, a2] via reward TD.
    
    Decision values:
    - Stage 1: Q1_SR[a] = sum_s M[a, s] * max_a2 Q2[s, a2].
    - Stage 2: softmax over Q2[s, :].
    
    Action selection includes:
    - Lapse noise epsilon_lapse: final policy mixes softmax with a uniform policy.
    - Stage-2 stay bias kappa_s2: adds a bias toward repeating the last second-stage action
      at the same planet (captures choice inertia at the alien level).
    
    Parameters (bounds):
    - alpha_r (0-1): Learning rate for second-stage rewards Q2.
    - alpha_M (0-1): Learning rate for SR/transition map M.
    - beta (0-10): Inverse temperature for softmax at both stages.
    - epsilon_lapse (0-1): Lapse probability; probability of choosing uniformly at random.
    - kappa_s2 (0-1): Strength of stage-2 stay bias added to the chosen alien's logit if repeated.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [alpha_r, alpha_M, beta, epsilon_lapse, kappa_s2]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    alpha_r, alpha_M, beta, epsilon_lapse, kappa_s2 = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Initialize SR-like map and Q-values
    M = np.ones((2, 2)) * 0.5   # predictive map from action to planet
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track last second-stage action per planet for stay bias
    last_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute SR-based first-stage values
        max_Q2 = np.max(Q2, axis=1)
        Q1_sr = M @ max_Q2  # shape (2,)

        # Stage 1 policy with lapse
        logits1 = beta * Q1_sr
        logits1 -= np.max(logits1)
        probs1_soft = np.exp(logits1)
        probs1_soft = probs1_soft / (np.sum(probs1_soft) + eps)
        probs1 = (1.0 - epsilon_lapse) * probs1_soft + epsilon_lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with stay bias and lapse
        logits2 = beta * Q2[s]
        if last_a2[s] != -1 and last_a2[s] < 2:
            logits2[last_a2[s]] += kappa_s2
        logits2 -= np.max(logits2)
        probs2_soft = np.exp(logits2)
        probs2_soft = probs2_soft / (np.sum(probs2_soft) + eps)
        probs2 = (1.0 - epsilon_lapse) * probs2_soft + epsilon_lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning updates

        # SR/transition map update toward observed planet (one-step SR)
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        M[a1] = (1.0 - alpha_M) * M[a1] + alpha_M * target_row
        M[a1] = M[a1] / (np.sum(M[a1]) + eps)

        # Second-stage TD update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * delta2

        # Update last stage-2 action at this planet
        last_a2[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with eligibility trace, rare-transition misattribution, and value decay.
    
    The agent does not maintain an explicit transition model. Instead, it:
    - Learns second-stage values Q2[s, a2] from reward.
    - Updates first-stage MF values Q1_mf via an eligibility trace from the second-stage TD error.
    - Applies misattribution of credit on rare transitions: when the observed planet is rare for the
      chosen spaceship (assuming common mapping A->X, U->Y), a fraction of the credit (phi_mis)
      is assigned to the unchosen spaceship (captures state-dependent choice perseveration vs. credit assignment errors).
    - Applies value decay toward zero each trial to both Q1_mf and Q2 (captures forgetting/drift).
    
    Parameters (bounds):
    - eta_q (0-1): Learning rate for second-stage TD updates (Q2).
    - beta (0-10): Inverse temperature for softmax at both stages.
    - elig (0-1): Eligibility trace strength scaling how much second-stage TD error updates Q1_mf.
    - phi_mis (0-1): Fraction of the eligibility credit given to the unchosen first-stage action on rare transitions.
    - nu_decay (0-1): Per-trial value decay toward 0 for both Q1_mf and Q2.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_q, beta, elig, phi_mis, nu_decay]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_q, beta, elig, phi_mis, nu_decay = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Initialize MF values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage 1 policy from MF values
        logits1 = beta * Q1_mf
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy from Q2 at observed planet
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Apply per-trial decay (forgetting/drift) before learning from current outcome
        Q1_mf *= (1.0 - nu_decay)
        Q2 *= (1.0 - nu_decay)

        # Second-stage TD learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += eta_q * delta2

        # Determine whether the transition was common or rare under fixed mapping
        # Common: A->X (0->0) or U->Y (1->1)
        is_common = (a1 == s)

        # Eligibility update to first-stage MF
        Q1_mf[a1] += elig * delta2

        # Misattribution on rare transitions: credit unchosen action
        if not is_common:
            unchosen = 1 - a1
            Q1_mf[unchosen] += phi_mis * elig * delta2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss