def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free controller with learned transitions and first-stage perseveration.
    
    This model learns:
      - Second-stage action values (Q2) via TD learning from rewards.
      - First-stage model-free values (Q1_MF) via bootstrapping from reached Q2.
      - First-stage transition probabilities T(a1 -> state) via a transition learning rate.
    First-stage choice values are a convex combination of model-based (using T and Q2)
    and model-free (Q1_MF) values, plus a perseveration bias to repeat the previous
    first-stage action. Second-stage choices are softmax over Q2 in the reached state.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage action within the reached state: 0/1 are the two aliens.
    reward : array-like of float (e.g., 0 or 1)
        Obtained reward on each trial.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, omega, eta, kappa]
        - alpha in [0,1]: Learning rate for Q-values (both stages).
        - beta in [0,10]: Inverse temperature for softmax policies at both stages.
        - omega in [0,1]: Weight on model-based values at the first stage (1=fully MB).
        - eta in [0,1]: Transition learning rate for T(a1 -> state).
        - kappa in [0,1]: First-stage perseveration strength (bias to repeat a1).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, omega, eta, kappa = model_parameters
    n_trials = len(action_1)

    # Initialize values
    Q1_MF = np.zeros(2)           # model-free first-stage action values
    Q2 = np.zeros((2, 2))         # second-stage action values per state
    # Initialize transition matrix close to common transitions: A->X, U->Y
    T = np.array([[0.7, 0.3],     # P(state | action_1=0 (A))
                  [0.3, 0.7]])    # P(state | action_1=1 (U))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1

    for t in range(n_trials):
        s = state[t]

        # Model-based first-stage values: expected max Q2 under learned transitions
        maxQ2 = np.max(Q2, axis=1)         # shape (2,)
        Q1_MB = T @ maxQ2                  # shape (2,)

        # Combine MB and MF, add first-stage perseveration
        Q1_eff = omega * Q1_MB + (1.0 - omega) * Q1_MF
        if prev_a1 >= 0:
            Q1_eff[prev_a1] += kappa

        # First-stage policy
        q1 = Q1_eff - np.max(Q1_eff)
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        q2 = Q2[s, :].copy()
        q2 = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second-stage values (TD learning)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Update first-stage model-free value via bootstrapping to reached Q2
        bootstrap = Q2[s, a2]
        delta1 = bootstrap - Q1_MF[a1]
        Q1_MF[a1] += alpha * delta1

        # Learn transitions for the chosen first-stage action
        # Move the chosen row towards the one-hot of observed state
        T[a1, :] = (1.0 - eta) * T[a1, :]
        T[a1, s] += eta
        # Ensure normalization (numerical stability)
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based controller with transition-dependent perseveration and lapses.
    
    This model keeps reward expectations for each alien (state-action values) and uses a
    fixed known transition structure (common: A->X, U->Y) to plan at the first stage.
    It adds a transition-dependent bias: after a common transition, repeat the same a1;
    after a rare transition, switch a1. It also includes a small uniform lapse at both
    stages and second-stage perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage action within the reached state: 0/1 are the two aliens.
    reward : array-like of float (e.g., 0 or 1)
        Obtained reward on each trial.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, bias_c, epsilon, kappa2]
        - alpha in [0,1]: Learning rate for second-stage reward expectations.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - bias_c in [0,1]: Transition-dependent perseveration strength at stage 1.
                           Repeat after common; switch after rare.
        - epsilon in [0,1]: Lapse rate; with prob epsilon choose uniformly at random.
        - kappa2 in [0,1]: Second-stage perseveration (repeat previous a2).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, bias_c, epsilon, kappa2 = model_parameters
    n_trials = len(action_1)

    # Second-stage reward expectations
    Q2 = np.zeros((2, 2))

    # Fixed transition matrix (known task structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_s = -1
    prev_a2 = -1

    for t in range(n_trials):
        s = state[t]

        # Model-based first-stage values: expected max Q2 under fixed T
        maxQ2 = np.max(Q2, axis=1)
        Q1_MB = T @ maxQ2  # shape (2,)

        # Transition-dependent stickiness at stage 1
        bias = np.zeros(2)
        if prev_a1 >= 0 and prev_s >= 0:
            was_common = ((prev_a1 == 0 and prev_s == 0) or (prev_a1 == 1 and prev_s == 1))
            if was_common:
                bias[prev_a1] += bias_c
            else:
                bias[1 - prev_a1] += bias_c

        q1 = Q1_MB + bias
        q1 = q1 - np.max(q1)
        soft_1 = np.exp(beta * q1)
        soft_1 = soft_1 / np.sum(soft_1)
        # Lapse mixture with uniform
        probs_1 = (1.0 - epsilon) * soft_1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with perseveration and lapse
        q2 = Q2[s, :].copy()
        if prev_a2 >= 0:
            q2[prev_a2] += kappa2
        q2 = q2 - np.max(q2)
        soft_2 = np.exp(beta * q2)
        soft_2 = soft_2 / np.sum(soft_2)
        probs_2 = (1.0 - epsilon) * soft_2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second-stage expectations
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        prev_a1 = a1
        prev_s = s
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor-like planning with discount, global forgetting, and asymmetric PE scaling.
    
    At the first stage, the agent plans using a discounted expectation over next-state
    values: Q1(a1) = gamma * sum_s T[a1,s] * max_a Q2[s,a], where T reflects the task's
    common/rare transition structure (fixed here). This is akin to a one-step successor
    representation with discount gamma. At the second stage, Q2 is learned via TD with
    a single learning rate, but the prediction error is multiplicatively scaled based on
    its sign by xi (risk/utility shaping). Additionally, unchosen second-stage actions
    decay toward zero via a forgetting parameter.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage action within the reached state: 0/1 are the two aliens.
    reward : array-like of float (e.g., 0 or 1)
        Obtained reward on each trial.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, gamma, decay, xi]
        - alpha in [0,1]: Learning rate for second-stage Q-values.
        - beta in [0,10]: Inverse temperature for softmax at both stages.
        - gamma in [0,1]: Discount controlling weight of future state values at stage 1.
        - decay in [0,1]: Global forgetting rate for all unchosen Q2 entries each trial.
        - xi in [0,1]: Asymmetric PE scaling: multiply positive PEs by (1+xi),
                       negative PEs by (1-xi).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, gamma, decay, xi = model_parameters
    n_trials = len(action_1)

    # Second-stage values
    Q2 = np.zeros((2, 2))

    # Fixed transition structure used for SR-like planning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]

        # Stage-1 planning via discounted expected next-state values
        maxQ2 = np.max(Q2, axis=1)           # V2(s)
        Q1_SR = gamma * (T @ maxQ2)          # discounted SR-like first-stage values

        # First-stage policy
        q1 = Q1_SR - np.max(Q1_SR)
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        q2 = Q2[s, :] - np.max(Q2[s, :])
        probs_2 = np.exp(beta * q2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD update at second stage with asymmetric PE scaling
        delta2 = r - Q2[s, a2]
        scale = (1.0 + xi) if delta2 >= 0 else (1.0 - xi)
        Q2[s, a2] += alpha * scale * delta2

        # Global forgetting for all unchosen Q2 entries
        # Apply decay to every state-action except the chosen (s, a2)
        for ss in (0, 1):
            for aa in (0, 1):
                if not (ss == s and aa == a2):
                    Q2[ss, aa] *= (1.0 - decay)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll