def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """MB-MF mixture with learned transitions and first-stage perseveration.
    This model learns the state transition function over time and blends model-based
    and model-free estimates at the first stage. It also includes a first-stage
    choice-stickiness bias.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens on the planet).
    reward : array-like of float
        Outcome per trial (coins).
    model_parameters : tuple/list of 5 floats
        - alpha (0 to 1): learning rate for model-free Q-value updates (both stages).
        - beta (0 to 10): inverse temperature for softmax at both stages.
        - eta (0 to 1): transition learning rate to update P(state | action_1).
        - phi (0 to 1): first-stage perseveration strength; added to the previous first-stage action.
        - tau_mf (0 to 1): mixing weight of model-free values at stage 1 (0=MB-only, 1=MF-only).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, eta, phi, tau_mf = model_parameters
    n_trials = len(action_1)

    # Initialize choice likelihood containers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize values
    q1_mf = np.zeros(2)          # model-free values over spaceships
    q2 = np.zeros((2, 2))        # model-free values over aliens: [planet, alien]

    # Learn transitions: start from canonical common/rare structure but adapt via eta
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    prev_a1 = None  # for perseveration at stage 1

    for t in range(n_trials):
        # Model-based component at stage 1 from current transition beliefs
        max_q2 = np.max(q2, axis=1)          # best alien per planet
        q1_mb = T @ max_q2                   # expected value of each spaceship

        # Mixture of MB and MF for stage 1
        q1_mix = (1.0 - tau_mf) * q1_mb + tau_mf * q1_mf

        # Add first-stage perseveration bias (stickiness) to logits
        stick_vec = np.zeros(2)
        if prev_a1 is not None:
            stick_vec[prev_a1] = phi

        logits1 = beta * (q1_mix - np.max(q1_mix)) + stick_vec
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (pure MF softmax within observed planet)
        s = state[t]
        logits2 = beta * (q2[s, :] - np.max(q2[s, :]))
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: stage 2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning: stage 1 MF TD update towards realized second-stage chosen value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Learn transitions from observed (a1 -> s)
        # Move the row T[a1,:] toward the one-hot of s
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)], dtype=float)
        T[a1, :] = (1.0 - eta) * T[a1, :] + eta * oh
        # Row remains normalized by construction

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free SARSA with risk-sensitive utility, value decay, and second-stage stickiness.
    This model uses only model-free learning across both stages. Rewards are transformed
    by a concave/convex utility to capture risk sensitivity, values decay over time,
    and there is a within-planet perseveration bias at the second stage.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens on the planet).
    reward : array-like of float
        Outcome per trial (coins).
    model_parameters : tuple/list of 5 floats
        - alpha (0 to 1): learning rate for MF Q-value updates at both stages.
        - beta (0 to 10): inverse temperature for softmax at both stages.
        - rho (0 to 1): risk-sensitivity for utility; u(r) = sign(r) * |r|^rho.
        - decay (0 to 1): forgetting rate; Q-values shrink by (1 - decay) each trial.
        - sigma2 (0 to 1): second-stage choice stickiness (within visited planet).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, rho, decay, sigma2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)         # over spaceships
    q2 = np.zeros((2, 2))    # over aliens by planet

    # Keep track of last chosen alien per planet for second-stage stickiness
    prev_a2_for_state = [None, None]

    for t in range(n_trials):
        # Apply value decay (forgetting)
        q1 *= (1.0 - decay)
        q2 *= (1.0 - decay)

        # Stage 1 softmax policy
        logits1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy within observed planet with stickiness
        s = state[t]
        stick2 = np.zeros(2)
        if prev_a2_for_state[s] is not None:
            stick2[prev_a2_for_state[s]] = sigma2

        logits2 = beta * (q2[s, :] - np.max(q2[s, :])) + stick2
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Risk-sensitive utility transform
        r = reward[t]
        u = np.sign(r) * (np.abs(r) ** rho)

        # MF learning: stage 2 first
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # MF learning: stage 1 towards realized second-stage chosen value
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * td1

        prev_a2_for_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-guided arbitration between MB and MF with learned transitions and exploration bonus.
    The model learns transition probabilities, updates model-free values, and weights
    MB vs. MF control at stage 1 according to the current transition uncertainty (entropy).
    An additional directed exploration bonus favors first-stage actions with higher
    transition uncertainty.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the two aliens on the planet).
    reward : array-like of float
        Outcome per trial (coins).
    model_parameters : tuple/list of 5 floats
        - alpha2 (0 to 1): learning rate for second-stage MF value updates.
        - beta (0 to 10): inverse temperature for softmax at both stages.
        - tauT (0 to 1): learning rate for transition matrix updates.
        - arb (0 to 1): baseline arbitration level; minimum MB weight when transitions are maximally uncertain.
        - bonus (0 to 1): directed exploration bonus scaling for transition uncertainty at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha2, beta, tauT, arb, bonus = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_mf = np.zeros(2)       # model-free values over spaceships
    q2 = np.zeros((2, 2))     # model-free values over aliens by planet

    # Learn transitions from experience
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    prev_a1 = None  # no explicit stickiness here; arbitration and bonus drive behavior

    def row_entropy(p_row):
        # Binary entropy H(p), max is log(2) ~ 0.693, but we'll normalize by log(2) to keep in [0,1]
        eps = 1e-12
        p = np.clip(p_row, eps, 1.0 - eps)
        H = -(p[0] * np.log(p[0]) + p[1] * np.log(p[1]))
        return H / np.log(2.0)

    for t in range(n_trials):
        # MB component from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Compute per-action transition uncertainty (normalized entropy)
        H_rows = np.array([row_entropy(T[0, :]), row_entropy(T[1, :])])

        # Arbitration weight: more MB when transitions are certain (low entropy)
        # MB weight ranges from arb (at max uncertainty) up to 1 (at zero uncertainty)
        H_avg = 0.5 * (H_rows[0] + H_rows[1])
        wMB = arb + (1.0 - arb) * (1.0 - H_avg)
        wMB = np.clip(wMB, 0.0, 1.0)

        # Combine MB and MF for stage 1
        q1_mix = wMB * q1_mb + (1.0 - wMB) * q1_mf

        # Add directed exploration bonus toward first-stage actions with higher transition uncertainty
        q1_bonus = bonus * H_rows
        logits1 = beta * (q1_mix - np.max(q1_mix)) + q1_bonus
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (pure MF at visited planet)
        s = state[t]
        logits2 = beta * (q2[s, :] - np.max(q2[s, :]))
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Learning: stage-1 MF TD update towards realized second-stage chosen value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha2 * td1

        # Learn transitions from observed (a1 -> s)
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)], dtype=float)
        T[a1, :] = (1.0 - tauT) * T[a1, :] + tauT * oh

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll