def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Surprise-gated hybrid (MB+MF) with single learning rate and cross-stage stickiness.
    
    Idea:
    - Stage-1 choice values are a weighted mixture of model-free (MF) and model-based (MB) values.
    - The MB/MF arbitration weight is dynamically modulated by transition surprise (rare vs common).
    - A single learning rate updates second-stage MF values toward reward and first-stage MF toward the
      value of the reached second-stage chosen action (SARSA-style bootstrapping).
    - Choice stickiness biases repeating the most recent action at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int in {0,1}
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int in {0,1}
        Second-stage choices (aliens within the reached planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, omega0, eta_surprise, stick]
        - alpha: [0,1] learning rate for both stages' MF Q-updates.
        - beta:  [0,10] inverse temperature for both stages' softmax.
        - omega0: [0,1] baseline weight on model-based value at stage 1.
        - eta_surprise: [0,1] strength of modulation of omega by transition surprise
                        (rare transitions increase MB weight; common decrease, via a logit adjustment).
        - stick: [0,1] stickiness weight added to the logit of the previously chosen action at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, omega0, eta_surprise, stick = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common = 0.7, rare = 0.3)
    transition_matrix = np.array([[0.7, 0.3],  # from A to X,Y
                                  [0.3, 0.7]]) # from U to X,Y

    # Probabilities for log-likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)         # stage-1 MF values
    q2_mf = np.zeros((2, 2))    # stage-2 MF values: [state, action]

    # Stickiness trackers
    last_a1 = -1
    last_a2 = {-1: -1, 0: -1, 1: -1}

    # Helper for surprise-gated arbitration using a logit transform
    def clip01(x, eps=1e-6):
        return np.minimum(1 - eps, np.maximum(eps, x))

    def sigmoid(z):
        return 1.0 / (1.0 + np.exp(-z))

    base_logit_omega = np.log(clip01(omega0) / (1 - clip01(omega0)))

    for t in range(n_trials):
        s = state[t]

        # Stage-2 policy with stickiness
        bias2 = np.zeros(2)
        prev_a2 = last_a2.get(s, -1)
        if prev_a2 in (0, 1):
            bias2[prev_a2] += stick
        logits2 = beta * q2_mf[s, :] + bias2
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB computation from MF second-stage values
        max_q2_by_state = np.max(q2_mf, axis=1)  # [X, Y]
        q1_mb = transition_matrix @ max_q2_by_state  # shape (2,)

        # Surprise from previous transition realized now (common vs rare for the chosen a1)
        # We approximate surprise gating for current trial using the just-experienced transition:
        # If A chosen and ended in Y, rare; if U chosen and ended in X, rare; else common.
        a1_prev = action_1[t]
        is_rare = 1.0 if ((a1_prev == 0 and s == 1) or (a1_prev == 1 and s == 0)) else 0.0
        # Modulate omega via logit update toward more MB after rare, less after common
        # Centered surprise term: rare=+0.5, common=-0.5
        centered = (is_rare - 0.5)
        omega_t = sigmoid(base_logit_omega + eta_surprise * (2.0 * centered))
        omega_t = clip01(omega_t)

        # Stage-1 policy: hybrid Q
        q1_hybrid = omega_t * q1_mb + (1.0 - omega_t) * q1_mf

        bias1 = np.zeros(2)
        if last_a1 in (0, 1):
            bias1[last_a1] += stick
        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        r = reward[t]

        # TD learning updates (model-free)
        # Stage-2 update toward reward
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update toward the value of the chosen second-stage action (SARSA-style)
        target1 = q2_mf[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update stickiness trackers
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Leaky model-free SARSA with stage-specific stickiness and value forgetting.
    
    Idea:
    - Purely model-free; no model-based projection.
    - A single learning rate updates second-stage values toward reward and first-stage values toward
      the reached second-stage chosen value (SARSA).
    - Leaky (forgetful) values: at each trial, both stage-1 and stage-2 values decay toward zero.
    - Separate stickiness parameters for stage 1 and stage 2 bias repeating the previous choice.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int in {0,1}
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int in {0,1}
        Second-stage choices (aliens within the reached planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, decay, stick1, stick2]
        - alpha:  [0,1] learning rate for MF Q-updates (both stages).
        - beta:   [0,10] inverse temperature for both stages' softmax.
        - decay:  [0,1] per-trial forgetting rate; Q <- (1 - decay) * Q before learning.
        - stick1: [0,1] stage-1 choice stickiness weight added to the logit of the previous stage-1 action.
        - stick2: [0,1] stage-2 choice stickiness weight added to the logit of the previous action within a state.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, decay, stick1, stick2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)          # stage-1 action values
    q2 = np.zeros((2, 2))     # stage-2 action values

    last_a1 = -1
    last_a2 = {-1: -1, 0: -1, 1: -1}

    for t in range(n_trials):
        s = state[t]

        # Apply forgetting to both stages before computing policy and updating
        q1 *= (1.0 - decay)
        q2 *= (1.0 - decay)

        # Stage-2 policy with stickiness
        bias2 = np.zeros(2)
        prev_a2 = last_a2.get(s, -1)
        if prev_a2 in (0, 1):
            bias2[prev_a2] += stick2
        logits2 = beta * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy with stickiness
        bias1 = np.zeros(2)
        if last_a1 in (0, 1):
            bias1[last_a1] += stick1
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        r = reward[t]

        # TD learning updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Count-based exploration bonus with learned transitions and MB/MF hybridization.
    
    Idea:
    - Learn second-stage MF values via TD.
    - Learn the first-stage transition model from experience via simple counts (Dirichlet-MAP).
    - Stage-2 choices include an uncertainty-driven exploration bonus that decays with visit count.
    - Stage-1 choices use a hybrid of MF and MB values with a fixed mixing weight.
    - First-stage perseveration encourages repeating the last first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int in {0,1}
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int in {0,1}
        Second-stage choices (aliens within the reached planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, mix_mb, bonus, rep1]
        - alpha:  [0,1] learning rate for MF reward learning at stage 2 (and bootstrapped update at stage 1).
        - beta:   [0,10] inverse temperature for both stages' softmax.
        - mix_mb: [0,1] mixture weight for MB value at stage 1 (0=pure MF, 1=pure MB).
        - bonus:  [0,1] scale of exploration bonus added to stage-2 logits: bonus / sqrt(visit_count+1).
        - rep1:   [0,1] first-stage perseveration weight added to the logit of the previous stage-1 action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, mix_mb, bonus, rep1 = model_parameters
    n_trials = len(action_1)

    # Transition counts for learning T(action -> state)
    # Dirichlet prior of 1 per outcome to avoid zeros (uniform prior)
    trans_counts = np.ones((2, 2))  # rows: action A/U; cols: state X/Y

    # MF Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Visit counts for exploration bonus at stage 2
    visit_counts = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = -1

    for t in range(n_trials):
        s = state[t]

        # Estimate transition probabilities from counts (row-normalized)
        row_sums = np.sum(trans_counts, axis=1, keepdims=True)
        T_est = trans_counts / row_sums  # shape (2,2)

        # Stage-2 policy with count-based exploration bonus
        a2 = action_2[t]
        # Uncertainty bonus: higher when action is less explored in that state
        bonus_vec = bonus / np.sqrt(visit_counts[s, :] + 1.0)
        logits2 = beta * q2_mf[s, :] + bonus_vec
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        p_choice_2[t] = probs2[a2]

        # Stage-1 hybrid policy: MB from estimated transitions, MF from learned q1_mf
        max_q2_by_state = np.max(q2_mf, axis=1)
        q1_mb = T_est @ max_q2_by_state
        q1_hybrid = mix_mb * q1_mb + (1.0 - mix_mb) * q1_mf

        # First-stage perseveration
        bias1 = np.zeros(2)
        if last_a1 in (0, 1):
            bias1[last_a1] += rep1

        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        r = reward[t]

        # Learning updates
        # Update transition counts with the actually experienced transition (a1 -> s)
        trans_counts[a1, s] += 1.0

        # Update visit counts for stage-2 state-action
        visit_counts[s, a2] += 1.0

        # Stage-2 MF update
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update (bootstrapping toward second-stage chosen value)
        target1 = q2_mf[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll