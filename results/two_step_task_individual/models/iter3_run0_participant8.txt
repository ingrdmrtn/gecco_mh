def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Transition-dependent model-free learning with perseveration and selective forgetting.

    Mechanism
    - Identify whether the observed transition (from first-stage action to second-stage state)
      was common or rare under the task's fixed mapping (A->X, U->Y with 0.7 common).
    - Use separate model-free learning rates for common vs. rare transitions to update values.
    - Second-stage Q-values (per state and action) are learned toward reward.
    - First-stage Q-values are learned directly toward reward (TD(1)-style model-free).
    - Apply selective forgetting (decay toward zero) to unchosen second-stage actions in the visited state.
    - Softmax choice at both stages; add a first-stage perseveration (stickiness) bias.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state: 0/1 for the two aliens on that planet.
    reward : array-like of float (0 or 1)
        Reward outcome on each trial (coins obtained).
    model_parameters : iterable of 5 floats
        [alpha_common, alpha_rare, beta, kappa, forget]
        - alpha_common in [0,1]: learning rate used on trials with common transitions.
        - alpha_rare in [0,1]: learning rate used on trials with rare transitions.
        - beta in [0,10]: inverse temperature for both stages.
        - kappa in [0,1]: first-stage perseveration weight added to last chosen action's preference.
        - forget in [0,1]: per-trial decay applied to the unchosen second-stage action in the visited state.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_common, alpha_rare, beta, kappa, forget = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure of the task
    # Spaceship A (0) commonly -> X (0); U (1) commonly -> Y (1)
    p_common = 0.7

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)        # first-stage model-free Q
    q2 = np.zeros((2, 2))   # second-stage Q: state x action

    prev_a1 = -1

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Identify common vs rare transition given action and observed state
        expected_s = a1  # by task design: A->X(0), U->Y(1)
        is_common = (s == expected_s)
        alpha = alpha_common if is_common else alpha_rare

        # First-stage policy with perseveration
        pref1 = q1.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += kappa
        pref1 -= np.max(pref1)
        probs1 = np.exp(beta * pref1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (no perseveration at this stage)
        pref2 = q2[s, :].copy()
        pref2 -= np.max(pref2)
        probs2 = np.exp(beta * pref2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Updates
        # Second-stage MF update toward reward
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Selective forgetting for the unchosen second-stage action in the visited state
        other_a2 = 1 - a2
        q2[s, other_a2] *= (1.0 - forget)

        # First-stage MF update directly toward reward (TD(1))
        pe1 = r - q1[a1]
        q1[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Surprise-based arbitration between learned model-based and model-free values.

    Mechanism
    - Learn a first-stage transition model M (rows: actions A/U; cols: states X/Y) via delta rule.
    - Learn second-stage Q-values toward reward.
    - Learn a first-stage model-free value toward the second-stage action value (eligibility-like).
    - Compute a dynamic arbitration weight w_t based on transition surprise:
        surprise_t = 1 - M[a1, s]; weight = clip(w0 + arb * (surprise_t - 0.5), 0, 1).
      When transitions are surprising (rare under learned M), arbitration favors model-based control.
    - First-stage value used for choice: q1 = (1 - w_t) * q1_mf + w_t * q1_mb.
    - Softmax choice at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha_q, beta, w0, arb, alpha_m]
        - alpha_q in [0,1]: learning rate for second-stage Q and first-stage MF value.
        - beta in [0,10]: inverse temperature for both stages.
        - w0 in [0,1]: baseline arbitration weight (0=MF, 1=MB).
        - arb in [0,1]: sensitivity of arbitration to transition surprise.
        - alpha_m in [0,1]: learning rate for the transition model M.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_q, beta, w0, arb, alpha_m = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize transition model M to uniform (row-stochastic)
    M = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage values from learned transition model and current second-stage values
        v2 = np.max(q2, axis=1)
        q1_mb = M @ v2

        # Dynamic arbitration weight from surprise
        surprise = 1.0 - M[a1, s]  # in [0,1]
        w_t = w0 + arb * (surprise - 0.5)
        if w_t < 0.0:
            w_t = 0.0
        elif w_t > 1.0:
            w_t = 1.0

        q1 = (1.0 - w_t) * q1_mf + w_t * q1_mb

        # First-stage policy
        pref1 = q1.copy()
        pref1 -= np.max(pref1)
        probs1 = np.exp(beta * pref1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        pref2 = q2[s, :].copy()
        pref2 -= np.max(pref2)
        probs2 = np.exp(beta * pref2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Second-stage Q-learning toward reward
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # First-stage MF toward second-stage chosen action value (eligibility-like)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # Update transition model row for the chosen first-stage action
        target_trans = np.array([0.0, 0.0])
        target_trans[s] = 1.0
        M[a1, :] = (1.0 - alpha_m) * M[a1, :] + alpha_m * target_trans
        # Normalize to ensure row-stochasticity (guards against numerical drift)
        row_sum = np.sum(M[a1, :]) + 1e-12
        M[a1, :] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Utility-curved SARSA(λ)-style model-free learning with choice lapse.

    Mechanism
    - Transform outcomes by a concave utility u(r) = r^eta (risk/utility curvature).
    - Second-stage Q-values learned toward u(r).
    - First-stage Q-values learned toward a mixture target:
        target1 = trace * u(r) + (1 - trace) * Q2[s, a2]
      so that trace controls how directly reward propagates to stage-1 vs. via stage-2 value.
    - Choices follow softmax with inverse temperature beta at each stage,
      mixed with a lapse rate epsilon (uniform random responding component).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, trace, eta, epsilon]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - trace in [0,1]: eligibility-like mixing weight from reward to stage-1 update (λ analog).
        - eta in [0,1]: utility curvature for reward (0 = flat, 1 = linear).
        - epsilon in [0,1]: lapse rate; probability mass mixed with a uniform policy (0.5 per action).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, trace, eta, epsilon = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Utility-transformed reward
        u = (r + 1e-12) ** eta  # stable even if r=0

        # First-stage policy (softmax with lapse)
        pref1 = q1.copy()
        pref1 -= np.max(pref1)
        sm1 = np.exp(beta * pref1)
        sm1 /= np.sum(sm1)
        probs1 = (1.0 - epsilon) * sm1 + epsilon * 0.5
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (softmax with lapse)
        pref2 = q2[s, :].copy()
        pref2 -= np.max(pref2)
        sm2 = np.exp(beta * pref2)
        sm2 /= np.sum(sm2)
        probs2 = (1.0 - epsilon) * sm2 + epsilon * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Second-stage update toward utility
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First-stage update toward mixture of utility and second-stage value
        target1 = trace * u + (1.0 - trace) * q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll