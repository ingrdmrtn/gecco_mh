def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB planner with fixed-vs-learned transition blending and reward sensitivity.
    
    This model learns the transition structure online from experience (Dirichlet-like
    counting) and blends it with the known canonical transition matrix (0.7/0.3).
    Stage-1 values are computed model-based from the blended transitions and the
    current stage-2 action values. Stage-2 values are learned via TD with a reward
    sensitivity parameter that contracts/expands outcomes around 0.5. Separate
    inverse temperatures are used for each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices A=0, U=1 per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial (X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choice per trial within the reached state.
    reward : array-like of float (e.g., 0/1)
        Obtained outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta1, beta2, c_T, c_r]
        - alpha: stage-2 learning rate in [0,1]
        - beta1: stage-1 inverse temperature in [0,10]
        - beta2: stage-2 inverse temperature in [0,10]
        - c_T: blend between learned and fixed transitions in [0,1]
               T_eff = c_T * T_learned + (1 - c_T) * T_fixed
        - c_r: reward sensitivity in [0,1], transforms reward around 0.5 via
               r_eff = 0.5 + c_r * (reward - 0.5)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """
    alpha, beta1, beta2, c_T, c_r = model_parameters
    n_trials = len(action_1)

    # Fixed canonical transition matrix (rows: a1 in {A,U}, cols: state in {X,Y})
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Learned transition counts (Dirichlet-like with uniform prior)
    counts = np.ones((2, 2), dtype=float)  # start uninformative
    T_learned = counts / counts.sum(axis=1, keepdims=True)

    # Action probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values: Q2[state, action2]
    q2 = np.zeros((2, 2), dtype=float)

    for t in range(n_trials):

        # Compute blended transition model and MB stage-1 values
        T_eff = c_T * T_learned + (1.0 - c_T) * T_fixed
        max_q2 = np.max(q2, axis=1)  # best available value in each state
        q1_mb = T_eff @ max_q2       # model-based backup

        # Stage-1 policy (softmax with beta1)
        pref1 = beta1 * q1_mb
        pref1 -= np.max(pref1)
        probs_1 = np.exp(pref1) / np.sum(np.exp(pref1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (softmax with beta2)
        s = state[t]
        pref2 = beta2 * q2[s, :]
        pref2 -= np.max(pref2)
        probs_2 = np.exp(pref2) / np.sum(np.exp(pref2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome with reward sensitivity around 0.5
        r = reward[t]
        r_eff = 0.5 + c_r * (r - 0.5)

        # Update learned transitions from observed transition (a1 -> s)
        counts[a1, s] += 1.0
        T_learned = counts / counts.sum(axis=1, keepdims=True)

        # TD update at stage 2
        delta2 = r_eff - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free dual-stage TD with choice kernels and outcome curvature.
    
    This model learns values purely model-free:
    - Stage 2: Q2 is updated toward the received (nonlinearly transformed) outcome.
    - Stage 1: Q1 is updated toward the realized Stage-2 action value (bootstrapping).
    Choice kernels at both stages capture short-term perseveration/exploration and
    are learned via their own learning rate and weighted into the logits.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached state.
    reward : array-like of float/int
        Obtained outcome per trial (e.g., 0/1).
    model_parameters : iterable of floats
        [alpha, beta, kappa_lr, kappa_w, gamma]
        - alpha: TD learning rate for Q-values in [0,1]
        - beta: inverse temperature applied at both stages in [0,10]
        - kappa_lr: choice-kernel learning/decay rate in [0,1]
        - kappa_w: weight of choice kernels added to logits in [0,1]
        - gamma: outcome curvature in [0,1]; reward is transformed as r_eff = reward**gamma
                 (concave for gamma<1, linear at gamma=1)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """
    alpha, beta, kappa_lr, kappa_w, gamma = model_parameters
    n_trials = len(action_1)

    # Values
    q1 = np.zeros(2, dtype=float)        # Q1[action_1]
    q2 = np.zeros((2, 2), dtype=float)   # Q2[state, action_2]

    # Choice kernels (tendency to repeat) per stage
    ck1 = np.zeros(2, dtype=float)       # kernel over first-stage actions
    ck2 = np.zeros((2, 2), dtype=float)  # kernel per state over second-stage actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage-1 policy: Q1 + choice kernel influence
        logits1 = beta * q1 + kappa_w * ck1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy: Q2[s,:] + choice kernel influence
        s = state[t]
        logits2 = beta * q2[s, :] + kappa_w * ck2[s, :]
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward transformation
        r = reward[t]
        r_eff = (r + 1e-12) ** gamma  # keep numerically safe for r=0

        # TD updates
        # Stage-2
        delta2 = r_eff - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 bootstrapping towards realized second-stage action value
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

        # Update choice kernels (decay-all + increment chosen)
        # Stage 1 kernel
        ck1 *= (1.0 - kappa_lr)
        ck1[a1] += kappa_lr * (1.0 - ck1[a1])
        # Stage 2 kernel
        ck2[s, :] *= (1.0 - kappa_lr)
        ck2[s, a2] += kappa_lr * (1.0 - ck2[s, a2])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with transition-dependent reinforcement bias and forgetting.
    
    The model plans at stage 1 using the fixed transition matrix (0.7/0.3) to back up
    stage-2 values. It includes:
      - A transition-dependent reinforcement (TDR) bias: after each trial, the next
        stage-1 logits receive a bias to repeat vs. switch the previous action depending
        on whether the previous transition was common or rare and whether it was rewarded.
      - Value forgetting at stage 2.
      - Lapse (epsilon) that mixes softmax with uniform choice at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the reached state.
    reward : array-like of float/int
        Obtained outcome per trial.
    model_parameters : iterable of floats
        [alpha, beta, nu, zeta, eps]
        - alpha: stage-2 learning rate in [0,1]
        - beta: inverse temperature at both stages in [0,10]
        - nu: strength of transition-dependent reinforcement bias in [0,1]
              Applied to the previous first-stage choice on the next trial:
              after reward: +nu if common, -nu if rare; after no reward: -nu if common, +nu if rare.
        - zeta: forgetting/decay rate on Q2 in [0,1] applied each trial to all Q2 entries
        - eps: lapse rate in [0,1]; final policy = (1-eps)*softmax + eps*uniform
    
    Returns
    -------
    float
        Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """
    alpha, beta, nu, zeta, eps = model_parameters
    n_trials = len(action_1)

    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Variables needed for TDR bias
    last_a1 = None
    last_s = None
    last_r = None

    for t in range(n_trials):
        # Model-based stage-1 values from fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Transition-dependent reinforcement bias applied to logits at t
        bias1 = np.zeros(2, dtype=float)
        if last_a1 is not None:
            # Determine if last transition was common or rare
            was_common = ((last_a1 == 0 and last_s == 0) or (last_a1 == 1 and last_s == 1))
            # Compute signed bias magnitude based on last reward and transition type
            if last_r > 0:
                bmag = nu if was_common else -nu
            else:
                bmag = -nu if was_common else nu
            # Apply bias to repeat vs. switch
            bias1[last_a1] += bmag
            bias1[1 - last_a1] -= bmag

        # Stage-1 policy with lapse
        pref1 = beta * q1_mb + bias1
        pref1 -= np.max(pref1)
        soft1 = np.exp(pref1) / np.sum(np.exp(pref1))
        probs_1 = (1.0 - eps) * soft1 + eps * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with lapse
        s = state[t]
        pref2 = beta * q2[s, :]
        pref2 -= np.max(pref2)
        soft2 = np.exp(pref2) / np.sum(np.exp(pref2))
        probs_2 = (1.0 - eps) * soft2 + eps * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning with forgetting
        r = reward[t]
        q2 *= (1.0 - zeta)  # decay all Q2 entries
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Store for next trial's TDR bias
        last_a1 = a1
        last_s = s
        last_r = r

    eps_small = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps_small)) + np.sum(np.log(p_choice_2 + eps_small)))
    return nll