def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MBâ€“MF with transition-surprise gated model-based control and stage-1 perseveration.

    This model combines model-based (MB) and model-free (MF) control at stage 1 with
    a dynamically updated weight. The MB/MF weight increases after predictable (common)
    transitions and decreases after surprising (rare) transitions. Stage-2 uses a standard
    value update. A perseveration bias encourages repeating the previous stage-1 choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship per trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet per trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien per trial at the visited planet.
    reward : array-like of float/int (0 or 1)
        Coins received per trial.
    model_parameters : sequence of floats
        [alpha, beta, w0, tauS, phi]
        - alpha (learning rate, [0,1]): for updating both second-stage values and MF stage-1 values.
        - beta (inverse temperature, [0,10]): softmax for both stages.
        - w0 (initial MB weight, [0,1]): initial reliance on model-based planning at stage 1.
        - tauS (surprise adaptation rate, [0,1]): updates MB weight based on transition surprise.
            New weight w <- (1 - tauS)*w + tauS * P(observed_state | chosen_action).
            Thus, w increases after common transitions and decreases after rare transitions.
        - phi (perseveration, [0,1]): additive bias to repeat the previous stage-1 action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w0, tauS, phi = model_parameters
    n_trials = len(action_1)

    # Known transition structure (common = 0.7, rare = 0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q2 = np.full((2, 2), 0.5)     # second-stage values (state x action), optimistic neutral init
    q1_mf = np.zeros(2)           # stage-1 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Dynamic MB weight and perseveration memory
    w = w0
    last_a1 = None

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based stage-1 values from transition model and current q2
        max_q2 = np.max(q2, axis=1)           # best second-stage action per state
        q1_mb = T @ max_q2                    # expected value for each spaceship

        # Hybrid combination with dynamic weight
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf

        # Perseveration bias
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += phi

        # Stage-1 policy
        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure softmax on q2 at observed state)
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Updates
        # 1) Update MB weight using transition surprise (probability of observed state)
        p_obs = T[a1, s2]
        w = (1.0 - tauS) * w + tauS * p_obs
        w = np.clip(w, 0.0, 1.0)

        # 2) Second-stage value update
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # 3) Stage-1 MF update using bootstrapped target from second stage
        target1 = q2[s2, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """MB planner with uncertainty bonus, lapse, and choice-kernel stickiness at both stages.

    This model plans using the known transition structure and an uncertainty-driven
    exploration bonus at stage 2. A lapse parameter mixes in uniform random choice.
    Choice kernels (one for stage-1 and one per state at stage-2) capture short-term
    perseveration/stickiness and are learned online.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship per trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet per trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien per trial at the visited planet.
    reward : array-like of float/int (0 or 1)
        Coins received per trial.
    model_parameters : sequence of floats
        [alpha, beta, ubonus, lapse, phiK]
        - alpha (reward learning rate, [0,1]): updates second-stage reward expectations (p_hat).
        - beta (inverse temperature, [0,10]): softmax gain for both stages.
        - ubonus (uncertainty bonus weight, [0,1]): scales p*(1-p) bonus at stage-2.
        - lapse (uniform lapse probability, [0,1]): with probability 'lapse' select uniformly at random.
        - phiK (choice-kernel learning rate/strength, [0,1]): governs both the update rate and
          the magnitude of additive biases from choice kernels at both stages.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, ubonus, lapse, phiK = model_parameters
    n_trials = len(action_1)

    # Known transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Reward probability estimates and uncertainty
    p_hat = np.full((2, 2), 0.5)  # estimated reward probabilities per state-action
    # Choice kernels (additive biases)
    K1 = np.zeros(2)              # stage-1 kernel over spaceships
    K2 = np.zeros((2, 2))         # stage-2 kernel per state over aliens

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Uncertainty bonus: p*(1-p)
        unc = p_hat * (1.0 - p_hat)
        q2_eff = p_hat + ubonus * unc

        # Stage-1 MB planning using q2_eff
        max_q2 = np.max(q2_eff, axis=1)
        q1 = T @ max_q2

        # Stage-1 policy with choice-kernel bias
        logits1 = beta * q1 + phiK * K1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5  # lapse to uniform over 2 actions
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with choice-kernel bias
        logits2 = beta * q2_eff[s2] + phiK * K2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        probs2 = (1.0 - lapse) * probs2 + lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Updates
        # 1) Reward model update (p_hat)
        p_hat[s2, a2] += alpha * (r - p_hat[s2, a2])

        # 2) Choice kernels update: move chosen toward 1, others toward 0 (decay)
        # Stage-1
        for a in range(2):
            target = 1.0 if a == a1 else 0.0
            K1[a] += phiK * (target - K1[a])
        # Stage-2 (for the visited state only)
        for a in range(2):
            target = 1.0 if a == a2 else 0.0
            K2[s2, a] += phiK * (target - K2[s2, a])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor-like planning at stage-1 with eligibility-weighted MF blending, utility asymmetry, and forgetting.

    This model learns a cached mapping from stage-1 actions to stage-2 states (a simple
    successor representation, SR) and uses it to plan. It blends SR-based values with
    a model-free (MF) stage-1 value using an eligibility parameter. Rewards are transformed
    to allow asymmetric valuation of rewards vs. no-rewards. Second-stage values undergo
    trial-by-trial forgetting toward a neutral baseline.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship per trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet per trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien per trial at the visited planet.
    reward : array-like of float/int (0 or 1)
        Coins received per trial.
    model_parameters : sequence of floats
        [alpha, beta, kappaE, mu, omegaF]
        - alpha (learning rate, [0,1]): updates second-stage values and MF stage-1 values.
        - beta (inverse temperature, [0,10]): softmax for both stages.
        - kappaE (eligibility/SR strength, [0,1]):
            (i) learning rate for updating the SR row of the chosen action toward the observed state;
            (ii) mixing weight between SR-based and MF stage-1 values: Q1 = kappaE*Q1_SR + (1-kappaE)*Q1_MF.
        - mu (utility asymmetry, [0,1]): transforms rewards as u = r - mu*(1 - r),
          i.e., no-reward is valued as -mu, enabling loss sensitivity.
        - omegaF (forgetting, [0,1]): per-trial decay of second-stage values toward 0.5.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, kappaE, mu, omegaF = model_parameters
    n_trials = len(action_1)

    # Initialize SR-like mapping from actions to states; start from common/rare structure
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: actions, cols: states

    # Value functions
    q2 = np.full((2, 2), 0.5)  # second-stage values (state x action)
    q1_mf = np.zeros(2)        # model-free stage-1 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r_raw = reward[t]

        # Utility transform for asymmetric valuation
        u = r_raw - mu * (1.0 - r_raw)  # 1 -> 1, 0 -> -mu

        # Forgetting at stage-2 toward 0.5 baseline
        q2 = (1.0 - omegaF) * q2 + omegaF * 0.5

        # SR-based stage-1 values using current mapping and q2
        max_q2 = np.max(q2, axis=1)      # best second-stage action per state
        q1_sr = M @ max_q2               # expected value for each stage-1 action

        # Blend SR and MF
        q1 = kappaE * q1_sr + (1.0 - kappaE) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Updates
        # 1) Update SR mapping row for chosen action toward one-hot observed state
        oh = np.array([0.0, 0.0])
        oh[s2] = 1.0
        M[a1] = (1.0 - kappaE) * M[a1] + kappaE * oh
        # keep row normalized
        M[a1] = np.clip(M[a1], 1e-6, 1.0)
        M[a1] /= np.sum(M[a1])

        # 2) Second-stage value update with transformed utility
        delta2 = u - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # 3) Stage-1 MF update using bootstrapped target from current second-stage value
        target1 = q2[s2, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll