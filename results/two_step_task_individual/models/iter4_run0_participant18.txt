def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Surprise-gated hybrid with learned transitions and selective forgetting.
    
    This model learns the transition matrix online and combines model-based (MB)
    and model-free (MF) action values at stage 1 without an explicit mixing
    parameter: MF influence emerges via learning and forgetting. The eligibility
    trace passed from stage 2 to stage 1 is scaled by transition surprise, so
    rare/unexpected transitions strengthen MF credit assignment to the chosen
    first-stage action. Unchosen action values decay (forgetting) at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 for the two aliens on the visited planet.
    reward : array-like of float (typically 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha, beta, eta_T, zeta_surprise, phi_forget]
        - alpha (learning rate, [0,1]): MF learning rate for Q-value updates.
        - beta (inverse temperature, [0,10]): Softmax sensitivity at both stages.
        - eta_T (transition learning rate, [0,1]): Step size for learning the
          transition probabilities of each spaceship.
        - zeta_surprise (surprise-to-eligibility scaling, [0,1]): Scales the
          eligibility trace by the transition surprise (1 - predicted prob).
        - phi_forget (forgetting/decay, [0,1]): Decay applied to unchosen actions'
          values at both stages on each trial.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, eta_T, zeta_surprise, phi_forget = model_parameters
    n_trials = len(action_1)

    # Initialize with the canonical common/rare structure as a prior
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: a1 (A,U); cols: next state (X,Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)       # stage-1 MF values (A,U)
    q2 = np.zeros((2, 2))     # stage-2 MF values for each planet/state and alien

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # MB action values from current learned transition matrix
        max_q2 = np.max(q2, axis=1)  # best alien value on each planet
        q1_mb = T @ max_q2           # expectation over next-state values

        # Combine MB and MF additively (implicit, data-driven mixture)
        q1_eff = q1_mb + q1_mf

        # Stage-1 policy
        q1_centered = q1_eff - np.max(q1_eff)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        q2_eff = q2[s].copy()
        q2_centered = q2_eff - np.max(q2_eff)
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Save old values for TD errors and apply selective forgetting (unchosen decay)
        q2_old = q2[s, a2]
        # Forget unchosen stage-2 actions (both aliens on visited state except chosen)
        for a in (0, 1):
            if a != a2:
                q2[s, a] *= (1.0 - phi_forget)
        # Also decay unvisited states' aliens slightly (optional but consistent)
        other_state = 1 - s
        q2[other_state, 0] *= (1.0 - phi_forget)
        q2[other_state, 1] *= (1.0 - phi_forget)

        # TD update at stage 2
        delta2 = r - q2_old
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update: bootstrap + surprise-gated eligibility
        # Forget unchosen stage-1 action
        unchosen_a1 = 1 - a1
        q1_mf[unchosen_a1] *= (1.0 - phi_forget)

        # Bootstrap target uses current (post-update) q2[s, a2]
        delta1_boot = q2[s, a2] - q1_mf[a1]
        # Compute transition surprise before updating T (use current T prediction)
        p_pred = T[a1, s]
        surprise = 1.0 - p_pred
        lam_eff = zeta_surprise * surprise
        q1_mf[a1] += alpha * delta1_boot + lam_eff * alpha * delta2

        # Learn transition probabilities for the chosen spaceship toward the observed state
        # One-step Bayes-like/Rescorla-Wagner update on the chosen row
        for sp_state in (0, 1):
            target = 1.0 if sp_state == s else 0.0
            T[a1, sp_state] += eta_T * (target - T[a1, sp_state])
        # Ensure row remains normalized and bounded due to numeric issues
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = np.clip(T[a1] / row_sum, 1e-6, 1 - 1e-6)
            T[a1] /= np.sum(T[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Asymmetric MF learning with transition-contingent stickiness and lapses.
    
    A purely model-free controller with different learning rates for rewards vs.
    no-rewards at both stages. The first-stage policy includes a transition-
    contingent stickiness bias: the agent tends to repeat the last first-stage
    action after a common transition but tends to switch after a rare transition.
    A lapse parameter mixes the softmax with uniform random choice at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 for the two aliens on the visited planet.
    reward : array-like of float (typically 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha_pos, alpha_neg, beta, chi, xi]
        - alpha_pos (learning rate for rewarded outcomes, [0,1])
        - alpha_neg (learning rate for non-rewarded outcomes, [0,1])
        - beta (inverse temperature, [0,10]) for both stages
        - chi (transition-contingent stickiness strength, [0,1]): bias magnitude
          to repeat after common or to switch after rare transitions.
        - xi (lapse rate, [0,1]): mixture with uniform random choice (0.5) at both stages.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, chi, xi = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1 = np.zeros(2)          # stage-1 action values (A,U)
    q2 = np.zeros((2, 2))     # stage-2 action values (state x alien)

    # For transition-contingent stickiness
    last_a1 = None
    last_s = None

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Build transition-contingent stickiness bias for stage 1
        bias = np.zeros(2)
        if last_a1 is not None and last_s is not None:
            # Common if A->X or U->Y; rare otherwise
            was_common = (last_a1 == 0 and last_s == 0) or (last_a1 == 1 and last_s == 1)
            if was_common:
                bias[last_a1] += 1.0
            else:
                bias[1 - last_a1] += 1.0  # encourage switching after rare
        q1_eff = q1 + chi * bias

        # Stage-1 policy with lapse
        q1_center = q1_eff - np.max(q1_eff)
        soft1 = np.exp(beta * q1_center)
        soft1 = soft1 / np.sum(soft1)
        probs1 = (1.0 - xi) * soft1 + xi * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapse
        q2_center = q2[s] - np.max(q2[s])
        soft2 = np.exp(beta * q2_center)
        soft2 = soft2 / np.sum(soft2)
        probs2 = (1.0 - xi) * soft2 + xi * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning rates conditional on outcome valence (rewarded or not)
        a_lr = alpha_pos if r > 0.0 else alpha_neg

        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += a_lr * delta2

        # Stage-1 MF update via bootstrap from updated second-stage value
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += a_lr * delta1

        # Update memory for next trial's stickiness
        last_a1 = a1
        last_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Subjective-transition model-based planner with learned habit and UCB bonus.
    
    Stage 1 uses a model-based planner with a subjective belief about the common
    transition probability (p_common). In parallel, a stimulus-response habit is
    learned over first-stage actions and blended with the MB values. Stage 2 uses
    MF learning augmented with an intrinsic exploration bonus (UCB-like) based on
    inverse square-root visitation counts.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 for the two aliens on the visited planet.
    reward : array-like of float (typically 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha, beta, w_habit, bonus, p_common]
        - alpha (learning rate, [0,1]): for MF value updates and habit learning.
        - beta (inverse temperature, [0,10]): for both stages.
        - w_habit (habit weight, [0,1]): mixture weight on the habitual policy
          at stage 1 (0 = pure MB, 1 = pure habit).
        - bonus (UCB bonus scale, [0,1]): scales intrinsic exploration bonus
          bonus/sqrt(N) added to stage-2 action values and used for planning.
        - p_common (subjective common transition probability, [0,1]): belief that
          A->X and U->Y occur with this probability (rare = 1 - p_common).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w_habit, bonus, p_common = model_parameters
    n_trials = len(action_1)

    # Subjective transition matrix used for planning
    T_subj = np.array([[p_common, 1.0 - p_common],
                       [1.0 - p_common, p_common]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values and counts
    q2 = np.zeros((2, 2))        # stage-2 MF values
    N = np.zeros((2, 2)) + 1.0   # visitation counts (start at 1 to avoid div by zero)

    # Habit strength over first-stage actions
    h1 = np.zeros(2)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Effective stage-2 values with UCB bonus
        bonus_vec = bonus / np.sqrt(N)
        q2_eff_all = q2 + bonus_vec  # used both for stage-2 choice and planning

        # Stage-1 MB values via subjective transitions and current q2_eff
        max_q2_eff = np.max(q2_eff_all, axis=1)  # best alien on each planet
        q1_mb = T_subj @ max_q2_eff

        # Blend MB values with habit
        q1_mix = (1.0 - w_habit) * q1_mb + w_habit * h1

        # Stage-1 policy
        q1_center = q1_mix - np.max(q1_mix)
        probs1 = np.exp(beta * q1_center)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with UCB-augmented values
        q2_eff = q2_eff_all[s]
        q2_center = q2_eff - np.max(q2_eff)
        probs2 = np.exp(beta * q2_center)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 learning
        N[s, a2] += 1.0
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Habit learning: move h1 toward one-hot of chosen action
        # Simple exponential recency: decay all, then boost chosen
        h1 *= (1.0 - alpha)
        h1[a1] += alpha

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll