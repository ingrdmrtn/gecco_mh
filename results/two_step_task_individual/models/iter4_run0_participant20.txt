def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MBâ€“MF controller with transition-surprise arbitration and counterfactual learning.

    The agent maintains model-free (MF) first-stage values and model-based (MB) first-stage values
    computed from a fixed transition structure (A->X, U->Y common, 0.7/0.3). The arbitration weight
    between MB and MF is adapted online as a function of whether the experienced transition was
    common or rare. Second-stage values are learned with a standard delta rule, augmented by
    counterfactual (fictive) learning on the unchosen second-stage action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (gold coins) per trial, typically 0/1.
    model_parameters : sequence of floats
        [alpha, beta, omega0, kappa_cf, chi_arbit]
        - alpha in [0,1]: learning rate for second-stage values and MF first-stage bootstrapping.
        - beta in [0,10]: inverse temperature for both stages' softmax policies.
        - omega0 in [0,1]: baseline weight on model-based values at stage 1.
        - kappa_cf in [0,1]: strength of counterfactual update to the unchosen alien value.
                              Larger values mean stronger fictive learning.
        - chi_arbit in [0,1]: strength of arbitration shift toward MB on rare transitions
                              and away from MB on common transitions.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega0, kappa_cf, chi_arbit = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 Q-values (states x actions), initialized to 0.5
    q2 = np.zeros((2, 2)) + 0.5
    # Stage-1 model-free Q-values over actions
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    # Helper for stable sigmoid with omega0 baseline in [0,1]
    def _sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    # Convert baseline omega0 in [0,1] to logit space once
    w0_logit = np.log((omega0 + eps) / (1.0 - omega0 + eps))

    for t in range(n_trials):
        # Model-based Q for stage 1 from current second-stage values
        max_q2 = np.max(q2, axis=1)  # for states X and Y
        q1_mb = T @ max_q2

        # Arbitration weight depends on whether current transition is rare or common
        a1 = action_1[t]
        s = state[t]
        # Rare if reached the uncommon state for the chosen action
        rare = int((a1 == 0 and s == 1) or (a1 == 1 and s == 0))
        # Shift arbitration toward MB on rare, away on common
        w_logit = w0_logit + chi_arbit * (1 if rare else -1)
        omega_t = _sigmoid(w_logit)

        # Mixture for stage-1 action values
        q1 = omega_t * q1_mb + (1.0 - omega_t) * q1_mf

        # Stage-1 policy
        l1 = beta * (q1 - np.max(q1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy at observed state
        a2 = action_2[t]
        l2 = beta * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Second-stage learning (chosen)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Counterfactual (unchosen) update uses a fictive target (1 - r)
        other = 1 - a2
        pe2_cf = (1.0 - r) - q2[s, other]
        q2[s, other] += alpha * kappa_cf * pe2_cf

        # Model-free bootstrapping for stage-1 chosen action
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with learned transitions, volatility-adaptive temperature, and WSLS bias.

    The agent learns the state transition matrix from experience and plans at stage 1 using the
    learned transitions and current second-stage values. The inverse temperature is modulated
    by a running estimate of volatility (the magnitude of recent reward PEs): higher volatility
    reduces beta (encouraging exploration). A win-stay/lose-shift (WSLS) bias acts at stage 1.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (gold coins) per trial, typically 0/1.
    model_parameters : sequence of floats
        [alpha, beta, eta_T, v_sens, wsls]
        - alpha in [0,1]: learning rate for second-stage values.
        - beta in [0,10]: base inverse temperature before volatility modulation.
        - eta_T in [0,1]: learning rate for the transition matrix rows toward the observed state.
        - v_sens in [0,1]: sensitivity for volatility estimate; higher values react faster to changes.
        - wsls in [0,1]: strength of win-stay/lose-shift bias at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, eta_T, v_sens, wsls = model_parameters
    n_trials = len(action_1)

    # Initialize transitions to uniform; learn from experience
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Second-stage Q
    q2 = np.zeros((2, 2)) + 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Volatility tracker (EMA of |PE|)
    vol = 0.0
    eps = 1e-12

    prev_a1 = None
    prev_r = None

    for t in range(n_trials):
        # Model-based Q for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # WSLS bias vector
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_r is not None:
            if prev_r > 0.0:
                # win-stay
                bias1[prev_a1] += wsls
            else:
                # lose-shift
                bias1[1 - prev_a1] += wsls

        # Effective beta reduced under volatility
        beta_eff = beta / (1.0 + vol)

        # Stage-1 policy
        logits1 = q1_mb + bias1
        l1 = beta_eff * (logits1 - np.max(logits1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        s = state[t]
        l2 = beta_eff * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Second-stage learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Transition learning (row-wise toward observed state)
        T[a1, s] += eta_T * (1.0 - T[a1, s])
        other_s = 1 - s
        T[a1, other_s] += eta_T * (0.0 - T[a1, other_s])

        # Renormalize rows to guard numerical drift
        T[0] = T[0] / (np.sum(T[0]) + eps)
        T[1] = T[1] / (np.sum(T[1]) + eps)

        # Update volatility estimate from absolute PE
        vol = (1.0 - v_sens) * vol + v_sens * np.abs(pe2)

        # Carry over for WSLS
        prev_a1 = a1
        prev_r = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Successor-representation planning with UCB exploration and value forgetting.

    The agent learns a two-state successor representation (SR) over second-stage planets for each
    first-stage action, i.e., expected occupancy of planets X and Y after choosing A or U. Stage-1
    action values are computed by projecting the SR onto the current estimates of the best alien
    values per planet. At stage 2, the agent uses an Upper Confidence Bound (UCB) exploration bonus
    that depends on both value uncertainty (via Bernoulli variance p*(1-p)) and visitation counts.
    Second-stage values also undergo recency-based forgetting toward 0.5.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (gold coins) per trial, typically 0/1.
    model_parameters : sequence of floats
        [alpha, beta, kappa_sr, xi_ucb, phi_f]
        - alpha in [0,1]: learning rate for second-stage values.
        - beta in [0,10]: inverse temperature for both stages' softmax policies.
        - kappa_sr in [0,1]: learning rate for the SR (expected planet occupancy) per action.
        - xi_ucb in [0,1]: strength of the UCB exploration bonus at stage 2.
        - phi_f in [0,1]: forgetting rate pulling all second-stage values toward 0.5 each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, kappa_sr, xi_ucb, phi_f = model_parameters
    n_trials = len(action_1)

    # Successor representation over second-stage states for each first-stage action (2 actions x 2 states)
    # Initialized as uniform occupancy (agnostic)
    SR = np.array([[0.5, 0.5],
                   [0.5, 0.5]], dtype=float)

    # Second-stage Q-values and visitation counts for UCB
    q2 = np.zeros((2, 2)) + 0.5
    N = np.ones((2, 2), dtype=float)  # start at 1 to keep bonus finite

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Compute max Q per state to project through SR
        max_q2 = np.max(q2, axis=1)
        # Stage-1 values via SR projection
        q1 = SR @ max_q2

        # Stage-1 policy
        l1 = beta * (q1 - np.max(q1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with UCB bonus
        s = state[t]
        # Bernoulli variance proxy and count-based scaling
        var = q2[s] * (1.0 - q2[s])
        bonus = xi_ucb * np.sqrt(var / (N[s] + eps))
        logits2 = q2[s] + bonus
        l2 = beta * (logits2 - np.max(logits2))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Second-stage forgetting toward 0.5 (applied to all entries)
        q2 = (1.0 - phi_f) * q2 + phi_f * 0.5

        # Second-stage learning at chosen option
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update counts
        N[s, a2] += 1.0

        # SR learning for the chosen first-stage action toward the observed state one-hot
        target = np.array([1.0 if i == s else 0.0 for i in range(2)], dtype=float)
        SR[a1] += kappa_sr * (target - SR[a1])

        # Row-normalize SR to keep it interpretable as occupancy probabilities
        SR[0] = SR[0] / (np.sum(SR[0]) + eps)
        SR[1] = SR[1] / (np.sum(SR[1]) + eps)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll