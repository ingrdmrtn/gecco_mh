def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-weighted hybrid with learned transitions, eligibility trace, and action bias.

    This model blends model-based planning with a model-free learner. It:
    - Learns second-stage action values with a reward learning rate.
    - Learns transition probabilities online and uses their uncertainty to arbitrate MB vs. MF control.
    - Propagates reward back to first-stage via an eligibility-trace-like MF update.
    - Adds a constant bias toward spaceship A at choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (0/1) for each trial.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha_r, beta, v_vol, lambda_elig, bA_bias]
        - alpha_r (0..1): Reward learning rate for second-stage Q-values.
        - beta (0..10): Inverse temperature for softmax choices at both stages.
        - v_vol (0..1): Volatility/uncertainty sensitivity. Higher values increase MB weight when transitions are certain,
                        and act as a learning rate for transition updates.
        - lambda_elig (0..1): Eligibility trace strength to backpropagate second-stage value to first-stage MF Q.
        - bA_bias (0..1): Additive bias toward choosing spaceship A (action 0) at first stage.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, v_vol, lambda_elig, bA_bias = model_parameters
    n_trials = len(action_1)

    # Likelihood containers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_mf = np.zeros(2)         # model-free first-stage values
    q2 = np.zeros((2, 2))       # second-stage values

    # Transition model T[a, s] learned online; start uninformative
    T = np.full((2, 2), 0.5)

    for t in range(n_trials):
        # Compute model-based first-stage values from current T and second-stage values
        max_q2 = np.max(q2, axis=1)  # max over actions within each state
        q1_mb = T @ max_q2

        # Arbitration: uncertainty-weighted MB fraction
        # Row-wise entropy of transitions (0..1 normalized by log2)
        eps = 1e-12
        H_rows = []
        for a in range(2):
            pa = T[a]
            Ha = -(pa[0] * np.log(pa[0] + eps) + pa[1] * np.log(pa[1] + eps))
            Ha_norm = Ha / np.log(2.0)  # normalize to [0,1]
            H_rows.append(Ha_norm)
        H_mean = 0.5 * (H_rows[0] + H_rows[1])
        confidence = 1.0 - H_mean
        w_mb = v_vol * confidence + (1.0 - v_vol) * 0.5  # in [0,1]

        # Hybrid first-stage action values plus constant bias toward A
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        bias1 = np.array([bA_bias, 0.0])

        # First-stage policy
        logits1 = beta * (q1_hybrid + bias1 - np.max(q1_hybrid + bias1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy in reached state
        s2 = state[t]
        q2s = q2[s2]
        logits2 = beta * (q2s - np.max(q2s))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage values (simple delta rule)
        td2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_r * td2

        # Update first-stage MF via eligibility-trace-like backprop from obtained second-stage action value
        target1 = q2[s2, a2]
        td1 = target1 - q1_mf[a1]
        q1_mf[a1] += (alpha_r * lambda_elig) * td1

        # Learn transitions online with volatility-sensitive step size v_vol
        # Observed transition: a1 -> s2
        for s in range(2):
            indicator = 1.0 if s == s2 else 0.0
            T[a1, s] = (1.0 - v_vol) * T[a1, s] + v_vol * indicator

        # Keep rows normalized (they should already be)
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive model-free control with transition–outcome interaction and leaky choice traces.

    This model is primarily model-free but captures the classic model-based stay/switch signature via
    a transition–outcome interaction bias on first-stage choices. It also includes leaky choice traces
    (autocorrelation) that bias repeating recent actions at both stages, and risk-sensitive utility.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received.
    model_parameters : iterable of floats
        [alpha, beta, gamma_risk, xi_interact, tau_trace]
        - alpha (0..1): Learning rate for model-free updates at both stages.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - gamma_risk (0..1): Risk sensitivity; utility u(r)=r^(gamma_risk). Lower values overweight small rewards.
        - xi_interact (0..1): Strength of transition–outcome interaction bias on first-stage stay/switch.
                              Positive values increase staying after rewarded-common or unrewarded-rare outcomes.
        - tau_trace (0..1): Leak factor for choice traces; closer to 1 retains more past choices.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, gamma_risk, xi_interact, tau_trace = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Leaky choice traces for biases (separate trace per state at stage 2)
    trace1 = np.zeros(2)
    trace2 = np.zeros((2, 2))

    # Variables to compute transition–outcome interaction on the next trial
    prev_a1 = None
    prev_common = None
    prev_r = None

    for t in range(n_trials):
        # Build first-stage bias from leaky trace
        bias1 = trace1.copy()

        # Add transition–outcome interaction bias to previous action, if available
        if prev_a1 is not None and prev_common is not None and prev_r is not None:
            signed_r = (2.0 * prev_r - 1.0)  # +1 if reward=1, -1 if reward=0
            signed_trans = 1.0 if prev_common else -1.0
            interact_bias = xi_interact * (signed_r * signed_trans)
            bias1[prev_a1] += interact_bias

        # First-stage policy
        logits1 = beta * (q1 + bias1 - np.max(q1 + bias1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with state-specific leaky trace bias
        s2 = state[t]
        bias2 = trace2[s2].copy()
        logits2 = beta * (q2[s2] + bias2 - np.max(q2[s2] + bias2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observations
        r = reward[t]
        # Risk-sensitive utility
        u = (r + 1e-12) ** gamma_risk

        # Update second-stage Q by delta rule with utility u
        td2 = u - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # Back up to first stage via SARSA(1) style target (use realized second-stage action value)
        target1 = q2[s2, a2]
        td1 = target1 - q1[a1]
        q1[a1] += alpha * td1

        # Update leaky traces
        trace1 *= tau_trace
        trace1[a1] += 1.0
        trace2[s2] *= tau_trace
        trace2[s2, a2] += 1.0

        # Store info for next-trial interaction bias
        prev_a1 = a1
        prev_r = r
        # Determine if current transition was common (A->X or U->Y)
        prev_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Kalman-filtered second-stage values with forgetting and hybrid planning.

    This model treats each second-stage action value as a drifting latent reward probability and
    updates it with a scalar Kalman filter (tracking mean and uncertainty). First-stage policy
    is a hybrid of model-based planning (using fixed transition structure) and a model-free value,
    with value forgetting applied over time. The model-free first-stage value is updated toward the
    realized second-stage action value.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received.
    model_parameters : iterable of floats
        [beta, kappa_drift, kappa_noise, w_plan, mu_forget]
        - beta (0..10): Inverse temperature for softmax at both stages.
        - kappa_drift (0..1): Process noise (drift) controlling how quickly second-stage values are expected to change.
        - kappa_noise (0..1): Observation noise for the Kalman filter update at second stage.
        - w_plan (0..1): Weight on model-based plan vs. model-free at first stage.
        - mu_forget (0..1): Forgetting factor applied to first-stage MF values each trial, and learning rate for updating them.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    beta, kappa_drift, kappa_noise, w_plan, mu_forget = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage Kalman means and variances
    m2 = np.zeros((2, 2))           # means for each state-action
    v2 = np.full((2, 2), 0.25)      # initial uncertainty

    # First-stage model-free values
    q1_mf = np.zeros(2)

    # Fixed (known) transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    for t in range(n_trials):
        # Model-based first-stage values from current second-stage means
        max_m2 = np.max(m2, axis=1)
        q1_mb = T @ max_m2

        # Hybrid first-stage values with forgetting applied to MF values
        q1_mf *= (1.0 - mu_forget)
        q1 = w_plan * q1_mb + (1.0 - w_plan) * q1_mf

        # First-stage policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy from current KF means
        s2 = state[t]
        q2s = m2[s2]
        logits2 = beta * (q2s - np.max(q2s))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observation
        r = reward[t]

        # Kalman prediction step: increase uncertainty due to drift
        v2 += kappa_drift

        # Kalman update only for observed (s2, a2)
        prior_mean = m2[s2, a2]
        prior_var = v2[s2, a2]
        # Scalar Kalman gain
        K = prior_var / (prior_var + kappa_noise + 1e-12)
        # Update mean and variance
        m2[s2, a2] = prior_mean + K * (r - prior_mean)
        v2[s2, a2] = (1.0 - K) * prior_var

        # Update first-stage MF toward realized second-stage value (post-update mean)
        target1 = m2[s2, a2]
        td1 = target1 - q1_mf[a1]
        q1_mf[a1] += mu_forget * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll