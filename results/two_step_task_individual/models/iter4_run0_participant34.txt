def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Adaptive MB-reliance via transition surprise, with stage-1 perseveration; stage-2 model-free.
    
    Idea
    ----
    - Stage-2 action values are learned model-free (Q-learning).
    - Stage-1 combines model-based (MB) and model-free (MF) values with a time-varying weight omega_t.
      The weight adapts based on transition surprise and reward:
        omega_{t+1} = (1 - eta) * omega_t + eta * target_t,
      where target_t = (1 - p_trans) * reward_t. Thus, rare-and-rewarded transitions push omega upward,
      whereas common or unrewarded transitions keep it lower.
    - A perseveration bias at stage 1 favors repeating the previous first-stage choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=spaceship A, 1=spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited: 0=planet X, 1=planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: alien chosen within the visited planet.
    reward : array-like of float (0 or 1)
        Reward obtained at the end of the trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, omega0, eta, psi]
        - alpha in [0,1]: learning rate for MF value updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega0 in [0,1]: initial model-based weight at stage 1.
        - eta in [0,1]: adaptation rate controlling how quickly omega updates to the surprise-reward target.
        - psi in [0,1]: perseveration strength added to the previous stage-1 choice's logit.
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    alpha, beta, omega0, eta, psi = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q1_mf = np.zeros(2)    # model-free first-stage values
    q2 = np.zeros((2, 2))  # stage-2 MF action values

    omega = omega0         # adaptive MB weight
    prev_a1 = None         # for perseveration

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation for stage 1
        max_q2 = np.max(q2, axis=1)   # value of each second-stage state
        q1_mb = T @ max_q2            # expected value for each first-stage action

        # Combine MB and MF with adaptive omega
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Add perseveration bias to logits (favor repeating the last chosen first-stage action)
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = psi

        # Stage-1 policy
        logits1 = beta * q1 + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure MF)
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes and updates
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update using observed second-stage realized value (SARSA(0)-like backup)
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update omega based on transition surprise and reward
        # Compute probability of observed transition under chosen action
        p_trans = T[a1, s]  # 0.7 for common; 0.3 for rare
        target = (1.0 - p_trans) * r  # high when rare+rewarded, zero otherwise
        omega = (1.0 - eta) * omega + eta * target
        omega = min(max(omega, 0.0), 1.0)

        # Update perseveration memory
        prev_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Learned transition model with uncertainty bonus at stage 1, MF at stage 2, and stage-2 perseveration.

    Idea
    ----
    - Learn the transition structure T(a -> s) online (rather than assuming it).
      Each first-stage action has a transition probability vector updated toward the observed planet.
    - Stage-1 values are purely model-based using the learned T:
        Q1_MB(a) = sum_s T[a,s] * max_a2 Q2[s, a2] + upsilon * H(T[a])
      where H is the entropy of T[a], providing an uncertainty-driven exploration bonus.
    - Stage-2 values are model-free; stage-2 choices also have a perseveration bias.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Second-stage state visited.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Reward per trial.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, nu, upsilon, chi2]
        - alpha in [0,1]: learning rate for MF Q2 value updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - nu in [0,1]: transition learning rate updating each row T[a] toward the observed one-hot state.
        - upsilon in [0,1]: strength of the entropy-based uncertainty bonus at stage 1.
        - chi2 in [0,1]: perseveration strength at stage 2 (bias to repeat previous second-stage action in a state).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, nu, upsilon, chi2 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition with mild prior toward common mapping
    # Rows sum to 1; start near [0.6, 0.4] for A and [0.4, 0.6] for U to be agnostic yet biased.
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    q2 = np.zeros((2, 2))  # stage-2 MF values

    # Stage-2 perseveration traces per state
    prev_a2 = np.array([-1, -1], dtype=int)  # -1 indicates no previous choice recorded

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # Compute MB value at stage 1 using learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2  # expected values for actions A,U

        # Entropy bonus for uncertainty-driven exploration
        # H(p) = -sum p log p, with safe handling for 0 entries
        H = np.zeros(2)
        for a in range(2):
            pa = T[a]
            h = 0.0
            for ps in pa:
                if ps > 0:
                    h -= ps * np.log(ps)
            H[a] = h

        q1 = q1_mb + upsilon * H

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration per state
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] = chi2

        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Reward and updates
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Learn transitions: update the row for chosen first-stage action toward observed state one-hot
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        # Row update with nu, followed by renormalization for numerical stability
        T[a1] = (1.0 - nu) * T[a1] + nu * target
        # Ensure normalization
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

        # Update stage-2 perseveration memory
        prev_a2[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with reward utility curvature and global value decay (forgetting).

    Idea
    ----
    - Stage-2 action values are model-free, with reward transformed by a concave/convex utility:
        r_tilde = r ** lambda_u, where lambda_u in [0,1] (smaller -> more concave utility).
    - Stage-1 uses a hybrid of model-based (fixed transitions) and model-free Q1:
        Q1 = theta * Q1_MB + (1 - theta) * Q1_MF.
    - Global decay (forgetting) shrinks all Q-values toward zero each trial before learning updates,
      controlled by mu in [0,1].

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Second-stage state visited.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float (0 or 1)
        Obtained reward.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, lambda_u, mu, theta]
        - alpha in [0,1]: learning rate for MF value updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - lambda_u in [0,1]: reward utility curvature; r_tilde = r ** lambda_u.
        - mu in [0,1]: global decay toward zero applied to all Q-values before each update.
        - theta in [0,1]: weight on model-based value at stage 1; (1-theta) weights MF Q1.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, lambda_u, mu, theta = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10
    for t in range(n_trials):
        s = state[t]

        # Apply global decay (forgetting) to all Q-values before computing policy/updates
        q1_mf *= (1.0 - mu)
        q2 *= (1.0 - mu)

        # Model-based contribution for stage 1 from fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid value
        q1 = theta * q1_mb + (1.0 - theta) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Reward with utility curvature
        r = reward[t]
        r_tilde = r ** lambda_u

        # Stage-2 update (MF)
        pe2 = r_tilde - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update via bootstrapped value from realized second-stage
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss