def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with learned transitions and uncertainty bonus (UCB-style).

    Mechanism:
    - Second-stage (alien) values Q2 are learned model-free via TD(0).
    - First-stage values blend a learned-model-based planner and a model-free cache:
        Q1 = mb * Q1_MB + (1 - mb) * Q1_MF
      where Q1_MB uses an online estimate of the transition matrix T_est and
      plans over max(Q2 + bonus).
    - An uncertainty bonus encourages exploration at the second stage:
        bonus(s,a) = bonus / sqrt(N(s,a) + 1),
      which is added to Q2 both for second-stage choice and for planning.
    - The transition model T_est is learned online from experienced transitions.

    Parameters (bounds):
    - lr2 in [0, 1]: Learning rate for second-stage Q2 and MF first-stage cache updates.
    - beta in [0, 10]: Inverse temperature for softmax policies at both stages.
    - mb in [0, 1]: Weight of model-based value in first-stage action values.
    - trans_lr in [0, 1]: Learning rate for updating the transition matrix estimate.
    - bonus in [0, 1]: Strength of the uncertainty bonus (UCB-style).

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states per trial (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, second-stage actions per trial (within observed state).
    - reward: array-like of floats/ints (typically 0/1), reward outcome per trial.
    - model_parameters: iterable [lr2, beta, mb, trans_lr, bonus].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    lr2, beta, mb, trans_lr, bonus = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix (rows: actions A/U, cols: states X/Y)
    T_est = np.full((2, 2), 0.5)

    # Model-free caches
    q1_mf = np.zeros(2)       # first-stage MF values
    q2 = np.zeros((2, 2))     # second-stage MF values

    # Uncertainty tracking for bonus
    N_sa = np.zeros((2, 2))   # visitation counts for second-stage state-action pairs

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Construct bonus-adjusted second-stage values for planning
        b2 = bonus / np.sqrt(N_sa + 1.0)
        q2_planning = q2 + b2

        # Model-based first-stage values: expected max over next-state Q2 with bonus
        V_next = np.max(q2_planning, axis=1)  # length-2: value of state X and Y
        q1_mb = T_est @ V_next                # length-2

        # Blend MB and MF
        q1 = mb * q1_mb + (1.0 - mb) * q1_mf

        # First-stage policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy uses bonus-augmented values in the current state
        s = state[t]
        logits2 = beta * (q2[s] + b2[s])
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: Second-stage TD(0)
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr2 * delta2

        # Update MF first-stage cache towards realized second-stage value
        q1_mf[a1] += lr2 * (q2[s, a2] - q1_mf[a1])

        # Update transition model T_est for the chosen first-stage action
        onehot_s = np.zeros(2)
        onehot_s[s] = 1.0
        T_est[a1] += trans_lr * (onehot_s - T_est[a1])

        # Update visitation counts for uncertainty bonus
        N_sa[s, a2] += 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with Pearceâ€“Hall associability and one-step perseveration.

    Mechanism:
    - Second-stage Q-values are learned with an associability-modulated learning rate.
      Each state-action has an associability A(s,a) that tracks recent surprise:
        A <- (1 - assoc_tau)*A + assoc_tau*|delta|
      Effective learning rate is base_lr * A(s,a).
    - First-stage values are model-based using the known transition structure and
      the current second-stage Q-values.
    - Perseveration biases additively increase the logit of repeating the previous
      action at each stage (separately parameterized for stages 1 and 2).

    Parameters (bounds):
    - base_lr in [0, 1]: Base learning rate scaled by associability at stage 2.
    - beta in [0, 10]: Inverse temperature for softmax at both stages.
    - assoc_tau in [0, 1]: Smoothing for associability updates (higher -> more reactive).
    - stick1 in [0, 1]: Strength of first-stage perseveration bias.
    - stick2 in [0, 1]: Strength of second-stage perseveration bias.

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states per trial (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, second-stage actions within observed state.
    - reward: array-like of floats/ints (typically 0/1), reward outcome per trial.
    - model_parameters: iterable [base_lr, beta, assoc_tau, stick1, stick2].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    base_lr, beta, assoc_tau, stick1, stick2 = model_parameters
    n_trials = len(action_1)

    # Known transition structure (rows: A/U -> cols: X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Second-stage values and associability
    q2 = np.zeros((2, 2))
    A = np.ones((2, 2))  # start with full associability to allow early learning

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # One-step perseveration memory
    prev_a1 = None
    prev_a2 = [None, None]  # per state

    for t in range(n_trials):
        # Model-based first-stage values from current Q2
        V = np.max(q2, axis=1)   # value of states X and Y
        q1_mb = T @ V

        # Add first-stage perseveration bias to logits
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick1

        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice with perseveration bias
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick2

        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Associability-modulated TD learning at stage 2
        delta2 = r - q2[s, a2]
        A[s, a2] = (1.0 - assoc_tau) * A[s, a2] + assoc_tau * abs(delta2)
        eff_lr = base_lr * A[s, a2]
        q2[s, a2] += eff_lr * delta2

        # Update perseveration memories
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with eligibility trace and transition-dependent credit assignment.

    Mechanism:
    - Both stages use model-free values: Q1 for first-stage actions and Q2 for second-stage actions.
    - Second-stage values are updated with standard TD(0).
    - First-stage values are updated toward the observed second-stage value using an
      eligibility trace 'trace'. Credit assignment to the first-stage action is reduced
      following rare transitions, controlled by 'rare_discount':
         eff_trace = trace                if common transition
         eff_trace = trace * (1 - rare_discount) if rare transition
      Rare transitions thus weaken how much the first-stage action is credited for the
      second-stage outcome, capturing sensitivity to transition structure without planning.

    Parameters (bounds):
    - lr2 in [0, 1]: Learning rate for second-stage Q2 updates.
    - lr1 in [0, 1]: Learning rate for first-stage Q1 updates.
    - beta in [0, 10]: Inverse temperature for softmax at both stages.
    - trace in [0, 1]: Eligibility trace strength for propagating value to Q1.
    - rare_discount in [0, 1]: Fractional reduction of credit on rare transitions.

    Inputs:
    - action_1: array-like of ints in {0,1}, first-stage choices per trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states per trial (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, second-stage actions within observed state.
    - reward: array-like of floats/ints (typically 0/1), reward outcome per trial.
    - model_parameters: iterable [lr2, lr1, beta, trace, rare_discount].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    lr2, lr1, beta, trace, rare_discount = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)        # first-stage MF values for actions A/U
    q2 = np.zeros((2, 2))   # second-stage MF values for states X/Y and actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage policy from MF Q1
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy from MF Q2 in observed state
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr2 * delta2

        # Transition type: common if s == a1 given task structure (A->X, U->Y)
        is_common = (s == a1)
        eff_trace = trace if is_common else trace * (1.0 - rare_discount)

        # Propagate credit to first-stage action toward the realized second-stage value
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += lr1 * eff_trace * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll