def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with learned transitions and surprise-driven arbitration.
    
    The agent:
    - Learns second-stage values Q2[s, a2] from reward.
    - Learns the first-stage transition matrix T[s | a1] with a Rescorla–Wagner update.
    - Forms model-based Q1_mb = T @ max_a2 Q2[s, a2].
    - Maintains a model-free first-stage value Q1_mf updated by the second-stage TD error.
    - Arbitrates between MB and MF at the first stage with a dynamic weight that increases
      with transition surprise from the previous trial: w_t = clip(mix_base + psi_surprise*(1 - p_obs)),
      where p_obs is the probability of the last observed transition under the agent’s current T.
    
    Parameters (bounds):
    - lr_r (0-1): Learning rate for Q2 (second-stage values), and for propagating reward to Q1_mf.
    - beta (0-10): Inverse temperature for softmax at both stages.
    - lr_T (0-1): Learning rate for the first-stage transition matrix T.
    - psi_surprise (0-1): Sensitivity of the MB arbitration weight to transition surprise.
    - mix_base (0-1): Baseline model-based arbitration weight (used on the first trial and as a base).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [lr_r, beta, lr_T, psi_surprise, mix_base]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    lr_r, beta, lr_T, psi_surprise, mix_base = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Initialize learned transition matrix T[s | a1] and values
    T = np.full((2, 2), 0.5)  # start agnostic about transitions
    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Arbitration weight carried from previous trial (start at baseline)
    arb_weight = np.clip(mix_base, 0.0, 1.0)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage values
        V2 = np.max(Q2, axis=1)
        Q1_mb = T @ V2

        # Blend MB and MF
        Q1 = arb_weight * Q1_mb + (1.0 - arb_weight) * Q1_mf

        # First-stage policy
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning: second stage
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += lr_r * delta2

        # Model-free backup to first stage (eligibility = 1)
        Q1_mf[a1] += lr_r * delta2

        # Learning: transitions
        # Store current predicted probability of the observed transition (for arbitration next trial)
        p_obs = T[a1, s]
        # RW update toward one-hot observed state
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = T[a1] + lr_T * (oh - T[a1])
        # Renormalize row to keep probabilities valid
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum
        else:
            T[a1] = np.array([0.5, 0.5])

        # Update arbitration weight for next trial based on surprise from this trial
        surprise = 1.0 - p_obs
        arb_weight = np.clip(mix_base + psi_surprise * surprise, 0.0, 1.0)

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Successor-representation first stage with reward learning at second stage.
    
    The agent:
    - Learns second-stage values Q2[s, a2] from reward.
    - Maintains a 2x2 successor representation M[a1, s] mapping first-stage actions to expected
      discounted occupancy of second-stage states. In this two-step task, M approximates the
      transition structure but allows gradual adaptation and discounting via gamma_sr.
    - First-stage MB-like values are computed via SR projection: Q1_sr = M @ V2,
      where V2[s] = max_a2 Q2[s, a2].
    
    SR update (on choosing a1 and observing s):
      target = (1 - gamma_sr) * M[a1, :] + one_hot(s)
      M[a1, :] <- M[a1, :] + alpha_sr * (target - M[a1, :])
    
    Parameters (bounds):
    - alpha_sr (0-1): Learning rate for the SR matrix M.
    - alpha_r (0-1): Learning rate for second-stage values Q2.
    - beta (0-10): Inverse temperature for softmax at both stages.
    - gamma_sr (0-1): Discount factor controlling persistence in SR (higher -> slower change).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [alpha_sr, alpha_r, beta, gamma_sr]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    alpha_sr, alpha_r, beta, gamma_sr = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Initialize SR, Q-values
    M = np.full((2, 2), 0.5)  # start agnostic mapping action->state
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # First-stage values via SR projection
        V2 = np.max(Q2, axis=1)
        Q1_sr = M @ V2

        # First-stage policy
        logits1 = beta * Q1_sr
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learn second-stage values
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * delta2

        # Update SR for the chosen first-stage action toward observed state with discount control
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        target = (1.0 - gamma_sr) * M[a1] + oh
        M[a1] = M[a1] + alpha_sr * (target - M[a1])

        # Keep SR rows nonnegative and normalized (optional soft normalization)
        M[a1] = np.maximum(M[a1], 0.0)
        row_sum = np.sum(M[a1])
        if row_sum > 0:
            M[a1] = M[a1] / row_sum
        else:
            M[a1] = np.array([0.5, 0.5])

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based first stage with risk-sensitive utility and uncertainty-driven exploration at second stage.
    
    The agent:
    - Uses a fixed known transition structure (common A->X, U->Y with 0.7 common, 0.3 rare).
    - Tracks Q2[s, a2] via reward-prediction learning, but on the utility of outcomes rather than raw reward:
        u(r) = r if r >= 0 else theta_loss * r  (loss aversion when theta_loss > 1)
    - Maintains an uncertainty estimate U[s, a2] updated from squared TD errors, and adds an
      exploration bonus xi_u * U to action values at decision time.
    - First-stage values are purely model-based: Q1_mb = T_fixed @ max_a2 (Q2[s, a2] + xi_u * U[s, a2]).
    
    Parameters (bounds):
    - eta_q (0-1): Learning rate for second-stage Q2 updates on utility prediction error.
    - beta (0-10): Inverse temperature for softmax at both stages.
    - nu_u (0-1): Learning/decay rate for uncertainty estimator U (higher -> faster adapting uncertainty).
    - xi_u (0-1): Weight of the uncertainty bonus added to second-stage action values.
    - theta_loss (0-1): Loss-sensitivity multiplier applied to negative rewards (set <1 for loss attenuation, >1 not allowed by bounds, so interpretable as attenuation within bound).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_q, beta, nu_u, xi_u, theta_loss]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_q, beta, nu_u, xi_u, theta_loss = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Fixed transition structure (common: 0.7; rare: 0.3). Order: actions A,U; states X,Y.
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    Q2 = np.zeros((2, 2))
    U = np.zeros((2, 2))  # uncertainty proxy

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Utility transformation (risk-/loss-sensitive)
        util = r if r >= 0 else theta_loss * r

        # Second-stage decision uses exploration bonus
        Q2_eff_row = Q2[s] + xi_u * U[s]
        logits2 = beta * Q2_eff_row
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # First-stage decision uses model-based plan over Q2_eff
        V2_eff = np.max(Q2 + xi_u * U, axis=1)
        Q1_mb = T_fixed @ V2_eff
        logits1 = beta * Q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Learning: second-stage Q-values
        delta2 = util - Q2[s, a2]
        Q2[s, a2] += eta_q * delta2

        # Update uncertainty proxy from squared PE with exponential smoothing
        U[s, a2] = (1.0 - nu_u) * U[s, a2] + nu_u * (delta2 ** 2)

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss