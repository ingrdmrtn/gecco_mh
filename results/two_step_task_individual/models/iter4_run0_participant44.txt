def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Stage-1 Successor Representation with transition learning, stage-2 TD learning,
    and forgetting of unchosen options.

    This model assumes the agent learns an action->state successor representation (SR)
    for the first stage (i.e., transition weights from spaceships to planets) and uses
    it to compute model-based first-stage values by projecting onto the current
    second-stage values. Stage-2 values are learned via TD with optional forgetting
    for the unchosen action. A perseveration bias at stage 1 favors repeating the
    previous first-stage action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state: 0/1 are the two aliens.
    reward : array-like of float
        Obtained reward on each trial (e.g., 0 or 1).
    model_parameters : list/array of 5 floats
        [alpha2, beta, sigma_sr, kappa_rep, xi_forget]
        - alpha2 in [0,1]: TD learning rate for second-stage action values.
        - beta in [0,10]: Inverse temperature used at both stages.
        - sigma_sr in [0,1]: Learning rate for the stage-1 SR (action->state weights).
        - kappa_rep in [0,1]: First-stage perseveration strength (adds to last chosen a1).
        - xi_forget in [0,1]: Forgetting/decay applied to the unchosen second-stage action
          in the visited state.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha2, beta, sigma_sr, kappa_rep, xi_forget = model_parameters
    n_trials = len(action_1)

    # Successor representation (action -> state occupancy), initialized to canonical common transitions
    SR = np.array([[0.7, 0.3],
                   [0.3, 0.7]], dtype=float)

    # Second-stage Q-values: Q2[state, action]
    Q2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1

    for t in range(n_trials):
        s = state[t]

        # Compute first-stage model-based values via SR projection onto max Q2 per state
        max_q2 = np.max(Q2, axis=1)  # shape (2,)
        Q1_mb = SR @ max_q2          # shape (2,)

        # Add first-stage perseveration bias
        Q1_eff = Q1_mb.copy()
        if prev_a1 >= 0:
            Q1_eff[prev_a1] += kappa_rep

        # First-stage policy
        Q1_eff = Q1_eff - np.max(Q1_eff)
        exp_q1 = np.exp(beta * Q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        Q2_eff = Q2[s, :].copy()
        Q2_eff = Q2_eff - np.max(Q2_eff)
        exp_q2 = np.exp(beta * Q2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD update at stage 2 for chosen action
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha2 * delta2

        # Forgetting for the unchosen action in the visited state
        alt2 = 1 - a2
        Q2[s, alt2] *= (1.0 - xi_forget)

        # Update SR row for the chosen first-stage action (move toward observed state)
        a1_idx = a1
        # Exponential moving average toward one-hot of observed state
        for st in range(2):
            target = 1.0 if st == s else 0.0
            SR[a1_idx, st] += sigma_sr * (target - SR[a1_idx, st])

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-free two-stage TD with eligibility to stage 1 and counterfactual (opponent)
    learning at stage 2, plus a baseline bias at stage 1.

    This model learns both stages model-free. The obtained TD error at stage 2 is used
    to update the chosen second-stage action value, and a counterfactual/opponent update
    nudges the unchosen action in the opposite direction. Stage-1 values are updated
    by bootstrapping from stage-2 via an eligibility parameter. A first-stage baseline
    bias captures stable preference for one spaceship.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state: 0/1 are the two aliens.
    reward : array-like of float
        Obtained reward on each trial (e.g., 0 or 1).
    model_parameters : list/array of 5 floats
        [alpha, beta, elig, rho_cf, b0]
        - alpha in [0,1]: TD learning rate for both stage-1 and stage-2 updates.
        - beta in [0,10]: Inverse temperature used at both stages.
        - elig in [0,1]: Eligibility strength for backing up from stage 2 to stage 1.
        - rho_cf in [0,1]: Counterfactual/opponent learning strength applied to the
          unchosen second-stage action (fraction of the chosen-action TD error).
        - b0 in [0,1]: Baseline bias toward spaceship A vs. U; transformed to signed
          bias b = (b0 - 0.5) added to Q1 logits (positive favors A).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, elig, rho_cf, b0 = model_parameters
    n_trials = len(action_1)

    # Model-free Q-values
    Q1 = np.zeros(2, dtype=float)       # stage-1: actions A/U
    Q2 = np.zeros((2, 2), dtype=float)  # stage-2: states X/Y x actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Signed baseline bias for stage-1 choice
    bias = b0 - 0.5

    for t in range(n_trials):
        s = state[t]

        # Stage-1 policy with baseline bias
        Q1_eff = Q1.copy()
        # Add bias as a difference: +bias to A (0), -bias to U (1)
        Q1_eff[0] += bias
        Q1_eff[1] -= bias
        Q1_eff = Q1_eff - np.max(Q1_eff)
        exp_q1 = np.exp(beta * Q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        Q2_eff = Q2[s, :].copy()
        Q2_eff = Q2_eff - np.max(Q2_eff)
        exp_q2 = np.exp(beta * Q2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD update at stage 2
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Counterfactual/opponent update for the unchosen action in the visited state
        alt2 = 1 - a2
        # Move the unchosen value in the opposite direction of the chosen TD error
        Q2[s, alt2] -= alpha * rho_cf * delta2

        # Back up to stage 1 via eligibility
        bootstrap = Q2[s, a2]
        delta1 = bootstrap - Q1[a1]
        Q1[a1] += alpha * elig * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with intrinsic motivation: transition-surprise and novelty bonuses.

    This model plans at stage 1 using the known common transitions (0.7/0.3) and current
    stage-2 values. At stage 2, the agent receives a mixture of extrinsic reward and
    intrinsic signals: (a) surprise for rare transitions and (b) a novelty bonus based
    on inverse visitation counts. Intrinsic signals also shape choice by adding a bonus
    to the second-stage action values at decision time.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices on each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state: 0/1 are the two aliens.
    reward : array-like of float
        Obtained reward on each trial (e.g., 0 or 1).
    model_parameters : list/array of 5 floats
        [alpha2, beta, nu_surp, tau_bonus, xi_mix]
        - alpha2 in [0,1]: TD learning rate for second-stage values.
        - beta in [0,10]: Inverse temperature used at both stages.
        - nu_surp in [0,1]: Scaling of transition-surprise intrinsic reward (1 for rare, 0 for common).
        - tau_bonus in [0,1]: Controls decay of novelty bonus with visit count: bonus ~ 1 / (1 + count)^tau_bonus.
        - xi_mix in [0,1]: Mixture between extrinsic and intrinsic outcomes in learning:
          effective_reward = xi_mix * extrinsic + (1 - xi_mix) * intrinsic.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha2, beta, nu_surp, tau_bonus, xi_mix = model_parameters
    n_trials = len(action_1)

    # Known transition structure for planning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: a1 (A,U), cols: state (X,Y)

    # Second-stage Q-values
    Q2 = np.zeros((2, 2), dtype=float)

    # Visit counts for novelty bonus
    counts = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]

        # Stage-1 model-based values via one-step lookahead
        max_q2 = np.max(Q2, axis=1)             # per state
        Q1_mb = T @ max_q2                      # per action

        # Stage-1 policy
        Q1_eff = Q1_mb - np.max(Q1_mb)
        exp_q1 = np.exp(beta * Q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Intrinsic novelty bonuses for both second-stage actions in current state
        novelty_bonus = 1.0 / ((1.0 + counts[s, :]) ** max(tau_bonus, 1e-8))
        # Decision-time second-stage values include novelty bias
        Q2_eff = Q2[s, :] + (1.0 - xi_mix) * novelty_bonus
        Q2_eff = Q2_eff - np.max(Q2_eff)
        exp_q2 = np.exp(beta * Q2_eff)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r_ext = reward[t]

        # Transition surprise: rare=1, common=0 relative to canonical mapping
        common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        r_surp = 0.0 if common else 1.0
        r_intrinsic = nu_surp * r_surp + novelty_bonus[a2]

        # Learning mixture of extrinsic and intrinsic outcomes
        r_eff = xi_mix * r_ext + (1.0 - xi_mix) * r_intrinsic

        # TD update at stage 2 with effective reward
        delta2 = r_eff - Q2[s, a2]
        Q2[s, a2] += alpha2 * delta2

        # Update counts after acting
        counts[s, a2] += 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll