def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Hybrid MB-MF with learned transition matrix and separate stage temperatures.
    
    Summary
    -------
    The agent learns:
      - Second-stage model-free Q-values from rewards.
      - The first-stage transition matrix from experienced transitions.
    First-stage action values combine model-based evaluation (using the learned transition matrix
    applied to second-stage values) and model-free stage-1 values. Separate softmax temperatures
    are used for stage 1 and stage 2.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1 corresponding to the two aliens on that planet).
    reward : array-like of float (0 or 1)
        Binary reward outcome per trial.
    model_parameters : iterable of floats
        [theta_q, beta1, beta2, rho, chi_tr]
        - theta_q in [0,1]: learning rate for model-free Q updates (both stage 1 MF and stage 2).
        - beta1 in [0,10]: inverse temperature for stage-1 choices.
        - beta2 in [0,10]: inverse temperature for stage-2 choices.
        - rho in [0,1]: arbitration weight on model-based value at stage 1 (1=fully MB, 0=fully MF).
        - chi_tr in [0,1]: learning rate for updating the transition matrix from data.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    theta_q, beta1, beta2, rho, chi_tr = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T_hat (rows: actions at stage 1; cols: states)
    T_hat = np.full((2, 2), 0.5)

    # Model-free values
    q1_mf = np.zeros(2)         # stage-1 MF Q
    q2 = np.zeros((2, 2))       # stage-2 Q per state and action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy
        logits2 = beta2 * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy: MB from learned transitions + MF arbitration
        V_states = np.max(q2, axis=1)
        q1_mb = T_hat @ V_states
        q1_net = rho * q1_mb + (1.0 - rho) * q1_mf

        logits1 = beta1 * q1_net
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Learn transition model T_hat from the experienced transition (a1 -> s)
        # Delta rule towards one-hot of observed state
        for s_idx in range(2):
            target = 1.0 if s_idx == s else 0.0
            T_hat[a1, s_idx] += chi_tr * (target - T_hat[a1, s_idx])
        # (Row remains normalized by construction of the delta rule)

        # Stage-2 value update (model-free)
        delta2 = r - q2[s, a2]
        q2[s, a2] += theta_q * delta2

        # Stage-1 model-free update bootstrapping on obtained second-stage value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += theta_q * delta1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Successor-representation planning at stage 1 with leak and shared temperature.
    
    Summary
    -------
    Stage-2 values are learned model-free. Stage-1 planning uses a learned
    (action -> state) successor map M with leaky integration. The SR row for the chosen
    action is updated toward the observed second-stage state with a discount factor gamma
    that damps how quickly the map moves toward the one-hot state. A stage-1 choice
    perseveration bias encourages repeating the previous spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcomes.
    model_parameters : iterable of floats
        [lr2, beta, gamma, sr_leak, bias1]
        - lr2 in [0,1]: learning rate for second-stage Q-value updates.
        - beta in [0,10]: inverse temperature used for both stages.
        - gamma in [0,1]: SR discount factor controlling resistance to change (0=follows outcomes fast).
        - sr_leak in [0,1]: integration rate (leak) for updating the SR map.
        - bias1 in [0,1]: strength of stage-1 perseveration bias toward the previous choice.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr2, beta, gamma, sr_leak, bias1 = model_parameters
    n_trials = len(action_1)

    # Successor map from action (rows) to second-stage states (cols)
    M = np.zeros((2, 2))
    q2 = np.zeros((2, 2))  # stage-2 Q

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 SR-based policy with perseveration bias
        V_states = np.max(q2, axis=1)
        q1_sr = M @ V_states

        bias_vec = np.zeros(2)
        if prev_a1 is not None:
            bias_vec[prev_a1] = bias1

        logits1 = beta * q1_sr + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Update SR map for chosen action: leaky move toward observed state,
        # damped by gamma (higher gamma -> slower to adopt new evidence).
        onehot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        target_row = (1.0 - gamma) * onehot_s + gamma * M[a1, :]
        M[a1, :] += sr_leak * (target_row - M[a1, :])

        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr2 * delta2

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Surprise-modulated exploration and risk-sensitive rewards (MB stage-1).
    
    Summary
    -------
    - Stage 2: model-free value learning from a risk-sensitive utility of reward,
      with a learning rate scaled by transition surprise on each trial.
    - Stage 1: purely model-based evaluation using the known transition matrix (0.7/0.3),
      with softmax temperature reduced after surprising transitions to promote exploration.
    - A stage-1 perseveration bias encourages repeating the previous spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcomes.
    model_parameters : iterable of floats
        [alpha, beta, kappa, chi, stick1]
        - alpha in [0,1]: base learning rate for second-stage Q updates.
        - beta in [0,10]: baseline inverse temperature for both stages.
        - kappa in [0,1]: risk sensitivity; rewards are shrunk toward 0.5 by kappa.
        - chi in [0,1]: surprise modulation strength; increases learning rate and
                        decreases temperature on surprising transitions.
        - stick1 in [0,1]: stage-1 perseveration bias magnitude toward previous choice.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, kappa, chi, stick1 = model_parameters
    n_trials = len(action_1)

    # Known transition matrix (rows: A,U; cols: X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))  # stage-2 Q-values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute transition surprise for the experienced (a1 -> s)
        p_trans = T[a1, s]
        surpr = 1.0 - p_trans  # 0.3 for common, 0.7 for rare

        # Surprise-modulated temperature and learning rate
        beta_eff = beta * (1.0 - chi * surpr)
        beta_eff = max(beta_eff, 1e-6)
        alpha_eff = alpha * (1.0 + chi * surpr)
        alpha_eff = min(alpha_eff, 1.0)

        # Stage-2 policy
        logits2 = beta_eff * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 model-based policy with perseveration bias
        V_states = np.max(q2, axis=1)
        q1_mb = T @ V_states

        bias_vec = np.zeros(2)
        if prev_a1 is not None:
            bias_vec[prev_a1] = stick1

        logits1 = beta_eff * q1_mb + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Risk-sensitive utility transform of reward
        r_tilde = (1.0 - kappa) * r + kappa * 0.5

        # Stage-2 MF update with surprise-modulated learning rate
        delta2 = r_tilde - q2[s, a2]
        q2[s, a2] += alpha_eff * delta2

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss