def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Hybrid model-based/model-free with eligibility trace and first-stage perseveration.
    
    This model blends a model-based (MB) plan at stage 1 with a model-free (MF) SARSA backbone.
    The MB component uses the known common/rare transition structure (0.7/0.3) to project second-stage
    values to the first stage. The MF component is updated with an eligibility trace so that reward
    prediction errors at stage 2 propagate back to stage 1. A first-stage perseveration term biases
    repeating the previous first-stage choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within planet; W/P=0, S/H=1).
    reward : array-like of float
        Reward received on each trial (can be negative, zero, or positive).
    model_parameters : iterable of 5 floats
        [alpha, beta, eta, omega, rho]
        - alpha: [0,1] learning rate for value updates (both stages)
        - beta:  [0,10] inverse temperature for softmax at both stages
        - eta:   [0,1] eligibility trace strength to propagate stage-2 errors to stage-1 MF values
        - omega: [0,1] weight of model-based values in the first-stage choice (0=MF only, 1=MB only)
        - rho:   [0,1] first-stage perseveration weight added to the logit of the last first-stage action
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, eta, omega, rho = model_parameters
    n_trials = len(action_1)

    # Fixed known transition structure: rows=actions (A=0, U=1), cols=states (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q1_mf = np.zeros(2)        # stage-1 MF values
    q2 = np.zeros((2, 2))      # stage-2 values q2[state, action]

    last_a1 = -1  # for perseveration

    for t in range(n_trials):
        s = state[t]

        # Stage-2 policy (softmax over q2 in the observed state)
        logits2 = beta * q2[s, :].copy()
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB projection: expected max Q2 after each spaceship via transition matrix
        max_q2_by_state = np.max(q2, axis=1)  # [X, Y]
        q1_mb = T @ max_q2_by_state           # [A, U]

        # Stage-1 mixture and perseveration
        q1_mix = (1.0 - omega) * q1_mf + omega * q1_mb
        bias1 = np.zeros(2)
        if last_a1 in (0, 1):
            bias1[last_a1] += rho
        logits1 = beta * q1_mix + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace:
        # Combine immediate bootstrapping toward Q2 with propagation of the reward error.
        # - Bootstrapping term: Q2 value of the actually chosen stage-2 action
        # - Eligibility term: propagate the stage-2 prediction error back
        bootstrap = q2[s, a2]
        delta1_boot = bootstrap - q1_mf[a1]
        q1_mf[a1] += alpha * ((1.0 - eta) * delta1_boot + eta * delta2)

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Transition-belief learning, SARSA values, and outcome-gated perseveration.
    
    This model learns separate beliefs about each spaceship's common destination (A->X, U->Y)
    via a simple delta rule on transition probabilities. First-stage choices are guided by a
    model-based projection using these learned transition beliefs, blended with a model-free
    SARSA estimate. Perseveration at the first stage is gated by the previous outcome: the
    bias to repeat the last action is stronger after better outcomes.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within planet; W/P=0, S/H=1).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, stick, gate, alpha_c]
        - alpha:   [0,1] learning rate for Q-value updates (both stages)
        - beta:    [0,10] inverse temperature for softmax at both stages
        - stick:   [0,1] base perseveration weight added to the previous first-stage action's logit
        - gate:    [0,1] outcome gate; bias magnitude becomes stick*gate if last reward > 0,
                            and stick*(1-gate) otherwise (applied at stage 1)
        - alpha_c: [0,1] learning rate for transition-belief updates per chosen spaceship
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, stick, gate, alpha_c = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)      # stage-1 MF Q
    q2 = np.zeros((2, 2))    # stage-2 Q

    # Learned transition beliefs: cA ~ P(X|A), cU ~ P(Y|U); initialize at 0.7 (common)
    cA = 0.7
    cU = 0.7

    # Bookkeeping for perseveration and last outcome
    last_a1 = -1
    last_r = 0.0

    for t in range(n_trials):
        s = state[t]

        # Stage-2 policy
        logits2 = beta * q2[s, :].copy()
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Model-based projection using learned transition beliefs
        # Expected max Q2 after each spaceship
        max_q2 = np.max(q2, axis=1)  # [X, Y]
        # For A: P(X)=cA, P(Y)=1-cA; For U: P(Y)=cU, P(X)=1-cU
        q1_mb_A = cA * max_q2[0] + (1.0 - cA) * max_q2[1]
        q1_mb_U = (1.0 - cU) * max_q2[0] + cU * max_q2[1]
        q1_mb = np.array([q1_mb_A, q1_mb_U])

        # Blend MB with MF equally via a convex combination driven indirectly by learned values:
        # Here we use a simple average to avoid extra parameters; MF part: q1_mf
        q1_blend = 0.5 * q1_mf + 0.5 * q1_mb

        # Outcome-gated perseveration at stage 1
        bias1 = np.zeros(2)
        if last_a1 in (0, 1):
            mag = stick * (gate if last_r > 0 else (1.0 - gate))
            bias1[last_a1] += mag

        logits1 = beta * q1_blend + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Observe reward and update values
        r = reward[t]

        # Update transition beliefs for the chosen first-stage action
        # Move belief toward the actually observed state being the common one.
        if a1 == 0:
            target = 1.0 if s == 0 else 0.0  # A commonly->X
            cA += alpha_c * (target - cA)
            # keep within [0,1] via mild clipping
            cA = 0.000001 if cA < 0.000001 else (0.999999 if cA > 0.999999 else cA)
        else:
            target = 1.0 if s == 1 else 0.0  # U commonly->Y
            cU += alpha_c * (target - cU)
            cU = 0.000001 if cU < 0.000001 else (0.999999 if cU > 0.999999 else cU)

        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update toward the actually visited second-stage chosen value (SARSA)
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Bookkeeping
        last_a1 = a1
        last_r = r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Pure model-based stage 1 with uncertainty-guided exploration at stage 2 and forgetting.
    
    Stage 1 is purely model-based, using the known transition structure (0.7/0.3) to compute
    expected second-stage values. Stage 2 implements uncertainty-guided exploration: a leaky
    running estimate of value uncertainty adds a bonus to action values, encouraging sampling
    of less-certain aliens. Additionally, second-stage values of unchosen actions decay toward
    zero (forgetting), modeling limited memory or nonstationarity tracking.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within planet; W/P=0, S/H=1).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, bonus, decay, psi]
        - alpha: [0,1] learning rate for value updates and uncertainty tracking at stage 2
        - beta:  [0,10] inverse temperature for softmax at both stages
        - bonus: [0,1] weight on the uncertainty bonus added to second-stage values
        - decay: [0,1] forgetting rate applied to unchosen second-stage values each trial
        - psi:   [0,1] stage-2 perseveration weight added to the last chosen action in the same state
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, bonus, decay, psi = model_parameters
    n_trials = len(action_1)

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage values and uncertainty
    q2 = np.zeros((2, 2))
    u2 = np.ones((2, 2)) * 0.5  # initialize moderate uncertainty

    # Stage-2 perseveration memory: last chosen action per state
    last_a2 = {-1: -1, 0: -1, 1: -1}

    for t in range(n_trials):
        s = state[t]

        # Stage-2 policy with uncertainty bonus and perseveration
        bonus_vec = bonus * u2[s, :]
        bias2 = np.zeros(2)
        prev_a2 = last_a2.get(s, -1)
        if prev_a2 in (0, 1):
            bias2[prev_a2] += psi

        logits2 = beta * (q2[s, :] + bonus_vec) + bias2
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 pure model-based using known transitions and current q2
        max_q2 = np.max(q2, axis=1)  # [X, Y]
        q1_mb = T @ max_q2

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning updates at stage 2
        r = reward[t]
        delta2 = r - q2[s, a2]

        # Decay uncertainty toward baseline and update chosen with absolute TD error
        # (higher absolute error -> higher estimated uncertainty)
        for ss in (0, 1):
            for aa in (0, 1):
                # uncertainty mean-reversion to 0.5
                u2[ss, aa] = (1.0 - decay) * u2[ss, aa] + decay * 0.5

        u2[s, a2] = (1.0 - alpha) * u2[s, a2] + alpha * np.abs(delta2)

        # Forgetting: decay all unchosen values toward 0
        for ss in (0, 1):
            for aa in (0, 1):
                if not (ss == s and aa == a2):
                    q2[ss, aa] *= (1.0 - decay)

        # Chosen action value update
        q2[s, a2] += alpha * delta2

        # Update stage-2 perseveration memory
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll