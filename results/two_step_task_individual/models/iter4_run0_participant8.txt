def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Uncertainty-bonus hybrid with learned transitions.

    Mechanism
    - Learn the first-stage action→state transition probabilities (row-stochastic T) via a delta rule.
    - Learn second-stage Q-values via model-free TD(0).
    - First-stage values are a hybrid of model-free Q1 and model-based values (from learned T and Q2),
      plus an exploration bonus proportional to transition uncertainty (row entropy).
    - Policies at both stages are softmax with shared inverse temperature.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (aliens) within the observed state (0/1).
    reward : array-like of float {0,1}
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, mb_w, trans_lr, bonus]
        - alpha in [0,1]: learning rate for second-stage Q-learning and first-stage MF bootstrap.
        - beta in [0,10]: inverse temperature for both stages.
        - mb_w in [0,1]: weight on model-based value at the first stage.
        - trans_lr in [0,1]: learning rate for transition probabilities T.
        - bonus in [0,1]: exploration bonus weight based on transition uncertainty (entropy) per first-stage action.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, mb_w, trans_lr, bonus = model_parameters
    n_trials = len(action_1)

    # Initialize
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Transition model T[a1, s], start agnostic
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # MF values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Helper: entropy of a binary distribution normalized to [0,1]
    def row_entropy01(p):
        # p is length-2 probs
        p = np.clip(p, 1e-12, 1 - 1e-12)
        h = -(p[0] * np.log2(p[0]) + p[1] * np.log2(p[1]))
        return h / 1.0  # max entropy for binary is 1

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q at stage 1: expected max over Q2 under learned T
        v2 = np.max(Q2, axis=1)            # value per second-stage state
        Q1_mb = T @ v2                     # expected value per first-stage action

        # Uncertainty bonus: entropy per row of T
        U = np.array([row_entropy01(T[0, :]),
                      row_entropy01(T[1, :])])
        Q1 = (1.0 - mb_w) * Q1_mf + mb_w * Q1_mb + bonus * U

        # Stage-1 policy
        pref1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(pref1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        q2_row = Q2[s, :]
        pref2 = beta * (q2_row - np.max(q2_row))
        probs2 = np.exp(pref2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Second-stage TD(0)
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # First-stage MF bootstraps from realized second-stage action value
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        # Transition learning: move row a1 toward the observed state s
        target_T = np.array([0.0, 0.0])
        target_T[s] = 1.0
        T[a1, :] = (1.0 - trans_lr) * T[a1, :] + trans_lr * target_T
        # ensure normalization
        T[a1, :] /= (np.sum(T[a1, :]) + 1e-12)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Surprise-gated arbitration with learned transitions and stage-1 stickiness.

    Mechanism
    - Learn the transition matrix T via a delta rule.
    - Learn second-stage Q-values via model-free TD(0).
    - First-stage arbitration weight is trial-wise and depends on "transition surprise":
        surprise_t = 1 - T[a1_t, s_t] (higher when transition is rarer under current T).
      The weight on model-based value is w_t = (1 - theta)*0.5 + theta*surprise_t,
      blending a neutral baseline 0.5 with surprise (theta scales surprise influence).
    - Add stage-1 choice stickiness toward repeating the previous first-stage action.
    - Policies at both stages are softmax with shared beta.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial within the reached state.
    reward : array-like of float {0,1}
        Reward outcome.
    model_parameters : iterable of 5 floats
        [alpha, beta, theta, trans_lr, stick]
        - alpha in [0,1]: learning rate for Q2 and Q1_mf bootstrapping.
        - beta in [0,10]: inverse temperature for both stages.
        - theta in [0,1]: sensitivity of arbitration weight to surprise (0=constant 0.5, 1=equal to surprise).
        - trans_lr in [0,1]: learning rate for transition matrix T.
        - stick in [0,1]: stickiness bonus added to the previous first-stage action's preference.

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha, beta, theta, trans_lr, stick = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    prev_a1 = -1

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute model-based values
        v2 = np.max(Q2, axis=1)
        Q1_mb = T @ v2

        # Surprise from last transition under current model (use current trial a1,s)
        surprise = 1.0 - T[a1, s]  # in [0,1]
        w_t = (1.0 - theta) * 0.5 + theta * surprise  # in [0,1]

        Q1 = (1.0 - w_t) * Q1_mf + w_t * Q1_mb

        # Add stage-1 stickiness to preferences
        pref1 = Q1.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += stick

        pref1 = beta * (pref1 - np.max(pref1))
        probs1 = np.exp(pref1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        q2_row = Q2[s, :]
        pref2 = beta * (q2_row - np.max(q2_row))
        probs2 = np.exp(pref2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        # Update transitions
        target_T = np.array([0.0, 0.0])
        target_T[s] = 1.0
        T[a1, :] = (1.0 - trans_lr) * T[a1, :] + trans_lr * target_T
        T[a1, :] /= (np.sum(T[a1, :]) + 1e-12)

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Option-chunking model with cross-state generalization and lapse.

    Mechanism
    - Fixed known transitions: A→X common, U→Y common (0.7/0.3).
    - Learn second-stage Q-values per state via MF TD(0), with cross-state generalization:
      when updating Q2[s,a], also partially update the same action in the other state.
    - First-stage valuation blends:
        (i) Standard model-based value: expected max over Q2 using fixed transitions.
        (ii) "Option" value: for each first-stage action, commit to the second-stage action
             that is best in its most likely state, and evaluate that committed action across states.
      Weighted by option_w.

    - Policies at both stages use softmax with shared beta and a symmetric lapse: with probability
      'lapse' choices are random (uniform), otherwise follow softmax.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial within the reached state.
    reward : array-like of float {0,1}
        Reward outcome.
    model_parameters : iterable of 5 floats
        [alpha, beta, option_w, generalize, lapse]
        - alpha in [0,1]: learning rate for Q2 updates.
        - beta in [0,10]: inverse temperature for both stages.
        - option_w in [0,1]: weight on option value in first-stage valuation
                             (0=only standard MB, 1=only option-based).
        - generalize in [0,1]: degree of cross-state generalization when updating Q2
                               (fraction of PE applied to the other state's same action).
        - lapse in [0,1]: probability of random choice overriding softmax at both stages.

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha, beta, option_w, generalize, lapse = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Fixed known transitions (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))  # per state and second-stage action
    Q1_mf = np.zeros(2)    # keep a parallel MF for richness (bootstraps from Q2)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Standard MB: expected max over Q2
        v2 = np.max(Q2, axis=1)      # value per second-stage state
        Q1_mb = T @ v2

        # Option value: commit to the action best in the most likely state for each first-stage action
        option_val = np.zeros(2)
        for a in (0, 1):
            # most likely state for action a
            s_star = 0 if T[a, 0] >= T[a, 1] else 1
            a_star = np.argmax(Q2[s_star, :])
            # evaluate committing to a_star across possible states under T[a]
            option_val[a] = T[a, 0] * Q2[0, a_star] + T[a, 1] * Q2[1, a_star]

        # Blend MB and option valuation, plus a light MF bootstrap (for asymptotic flexibility)
        Q1 = (1.0 - option_w) * Q1_mb + option_w * option_val
        # Add a small MF component via Q1_mf to ensure MF is learned and used (not wasted variable)
        Q1 = 0.9 * Q1 + 0.1 * Q1_mf

        # Stage-1 policy with lapse
        pref1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(pref1)
        probs1 /= np.sum(probs1)
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapse
        q2_row = Q2[s, :]
        pref2 = beta * (q2_row - np.max(q2_row))
        probs2 = np.exp(pref2)
        probs2 /= np.sum(probs2)
        probs2 = (1.0 - lapse) * probs2 + lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Second-stage TD(0) with cross-state generalization
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2
        other_s = 1 - s
        # Apply generalized update to the same action in the other state
        Q2[other_s, a2] += generalize * alpha * (r - Q2[other_s, a2])

        # First-stage MF bootstrap from realized second-stage action value
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll