def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with uncertainty-bonus exploration and global forgetting.

    This model blends model-based (MB) planning with model-free (MF) values at stage 1,
    adds an uncertainty-driven exploration bonus at stage 2 (and propagates that into MB
    planning), and applies global forgetting of second-stage values toward 0.5.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien at the visited planet (0 or 1).
    reward : array-like of float/int (0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, omega, kappaU, alphaF]
        - alpha (reward learning rate, [0,1]): TD learning for second-stage values and
          model-free credit at stage 1.
        - beta (inverse temperature, [0,10]): softmax sensitivity for both stages.
        - omega (MB/MF mixture at stage 1, [0,1]): 1 = purely MB; 0 = purely MF.
        - kappaU (uncertainty bonus weight, [0,1]): added to second-stage action values
          proportional to uncertainty sqrt(p*(1-p)), where p is the learned reward prob.
        - alphaF (global forgetting rate, [0,1]): per-trial decay of all second-stage Q
          values toward 0.5.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, omega, kappaU, alphaF = model_parameters
    n_trials = len(action_1)

    # Fixed (known) transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage value estimates (approximate reward probability)
    q2 = np.full((2, 2), 0.5, dtype=float)
    # Stage-1 model-free values
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        # Uncertainty for each second-stage action (Bernoulli variance sqrt)
        p_hat = np.clip(q2, 1e-6, 1 - 1e-6)
        uncert = np.sqrt(p_hat * (1.0 - p_hat))
        q2_aug = q2 + kappaU * uncert

        # Stage-1 MB plan uses augmented q2 (so exploration value is considered in planning)
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_q2_aug

        # Hybrid stage-1 values
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy for the visited state
        s2 = state[t]
        logits2 = beta * q2_aug[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Global forgetting of q2 toward 0.5
        q2 = (1.0 - alphaF) * q2 + alphaF * 0.5

        # TD update for the selected second-stage action
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Model-free credit to stage-1 chosen action (TD(1))
        q1_mf[a1] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Learned-transitions MB planner with volatility-adaptive temperature and stage-2 stay bias.

    This model learns the transition structure online, adapts decision noise based on
    recent reward volatility, and includes a state-dependent perseveration (stay) bias
    at the second stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien at the visited planet (0 or 1).
    reward : array-like of float/int (0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta0, etaB, alphaT, psi2]
        - alpha (reward learning rate, [0,1]): TD learning for second-stage values.
        - beta0 (base inverse temperature, [0,10]): maximum softmax precision.
        - etaB (volatility learning rate, [0,1]): updates running volatility from |RPE|.
          Higher volatility reduces the effective beta.
        - alphaT (transition learning rate, [0,1]): for updating P(state | action_1).
        - psi2 (stage-2 stay bias, [0,1]): additive bias to repeat the last action within
          each state (separate memory per state).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta0, etaB, alphaT, psi2 = model_parameters
    n_trials = len(action_1)

    # Initialize transition beliefs near common/rare but learnable
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Second-stage values (reward expectations)
    q2 = np.full((2, 2), 0.5, dtype=float)

    # Running estimate of reward volatility (|RPE|), starts moderate
    vol = 0.5

    # For stage-2 state-dependent perseveration: last action per state
    last_a2_for_state = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        # Effective inverse temperature decreases with volatility
        beta_eff = beta0 * max(0.0, 1.0 - vol)
        # Keep some minimal precision to avoid numerical underflow
        beta_eff = max(beta_eff, 1e-6)

        # Model-based stage-1 values from current transition estimates
        max_q2 = np.max(q2, axis=1)
        q1 = T @ max_q2

        # Stage-1 policy
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with state-dependent stay bias
        s2 = state[t]
        bias2 = np.zeros(2, dtype=float)
        if last_a2_for_state[s2] != -1:
            bias2[last_a2_for_state[s2]] += psi2

        logits2 = beta_eff * q2[s2] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and learn
        r = reward[t]

        # Update transitions using a simple delta rule on the chosen first-stage action
        oh = np.array([0.0, 0.0], dtype=float)
        oh[s2] = 1.0
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        # Normalize and clip for numerical stability
        T[a1] = np.clip(T[a1], 1e-8, 1.0)
        T[a1] /= np.sum(T[a1])

        # TD update at stage 2
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Volatility update from absolute RPE
        vol += etaB * (abs(delta2) - vol)
        vol = np.clip(vol, 0.0, 1.0)

        # Update stage-2 perseveration memory
        last_a2_for_state[s2] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Adaptive arbitration: MB/MF weight modulated by recent transition surprise + stage-1 stay.

    This model mixes model-based planning and model-free values at stage 1, with the
    arbitration weight adapting to the previous trialâ€™s transition surprise
    (1 - P(observed_state | chosen_action)). A stage-1 perseveration bias encourages
    repeating the previous spaceship. Second-stage values are learned via TD.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien at the visited planet (0 or 1).
    reward : array-like of float/int (0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, omega0, kappaS, phi1]
        - alpha (reward learning rate, [0,1]): TD learning for second-stage values and
          model-free credit to stage 1.
        - beta (inverse temperature, [0,10]): softmax sensitivity for both stages.
        - omega0 (base MB weight, [0,1]): baseline weight on model-based value at stage 1.
        - kappaS (surprise sensitivity, [0,1]): how strongly previous transition surprise
          shifts the MB weight upward on the next trial.
        - phi1 (stage-1 stay bias, [0,1]): additive bias to repeat the previous stage-1 action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, omega0, kappaS, phi1 = model_parameters
    n_trials = len(action_1)

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage values and stage-1 model-free values
    q2 = np.full((2, 2), 0.5, dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Memory for previous trial features
    prev_a1 = None
    prev_surprise = 0.0  # initial arbitration modulation

    for t in range(n_trials):
        # Compute model-based values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Adaptive arbitration weight using previous trial's surprise
        omega_t = omega0 + kappaS * prev_surprise
        omega_t = np.clip(omega_t, 0.0, 1.0)

        # Hybrid value
        q1 = omega_t * q1_mb + (1.0 - omega_t) * q1_mf

        # Stage-1 stay bias
        bias1 = np.zeros(2, dtype=float)
        if prev_a1 is not None:
            bias1[prev_a1] += phi1

        # Stage-1 policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = state[t]
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # MF credit to stage-1 choice
        q1_mf[a1] += alpha * delta2

        # Update arbitration driver: transition surprise from current trial
        # surprise = 1 - P(observed state | chosen action)
        surprise_t = 1.0 - T[a1, s2]
        prev_surprise = np.clip(surprise_t, 0.0, 1.0)

        # Update stay memory
        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll