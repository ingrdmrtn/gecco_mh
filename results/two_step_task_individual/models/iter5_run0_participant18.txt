def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Surprise-gated arbitration between model-based and model-free with perseveration.

    This model uses a hybrid of model-based (MB) and model-free (MF) valuation at stage 1.
    The MB/MF mixing weight is dynamically modulated by transition surprise on each trial.
    Stage 2 uses MF learning. A perseveration (stickiness) bias encourages repeating the
    previous actions at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 correspond to the two aliens on the planet.
    reward : array-like of float (typically 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha, beta, w0, gamma, kappa]
        - alpha (learning rate, [0,1]): MF learning rate used at stage 2 and for stage-1 MF bootstrapping.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - w0 (baseline MB weight, [0,1]): Baseline weight given to the MB value at stage 1.
        - gamma (surprise gain, [0,1]): Scales the impact of transition surprise on the MB/MF arbitration.
        - kappa (perseveration, [0,1]): Strength of repeating the previous action (both stages).
          A value of 0 disables perseveration; 1 yields strong stickiness.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha, beta, w0, gamma, kappa = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows are actions (A,U), columns are states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)       # MF values at stage 1 (for spaceships A,U)
    q2 = np.zeros((2, 2))     # MF values at stage 2 (for each planet, two aliens)

    # Perseveration traces (one-hot of last choice)
    pers1 = np.zeros(2)
    pers2 = np.zeros((2, 2))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    # Convert baseline w0 in (0,1) to logit for stable modulation
    w0_logit = np.log((w0 + 1e-10) / (1 - w0 + 1e-10))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB action values at stage 1: expected max over next-state values
        max_q2 = np.max(q2, axis=1)        # best alien value at each planet
        q1_mb = T @ max_q2                 # MB values for A,U

        # Surprise from the realized transition given chosen action (higher for rare transitions)
        p_trans = T[a1, s]                 # probability of observed state given action
        surprise = 1.0 - p_trans           # 0.3 for common, 0.7 for rare

        # Surprise-gated arbitration weight (in [0,1]) via logistic transform
        # Center surprise around 0.5 so gamma is a symmetric gain
        w_t = sigmoid(w0_logit + gamma * (surprise - 0.5) * 4.0)  # scale to use range effectively

        # Effective stage-1 decision values with perseveration
        q1_eff = w_t * q1_mb + (1.0 - w_t) * q1_mf + kappa * pers1

        # Stage-1 choice probability
        q1_eff_centered = q1_eff - np.max(q1_eff)
        exp_q1 = np.exp(beta * q1_eff_centered)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 decision values with perseveration
        q2_eff = q2[s].copy() + kappa * pers2[s]
        q2_eff_centered = q2_eff - np.max(q2_eff)
        exp_q2 = np.exp(beta * q2_eff_centered)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning: stage 2 MF
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning: stage 1 MF bootstraps from current second-stage value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update perseveration traces (one-hot for last actions)
        pers1[:] = 0.0
        pers1[a1] = 1.0
        pers2[:, :] = 0.0
        pers2[s, a2] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-learning MB/MF arbitration based on transition certainty with decaying habit.

    This model learns the transition matrix from experience and arbitrates between MB and MF
    action values at stage 1 according to the certainty (inverse entropy) of the learned transitions.
    A decaying habit trace at stage 1 contributes additively when arbitration favors MF.
    Stage 2 uses MF learning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 for the two aliens on that planet.
    reward : array-like of float (typically 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha2, beta, alpha_T, zeta, nu]
        - alpha2 (learning rate stage 2, [0,1]): MF learning rate for second-stage values; also used to bootstrap stage-1 MF.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - alpha_T (transition learning rate, [0,1]): Learning rate for updating the transition probabilities.
        - zeta (arbitration sensitivity, [0,1]): Scales how strongly transition certainty shifts weight toward MB.
        - nu (habit decay, [0,1]): Exponential decay factor for a stage-1 habit trace; also controls how long habit persists.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha2, beta, alpha_T, zeta, nu = model_parameters
    n_trials = len(action_1)

    # Initialize transition model rows for A and U as uniform (agnostic prior)
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)       # MF values at stage 1 (A,U)
    q2 = np.zeros((2, 2))     # MF values at stage 2 (planets x aliens)

    # Stage-1 habit trace (decays with nu)
    habit = np.zeros(2)

    def softmax(x, temp):
        x = x - np.max(x)
        ex = np.exp(temp * x)
        return ex / np.sum(ex)

    def row_entropy(p_row):
        # Binary entropy in nats
        p = np.clip(p_row, 1e-10, 1 - 1e-10)
        return -np.sum(p * np.log(p))

    # Max binary entropy in nats is ln(2)
    max_entropy = np.log(2.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # MB action values from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Arbitration weight based on certainty (1 - normalized entropy) of chosen row
        H = row_entropy(T[a1])
        certainty = 1.0 - (H / max_entropy)             # in [0,1]; 0 uncertain, 1 certain
        # Convert certainty to a dynamic MB weight applied to both actions
        # Use zeta to scale certainty effect around 0.5 via sigmoid
        z = (certainty - 0.5) * 4.0 * zeta
        w_t = 1.0 / (1.0 + np.exp(-z))                  # in (0,1)
        # Use same w_t for both actions this trial (agent-level arbitration state)
        q1_eff_base = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Habit contribution: decaying trace; emphasize when arbitration favors MF (1 - w_t)
        q1_eff = q1_eff_base + (1.0 - w_t) * habit

        # Stage 1 choice probability
        probs1 = softmax(q1_eff, beta)
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice probability
        q2_eff = q2[s]
        probs2 = softmax(q2_eff, beta)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 (MF)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Stage 1 MF bootstrapping from current second-stage value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha2 * delta1

        # Transition learning: update chosen row toward the observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - alpha_T) * T[a1] + alpha_T * target
        # Keep row normalized within numerical precision
        T[a1] /= np.sum(T[a1])

        # Update habit trace: decay and add one-hot of chosen action
        habit *= nu
        habit[a1] += (1.0 - nu)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive planning with learned mean-variance utilities and common-transition bias.

    This model tracks, for each alien, both the mean and variance of rewards. The second-stage
    utility is mean - lambda * variance, capturing risk aversion (lambda > 0). At stage 1,
    the agent plans model-based using the fixed transition structure to the planets and the
    current risk-sensitive utilities. Additionally, a bias term favors spaceships with common
    transitions (A->X, U->Y), reflecting a prior over transition structure.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the reached planet (0/1).
    reward : array-like of float (typically 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha_m, alpha_v, beta, lambda_risk, b_comm]
        - alpha_m (mean learning rate, [0,1]): Step size to update per-alien reward means.
        - alpha_v (variance learning rate, [0,1]): Step size to update per-alien reward variances.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - lambda_risk (risk aversion, [0,1]): Weight on variance penalty in utility (higher = more risk-averse).
        - b_comm (common-transition bias, [0,1]): Strength of prior bias towards common-transition spaceships at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha_m, alpha_v, beta, lambda_risk, b_comm = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track per-alien mean and variance of reward
    mu = np.zeros((2, 2))     # mean reward for each planet's two aliens
    var = np.ones((2, 2)) * 0.25  # initialize variance moderately (e.g., 0.25 for Bernoulli ~0.5)

    # Constant bias vector favoring common transitions at stage 1; centered around 0
    # For A: +0.2 (0.7 - 0.5), for U: +0.2 (its common to Y), but symmetry implies same magnitude.
    common_prob = np.array([0.7, 0.7])         # prob of common transition for A and U to their preferred planets
    bias_comm = b_comm * (common_prob - 0.5)   # centered bias in [-0.5*b_comm, 0.5*b_comm]

    def softmax(x, temp):
        x = x - np.max(x)
        ex = np.exp(temp * x)
        return ex / np.sum(ex)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Risk-sensitive utilities at stage 2
        util = mu - lambda_risk * var

        # Stage-2 choice probabilities based on utilities in the reached state
        probs2 = softmax(util[s], beta)
        p_choice_2[t] = probs2[a2]

        # Update mean and variance for the chosen alien in the reached state
        # Keep a copy of previous mean for variance update
        mu_prev = mu[s, a2]
        # Mean update
        mu[s, a2] = mu_prev + alpha_m * (r - mu_prev)
        # Variance update using squared prediction error around previous mean
        sq_err = (r - mu_prev) ** 2
        var[s, a2] = (1.0 - alpha_v) * var[s, a2] + alpha_v * sq_err
        # Prevent negative/zero variance due to numerical issues
        var[s, a2] = max(var[s, a2], 1e-6)

        # Recompute utilities after update to propagate to stage 1 consistently
        util = mu - lambda_risk * var

        # Stage-1 model-based values: expected max utility at next state
        max_util = np.max(util, axis=1)
        q1_mb = T @ max_util

        # Add common-transition prior bias
        q1_eff = q1_mb + bias_comm

        # Stage-1 choice probabilities
        probs1 = softmax(q1_eff, beta)
        p_choice_1[t] = probs1[a1]

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll