def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free agent with surprise-gated planning and eligibility traces.

    The agent combines model-free first-stage values with a model-based plan computed from
    second-stage values. The effective weight on model-based planning is increased on
    surprising (rare) transitions via a surprise-sensitivity gate. Second-stage learning
    uses a standard TD rule, and the model-free first-stage values are updated by an
    eligibility trace using the second-stage prediction error.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 index of alien on reached planet).
    reward : array-like of float
        Reward outcome (e.g., 0 or 1) per trial.
    model_parameters : sequence of floats
        [alpha, beta, w, lam, zeta]
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w in [0,1]: baseline weight on model-based values at stage 1.
        - lam in [0,1]: eligibility trace weight to back up the stage-2 PE to stage-1 MF values.
        - zeta in [0,1]: surprise sensitivity; increases model-based weight on rare transitions.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, lam, zeta = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (A->X, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q2 = np.zeros((2, 2)) + 0.5  # second-stage MF values per (planet, alien)
    q1_mf = np.zeros(2) + 0.0    # first-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first-stage action values from current q2
        max_q2 = np.max(q2, axis=1)   # value of each planet
        q1_mb = T @ max_q2            # expected value per first-stage action

        # Compute surprise for this trial relative to chosen action and observed state
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # The probability mass assigned by the chosen action to the observed state
        p_obs = T[a1, s]
        surprise = 1.0 - p_obs  # 0 for common, ~0.7 for rare under this T

        # Surprise-gated model-based weight (clipped to [0,1])
        w_eff = w + zeta * surprise
        if w_eff < 0.0:
            w_eff = 0.0
        elif w_eff > 1.0:
            w_eff = 1.0

        # Combine MB and MF values for stage 1
        q1 = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Stage-1 softmax policy
        logits1 = q1
        logits1 = logits1 - np.max(logits1)
        p1 = np.exp(beta * logits1)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 softmax policy (on reached state)
        logits2 = q2[s]
        logits2 = logits2 - np.max(logits2)
        p2 = np.exp(beta * logits2)
        p2 = p2 / (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Learning
        # Stage-2 TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF backup via eligibility trace from the stage-2 PE
        q1_mf[a1] += lam * alpha * pe2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with counterfactual learning, lapse, and planet preference bias.

    The agent plans at the first stage using the known transition matrix and current
    second-stage values. At the second stage, the chosen alien is updated by standard TD,
    and the unchosen alien on the same planet is updated by a counterfactual rule that
    assumes a complementary outcome (1 - r), weighted by a counterfactual learning weight.
    The first-stage policy includes a constant bias toward the spaceship that commonly
    visits planet X, and both stages are subject to a lapse process mixing the softmax
    policy with a uniform random choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 index of alien on reached planet).
    reward : array-like of float
        Reward outcome per trial.
    model_parameters : sequence of floats
        [alpha, beta, phi_cf, lapse, psi]
        - alpha in [0,1]: learning rate for chosen second-stage action.
        - beta in [0,10]: inverse temperature for softmax.
        - phi_cf in [0,1]: counterfactual learning weight for unchosen second-stage action
                           toward outcome (1 - reward).
        - lapse in [0,1]: lapse probability; with probability lapse, choices are uniform random.
        - psi in [0,1]: constant bias added to the logit of spaceship A (planet X preference).
                        Implemented as +psi to action 0 and -psi to action 1 in first-stage logits.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, phi_cf, lapse, psi = model_parameters
    n_trials = len(action_1)

    # Known transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage values
    q2 = np.zeros((2, 2)) + 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first-stage values from q2
        max_q2 = np.max(q2, axis=1)  # best alien on each planet
        q1_mb = T @ max_q2

        # First-stage softmax with planet-X bias and lapse
        bias_vec = np.array([psi, -psi], dtype=float)
        logits1 = q1_mb + bias_vec
        logits1 = logits1 - np.max(logits1)
        p1_soft = np.exp(beta * logits1)
        p1_soft = p1_soft / (np.sum(p1_soft) + eps)
        p1 = (1.0 - lapse) * p1_soft + lapse * 0.5

        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage softmax with lapse
        s = state[t]
        logits2 = q2[s]
        logits2 = logits2 - np.max(logits2)
        p2_soft = np.exp(beta * logits2)
        p2_soft = p2_soft / (np.sum(p2_soft) + eps)
        p2 = (1.0 - lapse) * p2_soft + lapse * 0.5

        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning at stage 2: chosen update
        r = reward[t]
        pe_chosen = r - q2[s, a2]
        q2[s, a2] += alpha * pe_chosen

        # Counterfactual update for unchosen alien on same planet toward (1 - r)
        a2_other = 1 - a2
        pe_unchosen = (1.0 - r) - q2[s, a2_other]
        q2[s, a2_other] += phi_cf * pe_unchosen

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Transition-sensitivity learning with reward-by-transition bias (MB+MF hybrid).

    The agent learns second-stage values with different learning rates for common vs rare
    transitions, allowing transition-dependent credit assignment. First-stage values
    include a model-free component updated by backing up the second-stage prediction error
    and a model-based component from the transition matrix. The first-stage choice logits
    also include a reward-by-transition interaction bias: after a rewarded common transition,
    the agent tends to repeat the same first-stage action, while after a rewarded rare
    transition, the agent tends to switch. The magnitude of this dynamic bias is controlled
    by rho_tri.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 index of alien on reached planet).
    reward : array-like of float
        Reward outcome per trial.
    model_parameters : sequence of floats
        [alpha_c, alpha_r, beta, g, rho_tri]
        - alpha_c in [0,1]: learning rate for second-stage values following common transitions.
        - alpha_r in [0,1]: learning rate for second-stage values following rare transitions.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - g in [0,1]: gain for backing up the second-stage PE to first-stage MF values.
        - rho_tri in [0,1]: strength of reward-by-transition interaction bias on first-stage logits.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_c, alpha_r, beta, g, rho_tri = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2)) + 0.5     # second-stage values
    q1_mf = np.zeros(2) + 0.0       # first-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Keep a dynamic bias vector for first-stage choices shaped by last trial's reward x transition
    bias1 = np.zeros(2)

    eps = 1e-12

    for t in range(n_trials):
        # Compute model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MF and MB (simple average without extra parameter; MF learned via g)
        q1 = 0.5 * q1_mb + 0.5 * q1_mf

        # First-stage softmax with dynamic reward-by-transition bias
        logits1 = q1 + bias1
        logits1 = logits1 - np.max(logits1)
        p1 = np.exp(beta * logits1)
        p1 = p1 / (np.sum(p1) + eps)

        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = q2[s]
        logits2 = logits2 - np.max(logits2)
        p2 = np.exp(beta * logits2)
        p2 = p2 / (np.sum(p2) + eps)

        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Determine whether current transition is common or rare relative to chosen action
        # Common if T[a1, s] is the larger entry (>0.5)
        is_common = 1 if T[a1, s] >= 0.5 else 0

        # Learning rate conditioned on transition type
        alpha = alpha_c if is_common == 1 else alpha_r

        # Stage-2 update
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF backup using g
        q1_mf[a1] += g * alpha * pe2

        # Update the reward-by-transition interaction bias for the next trial
        # Bias rule:
        # - After reward and common: bias toward repeating a1.
        # - After reward and rare: bias toward switching away from a1.
        # - After no reward: decay bias toward zero (handled implicitly by symmetric update).
        bias1 *= (1.0 - rho_tri)  # decay a fraction of existing bias
        if r > 0.0:
            if is_common == 1:
                # Encourage repeat: add +rho to chosen, -rho to unchosen
                bias1[a1] += rho_tri
                bias1[1 - a1] -= rho_tri
            else:
                # Encourage switch: subtract at chosen, add at unchosen
                bias1[a1] -= rho_tri
                bias1[1 - a1] += rho_tri
        # If r == 0, only decay acts (no additional push)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll