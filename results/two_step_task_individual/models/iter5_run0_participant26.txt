def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Information-seeking model-based planner with model-free backup and perseveration.

    This model combines:
    - Model-based (MB) planning at stage 1 using the known transition matrix.
    - Model-free (MF) TD learning at both stages.
    - An information-seeking bonus for uncertain second-stage states based on entropy of their action values.
    - Perseveration (tendency to repeat the last first-stage choice).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage action within the reached state (0 or 1).
    reward : array-like of float (0 or 1)
        Received reward.
    model_parameters : iterable of floats
        [alpha, beta, omega0, kappa_info, psi_pers]
        - alpha (0..1): Learning rate for model-free Q updates (both stages).
        - beta (0..10): Inverse temperature used for softmax at both stages.
        - omega0 (0..1): Weight of MB value in the first-stage choice; MF weight is (1-omega0).
        - kappa_info (0..1): Scale of information-seeking bonus; more weight for uncertain second-stage states.
                             Uncertainty is the entropy of the softmax over second-stage Q-values.
        - psi_pers (0..1): Perseveration bias added to the previously chosen first-stage action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, omega0, kappa_info, psi_pers = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows are A(0)/U(1); columns are X(0)/Y(1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)          # stage-1 MF values for A/U
    q2 = np.zeros((2, 2))        # stage-2 action values per state

    prev_a1 = None

    for t in range(n_trials):
        # Information-seeking bonus per second-stage state: entropy of softmax over q2
        # Compute softmax for each state to get entropy H(s) in [0, ln2]; normalize by ln2.
        H_norm = np.zeros(2)
        for s in range(2):
            logits_s = beta * (q2[s] - np.max(q2[s]))
            ps = np.exp(logits_s)
            ps /= (np.sum(ps) + 1e-12)
            # entropy with small epsilon for stability
            eps_s = 1e-12
            ent = -np.sum(ps * np.log(ps + eps_s))
            H_norm[s] = ent / np.log(2.0)  # normalize to [0,1]

        # MB evaluation at stage 1: expected max over second-stage actions + info bonus
        max_q2 = np.max(q2, axis=1)
        info_bonus = kappa_info * H_norm
        q1_mb = transition_matrix @ (max_q2 + info_bonus)

        # Combine MB and MF for stage 1; add perseveration on previous first-stage choice
        q1_combined = omega0 * q1_mb + (1.0 - omega0) * q1_mf
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += psi_pers

        logits1 = beta * (q1_combined + bias1 - np.max(q1_combined + bias1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in the reached state
        s2 = state[t]
        logits2 = beta * (q2[s2] - np.max(q2[s2]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF learning at stage 2
        td2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # MF learning at stage 1: bootstrap towards the obtained stage-2 value (SARSA-style)
        target1 = q2[s2, a2]
        td1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-aware model-free control with separate stage temperatures and stay/switch bias.

    This model is primarily model-free at both stages but incorporates:
    - A transition–outcome-dependent bias on first-stage choices that captures model-based stay/switch patterns.
    - Separate inverse temperatures for the first and second stages.
    - A generic perseveration tendency to repeat the last first-stage choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received.
    model_parameters : iterable of floats
        [alpha, beta1, beta2, theta_trans_bias, rho_stay]
        - alpha (0..1): Learning rate for model-free updates at both stages.
        - beta1 (0..10): Inverse temperature for first-stage softmax.
        - beta2 (0..10): Inverse temperature for second-stage softmax.
        - theta_trans_bias (0..1): Strength of transition–outcome bias. Positive values increase
                                   staying after rewarded-common or unrewarded-rare outcomes, and
                                   promote switching after rewarded-rare or unrewarded-common.
        - rho_stay (0..1): Perseveration bias on repeating the previous first-stage action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta1, beta2, theta_trans_bias, rho_stay = model_parameters
    n_trials = len(action_1)

    # Transition matrix for determining common vs rare transitions
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    prev_a1 = None
    prev_common = None
    prev_r = None

    for t in range(n_trials):
        # Compute first-stage bias from previous trial's transition-outcome
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_common is not None and prev_r is not None:
            # +1 for reward, -1 for no reward
            signed_r = 2.0 * prev_r - 1.0
            # +1 for common, -1 for rare
            signed_trans = 1.0 if prev_common else -1.0
            # Positive value favors staying; negative favors switching
            b_val = theta_trans_bias * (signed_r * signed_trans)

            # Apply the bias symmetrically: push towards or away from prev_a1
            bias1[prev_a1] += b_val
            bias1[1 - prev_a1] -= b_val

        # Add generic perseveration to the previous first-stage action
        if prev_a1 is not None:
            bias1[prev_a1] += rho_stay

        # First-stage policy
        logits1 = beta1 * (q1 + bias1 - np.max(q1 + bias1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy in reached state
        s2 = state[t]
        logits2 = beta2 * (q2[s2] - np.max(q2[s2]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF learning: stage 2
        td2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # MF learning: stage 1 (bootstrapping to obtained stage-2 value)
        target1 = q2[s2, a2]
        td1 = target1 - q1[a1]
        q1[a1] += alpha * td1

        # Prepare for next trial
        prev_a1 = a1
        # Determine whether the transition was common given the chosen first-stage action
        # Common if (A->X) or (U->Y)
        prev_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        prev_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Kalman filter learning of second-stage values with MB–MF arbitration and repetition bias.

    This model maintains Gaussian beliefs over second-stage action values using a Kalman filter,
    enabling adaptive learning rates based on uncertainty and drift. First-stage choices combine
    model-based planning (using the transition matrix and the current posterior means) with a
    model-free first-stage cache updated via uncertainty-weighted bootstrapping. A repetition bias
    applies at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received.
    model_parameters : iterable of floats
        [kappa_mb, beta, sigma0, tau_drift, rho_rep]
        - kappa_mb (0..1): Weight on model-based values in first-stage decisions (MF weight is 1-kappa_mb).
        - beta (0..10): Inverse temperature for softmax at both stages.
        - sigma0 (0..1): Initial posterior variance for each second-stage action; also informs observation noise.
        - tau_drift (0..1): Process noise added each trial to capture reward drift (higher => more volatility).
        - rho_rep (0..1): Repetition bias magnitude applied to repeat the most recent action at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    kappa_mb, beta, sigma0, tau_drift, rho_rep = model_parameters
    n_trials = len(action_1)

    # Transition matrix (A->X common; U->Y common)
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Kalman beliefs for stage-2 action values
    mu2 = np.zeros((2, 2)) + 0.5  # initialize near 0.5
    var2 = np.zeros((2, 2)) + sigma0  # initial uncertainty

    # Stage-1 MF cache
    q1_mf = np.zeros(2)

    # Observation noise; tie to sigma0 to use the parameter meaningfully
    R = np.clip(1.0 - sigma0, 1e-6, 1.0)  # higher sigma0 -> lower observation noise

    prev_a1 = None
    prev_a2_by_state = [-1, -1]  # last second-stage action taken in each state

    for t in range(n_trials):
        # Predict step for Kalman: add process noise to all second-stage actions
        var2 = var2 + tau_drift

        # Model-based value for stage 1 from current posterior means
        max_mu2 = np.max(mu2, axis=1)
        q1_mb = T @ max_mu2

        # Combine MB and MF for stage 1
        q1 = (1.0 - kappa_mb) * q1_mf + kappa_mb * q1_mb

        # Add repetition bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_rep

        logits1 = beta * (q1 + bias1 - np.max(q1 + bias1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state with repetition bias within state
        s2 = state[t]
        bias2 = np.zeros(2)
        if prev_a2_by_state[s2] != -1:
            bias2[prev_a2_by_state[s2]] += rho_rep

        logits2 = beta * (mu2[s2] + bias2 - np.max(mu2[s2] + bias2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Kalman update for the chosen second-stage action
        # Gain depends on current uncertainty
        K = var2[s2, a2] / (var2[s2, a2] + R)
        mu2[s2, a2] = mu2[s2, a2] + K * (r - mu2[s2, a2])
        var2[s2, a2] = (1.0 - K) * var2[s2, a2]

        # Uncertainty-weighted MF update at stage 1: use K as adaptive learning rate
        alpha_eff = K
        target1 = mu2[s2, a2]
        q1_mf[a1] += alpha_eff * (target1 - q1_mf[a1])

        # Update repetition trackers
        prev_a1 = a1
        prev_a2_by_state[s2] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll