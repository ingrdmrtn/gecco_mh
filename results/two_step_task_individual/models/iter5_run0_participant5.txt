def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Hybrid planning with learned transition model and MF first-stage critic + stage-2 perseveration.
    
    Summary
    -------
    - Stage-2 values (Q2) are learned model-free from reward.
    - Stage-1 uses a hybrid of:
        (a) model-based values computed via a learned action→state transition model T, and
        (b) a model-free first-stage critic Q1 updated by TD toward the reached state's value.
      The mixture weight w_plan trades off MB and MF at stage 1.
    - Stage-2 action selection includes a perseveration bias toward repeating the previous
      second-stage action in the same state (stick2).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=spaceship A, 1=spaceship U).
    state : array-like of int (0 or 1)
        Reached second-stage state per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 = two aliens available on each planet).
    reward : array-like of float (0 or 1)
        Binary reward outcome at the end of each trial.
    model_parameters : iterable of floats
        [lr2, beta, w_plan, lrT, stick2]
        - lr2 in [0,1]: learning rate for reward value updates (used for both Q2 and Q1 TD updates).
        - beta in [0,10]: softmax inverse temperature shared across both stages.
        - w_plan in [0,1]: weight of model-based value at stage 1 (1=fully MB, 0=fully MF Q1).
        - lrT in [0,1]: learning rate for updating the action→state transition model T.
        - stick2 in [0,1]: perseveration strength for repeating the previous stage-2 action in the same state.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed stage-1 and stage-2 choices under the model.
    """
    lr2, beta, w_plan, lrT, stick2 = model_parameters
    n_trials = len(action_1)

    # Transition model T[a, s]: P(state=s | action=a); initialize agnostic (0.5/0.5)
    T = np.full((2, 2), 0.5)

    # Model-free values
    q2 = np.zeros((2, 2))  # second-stage Q
    q1_mf = np.zeros(2)    # first-stage MF critic

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # For stage-2 perseveration within the same state
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy with within-state perseveration
        bias2 = np.zeros(2)
        prev_a2 = prev_a2_by_state[s]
        if prev_a2 is not None:
            bias2[prev_a2] += stick2

        logits2 = beta * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB value from learned transition model
        V_states = np.max(q2, axis=1)
        q1_mb = T @ V_states

        # Stage-1 hybrid value
        q1_mix = w_plan * q1_mb + (1.0 - w_plan) * q1_mf

        logits1 = beta * q1_mix
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Update learned transition model T toward the observed state given chosen action
        # One-hot target over states for the observed transition
        target = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] += lrT * (target - T[a1, :])
        # Normalize rows to avoid drift from numerical issues
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr2 * delta2

        # Stage-1 MF TD update toward the reached state's max value
        td1 = V_states[s] - q1_mf[a1]
        q1_mf[a1] += lr2 * td1  # uses lr2 as a general learning rate

        # Update perseveration memory
        prev_a2_by_state[s] = a2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: UCB exploration at stage 2 and transition-dependent perseveration at stage 1.
    
    Summary
    -------
    - Stage-2 values are learned model-free, but action selection is biased by an
      uncertainty bonus (UCB) based on inverse square-root of choice counts.
    - Stage-1 planning is purely model-based using the fixed transition structure
      (A->X common, U->Y common). A transition-dependent perseveration applies:
      after common transitions, bias to repeat previous spaceship; after rare transitions,
      bias to switch. An additional baseline perseveration bias encourages repeating the
      previous stage-1 action regardless of transition type.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcome.
    model_parameters : iterable of floats
        [lr2, beta, ucb_gain, rho_td, stick1]
        - lr2 in [0,1]: learning rate for stage-2 Q-value updates.
        - beta in [0,10]: inverse temperature used for both stages.
        - ucb_gain in [0,1]: weight on the UCB bonus at stage 2; bonus = ucb_gain / sqrt(N+1).
        - rho_td in [0,1]: strength of transition-dependent perseveration at stage 1
                           (positive: stay after common, switch after rare).
        - stick1 in [0,1]: baseline perseveration for repeating previous stage-1 choice.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr2, beta, ucb_gain, rho_td, stick1 = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (agent knows the task)
    T_fixed = np.array([[0.7, 0.3], [0.3, 0.7]])

    q2 = np.zeros((2, 2))
    n_counts = np.zeros((2, 2))  # visit counts per (state, action) for UCB

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a1_tr = None  # previous trial's action for TD perseveration
    prev_s = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy: softmax over Q2 + UCB bonus
        bonus = np.zeros(2)
        # UCB bonus depends on current counts for this state's actions
        for a in range(2):
            bonus[a] = ucb_gain / np.sqrt(n_counts[s, a] + 1.0)

        logits2 = beta * (q2[s, :] + bonus)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB evaluation via fixed transitions
        V_states = np.max(q2, axis=1)
        q1_mb = T_fixed @ V_states

        # Transition-dependent perseveration bias
        bias_vec = np.zeros(2)
        if prev_a1 is not None and prev_s is not None:
            was_common = (prev_a1 == prev_s)  # common if A->X (0->0) or U->Y (1->1)
            # Apply bias toward staying after common, switching after rare
            if was_common:
                bias_vec[prev_a1] += rho_td
            else:
                bias_vec[1 - prev_a1] += rho_td

        # Baseline perseveration toward previous action
        if prev_a1 is not None:
            bias_vec[prev_a1] += stick1

        logits1 = beta * q1_mb + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Learning updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr2 * delta2
        n_counts[s, a2] += 1.0

        # Update previous-trial bookkeeping
        prev_a1 = a1
        prev_s = s

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Latent-state inference over transition structure with hazard rate; MF stage-2 learning.
    
    Summary
    -------
    The agent infers which of two latent environments is active on each trial:
      - Standard: A->X, U->Y are common (0.7).
      - Reversed: A->Y, U->X are common.
    Belief over the environments is updated online with a simple hazard-rate HMM filter:
    a hazard parameter controls spontaneous switches. Stage-1 planning uses the belief-
    weighted transition matrix. Stage-2 values are learned model-free. Perseveration
    biases are included at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcome.
    model_parameters : iterable of floats
        [lr2, beta, hazard, bias1, bias2]
        - lr2 in [0,1]: learning rate for stage-2 Q updates.
        - beta in [0,10]: inverse temperature shared across stages.
        - hazard in [0,1]: prior probability that the latent transition structure flips between trials.
        - bias1 in [0,1]: stage-1 perseveration toward the previous spaceship.
        - bias2 in [0,1]: stage-2 within-state perseveration toward the previous alien.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr2, beta, hazard, bias1, bias2 = model_parameters
    n_trials = len(action_1)

    # Define the two latent transition structures
    T_std = np.array([[0.7, 0.3], [0.3, 0.7]])
    T_rev = np.array([[0.3, 0.7], [0.7, 0.3]])

    # Stage-2 values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Belief that the environment is "standard"
    b_std = 0.5

    prev_a1 = None
    prev_a2_by_state = [None, None]

    # For filtering, we need to incorporate the likelihood of the previous transition
    # Initialize with no prior observation; update after first trial is observed.
    have_prev_obs = False
    prev_obs_a1 = None
    prev_obs_s = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Update latent-state belief using the previous trial's observed transition
        if have_prev_obs:
            # 1) Hazard mixing (prior prediction step)
            b_prior = (1.0 - hazard) * b_std + hazard * (1.0 - b_std)

            # 2) Likelihood of the observed transition under each latent structure
            # Previous transition: P(s_prev | a1_prev)
            if (prev_obs_a1 == 0 and prev_obs_s == 0) or (prev_obs_a1 == 1 and prev_obs_s == 1):
                p_std = 0.7
            else:
                p_std = 0.3
            p_rev = 1.0 - p_std

            # 3) Posterior update with normalization
            numer_std = b_prior * p_std
            numer_rev = (1.0 - b_prior) * p_rev
            denom = numer_std + numer_rev
            if denom > 0:
                b_std = numer_std / denom
            else:
                b_std = 0.5
        else:
            # No data yet; keep initial belief
            pass

        # Decision stage for current trial uses current belief
        T_expect = b_std * T_std + (1.0 - b_std) * T_rev

        # Stage-2 policy with within-state perseveration
        bias2_vec = np.zeros(2)
        prev_a2 = prev_a2_by_state[s]
        if prev_a2 is not None:
            bias2_vec[prev_a2] += bias2

        logits2 = beta * q2[s, :] + bias2_vec
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 model-based planning using belief-weighted transitions
        V_states = np.max(q2, axis=1)
        q1_mb = T_expect @ V_states

        # Add perseveration bias at stage 1
        bias1_vec = np.zeros(2)
        if prev_a1 is not None:
            bias1_vec[prev_a1] += bias1

        logits1 = beta * q1_mb + bias1_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Learning updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr2 * delta2

        # Bookkeeping for next iteration
        prev_a1 = a1
        prev_a2_by_state[s] = a2
        prev_obs_a1 = a1
        prev_obs_s = s
        have_prev_obs = True

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss