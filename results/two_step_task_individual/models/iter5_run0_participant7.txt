def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Hybrid arbitration via learned transitions with confidence-weighted mixing and first-stage perseveration.
    
    This model learns second-stage rewards (model-free) and learns the stage-1 transition matrix.
    First-stage action values are a confidence-weighted mixture of:
      - model-based values computed from the learned transition matrix and current second-stage values
      - model-free stage-1 values bootstrapped toward the chosen second-stage value (SARSA)
    The arbitration weight for each action depends on the certainty of its transition (distance from 0.5),
    modulated by a sensitivity parameter. A first-stage perseveration bias favors repeating the prior
    stage-1 action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (planet X=0, Y=1) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens W/S or P/H coded 0/1 within planet) for each trial.
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha2, beta, chi_T, zeta, psi]
        - alpha2: [0,1] learning rate for second-stage reward values and for stage-1 MF bootstrapping
        - beta:   [0,10] inverse temperature for both stages' softmax
        - chi_T:  [0,1] learning rate for the transition matrix rows (per chosen stage-1 action)
        - zeta:   [0,1] arbitration sensitivity; higher values sharpen confidence into MB weighting
        - psi:    [0,1] first-stage perseveration weight added to the logit of the previously chosen action
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices under the model.
    """
    alpha2, beta, chi_T, zeta, psi = model_parameters
    n_trials = len(action_1)

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Learned values
    q2 = np.zeros((2, 2))   # second-stage Q-values: q2[state, action]
    q1_mf = np.zeros(2)     # first-stage model-free Q-values

    # Learned transition matrix T[a, s] (rows sum to 1); initialize near unbiased
    T = np.full((2, 2), 0.5)

    last_a1 = -1  # for perseveration

    for t in range(n_trials):
        s = state[t]

        # Second-stage policy (softmax over Q2 at observed state)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Model-based component for stage 1 using learned transitions
        max_q2 = np.max(q2, axis=1)  # best available value in each state
        q1_mb = T @ max_q2           # MB projection per first-stage action

        # Confidence for each action's transition (0 when ~0.5, 1 when certain)
        conf = 2.0 * np.abs(T[:, 0] - 0.5)  # vector length 2 in [0,1]
        # Arbitration weight per action (sharpened by zeta)
        # Ensure zeta=0 gives uniform weak weighting; zeta=1 uses conf as-is
        w = conf ** (1e-8 + zeta)

        # Combine MB and MF into net stage-1 values
        q1_combined = w * q1_mb + (1.0 - w) * q1_mf

        # Add first-stage perseveration bias
        bias1 = np.zeros(2)
        if last_a1 in (0, 1):
            bias1[last_a1] += psi

        logits1 = beta * q1_combined + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Outcomes
        r = reward[t]

        # Learn transitions T for chosen first-stage action toward observed state
        # Row-wise update while preserving row sum = 1
        a = a1
        s_obs = s
        # Increase probability of observed state
        T[a, s_obs] += chi_T * (1.0 - T[a, s_obs])
        # Decrease probability of the other state
        other = 1 - s_obs
        T[a, other] += chi_T * (0.0 - T[a, other])

        # Second-stage MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # First-stage MF bootstrapping toward realized second-stage action value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * delta1

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Kalman-filtered reward learning with uncertainty bonus (UCB) and lapse, model-based at stage 1.
    
    This model treats each second-stage action's reward probability as a drifting process and uses a
    scalar Kalman filter per state-action to track mean and uncertainty. Action values at the second
    stage include an optimism bonus proportional to posterior standard deviation (UCB).
    First-stage values are model-based projections through the fixed transition structure (0.7/0.3).
    A lapse parameter mixes softmax with a uniform random policy at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within planet) for each trial.
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [beta, kappa, tau0, ucb, xi]
        - beta:  [0,10] inverse temperature for both stages' softmax
        - kappa: [0,1] process noise scale; higher means faster reward drift (more uncertainty accrues)
        - tau0:  [0,1] initial uncertainty scale for second-stage values
        - ucb:   [0,1] weight on uncertainty bonus added to Q at second stage (and propagated MB to stage 1)
        - xi:    [0,1] lapse probability; with prob xi, choices are uniform random at each stage
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    beta, kappa, tau0, ucb, xi = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Fixed transition matrix (common 0.7; rare 0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Kalman filter parameters and states per (state, action)
    # Initialize means to 0 and variances to small positive quantity scaled by tau0
    mu = np.zeros((2, 2))
    var = np.full((2, 2), 0.01 + 1.0 * tau0)  # initial uncertainty

    # Process and observation noise
    q_proc = 1e-4 + 0.5 * kappa   # process noise added each trial to variance
    r_obs = 1.0                   # observation noise (reward variability scale)

    for t in range(n_trials):
        s = state[t]

        # Form second-stage action values with UCB bonus
        std = np.sqrt(np.maximum(var[s, :], 1e-12))
        q2_aug = mu[s, :] + ucb * std

        # Second-stage policy with lapse
        logits2 = beta * q2_aug
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        soft2 = e2 / np.sum(e2)
        probs2 = (1.0 - xi) * soft2 + xi * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # First-stage MB projection (propagate augmented values)
        max_q2_aug = np.max(mu + ucb * np.sqrt(np.maximum(var, 1e-12)), axis=1)
        q1_mb = T @ max_q2_aug

        # First-stage policy with lapse
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        soft1 = e1 / np.sum(e1)
        probs1 = (1.0 - xi) * soft1 + xi * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Observe reward and update Kalman filter for the selected (s, a2)
        # Time update (increase uncertainty due to process noise)
        var[s, a2] = var[s, a2] + q_proc
        # Measurement update
        K = var[s, a2] / (var[s, a2] + r_obs)
        mu[s, a2] = mu[s, a2] + K * (reward[t] - mu[s, a2])
        var[s, a2] = (1.0 - K) * var[s, a2]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Transition-dependent credit assignment with eligibility traces (TD(Î»)) and two-stage temperatures.
    
    This is a purely model-free controller with eligibility traces for first-stage credit assignment.
    The strength of credit assignment to the first stage depends on whether the experienced transition
    was common or rare, via a transition-sensitivity parameter. Second-stage values learn from reward
    prediction error. Stage-1 and stage-2 have distinct inverse temperatures.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within planet, 0/1).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta1, beta2, lam_e, omega]
        - alpha:  [0,1] learning rate for value updates at both stages
        - beta1:  [0,10] inverse temperature for first-stage softmax
        - beta2:  [0,10] inverse temperature for second-stage softmax
        - lam_e:  [0,1] eligibility trace persistence for first-stage actions
        - omega:  [0,1] transition sensitivity; higher weights learning after common transitions,
                  while 1-omega weights learning after rare transitions
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta1, beta2, lam_e, omega = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize values
    q1 = np.zeros(2)        # stage-1 action values
    q2 = np.zeros((2, 2))   # stage-2 action values

    # Eligibility trace over stage-1 actions
    e1 = np.zeros(2)

    for t in range(n_trials):
        s = state[t]

        # Second-stage policy
        logits2 = beta2 * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # First-stage policy
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        e1_soft = np.exp(logits1)
        probs1 = e1_soft / np.sum(e1_soft)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        r = reward[t]

        # TD errors
        delta2 = r - q2[s, a2]
        # Update second-stage value
        q2[s, a2] += alpha * delta2

        # Update eligibility trace for first-stage actions
        e1 *= lam_e
        e1[a1] += 1.0

        # Determine if transition was common (A->X or U->Y) or rare
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        trans_weight = omega if is_common else (1.0 - omega)

        # First-stage TD error bootstrapping on realized second-stage value
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]

        # Transition-dependent credit assignment with eligibility
        q1 += alpha * trans_weight * e1 * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll