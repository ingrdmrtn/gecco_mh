def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid arbitration between model-based and model-free control via online uncertainty comparison.
    
    This model blends model-based (MB) and model-free (MF) action values at stage 1 with a dynamic
    arbitration weight that adapts to the relative uncertainty (entropy) of the two controllers.
    The second stage learns reward values with a simple delta rule. A perseveration bias at stage 1
    favors repeating the previous first-stage choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, etaC, nuU, psiS]
        - alpha (reward learning rate, [0,1]): learning rate for second-stage values and MF bootstrapping.
        - beta (inverse temperature, [0,10]): softmax temperature for both stages.
        - etaC (controller update rate, [0,1]): step size for updating the arbitration weight.
        - nuU (uncertainty sensitivity, [0,1]): scales how strongly entropy differences shift arbitration.
        - psiS (stage-1 perseveration, [0,1]): additive bias to repeat the last first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, etaC, nuU, psiS = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix: rows = spaceship (A,U), cols = planet (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value stores
    q2 = np.zeros((2, 2))     # second-stage MF values: state x action
    q1_mf = np.zeros(2)       # first-stage MF values

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Arbitration weight w in [0,1]; start neutral
    w = 0.5

    last_a1 = None

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based stage-1 values via transition expectation of max stage-2 value
        max_q2 = np.max(q2, axis=1)  # per state (planet)
        q1_mb = T @ max_q2           # plan using fixed transitions

        # Compute controller entropies (pre-choice) to adapt arbitration
        # MF policy at stage 1
        logits_mf = beta * q1_mf
        logits_mf -= np.max(logits_mf)
        pmf = np.exp(logits_mf)
        pmf /= np.sum(pmf)
        H_mf = -np.sum(pmf * (np.log(pmf + 1e-12)))

        # MB policy at stage 1
        logits_mb = beta * q1_mb
        logits_mb -= np.max(logits_mb)
        pmb = np.exp(logits_mb)
        pmb /= np.sum(pmb)
        H_mb = -np.sum(pmb * (np.log(pmb + 1e-12)))

        # Update arbitration weight toward the lower-entropy controller
        # Use logistic parameterization to keep w in (0,1)
        w_logit = np.log(w + 1e-12) - np.log(1.0 - w + 1e-12)
        w_logit = w_logit + etaC * nuU * (H_mf - H_mb)
        w = 1.0 / (1.0 + np.exp(-w_logit))

        # Blend MB and MF values for stage-1 decision
        q1_blend = w * q1_mb + (1.0 - w) * q1_mf

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += psiS

        # Stage-1 choice probability
        logits1 = beta * q1_blend + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probability (pure MF values)
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-1 MF bootstrapping toward current second-stage action value
        delta1 = q2[s2, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Stage-2 reward prediction error
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Successor-like transition learning with novelty bonus and stage-1 side bias.
    
    This model learns action-to-state transition probabilities online (akin to a one-step
    successor representation). It plans at stage 1 using the learned transitions and adds
    a novelty-driven exploration bonus to stage-2 values that decays with visitation.
    A baseline side bias at stage 1 favors one spaceship over the other.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, alphaSR, rhoB, b0]
        - alpha (reward learning rate, [0,1]): learning rate for second-stage reward values.
        - beta (inverse temperature, [0,10]): softmax for both stages.
        - alphaSR (transition learning rate, [0,1]): rate for updating P(planet | spaceship).
        - rhoB (novelty bonus scale, [0,1]): adds rhoB / sqrt(N+1) to second-stage Q to encourage exploration.
        - b0 (side bias, [0,1]): baseline bias toward spaceship A vs U; mapped to [-1,+1] and added to logits.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, alphaSR, rhoB, b0 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix; start near uniform/slightly structured
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Second-stage values and visitation counts (for novelty bonus)
    q2 = np.zeros((2, 2))
    N = np.zeros((2, 2))  # counts per state-action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-1 side bias: map b0 in [0,1] to [-1,+1], favoring A if positive
    side = (b0 - 0.5) * 2.0
    bias1 = np.array([side, -side], dtype=float)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Novelty bonus for stage-2 values
        bonus = rhoB / np.sqrt(N + 1.0)
        q2_bonus = q2 + bonus

        # Model-based planning with learned transitions
        max_q2 = np.max(q2_bonus, axis=1)
        q1 = T @ max_q2

        # Stage-1 choice probability with side bias
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probability with novelty bonus
        logits2 = beta * q2_bonus[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Transition learning: move chosen row toward observed state one-hot
        oh = np.array([0.0, 0.0])
        oh[s2] = 1.0
        T[a1] = (1.0 - alphaSR) * T[a1] + alphaSR * oh
        # Normalize to guard against drift
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        # Second-stage learning and count update
        N[s2, a2] += 1.0
        delta = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Kalman reward learner with concave utility and stage-2 choice kernel.
    
    Rewards at the second stage are learned with a simple scalar Kalman filter for each
    state-action, allowing adaptation to drifting reward probabilities. Decision values
    apply a concave power utility to the mean reward estimates. A state-specific choice
    kernel at stage 2 captures short-term perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [beta, nu, sigma, gammaU, kappaK]
        - beta (inverse temperature, [0,10]): softmax for both stages.
        - nu (process noise, [0,1]): per-trial diffusion added to the reward variance.
        - sigma (observation noise, [0,1]): measurement noise for reward observations.
        - gammaU (utility concavity, [0,1]): exponent on reward mean; <1 yields risk aversion.
        - kappaK (choice-kernel gain, [0,1]): strength and persistence of stage-2 choice bias.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    beta, nu, sigma, gammaU, kappaK = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Kalman filter parameters per state-action: mean m and variance v
    m = np.full((2, 2), 0.5, dtype=float)
    v = np.full((2, 2), 0.25, dtype=float)  # initial uncertainty

    # Stage-2 choice kernel per state
    k2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Precompute observation noise (ensure small positive)
    sig = max(sigma, 1e-6)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Utility-transformed values for decision
        u = np.power(np.clip(m, 1e-6, 1.0), gammaU)

        # Stage-1 values via model-based planning using expected best utility per state
        max_u = np.max(u, axis=1)
        q1 = T @ max_u

        # Stage-1 choice probability
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probability with choice kernel bias
        logits2 = beta * u[s2] + k2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Kalman filter update for the visited state-action
        # Predict step: increase variance by process noise
        v[s2, a2] = v[s2, a2] + nu

        # Kalman gain
        K = v[s2, a2] / (v[s2, a2] + sig)

        # Update mean and variance with observed reward
        m[s2, a2] = m[s2, a2] + K * (r - m[s2, a2])
        v[s2, a2] = (1.0 - K) * v[s2, a2]

        # Update stage-2 choice kernel: decay others and reinforce chosen action
        k2[s2] *= (1.0 - kappaK)
        k2[s2, a2] += kappaK

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll