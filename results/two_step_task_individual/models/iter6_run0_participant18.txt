def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Pure model-based planning with outcome sensitivity, forgetting of unchosen second-stage values, and transition-contingent repetition bias.
    
    This model selects the first-stage action using model-based (MB) values computed
    from the fixed transition matrix and the current estimates of second-stage action values.
    Second-stage values are learned via temporal-difference learning with (i) an outcome
    sensitivity scaling of rewards and (ii) forgetting applied to all unchosen second-stage
    actions. A transition-contingent bias at the first stage encourages repeating the
    previous first-stage choice after a common transition and discourages it after a rare transition.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A (commonly to X), 1 = spaceship U (commonly to Y).
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 correspond to the two aliens available on the reached planet.
    reward : array-like of float (e.g., 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha, beta, phi, xi, chi]
        - alpha (learning rate, [0,1]): Learning rate for updating second-stage Q-values.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - phi (outcome sensitivity, [0,1]): Scales the magnitude of reward in TD errors (effective reward = phi * reward).
        - xi (forgetting rate, [0,1]): Per-trial proportional decay applied to all unchosen second-stage Q-values.
        - chi (transition-contingent repetition bias, [0,1]): Additive bias to repeat last first-stage action
               after a common transition; subtract the same bias after a rare transition.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, phi, xi, chi = model_parameters
    n_trials = len(action_1)

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],   # A
                                  [0.3, 0.7]])  # U

    # Prob tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage Q-values for each planet and its two aliens
    q2 = np.zeros((2, 2))

    # Track last first-stage choice and whether it yielded a common transition
    prev_a1 = None
    prev_common = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # -------- First-stage policy: Model-based + transition-contingent bias --------
        max_q2 = np.max(q2, axis=1)                   # value of each planet
        q1_mb = transition_matrix @ max_q2            # expected value of A/U

        # Transition-contingent repetition bias
        bias = np.zeros(2)
        if prev_a1 is not None and prev_common is not None:
            # If repeating previous action after common -> +chi; after rare -> -chi
            if a1 == prev_a1:
                bias_val = chi if prev_common else -chi
            else:
                bias_val = 0.0
            # Bias acts on current trial's logits. For probability evaluation,
            # we include it for both actions in softmax, not only the chosen one.
            # Only the dimension of previous action receives the bias.
            bias[prev_a1] += chi if prev_common else -chi
            # To ensure the above uses "prev_common" consistently, we also apply zero to other action.

        # Softmax over MB values plus bias
        logits1 = q1_mb + bias
        logits1 = logits1 - np.max(logits1)          # stabilize
        exp_q1 = np.exp(beta * logits1)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # -------- Second-stage policy: MF Q-learning with forgetting --------
        logits2 = q2[s].copy()
        logits2 = logits2 - np.max(logits2)
        exp_q2 = np.exp(beta * logits2)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # -------- Learning updates --------
        # Apply forgetting to all unchosen second-stage state-action values
        # For both planets and both aliens: decay unchosen by (1 - xi*alpha)
        if xi > 0:
            decay = 1.0 - xi * alpha
            decay = max(0.0, min(1.0, decay))
            # Decay all
            q2 *= decay
            # Restore the chosen state-action to its pre-decay value before TD update
            # by dividing back its decay (since we decayed all)
            if decay > 0:
                q2[s, a2] /= decay

        # TD update for chosen second-stage action with outcome sensitivity
        delta2 = (phi * r) - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Record last transition's commonality for next trial bias
        # A->X (s=0) is common; U->Y (s=1) is common.
        prev_a1 = a1
        prev_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Surprise-gated MB/MF arbitration with recency choice kernels.
    
    This model arbitrates between model-based (MB) and model-free (MF) values at the first stage
    using a mixing weight that adapts to recent transition surprise (rare vs. common).
    The mixing weight m_t is updated each trial toward 1 after rare transitions and toward 0 after common transitions,
    controlled by a surprise-gating rate. A recency-based choice kernel (choice trace) at both stages
    biases repeating recently chosen actions within each stage/state. Second-stage values are learned via MF TD learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A (commonly to X), 1 = spaceship U (commonly to Y).
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 correspond to the two aliens available on the reached planet.
    reward : array-like of float (e.g., 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha, beta, kappa, theta, nu]
        - alpha (learning rate, [0,1]): Learning rate for MF updates (stage 2) and for bootstrapped MF at stage 1.
        - beta (inverse temperature, [0,10]): Softmax sensitivity applied at both stages.
        - kappa (choice-kernel learning rate, [0,1]): How quickly choice traces update toward the last chosen action.
        - theta (choice-kernel weight, [0,1]): Additive weight on choice-kernel signals in softmax logits at both stages.
        - nu (surprise-gating rate, [0,1]): Rate controlling how fast the MB/MF mixing weight m_t moves toward 1 after rare
               and toward 0 after common transitions. m_t initializes at 0.5.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, kappa, theta, nu = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)     # stage-1 MF
    q2 = np.zeros((2, 2))   # stage-2 MF

    # Choice kernels (recency traces)
    ck1 = np.zeros(2)       # stage-1 kernel over A/U
    ck2 = np.zeros((2, 2))  # state-specific kernels over the two aliens

    # Surprise-gated MB/MF mixing weight
    m = 0.5  # starts neutral between MB and MF

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB action values at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Mixed first-stage values with choice kernel bias
        q1_mix = m * q1_mb + (1 - m) * q1_mf
        logits1 = q1_mix + theta * ck1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with choice kernel bias
        logits2 = q2[s].copy() + theta * ck2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # -------- Learning updates --------
        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF bootstrapping toward realized second-stage value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update choice kernels (soft memory of recent choices)
        ck1 = (1 - kappa) * ck1
        ck1[a1] += kappa
        ck2[s] = (1 - kappa) * ck2[s]
        ck2[s, a2] += kappa

        # Update mixing weight m_t based on transition surprise (rare=1, common=0)
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        surprise = 0.0 if is_common else 1.0
        # Move m toward 1 if rare, toward 0 if common
        m = (1 - nu) * m + nu * surprise
        # Keep in [0,1]
        m = max(0.0, min(1.0, m))

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Value-difference-gated arbitration, dual learning rates, and lapse.
    
    This hybrid model computes first-stage values as a dynamic mixture of model-based (MB)
    and model-free (MF) values. The arbitration weight w_t is a deterministic function of
    the instantaneous difference in the best available values across planets:
    w_t = min(1, delta * |maxQ(X) - maxQ(Y)|). Thus, when the two planets differ more in
    value, planning (MB) gains more influence. The model uses separate learning rates for
    first-stage MF and second-stage MF updates and includes a lapse parameter that mixes
    softmax choice with uniform randomness at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A (commonly to X), 1 = spaceship U (commonly to Y).
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices: 0/1 correspond to the two aliens available on the reached planet.
    reward : array-like of float (e.g., 0 or 1)
        Outcome received at the end of the trial.
    model_parameters : sequence
        [alpha1, alpha2, beta, delta, epsilon]
        - alpha1 (stage-1 MF learning rate, [0,1]): Learning rate for bootstrapping MF values at stage 1.
        - alpha2 (stage-2 MF learning rate, [0,1]): Learning rate for updating second-stage Q-values.
        - beta (inverse temperature, [0,10]): Softmax sensitivity (before lapse) for both stages.
        - delta (MB arbitration gain, [0,1]): Scales how strongly planet value differences increase MB control.
        - epsilon (lapse rate, [0,1]): Probability of making a random choice (uniform) at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha1, alpha2, beta, delta, epsilon = model_parameters
    n_trials = len(action_1)

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # MB values for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Arbitration weight based on planet value difference
        dv = abs(max_q2[0] - max_q2[1])           # in [0, max reward]
        w = delta * dv
        w = max(0.0, min(1.0, w))

        # Stage-1 policy (with lapse)
        q1_mix = w * q1_mb + (1 - w) * q1_mf
        logits1 = q1_mix - np.max(q1_mix)
        probs1_soft = np.exp(beta * logits1)
        probs1_soft = probs1_soft / np.sum(probs1_soft)
        probs1 = (1 - epsilon) * probs1_soft + epsilon * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (with lapse)
        logits2 = q2[s].copy() - np.max(q2[s])
        probs2_soft = np.exp(beta * logits2)
        probs2_soft = probs2_soft / np.sum(probs2_soft)
        probs2 = (1 - epsilon) * probs2_soft + epsilon * 0.5
        p_choice_2[t] = probs2[a2]

        # -------- Learning updates --------
        # Second-stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # First-stage MF bootstrapping toward realized second-stage value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha1 * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll