def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-free/model-based agent with eligibility-weighted stage-1 learning
    and utility curvature on rewards.

    The agent learns second-stage Q-values model-free and uses a weighted blend of
    model-free and model-based values to choose at stage 1. The stage-1 model-free
    values are updated via an eligibility-weighted backup from the chosen second-stage
    action value. Rewards are transformed by a concavity/convexity parameter to allow
    risk/utility curvature.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta, chi, gamma, kappa_u]
        - alpha in [0,1]: learning rate for value updates.
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - chi in [0,1]: weight of model-based planning in stage-1 decision (0=MF only, 1=MB only).
        - gamma in [0,1]: eligibility/backup weight scaling the stage-1 model-free update.
        - kappa_u in [0,1]: utility curvature on rewards; effective utility is reward**kappa_u
                             (concave if kappa_u<1, linear at 1).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, chi, gamma, kappa_u = model_parameters
    n_trials = len(action_1)

    # Fixed, known transition structure: A->X, U->Y are common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]], dtype=float)

    # Action selection likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2, dtype=float)  # model-free values for spaceships
    q_stage2_mf = np.zeros((2, 2), dtype=float) + 0.5  # values for aliens on each planet

    eps = 1e-12

    for t in range(n_trials):
        # Model-based evaluation for stage 1 from current second-stage values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # best alien per planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid stage-1 value
        q1 = (1.0 - chi) * q_stage1_mf + chi * q_stage1_mb

        # Stage-1 policy
        l1 = beta * (q1 - np.max(q1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (on reached planet)
        s = state[t]
        q2_s = q_stage2_mf[s]
        l2 = beta * (q2_s - np.max(q2_s))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Outcome with utility curvature
        r = reward[t]
        util = (r ** kappa_u)

        # Stage-2 update (model-free TD)
        pe2 = util - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * pe2

        # Stage-1 model-free update with eligibility/backup from chosen stage-2 value
        backed_up = q_stage2_mf[s, a2]
        pe1 = backed_up - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * gamma * pe1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with learned transitions, value forgetting, and WSLS action bias.

    The agent learns the transition matrix from experience and plans at stage 1 using the
    learned transitions and current second-stage values. Second-stage values drift toward
    an uninformative prior (0.5) via controlled forgetting. In addition, both stages exhibit
    a Win-Stay/Lose-Switch (WSLS) choice bias implemented as a decaying kernel added to logits.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [lr, beta, theta_T, mu_forget, epsilon_wsls]
        - lr in [0,1]: learning rate for second-stage Q-value updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - theta_T in [0,1]: learning rate for updating the transition matrix rows toward the observed state.
        - mu_forget in [0,1]: forgetting rate driving Q2 toward 0.5 each trial; also controls bias-kernel decay.
        - epsilon_wsls in [0,1]: strength of WSLS bias added to logits (positive favors stay after win, switch after loss).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    lr, beta, theta_T, mu_forget, epsilon_wsls = model_parameters
    n_trials = len(action_1)

    # Start with agnostic transition beliefs
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Second-stage values and WSLS kernels
    q2 = np.zeros((2, 2), dtype=float) + 0.5
    K1 = np.zeros(2, dtype=float)
    K2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    # Track last actions/rewards for WSLS updates
    last_a1 = None
    last_a2 = None
    last_s = None
    last_r = None

    for t in range(n_trials):
        # Model-based stage-1 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add WSLS bias at stage 1
        logits1 = q1_mb + epsilon_wsls * K1
        l1 = beta * (logits1 - np.max(logits1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage 2 policy with WSLS bias on reached planet
        s = state[t]
        logits2 = q2[s] + epsilon_wsls * K2[s]
        l2 = beta * (logits2 - np.max(logits2))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Outcome
        r = reward[t]

        # Update Q2 with learning and forgetting toward 0.5
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2
        q2 = (1.0 - mu_forget) * q2 + mu_forget * 0.5

        # Learn transitions row for chosen action toward observed state
        T[a1, s] += theta_T * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] += theta_T * (0.0 - T[a1, other])
        # Normalize rows
        row0_sum = T[0, 0] + T[0, 1] + eps
        row1_sum = T[1, 0] + T[1, 1] + eps
        T[0, 0], T[0, 1] = T[0, 0] / row0_sum, T[0, 1] / row0_sum
        T[1, 0], T[1, 1] = T[1, 0] / row1_sum, T[1, 1] / row1_sum

        # Update WSLS kernels with decay controlled by mu_forget
        K1 *= (1.0 - mu_forget)
        K2 *= (1.0 - mu_forget)

        # Apply WSLS rule based on last outcome (if available)
        if last_a1 is not None and last_r is not None:
            if last_r > 0.0:
                # Win: favor staying with last action
                K1[last_a1] += 1.0
            else:
                # Loss: discourage staying (equivalently favor switching)
                K1[last_a1] -= 1.0

        if last_a2 is not None and last_s is not None and last_r is not None:
            if last_r > 0.0:
                K2[last_s, last_a2] += 1.0
            else:
                K2[last_s, last_a2] -= 1.0

        # Store current as last for the next trial
        last_a1 = a1
        last_a2 = a2
        last_s = s
        last_r = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-driven exploration with bonus and perseveration at stage 1.

    The agent maintains both value and an uncertainty estimate for each second-stage
    action. Uncertainty is updated from absolute prediction errors and drives
    directed exploration by adding an uncertainty bonus to action values. Planning
    at stage 1 uses model-based values computed from the bonus-augmented second-stage
    values. A perseveration bias at stage 1 captures tendency to repeat the previous
    first-stage choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float (e.g., 0 or 1)
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta, kappa_unc, tau_u, rho_stay]
        - alpha in [0,1]: learning rate for second-stage value updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa_unc in [0,1]: weight on uncertainty bonus added to second-stage values.
        - tau_u in [0,1]: update rate for uncertainty estimates (and decay for the stay kernel).
        - rho_stay in [0,1]: strength of first-stage perseveration kernel added to logits.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, kappa_unc, tau_u, rho_stay = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common transitions)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Initialize values and uncertainty
    q2 = np.zeros((2, 2), dtype=float) + 0.5
    u2 = np.zeros((2, 2), dtype=float) + 0.5  # initial uncertainty

    # Perseveration kernel for stage 1
    K1 = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Compute bonus-augmented second-stage values
        bonus2 = kappa_unc * u2
        q2_aug = q2 + bonus2

        # Model-based planning for stage 1 uses augmented values
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_q2_aug

        # Stage-1 policy with perseveration
        logits1 = q1_mb + rho_stay * K1
        l1 = beta * (logits1 - np.max(logits1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy on reached planet with bonus
        s = state[t]
        logits2 = q2_aug[s]
        l2 = beta * (logits2 - np.max(logits2))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Outcome and updates
        r = reward[t]

        # Value update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Uncertainty update from absolute prediction error
        u2[s, a2] = (1.0 - tau_u) * u2[s, a2] + tau_u * np.abs(pe2)

        # Update perseveration kernel (decay then add 1 for chosen action)
        K1 *= (1.0 - tau_u)
        K1[a1] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll