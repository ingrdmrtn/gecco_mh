def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-gated model-based control with learned transitions and surprise-driven bonuses.

    This model learns second-stage values and first-stage transition probabilities online.
    First-stage decisions blend model-free and model-based values, where the model-based
    weight is dynamically gated by the agent's confidence (1 - entropy) in its learned
    transition model. Additionally, a Bayesian-surprise-like exploration bonus is added
    to first-stage action values in proportion to transition uncertainty (entropy).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha_q, beta, eta_trans, psi_surprise, omega_conf]
        - alpha_q (0..1): Learning rate for second-stage Q-values.
        - beta (0..10): Inverse temperature for both stages.
        - eta_trans (0..1): Learning rate for first-stage transition probabilities.
        - psi_surprise (0..1): Weight of uncertainty (entropy) bonus added to first-stage action values.
        - omega_conf (0..1): Base strength of using model-based values, scaled by transition confidence.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_q, beta, eta_trans, psi_surprise, omega_conf = model_parameters

    n_trials = len(action_1)
    eps = 1e-12

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize learned transition matrix T[a, s]; start uninformative at 0.5
    T = np.full((2, 2), 0.5)
    # Second-stage Q-values
    Q2 = np.zeros((2, 2))
    # First-stage model-free values (for completeness; learned from Q2 targets)
    Q1_mf = np.zeros(2)

    # Helper to compute normalized entropy H in [0,1]
    def norm_entropy(p_row):
        p1 = np.clip(p_row[0], eps, 1 - eps)
        p2 = np.clip(p_row[1], eps, 1 - eps)
        H = -(p1 * np.log(p1) + p2 * np.log(p2))
        return H / np.log(2.0)  # max entropy for 2 outcomes is log 2

    for t in range(n_trials):
        # Compute current model-based action values at stage 1
        maxQ2 = np.max(Q2, axis=1)  # shape (2,)
        Q1_mb = T @ maxQ2  # expected max value per action via learned transitions

        # Uncertainty bonuses via entropy of learned transitions
        H0 = norm_entropy(T[0])
        H1 = norm_entropy(T[1])
        bonus = psi_surprise * np.array([H0, H1])

        # Confidence for gating MB weight: (1 - entropy), per action
        conf = 1.0 - np.array([H0, H1])
        # Action-specific mixing of MF and MB
        w_mb_action = omega_conf * conf
        Q1 = (1.0 - w_mb_action) * Q1_mf + w_mb_action * Q1_mb + bonus

        # First-stage policy
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (pure softmax on Q2 of reached state)
        s = state[t]
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage Q-values
        td2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_q * td2

        # Model-free first-stage bootstrapping from experienced branch
        target1 = Q2[s, a2]
        td1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_q * td1

        # Update learned transitions for the chosen first-stage action
        # Move T[a1] toward the observed one-hot state with eta_trans
        onehot_s = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1] = (1.0 - eta_trans) * T[a1] + eta_trans * onehot_s

        # Keep each row normalized (should already be, but ensure numerical stability)
        T[a1] /= (np.sum(T[a1]) + eps)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Successor-representation control with model-based correction, reward-rate adaptation, and perseveration.

    The first-stage values are computed from a learned one-step successor representation (SR) that maps
    first-stage actions to expected occupancy of second-stage states. The SR is updated from observed
    transitions. Second-stage values are learned via TD on reward minus a running reward-rate baseline.
    A model-based correction term adjusts the SR-predicted values toward those implied by the canonical
    transition structure (0.7 common), and a perseveration bias favors repeating the previous first-stage action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha_sr, beta, zeta_corr, rho_rate, kappa_stay]
        - alpha_sr (0..1): Learning rate for SR and second-stage Q-values.
        - beta (0..10): Inverse temperature for both stages.
        - zeta_corr (0..1): Weight of model-based correction using canonical transitions.
        - rho_rate (0..1): Learning rate of average reward baseline.
        - kappa_stay (0..1): Strength of perseveration bias to repeat the previous first-stage choice.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_sr, beta, zeta_corr, rho_rate, kappa_stay = model_parameters

    n_trials = len(action_1)
    eps = 1e-12

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # One-step SR: M[a, s] approximates P(s | a)
    M = np.full((2, 2), 0.5)  # start uninformative
    # Second-stage Q-values
    Q2 = np.zeros((2, 2))
    # Reward-rate baseline
    r_bar = 0.0
    # Perseveration logit bias
    prev_a1 = None

    # Canonical transition matrix (A->X common, U->Y common)
    T_canon = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    for t in range(n_trials):
        maxQ2 = np.max(Q2, axis=1)  # shape (2,)

        # SR value for first-stage actions
        Q1_sr = M @ maxQ2

        # Model-based correction nudges SR values toward canonical planning values
        Q1_mb_canon = T_canon @ maxQ2
        Q1 = Q1_sr + zeta_corr * (Q1_mb_canon - Q1_sr)

        # Add perseveration bias on logits
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += kappa_stay

        logits1 = beta * (Q1 - np.max(Q1)) + bias
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]
        # Reward-rate adjusted utility
        u = r - r_bar
        r_bar += rho_rate * (r - r_bar)

        # Second-stage TD update
        td2 = u - Q2[s, a2]
        Q2[s, a2] += alpha_sr * td2

        # Update SR entry for the chosen action toward observed state one-hot
        onehot_s = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        M[a1] = (1.0 - alpha_sr) * M[a1] + alpha_sr * onehot_s
        M[a1] /= (np.sum(M[a1]) + eps)

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Surprise-weighted eligibility propagation with counterfactual updates and value forgetting.

    This is a primarily model-free learner with three key mechanisms:
      1) Surprise-weighted eligibility: First-stage values receive credit from second-stage outcomes
         scaled by the surprise of the observed transition (rare > common).
      2) Counterfactual inference: The unchosen second-stage action in the reached state is updated
         toward the counter-outcome (1 - reward), capturing a tendency to infer missed opportunity.
      3) Value forgetting: All Q-values decay toward zero each trial, modeling volatility or memory limits.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha, beta, kappa_elig, zeta_cf, tau_forget]
        - alpha (0..1): Base learning rate for Q updates.
        - beta (0..10): Inverse temperature for both stages.
        - kappa_elig (0..1): Strength of propagating second-stage value to first stage (eligibility).
        - zeta_cf (0..1): Weight of counterfactual update for the unchosen action at stage 2.
        - tau_forget (0..1): Per-trial forgetting factor; larger values imply stronger decay toward 0.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, kappa_elig, zeta_cf, tau_forget = model_parameters

    n_trials = len(action_1)
    eps = 1e-12

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Fixed canonical transitions for computing surprise
    # Common if (a1==0 and s==0) or (a1==1 and s==1)
    p_common = 0.7

    for t in range(n_trials):
        # Policies
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Apply forgetting to all values before updates
        Q1 *= (1.0 - tau_forget)
        Q2 *= (1.0 - tau_forget)

        # Stage-2 TD update
        td2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * td2

        # Counterfactual update of the unchosen second-stage action in the reached state
        a2_cf = 1 - a2
        cf_target = 1.0 - r  # infer missed outcome is opposite
        Q2[s, a2_cf] += zeta_cf * alpha * (cf_target - Q2[s, a2_cf])

        # Surprise of transition for eligibility propagation
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        trans_prob = p_common if is_common else (1.0 - p_common)
        surprise = 1.0 - trans_prob  # 0.3 for common, 0.7 for rare

        # Propagate second-stage signal back to first-stage chosen action
        backprop_target = Q2[s, a2]
        Q1[a1] += kappa_elig * alpha * (backprop_target - Q1[a1]) * (surprise + eps)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll