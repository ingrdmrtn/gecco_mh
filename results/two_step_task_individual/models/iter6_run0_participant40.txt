def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Surprise-tempered model-free controller with eligibility trace.
    
    Idea:
    - A purely model-free agent learns second-stage values Q2[s, a2] from reward.
    - First-stage values Q1 are updated via an eligibility trace from the second-stage TD error.
    - Policy temperature adapts to transition surprise: after rare transitions, the first-stage
      softmax becomes sharper by a multiplicative factor determined by z_surprise (captures
      transient control tightening after unexpected outcomes).
    
    Parameters (bounds):
    - eta_q (0-1): Learning rate for second-stage TD updates (Q2) and for propagating TD error to Q1 via eligibility.
    - beta (0-10): Base inverse temperature for softmax at both stages.
    - lam (0-1): Eligibility strength scaling how much second-stage TD error updates Q1.
    - z_surprise (0-1): Surprise gain; on rare transitions the stage-1 temperature becomes beta*(1+z_surprise).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet (for X: 0=W,1=S; for Y: 0=P,1=H).
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_q, beta, lam, z_surprise]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_q, beta, lam, z_surprise = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Values
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Surprise depends on whether the observed state matches the commonly associated state.
        # Common: A->X (0->0), U->Y (1->1)
        is_common = (a1 == s)
        beta1_t = beta * (1.0 + (0.0 if is_common else z_surprise))

        # Stage 1 policy
        logits1 = beta1_t * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (uses base beta)
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning
        delta2 = r - Q2[s, a2]
        # Update second-stage value
        Q2[s, a2] += eta_q * delta2
        # Eligibility trace update to stage-1 chosen action
        Q1[a1] += lam * eta_q * delta2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Hybrid arbitration based on learned transition confidence.
    
    Idea:
    - The agent learns transition probabilities P(s|a1) via a simple delta-rule (lr_T).
    - It forms model-based first-stage values by combining the learned transition with second-stage values.
    - It also maintains a model-free first-stage value updated via second-stage TD error.
    - Arbitration between model-based and model-free control depends on the confidence in the transition model:
      higher confidence (probabilities far from 0.5) increases the MB weight via a sigmoidal mapping.
    
    Parameters (bounds):
    - lr_q (0-1): Learning rate for second-stage TD (Q2) and MF propagation to Q1.
    - beta (0-10): Inverse temperature for softmax at both stages.
    - lr_T (0-1): Learning rate for updating transition probabilities P(s|a1).
    - conf0 (0-1): Reference confidence offset; higher conf0 shifts arbitration toward MF unless confidence exceeds this offset.
    - omega_mix (0-1): Gain scaling how strongly confidence modulates MB weight (sigmoid slope).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [lr_q, beta, lr_T, conf0, omega_mix]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    lr_q, beta, lr_T, conf0, omega_mix = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Initialize values
    Q1_mf = np.zeros(2)              # model-free first-stage values
    Q2 = np.zeros((2, 2))            # second-stage values

    # Initialize transition model: rows are actions {A,U}, columns are states {X,Y}
    # Start unbiased at 0.5/0.5
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute model-based Q1 using current transition beliefs
        max_Q2 = np.max(Q2, axis=1)  # for each state, best second-stage value
        Q1_mb = T @ max_Q2           # expected value per first-stage action

        # Confidence: how far each action's transition prob is from 0.5 (averaged across actions)
        # For each a1: conf_a = 2*|P(X|a1) - 0.5| (equivalently using either state column)
        conf_a0 = 2.0 * abs(T[0, 0] - 0.5)
        conf_a1 = 2.0 * abs(T[1, 1] - 0.5)
        conf_mean = 0.5 * (conf_a0 + conf_a1)  # in [0,1]

        # Arbitration weight via sigmoid of confidence relative to conf0
        x = (conf_mean - conf0)
        # constrain omega_mix in [0,1] -> effective slope scaled to reasonable range
        slope = 10.0 * omega_mix
        w_mb = 1.0 / (1.0 + np.exp(-slope * x))

        # Hybrid first-stage values
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage 1 policy
        logits1 = beta * Q1_hyb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning: update Q2
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += lr_q * delta2

        # Model-free Q1 update via propagated TD error
        Q1_mf[a1] += lr_q * delta2

        # Update transition model row for chosen first-stage action toward observed state
        # Target vector is one-hot on observed state.
        target = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        T[a1] += lr_T * (target - T[a1])

        # Keep rows normalized (they remain normalized under this update, but ensure numerical stability)
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free valuation with loss aversion, value/kernal forgetting, and choice stickiness at both stages.
    
    Idea:
    - Rewards are transformed into utilities with loss aversion: negative outcomes loom larger by rho_loss.
    - Second-stage values Q2 are learned from utility via TD learning.
    - First-stage values Q1 receive TD credit from second stage (MF propagation).
    - Choice stickiness kernels (K1 for stage 1, K2 for stage 2) bias softmax policies toward recent choices,
      with separate contributions at each stage, and both values and kernels undergo per-trial forgetting.
    
    Parameters (bounds):
    - eta_q (0-1): Learning rate for TD updates of Q2 and propagation to Q1.
    - beta (0-10): Inverse temperature for softmax.
    - rho_loss (0-1): Loss aversion factor; utility u = r for r>=0, u = -rho_loss*|r| for r<0.
    - xi_stick (0-1): Strength of stickiness kernels added to logits at both stages.
    - nu_forget (0-1): Per-trial forgetting of values and kernels (multiplicative decay each trial).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_q, beta, rho_loss, xi_stick, nu_forget]
    
    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_q, beta, rho_loss, xi_stick, nu_forget = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Values
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Stickiness kernels
    K1 = np.zeros(2)      # stage-1 choice kernel
    K2 = np.zeros((2, 2)) # stage-2 state-conditional choice kernel

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Decay values and kernels (forgetting)
        Q1 *= (1.0 - nu_forget)
        Q2 *= (1.0 - nu_forget)
        K1 *= (1.0 - nu_forget)
        K2 *= (1.0 - nu_forget)

        # Stage 1 policy with stickiness
        logits1 = beta * Q1 + xi_stick * K1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with stickiness (state-conditional kernel)
        logits2 = beta * Q2[s] + xi_stick * K2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Update stickiness kernels after observing choices
        K1[a1] += 1.0
        K2[s, a2] += 1.0

        # Utility with loss aversion
        u = r if r >= 0 else -rho_loss * abs(r)

        # TD learning at second stage
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += eta_q * delta2

        # Propagate MF credit to first-stage chosen action
        Q1[a1] += eta_q * delta2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss