def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Transition-uncertainty bonus (UCB-style) with learned transitions and separate stage temperatures.

    Summary
    -------
    - Learns second-stage action values (Q2) via model-free TD learning.
    - Learns the stage-1 action-to-state transition matrix T via exponential averaging.
    - At stage 1, plans using the learned T to compute expected values of max Q2 per state
      and adds an uncertainty-driven exploration bonus proportional to transition uncertainty
      for each spaceship (higher when T is closer to 0.5).
    - Uses separate softmax temperatures for stage 1 and stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=Spaceship A, 1=Spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=Planet X, 1=Planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 for the two aliens on that planet).
    reward : array-like of float (0 or 1)
        Reward outcome on each trial.
    model_parameters : iterable of floats
        [lr2, beta1, beta2, lrT, ucb_gain]
        - lr2 in [0,1]: learning rate for second-stage Q-values.
        - beta1 in [0,10]: inverse temperature for stage-1 choices.
        - beta2 in [0,10]: inverse temperature for stage-2 choices.
        - lrT in [0,1]: learning rate for updating the transition matrix T.
        - ucb_gain in [0,1]: scale of transition-uncertainty exploration bonus added to stage-1 values.

    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of stage-1 and stage-2 choices.
    """
    lr2, beta1, beta2, lrT, ucb_gain = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix T[a, s]; start from weak prior (uniform)
    T = np.full((2, 2), 0.5)
    # Initialize second-stage Q-values: Q2[state, action]
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy
        logits2 = beta2 * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 planning value using learned transitions
        V_states = np.max(q2, axis=1)  # value of each planet as max over its aliens
        q1_mb = T @ V_states  # expected value for each spaceship

        # Transition uncertainty bonus: for binary transition Bernoulli variance p(1-p)
        # Uncertainty highest when T[a,1] ~ 0.5 (or equivalently either column)
        p_to_Y = T[:, 1]
        unc = p_to_Y * (1.0 - p_to_Y)  # shape (2,)
        q1_with_bonus = q1_mb + ucb_gain * unc

        logits1 = beta1 * q1_with_bonus
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Transition learning: move T[a1,:] toward the observed state s (one-hot)
        onehot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] += lrT * (onehot_s - T[a1, :])

        # Stage-2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr2 * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Dynamic arbitration between model-based and model-free control via transition PE.

    Summary
    -------
    - Learns second-stage Q-values (Q2) via model-free TD learning.
    - Learns the transition matrix T via exponential averaging.
    - Maintains a model-free stage-1 value Q1_MF updated by SARSA(0) backup from the observed second-stage choice.
    - Computes model-based stage-1 values Q1_MB by planning with learned T over max(Q2) per state.
    - Arbitration weight w_t is dynamic: it blends a baseline weight with a term that increases when
      the current transition prediction error is small (i.e., transitions are as expected).
      This yields more model-based control when the transition model is reliable.
    - A single temperature beta is used for both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Rewards per trial.
    model_parameters : iterable of floats
        [lr2, beta, lrT, w_base, gate]
        - lr2 in [0,1]: learning rate for Q2 and for Q1_MF SARSA backup.
        - beta in [0,10]: inverse temperature shared across stages.
        - lrT in [0,1]: learning rate for the transition matrix T.
        - w_base in [0,1]: baseline weight on model-based values.
        - gate in [0,1]: strength of dynamic gating by transition reliability.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr2, beta, lrT, w_base, gate = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model and value functions
    T = np.full((2, 2), 0.5)      # transition probabilities
    q2 = np.zeros((2, 2))         # stage-2 Q-values
    q1_mf = np.zeros(2)           # stage-1 model-free Q-values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy (shared beta)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Model-based Q1 via learned transitions over max Q2
        V_states = np.max(q2, axis=1)
        q1_mb = T @ V_states

        # Dynamic arbitration weight using transition prediction error
        # Transition PE magnitude for the chosen action and observed state: pe = 1 - T[a1, s]
        pe = 1.0 - T[a1, s]
        reliability = 1.0 - pe  # higher when transitions are well predicted
        w_t = (1.0 - gate) * w_base + gate * reliability
        w_t = max(0.0, min(1.0, w_t))  # clamp for safety

        q1_blend = w_t * q1_mb + (1.0 - w_t) * q1_mf

        logits1 = beta * q1_blend
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Update transition model
        onehot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] += lrT * (onehot_s - T[a1, :])

        # Stage-2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr2 * delta2

        # Stage-1 model-free SARSA(0) backup toward the (pre-update) second-stage Q of the taken action
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += lr2 * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Volatility-adaptive temperature with fixed transition structure and stage-1 stickiness.

    Summary
    -------
    - Uses a fixed, known transition structure (common=0.7, rare=0.3) to plan at stage 1.
    - Learns second-stage Q-values with model-free TD.
    - Tracks recent reward volatility via an exponentially weighted average of absolute reward
      prediction errors at stage 2. High volatility reduces effective temperature (more exploration).
    - Includes a perseveration bias at stage 1 to repeat the previous spaceship.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage states per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Rewards per trial.
    model_parameters : iterable of floats
        [lr2, beta, temp_mod, v_lr, stick1]
        - lr2 in [0,1]: learning rate for Q2 updates.
        - beta in [0,10]: base inverse temperature.
        - temp_mod in [0,1]: scales how strongly volatility reduces the temperature.
        - v_lr in [0,1]: learning rate for updating the volatility signal.
        - stick1 in [0,1]: strength of stage-1 perseveration bias toward the previous choice.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr2, beta, temp_mod, v_lr, stick1 = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix (A->X common, U->Y common)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    q2 = np.zeros((2, 2))  # stage-2 Q-values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    volatility = 0.0
    prev_a1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Effective temperature decreases with volatility: beta_eff = beta*(1 - temp_mod * volatility)
        beta_eff = beta * (1.0 - temp_mod * volatility)
        beta_eff = max(0.0, beta_eff)  # ensure non-negative

        # Stage-2 policy
        logits2 = beta_eff * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 model-based planning with fixed transitions
        V_states = np.max(q2, axis=1)
        q1_mb = T_fixed @ V_states

        # Add perseveration bias on the previously chosen spaceship
        bias_vec = np.zeros(2)
        if prev_a1 is not None:
            bias_vec[prev_a1] = stick1

        logits1 = beta_eff * q1_mb + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 learning and volatility update
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr2 * pe2

        # Update volatility as EWMA of absolute PE
        volatility += v_lr * (abs(pe2) - volatility)

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll