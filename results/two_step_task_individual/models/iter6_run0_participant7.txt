def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Uncertainty-gated hybrid (learned transitions; action-specific gating by transition certainty)

    This model learns:
      - Second-stage action values (Q2) via delta-rule.
      - The first-stage transition matrix T via delta-rule.
    It computes a model-based first-stage value Q1_MB = T @ max(Q2).
    In parallel, it learns a model-free first-stage value Q1_MF from rewards directly.
    The final first-stage decision value is an action-specific mixture of MB and MF,
    with the mixture weight w_a determined by the certainty (1 - entropy) of the
    learned transition row for that action and a sigmoidal gating controlled by (chi, zeta).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Observed second-stage state (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within each planet).
    reward : array-like of float
        Obtained reward per trial.
    model_parameters : iterable of 5 floats
        [alpha_q, alpha_t, beta, chi, zeta]
        - alpha_q: [0,1] learning rate for Q-learning at stage 2 and MF first-stage update
        - alpha_t: [0,1] learning rate for transition matrix updates
        - beta:   [0,10] inverse temperature for both stages
        - chi:    [0,10] sharpness of the gating sigmoid from transition certainty to MB weight
        - zeta:   [0,1] gating offset (certainty threshold) for model-based reliance

    Returns
    -------
    float
        Negative log-likelihood of the observed choices at both stages.
    """
    alpha_q, alpha_t, beta, chi, zeta = model_parameters
    n_trials = len(action_1)

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Learned components
    q2 = np.zeros((2, 2))      # stage-2 action values: q2[state, action]
    q1_mf = np.zeros(2)        # model-free first-stage values (bandit toward reward)
    T = np.full((2, 2), 0.5)   # learned transition probs: T[action1, state]

    log2 = np.log(2.0)  # for entropy normalization

    for t in range(n_trials):
        s = state[t]
        a2 = action_2[t]
        a1 = action_1[t]
        r = reward[t]

        # Stage-2 policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        p_choice_2[t] = probs2[a2]

        # Model-based Q1 from learned transitions
        max_q2 = np.max(q2, axis=1)  # best attainable value at each state
        q1_mb = T @ max_q2           # size (2,)

        # Compute action-specific MB weight from transition certainty via sigmoid
        # certainty_a = 1 - H(T[a,:])/log(2)
        H_rows = -np.sum(T * (np.log(T + 1e-12)), axis=1)
        certainty = 1.0 - (H_rows / (log2 + 1e-12))
        # Sigmoid gating per action: w_a = sigmoid(chi * (certainty_a - zeta))
        w = 1.0 / (1.0 + np.exp(-chi * (certainty - zeta)))
        # Mixture for each action separately
        q1_mix = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1_mix
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        p_choice_1[t] = probs1[a1]

        # Learning updates
        # 1) Update second-stage value
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * delta2

        # 2) Update model-free first-stage value toward obtained reward
        delta1_mf = r - q1_mf[a1]
        q1_mf[a1] += alpha_q * delta1_mf

        # 3) Update transition probabilities for chosen first-stage action
        # Move probability mass toward observed state with alpha_t and renormalize row
        T[a1, s] += alpha_t * (1.0 - T[a1, s])
        other_s = 1 - s
        T[a1, other_s] = 1.0 - T[a1, s]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Surprise-modulated learning with eligibility and outcome-dependent perseveration

    This model is purely model-free with two features:
      - Surprise modulation: learning rates scale with the absolute second-stage prediction error.
      - Eligibility trace from stage-2 to stage-1: first-stage values are updated by the stage-2 PE.
      - Outcome-dependent perseveration: a bias to repeat the last first-stage action is
        scaled by the sign of the previous reward.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships).
    state : array-like of int (0 or 1)
        Observed second-stage state (planet).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens).
    reward : array-like of float
        Rewards per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, lambda_e, kappa_surp, pi]
        - alpha:       [0,1] base learning rate
        - beta:        [0,10] inverse temperature used at both stages
        - lambda_e:    [0,1] eligibility factor scaling the PE passed to stage-1 update
        - kappa_surp:  [0,10] multiplicative sensitivity to surprise (|PE|) for adaptive learning rates
        - pi:          [0,1] magnitude of outcome-dependent perseveration bias at stage-1
                         (added to the logit of the previously chosen action, scaled by sign of last reward)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices at both stages.
    """
    alpha, beta, lambda_e, kappa_surp, pi = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    last_a1 = -1
    last_reward_sign = 0.0  # in {-1, 0, +1}

    for t in range(n_trials):
        s = state[t]
        a2 = action_2[t]
        a1 = action_1[t]
        r = reward[t]

        # Stage-2 policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        p_choice_2[t] = probs2[a2]

        # Stage-1 outcome-dependent perseveration bias
        bias1 = np.zeros(2)
        if last_a1 in (0, 1):
            bias1[last_a1] += pi * last_reward_sign

        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        p_choice_1[t] = probs1[a1]

        # Learning at stage-2 with surprise-modulated rate
        pe2 = r - q2[s, a2]
        alpha2_t = alpha * (1.0 + kappa_surp * np.abs(pe2))
        if alpha2_t > 1.0:
            alpha2_t = 1.0
        q2[s, a2] += alpha2_t * pe2

        # Stage-1 update via eligibility trace driven by the stage-2 PE
        alpha1_t = alpha * (1.0 + kappa_surp * np.abs(pe2))
        if alpha1_t > 1.0:
            alpha1_t = 1.0
        q1[a1] += alpha1_t * lambda_e * pe2

        # Update perseveration memory
        last_a1 = a1
        last_reward_sign = 0.0
        if r > 0:
            last_reward_sign = 1.0
        elif r < 0:
            last_reward_sign = -1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Kalman-filtered stage-2 with learned transitions and MB/MF arbitration

    This model treats each second-stage option as a drifting payoff tracked by a Kalman filter.
    - Stage-2 values (means) are updated with trial-wise Kalman gains derived from process and observation noise.
    - Transitions from first-stage actions to states are learned.
    - First-stage action values are a mixture of model-based planning from the learned transitions
      and a model-free cache that backs up the (Kalman) prediction error from stage-2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships).
    state : array-like of int (0 or 1)
        Observed second-stage state (planet).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens).
    reward : array-like of float
        Rewards per trial.
    model_parameters : iterable of 5 floats
        [beta, tau_p, sigma_o, alpha_t, omega_mb]
        - beta:     [0,10] inverse temperature for both stages
        - tau_p:    [0,1] process noise added to second-stage value variances each trial
        - sigma_o:  [0,1] observation noise for rewards (controls Kalman gain)
        - alpha_t:  [0,1] learning rate for updating transition probabilities
        - omega_mb: [0,1] mixture weight for model-based values at stage-1 (1=fully MB)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices at both stages.
    """
    beta, tau_p, sigma_o, alpha_t, omega_mb = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 means and variances for Kalman filter
    mu2 = np.zeros((2, 2))         # mean reward per (state, action)
    var2 = np.ones((2, 2)) * 1.0   # uncertainty per (state, action)

    # Learned transitions and MF cache
    T = np.full((2, 2), 0.5)       # T[action1, state]
    q1_mf = np.zeros(2)            # model-free first-stage values

    for t in range(n_trials):
        s = state[t]
        a2 = action_2[t]
        a1 = action_1[t]
        r = reward[t]

        # Stage-2 policy uses current means
        logits2 = beta * mu2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        p_choice_2[t] = probs2[a2]

        # Model-based first-stage value from learned transitions and current mu2
        max_mu2 = np.max(mu2, axis=1)
        q1_mb = T @ max_mu2

        # Mixture with MF cache
        q1 = omega_mb * q1_mb + (1.0 - omega_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        p_choice_1[t] = probs1[a1]

        # Learning updates

        # 1) Kalman filter update for the chosen second-stage option
        # Prior variance (process noise)
        var_prior = var2[s, a2] + tau_p
        mu_prior = mu2[s, a2]
        # Kalman gain
        denom = var_prior + sigma_o + 1e-12
        K = var_prior / denom
        # Prediction error based on prior mean
        pe = r - mu_prior
        # Posterior update
        mu_post = mu_prior + K * pe
        var_post = (1.0 - K) * var_prior
        mu2[s, a2] = mu_post
        var2[s, a2] = var_post

        # 2) Update MF first-stage value using the same PE gated by the Kalman gain
        q1_mf[a1] += K * pe

        # 3) Update transition probabilities for chosen first-stage action
        T[a1, s] += alpha_t * (1.0 - T[a1, s])
        other_s = 1 - s
        T[a1, other_s] = 1.0 - T[a1, s]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll