Below are three distinct cognitive models tailored to the two-step task. Each function:

- Takes action_1, state, action_2, reward, model_parameters
- Uses all parameters meaningfully (with bounds: most in [0,1], beta in [0,10])
- Returns the negative log-likelihood of the observed choices
- Implements a two-stage policy and learning rules appropriate for the task
- Uses no imports inside the code (assumes numpy as np is available)

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Uncertainty-weighted hybrid control with learned transitions and stage-1 stickiness.

    Mechanism
    - Learn a per-action transition model T (rows sum to 1) using a delta rule.
    - Learn second-stage Q-values with model-free RL.
    - Learn first-stage model-free Q-values via TD(0) bootstrapping from second-stage Q.
    - Arbitration at stage 1 is action-specific: each action's MB weight is proportional
      to the certainty (1 - entropy) of its learned transition row, modulated by mb_gate.
    - Stage 1 includes a stickiness bias toward repeating the last first-stage action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 within the reached state).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha_v, beta, trans_alpha, mb_gate, kappa_s1]
        - alpha_v in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for both stages.
        - trans_alpha in [0,1]: transition learning rate for T rows.
        - mb_gate in [0,1]: how strongly certainty boosts model-based control at stage 1.
        - kappa_s1 in [0,1]: stickiness toward the previous stage-1 action.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_v, beta, trans_alpha, mb_gate, kappa_s1 = model_parameters
    n_trials = len(action_1)

    # Prob tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Transition model T[a1, s] learned from experience; initialize uniform
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)        # model-free first-stage
    q2 = np.zeros((2, 2))      # second-stage

    prev_a1 = -1

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute stage-1 MB values using learned transitions and current q2
        v2 = np.max(q2, axis=1)         # value of each second-stage state
        q1_mb = T @ v2                  # model-based value per first-stage action

        # Action-specific arbitration weight based on certainty = 1 - entropy(row)
        # Entropy for binary row: H(p) = -sum p log2 p; normalized by 1 bit.
        row_ent = np.zeros(2)
        for a in range(2):
            p_row = T[a]
            # avoid log(0)
            p_safe = np.clip(p_row, 1e-12, 1.0)
            H = -(p_safe[0] * np.log2(p_safe[0]) + p_safe[1] * np.log2(p_safe[1]))
            row_ent[a] = H  # in [0,1] for binary distribution
        certainty = 1.0 - row_ent               # in [0,1]
        # Arbitration: higher certainty -> more MB (scaled by mb_gate), fallback 0.5 when uncertain
        w = mb_gate * certainty + (1.0 - mb_gate) * 0.5

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 stickiness toward previous a1
        pref1 = q1.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += kappa_s1

        # Softmax stage 1
        pref1 -= np.max(pref1)
        pr1 = np.exp(beta * pref1)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        # Stage-2 softmax policy
        pref2 = q2[s, :].copy()
        pref2 -= np.max(pref2)
        pr2 = np.exp(beta * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]

        # Learning updates
        # Second-stage MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_v * pe2

        # First-stage MF TD(0) bootstrap from attained second-stage action value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_v * pe1

        # Transition learning for chosen first-stage action toward observed state s
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T[a1, :] = (1.0 - trans_alpha) * T[a1, :] + trans_alpha * target_row
        T[a1, :] /= (np.sum(T[a1, :]) + 1e-12)

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Surprise-gated eligibility with learned transitions and stage-2 perseveration.

    Mechanism
    - Learn transitions T per first-stage action online (rows sum to 1).
    - Second-stage: model-free Q-learning.
    - First-stage: model-free Q-learning augmented by an eligibility-like direct
      reward update scaled by elig and gated by transition surprise from previous trial.
    - Surprise-driven choice bias at stage 1: when the previous trial was both surprising
      and rewarded, bias current choice toward the action more likely (under T) to reach
      the previously rewarded state.
    - Stage-2 includes choice perseveration within each state.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 within reached state).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, elig, surprise_w, rho2]
        - alpha in [0,1]: learning rate for Q-values and transitions.
        - beta in [0,10]: inverse temperature for both stages.
        - elig in [0,1]: strength of direct reward credit to first-stage action.
        - surprise_w in [0,1]: strength of surprise-driven bias at stage 1.
        - rho2 in [0,1]: stage-2 perseveration (repeat last action in that state).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, elig, surprise_w, rho2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Transition model learned
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # For surprise-driven bias
    prev_a1 = -1
    prev_s = -1
    prev_r = 0.0
    prev_surprise = 0.0

    # Stage-2 perseveration memory per state
    prev_a2 = np.full(2, -1, dtype=int)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Surprise-driven bias term based on previous trial
        # surprise_t-1 â‰ˆ 1 - T[a1_{t-1}, s_{t-1}]
        bias1 = np.zeros(2)
        if prev_s in (0, 1) and prev_a1 in (0, 1):
            # Favor actions more likely to reach the previously rewarded state if prev trial was surprising and rewarded
            desirability = T[:, prev_s] - 0.5  # centered about 0
            bias_strength = surprise_w * prev_r * prev_surprise
            bias1 = bias_strength * desirability

        # Stage-1 policy (MF + bias)
        pref1 = q1_mf.copy() + bias1
        pref1 -= np.max(pref1)
        pr1 = np.exp(beta * pref1)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        # Stage-2 policy with perseveration
        pref2 = q2[s, :].copy()
        if prev_a2[s] in (0, 1):
            pref2[prev_a2[s]] += rho2
        pref2 -= np.max(pref2)
        pr2 = np.exp(beta * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]

        # Learning: second-stage Q
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Learning: first-stage MF (TD from attained second-stage action value)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Eligibility-like direct reward assignment to stage-1 action
        q1_mf[a1] += alpha * elig * (r - q1_mf[a1])

        # Transition learning for chosen first-stage action
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T[a1, :] = (1.0 - alpha) * T[a1, :] + alpha * target_row
        T[a1, :] /= (np.sum(T[a1, :]) + 1e-12)

        # Update surprise for next trial's bias computation
        if prev_a1 in (0, 1) and prev_s in (0, 1):
            prev_surprise = 1.0 - T[prev_a1, prev_s]
        else:
            prev_surprise = 0.0

        prev_a1 = a1
        prev_s = s
        prev_r = r
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Volatility-gated model-based control with adaptive temperature, forgetting, and lapse.

    Mechanism
    - Track outcome volatility via an exponential moving average of absolute second-stage PEs.
    - Use volatility v_t to:
        (i) adaptively reduce reliance on model-based control (w_t = phi * (1 - v_t)),
        (ii) reduce effective inverse temperature beta_eff = beta0 * (1 - 0.5*v_t),
        (iii) apply volatility-proportional forgetting to Q-values each trial.
    - Second-stage: model-free Q-learning.
    - First-stage: hybrid of MF and MB values; MB uses a fixed transition structure (0.7 common).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 within reached state).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alr, beta0, vol, phi, eps_lapse]
        - alr in [0,1]: learning rate for Q-value updates.
        - beta0 in [0,10]: base inverse temperature for both stages.
        - vol in [0,1]: volatility sensitivity (EMA step size) and forgetting scale.
        - phi in [0,1]: maximal weight on model-based value at low volatility.
        - eps_lapse in [0,1]: lapse probability mixed with uniform choice at both stages.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alr, beta0, vol, phi, eps_lapse = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Fixed (assumed known) transition structure for MB planner
    # Row 0: spaceship A -> [X, Y]; Row 1: spaceship U -> [X, Y]
    trans_fixed = np.array([[0.7, 0.3],
                            [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Volatility tracker (EMA of |PE2|)
    v = 0.0

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Volatility-proportional forgetting applied each trial
        forget_factor = 1.0 - vol * v  # in [0,1]
        forget_factor = max(0.0, min(1.0, forget_factor))
        q1_mf *= forget_factor
        q2 *= forget_factor

        # Compute MB values via fixed transitions
        v2 = np.max(q2, axis=1)
        q1_mb = trans_fixed @ v2

        # Volatility-gated MB weight and temperature
        w = phi * (1.0 - v)           # more MB when volatility is low
        w = max(0.0, min(1.0, w))
        beta_eff = beta0 * (1.0 - 0.5 * v)
        beta_eff = max(1e-8, min(10.0, beta_eff))

        q1 = (1.0 - w) * q1_mf + w * q1_mb

        # Stage-1 policy with lapse
        pref1 = q1 - np.max(q1)
        sm1 = np.exp(beta_eff * pref1)
        sm1 /= np.sum(sm1)
        pr1 = (1.0 - eps_lapse) * sm1 + eps_lapse * 0.5
        p_choice_1[t] = pr1[a1]

        # Stage-2 policy with lapse
        pref2 = q2[s, :] - np.max(q2[s, :])
        sm2 = np.exp(beta_eff * pref2)
        sm2 /= np.sum(sm2)
        pr2 = (1.0 - eps_lapse) * sm2 + eps_lapse * 0.5
        p_choice_2[t] = pr2[a2]

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += alr * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alr * pe1

        # Update volatility tracker from absolute second-stage PE
        v = (1.0 - vol) * v + vol * abs(pe2)
        v = max(0.0, min(1.0, v))

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll