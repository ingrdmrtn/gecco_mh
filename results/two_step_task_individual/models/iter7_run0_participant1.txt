def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with transition-contingent perseveration.
    
    This model blends model-based (MB) planning with model-free (MF) values at stage 1,
    learns second-stage rewards with a single learning rate, and includes a stage-1
    perseveration bias whose strength depends on whether the previous transition was common or rare.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A (commonly -> X), 1 = U (commonly -> Y).
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien at the visited planet (0/1).
    reward : array-like of float/int
        Coins received on each trial (typically 0 or 1).
    model_parameters : sequence of floats
        [alpha, beta, omegaMB, psi, zeta]
        - alpha (0..1): learning rate for both stage-1 MF and stage-2 values.
        - beta (0..10): inverse temperature for softmax at both stages.
        - omegaMB (0..1): weight on model-based action values at stage 1
                          (1 - omegaMB) weights model-free values.
        - psi (0..1): base perseveration strength for repeating the previous stage-1 action.
        - zeta (0..1): transition-contingent modulation of perseveration.
                       If the previous transition was common, perseveration magnitude is psi*(1+zeta);
                       if rare, it is psi*(1-zeta).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, omegaMB, psi, zeta = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2)       # model-free stage-1 values
    q2 = np.zeros((2, 2))     # stage-2 values per state and action

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_s2 = None

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 model-based values via planning over Q2
        max_q2 = np.max(q2, axis=1)           # best attainable value on each planet
        q1_mb = T @ max_q2                    # expected value of choosing each spaceship

        # Transition-contingent perseveration bias
        bias1 = np.zeros(2)
        if last_a1 is not None and last_s2 is not None:
            # Was the previous transition common?
            prev_common = (last_s2 == np.argmax(T[last_a1]))
            mag = psi * (1.0 + zeta) if prev_common else psi * (1.0 - zeta)
            bias1[last_a1] += mag

        # Mixture of MB and MF for stage-1 decision
        q1 = omegaMB * q1_mb + (1.0 - omegaMB) * q1_mf
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-1 MF update bootstrapped by experienced stage-2 chosen value
        delta1 = q2[s2, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Stage-2 update from reward
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        last_a1 = a1
        last_s2 = s2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Bayesian reward tracking with exploration bonus and learned transitions.
    
    This model maintains Beta-Bernoulli posteriors over each second-stage action’s
    reward probability, adds an uncertainty bonus for directed exploration, and
    learns the transition matrix online. Stage-2 decisions include a state-specific
    stickiness bias to repeat the previous alien in the same planet.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int
        Coins received on each trial (0/1).
    model_parameters : sequence of floats
        [beta, alphaT, kappa0, xi, chi]
        - beta (0..10): inverse temperature for both stages.
        - alphaT (0..1): transition learning rate for P(state | action_1).
        - kappa0 (0..1): prior strength for Beta posteriors; mapped to total prior pseudo-count
                         N0 = 2 + 18*kappa0 (from 2 to 20). Symmetric prior over success/failure.
        - xi (0..1): uncertainty bonus weight (UCB-like) added to stage-2 values.
                     Bonus is xi * sqrt(p*(1-p) / (n+1)), where n is total pseudo-count.
        - chi (0..1): state-dependent stickiness for repeating the previous stage-2 action.
                      Adds +chi to the last action’s logit within the same state.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    beta, alphaT, kappa0, xi, chi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix fairly neutral
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Beta prior parameters for each (state, action2)
    N0 = 2.0 + 18.0 * kappa0
    a = np.full((2, 2), N0 / 2.0)  # successes
    b = np.full((2, 2), N0 / 2.0)  # failures

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track last second-stage action per state for stickiness
    last_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Posterior means and uncertainty bonuses
        p_hat = a / (a + b)
        n_eff = (a + b)
        bonus = xi * np.sqrt(p_hat * (1.0 - p_hat) / (n_eff + 1.0))
        q2 = p_hat + bonus  # augmented value for directed exploration

        # Stage-1 planning using current transition estimates
        max_q2 = np.max(q2, axis=1)
        q1 = T @ max_q2

        # Softmax for stage 1
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with state-dependent stickiness
        logits2 = beta * q2[s2]
        if last_a2[s2] is not None:
            logits2[last_a2[s2]] += chi
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Transition learning: update T[a1] toward observed state s2
        oh = np.array([0.0, 0.0])
        oh[s2] = 1.0
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        # Beta posterior update for the experienced (state, action2)
        a[s2, a2] += r
        b[s2, a2] += (1.0 - r)

        last_a2[s2] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Arbitrated MB/MF control by reward volatility with stage-1 stickiness.
    
    This model learns second-stage values with TD, computes model-based (MB) values
    via a fixed transition model, and model-free (MF) stage-1 values via bootstrapping.
    An arbitration weight depends on an online estimate of reward volatility: under
    stable rewards, the agent relies more on MB; under volatile rewards, more on MF.
    A stage-1 perseveration bias encourages repeating the last spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int
        Coins received on each trial (0/1).
    model_parameters : sequence of floats
        [alpha, beta, nuV, arb0, psi]
        - alpha (0..1): learning rate for stage-2 values and stage-1 MF backup.
        - beta (0..10): inverse temperature for both stages.
        - nuV (0..1): volatility learning rate; controls how quickly volatility tracks
                      the magnitude of recent TD errors.
        - arb0 (0..1): baseline arbitration scale; effective MB weight is arb0*(1 - volatility).
        - psi (0..1): stage-1 perseveration bias added to the previously chosen spaceship.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, nuV, arb0, psi = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A commonly->X, U commonly->Y
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    volatility = 0.0
    last_a1 = None

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based planning at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Arbitration weight depends on current volatility
        w_mb = arb0 * (1.0 - volatility)
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # Stage-1 bias from perseveration
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += psi

        # Combined stage-1 values and policy
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Volatility update from absolute TD error
        volatility = (1.0 - nuV) * volatility + nuV * np.abs(delta2)
        volatility = np.clip(volatility, 0.0, 1.0)

        # Stage-1 MF backup from realized stage-2 value
        delta1 = q2[s2, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll