def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with learned transitions and entropy-weighted arbitration plus value forgetting.
    
    This model jointly learns second-stage values and the first-stage transition matrix,
    and arbitrates between model-based (MB) and model-free (MF) values at stage 1 using
    a dynamic weight derived from each action's transition uncertainty (row entropy).
    A small forgetting term pulls values toward a neutral baseline to accommodate
    nonstationarity.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet: 0/1 = two aliens there.
    reward : array-like of float (e.g., 0 or 1)
        Coins received at the end of each trial.
    model_parameters : sequence
        [alpha, beta, w0, gamma, phi]
        - alpha (learning rate, [0,1]): MF learning rate for Q-values.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - w0 (baseline MB weight, [0,1]): Baseline tendency toward MB at stage 1.
        - gamma (transition learning rate, [0,1]): Learning rate for transition probabilities.
        - phi (forgetting, [0,1]): Amount of decay toward neutral (0.5) each trial for Q-values.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w0, gamma, phi = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix rows to uniform (uncertain)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # MF Q-values
    q1_mf = np.zeros(2)            # stage-1 MF values
    q2 = np.zeros((2, 2))          # stage-2 MF values per planet/state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10
    log2 = np.log(2.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based value at stage 1: expect best attainable value via learned transitions
        max_q2 = np.max(q2, axis=1)          # value of best alien on each planet
        q1_mb = T @ max_q2                   # expected values for the two ships

        # Compute row entropies for each ship's transition distribution (normalized to [0,1])
        H_rows = np.zeros(2)
        for a in range(2):
            p_row = T[a]
            # Handle logs safely
            H = 0.0
            for k in range(2):
                if p_row[k] > 0:
                    H -= p_row[k] * np.log(p_row[k])
            H_rows[a] = H / log2  # normalize by log(2) to be in [0,1]

        # Dynamic arbitration weight per action: push w toward MB when transitions are certain (low entropy)
        # and toward MF when transitions are uncertain (high entropy).
        # w_a = w0*(1 - H) + (1 - w0)*H
        w_actions = w0 * (1.0 - H_rows) + (1.0 - w0) * H_rows

        # Effective stage-1 action values: action-wise weighted combination
        q1_eff = w_actions * q1_mb + (1.0 - w_actions) * q1_mf

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy for the observed planet
        q2_eff = q2[s].copy()
        exp_q2 = np.exp(beta * (q2_eff - np.max(q2_eff)))
        probs2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs2[a2]

        # Value forgetting toward neutral baseline 0.5 (for nonstationary environments)
        q2 = (1.0 - phi) * q2 + phi * 0.5
        q1_mf = (1.0 - phi) * q1_mf + phi * 0.5

        # MF learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update stage-1 MF with bootstrapped value from the reached state/action
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Learn the transition matrix row for the chosen ship: move toward the observed outcome state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - gamma) * T[a1] + gamma * target
        # Re-normalize defensively
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-free with decaying choice kernels and directed exploration bonus (UCB-like).
    
    This model learns MF Q-values at both stages. It augments decision values with:
    - A decaying choice kernel that captures a graded tendency to repeat recent actions.
    - A directed exploration bonus inversely proportional to choice counts (higher for
      less tried actions), applied separately at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet: 0/1.
    reward : array-like of float
        Coins received at the end of each trial.
    model_parameters : sequence
        [alpha, beta, kappa, tau, b_unc]
        - alpha (learning rate, [0,1]): MF learning rate for Q-values.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - kappa (choice-kernel gain, [0,1]): Weight of the choice kernel added to values.
        - tau (kernel decay, [0,1]): Fractional decay of the choice kernel per trial.
        - b_unc (uncertainty bonus, [0,1]): Weight of directed exploration bonus (UCB-like).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, kappa, tau, b_unc = model_parameters
    n_trials = len(action_1)

    # MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels (decaying traces of recent selections)
    K1 = np.zeros(2)
    K2 = np.zeros((2, 2))

    # Choice counts for UCB-like uncertainty bonus
    N1 = np.zeros(2)
    N2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Directed exploration bonuses (higher for less-sampled actions)
        bonus1 = b_unc / np.sqrt(N1 + 1.0)
        bonus2 = b_unc / np.sqrt(N2[s] + 1.0)

        # Effective decision values with choice kernels and exploration bonuses
        v1 = q1 + kappa * K1 + bonus1
        exp_v1 = np.exp(beta * (v1 - np.max(v1)))
        probs1 = exp_v1 / (np.sum(exp_v1) + eps)
        p_choice_1[t] = probs1[a1]

        v2 = q2[s] + kappa * K2[s] + bonus2
        exp_v2 = np.exp(beta * (v2 - np.max(v2)))
        probs2 = exp_v2 / (np.sum(exp_v2) + eps)
        p_choice_2[t] = probs2[a2]

        # Decay choice kernels
        K1 *= (1.0 - tau)
        K2 *= (1.0 - tau)

        # Reinforce chosen actions in kernels
        K1[a1] += 1.0
        K2[s, a2] += 1.0

        # Update counts after observing the choice
        N1[a1] += 1.0
        N2[s, a2] += 1.0

        # MF learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # MF learning at stage 1 via bootstrapped value from stage 2 choice
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Surprise-adaptive learning with risk-sensitive choice and dynamic MB/MF arbitration.
    
    This model:
    - Uses a surprise-modulated learning rate combining transition surprise and reward surprise.
    - Applies risk-sensitive probability distortion at stage 2 when forming choice probabilities.
    - At stage 1, dynamically arbitrates between MB and MF values based on transition surprise
      (greater surprise -> rely more on MF), without introducing an explicit mixing parameter.

    Transitions are assumed known (A->X and U->Y common): [[0.7, 0.3], [0.3, 0.7]].

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet.
    reward : array-like of float
        Coins received at the end of each trial.
    model_parameters : sequence
        [alpha0, beta1, beta2, chi, rho]
        - alpha0 (base learning rate, [0,1]): Baseline MF learning rate.
        - beta1 (inverse temperature stage 1, [0,10]): Softmax sensitivity at stage 1.
        - beta2 (inverse temperature stage 2, [0,10]): Softmax sensitivity at stage 2.
        - chi (surprise gain, [0,1]): Scales how strongly surprise increases learning and shifts arbitration.
        - rho (probability distortion, [0,1]): Distorts estimated success probabilities at stage 2;
          rho=1 gives no distortion; lower values increase S-shaped distortion.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha0, beta1, beta2, chi, rho = model_parameters
    n_trials = len(action_1)

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based value at stage 1 from known transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Transition surprise for the chosen ship: 1 - P(reached state | chosen ship)
        s_trans = 1.0 - T[a1, s]

        # Stage-1 arbitration weight toward MB: w_mb = 1 - chi * s_trans (bounded [0,1])
        w_mb = 1.0 - chi * s_trans
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # Effective stage-1 values and policy
        q1_eff = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        exp_q1 = np.exp(beta1 * (q1_eff - np.max(q1_eff)))
        probs1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs1[a1]

        # Risk-sensitive probability distortion at stage 2 when forming policy
        # Interpret q2[s] as success probabilities; apply symmetric distortion:
        # p_tilde = q^rho / (q^rho + (1-q)^rho)
        q_state = q2[s].copy()
        q_pow = np.power(np.clip(q_state, 0.0, 1.0), rho)
        q_comp_pow = np.power(np.clip(1.0 - q_state, 0.0, 1.0), rho)
        denom = q_pow + q_comp_pow
        p_tilde = np.divide(q_pow, np.maximum(denom, eps))

        exp_q2 = np.exp(beta2 * (p_tilde - np.max(p_tilde)))
        probs2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs2[a2]

        # Reward surprise for the chosen second-stage action
        s_rew = np.abs(r - q2[s, a2])

        # Surprise-adaptive learning rate (bounded to [0,1])
        surpr = 0.5 * (s_trans + s_rew)
        alpha_t = alpha0 * (1.0 + chi * surpr)
        alpha_t = np.clip(alpha_t, 0.0, 1.0)

        # Update MF values
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_t * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll