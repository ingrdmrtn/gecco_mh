def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MBâ€“MF with learned transitions and eligibility trace at stage 1.

    The agent learns:
    - Second-stage values (Q2) via TD learning.
    - First-stage model-free values (Q1_MF) via an eligibility-trace backup from Q2.
    - The transition model T (from spaceship to planet) via a simple delta rule.

    Action selection at stage 1 uses a convex combination of model-free and
    model-based values, where model-based values are computed using the learned
    transition matrix T and the current second-stage values. Stage 2 uses a
    standard softmax over Q2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0 or 1 = alien on the reached planet).
    reward : array-like of float
        Reward outcome (e.g., 0 or 1) per trial.
    model_parameters : sequence of floats
        [alpha, beta, w_mb, eta_elig, tau_T]
        - alpha in [0,1]: learning rate for value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w_mb in [0,1]: weight on model-based values at stage 1
                         (0 = purely MF, 1 = purely MB).
        - eta_elig in [0,1]: eligibility trace strength to back up Q2 onto Q1_MF.
        - tau_T in [0,1]: learning rate for the transition matrix T rows.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w_mb, eta_elig, tau_T = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model (rows sum to 1)
    T = np.ones((2, 2), dtype=float) * 0.5

    # Values
    q2 = np.zeros((2, 2), dtype=float) + 0.5
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based stage-1 values from learned transition model and Q2
        max_q2 = np.max(q2, axis=1)  # for each planet
        q1_mb = T @ max_q2

        # Hybrid arbitration
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 policy
        l1 = beta * (q1 - np.max(q1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        s = state[t]
        l2 = beta * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Value learning
        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF eligibility-trace backup from updated Q2
        backup_target = q2[s, a2]
        pe1 = backup_target - q1_mf[a1]
        q1_mf[a1] += (alpha * eta_elig) * pe1

        # Learn the transition model row corresponding to chosen spaceship
        # Move the row toward the observed next state (one-hot vector)
        target_row = np.array([0.0, 0.0], dtype=float)
        target_row[s] = 1.0
        T[a1] = (1.0 - tau_T) * T[a1] + tau_T * target_row
        # Numerical safety and re-normalize (should already sum to ~1)
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planning with adaptive learning rate from volatility and stage-1 perseveration.

    The agent plans at stage 1 using a fixed (known) transition model, but the second-stage
    value learning rate adapts to recent outcome volatility (unsigned prediction errors).
    Volatility increases the effective learning rate, enabling faster adaptation to changes.
    A stage-1 perseveration kernel captures tendency to repeat the last first-stage choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha_base, beta, kappa_vol, tau_v, rho_stay]
        - alpha_base in [0,1]: baseline learning rate for second-stage values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa_vol in [0,1]: sensitivity of learning rate to estimated volatility.
        - tau_v in [0,1]: update rate for volatility (EMA of unsigned PE).
        - rho_stay in [0,1]: strength of first-stage perseveration kernel.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_base, beta, kappa_vol, tau_v, rho_stay = model_parameters
    n_trials = len(action_1)

    # Fixed known transition model (common = 0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float) + 0.5

    # Volatility tracker (unsigned PE EMA)
    vol = 0.0

    # Stage-1 perseveration kernel
    K1 = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # Effective learning rate increases with volatility, clipped to [0,1]
        alpha_eff = alpha_base + kappa_vol * vol
        if alpha_eff > 1.0:
            alpha_eff = 1.0

        # Model-based stage-1 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Apply perseveration kernel to logits
        logits1 = q1_mb + rho_stay * K1

        # Stage-1 policy
        l1 = beta * (logits1 - np.max(logits1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        s = state[t]
        l2 = beta * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Update Q2 with adaptive learning rate
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * pe2

        # Update volatility estimate (EMA of |PE|)
        vol = (1.0 - tau_v) * vol + tau_v * np.abs(pe2)

        # Update perseveration kernel (decay then add to chosen action)
        K1 *= (1.0 - tau_v)
        K1[a1] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Transition-contingent credit assignment with dynamic arbitration.

    The agent maintains both:
    - Model-free first-stage values (Q1_MF) updated via an eligibility trace from Q2.
    - Model-based first-stage values (Q1_MB) from a fixed transition model and Q2.

    Arbitration weight w_t is dynamic: when recent second-stage prediction errors are
    small (MB reliable) relative to recent MF TD errors, the agent relies more on MB;
    when MF errors are smaller, it relies more on MF. Additionally, model-free credit
    assigned to the chosen first-stage action is down-weighted on rare transitions
    (transition-contingent credit assignment).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (gold coins) per trial.
    model_parameters : sequence of floats
        [alpha, beta, kappa_rel, tau_rel, zeta_rare]
        - alpha in [0,1]: learning rate for value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa_rel in [0,1]: sensitivity of arbitration to reliability difference.
                              Higher values make w_t swing more strongly.
        - tau_rel in [0,1]: update rate for running reliability signals (EMAs of errors).
        - zeta_rare in [0,1]: discount factor on MF credit after rare transitions
                              (0 = no MF credit on rare, 1 = same as common).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, kappa_rel, tau_rel, zeta_rare = model_parameters
    n_trials = len(action_1)

    # Fixed known transition model
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Determine which next-state is "common" for each first-stage action
    common_next = np.array([0, 1], dtype=int)  # A -> X(0), U -> Y(1)

    q2 = np.zeros((2, 2), dtype=float) + 0.5
    q1_mf = np.zeros(2, dtype=float)

    # Running reliability signals (lower error => higher reliability)
    # Track unsigned PEs for MB (stage-2) and MF (stage-1 backup)
    err_mb = 0.5
    err_mf = 0.5

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Dynamic arbitration: higher weight to the controller with lower recent error
        # Compute a signed reliability difference (MB more reliable => positive)
        # Use inverse-error proxy and squash with sigmoid-like mapping via tanh approximation
        rel_diff = (err_mf - err_mb)  # positive => MB better
        w_t = 1.0 / (1.0 + np.exp(-4.0 * kappa_rel * rel_diff))  # in (0,1)

        # Combined first-stage values
        q1 = (1.0 - w_t) * q1_mf + w_t * q1_mb

        # Stage-1 policy
        l1 = beta * (q1 - np.max(q1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        s = state[t]
        l2 = beta * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Stage-2 TD update (drives MB reliability)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update MB error tracker (unsigned PE)
        err_mb = (1.0 - tau_rel) * err_mb + tau_rel * np.abs(pe2)

        # Transition-contingent MF credit assignment
        is_common = 1.0 if s == common_next[a1] else 0.0
        credit_scale = is_common + (1.0 - is_common) * zeta_rare  # =1 for common, =zeta_rare for rare

        # MF backup from current Q2 (post-update) with eligibility and rare discount
        backup_target = q2[s, a2]
        pe1 = backup_target - q1_mf[a1]
        q1_mf[a1] += (alpha * credit_scale) * pe1

        # Update MF error tracker (unsigned backup PE, scaled by credit used)
        err_mf = (1.0 - tau_rel) * err_mf + tau_rel * (credit_scale * np.abs(pe1))

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll