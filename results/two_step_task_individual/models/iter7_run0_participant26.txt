def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Entropy-gated arbitration with learned transitions and perseveration.

    This model blends model-free (MF) and model-based (MB) control at the first stage,
    where the arbitration weight depends on the agent's uncertainty about second-stage
    action values (entropy). The transition structure is learned from experience.
    Perseveration biases repeating recent actions at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0 or 1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha, beta, zeta_arbit, sigma_trans, kappa_choice]
        - alpha (0..1): Learning rate for value updates at both stages (MF updates).
        - beta (0..10): Inverse temperature for softmax at both stages.
        - zeta_arbit (0..1): Scales arbitration based on uncertainty; higher increases
                             MB influence when second-stage policies are confident.
        - sigma_trans (0..1): Learning rate for updating the transition matrix from observed transitions.
        - kappa_choice (0..1): Strength of perseveration bias for repeating the last chosen action.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, zeta_arbit, sigma_trans, kappa_choice = model_parameters
    n_trials = len(action_1)

    # Prob of common transitions is learned; initialize to agnostic transitions
    T = np.full((2, 2), 0.5)  # rows: action A/U; cols: states X/Y

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Simple perseveration traces (last choice only)
    last_a1 = None
    last_a2 = [None, None]  # per state

    for t in range(n_trials):
        # Compute uncertainty (entropy) of second-stage softmax policies
        entropies = np.zeros(2)
        for s in range(2):
            logits2_s = beta * (q2[s] - np.max(q2[s]))
            probs2_s = np.exp(logits2_s)
            probs2_s /= (np.sum(probs2_s) + 1e-12)
            # entropy base-e
            ent = -(probs2_s * (np.log(probs2_s + 1e-12))).sum()
            # normalize entropy to [0, ln(2)] -> [0,1]
            entropies[s] = ent / np.log(2.0 + 1e-12)
        u_uncert = np.clip(entropies.mean(), 0.0, 1.0)
        # Arbitration weight increases as uncertainty decreases
        w_mb = np.clip(zeta_arbit * (1.0 - u_uncert), 0.0, 1.0)

        # Model-based Q from current transition beliefs and second-stage values
        max_q2 = np.max(q2, axis=1)  # value of each state
        q1_mb = np.zeros(2)
        for a in range(2):
            q1_mb[a] = T[a, 0] * max_q2[0] + T[a, 1] * max_q2[1]

        # Blend MF and MB
        q1_blend = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Add perseveration biases
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa_choice

        logits1 = beta * (q1_blend + bias1 - np.max(q1_blend + bias1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s2 = state[t]

        bias2 = np.zeros(2)
        if last_a2[s2] is not None:
            bias2[last_a2[s2]] += kappa_choice

        logits2 = beta * (q2[s2] + bias2 - np.max(q2[s2] + bias2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage values (MF)
        td2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # Update first-stage MF toward realized second-stage value (SR-like one-step backup)
        target1 = q2[s2, a2]
        td1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Update transition beliefs from observed transition (a1 -> s2)
        # One-hot outcome for s2 under action a1
        for s in range(2):
            target = 1.0 if s == s2 else 0.0
            T[a1, s] += sigma_trans * (target - T[a1, s])
        # Normalize row to be safe
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

        last_a1 = a1
        last_a2[s2] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Baseline-subtracted utility with confirmatory first-stage learning and global stay bias.

    This model is purely model-free in control but incorporates:
    - A running average reward baseline to compute utility prediction errors (satiety/contrast).
    - Confirmatory learning at the first stage: learning is amplified after common transitions and
      attenuated after rare transitions, producing a model-based-like stay/switch signature.
    - A global stay bias that encourages repeating the last chosen action at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0 or 1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha, beta, rho_sat, chi_confirm, psi_stay]
        - alpha (0..1): Base learning rate for both stages.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - rho_sat (0..1): Update rate for average reward baseline; higher tracks faster.
        - chi_confirm (0..1): Degree of confirmatory scaling on first-stage learning:
                              effective alpha1 = alpha * (1+chi_confirm) after common transitions
                              and alpha1 = alpha * (1-chi_confirm) after rare transitions.
        - psi_stay (0..1): Strength of stay bias for repeating the immediately previous action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, rho_sat, chi_confirm, psi_stay = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Running reward baseline
    r_bar = 0.5

    last_a1 = None
    last_a2 = [None, None]

    for t in range(n_trials):
        # Stay biases
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += psi_stay

        logits1 = beta * (q1 + bias1 - np.max(q1 + bias1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s2 = state[t]

        bias2 = np.zeros(2)
        if last_a2[s2] is not None:
            bias2[last_a2[s2]] += psi_stay

        logits2 = beta * (q2[s2] + bias2 - np.max(q2[s2] + bias2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Utility is reward relative to running baseline (contrast/satiety)
        u = r - r_bar

        # Second-stage update
        td2 = u - (q2[s2, a2] - r_bar)  # consistent baseline frame; equivalently r - q2 + (r_bar - r_bar)
        # Simplify to standard TD in reward space:
        td2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # Determine if transition was common or rare
        common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        alpha1 = alpha * (1.0 + chi_confirm if common else 1.0 - chi_confirm)

        # First-stage update toward realized second-stage value (confirmatory scaling)
        target1 = q2[s2, a2]
        td1 = target1 - q1[a1]
        q1[a1] += alpha1 * td1

        # Update running baseline
        r_bar = (1.0 - rho_sat) * r_bar + rho_sat * r

        last_a1 = a1
        last_a2[s2] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Pruned model-based valuation with leaky macro-chunking bias.

    This model integrates:
    - Model-free learning at both stages.
    - Model-based evaluation at the first stage with pruning: downweights the contribution
      of the currently less valuable planet when planning from each spaceship.
    - A leaky macro-action (chunking) bias: successful second-stage choices create a bias
      that associates the chosen spaceship with the obtained second-stage value, decaying over time.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0 or 1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received on each trial.
    model_parameters : iterable of floats
        [alpha, beta, pr_prune, eta_chunk, tau_leak]
        - alpha (0..1): Learning rate for value updates.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - pr_prune (0..1): Pruning strength; the contribution of the currently worse planet
                           to each spaceshipâ€™s MB value is multiplied by (1 - pr_prune).
        - eta_chunk (0..1): Learning rate for forming macro-action chunk biases from observed outcomes.
        - tau_leak (0..1): Leak (retention) factor for chunk biases across trials; closer to 1 decays slower.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, pr_prune, eta_chunk, tau_leak = model_parameters
    n_trials = len(action_1)

    # Fixed known transition matrix (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Chunk bias attached to first-stage actions (macro-action propensity)
    chunk_bias = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute planet values
        v_state = np.max(q2, axis=1)  # value of X and Y

        # Identify which planet is currently worse
        worse_state = 0 if v_state[0] < v_state[1] else 1
        better_state = 1 - worse_state

        # Pruned MB evaluation: downweight the worse state's contribution
        v_pruned = v_state.copy()
        v_pruned[worse_state] = (1.0 - pr_prune) * v_pruned[worse_state]

        q1_mb = T @ v_pruned

        # Combine MF with MB by simple averaging; MF will be learned via TD
        q1_combined = 0.5 * q1_mf + 0.5 * q1_mb

        # Add chunking bias
        logits1 = beta * (q1_combined + chunk_bias - np.max(q1_combined + chunk_bias))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s2 = state[t]

        logits2 = beta * (q2[s2] - np.max(q2[s2]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second-stage MF update
        td2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # First-stage MF update toward realized value (SARSA-style one-step)
        target1 = q2[s2, a2]
        td1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Update chunk bias: associate chosen ship with obtained second-stage value
        # Leak the bias
        chunk_bias *= tau_leak
        # Potentiate the chosen ship by outcome-valued signal
        # Use the advantage of the reached state over the alternative to favor informative chunks
        advantage = v_state[s2] - v_state[1 - s2]
        chunk_bias[a1] += eta_chunk * (r + 0.5 * advantage)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll