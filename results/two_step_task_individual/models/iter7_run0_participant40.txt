def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB/MF with counterfactual generalization and entropy-seeking at the first stage.

    Mechanisms:
    - Learns second-stage Q-values (Q2[s, a2]) from reward with learning rate eta_q.
    - Counterfactual generalization on the visited planet: the unchosen alien's value is updated
      toward the same reward with reduced learning rate g_cf * eta_q.
    - First-stage action values are a convex combination of:
        - Model-based values: expected max Q2 under a fixed transition model (A->X, U->Y common).
        - Model-free values: directly bootstrap from second-stage value via TD backup.
      The mixing weight is w_hyb (0=model-free only, 1=model-based only).
    - Entropy-seeking bonus at the first stage: adds kappa_ent times the expected second-stage policy
      entropy of the destination planet to the first-stage model-based values.

    Parameters (bounds):
    - eta_q in [0,1]: learning rate for second-stage Q2 updates.
    - g_cf in [0,1]: counterfactual generalization strength for the unchosen alien on the same planet.
    - w_hyb in [0,1]: arbitration weight between MB and MF at the first stage.
    - kappa_ent in [0,1]: weight of expected second-stage entropy bonus added to first-stage values.
    - beta in [0,10]: inverse temperature for softmax at both stages.

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_q, g_cf, w_hyb, kappa_ent, beta]

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_q, g_cf, w_hyb, kappa_ent, beta = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Fixed common/rare transition structure (A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # shape (2 actions, 2 states)

    # Value tables
    Q2 = np.zeros((2, 2))      # second-stage action values
    Q1_mf = np.zeros(2)        # first-stage model-free values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute current second-stage softmax for both states (for entropy bonus)
        # Use "logits2_all" derived from current Q2
        logits2_all = beta * (Q2 - np.max(Q2, axis=1, keepdims=True))
        probs2_all = np.exp(logits2_all)
        probs2_all = probs2_all / (np.sum(probs2_all, axis=1, keepdims=True) + eps)

        # Entropy for each state
        H2 = -np.sum(probs2_all * (np.log(probs2_all + eps)), axis=1)  # shape (2,)

        # Model-based first-stage values: expected max Q2 plus entropy-seeking bonus
        maxQ2 = np.max(Q2, axis=1)  # per state
        Q1_mb = T @ maxQ2 + kappa_ent * (T @ H2)

        # Combine MB and MF for policy at stage 1
        Q1_mix = w_hyb * Q1_mb + (1.0 - w_hyb) * Q1_mf

        # First-stage choice probability
        logits1 = beta * (Q1_mix - np.max(Q1_mix))
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability in observed state
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning updates

        # Second stage: chosen alien update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += eta_q * delta2

        # Counterfactual generalization: update unchosen alien on the same planet toward r
        a2_uc = 1 - a2
        delta2_cf = r - Q2[s, a2_uc]
        Q2[s, a2_uc] += (g_cf * eta_q) * delta2_cf

        # First-stage model-free bootstrap toward the realized second-stage value
        # (SARSA(0)-like from second stage value)
        backup = Q2[s, a2]
        Q1_mf[a1] += eta_q * (backup - Q1_mf[a1])

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive model-free learning with diminishing sensitivity and choice perseveration.

    Mechanisms:
    - Prospect-like utility transforms outcomes before learning:
        u(r) = r for r >= 0, and u(r) = -lambda_loss * |r| for r < 0.
      This captures loss aversion (lambda_loss > 1) or loss seeking (lambda_loss < 1).
    - Diminishing sensitivity in choice: action values used for softmax are transformed as
      V = sign(Q) * |Q|^(omega_pw), where omega_pw in (0,1] compresses large magnitudes.
    - Perseveration kernels at both stages with leak: each trial,
      K *= (1 - pi_stay) and the chosen action's kernel increases by pi_stay.
      Kernels additively bias the softmax logits at each stage.

    Parameters (bounds):
    - eta_q in [0,1]: learning rate for second-stage Q2 updates (using utility u(r)).
    - beta in [0,10]: inverse temperature for softmax at both stages.
    - lambda_loss in [0,1]: loss sensitivity scaling for negative outcomes (0-1 range per spec).
      Note: values near 1 approximate loss neutrality; smaller values reduce negative impact.
    - omega_pw in [0,1]: exponent for diminishing sensitivity in choice values (<=1 compresses).
    - pi_stay in [0,1]: strength of perseveration with built-in leak (also controls decay).

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_q, beta, lambda_loss, omega_pw, pi_stay]

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_q, beta, lambda_loss, omega_pw, pi_stay = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Stage-2 Q-values and Stage-1 MF values
    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    # Perseveration kernels
    K1 = np.zeros(2)
    K2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Value transforms for choice (diminishing sensitivity)
        def transform_for_choice(qvals):
            # sign(q) * |q|^omega
            return np.sign(qvals) * (np.abs(qvals) ** omega_pw)

        # First-stage logits: transformed MF values plus perseveration bias
        v1 = transform_for_choice(Q1_mf) + K1
        logits1 = beta * (v1 - np.max(v1))
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage logits: transformed Q2 values plus perseveration bias for the observed state
        v2 = transform_for_choice(Q2[s]) + K2[s]
        logits2 = beta * (v2 - np.max(v2))
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Utility transform of outcome for learning
        ur = r if r >= 0 else -lambda_loss * abs(r)

        # Second-stage learning
        delta2 = ur - Q2[s, a2]
        Q2[s, a2] += eta_q * delta2

        # First-stage MF backup from second-stage value (using transformed utility as target)
        Q1_mf[a1] += eta_q * (Q2[s, a2] - Q1_mf[a1])

        # Perseveration kernel updates with leak
        K1 *= (1.0 - pi_stay)
        K2 *= (1.0 - pi_stay)
        K1[a1] += pi_stay
        K2[s, a2] += pi_stay

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-guided model-based control with learned transitions and UCB exploration.

    Mechanisms:
    - Each alien's payout is tracked with a mean and uncertainty (variance-like) pair (m, v).
      Updates use a simple Kalman-like scheme:
        Prediction: v <- v + z_forget  (process noise grows uncertainty between observations)
        Update: m <- m + eta_r * (r - m); v <- (1 - eta_r) * v  (posterior shrinkage)
    - The second-stage choice uses an upper-confidence-bound (UCB) value:
        UCB[s, a2] = m[s, a2] + xi_ucb * sqrt(max(v[s, a2], 0))
    - First-stage values are purely model-based, using a learned transition matrix T:
        Q1[a1] = sum_s T[a1, s] * max_a2 UCB[s, a2]
      The transition matrix is updated via a delta rule toward the observed planet:
        T[a1, :] <- T[a1, :] + tau_T * (onehot(s) - T[a1, :])
      Rows remain normalized by construction.

    Parameters (bounds):
    - eta_r in [0,1]: mean update rate for alien payouts (Kalman gain).
    - tau_T in [0,1]: transition learning rate for the first-stage mapping.
    - xi_ucb in [0,1]: weight of uncertainty bonus in UCB values.
    - z_forget in [0,1]: process noise added to uncertainty each trial (drifting rewards).
    - beta in [0,10]: inverse temperature for softmax at both stages.

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_r, tau_T, xi_ucb, z_forget, beta]

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_r, tau_T, xi_ucb, z_forget, beta = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Transition model initialized to common mapping (rows sum to 1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Reward belief: means and variances per state-alien
    m = np.zeros((2, 2))
    v = np.ones((2, 2)) * 0.25  # initial moderate uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Prediction step: uncertainty grows slightly each trial for all aliens
        v = v + z_forget

        # UCB values for second-stage choices in both states
        std = np.sqrt(np.maximum(v, 0.0))
        UCB = m + xi_ucb * std

        # First-stage MB values: expectation over learned transitions of max UCB at each planet
        max_ucb = np.max(UCB, axis=1)  # per state
        Q1_mb = T @ max_ucb

        # First-stage policy
        logits1 = beta * (Q1_mb - np.max(Q1_mb))
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy in realized state s using UCB values
        logits2 = beta * (UCB[s] - np.max(UCB[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Observation update for the chosen alien
        # Mean update (Kalman-like)
        m[s, a2] += eta_r * (r - m[s, a2])
        # Uncertainty shrinks after observation
        v[s, a2] *= (1.0 - eta_r)

        # Update transition model toward observed state for the chosen first-stage action
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = T[a1, :] + tau_T * (oh - T[a1, :])
        # Numerical stability: re-normalize row (should already sum to 1)
        T[a1, :] = T[a1, :] / (np.sum(T[a1, :]) + eps)

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss