def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Hybrid stage-1 (MB+MF) with asymmetric transition learning (common vs rare).
    
    Summary
    -------
    - Stage 2: Model-free Q-learning of alien values.
    - Stage 1: Hybrid of model-based planning using a learned transition model T(a -> state)
      and model-free TD values for the spaceships.
    - Transition learning uses two learning rates: one for "common" (as currently believed)
      transitions and another for "rare" transitions, allowing asymmetric adaptation.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=Spaceship A, 1=Spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0=Planet X, 1=Planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 for the two aliens on the planet).
    reward : array-like of float (0 or 1)
        Binary reward outcome per trial.
    model_parameters : iterable of floats
        [lr2, beta, lrT_plus, lrT_minus, w_hyb]
        - lr2 in [0,1]: learning rate for both stage-2 Q updates and stage-1 MF TD updates.
        - beta in [0,10]: inverse temperature for both stages' softmax policies.
        - lrT_plus in [0,1]: transition learning rate when the observed transition
          matches the currently more-probable (common) branch for the chosen action.
        - lrT_minus in [0,1]: transition learning rate when the observed transition
          is currently the less-probable (rare) branch for the chosen action.
        - w_hyb in [0,1]: weight on model-based value at stage 1 (1=fully MB, 0=fully MF).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr2, beta, lrT_plus, lrT_minus, w_hyb = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model T[a, s] and Q-values
    T = np.full((2, 2), 0.5)  # start with uncertainty about transitions
    q1_mf = np.zeros(2)       # stage-1 model-free values
    q2 = np.zeros((2, 2))     # stage-2 (state, action) values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy: Hybrid of MB and MF using learned T
        V_states = np.max(q2, axis=1)       # value of each planet by best alien
        q1_mb = T @ V_states                # expected value of each spaceship
        q1_hyb = w_hyb * q1_mb + (1.0 - w_hyb) * q1_mf

        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Transition learning: update only the chosen action's row toward the observed state
        # Determine whether the observed transition was "common" under current T
        row = T[a1, :]
        common_state = 0 if row[0] >= row[1] else 1
        is_common = (s == common_state)
        lrT = lrT_plus if is_common else lrT_minus
        target = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] += lrT * (target - T[a1, :])
        # Ensure probabilities remain valid numerically
        T[a1, :] = np.clip(T[a1, :], 1e-6, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

        # Value learning
        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr2 * pe2

        # Stage-1 MF TD update bootstrapping on reached state's chosen action value
        # (uses the immediate value at stage 2 as the target)
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += lr2 * pe1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Model-free with eligibility trace, surprise-gated learning, and risk-sensitive utility.
    
    Summary
    -------
    - Stage 2: Model-free Q-learning of alien values with the learning rate gated by outcome surprise.
    - Stage 1: Model-free values updated via an eligibility trace (lambda) using the stage-2 prediction error.
    - Rewards are passed through a risk-sensitive utility u(r) = r^rho (rho in [0,1]).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcomes per trial.
    model_parameters : iterable of floats
        [lr, beta, lam, surprise_gain, rho]
        - lr in [0,1]: base learning rate for Q-value updates.
        - beta in [0,10]: inverse temperature for both stages.
        - lam in [0,1]: eligibility trace parameter transferring PE2 to stage-1 update.
        - surprise_gain in [0,1]: scales how much absolute PE2 amplifies the learning rate.
        - rho in [0,1]: risk sensitivity for utility; lower values compress rewards (risk-averse).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, beta, lam, surprise_gain, rho = model_parameters
    n_trials = len(action_1)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Policies
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Risk-sensitive utility
        # Handle 0**0 if rho=0 and r=0 by defining 0**0 -> 0
        if r == 0.0 and rho == 0.0:
            u = 0.0
        else:
            u = r ** max(rho, 1e-8)

        # Stage-2 update with surprise-gated learning rate
        pe2 = u - q2[s, a2]
        lr_eff = lr * (1.0 + surprise_gain * abs(pe2))
        lr_eff = min(max(lr_eff, 0.0), 1.0)  # keep within [0,1]
        q2[s, a2] += lr_eff * pe2

        # Stage-1 eligibility trace update using stage-2 PE
        q1[a1] += lr * lam * pe2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Uncertainty-adaptive temperature with forgetting and model-based stage-1.
    
    Summary
    -------
    - Stage 2: Model-free Q-learning with decay (forgetting).
    - Temperature adaptation: The inverse temperature is increased when second-stage policies
      are more certain (low entropy) and decreased when they are uncertain (high entropy).
      The same adaptive temperature is applied to both stages each trial.
    - Stage 1: Model-based planning using a fixed transition structure (common=0.7, rare=0.3),
      plus a perseveration bias toward repeating the previous spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Binary reward outcomes per trial.
    model_parameters : iterable of floats
        [lr2, beta_base, decay, temp_slope, stick1]
        - lr2 in [0,1]: learning rate for stage-2 Q-updates.
        - beta_base in [0,10]: base inverse temperature before uncertainty modulation.
        - decay in [0,1]: per-trial forgetting applied to all stage-2 Q-values.
        - temp_slope in [0,1]: strength of uncertainty-to-temperature modulation.
        - stick1 in [0,1]: perseveration bias added to the previously chosen stage-1 action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr2, beta_base, decay, temp_slope, stick1 = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix (A->X common, U->Y common)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])
    q2 = np.zeros((2, 2))  # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply forgetting to stage-2 values before action selection and learning
        q2 *= (1.0 - decay)

        # Compute uncertainty via entropy of second-stage policies (averaged across states)
        # Using base temperature to derive a baseline softmax distribution
        entropies = np.zeros(2)
        for ss in range(2):
            logits_tmp = beta_base * q2[ss, :]
            logits_tmp -= np.max(logits_tmp)
            p_tmp = np.exp(logits_tmp) / np.sum(np.exp(logits_tmp))
            # Entropy for 2 actions
            eps_h = 1e-12
            entropies[ss] = -np.sum(p_tmp * np.log(p_tmp + eps_h))
        H = np.mean(entropies)
        H_max = np.log(2.0)
        uncertainty_factor = 1.0 - (H / H_max)  # 0 when max uncertainty, 1 when deterministic
        beta_adapt = beta_base * (1.0 + temp_slope * uncertainty_factor)

        # Stage-2 policy with adaptive temperature
        logits2 = beta_adapt * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-1 model-based value using fixed transitions and current q2
        V_states = np.max(q2, axis=1)
        q1_mb = T_fixed @ V_states

        # Add perseveration bias toward previous a1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick1

        logits1 = beta_adapt * q1_mb + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr2 * pe2

        prev_a1 = a1

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss