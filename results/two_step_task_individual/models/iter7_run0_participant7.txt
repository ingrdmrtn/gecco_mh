def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Hybrid MF/MB with learned transitions and surprise-gated arbitration.
    
    This model learns both second-stage Q-values (model-free) and the first-stage
    transition model. First-stage choice values are a convex combination of
    model-based (learned transition matrix times second-stage state values) and
    model-free (SARSA) values. The arbitration weight is dynamically gated by
    recent transition surprise: higher surprise shifts control toward the model-based
    system. Surprise is computed from the previous trial’s transition prediction error,
    and used on the current trial (avoids post-choice leakage).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens 0/1 within planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_v, alpha_tr, beta, w_mb, shock]
        - alpha_v  [0,1]: learning rate for value updates (both stages and MF Q1)
        - alpha_tr [0,1]: learning rate for transition matrix learning
        - beta     [0,10]: inverse temperature used at both stages
        - w_mb     [0,1]: baseline weight for model-based control (transformed via logit)
        - shock    [0,1]: sensitivity of arbitration to recent surprise (higher -> more MB after surprise)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_v, alpha_tr, beta, w_mb, shock = model_parameters
    n_trials = len(action_1)

    # Probabilities of the chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize values
    q2 = np.zeros((2, 2))   # second-stage Q-values: q2[state, action]
    q1_mf = np.zeros(2)     # first-stage model-free values

    # Initialize learned transition model T[a, s]; start agnostic (0.5/0.5)
    T = np.full((2, 2), 0.5)

    # Keep a running surprise signal from previous trial (used to gate arbitration at t)
    # Initialize to 0.5 (moderate surprise)
    last_surprise = 0.5

    # Helper: stable softmax
    def softmax(logits):
        m = np.max(logits)
        z = np.exp(logits - m)
        return z / np.sum(z)

    # Map baseline w_mb in [0,1] onto logit space intercept
    # We use: weight_t = sigmoid(logit(w_mb) + shock * (last_surprise - 0.5) * 6)
    # The scaling 6 expands the dynamic range of surprise gating.
    tiny = 1e-8
    logit_w0 = np.log((w_mb + tiny) / (1 - w_mb + tiny))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute arbitration weight from last trial's surprise
        arb_input = logit_w0 + (2.0 * last_surprise - 1.0) * (6.0 * shock)
        w_t = 1.0 / (1.0 + np.exp(-arb_input))  # in (0,1)

        # Model-based first-stage values from learned transitions and current V2
        V2 = np.max(q2, axis=1)         # value of each second-stage state
        q1_mb = T @ V2                  # expected value for each first-stage action

        # Mixture for first-stage policy
        q1_mix = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # First-stage choice probability
        probs1 = softmax(beta * q1_mix)
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability
        probs2 = softmax(beta * q2[s, :])
        p_choice_2[t] = probs2[a2]

        # Learning updates

        # Second-stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_v * delta2

        # First-stage model-free SARSA-style update toward actual Q2 value
        target1_mf = q2[s, a2]
        q1_mf[a1] += alpha_v * (target1_mf - q1_mf[a1])

        # Transition model update for chosen action: prior prob for observed state
        p_obs_prev = T[a1, s]
        # Exponential recency-weighted update toward one-hot(s)
        T[a1, :] = (1.0 - alpha_tr) * T[a1, :]
        T[a1, s] += alpha_tr
        # Normalize defensively (should already sum to 1)
        T[a1, :] /= np.sum(T[a1, :])

        # Compute surprise for use on next trial (higher = more unexpected transition)
        last_surprise = 1.0 - p_obs_prev

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Risk-sensitive MF with entropy-adaptive exploration and perseveration.
    
    This model is purely model-free but includes:
    - Risk-sensitive utility transform on rewards (concave/convex via 'risk').
    - Adaptive exploration: the effective inverse temperature is reduced when
      the recent policy entropy is high (uncertainty), controlled by 'ent'.
    - Perseveration: bias to repeat the most recent action at each stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens 0/1 within planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, risk, ent, persev]
        - alpha   [0,1]: learning rate for MF updates (both stages and first-stage SARSA)
        - beta    [0,10]: base inverse temperature (scaled by entropy adaptively)
        - risk    [0,1]: risk sensitivity exponent in utility u(r)=sign(r)*|r|^risk
        - ent     [0,1]: strength of entropy-based temperature reduction
        - persev  [0,1]: perseveration bias added to the previously chosen action’s logit at each stage
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, risk, ent, persev = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)       # first-stage MF Q-values
    q2 = np.zeros((2, 2))  # second-stage MF Q-values

    # Track last choices for perseveration
    last_a1 = -1
    last_a2 = {-1: -1, 0: -1, 1: -1}

    # Track recent policy entropy (natural units)
    ln2 = np.log(2.0)
    H1_prev = 0.5 * ln2  # initialize with moderate entropy
    H2_prev = {0: 0.5 * ln2, 1: 0.5 * ln2}

    def softmax_with_bias(values, beta_eff, prev_action):
        logits = beta_eff * values
        if prev_action in (0, 1):
            logits[prev_action] += persev
        m = np.max(logits)
        z = np.exp(logits - m)
        probs = z / np.sum(z)
        # entropy of the resulting policy (for next trial's adaptation)
        ent = -np.sum(probs * np.log(probs + 1e-12))
        return probs, ent

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Adaptive temperatures based on previous entropies
        beta1_eff = beta * max(1e-3, 1.0 - ent * (H1_prev / ln2))
        beta2_eff = beta * max(1e-3, 1.0 - ent * (H2_prev.get(s, 0.5 * ln2) / ln2))

        # Stage-2 policy and likelihood
        probs2, H2_now = softmax_with_bias(q2[s, :].copy(), beta2_eff, last_a2.get(s, -1))
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy and likelihood
        probs1, H1_now = softmax_with_bias(q1.copy(), beta1_eff, last_a1)
        p_choice_1[t] = probs1[a1]

        # Risk-sensitive utility transform
        u = np.sign(r) * (np.abs(r) ** risk)

        # Learning updates
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # First-stage SARSA toward second-stage Q of actually chosen action
        target1 = q2[s, a2]
        q1[a1] += alpha * (target1 - q1[a1])

        # Update trackers for next trial
        last_a1 = a1
        last_a2[s] = a2
        H1_prev = H1_now
        H2_prev[s] = H2_now

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Successor-representation and model-free hybrid with eligibility.
    
    This model approximates model-based control at the first stage using a
    learned one-step successor representation over second-stage states (equivalent
    to a learned transition model) to evaluate actions via V2. It blends this SR
    estimate with a model-free first-stage value. An eligibility parameter allows
    reward prediction errors to directly influence first-stage MF values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage state observed (planet X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens 0/1 within planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, gamma, trace, rep]
        - alpha [0,1]: learning rate for value and SR updates
        - beta  [0,10]: inverse temperature used at both stages
        - gamma [0,1]: weight mixing SR-based and MF first-stage values (also discount in SR evaluation)
        - trace [0,1]: eligibility weight on first-stage MF credit assignment from reward
                       (trace=0 => update toward Q2; trace=1 => direct toward reward)
        - rep   [0,1]: perseveration bias added to the previously chosen first-stage action’s logit
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, gamma, trace, rep = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q2 = np.zeros((2, 2))   # second-stage Q-values
    q1_mf = np.zeros(2)     # first-stage MF Q-values

    # Successor representation over next states for each first-stage action
    # Here, with a two-step task, SR reduces to learning the transition distribution.
    M = np.full((2, 2), 0.5)  # initialize agnostic

    last_a1 = -1  # for perseveration

    def softmax(logits):
        m = np.max(logits)
        z = np.exp(logits - m)
        return z / np.sum(z)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute SR-evaluated first-stage action values via V2
        V2 = np.max(q2, axis=1)
        q1_sr = M @ (gamma * V2)  # evaluation uses gamma as a discount-like weight

        # Blend SR and MF for choice
        q1_mix = gamma * q1_sr + (1.0 - gamma) * q1_mf

        # Perseveration bias on first-stage logits
        logits1 = beta * q1_mix
        if last_a1 in (0, 1):
            logits1[last_a1] += rep
        probs1 = softmax(logits1)
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probabilities
        probs2 = softmax(beta * q2[s, :])
        p_choice_2[t] = probs2[a2]

        # Learning updates

        # Second stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # First stage MF update with eligibility mixing:
        # combine drive toward immediate reward (trace) and toward Q2 (1-trace)
        target_mf = (1.0 - trace) * q2[s, a2] + trace * r
        q1_mf[a1] += alpha * (target_mf - q1_mf[a1])

        # SR update for chosen action toward one-hot of observed state
        one_hot = np.array([1.0 if i == s else 0.0 for i in range(2)])
        M[a1, :] += alpha * (one_hot - M[a1, :])
        # Normalize defensively
        row_sum = np.sum(M[a1, :])
        if row_sum > 0:
            M[a1, :] /= row_sum

        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll