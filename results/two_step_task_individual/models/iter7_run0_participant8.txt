def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Dynamic-exploration hybrid: model-based/model-free arbitration with reward-surprise-modulated inverse temperature
    and first-stage choice stickiness.

    Mechanism
    - Second-stage values are learned model-free via a delta rule.
    - First-stage has both model-free (bootstrapped from reached second-stage action) and model-based components
      (computed via a known transition structure), mixed via mb_ratio.
    - The first-stage softmax inverse temperature is dynamically increased on trials following large unsigned
      reward prediction errors (surprise) from the second stage: beta_t = beta0 * (1 + beta_gain * UPE_prev),
      encouraging exploitation after surprising outcomes.
    - A first-stage stickiness bias favors repeating the previous first-stage action.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices per trial within the reached state (0/1).
    reward : array-like of float in {0,1}
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta0, beta_gain, mb_ratio, rep_s1]
        - alpha in [0,1]: learning rate for Q updates (both stages and first-stage bootstrapping).
        - beta0 in [0,10]: baseline inverse temperature for softmax.
        - beta_gain in [0,1]: modulation strength of beta by previous unsigned PE at stage 2.
        - mb_ratio in [0,1]: mixture weight of model-based value at stage 1 (1=fully MB).
        - rep_s1 in [0,1]: stickiness bonus added to the last first-stage action's preference.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta0, beta_gain, mb_ratio, rep_s1 = model_parameters
    n_trials = len(action_1)

    # Known transition structure (common transitions 0.7)
    T_known = np.array([[0.7, 0.3],   # A -> X (0.7), Y (0.3)
                        [0.3, 0.7]])  # U -> X (0.3), Y (0.7)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)        # model-free first-stage Q
    q2 = np.zeros((2, 2))      # second-stage Q

    # Stickiness tracking
    prev_a1 = -1

    # Surprise memory for beta modulation
    prev_upe = 0.0

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage values
        v2 = np.max(q2, axis=1)          # value of each second-stage state
        q1_mb = T_known @ v2

        # Hybrid arbitration
        q1 = mb_ratio * q1_mb + (1.0 - mb_ratio) * q1_mf

        # First-stage preference with stickiness
        pref1 = q1.copy()
        if prev_a1 in (0, 1):
            pref1[prev_a1] += rep_s1

        # Dynamic inverse temperature at stage 1
        beta1 = beta0 * (1.0 + beta_gain * np.clip(prev_upe, 0.0, 1.0))

        # Softmax for stage 1
        pref1_stable = pref1 - np.max(pref1)
        pr1 = np.exp(beta1 * pref1_stable)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        # Stage-2 softmax with baseline beta
        pref2 = q2[s, :].copy()
        pref2 -= np.max(pref2)
        pr2 = np.exp(beta0 * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]

        # Learning updates
        # Stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 model-free bootstrapping from experienced second-stage action value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update surprise memory for next trial's beta
        prev_upe = abs(pe2)
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Average-reward RL with transition-structure bias and second-stage perseveration.

    Mechanism
    - Tracks an average reward rate rho and learns Q-values using reward minus opportunity cost (r - rho).
      This favors actions that improve long-run reward rate.
    - First-stage uses model-free values bootstrapped from reached second-stage action values (also adjusted by r - rho).
    - Adds an action-specific bias at stage 1 toward actions that historically produced common transitions (common_bonus).
      For each first-stage action, we remember whether its last experienced transition type was common vs rare, and bias toward
      actions with a "common" flag.
    - Second-stage includes within-state choice perseveration.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices per trial within the reached state (0/1).
    reward : array-like of float in {0,1}
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, lr_rho, common_bonus, stick_s2]
        - alpha in [0,1]: learning rate for Q updates (both stages and first-stage bootstrapping).
        - beta in [0,10]: inverse temperature for both stages.
        - lr_rho in [0,1]: learning rate for tracking average reward rho.
        - common_bonus in [0,1]: additive bias at stage 1 for actions whose last observed transition was common.
        - stick_s2 in [0,1]: within-state perseveration bonus added to the previously chosen second-stage action.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, lr_rho, common_bonus, stick_s2 = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Average reward rate
    rho = 0.0

    # Track whether the last transition for each first-stage action was common (1) or rare (0)
    last_common_flag = np.array([0.5, 0.5])  # start neutral; contributes half bias initially

    # Second-stage perseveration: remember last action per state
    prev_a2 = np.array([-1, -1])

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 preferences from model-free values
        pref1 = q1_mf.copy()

        # Add common-transition bias per action
        pref1 += common_bonus * last_common_flag

        # Softmax for stage 1
        pref1_stable = pref1 - np.max(pref1)
        pr1 = np.exp(beta * pref1_stable)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        # Stage-2 preferences with within-state perseveration
        pref2 = q2[s, :].copy()
        if prev_a2[s] in (0, 1):
            pref2[prev_a2[s]] += stick_s2

        pref2 -= np.max(pref2)
        pr2 = np.exp(beta * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]

        # Learning with average-reward opportunity cost
        # Update rho
        rho += lr_rho * (r - rho)

        # Stage 2 update uses adjusted reward
        pe2 = (r - rho) - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF bootstrapping from reached second-stage action value, adjusted by rho
        target1 = q2[s, a2]  # already reflects (r - rho)
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update common/rare flag for the chosen first-stage action based on observed transition
        # Common if (A->X) or (U->Y)
        is_common = 1.0 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0.0
        # Exponential moving average to stabilize the flag
        last_common_flag[a1] = 0.7 * last_common_flag[a1] + 0.3 * is_common

        # Update second-stage perseveration memory
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Transition-kernel planning with pruning and volatility-gated arbitration.

    Mechanism
    - Learns a per-action transition kernel T via a delta rule.
    - Computes a model-based (MB) first-stage value by projecting through T onto max second-stage values,
      but prunes very unlikely transitions: any transition probability below 'thresh' is set to zero
      and the remainder is renormalized (if any mass remains). This approximates limited-depth planning
      that ignores implausible branches.
    - Also learns model-free (MF) first-stage values by bootstrapping from reached second-stage values.
    - Tracks outcome volatility v_t (EMA of absolute reward change) with vol_lr, and uses it to gate arbitration:
      MB/MF mixture weight w_t = v_t (higher volatility favors MB control), updated online.
    - Second-stage values are learned via a standard delta rule.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices per trial within the reached state (0/1).
    reward : array-like of float in {0,1}
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, tau_T, thresh, vol_lr]
        - alpha in [0,1]: learning rate for Q updates (both stages and first-stage bootstrapping).
        - beta in [0,10]: inverse temperature for both stages.
        - tau_T in [0,1]: learning rate for updating the transition kernel rows T[action, :].
        - thresh in [0,1]: pruning threshold for transition probabilities in model-based planning.
        - vol_lr in [0,1]: learning rate for volatility estimation that gates MB/MF arbitration (w_t = volatility).

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, tau_T, thresh, vol_lr = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize learned transition kernel T (rows sum to 1)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Volatility tracker
    vol = 0.0
    prev_r = 0.0

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based value with pruning
        v2 = np.max(q2, axis=1)
        q1_mb = np.zeros(2)
        for a in range(2):
            p = T[a].copy()
            # Prune low-probability branches
            p_pruned = np.where(p >= thresh, p, 0.0)
            mass = np.sum(p_pruned)
            if mass > 1e-12:
                p_pruned = p_pruned / mass
                q1_mb[a] = np.dot(p_pruned, v2)
            else:
                # If everything pruned, fall back to unpruned expectation
                q1_mb[a] = np.dot(p, v2)

        # Volatility-gated arbitration weight
        w = np.clip(vol, 0.0, 1.0)
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 softmax
        pref1 = q1 - np.max(q1)
        pr1 = np.exp(beta * pref1)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        # Stage-2 softmax
        pref2 = q2[s, :] - np.max(q2[s, :])
        pr2 = np.exp(beta * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]

        # Learning
        # Stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF bootstrapping
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update transition kernel T for the chosen action
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T[a1, :] = (1.0 - tau_T) * T[a1, :] + tau_T * target_row
        # Ensure numerical stability: renormalize the updated row
        T[a1, :] = T[a1, :] / (np.sum(T[a1, :]) + 1e-12)

        # Update volatility (EMA of absolute reward change)
        dr = abs(r - prev_r)
        vol += vol_lr * (dr - vol)
        prev_r = r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll