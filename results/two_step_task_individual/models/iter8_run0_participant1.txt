def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Model-based planning with value decay and stage-specific lapses.
    
    This model plans at stage 1 using the known transition structure and
    second-stage action values (Q2). Q2 values undergo forgetting/decay toward
    a neutral prior each trial. Both stages include independent lapse probabilities
    that mix the softmax policy with uniform random choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alphaR, beta, tau, xi1, xi2]
        - alphaR (second-stage learning rate, [0,1]): TD update of Q2.
        - beta (inverse temperature, [0,10]): softmax inverse temperature for both stages.
        - tau (decay toward 0.5, [0,1]): per-trial decay of all Q2 entries toward 0.5.
        - xi1 (stage-1 lapse, [0,1]): probability of random stage-1 choice (uniform mixing).
        - xi2 (stage-2 lapse, [0,1]): probability of random stage-2 choice (uniform mixing).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alphaR, beta, tau, xi1, xi2 = model_parameters
    n_trials = len(action_1)

    # Fixed, known transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q2 = np.full((2, 2), 0.5)  # initialize to neutral prior

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Decay Q2 toward 0.5 prior
        q2 = (1.0 - tau) * q2 + tau * 0.5

        # Stage-1 model-based values: expected max Q2 under transitions
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T @ max_q2

        # Stage-1 policy with lapse
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        probs1 = (1.0 - xi1) * probs1 + xi1 * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapse
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        probs2 = (1.0 - xi2) * probs2 + xi2 * 0.5
        p_choice_2[t] = probs2[a2]

        # TD update at stage 2
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alphaR * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Hybrid MF/MB with eligibility and surprise-gated arbitration.
    
    This model blends model-free and model-based values at stage 1. The weight
    on model-based control is adaptively increased on surprising transitions
    (i.e., rare transitions), based on a gating sensitivity. A simple eligibility
    trace mixes immediate second-stage value and obtained reward in the MF update.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, lam, kappaS, w_base]
        - alpha (learning rate, [0,1]): shared TD learning rate.
        - beta (inverse temperature, [0,10]): softmax inverse temperature for both stages.
        - lam (eligibility mixing, [0,1]): mixes bootstrapped value and outcome in MF update.
        - kappaS (surprise sensitivity, [0,1]): scales arbitration increase on surprising transitions.
        - w_base (base MB weight, [0,1]): baseline weight on model-based control at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha, beta, lam, kappaS, w_base = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q2 = np.zeros((2, 2))        # stage-2 MF values
    q1_mf = np.zeros(2)          # stage-1 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 model-based values from Q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Surprise signal based on transition likelihood
        # surpr = 1 - P(s2 | a1)
        surpr = 1.0 - T[a1, s2]
        w_t = w_base + kappaS * surpr
        w_t = min(1.0, max(0.0, w_t))  # clamp to [0,1]

        # Hybrid Q for stage 1
        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD update
        v2_old = q2[s2, a2]
        delta2 = r - v2_old
        q2[s2, a2] += alpha * delta2

        # Stage-1 MF update with eligibility: target blends bootstrapped value and outcome
        target1 = (1.0 - lam) * v2_old + lam * r
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: MB planner with dual temperatures and WSLS bias; flexible transition belief.
    
    Stage 1 uses a model-based planner with a subjective common-transition strength.
    Action selection temperatures are allowed to differ across stages. A Win-Stay/
    Lose-Shift (WSLS) bias pushes the agent to repeat the previous stage-1 choice
    after reward and avoid it after no reward.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta1, beta2, wsls, ccommon]
        - alpha (learning rate, [0,1]): TD update of Q2.
        - beta1 (stage-1 inverse temperature, [0,10]): softmax temp at stage 1.
        - beta2 (stage-2 inverse temperature, [0,10]): softmax temp at stage 2.
        - wsls (WSLS bias magnitude, [0,1]): additive bias on previous stage-1 choice;
          added if last reward=1 and subtracted if last reward=0.
        - ccommon (subjective commonness, [0,1]): maps to common prob c = 0.5 + 0.5*ccommon.
          c=0.7 when ccommon=0.4; c=1 when ccommon=1; c=0.5 when ccommon=0.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha, beta1, beta2, wsls, ccommon = model_parameters
    n_trials = len(action_1)

    # Subjective transition matrix
    c = 0.5 + 0.5 * ccommon
    T = np.array([[c, 1.0 - c],
                  [1.0 - c, c]], dtype=float)

    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_r = None

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 MB values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # WSLS bias on stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None and last_r is not None:
            sign = 1.0 if last_r > 0 else -1.0
            bias1[last_a1] += wsls * sign

        # Stage-1 policy
        logits1 = beta1 * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta2 * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning update at stage 2
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # Carry state for next WSLS bias
        last_a1 = a1
        last_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll