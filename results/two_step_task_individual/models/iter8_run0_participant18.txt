Below are three standalone cognitive models expressed as Python functions. Each function follows the requested signature, uses all parameters, and returns the negative log-likelihood of the observed choices. Each model embodies a different computational mechanism.

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Transition-learning model-based control with entropy-driven exploration and action bias.
    
    Summary
    -------
    - Learns individual transition probabilities T(a1 -> s) with a dedicated learning rate.
    - Learns second-stage MF Q-values with a separate learning rate.
    - First-stage decisions are model-based using learned transitions and current second-stage values.
    - Adds an entropy-driven exploration bonus favoring actions with more uncertain transition models.
    - Includes a stable bias toward spaceship A.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet: 0/1.
    reward : array-like of float
        Coins received on each trial.
    model_parameters : sequence
        [alpha_T, alpha_Q, beta, xi_ent, bA]
        - alpha_T (transition learning rate, [0,1]): updates T(a1 -> s).
        - alpha_Q (value learning rate, [0,1]): updates second-stage Q-values.
        - beta (inverse temperature, [0,10]): softmax sensitivity at both stages.
        - xi_ent (entropy bonus weight, [0,1]): adds entropy(T[a]) to first-stage values to drive exploration.
        - bA (action bias toward spaceship A, [0,1]): value bonus added to A at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_T, alpha_Q, beta, xi_ent, bA = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition probabilities; start from neutral 0.5/0.5
    T = np.ones((2, 2)) * 0.5  # rows: a1 in {A,U}, cols: s in {X,Y}

    # Second-stage MF Q-values: q2[s, a2]
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute model-based Q for stage 1 using learned transitions and current second-stage values
        max_q2 = np.max(q2, axis=1)  # best value on each planet
        q1_mb = T @ max_q2  # MB evaluation using learned T

        # Entropy bonus for transition uncertainty (higher entropy => more exploration)
        # H(T[a]) = -sum_s T[a,s] * log T[a,s]
        H = np.zeros(2)
        for a in range(2):
            Ta = np.clip(T[a], eps, 1.0)
            H[a] = -np.sum(Ta * np.log(Ta))
        # Add bias toward spaceship A (action 0)
        bias_vec = np.array([bA, 0.0])

        v1 = q1_mb + xi_ent * H + bias_vec
        v1 = v1 - np.max(v1)
        probs1 = np.exp(beta * v1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second stage policy
        v2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * v2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Update transitions for chosen a1 based on observed s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += alpha_T * (target - T[a1])
        # Normalize to guard against numerical drift (optional but stable)
        T[a1] = np.clip(T[a1], eps, 1.0)
        T[a1] = T[a1] / np.sum(T[a1])

        # Second-stage MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_Q * delta2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: MF two-stage learner with surprise-adaptive temperature, stay bias, and value forgetting.
    
    Summary
    -------
    - Purely model-free learning at both stages (SARSA(1)-like for stage-1 via stage-2 values).
    - Inverse temperature beta is adapted on each trial based on surprise:
        surprise = average of transition surprise and reward surprise.
      Effective beta_t = beta0 * (1 + lambda_beta * surprise).
    - Stay bias at stage 1 encourages repeating the previous first-stage choice.
    - Global forgetting pulls all second-stage Q-values toward 0.5.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet: 0/1.
    reward : array-like of float
        Coins received on each trial.
    model_parameters : sequence
        [alpha, beta0, kappa_stay1, lambda_beta, eta_forget]
        - alpha (learning rate, [0,1]): MF learning rate for Q-values (both stages).
        - beta0 (base inverse temperature, [0,10]): baseline softmax sensitivity.
        - kappa_stay1 (stay bias weight, [0,1]): additive value bonus for repeating last stage-1 action.
        - lambda_beta (surprise sensitivity, [0,1]): scales how much surprise increases beta.
        - eta_forget (forgetting rate, [0,1]): per-trial decay of all q2 toward 0.5.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta0, kappa_stay1, lambda_beta, eta_forget = model_parameters
    n_trials = len(action_1)

    # MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # For stay bias at stage 1
    last_a1 = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute surprise to adapt beta
        # Transition structure: common 0.7 for A->X and U->Y; rare 0.3 otherwise
        if (a1 == 0 and s == 0) or (a1 == 1 and s == 1):
            p_trans = 0.7
        else:
            p_trans = 0.3
        trans_surprise = 1.0 - p_trans  # in [0,1]

        rew_surprise = abs(r - q2[s, a2])  # in [0,1] if rewards and Qs are bounded

        surprise = 0.5 * trans_surprise + 0.5 * rew_surprise
        beta_t = beta0 * (1.0 + lambda_beta * surprise)

        # Stay bias feature at stage 1
        stay_feat = np.array([0.0, 0.0])
        if last_a1 is not None:
            stay_feat[last_a1] = 1.0

        v1 = q1 + kappa_stay1 * stay_feat
        v1 = v1 - np.max(v1)
        probs1 = np.exp(beta_t * v1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        v2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta_t * v2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # MF updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update toward the obtained stage-2 value (SARSA(1)-like bootstrapping)
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

        # Global forgetting of q2 toward 0.5
        q2 = (1.0 - eta_forget) * q2 + eta_forget * 0.5

        last_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Arbitration between model-based and model-free control via reliability signals.
    
    Summary
    -------
    - Learns second-stage MF Q-values and stage-1 MF values.
    - Uses fixed known transition structure for MB planning (0.7 common).
    - Computes MB reliability from (i) transition determinism (1 - normalized entropy) and (ii)
      whether the observed transition was likely under the common mapping.
    - Computes MF uncertainty from running unsigned reward prediction error.
    - Arbitration weight mu_t is updated via a logistic dynamics toward higher MB reliability
      relative to MF uncertainty.
    - First-stage decision uses mixture of MB and MF values; second-stage remains MF.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet: 0/1.
    reward : array-like of float
        Coins received on each trial.
    model_parameters : sequence
        [alpha, beta, mu0, phi_arbit, zeta_inv]
        - alpha (learning rate, [0,1]): MF learning rate for Q-values (both stages).
        - beta (inverse temperature, [0,10]): softmax sensitivity for both stages.
        - mu0 (initial MB weight, [0,1]): initial mixture weight for MB control at stage 1.
        - phi_arbit (arbitration update rate, [0,1]): step size for updating the mixture weight.
        - zeta_inv (reliability blending, [0,1]): mixes two MB reliability components:
            MB_rel = zeta_inv*(1 - entropy_norm) + (1 - zeta_inv)*p_trans_observed.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, mu0, phi_arbit, zeta_inv = model_parameters
    n_trials = len(action_1)

    # Fixed transition matrix (known task structure)
    # rows: a1 in {A,U}; cols: s in {X,Y}
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Arbitration state: mixture weight mu in [0,1]
    # We maintain it in logit space for smooth updates
    eps = 1e-10
    def logit(p): 
        p = np.clip(p, eps, 1 - eps)
        return np.log(p / (1 - p))
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    mu_logit = logit(mu0)

    # Running MF uncertainty proxy: exp. avg of unsigned RPE
    u_mf = 0.0  # higher => less reliable MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Precompute MB entropy term and normalization
    # Entropy of T_fixed[a]: H = -sum p log p; normalize by log(2)
    Hmax = np.log(2.0)
    H_MB = np.zeros(2)
    for a in range(2):
        Ta = np.clip(T_fixed[a], eps, 1.0)
        H_MB[a] = -np.sum(Ta * np.log(Ta))
    entropy_norm = H_MB / Hmax  # in [0,1]

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based action values at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Current arbitration weight
        mu = sigmoid(mu_logit)
        q1 = mu * q1_mb + (1.0 - mu) * q1_mf

        # Stage 1 choice
        v1 = q1 - np.max(q1)
        probs1 = np.exp(beta * v1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice
        v2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * v2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # MF updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update MF uncertainty (running unsigned RPE)
        u_mf = (1.0 - phi_arbit) * u_mf + phi_arbit * abs(delta2)
        u_mf = np.clip(u_mf, 0.0, 1.0)

        # Compute MB reliability on this trial
        # Transition likelihood of observed state under chosen a1
        p_obs = T_fixed[a1, s]  # 0.7 if common, 0.3 if rare
        # Blend determinism (1 - entropy_norm) and observed likelihood
        MB_rel = zeta_inv * (1.0 - entropy_norm[a1]) + (1.0 - zeta_inv) * p_obs
        MB_rel = np.clip(MB_rel, 0.0, 1.0)

        # Arbitration update in logit space toward MB when MB_rel > (1 - u_mf)
        # Effective target difference: MB_rel - (1 - u_mf) = MB_rel + u_mf - 1
        # We nudge mu_logit by phi_arbit times this signed reliability gap
        gap = MB_rel - (1.0 - u_mf)
        mu_logit += phi_arbit * gap

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll