def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Surprise-gated arbitration between model-based planning and a learned model-free Stage-1 value, with Stage-2 perseveration.

    The agent maintains:
    - Stage-2 Q-values learned from reward (model-free).
    - A Stage-1 model-free value learned by bootstrapping from the observed Stage-2 action value.
    - A fixed transition model for model-based planning.
    Arbitration at Stage 1 is dynamically gated by transition surprise (rarity): higher surprise reduces reliance on MB planning.
    Stage-2 choices include a within-state perseveration bias (tendency to repeat the last second-stage action on that planet).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (number of coins) per trial.
    model_parameters : sequence of floats
        [alpha1_mf, alpha2, beta, gamma_surp, rho2_stay]
        - alpha1_mf in [0,1]: learning rate for Stage-1 model-free values updated from reached Stage-2 value (bootstrapped).
        - alpha2 in [0,1]: learning rate for Stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - gamma_surp in [0,1]: strength of surprise-gated shift away from MB planning (higher => less MB after surprising transitions).
        - rho2_stay in [0,1]: strength of within-state Stage-2 perseveration (bias to repeat last Stage-2 action in that state).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha1_mf, alpha2, beta, gamma_surp, rho2_stay = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: rows = first-stage action, cols = state
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2, dtype=float)          # Stage-1 model-free values for spaceships A/U
    q2 = np.zeros((2, 2), dtype=float) + 0.5  # Stage-2 values for aliens on planets X/Y

    # Last Stage-2 action per state for perseveration (initialize to "none" = -1)
    last_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]

        # Model-based Stage-1 action values: expected max over Stage-2 using fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2  # shape (2,)

        # Surprise computed as (1 - P(state | action1)) in [0.3, 0.7] -> surprise in [0.3, 0.7]
        p_s_given_a = T[a1, s]
        surprise = 1.0 - p_s_given_a  # larger for rarer transition

        # Arbitration weight for MB (no explicit "w"): higher surprise reduces MB reliance
        # Clamp to [0,1]
        w_mb_t = 1.0 - gamma_surp * surprise
        if w_mb_t < 0.0:
            w_mb_t = 0.0
        elif w_mb_t > 1.0:
            w_mb_t = 1.0

        # Blended Stage-1 values
        q1_blend = w_mb_t * q1_mb + (1.0 - w_mb_t) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1_blend - np.max(q1_blend))
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with within-state perseveration
        bias2 = np.zeros(2, dtype=float)
        if last_a2[s] >= 0:
            bias2[last_a2[s]] += rho2_stay
        logits2 = beta * (q2[s] - np.max(q2[s])) + bias2
        p2 = np.exp(logits2 - np.max(logits2))
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 value update (model-free)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage-1 model-free bootstrapped update toward observed Stage-2 chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1_mf * pe1

        # Update perseveration memory
        last_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Transition-learning model-based planner with confidence-weighted planning and Stage-1 perseveration.

    The agent learns its own transition model T_hat by observing which planet follows each spaceship.
    Stage-1 planning uses T_hat and the current Stage-2 values. Planning strength is down-weighted when
    transition confidence is low (high entropy in T_hat). Stage-1 perseveration captures a tendency
    to repeat the previous first-stage choice. Stage-2 values are learned model-free from reward.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (number of coins) per trial.
    model_parameters : sequence of floats
        [alpha2, beta, alpha_T, chi_conf, rho1_stay]
        - alpha2 in [0,1]: learning rate for Stage-2 Q-values.
        - beta in [0,10]: base inverse temperature for softmax at both stages.
        - alpha_T in [0,1]: learning rate for transition matrix T_hat (per chosen first-stage action).
        - chi_conf in [0,1]: confidence-sensitivity; higher values reduce planning when transitions are uncertain.
        - rho1_stay in [0,1]: strength of Stage-1 perseveration (bias to repeat last first-stage action).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, alpha_T, chi_conf, rho1_stay = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model near-uniform (weak prior)
    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float) + 0.5  # Stage-2 values

    last_a1 = -1  # for perseveration

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]

        # Confidence from T_hat entropy (per chosen action's row), scaled to [0,1]
        row = T_hat[a1].copy()
        row = np.clip(row, eps, 1.0)  # avoid log(0)
        row = row / np.sum(row)
        H = -np.sum(row * np.log(row))  # natural log entropy in [0, ln2]
        H_norm = H / np.log(2.0)        # normalize to [0,1]
        conf_scale = 1.0 - chi_conf * H_norm  # down-weight MB when entropy is high

        # Model-based Q1 from T_hat and current q2
        max_q2 = np.max(q2, axis=1)   # value per state
        q1_mb = T_hat @ max_q2        # shape (2,)

        # Stage-1 policy: confidence-weighted MB plus perseveration bias
        logits1 = conf_scale * q1_mb

        if last_a1 >= 0:
            stick = np.zeros(2, dtype=float)
            stick[last_a1] += rho1_stay
            logits1 = logits1 + stick

        l1 = beta * (logits1 - np.max(logits1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (softmax on q2 in reached state)
        l2 = beta * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Update Stage-2 value
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Update transition model T_hat for the chosen action row toward observed state
        target_vec = np.array([0.0, 0.0], dtype=float)
        target_vec[s] = 1.0
        T_hat[a1] = (1.0 - alpha_T) * T_hat[a1] + alpha_T * target_vec
        # Ensure normalization (numerical stability)
        T_hat[a1] = T_hat[a1] / (np.sum(T_hat[a1]) + eps)

        # Update perseveration memory
        last_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Surprise-triggered partial forgetting at Stage 2 and motivation-scaled planning temperature.

    The agent plans at Stage 1 using the fixed transition model. Motivation (derived from overall
    value availability) scales the Stage-1 softmax temperature, making choices more decisive when
    rewards are plentiful. After rare transitions, the agent partially resets (forgets) the reached
    state's Stage-2 values toward a neutral baseline before choosing, capturing surprise-induced
    disorientation. Stage-2 choices include a within-state perseveration bias.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (number of coins) per trial.
    model_parameters : sequence of floats
        [alpha2, beta, reset_rate, kappa_mot, rho2_stay]
        - alpha2 in [0,1]: learning rate for Stage-2 Q-values.
        - beta in [0,10]: base inverse temperature for softmax at both stages.
        - reset_rate in [0,1]: fraction by which Stage-2 values on the reached state are pulled toward 0.5 upon rare transitions (before choice).
        - kappa_mot in [0,1]: motivation sensitivity; scales Stage-1 beta by (1 + kappa_mot * mean(max_q2)).
        - rho2_stay in [0,1]: within-state Stage-2 perseveration strength.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, reset_rate, kappa_mot, rho2_stay = model_parameters
    n_trials = len(action_1)

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float) + 0.5  # Stage-2 values

    last_a2 = np.array([-1, -1], dtype=int)   # last action per state

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]

        # Compute MB Stage-1 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Motivation scaling based on average available value
        mean_max_q2 = 0.5 * (max_q2[0] + max_q2[1])
        beta1_eff = beta * (1.0 + kappa_mot * mean_max_q2)

        # Stage-1 policy
        l1 = beta1_eff * (q1_mb - np.max(q1_mb))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Determine if the transition is rare given chosen a1
        p_common = 0.7
        is_rare = ((a1 == 0 and s == 1) or (a1 == 1 and s == 0))

        # Surprise-triggered partial forgetting on reached state BEFORE Stage-2 choice
        if is_rare and reset_rate > 0.0:
            q2[s] = (1.0 - reset_rate) * q2[s] + reset_rate * 0.5

        # Stage-2 policy with within-state perseveration
        bias2 = np.zeros(2, dtype=float)
        if last_a2[s] >= 0:
            bias2[last_a2[s]] += rho2_stay
        l2 = beta * (q2[s] - np.max(q2[s])) + bias2
        p2 = np.exp(l2 - np.max(l2))
        p2 = p2 / (np.sum(p2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Update perseveration memory
        last_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll