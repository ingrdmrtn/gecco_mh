def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid model-based/model-free controller with learned transitions and uncertainty-driven exploration.

    This model learns second-stage Q-values model-free, learns the first-stage transition structure online
    from Dirichlet counts, and arbitrates between model-based and model-free action values at the first stage.
    Additionally, it adds an exploration bonus at the first stage that scales with each actionâ€™s transition
    uncertainty (entropy of the learned transition distribution).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received.
    model_parameters : iterable of floats
        [alpha, beta, omega_map, psi_bonus, kappa_dir]
        - alpha (0..1): Learning rate for model-free Q-learning at the second stage and TD backup to stage 1.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - omega_map (0..1): Weight of model-based value in the first-stage mixture with model-free value.
        - psi_bonus (0..1): Scale of transition-uncertainty (entropy) bonus added to first-stage action preferences.
        - kappa_dir (0..1): Strength of the Dirichlet prior added to transition counts (stabilizes early estimates).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega_map, psi_bonus, kappa_dir = model_parameters
    n_trials = len(action_1)

    # Storage for choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)          # first-stage MF values for actions A/U
    q2 = np.zeros((2, 2))        # second-stage Q-values per planet (X/Y) and alien (0/1)

    # Transition learning via Dirichlet counts (2 actions x 2 next states)
    # Initialize with symmetric prior kappa_dir over the two outcomes
    trans_counts = np.full((2, 2), kappa_dir)  # rows: actions, cols: next states
    # Helper to compute transition probabilities and entropies
    def get_T_and_entropy(counts):
        T = counts / np.clip(counts.sum(axis=1, keepdims=True), 1e-12, None)
        # Entropy per action over next-states
        ent = -np.sum(T * np.log(np.clip(T, 1e-12, 1.0)), axis=1)
        # Normalize entropy to [0,1] since 2 outcomes max entropy = ln(2)
        ent /= np.log(2.0)
        return T, ent

    for t in range(n_trials):
        # Current learned transition matrix and uncertainty bonus
        T, ent = get_T_and_entropy(trans_counts)

        # Model-based first-stage values: expected max second-stage value under learned T
        max_q2 = np.max(q2, axis=1)  # per planet
        q1_mb = T @ max_q2

        # Mixture with model-free and add uncertainty bonus
        q1_mix = (1.0 - omega_map) * q1_mf + omega_map * q1_mb + psi_bonus * ent

        # First-stage policy
        logits1 = beta * (q1_mix - np.max(q1_mix))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (purely MF here)
        s2 = state[t]
        logits2 = beta * (q2[s2] - np.max(q2[s2]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn transitions from observed (a1 -> s2)
        trans_counts[a1, s2] += 1.0

        # Second-stage MF learning
        td2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # First-stage MF learning with a one-step TD backup from the realized second-stage value
        target1 = q2[s2, a2]
        td1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Belief-biased controller with planet value inference, habitual repetition, and rare-transition aversion.

    The agent maintains a running belief about which planet (X or Y) currently yields higher returns,
    using planet-level value traces updated from observed rewards. First-stage decisions are biased by
    the expected planet values under the fixed transition structure (model-based), a habit (choice-kernel)
    that favors repeating recent actions, and an aversion to repeating the action that produced a rare
    transition on the previous trial. Second-stage choices are model-free.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received.
    model_parameters : iterable of floats
        [alpha, beta, tau_belief, kappa_habit, xi_rare]
        - alpha (0..1): Learning rate for second-stage Q-values (and for bootstrapping to stage 1).
        - beta (0..10): Inverse temperature for softmax at both stages.
        - tau_belief (0..1): Update rate for planet-level value beliefs; closer to 1 updates faster.
        - kappa_habit (0..1): Strength of leaky choice-kernel (habit) at both stages.
        - xi_rare (0..1): Penalty added to repeating the previous first-stage action after a rare transition.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, tau_belief, kappa_habit, xi_rare = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage model-free Q-values
    q2 = np.zeros((2, 2))

    # Planet-level value beliefs (expected value of best alien on each planet)
    v_planet = np.zeros(2)  # for X (0) and Y (1)

    # Choice kernels (habits)
    ck1 = np.zeros(2)
    ck2 = np.zeros((2, 2))

    prev_a1 = None
    prev_rare = False

    for t in range(n_trials):
        # Model-based first-stage values from current planet beliefs
        q1_mb = T @ v_planet

        # Rare-transition aversion bias: penalize repeating action that led to rare transition last trial
        rare_bias = np.zeros(2)
        if prev_a1 is not None and prev_rare:
            rare_bias[prev_a1] -= xi_rare

        # Combine MB values with first-stage habit
        pref1 = q1_mb + kappa_habit * ck1 + rare_bias
        logits1 = beta * (pref1 - np.max(pref1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with habit
        s2 = state[t]
        pref2 = q2[s2] + kappa_habit * ck2[s2]
        logits2 = beta * (pref2 - np.max(pref2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage Q-values
        td2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # Update planet belief with the realized outcome using a max-approximation for that planet
        # Belief update treats v_planet[s2] as running estimate of the best-available return on that planet
        v_planet[s2] = (1.0 - tau_belief) * v_planet[s2] + tau_belief * np.max(q2[s2])

        # Update habits (leaky choice kernels)
        ck1 *= (1.0 - kappa_habit)
        ck1[a1] += 1.0
        ck2[s2] *= (1.0 - kappa_habit)
        ck2[s2, a2] += 1.0

        # Track whether the last transition was rare
        is_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        prev_rare = (not is_common)
        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model-free control with counterfactual (fictive) updates and transition-consistent multi-backup to stage 1.

    This model is primarily model-free but augments learning with:
      - Fictive updates: within the visited second-stage state, the unchosen action is updated toward the
        obtained reward with a separate strength (learning from counterfactuals).
      - Transition-consistent backups: after each outcome, both first-stage actions are partially updated toward
        their transition-expected second-stage state values (i.e., a soft model-based sweep), while still retaining
        the direct TD update for the chosen first-stage action.
      - Lapse choices: with small probability, choices are random.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0/1) within the reached state.
    reward : array-like of float (0 or 1)
        Reward received.
    model_parameters : iterable of floats
        [alpha, beta, zeta_fict, eta_back, chi_lapse]
        - alpha (0..1): Base learning rate for model-free updates at both stages.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - zeta_fict (0..1): Strength of counterfactual update for the unchosen second-stage action.
        - eta_back (0..1): Strength of transition-consistent backups to first-stage values for both actions.
        - chi_lapse (0..1): Lapse probability; with this probability, choices are uniform random.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, zeta_fict, eta_back, chi_lapse = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        # First-stage policy with lapse
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        probs1 = (1.0 - chi_lapse) * probs1 + chi_lapse * 0.5  # uniform over 2 actions
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with lapse
        s2 = state[t]
        logits2 = beta * (q2[s2] - np.max(q2[s2]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        probs2 = (1.0 - chi_lapse) * probs2 + chi_lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second-stage MF learning for chosen action
        td2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # Fictive (counterfactual) update for the unchosen second-stage action within the same state
        a2_unchosen = 1 - a2
        td2_fict = r - q2[s2, a2_unchosen]
        q2[s2, a2_unchosen] += zeta_fict * alpha * td2_fict

        # Direct TD backup to the chosen first-stage action
        target1_direct = q2[s2, a2]
        td1 = target1_direct - q1[a1]
        q1[a1] += alpha * td1

        # Transition-consistent soft backups: update both first-stage actions toward their expected max Q2
        max_q2 = np.max(q2, axis=1)  # value of each planet
        q1_expected = T @ max_q2     # expectation for each first-stage action
        q1 += eta_back * alpha * (q1_expected - q1)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll