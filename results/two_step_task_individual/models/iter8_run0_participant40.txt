def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Uncertainty-gated hybrid MB/MF with learned transitions and choice-stickiness.
    
    The agent learns both second-stage values (Q2) and the first-stage transition model T(a1->state).
    The first-stage choice values are a trial-by-trial mixture of model-based (MB) and model-free (MF),
    where the mixture weight is adaptively gated by transition uncertainty: when the transition
    for the currently considered spaceship is confident (low entropy), the agent relies more on MB.
    A bounded stickiness term biases repeating the previous actions at each stage.

    Parameters (bounds):
    - eta_q (0-1): Learning rate for second-stage values Q2.
    - beta (0-10): Inverse temperature for softmax at both stages.
    - tau_T (0-1): Learning rate for updating the transition model T from observed transitions.
    - z_unc (0-1): Sensitivity to transition confidence; higher increases MB weight when confidence is high.
    - kappa_rep (0-1): Choice stickiness strength applied to repeating the previous action at both stages.

    Inputs:
    - action_1: array-like of ints {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_q, beta, tau_T, z_unc, kappa_rep]

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_q, beta, tau_T, z_unc, kappa_rep = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Initialize values
    Q2 = np.zeros((2, 2))          # second-stage values per planet and alien
    Q1_mf = np.zeros(2)            # first-stage model-free values
    # Initialize transition model close to neutral (0.5/0.5) to avoid overconfidence at start
    T = np.full((2, 2), 0.5)       # T[a1, s] = P(state=s | action=a1)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_in_state = np.array([None, None], dtype=object)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB first-stage values from learned transitions and current Q2
        maxQ2 = np.max(Q2, axis=1)              # value of each planet
        Q1_mb = T @ maxQ2                       # MB value for each spaceship

        # Transition confidence for each action via normalized entropy (0..1), then 1-H is confidence
        # H(p) normalized by log(2) so that binary entropy is in [0,1]
        pA = T[0]
        pU = T[1]
        H_A = -np.sum(pA * (np.log(pA + eps))) / np.log(2 + eps)
        H_U = -np.sum(pU * (np.log(pU + eps))) / np.log(2 + eps)
        conf = np.array([1.0 - H_A, 1.0 - H_U])          # higher is more confident

        # Adaptive weight favoring MB under confidence
        w = (1.0 - z_unc) + z_unc * conf                 # in [1-z_unc, 1]; per-action weight
        # Hybrid first-stage values
        Q1_hyb = w * Q1_mb + (1.0 - w) * Q1_mf

        # Add stickiness bias for repeating previous action(s)
        logits1 = beta * Q1_hyb
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = kappa_rep
            logits1 += beta * stick

        # Stage-1 choice probability
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness to previous a2 in that state
        logits2 = beta * Q2[s].copy()
        if prev_a2_in_state[s] is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2_in_state[s]] = kappa_rep
            logits2 += beta * stick2

        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Update transition model T using delta-rule toward one-hot of observed state
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            T[a1, ss] += tau_T * (target - T[a1, ss])
        # Renormalize to avoid drift due to numerical issues
        T[a1] = np.clip(T[a1], eps, 1.0)
        T[a1] = T[a1] / np.sum(T[a1])

        # Second-stage value update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += eta_q * delta2

        # Model-free first-stage update via bootstrapping from obtained second-stage value
        target1 = Q2[s, a2]
        delta1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += eta_q * delta1

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_in_state[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Risk-sensitive MF with surprise-triggered MB backup and stay bias at both stages.
    
    The agent learns second-stage values with asymmetric learning for gains vs. losses,
    propagates credit to first-stage MF values via eligibility, and performs an extra
    model-based (MB) backup to Q1 only when the observed transition was rare (surprising).
    A bounded stay-bias encourages repeating previous actions at both stages.

    Parameters (bounds):
    - eta_pos2 (0-1): Learning rate for second-stage updates when reward >= 0.
    - eta_neg2 (0-1): Learning rate for second-stage updates when reward < 0.
    - beta (0-10): Inverse temperature for softmax at both stages.
    - z_surMB (0-1): Strength of surprise-triggered MB backup to Q1 on rare transitions.
    - xi_stay (0-1): Stay bias strength applied to repeating the previous action at both stages.

    Inputs:
    - action_1: array-like of ints {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta_pos2, eta_neg2, beta, z_surMB, xi_stay]

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta_pos2, eta_neg2, beta, z_surMB, xi_stay = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Initialize
    Q2 = np.zeros((2, 2))
    Q1 = np.zeros(2)          # first-stage MF values
    # Fixed (known) transition structure for MB backup: A->X common, U->Y common
    # We'll use it to compute a simple MB value as the max Q2 on the observed planet when rare.
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_in_state = np.array([None, None], dtype=object)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Policies with stay biases
        logits1 = beta * Q1.copy()
        if prev_a1 is not None:
            stick1 = np.zeros(2)
            stick1[prev_a1] = xi_stay
            logits1 += beta * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * Q2[s].copy()
        if prev_a2_in_state[s] is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2_in_state[s]] = xi_stay
            logits2 += beta * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Second-stage risk-sensitive update
        eta2 = eta_pos2 if r >= 0 else eta_neg2
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += eta2 * delta2

        # First-stage MF credit assignment via eligibility from second-stage TD error
        eta_mf = 0.5 * (eta_pos2 + eta_neg2)   # coherent MF step-size derived from risk-sensitive rates
        Q1[a1] += eta_mf * delta2

        # Surprise-triggered MB backup on rare transitions
        # Rare if chosen spaceship's common planet != observed planet
        is_common = (a1 == s)  # under mapping A->X (0->0) and U->Y (1->1)
        if not is_common:
            target_mb = np.max(Q2[s])  # value of actually observed planet
            Q1[a1] += z_surMB * (target_mb - Q1[a1])

        # Update memory for stay biases
        prev_a1 = a1
        prev_a2_in_state[s] = a2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Dyna-style planning with learned transitions, replay-strength, and value decay.
    
    The agent learns a transition model and second-stage values from experience. After each real
    experience, it performs a small number of deterministic "planning" (replay) sweeps that
    re-apply recent experiences to Q2, effectively increasing learning without additional data.
    The first-stage policy is purely model-based using the learned transitions and current Q2.
    A per-trial decay pulls values toward zero to capture forgetting/drift.

    Parameters (bounds):
    - eta2 (0-1): Learning rate for second-stage value updates from real outcomes.
    - beta (0-10): Inverse temperature for softmax at both stages.
    - tau_tr (0-1): Learning rate for updating the transition model T from observed transitions.
    - z_sweeps (0-1): Strength/amount of replay; higher applies stronger/more planning updates to Q2.
    - rho_decay (0-1): Per-trial decay of both Q2 and the implicit first-stage value (via Q2) toward 0.

    Inputs:
    - action_1: array-like of ints {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints {0,1}, observed planet (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, chosen alien at the planet.
    - reward: array-like of floats/ints, obtained coins each trial.
    - model_parameters: [eta2, beta, tau_tr, z_sweeps, rho_decay]

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    eta2, beta, tau_tr, z_sweeps, rho_decay = model_parameters
    n_trials = len(action_1)
    eps = 1e-10

    # Initialize transition model near neutral, values at zero
    T = np.full((2, 2), 0.5)   # T[a1, s] = P(state=s | action=a1)
    Q2 = np.zeros((2, 2))

    # Simple buffer to support deterministic replay: last experienced (r) per (state, action_2)
    last_r = np.zeros((2, 2))
    seen_mask = np.zeros((2, 2), dtype=bool)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # MB first-stage values from current T and Q2
        V_planet = np.max(Q2, axis=1)
        Q1_mb = T @ V_planet

        # Stage-1 policy
        logits1 = beta * Q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Decay values each trial (captures forgetting/drift)
        Q2 *= (1.0 - rho_decay)

        # Update transition model with delta-rule toward observed state
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            T[a1, ss] += tau_tr * (target - T[a1, ss])
        T[a1] = np.clip(T[a1], eps, 1.0)
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Real experienced update to Q2
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += eta2 * delta2

        # Save experience for replay
        last_r[s, a2] = r
        seen_mask[s, a2] = True

        # Dyna-style planning: apply z_sweeps-weighted replays to all seen (s, a2)
        # Number of sweeps scales with z_sweeps; at z_sweeps=0 -> 0 extra sweeps; at 1 -> up to 3 sweeps
        K = int(np.floor(3.0 * z_sweeps + 1e-8))
        for _ in range(K):
            # For each seen state-action, re-apply a TD step toward its last observed reward
            for ss in (0, 1):
                for aa in (0, 1):
                    if seen_mask[ss, aa]:
                        td = last_r[ss, aa] - Q2[ss, aa]
                        Q2[ss, aa] += (z_sweeps * eta2) * td

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss