def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model 1: Hybrid model-based/model-free with learned transition probabilities and uncertainty-weighted arbitration.
    
    This model learns second-stage action values (aliens) and also learns the
    first-stage transition probabilities from each spaceship to each planet.
    First-stage decisions combine a model-based (MB) plan (using the learned
    transition matrix and current second-stage values) with a model-free (MF)
    SARSA estimate that backs up the value of the actually visited state/action.
    The arbitration weight between MB and MF increases with uncertainty
    (entropy) in the learned transition model.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage states observed (planets X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within the encountered planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [eta_v, tau_tr, beta, w_s, psi_p]
        - eta_v:  [0,1] learning rate for MF Q-values (both stages)
        - tau_tr: [0,1] learning rate for transition probabilities T(action->state)
        - beta:   [0,10] inverse temperature used at both stages
        - w_s:    [0,1] baseline MB weight in first-stage arbitration
        - psi_p:  [0,1] uncertainty gain: increases MB weight with transition entropy

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    eta_v, tau_tr, beta, w_s, psi_p = model_parameters
    n_trials = len(action_1)

    # Probabilities of choosing recorded actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize MF values and learned transition probabilities
    q1_mf = np.zeros(2)         # model-free first-stage values
    q2 = np.zeros((2, 2))       # second-stage values: q2[state, action]
    T = np.full((2, 2), 0.5)    # learned transition probabilities; rows sum to 1
    # Keep rows normalized explicitly after updates

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB action values for stage 1 from current T and Q2
        max_q2 = np.max(q2, axis=1)           # value of best alien per planet
        q1_mb = T @ max_q2                    # expected value per spaceship

        # Compute arbitration weight based on transition uncertainty (entropy)
        # Entropy per action: H = -sum p log p (natural log), max is ln(2)
        eps = 1e-12
        H_rows = -np.sum(T * (np.log(T + eps)), axis=1)
        H_mean = np.mean(H_rows) / np.log(2.0)  # normalize to [0,1]
        w = w_s + psi_p * H_mean
        w = min(1.0, max(0.0, w))  # clip to [0,1]

        # Hybrid first-stage values
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-2 policy (softmax)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy (softmax over hybrid values)
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        p_choice_1[t] = probs1[a1]

        # Learning: update Q2 with reward prediction error
        delta2 = r - q2[s, a2]
        q2[s, a2] += eta_v * delta2

        # MF SARSA backup to stage-1 chosen action
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta_v * delta1

        # Learn transition probabilities from observed (a1 -> s)
        # Move probability mass toward the observed state
        for sp in (0, 1):
            if sp == s:
                T[a1, sp] += tau_tr * (1.0 - T[a1, sp])
            else:
                T[a1, sp] += tau_tr * (0.0 - T[a1, sp])

        # Ensure numerical normalization
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model 2: Model-based planning with utility curvature and planet-stay bias at stage 1.
    
    This model uses a fixed, known transition structure (A->X, U->Y common)
    to compute model-based (MB) first-stage values from second-stage values.
    Rewards are transformed by a utility curvature parameter (concavity/convexity)
    before updating second-stage values. A planet-stay bias encourages repeating
    the planet visited on the previous trial by favoring the spaceship that commonly
    leads to that planet, independently of which spaceship was previously chosen.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage states observed (planets X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within the encountered planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_u, beta1, beta2, gamma, phi_p]
        - alpha_u: [0,1] learning rate for second-stage values
        - beta1:  [0,10] inverse temperature for first-stage softmax
        - beta2:  [0,10] inverse temperature for second-stage softmax
        - gamma:  [0,1] utility curvature; u = sign(r) * |r|^gamma
        - phi_p:  [0,1] planet-stay bias added to the logit of the ship that
                          commonly leads to the previously visited planet

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_u, beta1, beta2, gamma, phi_p = model_parameters
    n_trials = len(action_1)

    # Fixed known transition matrix: rows=ships (A=0, U=1), cols=planets (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage Q-values
    q2 = np.zeros((2, 2))

    # Track previous planet for bias
    prev_planet = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy
        logits2 = beta2 * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        probs2 = e2 / np.sum(e2)
        p_choice_2[t] = probs2[a2]

        # Compute model-based values for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Planet-stay bias: favor the ship more likely to reach the last planet
        bias1 = np.zeros(2)
        if prev_planet in (0, 1):
            # Ship A (0) is more likely to X (0); Ship U (1) is more likely to Y (1)
            preferred_ship = 0 if prev_planet == 0 else 1
            bias1[preferred_ship] += phi_p

        # Stage-1 policy
        logits1 = beta1 * q1_mb + bias1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        probs1 = e1 / np.sum(e1)
        p_choice_1[t] = probs1[a1]

        # Utility-transformed reward
        u = np.sign(r) * (np.abs(r) ** gamma)

        # Update second-stage values
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha_u * delta2

        # Update memory of previous planet
        prev_planet = s

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Model 3: Surprise-gated MF learning with decay and lapse, shared temperature.
    
    Purely model-free values guide both stages. The impact of the eligibility
    backup from the second to the first stage is gated by transition surprise:
    after a rare transition the stage-1 learning rate is amplified, and after
    a common transition it is attenuated. Unchosen second-stage actions decay
    in value within the encountered state. A lapse parameter mixes softmax
    choice with uniform randomness.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceships A=0, U=1).
    state : array-like of int (0 or 1)
        Second-stage states observed (planets X=0, Y=1).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within the encountered planet).
    reward : array-like of float
        Reward received on each trial.
    model_parameters : iterable of 5 floats
        [alpha_v, beta, rho_s, f_decay, eps_g]
        - alpha_v: [0,1] base learning rate for MF Q-updates (both stages)
        - beta:    [0,10] inverse temperature used at both stages
        - rho_s:   [0,1] surprise gating strength for stage-1 updates
                      (amplify after rare; attenuate after common)
        - f_decay: [0,1] decay rate applied to unchosen second-stage action
                      values in the encountered state
        - eps_g:   [0,1] lapse probability mixing softmax with uniform choice

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_v, beta, rho_s, f_decay, eps_g = model_parameters
    n_trials = len(action_1)

    # Known common transitions to determine surprise (A->X, U->Y common)
    # Define "common" mapping
    common_planet = {0: 0, 1: 1}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q1 = np.zeros(2)        # first-stage values
    q2 = np.zeros((2, 2))   # second-stage values

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy (softmax + lapse)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        e2 = np.exp(logits2)
        soft2 = e2 / np.sum(e2)
        probs2 = (1.0 - eps_g) * soft2 + eps_g * 0.5
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy (softmax over MF q1 + lapse)
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        e1 = np.exp(logits1)
        soft1 = e1 / np.sum(e1)
        probs1 = (1.0 - eps_g) * soft1 + eps_g * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_v * delta2

        # Decay unchosen second-stage action within encountered state
        other = 1 - a2
        q2[s, other] *= (1.0 - f_decay)

        # Surprise gating for stage-1 update
        is_common = (common_planet[a1] == s)
        # Gate factor in [0, 2]: 1 + rho_s if rare, 1 - rho_s if common
        gate = (1.0 + rho_s) if (not is_common) else (1.0 - rho_s)
        gate = max(0.0, gate)  # ensure non-negative

        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += (alpha_v * gate) * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll