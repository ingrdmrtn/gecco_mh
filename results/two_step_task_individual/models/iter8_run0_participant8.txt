def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Pruned model-based planning with model-free eligibility trace and lapse.

    Mechanism
    - Second-stage values are learned with a standard delta rule (model-free).
    - First-stage model-free values are updated via an eligibility-trace-like backup
      from second-stage TD error (scaled by lambda_et).
    - First-stage model-based evaluation uses a pruning heuristic: for each
      first-stage action, transition probability mass is shifted away from the
      lower-value successor toward the higher-value successor, controlled by 'prune'.
    - Choice policies at both stages are softmax with a lapse component.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the observed state.
    reward : array-like of float {0,1}
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, prune, lambda_et, eps]
        - alpha in [0,1]: learning rate for value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - prune in [0,1]: degree of pruning; 0=no pruning, 1=fully ignore the lower-valued successor
          when computing model-based first-stage values.
        - lambda_et in [0,1]: eligibility-trace strength backing up second-stage TD error to first-stage MF values.
        - eps in [0,1]: lapse probability; with probability eps choose uniformly at random.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, prune, lambda_et, eps = model_parameters
    n_trials = len(action_1)

    # Known transition structure for spaceships: A->X common, U->Y common
    T_fixed = np.array([[0.7, 0.3],   # A: P(X)=0.7, P(Y)=0.3
                        [0.3, 0.7]])  # U: P(X)=0.3, P(Y)=0.7

    # Probabilities of observed choices for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)        # model-free first-stage values
    q2 = np.zeros((2, 2))      # second-stage state-action values

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based planning at stage 1 with pruning
        v2 = np.max(q2, axis=1)  # value per second-stage state
        q1_mb = np.zeros(2)
        for a in range(2):
            # Identify higher- and lower-valued successor states
            if v2[0] >= v2[1]:
                high, low = 0, 1
            else:
                high, low = 1, 0
            p_high = T_fixed[a, high]
            p_low = T_fixed[a, low]
            # Move a fraction 'prune' of low branch mass to the high branch
            p_high_adj = p_high + prune * p_low
            p_low_adj = (1.0 - prune) * p_low
            # Expected value under pruned transitions
            q1_mb[a] = p_high_adj * v2[high] + p_low_adj * v2[low]

        # Combine MF and MB at stage 1 by simple averaging (no extra parameter)
        q1 = 0.5 * (q1_mf + q1_mb)

        # Stage-1 choice policy (softmax with lapse)
        pref1 = q1 - np.max(q1)
        pr1_soft = np.exp(beta * pref1)
        pr1_soft /= np.sum(pr1_soft)
        pr1 = (1.0 - eps) * pr1_soft + eps * 0.5
        p_choice_1[t] = pr1[a1]

        # Stage-2 choice policy (softmax with lapse)
        pref2 = q2[s, :] - np.max(q2[s, :])
        pr2_soft = np.exp(beta * pref2)
        pr2_soft /= np.sum(pr2_soft)
        pr2 = (1.0 - eps) * pr2_soft + eps * 0.5
        p_choice_2[t] = pr2[a2]

        # Learning updates
        # Second stage TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First stage MF value updated by backing up second-stage TD error (eligibility-like)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * lambda_et * pe1

    eps_small = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps_small)) + np.sum(np.log(p_choice_2 + eps_small)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Transition-credible arbitration with asymmetric reward learning and state-dependent perseveration.

    Mechanism
    - Learn a per-action transition model T via a delta rule (row-wise).
    - Compute model-based first-stage values from learned T and second-stage values.
    - Arbitration at stage 1 is action-specific and depends on the credibility of the learned transitions:
      weight w_a = 1 - exp(-trans_lr * KL(T_a || uniform)), increasing with certainty and sharpened by trans_lr.
    - Second-stage learning uses asymmetric learning rates for positive vs negative TD errors.
    - Second-stage choice includes perseveration toward repeating the last action within the same second-stage state.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the observed state.
    reward : array-like of float {0,1}
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha_up, alpha_down, beta, trans_lr, rep_s2]
        - alpha_up in [0,1]: learning rate for positive second-stage TD errors (pe2 >= 0).
        - alpha_down in [0,1]: learning rate for negative second-stage TD errors (pe2 < 0).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - trans_lr in [0,1]: transition learning rate; also scales arbitration sensitivity.
        - rep_s2 in [0,1]: perseveration strength added to the previously chosen action in the same second-stage state.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_up, alpha_down, beta, trans_lr, rep_s2 = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition model T (rows sum to 1)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track last second-stage action per state for perseveration
    last_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage values from learned transitions
        v2 = np.max(q2, axis=1)      # value per second-stage state
        q1_mb = T @ v2               # expected value per first-stage action

        # Arbitration weight per action based on transition credibility
        # KL divergence from uniform (0.5, 0.5) measures certainty
        w = np.zeros(2)
        for a in range(2):
            p = np.clip(T[a, :], 1e-12, 1.0)
            # D_KL(p || u) where u=(0.5,0.5) is sum p_i * log(p_i/0.5) = sum p_i*log(2*p_i)
            kl = p[0] * np.log(2.0 * p[0]) + p[1] * np.log(2.0 * p[1])
            # Map KL to [0,1) with a saturating transform; scaled by trans_lr
            w[a] = 1.0 - np.exp(-trans_lr * kl)

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        pref1 = q1 - np.max(q1)
        pr1 = np.exp(beta * pref1)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        # Stage-2 policy with perseveration
        pref2 = q2[s, :].copy()
        if last_a2[s] in (0, 1):
            pref2[last_a2[s]] += rep_s2
        pref2 -= np.max(pref2)
        pr2 = np.exp(beta * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]

        # Learning updates
        # Second-stage TD error and asymmetric learning
        pe2 = r - q2[s, a2]
        lr2 = alpha_up if pe2 >= 0.0 else alpha_down
        q2[s, a2] += lr2 * pe2

        # First-stage MF learning bootstrapped from second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use the same sign-dependent rate to propagate reward valence asymmetry to stage 1
        q1_mf[a1] += lr2 * pe1

        # Transition learning for the chosen first-stage action
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T[a1, :] = (1.0 - trans_lr) * T[a1, :] + trans_lr * target_row
        T[a1, :] /= (np.sum(T[a1, :]) + 1e-12)

        # Update perseveration memory
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Transition-contingent exploration and learning with fixed arbitration and choice bias.

    Mechanism
    - Fixed transition model is known (A->X common, U->Y common).
    - Stage-1 values are a fixed mixture of model-based and model-free values (mix_mb).
    - Stage-1 exploration temperature is reduced following a rare transition on the previous trial,
      controlled by xi_rare (rarity-sensitive adjustment of effective beta).
    - The same rarity sensitivity also scales the stage-1 MF learning rate on trials that were rare
      (smaller update on rare transitions).
    - Includes a static bias toward spaceship U (action=1), implemented as an additive preference.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the observed state.
    reward : array-like of float {0,1}
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, mix_mb, xi_rare, bias_u]
        - alpha in [0,1]: learning rate for value updates (both stages).
        - beta in [0,10]: base inverse temperature for softmax.
        - mix_mb in [0,1]: weight on model-based values at stage 1 (1=model-based, 0=model-free).
        - xi_rare in [0,1]: rarity sensitivity; reduces stage-1 temperature after rare transitions
          on the previous trial and downscales stage-1 MF learning on rare trials.
        - bias_u in [0,1]: static bias toward U; mapped to additive preferences [-b, +b] with b in [-1,1].

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, mix_mb, xi_rare, bias_u = model_parameters
    n_trials = len(action_1)

    # Fixed transitions
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Bias toward U (action=1)
    b = 2.0 * (bias_u - 0.5)  # maps [0,1] -> [-1,1]

    # Track previous-trial rarity to modulate next-trial exploration
    rare_prev = 0  # 0 for common or no previous, 1 for rare

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage values from fixed transitions
        v2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ v2

        # Arbitration
        q1 = mix_mb * q1_mb + (1.0 - mix_mb) * q1_mf

        # Stage-1 policy with transition-contingent exploration and static bias
        eff_beta1 = beta * (1.0 - xi_rare * rare_prev)  # lower temperature after rare previous trial
        pref1 = q1.copy()
        # Add symmetric bias toward U: [-b, +b]
        pref1[0] += -b
        pref1[1] += +b
        pref1 -= np.max(pref1)
        pr1 = np.exp(eff_beta1 * pref1)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        # Stage-2 policy (standard softmax with base beta)
        pref2 = q2[s, :] - np.max(q2[s, :])
        pr2 = np.exp(beta * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]

        # Learning updates
        # Second stage TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First stage MF update; scale by rarity of this trial's transition
        # Determine if current transition was common or rare
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        is_rare = 1 - is_common
        scale_rare = (1.0 - xi_rare) if is_rare == 1 else 1.0

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * scale_rare * pe1

        # Update rare_prev for the next trial
        rare_prev = is_rare

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll