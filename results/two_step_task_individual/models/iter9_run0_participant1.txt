def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Surprise- and-uncertaintyâ€“gated hybrid (MB+MF) with value forgetting.

    This model blends model-based (MB) and model-free (MF) action values at stage 1,
    where the arbitration weight increases with both transition surprise and
    second-stage reward uncertainty. Second-stage values undergo standard delta-rule
    learning with global value forgetting.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [alpha, beta, zetaS, tauF]
        - alpha (reward learning rate, [0,1]): learning rate for second-stage Q-values.
        - beta (inverse temperature, [0,10]): softmax inverse temperature (both stages).
        - zetaS (arbitration weight, [0,1]): mixes transition surprise and reward
          uncertainty to determine MB influence on stage-1 values.
        - tauF (forgetting, [0,1]): value forgetting toward 0.5 each trial for all Qs.

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, zetaS, tauF = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 MF values (interpreted as reward probability estimates)
    q2 = np.full((2, 2), 0.5, dtype=float)
    # Stage-1 MF values
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Forgetting toward 0.5 for Q2 and toward 0 for Q1_MF before action selection
        q2 = (1 - tauF) * q2 + tauF * 0.5
        q1_mf = (1 - tauF) * q1_mf  # anchor at 0 for MF stage-1

        # Model-based stage-1 values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Arbitration weight w in [0,1]: combine transition surprise and reward uncertainty
        # Transition surprise: -log P(s2 | a1) normalized by max surprise -log(0.3)
        p_s_given_a = T[a1, s2]
        surpr = -np.log(max(p_s_given_a, 1e-10))
        surpr_norm = np.clip(surpr / (-np.log(0.3)), 0.0, 1.0)

        # Reward uncertainty at visited state: use Bernoulli variance p(1-p) with p=q2
        p_est = np.clip(q2[s2], 1e-6, 1 - 1e-6)
        var_s2 = np.max(p_est * (1.0 - p_est))  # uncertainty of best available option at s2

        # Arbitration: higher weight on MB when surprise/uncertainty are high
        w = np.clip(zetaS * surpr_norm + (1.0 - zetaS) * var_s2, 0.0, 1.0)

        # Hybrid stage-1 values
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax over q2 at visited state)
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # 1) Stage-2 MF update
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # 2) Stage-1 MF update via bootstrapping from second-stage chosen value
        #    Use the observed second-stage value as target for chosen first-stage action.
        target1 = q2[s2, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Model-based planner with counterfactual second-stage learning and state-conditional stickiness.

    This model plans at stage 1 using the known transition structure and learned
    second-stage values. At stage 2, it updates both the chosen and unchosen alien:
    the unchosen alien is updated toward the counterfactual outcome (1 - reward),
    capturing an assumption of anti-correlation. Stage-2 choice exhibits state-conditional
    perseveration. Stage-1 choices include a transition-sensitive WSLS-like bias.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [eta, beta, kappaCF, phi2, psiT]
        - eta (reward learning rate, [0,1]): learning rate for second-stage Q-values.
        - beta (inverse temperature, [0,10]): softmax inverse temperature (both stages).
        - kappaCF (counterfactual strength, [0,1]): weight for updating the unchosen
          second-stage action toward 1 - reward.
        - phi2 (state-conditional perseveration, [0,1]): additive bias to repeat
          the previous second-stage action within the same state.
        - psiT (transition-sensitive WSLS bias, [0,1]): additive first-stage bias
          that promotes repeating the previous stage-1 action after rewarded-common
          transitions, and switching after rewarded-rare (sign flips for unrewarded).

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    eta, beta, kappaCF, phi2, psiT = model_parameters
    n_trials = len(action_1)

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage values
    q2 = np.full((2, 2), 0.5, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Track last actions for biases
    last_a2_by_state = [None, None]
    last_a1 = None
    last_s2 = None
    last_r = None

    for t in range(n_trials):
        s2 = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 MB values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Transition-sensitive WSLS-like bias at stage 1
        bias1 = np.zeros(2, dtype=float)
        if last_a1 is not None and last_s2 is not None and last_r is not None:
            # Determine whether last transition was common or rare for the last choice
            p_last = T[last_a1, last_s2]
            was_common = p_last >= 0.5
            # WSLS rule modulated by transition type
            # sign = +1 if reward, -1 if no reward
            sign = 1.0 if last_r > 0 else -1.0
            if was_common:
                # rewarded-common: repeat; unrewarded-common: switch
                bias1[last_a1] += psiT * sign
                bias1[1 - last_a1] -= psiT * sign
            else:
                # rewarded-rare: switch; unrewarded-rare: repeat
                bias1[last_a1] -= psiT * sign
                bias1[1 - last_a1] += psiT * sign

        # Stage-1 policy
        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with state-conditional perseveration
        bias2 = np.zeros(2, dtype=float)
        if last_a2_by_state[s2] is not None:
            bias2[last_a2_by_state[s2]] += phi2

        logits2 = beta * q2[s2] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates at stage 2
        # Chosen action update
        delta = r - q2[s2, a2]
        q2[s2, a2] += eta * delta
        # Counterfactual update for the unchosen action (toward 1 - r)
        a2_unc = 1 - a2
        delta_cf = (1.0 - r) - q2[s2, a2_unc]
        q2[s2, a2_unc] += eta * kappaCF * delta_cf

        # Update trackers
        last_a2_by_state[s2] = a2
        last_a1 = a1
        last_s2 = s2
        last_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Kalman-like learning of reward probabilities with uncertainty-driven exploration bonus.

    This purely model-based planner maintains a Gaussian belief over each
    second-stage reward probability (mean and variance per state-action), updated
    via a Kalman filter with fixed process and observation noise. Action values
    include an exploration bonus proportional to posterior uncertainty.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Chosen spaceship on each trial: 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Observed planet on each trial: 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Chosen alien on each trial at the visited planet.
    reward : array-like of float/int (typically 0 or 1)
        Coins received on each trial.
    model_parameters : sequence of floats
        [beta, nu, rvar, xiE]
        - beta (inverse temperature, [0,10]): softmax inverse temperature (both stages).
        - nu (process noise, [0,1]): assumed volatility per trial added to the posterior variance.
        - rvar (observation noise, [0,1]): assumed observation noise for reward outcomes.
        - xiE (exploration bonus, [0,1]): weight on uncertainty bonus (sqrt(variance)) added to Q.

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    beta, nu, rvar, xiE = model_parameters
    n_trials = len(action_1)

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Gaussian beliefs per (state, action): mean m and variance s2
    m = np.full((2, 2), 0.5, dtype=float)
    s2 = np.full((2, 2), 0.25, dtype=float)  # initial uncertainty (variance)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        s2_obs = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Exploration bonus from uncertainty
        bonus = xiE * np.sqrt(np.maximum(s2, 1e-12))
        q2 = m + bonus

        # Stage-1 MB values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 policy
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at the visited state
        logits2 = beta * q2[s2_obs]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Kalman-like update for the chosen (state, action)
        # Predict step: add process noise
        s2[s2_obs, a2] = s2[s2_obs, a2] + nu
        # Update step: compute gain and update mean/variance
        denom = s2[s2_obs, a2] + rvar
        K = s2[s2_obs, a2] / max(denom, 1e-10)
        m[s2_obs, a2] = m[s2_obs, a2] + K * (r - m[s2_obs, a2])
        s2[s2_obs, a2] = (1.0 - K) * s2[s2_obs, a2]

        # Keep beliefs within valid bounds
        m[s2_obs, a2] = np.clip(m[s2_obs, a2], 0.0, 1.0)
        s2[s2_obs, a2] = np.clip(s2[s2_obs, a2], 1e-12, 1.0)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll