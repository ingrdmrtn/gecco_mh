def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Hybrid MB-MF with replay-weighted first-stage valuation, first-stage stickiness, and lapse.
    
    This model learns model-free Q-values at both stages, and uses a model-based
    one-step lookahead at the first stage with a fixed transition model. The decision
    values at the first stage are a convex combination of MF Q1 and MB values,
    controlled by a replay/planning weight. A first-stage stickiness term captures
    a tendency to repeat the last first-stage choice. A lapse mixes softmax with
    uniform choice. Second-stage choices use MF Q-values only, with the same lapse.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet: 0/1.
    reward : array-like of float
        Coins received at the end of each trial (typically 0/1).
    model_parameters : sequence
        [alpha, beta, omega_replay, psi_stick1, xi_lapse]
        - alpha (learning rate, [0,1]): MF learning rate for Q-values at both stages.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - omega_replay (planning weight, [0,1]): Weight on model-based value in first-stage decision.
        - psi_stick1 (first-stage stickiness, [0,1]): Additive bias to repeat previous first-stage action.
        - xi_lapse (lapse rate, [0,1]): Probability of making a uniform random choice at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega_replay, psi_stick1, xi_lapse = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])  # shape (2 actions, 2 states)

    # Model-free Q-values
    q1 = np.zeros(2)         # first-stage MF action values
    q2 = np.zeros((2, 2))    # second-stage MF action values per state

    # Stickiness memory (previous first-stage action one-hot)
    last_a1_onehot = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based first-stage value = expected max second-stage value under transitions
        max_q2_per_state = np.max(q2, axis=1)  # shape (2,)
        q1_mb = transition_matrix @ max_q2_per_state  # shape (2,)

        # Hybrid decision value with stickiness
        v1 = (1.0 - omega_replay) * q1 + omega_replay * q1_mb + psi_stick1 * last_a1_onehot
        v2 = q2[s].copy()  # second-stage uses MF only

        # Softmax with lapse at both stages
        exp_v1 = np.exp(beta * (v1 - np.max(v1)))
        probs1 = exp_v1 / (np.sum(exp_v1) + eps)
        probs1 = (1.0 - xi_lapse) * probs1 + xi_lapse * 0.5
        p_choice_1[t] = probs1[a1]

        exp_v2 = np.exp(beta * (v2 - np.max(v2)))
        probs2 = exp_v2 / (np.sum(exp_v2) + eps)
        probs2 = (1.0 - xi_lapse) * probs2 + xi_lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning updates (MF)
        # Second-stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # First-stage TD update bootstrapping from second stage
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

        # Update stickiness memory for next trial
        last_a1_onehot[:] = 0.0
        last_a1_onehot[a1] = 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based with learned transitions, MF reward learning at stage 2, and forgetting.
    
    This model learns the transition probabilities from first-stage actions to second-stage states,
    and uses a model-based evaluation at the first stage by taking the expectation over learned
    transitions of the maximum second-stage value. Second-stage reward probabilities are learned
    via model-free TD. Unvisited/unchosen second-stage action values decay toward 0.5 (forgetting).
    A lapse parameter mixes softmax policies with uniform choice at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet: 0/1.
    reward : array-like of float
        Coins received at the end of each trial (typically 0/1).
    model_parameters : sequence
        [alpha_r, beta, kappa_tr, eta_forget, eps_lapse]
        - alpha_r (reward learning rate, [0,1]): MF learning rate for second-stage Q-values.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - kappa_tr (transition learning rate, [0,1]): Learning rate for action-state transition probs.
        - eta_forget (forgetting rate, [0,1]): Decay of unchosen/unvisited second-stage Q-values toward 0.5.
        - eps_lapse (lapse rate, [0,1]): Probability of uniform random responding at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, beta, kappa_tr, eta_forget, eps_lapse = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix toward the canonical common/rare structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: actions (A,U), cols: states (X,Y)

    # Second-stage MF values
    q2 = np.full((2, 2), 0.5)  # initialize ambiguously

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # First-stage model-based values: expectation over learned transitions of max Q2
        max_q2_per_state = np.max(q2, axis=1)  # (2,)
        v1 = T @ max_q2_per_state  # (2,)

        # Second-stage values
        v2 = q2[s].copy()

        # Policies with lapse
        exp_v1 = np.exp(beta * (v1 - np.max(v1)))
        probs1 = exp_v1 / (np.sum(exp_v1) + eps)
        probs1 = (1.0 - eps_lapse) * probs1 + eps_lapse * 0.5
        p_choice_1[t] = probs1[a1]

        exp_v2 = np.exp(beta * (v2 - np.max(v2)))
        probs2 = exp_v2 / (np.sum(exp_v2) + eps)
        probs2 = (1.0 - eps_lapse) * probs2 + eps_lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Learn transitions from observed outcome
        # Move T[a1, s] toward 1, and T[a1, 1-s] toward 0 with learning rate kappa_tr
        T[a1, s] += kappa_tr * (1.0 - T[a1, s])
        T[a1, 1 - s] = 1.0 - T[a1, s]  # keep row normalized and two-state

        # Reward learning at second stage (MF)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Forgetting toward 0.5 for unchosen and unvisited second-stage values
        # In visited state s: unchosen action decays
        other_a2 = 1 - a2
        q2[s, other_a2] += eta_forget * (0.5 - q2[s, other_a2])
        # In unvisited state: both actions decay toward 0.5
        s_other = 1 - s
        q2[s_other, 0] += eta_forget * (0.5 - q2[s_other, 0])
        q2[s_other, 1] += eta_forget * (0.5 - q2[s_other, 1])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """MF learning with leaky uncertainty bonus at stage 2 and model-based stay/switch bias at stage 1.
    
    This model learns MF Q-values at the second stage. First-stage decision values are computed
    from a model-based one-step lookahead (fixed transitions) and are modulated by a bias that
    captures the canonical model-based stay/switch pattern: after rewarded-common transitions,
    repeat the first-stage action; after rewarded-rare transitions, switch. The strength of this
    bias is governed by mu_mb_stay and depends on the previous trial's reward and transition type.
    
    At the second stage, a directed exploration bonus is added to values based on a leaky count
    of action usage (uncertainty bonus higher for less-tried actions). The count leaky-decays
    with rho_decay each trial.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state reached: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the reached planet: 0/1.
    reward : array-like of float
        Coins received at the end of each trial (typically 0/1).
    model_parameters : sequence
        [alpha, beta, zeta_unc2, rho_decay, mu_mb_stay]
        - alpha (learning rate, [0,1]): MF learning rate for second-stage Q-values.
        - beta (inverse temperature, [0,10]): Softmax sensitivity for both stages.
        - zeta_unc2 (uncertainty bonus weight, [0,1]): Weight of directed exploration bonus at stage 2.
        - rho_decay (leak rate for counts, [0,1]): Fractional decay of second-stage counts each trial.
        - mu_mb_stay (MB stay/switch bias, [0,1]): Strength of first-stage bias based on last trial's
          reward and transition commonality (positive encourages MB-consistent patterns).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, zeta_unc2, rho_decay, mu_mb_stay = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure for MB evaluation
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # MF second-stage Q-values and leaky counts for uncertainty bonus
    q2 = np.full((2, 2), 0.5)
    N2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # Previous-trial info for MB stay/switch bias
    prev_has_info = False
    prev_a1 = 0
    prev_s = 0
    prev_r = 0.0

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # First-stage MB evaluation from fixed transitions
        max_q2_per_state = np.max(q2, axis=1)
        v1 = T @ max_q2_per_state  # (2,)

        # Apply MB-consistent stay/switch bias based on previous trial
        bias1 = np.zeros(2)
        if prev_has_info:
            # Common if previous first-stage action matches previous state (A->X or U->Y)
            prev_common = (prev_a1 == prev_s)
            was_rewarded = (prev_r > 0.5)
            # MB rule of thumb: if rewarded and common -> stay; if rewarded and rare -> switch
            sign = 0.0
            if was_rewarded and prev_common:
                sign = 1.0
            elif was_rewarded and not prev_common:
                sign = -1.0
            else:
                sign = 0.0  # no bias if not rewarded
            # Add bias to repeating previous a1
            bias1[prev_a1] += mu_mb_stay * sign

        v1 = v1 + bias1

        # Second-stage uncertainty bonus from leaky counts
        # Leaky decay of counts before using them
        N2 *= (1.0 - rho_decay)
        bonus2 = zeta_unc2 / np.sqrt(N2[s] + 1.0)
        v2 = q2[s] + bonus2

        # Policies
        exp_v1 = np.exp(beta * (v1 - np.max(v1)))
        probs1 = exp_v1 / (np.sum(exp_v1) + eps)
        p_choice_1[t] = probs1[a1]

        exp_v2 = np.exp(beta * (v2 - np.max(v2)))
        probs2 = exp_v2 / (np.sum(exp_v2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update counts after observing choice
        N2[s, a2] += 1.0

        # Store info for next trial bias
        prev_has_info = True
        prev_a1 = a1
        prev_s = s
        prev_r = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll