def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Model-free two-step learner with transition-contingent eligibility trace.

    The agent learns model-free Q-values for both stages. Credit assignment from Stage-2 back to
    Stage-1 is gated by whether the observed transition was common or rare. This captures the idea
    that participants may propagate credit more strongly after expected (common) transitions than
    after surprising (rare) ones.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (number of coins) per trial.
    model_parameters : sequence of floats
        [alpha1, alpha2, beta, lam_common, lam_rare]
        - alpha1 in [0,1]: learning rate for Stage-1 Q-values.
        - alpha2 in [0,1]: learning rate for Stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - lam_common in [0,1]: eligibility strength for Stage-1 update after common transitions.
        - lam_rare in [0,1]: eligibility strength for Stage-1 update after rare transitions.

    Returns
    -------
    float
        Negative log-likelihood of observed choices across both stages.
    """
    alpha1, alpha2, beta, lam_common, lam_rare = model_parameters
    n_trials = len(action_1)

    # Known task structure used only to determine whether a transition was common vs rare (not for planning)
    # A commonly -> X, U commonly -> Y
    def is_common_transition(a1, s2):
        return (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)

    # Initialize Q-values
    q1 = np.zeros(2, dtype=float) + 0.5
    q2 = np.zeros((2, 2), dtype=float) + 0.5

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 policy: softmax over model-free Q1
        l1 = beta * (q1 - np.max(q1))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy: softmax over Q2 at reached state
        l2 = beta * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Learning at Stage-2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage-1 learning with transition-contingent eligibility trace
        lam = lam_common if is_common_transition(a1, s) else lam_rare
        # TD target uses realized Stage-2 action value (pure MF)
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha1 * lam * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Pure model-based planner with volatility-adaptive Stage-2 learning rates.

    The agent plans at Stage-1 using a fixed (veridical) transition model, but adapts how quickly it
    updates Stage-2 values based on an online estimate of reward volatility (running absolute PE).
    When volatility is high, the effective learning rate moves toward alpha_high; when low, toward
    alpha_low.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (number of coins) per trial.
    model_parameters : sequence of floats
        [alpha_low, alpha_high, beta, tau_vol]
        - alpha_low in [0,1]: lower bound on Stage-2 learning rate used when volatility is low.
        - alpha_high in [0,1]: upper bound on Stage-2 learning rate used when volatility is high.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - tau_vol in [0,1]: smoothing rate for the running volatility estimate (0 = static, 1 = fully reactive).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_low, alpha_high, beta, tau_vol = model_parameters
    n_trials = len(action_1)

    # Fixed known transition matrix (A->X common; U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 values
    q2 = np.zeros((2, 2), dtype=float) + 0.5

    # Volatility estimate (running abs PE)
    vol = 0.0

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute Stage-1 model-based values from current q2
        max_q2 = np.max(q2, axis=1)   # value per state
        q1_mb = T @ max_q2            # expected value per first-stage action

        # Stage-1 policy
        l1 = beta * (q1_mb - np.max(q1_mb))
        p1 = np.exp(l1)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        l2 = beta * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Learning at Stage-2 with volatility-adaptive alpha
        pe2 = r - q2[s, a2]
        vol = (1.0 - tau_vol) * vol + tau_vol * abs(pe2)
        alpha_eff = alpha_low + (alpha_high - alpha_low) * np.clip(vol, 0.0, 1.0)
        q2[s, a2] += alpha_eff * pe2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Assumed-transition model-based planner with first-stage lapse and cross-generalization at Stage-2.

    The agent plans at Stage-1 using an internal, possibly misspecified, fixed transition belief p_common.
    Stage-2 values are learned model-free from reward, and additionally exhibit cross-generalization:
    the unchosen alien on the visited planet is nudged toward the chosen alien's outcome.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien index on the reached planet).
    reward : array-like of float
        Reward outcome (number of coins) per trial.
    model_parameters : sequence of floats
        [alpha2, beta, p_common, lapse1, kappa_back]
        - alpha2 in [0,1]: learning rate for Stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - p_common in [0,1]: assumed probability that each spaceship makes its common transition.
                             A->X and U->Y are taken as "common" under this belief.
        - lapse1 in [0,1]: first-stage lapse; blends softmax policy with uniform random choice.
        - kappa_back in [0,1]: strength of cross-generalization to the unchosen alien on the visited planet.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, p_common, lapse1, kappa_back = model_parameters
    n_trials = len(action_1)

    # Internal (possibly incorrect) transition belief
    T_hat = np.array([[p_common, 1.0 - p_common],
                      [1.0 - p_common, p_common]], dtype=float)

    # Stage-2 values
    q2 = np.zeros((2, 2), dtype=float) + 0.5

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 model-based action values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2

        # Softmax with lapse at Stage-1
        l1 = beta * (q1_mb - np.max(q1_mb))
        p1_soft = np.exp(l1)
        p1_soft = p1_soft / (np.sum(p1_soft) + eps)
        p1 = (1.0 - lapse1) * p1_soft + lapse1 * 0.5
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (no lapse at Stage-2)
        l2 = beta * (q2[s] - np.max(q2[s]))
        p2 = np.exp(l2)
        p2 = p2 / (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Stage-2 learning with cross-generalization on the visited planet
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Cross-generalize to the unchosen alien on the same planet
        other = 1 - a2
        pe_other = r - q2[s, other]
        q2[s, other] += (kappa_back * alpha2) * pe_other

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll