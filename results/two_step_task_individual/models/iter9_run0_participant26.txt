Below are three standalone cognitive models that compute the negative log-likelihood of the observed choices in the two-step task. Each model uses all of its parameters meaningfully, respects the requested bounds, and explores mechanisms not covered by your previously tried combinations.

def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """Adaptive model-based control with learned transitions, surprise-modulated learning, and confidence-gated arbitration.

    Core ideas:
    - Learn both second-stage values and the first-stage transition matrix online.
    - Use model-based planning at stage 1 (via learned transitions).
    - The learning rate for second-stage values is amplified by transition surprise (Pearceâ€“Hall style).
    - The arbitration weight toward model-based control depends on the confidence in transition knowledge
      (operationalized as low entropy of the learned transition matrix).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0 or 1).
    reward : array-like of float (0 or 1)
        Received reward.
    model_parameters : iterable of floats
        [eta_q, beta, phi_surprise, omega_mb0, xi_conf]
        - eta_q (0..1): Base learning rate for second-stage Q-values.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - phi_surprise (0..1): Scales surprise-based boost of eta_q when the transition is unexpected.
        - omega_mb0 (0..1): Baseline weight on model-based values at stage 1.
        - xi_conf (0..1): Learning rate for updating transition probabilities; also shapes confidence.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    eta_q, beta, phi_surprise, omega_mb0, xi_conf = model_parameters
    n_trials = len(action_1)

    # Initialize learned transition matrix rows for each first-stage action
    # Start from an uninformative midpoint (0.5/0.5) to encourage initial exploration.
    T = np.ones((2, 2)) * 0.5  # rows: a1 in {0,1}; cols: s in {0(X),1(Y)}
    # Second-stage Q-values
    Q2 = np.zeros((2, 2))  # state x action
    # Model-free first-stage Q for arbitration mixture
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute model-based Q1 via learned transitions and max over second-stage actions
        max_Q2 = np.max(Q2, axis=1)  # value of states X and Y
        Q1_mb = T @ max_Q2

        # Confidence from transition entropy (lower entropy => higher confidence)
        # Row-wise entropy for each action's transition distribution.
        def row_entropy(p_row):
            p_clip = np.clip(p_row, eps, 1.0)
            return -(p_clip * np.log(p_clip) + (1 - p_clip) * np.log(1 - p_clip)).sum() / np.log(2)

        H0 = row_entropy(T[0])
        H1 = row_entropy(T[1])
        # Confidence per action: 1 - entropy in [0,1]
        conf = np.array([1.0 - H0, 1.0 - H1])
        # Arbitration weight per action: baseline scaled by confidence of that action's transition knowledge
        omega_vec = omega_mb0 * conf
        # Mixed action values for stage 1
        Q1_mix = omega_vec * Q1_mb + (1.0 - omega_vec) * Q1_mf

        # Policy stage 1
        logits1 = beta * (Q1_mix - np.max(Q1_mix))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Policy stage 2
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Surprise: mismatch between predicted transition prob for chosen action and observed state
        p_obs = T[a1, s]
        surprise = 1.0 - p_obs  # higher when the observed transition was unlikely
        # Surprise-modulated learning rate for Q2
        eta_eff = eta_q * (1.0 + phi_surprise * surprise)
        eta_eff = min(1.0, max(0.0, eta_eff))

        # Update Q2 (stage-2 TD)
        td2 = r - Q2[s, a2]
        Q2[s, a2] += eta_eff * td2

        # Back-propagate to Q1_mf using the obtained second-stage value (SARSA-style)
        target1 = Q2[s, a2]
        td1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += eta_eff * td1  # use the same effective rate to couple learning rates

        # Update learned transitions for chosen action (Bayes-like delta rule)
        # Move mass toward the observed state with rate xi_conf.
        T[a1, s] = (1 - xi_conf) * T[a1, s] + xi_conf * 1.0
        other = 1 - s
        T[a1, other] = 1.0 - T[a1, s]  # keep row normalized and binary complement since there are 2 states

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """Successor representation at stage 1 with forgetting, unified reward learning, and loss-averse utility.

    Core ideas:
    - Stage 1 uses an online-learned successor representation (SR) over the two second-stage states, conditional on
      the chosen first-stage action. The SR captures long-run visitation of states given an action.
    - Stage 2 Q-values are learned model-free.
    - A linear reward weight vector maps state features to stage-1 values via SR^T * state-reward vector.
    - Forgetting on SR enables adaptation to nonstationarities in transitions.
    - Utility is loss-averse: unrewarded outcomes are penalized more than a neutral 0 via a negative utility.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions (0 or 1).
    reward : array-like of float (0 or 1)
        Reward received.
    model_parameters : iterable of floats
        [eta_m, eta_r, beta, kappa_loss, chi_forget]
        - eta_m (0..1): Learning rate for SR updates (for chosen action's state occupancy).
        - eta_r (0..1): Learning rate for both second-stage Q-values and reward weights.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - kappa_loss (0..1): Loss aversion: utility u(r)=r - kappa_loss*(1-r), so u(0)=-kappa_loss.
        - chi_forget (0..1): Forgetting toward zero on SR rows each trial, enabling adaptation.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    eta_m, eta_r, beta, kappa_loss, chi_forget = model_parameters
    n_trials = len(action_1)

    # SR for each first-stage action over the two second-stage states
    # Rows: a1 in {0,1}; Cols: feature for state X and Y
    M = np.zeros((2, 2))
    # Reward weight vector over the two states (maps state features to value)
    w_r = np.zeros(2)
    # Second-stage Q-values (state x action)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Loss-averse utility
        u = r - kappa_loss * (1.0 - r)

        # Stage 1 value via SR: V1[a] = M[a] dot v_state, where v_state ~ max Q2 per state
        v_state = np.max(Q2, axis=1)  # [V(X), V(Y)]
        V1 = M @ (v_state + w_r)  # combine learned state values with reward weight vector for flexibility

        # Policy stage 1
        logits1 = beta * (V1 - np.max(V1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Policy stage 2
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Update second-stage Q-values
        td2 = u - Q2[s, a2]
        Q2[s, a2] += eta_r * td2

        # SR update for chosen first-stage action:
        # increment toward the one-hot of visited state (one-step SR with forgetting)
        one_hot_state = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        # Apply forgetting to the entire SR each trial (global drift to zero)
        M *= (1.0 - chi_forget)
        # Then update only the chosen action's row toward the observed state feature
        M[a1] += eta_m * (one_hot_state - M[a1])

        # Update reward weight vector based on utility prediction error at the visited state feature
        # Treat the one-hot state as a feature predicting utility.
        pred_u = np.dot(w_r, one_hot_state)
        pe_u = u - pred_u
        w_r += eta_r * pe_u * one_hot_state

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """Exploration-bonus planning with visit decay and rare-transition aversion.

    Core ideas:
    - Stage 1 uses model-based planning with the fixed transition matrix (common=0.7, rare=0.3).
    - Add an intrinsic exploration bonus for each second-stage state proportional to an inverse-visit term,
      with a decay factor so that visit counters partially reset over time (restless environments).
    - Introduce a "rare aversion" bias at stage 1: after a rare transition that was unrewarded,
      penalize repeating the action that produced it on the next trial.
    - Stage 2 is standard model-free.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage action (0 or 1).
    reward : array-like of float (0 or 1)
        Received reward.
    model_parameters : iterable of floats
        [alpha, beta, xi_explore, rho_rare, delta_decay]
        - alpha (0..1): Learning rate for second-stage Q-values and stage-1 MF bootstrapping.
        - beta (0..10): Inverse temperature for softmax at both stages.
        - xi_explore (0..1): Weight of intrinsic exploration bonus at the state level.
        - rho_rare (0..1): Penalty applied to the first-stage action value if previous trial had unrewarded rare transition.
        - delta_decay (0..1): Decay factor applied to state visit counters each trial (higher -> slower forgetting).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, xi_explore, rho_rare, delta_decay = model_parameters
    n_trials = len(action_1)

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    # State visit counters for intrinsic bonus (decayed over time)
    visits = np.ones(2) * 1.0  # start at 1 to avoid div-by-zero; encourages initial exploration equally

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_rare_unrew = False

    eps = 1e-12

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Intrinsic exploration bonus per state: bonus ~ 1 / sqrt(visits)
        bonus_state = xi_explore * (1.0 / np.sqrt(visits + eps))

        # Model-based values with exploration bonus
        max_Q2 = np.max(Q2 + bonus_state[:, None], axis=1)  # add bonus to both actions in each state
        Q1_mb = T @ max_Q2

        # Apply rare-transition aversion penalty to repeating the action if last trial had rare+unrewarded
        penalty = np.zeros(2)
        if last_a1 is not None and last_rare_unrew:
            penalty[last_a1] -= rho_rare

        # Combine model-based with a light model-free bootstrap
        Q1 = Q1_mb + penalty + Q1_mf

        # Policy stage 1
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Policy stage 2
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Update Q2
        td2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * td2

        # Back up to stage-1 MF with obtained second-stage value
        target1 = Q2[s, a2]
        td1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1

        # Update visit counters with decay, then count the visited state
        visits *= (1.0 - delta_decay)
        visits[s] += 1.0

        # Track whether this trial was a rare transition and unrewarded
        # Rare if observed state is the uncommon one for chosen action.
        common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        rare = not common
        last_rare_unrew = (rare and (r <= 0.0))
        last_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

Notes
- All parameters are used and constrained conceptually to 0..1, except beta in 0..10.
- Each model returns the negative log-likelihood of both the first- and second-stage choices.
- No external imports are included; assume numpy as np is already available.