def cognitive_model1(action_1, state, action_2, reward, model_parameters):
    """
    Successor-representation (SR) arbitration with eligibility traces and stage-2 stickiness.

    Mechanism
    - Maintain a successor representation M over first-stage actions to second-stage states.
      M[a, s] â‰ˆ expected discounted occupancy of second-stage state s after choosing action a.
      M is learned online with an eligibility trace (lambda_sr) to allow credit assignment
      to recently active first-stage actions.
    - Stage-2 Q-values are learned model-freely via a delta rule.
    - Stage-1 model-based values are computed as q1_mb = M @ v2, where v2 is the
      current stage-2 state value (max over aliens).
    - Arbitration is purely SR-based at stage 1 (no mixing parameter), but behavior is
      temperature-controlled by beta. Stage-2 includes a stickiness bias toward repeating
      the last second-stage action within the same planet.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 within the reached state).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha_sr, alpha_val, beta, lambda_sr, kappa_s2]
        - alpha_sr in [0,1]: learning rate for the SR matrix M.
        - alpha_val in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - lambda_sr in [0,1]: eligibility trace decay for SR credit assignment.
        - kappa_s2 in [0,1]: stickiness bias to repeat prior second-stage action within the state.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_sr, alpha_val, beta, lambda_sr, kappa_s2 = model_parameters
    n_trials = len(action_1)

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Successor representation (rows: first-stage actions, cols: second-stage states)
    M = np.zeros((2, 2), dtype=float)
    # Eligibility trace over first-stage actions
    e = np.zeros(2, dtype=float)

    # Stage-2 Q-values: q2[state, action]
    q2 = np.zeros((2, 2), dtype=float)

    # Track previous second-stage action within each state for stickiness
    prev_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy with stickiness within the current state
        pref2 = q2[s, :].copy()
        if prev_a2[s] in (0, 1):
            pref2[prev_a2[s]] += kappa_s2
        pref2 -= np.max(pref2)
        pr2 = np.exp(beta * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]

        # Compute state values and SR-based stage-1 values
        v2 = np.max(q2, axis=1)  # value of each second-stage state
        q1_mb = M @ v2

        # Stage-1 policy (pure SR-based)
        pref1 = q1_mb.copy()
        pref1 -= np.max(pref1)
        pr1 = np.exp(beta * pref1)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        # Learning updates
        # 1) Stage-2 Q update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_val * pe2

        # 2) SR update with eligibility trace
        # Update eligibility for chosen first-stage action
        e *= lambda_sr
        e[a1] += 1.0

        # Feature target: one-hot over reached second-stage state
        f_s = np.array([0.0, 0.0])
        f_s[s] = 1.0

        # Update SR rows for all first-stage actions, weighted by eligibility
        for a in range(2):
            M[a, :] += alpha_sr * e[a] * (f_s - M[a, :])

        # Update stickiness memory for stage-2
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, model_parameters):
    """
    Rare-transition-sensitive hybrid with persistent MB control and lapse.

    Mechanism
    - Stage-2 values are learned model-freely (delta rule).
    - Stage-1 mixes model-based (MB) and model-free (MF) values:
        q1 = w_t * q1_mb + (1 - w_t) * q1_mf
      where q1_mb uses the fixed task transition structure (A->X, U->Y).
    - The arbitration weight w_t evolves across trials with momentum (mb_persist),
      and is down-weighted following a rare transition by xi_rare_loss.
    - A small lapse parameter epsilon mixes the chosen softmax policy with uniform
      random responding at each stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 within the reached state).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha, beta, xi_rare_loss, mb_persist, epsilon]
        - alpha in [0,1]: learning rate for both stage-2 and stage-1 MF values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - xi_rare_loss in [0,1]: proportional reduction of w_t after a rare transition.
        - mb_persist in [0,1]: momentum of MB weight over trials (how much w_t
          stays near its previous value).
        - epsilon in [0,1]: lapse probability, mixing softmax with uniform choice.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, xi_rare_loss, mb_persist, epsilon = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Fixed known transition structure for MB evaluation
    # A (0) -> X (0) common, U (1) -> Y (1) common
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Model-free values
    q1_mf = np.zeros(2)      # first-stage MF action values
    q2 = np.zeros((2, 2))    # second-stage MF values

    # Initialize arbitration weight
    w_prev = 0.5  # baseline MB weight

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-2 policy (with lapse)
        pref2 = q2[s, :].copy()
        pref2 -= np.max(pref2)
        pi2 = np.exp(beta * pref2)
        pi2 /= np.sum(pi2)
        pr2 = (1 - epsilon) * pi2 + epsilon * 0.5
        p_choice_2[t] = pr2[a2]

        # Model-based stage-1 values via fixed transitions
        v2 = np.max(q2, axis=1)       # state values
        q1_mb = T_fixed @ v2

        # Arbitration weight with persistence and rare-transition sensitivity
        # Start from pulling toward baseline 0.5 with momentum around previous weight
        w_t = 0.5 + mb_persist * (w_prev - 0.5)

        # Determine if the immediately previous transition (current trial's transition)
        # was rare relative to T_fixed; if so, reduce MB influence this trial.
        is_rare = (a1 == 0 and s == 1) or (a1 == 1 and s == 0)
        if is_rare:
            w_t *= (1.0 - xi_rare_loss)

        # Combine MB and MF for stage-1 values
        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage-1 policy (with lapse)
        pref1 = q1.copy()
        pref1 -= np.max(pref1)
        pi1 = np.exp(beta * pref1)
        pi1 /= np.sum(pi1)
        pr1 = (1 - epsilon) * pi1 + epsilon * 0.5
        p_choice_1[t] = pr1[a1]

        # Learning updates
        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update bootstrapped from obtained second-stage action value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update persistent arbitration memory
        w_prev = w_t

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, model_parameters):
    """
    Volatility-gated transition learning with certainty-weighted arbitration and Q2 decay.

    Mechanism
    - Learn a per-action transition model T online. The effective transition learning rate
      is proportional to a latent volatility signal v_t, which is itself updated from
      recent transition prediction errors (kappa_vol controls its reactivity).
    - Stage-2 Q-values are learned model-freely, with per-trial multiplicative decay
      to capture forgetting.
    - Stage-1 arbitration weight is higher when (a) transitions are certain (low entropy
      of the selected action's transition row) and (b) volatility is low. The baseline
      contribution is set by w0_mb.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Reached second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0/1 within the reached state).
    reward : array-like of float (0 or 1)
        Reward outcome per trial.
    model_parameters : iterable of 5 floats
        [alpha_r, beta, kappa_vol, w0_mb, decay_q]
        - alpha_r in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa_vol in [0,1]: volatility update gain from transition prediction errors.
        - w0_mb in [0,1]: baseline strength of model-based control at stage 1.
        - decay_q in [0,1]: per-trial multiplicative decay of second-stage Q-values.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, kappa_vol, w0_mb, decay_q = model_parameters
    n_trials = len(action_1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Learned transitions: T[a, s], rows sum to 1
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Latent volatility (starts modest)
    v = 0.5 * kappa_vol + 0.25  # small positive initial volatility

    # Stage-1 MF values and Stage-2 Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply decay to stage-2 values before computing policies/updates
        q2 *= (1.0 - decay_q)

        # Stage-2 policy
        pref2 = q2[s, :].copy()
        pref2 -= np.max(pref2)
        pr2 = np.exp(beta * pref2)
        pr2 /= np.sum(pr2)
        p_choice_2[t] = pr2[a2]

        # Model-based evaluation from learned transitions
        v2 = np.max(q2, axis=1)
        q1_mb = T @ v2

        # Compute certainty from entropy of current action's transition row
        row = np.clip(T[a1, :], 1e-12, 1.0)
        H = -(row[0] * np.log2(row[0]) + row[1] * np.log2(row[1]))  # in [0,1] for binary
        certainty = 1.0 - H

        # Arbitration: more MB when certainty is high and volatility is low
        w = w0_mb * certainty * (1.0 - v) + (1.0 - w0_mb) * 0.5

        # Combine MF and MB at stage 1
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        pref1 = q1.copy()
        pref1 -= np.max(pref1)
        pr1 = np.exp(beta * pref1)
        pr1 /= np.sum(pr1)
        p_choice_1[t] = pr1[a1]

        # Learning updates
        # Stage-2 Q update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Stage-1 MF update (bootstrapped on realized second-stage action value)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

        # Transition learning with volatility-gated rate
        # One-step delta update toward the observed state
        alpha_T = np.clip(v, 0.0, 1.0)
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T[a1, :] = (1.0 - alpha_T) * T[a1, :] + alpha_T * target_row
        T[a1, :] /= (np.sum(T[a1, :]) + 1e-12)

        # Volatility update from transition prediction error magnitude
        pe_T = 1.0 - T[a1, s]  # how surprising the observed transition remains
        v = np.clip((1.0 - kappa_vol) * v + kappa_vol * pe_T, 0.0, 1.0)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll