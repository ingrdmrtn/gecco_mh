def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the learned-transitions model-based planner
    with risk sensitivity and perseveration (matching cognitive_model2).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list/tuple): [alpha, beta, alphaT, rho, phi]
            - alpha: reward learning rate for second-stage Q and p_hat updates.
            - beta: inverse temperature used at both stages.
            - alphaT: transition learning rate for P(state | action_1).
            - rho: risk sensitivity; subtracts rho * variance from second-stage Q.
            - phi: perseveration bias added to the last chosen stage-1 action.
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the two aliens in each second-stage state:
            state 0: [drift1[t], drift2[t]], state 1: [drift3[t], drift4[t]].

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    rng = np.random.default_rng()

    alpha, beta, alphaT, rho, phi = parameters

    # True environment transitions (used to draw the actual second-stage state)
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]], dtype=float)

    # Agent's internal estimates and values
    T_hat = np.array([[0.6, 0.4],
                      [0.4, 0.6]], dtype=float)  # initialized as in fitting code
    q2 = np.zeros((2, 2), dtype=float)           # second-stage Q-values
    p_hat = np.full((2, 2), 0.5, dtype=float)    # estimated Bernoulli reward prob for variance

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    last_a1 = None

    for t in range(n_trials):
        # Trial-wise true reward probabilities for each state-action
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Risk-adjusted planning
        var = p_hat * (1.0 - p_hat)
        q2_risk = q2 - rho * var

        # Stage-1 MB values from current transition estimate
        max_q2 = np.max(q2_risk, axis=1)   # best option value in each state
        q1 = T_hat @ max_q2

        # Perseveration bias on stage-1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += phi

        # Stage-1 choice
        logits1 = beta * q1 + bias1
        logits1 = logits1 - np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        a1 = rng.choice([0, 1], p=p1)

        # Transition to second-stage state (using true environment transitions)
        s2 = rng.choice([0, 1], p=T_true[a1])

        # Stage-2 choice using risk-adjusted values
        logits2 = beta * q2_risk[s2]
        logits2 = logits2 - np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        a2 = rng.choice([0, 1], p=p2)

        # Outcome based on drifting reward probabilities
        r = int(rng.random() < reward_probs[s2][a2])

        # Update transition belief for the chosen stage-1 action toward observed state
        oh = np.array([0.0, 0.0])
        oh[s2] = 1.0
        T_hat[a1] = (1.0 - alphaT) * T_hat[a1] + alphaT * oh
        T_hat[a1] = np.clip(T_hat[a1], 1e-6, 1.0)
        T_hat[a1] /= np.sum(T_hat[a1])

        # Update second-stage Q-values and reward probability estimates
        delta = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta
        p_hat[s2, a2] += alpha * (r - p_hat[s2, a2])

        # Store and proceed
        last_a1 = a1

        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward