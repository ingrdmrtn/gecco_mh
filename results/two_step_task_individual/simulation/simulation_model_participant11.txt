def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the associability-modulated learning model
    with transition-dependent credit assignment and perseveration.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list): [alpha0, beta, phi, xi, stick]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities for:
            - state 0, action 0 -> drift1
            - state 0, action 1 -> drift2
            - state 1, action 0 -> drift3
            - state 1, action 1 -> drift4

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha0, beta, phi, xi, stick = parameters

    # Transition structure: from action a1, common transition to state a1 with prob 0.7
    p_common = 0.7
    transition_matrix = np.array([[p_common, 1.0 - p_common],
                                  [1.0 - p_common, p_common]])

    rng = np.random.default_rng()

    # Initialize values
    q1 = np.zeros(2)           # Stage-1 action values
    q2 = np.zeros((2, 2))      # Stage-2 state-action values
    A = np.full((2, 2), 0.5)   # Associability per (state, action)

    # Perseveration memory (last choices)
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    # Storage
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # ------- Stage 1 choice with perseveration -------
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        pref1 = beta * q1 + bias1
        pref1 = pref1 - np.max(pref1)  # numerical stability
        probs_1 = np.exp(pref1)
        probs_1 /= np.sum(probs_1)

        a1 = rng.choice([0, 1], p=probs_1)

        # ------- Transition to Stage 2 state -------
        s = rng.choice([0, 1], p=transition_matrix[a1])

        # ------- Stage 2 choice with state-specific perseveration -------
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick

        pref2 = beta * q2[s] + bias2
        pref2 = pref2 - np.max(pref2)
        probs_2 = np.exp(pref2)
        probs_2 /= np.sum(probs_2)

        a2 = rng.choice([0, 1], p=probs_2)

        # ------- Outcome sampling from drifting reward probabilities -------
        reward_probs = [[drift1[t], drift2[t]], [drift3[t], drift4[t]]]
        r = int(rng.random() < reward_probs[s][a2])

        # ------- Learning updates -------
        # Stage-2 associability-modulated update
        alpha2_t = alpha0 * (0.5 + 0.5 * np.clip(A[s, a2], 0.0, 1.0))
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2_t * delta2

        # Update associability (Pearceâ€“Hall-like)
        A[s, a2] = (1.0 - phi) * A[s, a2] + phi * np.clip(abs(delta2), 0.0, 1.0)

        # Transition type
        is_common = (s == a1)

        # Target for stage-1 credit assignment uses the updated q2
        target = q2[s, a2]

        if is_common:
            # Standard MF update to chosen first-stage action
            delta_chosen = target - q1[a1]
            q1[a1] += alpha0 * delta_chosen
        else:
            # Transition-dependent credit assignment (shift some credit to unchosen)
            other = 1 - a1
            delta_chosen = target - q1[a1]
            delta_other = target - q1[other]
            q1[a1] += alpha0 * (1.0 - xi) * delta_chosen
            q1[other] += alpha0 * xi * delta_other

        # ------- Update perseveration memory -------
        prev_a1 = a1
        prev_a2[s] = a2

        # ------- Log trial data -------
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward