def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the SR-inspired hybrid with decay (forgetting).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list/tuple): [alpha, beta, sigma, w_sr, decay]
            - alpha in [0,1]: Learning rate for value updates (both stages).
            - beta in [0,10]: Inverse temperature for softmax at both stages.
            - sigma in [0,1]: Learning rate for the successor map (action->state occupancy).
            - w_sr in [0,1]: Weight on SR-derived value at stage 1 (vs MF cache).
            - decay in [0,1]: Per-trial forgetting toward zero for all Q-values.
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the two actions on each second-stage state:
              state 0: [drift1, drift2]
              state 1: [drift3, drift4]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, sigma, w_sr, decay = parameters

    rng = np.random.default_rng()

    # True environment transition matrix (fixed)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]], dtype=float)

    # Initialize SR map (action -> state occupancy), starts uniform like in fitting code
    M = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2, dtype=float)      # for actions at stage 1
    q_stage2_mf = np.zeros((2, 2), dtype=float) # for [state, action] at stage 2

    # Storage
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Construct current reward probabilities for each state-action
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]], dtype=float)

        # Compute SR-derived stage-1 values using current MF stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)   # shape (2,)
        q_stage1_sr = M @ max_q_stage2               # shape (2,)

        # Hybrid stage-1 value (SR vs MF) and softmax choice
        q1 = (1.0 - w_sr) * q_stage1_mf + w_sr * q_stage1_sr
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = rng.choice([0, 1], p=probs_1)

        # Environment transition to second-stage state
        s = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage-2 choice with softmax over MF values
        q2 = q_stage2_mf[s, :]
        q2c = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = rng.choice([0, 1], p=probs_2)

        # Sample reward from drifting Bernoulli probabilities
        r_prob = reward_probs[s, a2]
        r = int(rng.random() < r_prob)

        # Forgetting/decay toward zero (applied before learning updates)
        q_stage1_mf *= (1.0 - decay)
        q_stage2_mf *= (1.0 - decay)

        # Stage-2 TD update (model-free)
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update bootstrapping from updated stage-2 value
        target1 = q_stage2_mf[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # SR map update for chosen action's row toward one-hot of reached state
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            M[a1, sp] += sigma * (target - M[a1, sp])

        # Renormalize the updated row (numerical stability)
        row_sum = np.sum(M[a1, :])
        if row_sum > 0:
            M[a1, :] /= row_sum

        # Log trial data
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward