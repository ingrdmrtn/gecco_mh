def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards using the provided hybrid model-based/model-free
    2-step model with eligibility traces and perseveration.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or tuple): [alpha, beta, w_mb, lambda_et, kappa]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the two actions in each second-stage state:
            state 0: [drift1, drift2], state 1: [drift3, drift4]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, w_mb, lambda_et, kappa = parameters

    rng = np.random.default_rng()

    transition_matrix = np.array([[0.7, 0.3],  # from action 0 (A) to states [0,1]
                                  [0.3, 0.7]]) # from action 1 (U) to states [0,1]

    # Initialize values
    q_stage1_mf = np.zeros(2)           # model-free value for first-stage actions
    q_stage2 = np.full((2, 2), 0.5)     # second-stage Q-values initialized to 0.5

    # Storage
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    last_a1 = None

    for t in range(n_trials):
        # Trial-wise reward probabilities for second stage
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Compute model-based first-stage values from second-stage values
        max_q_stage2 = np.max(q_stage2, axis=1)     # best option on each second-stage state
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Perseveration bias on first-stage choices
        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] += kappa

        # Net first-stage action values
        q1 = (1.0 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb + bias

        # Softmax for stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = rng.choice([0, 1], p=probs_1)

        # Transition to second-stage state
        s = rng.choice([0, 1], p=transition_matrix[a1])

        # Softmax for stage 2 within the observed state
        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = rng.choice([0, 1], p=probs_2)

        # Generate reward from drifting probabilities
        r = int(rng.random() < reward_probs[s][a2])

        # Learning updates (match fitting code logic and order)
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        delta1_mf = (q_stage2[s, a2]) - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (lambda_et * delta2 + (1.0 - lambda_et) * delta1_mf)

        # Update perseveration memory
        last_a1 = a1

        # Store outcomes
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward