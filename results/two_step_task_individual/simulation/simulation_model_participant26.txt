import numpy as np

def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards using the risk-sensitive model-free control with
    transition–outcome interaction and leaky choice traces (matching cognitive_model2).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or tuple): [alpha, beta, gamma_risk, xi_interact, tau_trace]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities for
            second-stage state/action pairs:
              - state 0: action 0 -> drift1[t], action 1 -> drift2[t]
              - state 1: action 0 -> drift3[t], action 1 -> drift4[t]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Reached second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage actions (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    alpha, beta, gamma_risk, xi_interact, tau_trace = parameters

    rng = np.random.default_rng()

    # Fixed transition matrix (common = 0.7 to matched state, rare = 0.3 to mismatched)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize value functions
    q1 = np.zeros(2)          # First-stage Q-values for actions A(0) and U(1)
    q2 = np.zeros((2, 2))     # Second-stage Q-values: [state, action]

    # Leaky choice traces
    trace1 = np.zeros(2)      # First-stage traces
    trace2 = np.zeros((2, 2)) # Second-stage traces per state

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    # Previous trial info for transition–outcome interaction
    prev_a1 = None
    prev_common = None
    prev_r = None

    for t in range(n_trials):
        # Build reward probabilities for this trial
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # First-stage bias from leaky trace
        bias1 = trace1.copy()

        # Transition–outcome interaction bias applied to staying on previous action
        if prev_a1 is not None and prev_common is not None and prev_r is not None:
            signed_r = 1.0 if prev_r == 1 else -1.0
            signed_trans = 1.0 if prev_common else -1.0
            interact_bias = xi_interact * (signed_r * signed_trans)
            bias1[prev_a1] += interact_bias

        # First-stage choice via softmax over q1 + bias
        logits1 = beta * (q1 + bias1 - np.max(q1 + bias1))
        probs1 = np.exp(logits1)
        probs1 /= (probs1.sum() + 1e-12)
        a1 = rng.choice([0, 1], p=probs1)

        # Transition to second-stage state
        s2 = rng.choice([0, 1], p=transition_matrix[a1])

        # Second-stage choice with state-specific leaky trace bias
        bias2 = trace2[s2].copy()
        logits2 = beta * (q2[s2] + bias2 - np.max(q2[s2] + bias2))
        probs2 = np.exp(logits2)
        probs2 /= (probs2.sum() + 1e-12)
        a2 = rng.choice([0, 1], p=probs2)

        # Generate reward from drifting probabilities
        r = int(rng.random() < reward_probs[s2][a2])

        # Risk-sensitive utility
        u = (r + 1e-12) ** gamma_risk

        # Model-free learning at second stage
        td2 = u - q2[s2, a2]
        q2[s2, a2] += alpha * td2

        # Model-free first-stage update bootstrapping from updated second-stage value
        target1 = q2[s2, a2]
        td1 = target1 - q1[a1]
        q1[a1] += alpha * td1

        # Update leaky choice traces
        trace1 *= tau_trace
        trace1[a1] += 1.0
        trace2[s2] *= tau_trace
        trace2[s2, a2] += 1.0

        # Store outputs
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

        # Update previous trial info
        prev_a1 = a1
        prev_r = r
        prev_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)

    return stage1_choice, state2, stage2_choice, reward