def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the MB-MF mixture model with learned transition model
    and value forgetting at stage 2.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list): [alpha_q, beta, w_mix, alpha_T, zeta]
            - alpha_q: learning rate for Q-value updates (both stages).
            - beta: inverse temperature for softmax at both stages.
            - w_mix: mixture weight of model-based value at stage 1.
            - alpha_T: learning rate for transition probabilities T(s | a1).
            - zeta: forgetting/decay rate applied to all second-stage Q-values each trial.
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities for each state-action:
            state 0: [drift1, drift2], state 1: [drift3, drift4].

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha_q, beta, w_mix, alpha_T, zeta = parameters

    # Environment (true) transition matrix used to generate states
    env_T = np.array([[0.7, 0.3],
                      [0.3, 0.7]])

    rng = np.random.default_rng()

    # Learned transition model and value functions
    T_learned = np.full((2, 2), 0.5)  # initialized uniform, learned online
    q1_mf = np.zeros(2)               # model-free first-stage Q-values
    q2 = np.zeros((2, 2))             # second-stage Q-values [state, action]

    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Trial-wise reward probabilities for the two second-stage states and actions
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Compute model-based first-stage values from learned transitions and second-stage Q-values
        max_q2 = np.max(q2, axis=1)       # best action per second-stage state
        q1_mb = T_learned @ max_q2        # expected value under learned T

        # Mixture of MB and MF at stage 1
        q1_mix = w_mix * q1_mb + (1.0 - w_mix) * q1_mf

        # Stage-1 choice via softmax
        logits1 = beta * q1_mix
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        a1 = rng.choice([0, 1], p=p1)

        # Transition to second-stage state using environment transitions
        s2 = rng.choice([0, 1], p=env_T[a1])

        # Stage-2 choice via softmax over q2[s2]
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        a2 = rng.choice([0, 1], p=p2)

        # Outcome
        r = int(rng.random() < reward_probs[s2][a2])

        # Update learned transition model T(s | a1) with delta rule, row-wise
        T_learned[a1, :] = (1.0 - alpha_T) * T_learned[a1, :]
        T_learned[a1, s2] += alpha_T
        # (Row stays normalized because we shrink then add alpha_T to one entry)

        # Apply forgetting to all second-stage Q-values
        q2 *= (1.0 - zeta)

        # Stage-2 TD update
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_q * pe2

        # Stage-1 MF update toward updated second-stage value (as in fitting code)
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # Record
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward