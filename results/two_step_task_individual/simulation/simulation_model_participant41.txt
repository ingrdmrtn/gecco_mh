def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards using the Hybrid RL with learned transition model and stage-1 stickiness.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or array): [alpha, beta, w, eta_T, stick]
            - alpha: learning rate for MF value updates (both stages).
            - beta: inverse temperature for softmax (both stages).
            - w: weight on model-based plan at stage 1 (1=fully MB).
            - eta_T: learning rate for updating the transition model (agent's internal).
            - stick: stage-1 perseveration strength (additive bias to previous a1).
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the four second-stage state-action pairs:
            - state 0, action 0 -> drift1[t]
            - state 0, action 1 -> drift2[t]
            - state 1, action 0 -> drift3[t]
            - state 1, action 1 -> drift4[t]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np
    rng = np.random.default_rng()

    alpha, beta, w, eta_T, stick = parameters

    # Environment (true) transition matrix
    transition_matrix_env = np.array([[0.7, 0.3],
                                      [0.3, 0.7]], dtype=float)

    # Agent's learned transition model (initialized as in fitting code)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Initialize values
    q1_mf = np.zeros(2)               # stage-1 MF values
    q2_mf = np.ones((2, 2)) * 0.5     # stage-2 MF values per state-action

    # Tracking arrays for outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    last_a1 = None

    for t in range(n_trials):
        # Build trial-wise reward probability table
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]], dtype=float)

        # Compute model-based estimates for stage 1 from learned transitions
        max_q2 = np.max(q2_mf, axis=1)     # best action value in each second-stage state
        q1_mb = T @ max_q2                 # MB values with learned T

        # Blend MF and MB for stage 1, add stickiness bias to previous choice
        q1_base = (1 - w) * q1_mf + w * q1_mb
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stick
        q1_eff = q1_base + bias1

        # Softmax for stage 1
        q1c = q1_eff - np.max(q1_eff)
        p1_unnorm = np.exp(beta * q1c)
        p1 = p1_unnorm / np.sum(p1_unnorm)
        a1 = int(rng.choice([0, 1], p=p1))

        # Environment generates second-stage state given the chosen action
        s = int(rng.choice([0, 1], p=transition_matrix_env[a1]))

        # Stage-2 choice policy from MF values in observed state
        q2 = q2_mf[s].copy()
        q2c = q2 - np.max(q2)
        p2_unnorm = np.exp(beta * q2c)
        p2 = p2_unnorm / np.sum(p2_unnorm)
        a2 = int(rng.choice([0, 1], p=p2))

        # Reward from drifting Bernoulli probabilities
        r = int(rng.random() < reward_probs[s, a2])

        # Agent updates its learned transition model T toward observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1 - eta_T) * T[a1] + eta_T * target
        T[a1] = T[a1] / np.sum(T[a1])  # ensure normalization

        # MF value updates
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Persist for next trial
        last_a1 = a1

        # Store outputs
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward