import numpy as np

def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the provided hybrid model-based/model-free 2-step model
    with eligibility trace (lambda) and separate inverse temperatures.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or array): [alpha, beta1, beta2, w, lam]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the two aliens on planet X (drift1, drift2) and the two on planet Y (drift3, drift4).

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    alpha, beta1, beta2, w, lam = parameters

    rng = np.random.default_rng()
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Model-free values
    q1_mf = np.zeros(2)       # stage-1 MF values for spaceships (A=0, U=1)
    q2_mf = np.zeros((2, 2))  # stage-2 MF values for aliens on planet X=0 and Y=1

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Reward probability matrix for this trial:
        # rows = planet (0=X, 1=Y), cols = alien (0,1)
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]])

        # Compute model-based values for stage 1 by projecting best stage-2 values
        max_q2 = np.max(q2_mf, axis=1)        # best alien on each planet
        q1_mb = transition_matrix @ max_q2    # expected value for each spaceship

        # Hybrid arbitration
        q1 = w * q1_mb + (1 - w) * q1_mf

        # Stage-1 choice via softmax
        q1_stable = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1_stable)
        p_stage1 = exp_q1 / np.sum(exp_q1)
        a1 = rng.choice([0, 1], p=p_stage1)

        # Transition to stage-2 state
        s = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage-2 choice via softmax
        q2 = q2_mf[s]
        q2_stable = q2 - np.max(q2)
        exp_q2 = np.exp(beta2 * q2_stable)
        p_stage2 = exp_q2 / np.sum(exp_q2)
        a2 = rng.choice([0, 1], p=p_stage2)

        # Reward sampled from drifting probabilities
        r = int(rng.random() < reward_probs[s, a2])

        # Store outcomes
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

        # Learning updates (match fitting code)
        q2_old = q2_mf[s, a2]
        q1_old = q1_mf[a1]

        # Stage-2 TD update
        delta2 = r - q2_old
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace
        delta1_bootstrap = q2_old - q1_old
        delta1_outcome = r - q1_old
        q1_mf[a1] += alpha * ((1 - lam) * delta1_bootstrap + lam * delta1_outcome)

    return stage1_choice, state2, stage2_choice, reward