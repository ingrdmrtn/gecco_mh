filename,function_name,bic,code,param_names
iter0_run0_participant0.json,cognitive_model1,493.2443185259866,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    
    This model blends model-based (MB) and model-free (MF) control at stage 1 with
    an arbitration weight that decreases with anxiety (higher STAI => more MF).
    Stage-2 values are learned via TD; Stage-1 MF values use a two-step TD with
    an eligibility trace. One softmax (beta) is used at both stages.
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for both stages
    - lambda_e: [0,1] eligibility trace weighting for propagating stage-2 RPE to stage-1 MF
    - w_base: [0,1] baseline MB weight (before anxiety)
    - kappa: [0,1] strength of anxiety effect on MB weight (w_eff = w_base * (1 - kappa*stai))
    - beta: [0,10] inverse temperature for softmax at both stages
    
    Inputs:
    - action_1: array of length T with observed first-stage actions (0 or 1)
    - state: array of length T with observed second-stage states (0: X, 1: Y)
    - action_2: array of length T with observed second-stage actions (0 or 1)
    - reward: array of length T with received rewards (e.g., 0/1 or [0,1])
    - stai: array-like with one element in [0,1]; higher => higher anxiety
    - model_parameters: array-like [alpha, lambda_e, w_base, kappa, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha, lambda_e, w_base, kappa, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Transition matrix: rows = action at stage 1 (A=0, U=1), cols = state (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated arbitration weight (more anxiety => lower MB weight)
    w_eff = np.clip(w_base * (1.0 - kappa * stai_score), 0.0, 1.0)

    # Action value containers
    q_stage1_mf = np.zeros(2)          # MF values for stage-1 actions
    q_stage2 = np.zeros((2, 2))        # Q[state, action] for stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based value for stage-1: expected max over states given transitions
        max_q_stage2 = np.max(q_stage2, axis=1)            # max over actions, for each state
        q_stage1_mb = transition_matrix @ max_q_stage2     # expected value per stage-1 action

        # Hybrid stage-1 value
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Policy stage-1
        logits1 = beta * (q1 - np.max(q1))
        exp1 = np.exp(logits1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Policy stage-2 (conditioned on observed state)
        s = state[t]
        q2_s = q_stage2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        exp2 = np.exp(logits2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # TD updates
        # Stage-2 TD error
        r = reward[t]
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 MF update: bootstrap to stage-2 chosen value + eligibility trace for delta2
        bootstrap = q_stage2[s, a2]
        q_stage1_mf[a1] += alpha * (bootstrap - q_stage1_mf[a1]) + alpha * lambda_e * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'lambda_e', 'w_base', 'kappa', 'beta']"
iter0_run0_participant0.json,cognitive_model2,494.68443787983813,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid with anxiety-modulated perseveration and transition belief distortion.
    
    This model combines MB and MF control (fixed 50/50) at stage 1, adds a
    perseveration (choice stickiness) bonus on stage-1 choices that is
    strengthened or weakened by anxiety, and distorts the perceived transition
    structure as a function of anxiety (altering MB planning).
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for both stages
    - beta: [0,10] inverse temperature for both stages
    - b0: [0,1] baseline perseveration strength added to last chosen stage-1 action
    - eta: [0,1] strength of anxiety effect on perseveration (b_eff = b0*(1 + eta*(2*stai-1)))
    - xi: [0,1] strength of anxiety-driven distortion of transition certainty
           (effective exponent gamma = 1 + xi*(2*stai-1))
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, beta, b0, eta, xi]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha, beta, b0, eta, xi = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Initialize values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Anxiety-modulated perseveration and transition distortion
    b_eff = b0 * (1.0 + eta * (2.0 * stai_score - 1.0))     # can be < b0 if stai < 0.5
    gamma = 1.0 + xi * (2.0 * stai_score - 1.0)             # >1 => sharpen common, <1 => flatten
    # Distorted perceived transition probabilities
    p_common = (0.7 ** gamma) / ((0.7 ** gamma) + (0.3 ** gamma))
    transition_matrix = np.array([[p_common, 1 - p_common],
                                  [1 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None  # for perseveration

    for t in range(n_trials):
        # Model-based plan from distorted transitions
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF equally (fixed hybrid without extra parameter)
        q1 = 0.5 * q_stage1_mb + 0.5 * q_stage1_mf

        # Add perseveration bonus to the last chosen stage-1 action
        if last_a1 is not None:
            bonus = np.zeros(2)
            bonus[last_a1] = b_eff
            q1 = q1 + bonus

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax over current state's Q)
        s = state[t]
        logits2 = beta * (q_stage2[s] - np.max(q_stage2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # Stage-2 TD
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2
        # Stage-1 MF bootstrap toward the new stage-2 value
        q_stage1_mf[a1] += alpha * (q_stage2[s, a2] - q_stage1_mf[a1])

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'b0', 'eta', 'xi']"
iter0_run0_participant0.json,cognitive_model3,474.26855662726996,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric learning and anxiety-shifted arbitration with stage-2 stickiness.
    
    This model uses separate learning rates for positive vs. negative outcomes at
    stage 2, propagates value to stage 1 (MF), and arbitrates at stage 1 using a
    weight that decreases with anxiety (higher STAI => more MF). It also adds a
    stage-2 action stickiness that increases with anxiety, capturing habit-like
    tendencies under stress.
    
    Parameters (bounds):
    - alpha_pos: [0,1] learning rate when outcome > current Q (positive PE)
    - alpha_neg: [0,1] learning rate when outcome < current Q (negative PE)
    - w_base: [0,1] baseline MB arbitration weight at stage 1
    - stick2: [0,1] baseline stage-2 stickiness bonus for repeating last action in a state
    - beta: [0,10] inverse temperature for both stages
    
    Anxiety usage:
    - Arbitration: w_eff = clip(w_base * (1 - stai), 0, 1)
    - Stage-2 stickiness: stick_eff = stick2 * (1 + 0.5*(2*stai-1))  # stronger with higher anxiety
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha_pos, alpha_neg, w_base, stick2, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha_pos, alpha_neg, w_base, stick2, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Transition matrix (true, used for MB plan)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated arbitration and stage-2 stickiness
    w_eff = np.clip(w_base * (1.0 - stai_score), 0.0, 1.0)
    stick_eff = stick2 * (1.0 + 0.5 * (2.0 * stai_score - 1.0))  # ranges ~[0.5*stick2, 1.5*stick2]

    # Value tables
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # For stage-2 stickiness: last action taken in each state
    last_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB plan for stage-1
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid arbitration
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness bonus within the current state
        s = state[t]
        q2_s = q_stage2[s].copy()
        if last_a2[s] != -1:
            q2_s[last_a2[s]] += stick_eff

        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates with asymmetric rates
        r = reward[t]
        pe2 = r - q_stage2[s, a2]
        alpha = alpha_pos if pe2 >= 0 else alpha_neg
        q_stage2[s, a2] += alpha * pe2

        # Propagate to stage-1 MF (simple bootstrap)
        q_stage1_mf[a1] += alpha * (q_stage2[s, a2] - q_stage1_mf[a1])

        # Update stickiness memory for this state
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_pos', 'alpha_neg', 'w_base', 'stick2', 'beta']"
iter0_run0_participant1.json,cognitive_model1,534.452862529912,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated arbitration and eligibility trace.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., alien indices W/S on X, P/H on Y).
    reward : array-like of float (0 or 1)
        Received coins on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; used to modulate arbitration toward model-free control.
    model_parameters : list or array
        [alpha, beta, w0, lambda_e, gamma_anx]
        - alpha in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w0 in [0,1]: baseline weight on model-based control at stage 1.
        - lambda_e in [0,1]: eligibility trace for propagating second-stage PE to first-stage MF.
        - gamma_anx in [0,1]: strength of anxiety-driven shift toward model-free control.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """"""
    alpha, beta, w0, lambda_e, gamma_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Action value tables
    q_stage1_mf = np.zeros(2)               # model-free Q at stage 1
    q_stage2_mf = np.zeros((2, 2))          # model-free Q at stage 2 (state x action)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration: higher anxiety -> more MF (lower w)
    w = w0 * (1.0 - gamma_anx * stai_val)
    w = max(0.0, min(1.0, w))

    for t in range(n_trials):
        # Model-based Q at stage 1: Q_MB = T * max_a Q2(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Q at stage 1
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Softmax policy at stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy for realized state
        s2 = state[t]
        q2 = q_stage2_mf[s2]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD errors
        delta2 = r - q_stage2_mf[s2, a2]
        delta1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]  # bootstrap via observed state-action value

        # Updates
        q_stage2_mf[s2, a2] += alpha * delta2
        # Eligibility trace to propagate second-stage PE to first-stage MF
        q_stage1_mf[a1] += alpha * (delta1 + lambda_e * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'w0', 'lambda_e', 'gamma_anx']"
iter0_run0_participant10.json,cognitive_model1,297.42867752195815,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated arbitration and eligibility.
    
    This model blends model-based (MB) and model-free (MF) action values at stage 1.
    The MB weight is a function of the participant's anxiety (stai), allowing
    more anxious participants to shift arbitration. An eligibility trace from
    stage 2 reward to stage 1 MF values is also scaled by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the planet-specific aliens).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0]. Higher means more anxious.
    model_parameters : array-like of float
        [alpha, beta, w_base, w_sens]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - w_base in [0,1]: baseline MB weight at stai=0.
        - w_sens in [0,1]: sensitivity of MB weight to anxiety (signed effect via (0.5 - stai)).
          Effective MB weight = clip(w_base + w_sens * (0.5 - stai), 0, 1).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """"""
    alpha, beta, w_base, w_sens = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Anxiety-modulated arbitration and eligibility
    w_mb = w_base + w_sens * (0.5 - st)  # higher st reduces MB if w_sens>0
    w_mb = np.clip(w_mb, 0.0, 1.0)
    # Eligibility increases with anxiety: more anxious -> stronger credit assignment to stage 1
    lam = 0.3 + 0.7 * np.clip(st, 0.0, 1.0)  # in [0.3, 1.0]

    for t in range(n_trials):
        # Compute MB Q at stage 1 from current MF stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # value of each planet
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value of each spaceship

        # Hybrid Q at stage 1
        q1 = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # Policy and likelihood at stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Policy and likelihood at stage 2, conditional on observed state
        s = int(state[t])
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility: towards both the stage-2 value and final reward
        # First bootstrapped update toward Q2
        delta1_boot = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1_boot
        # Then eligibility update from final reward
        q_stage1_mf[a1] += alpha * lam * (r - q_stage1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_base', 'w_sens']"
iter0_run0_participant10.json,cognitive_model2,334.50945573257496,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based policy with reward-asymmetric learning and anxiety-modulated perseveration.
    
    This model uses a model-based first-stage policy (planning via the transition matrix),
    while second-stage values are learned model-free with separate learning rates for
    rewarded vs. unrewarded outcomes. Anxiety increases the tendency to perseverate
    (repeat previous actions) at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = A, 1 = U).
    state : array-like of int
        Second-stage state per trial (0 = X, 1 = Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0]. Higher means more anxious.
    model_parameters : array-like of float
        [alpha_win, alpha_loss, beta, stick_base]
        - alpha_win in [0,1]: learning rate when reward == 1.
        - alpha_loss in [0,1]: learning rate when reward == 0.
        - beta in [0,10]: inverse temperature.
        - stick_base in [0,1]: baseline perseveration weight.
          Effective stickiness = stick_base * (0.5 + stai/2), increasing with anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """"""
    alpha_win, alpha_loss, beta, stick_base = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 MF values
    q_stage2 = np.zeros((2, 2))

    # Perseveration terms (initialized neutral)
    prev_a1 = None
    prev_a2 = {0: None, 1: None}

    # Anxiety-modulated stickiness
    stick = stick_base * (0.5 + 0.5 * np.clip(st, 0.0, 1.0))  # increases with stai

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]
        alpha_t = alpha_win if r >= 0.5 else alpha_loss

        # MB evaluation at stage 1 from current Q(s2)
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        # Stage-1 policy and likelihood
        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration within each planet
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick

        q2_s = q_stage2[s]
        logits2 = beta * q2_s + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates at stage 2
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_t * delta2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_win', 'alpha_loss', 'beta', 'stick_base']"
iter0_run0_participant10.json,cognitive_model3,303.033734691552,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive learning with anxiety-modulated volatility and MB/MF mixing.
    
    This model adapts its learning rate on each trial based on a simple
    volatility proxy: the absolute prediction error. Anxiety increases the
    assumed volatility, leading to larger adaptive learning rates. First-stage
    choices are a hybrid of model-based and model-free values with a fixed
    mixing weight. Second-stage values are learned model-free with adaptive alpha.
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = A, 1 = U).
    state : array-like of int
        Second-stage state per trial (0 = X, 1 = Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0]. Higher means more anxious.
    model_parameters : array-like of float
        [base_alpha, beta, vol_base, vol_sens, w_mb]
        - base_alpha in [0,1]: minimum learning rate.
        - beta in [0,10]: inverse temperature for both stages.
        - vol_base in [0,1]: baseline gain on PE for adaptive alpha.
        - vol_sens in [0,1]: how much anxiety increases that gain.
          Effective volatility gain g = vol_base + vol_sens * stai.
        - w_mb in [0,1]: weight on model-based values at stage 1 (1-w_mb = MF weight).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """"""
    base_alpha, beta, vol_base, vol_sens, w_mb = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Effective volatility gain
    g = vol_base + vol_sens * np.clip(st, 0.0, 1.0)
    g = np.clip(g, 0.0, 1.0)  # keep within [0,1] to respect overall bounds

    # Eligibility fixed modest for stability
    lam = 0.5 + 0.3 * np.clip(st, 0.0, 1.0)

    for t in range(n_trials):
        # MB estimate from current MF stage-2 values
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid Q at stage 1
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2_s = q2_mf[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observed outcome
        r = reward[t]

        # Compute PE and adaptive learning rate
        pe2 = r - q2_mf[s, a2]
        alpha_t = base_alpha + g * np.abs(pe2)
        alpha_t = np.clip(alpha_t, 0.0, 1.0)

        # Update stage-2
        q2_mf[s, a2] += alpha_t * pe2

        # Update stage-1 MF via bootstrapping and eligibility to outcome
        # Bootstrapped target from updated Q2
        pe1_boot = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_t * pe1_boot
        # Eligibility to final reward
        q1_mf[a1] += alpha_t * lam * (r - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['base_alpha', 'beta', 'vol_base', 'vol_sens', 'w_mb']"
iter0_run0_participant11.json,cognitive_model1,382.66640696544255,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-modulated arbitration and stickiness.
    
    This model combines model-free (MF) and model-based (MB) action values at stage 1.
    Anxiety reduces the MB arbitration weight and increases perseveration.
    Stage 2 values are learned via TD(0), and MF stage-1 values are updated with an eligibility trace.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed choices at stage 1 (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Observed choices at stage 2 (0 or 1; alien index within visited planet).
    reward : array-like of float
        Reward received on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Trait anxiety score scaled to [0,1]; we use stai[0] as a scalar.
    model_parameters : array-like of float
        Parameters with bounds:
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w0 in [0,1]: baseline MB arbitration weight.
        - lam in [0,1]: eligibility trace for MF credit from stage 2 to stage 1.
        - kappa in [0,1]: base perseveration strength (converted to bias term).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, w0, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # rows: action1 (A,U), cols: next state (X,Y)

    # Q-values
    q1_mf = np.zeros(2)          # MF values for stage-1 actions A,U
    q2 = np.zeros((2, 2))        # Q2[state, action2]

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration traces (last choices), implemented as bias terms
    prev_a1 = 0
    prev_a2 = np.zeros(2, dtype=int)  # per-state last action2

    # Anxiety modulation:
    # - Reduce MB weight with anxiety (e.g., higher anxiety shifts toward MF control)
    w_eff = np.clip(w0 * (1.0 - 0.5 * stai_val), 0.0, 1.0)
    # - Increase stickiness with anxiety
    kappa_eff = kappa * (1.0 + stai_val)

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values from current stage-2 values
        max_q2 = np.max(q2, axis=1)        # value of each state
        q1_mb = T @ max_q2                 # MB value for A,U

        # Hybrid value with perseveration bias
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        bias1 = np.zeros(2)
        bias1[prev_a1] += kappa_eff
        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with state-dependent perseveration
        bias2 = np.zeros(2)
        bias2[prev_a2[s2]] += kappa_eff
        logits2 = beta * q2[s2] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning: Stage 2 TD(0)
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # Eligibility trace from stage 2 to MF stage 1
        target1 = (1.0 - lam) * q1_mf[a1] + lam * q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update perseveration traces
        prev_a1 = a1
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'lam', 'kappa']"
iter0_run0_participant11.json,cognitive_model2,457.0584318268983,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Valence-asymmetric learning with anxiety-driven exploration bonus for surprising transitions.
    
    Stage-2 values are learned with separate learning rates for positive and negative PEs.
    Stage-1 uses a fixed MB/MF mixture with an additional exploration bonus that is
    triggered by rare transitions; the bonus is scaled by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed stage-1 choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Observed stage-2 choices (alien index).
    reward : array-like of float
        Received reward on each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; we use stai[0].
    model_parameters : array-like of float
        Parameters with bounds:
        - alpha_pos in [0,1]: learning rate for positive PE at stage 2.
        - alpha_neg in [0,1]: learning rate for negative PE at stage 2.
        - beta in [0,10]: inverse temperature for both stages.
        - omega in [0,1]: weight on model-based value at stage 1 (1=fully MB).
        - xi in [0,1]: base coefficient for exploration bonus at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_pos, alpha_neg, beta, omega, xi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure
    # A commonly->X, U commonly->Y; rare otherwise
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Simple perseveration at stage 1 to stabilize choices (implicitly handled by MF value carry-over)
    prev_a1 = 0

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Identify whether observed transition was rare (surprise bonus)
        # Mapping: common if (a1==0 and s2==0) or (a1==1 and s2==1)
        is_rare = 1.0 if ((a1 == 0 and s2 == 1) or (a1 == 1 and s2 == 0)) else 0.0

        # Model-based values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Anxiety-driven exploration bonus for rare transitions
        # Larger anxiety => larger bonus weight on both actions after rare transition,
        # with direction favoring the action that would more likely reach the surprising state
        bonus = np.zeros(2)
        if is_rare > 0.5:
            # Reward the chosen action's alternative to encourage exploration after surprise
            # and inject curiosity proportional to anxiety
            bonus[1 - a1] += xi * stai_val

        # Hybrid value
        q1_hybrid = omega * q1_mb + (1.0 - omega) * q1_mf + bonus

        # Stage-1 choice
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 choice
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Stage-2 value update with valence asymmetry
        pe2 = r - q2[s2, a2]
        alpha = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s2, a2] += alpha * pe2

        # Propagate MF credit to stage 1 (simple SARSA-like backup)
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        # Use averaged learning rate to keep both alphas meaningful
        alpha1 = 0.5 * (alpha_pos + alpha_neg)
        q1_mf[a1] += alpha1 * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'omega', 'xi']"
iter0_run0_participant11.json,cognitive_model3,383.4368707723386,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning MB control with anxiety-weighted uncertainty aversion.
    
    This model learns the transition probabilities online and uses them to compute
    model-based values at stage 1. It blends MB and MF control with an anxiety-dependent
    arbitration (no extra parameter), and penalizes actions with high transition uncertainty
    proportionally to anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed stage-1 choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed stage-2 states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Observed stage-2 choices.
    reward : array-like of float
        Rewards received.
    stai : array-like of float
        Trait anxiety score in [0,1]; we use stai[0].
    model_parameters : array-like of float
        Parameters with bounds:
        - alpha_r in [0,1]: learning rate for stage-2 rewards (Q2).
        - alpha_t in [0,1]: learning rate for transition probabilities.
        - beta in [0,10]: inverse temperature for both stages.
        - rho in [0,1]: perseveration strength (converted to bias) at both stages.
        - eta in [0,1]: scaling of uncertainty aversion term at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_r, alpha_t, beta, rho, eta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix T_hat with weak prior toward common transitions
    T_hat = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration (previous choices)
    prev_a1 = 0
    prev_a2 = np.zeros(2, dtype=int)

    # Arbitration weight as a function of anxiety (higher anxiety -> more MF)
    w_mb = np.clip(1.0 - stai_val, 0.0, 1.0)

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute model-based values using learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2

        # Transition uncertainty penalty per action (entropy of each row of T_hat)
        # H(p) = -sum p log p (use small epsilon for stability)
        epsH = 1e-12
        ent = -(T_hat * (np.log(T_hat + epsH))).sum(axis=1)  # entropy of each action's transition
        unc_penalty = eta * stai_val * ent  # anxiety-weighted uncertainty aversion

        # Stage-1 values with arbitration, perseveration, and uncertainty penalty
        q1_comb = w_mb * q1_mb + (1.0 - w_mb) * q1_mf - unc_penalty
        bias1 = np.zeros(2)
        bias1[prev_a1] += rho
        logits1 = beta * q1_comb + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with perseveration
        bias2 = np.zeros(2)
        bias2[prev_a2[s2]] += rho
        logits2 = beta * q2[s2] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Outcome learning at stage 2
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_r * pe2

        # MF backup to stage 1 (simple copy of stage-2 value)
        pe1 = q2[s2, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

        # Learn transitions online: update only the row corresponding to chosen a1
        # Move T_hat[a1] toward the one-hot of observed s2
        oh = np.array([1.0 if s2 == 0 else 0.0, 1.0 if s2 == 1 else 0.0])
        T_hat[a1] = (1.0 - alpha_t) * T_hat[a1] + alpha_t * oh
        # Keep rows normalized and bounded
        T_hat[a1] = np.clip(T_hat[a1], 1e-6, 1.0)
        T_hat[a1] /= T_hat[a1].sum()

        # Update perseveration traces
        prev_a1 = a1
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'alpha_t', 'beta', 'rho', 'eta']"
iter0_run0_participant12.json,cognitive_model2,410.77910581177673,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with anxiety-modulated asymmetric learning rates and perseveration.
    
    This purely model-free controller learns second-stage values and backs them up to first-stage
    values via an eligibility trace. Anxiety (stai) creates asymmetry between positive and negative
    prediction-error learning rates and scales a perseveration (choice stickiness) bias at stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate learning-rate asymmetry and perseveration.
    model_parameters : list or array
        [alpha_base, beta, pi_base, stai_weight, lambd]
        Bounds:
          alpha_base in [0,1]  : base learning rate
          beta in [0,10]       : inverse temperature
          pi_base in [0,1]     : baseline perseveration strength
          stai_weight in [0,1] : sensitivity of parameters to STAI (0=no effect, 1=max effect)
          lambd in [0,1]       : eligibility trace from stage 2 to stage 1
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha_base, beta, pi_base, stai_weight, lambd = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Value functions
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory (stage 1)
    prev_a1 = None

    # Helper to compute anxiety-modulated alphas and pi
    # Map stai and stai_weight to a signed modulation in [-1,1]
    mod = (2.0 * stai - 1.0) * (2.0 * stai_weight - 1.0)
    # Learning-rate asymmetry: alpha_pos increases with mod, alpha_neg decreases with mod
    alpha_pos = np.clip(alpha_base * (1.0 + mod), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 - mod), 0.0, 1.0)
    # Perseveration scaling: stronger stickiness with positive mod, weaker with negative
    pi_eff = pi_base * (1.0 + mod)  # can vary approximately in [0, 2*pi_base]

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage-1 decision with perseveration bias
        q1 = q_stage1_mf.copy()
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            q1 = q1 + pi_eff * stick

        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 decision
        q2_s = q_stage2_mf[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Learning
        # Stage-2 TD error
        pe2 = reward[t] - q_stage2_mf[s, a2]
        if pe2 >= 0:
            a2_lr = alpha_pos
        else:
            a2_lr = alpha_neg
        q_stage2_mf[s, a2] += a2_lr * pe2

        # Back up to stage-1 MF via eligibility trace (use same sign-dependent lr)
        q_stage1_mf[a1] += a2_lr * lambd * pe2

        # Additionally, a direct stage-1 TD towards the active second-stage value (SARSA(0) style)
        td1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # Use base alpha here, modestly modulated by anxiety via stai_weight
        alpha1 = np.clip(alpha_base * (1.0 + 0.5 * mod), 0.0, 1.0)
        q_stage1_mf[a1] += alpha1 * td1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_base', 'beta', 'pi_base', 'stai_weight', 'lambd']"
iter0_run0_participant13.json,cognitive_model1,171.76442991861168,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free SARSA(Î») with anxiety-modulated arbitration and perseveration.
    
    This model learns second-stage Q-values (per planet/alien) and a model-free first-stage value.
    First-stage choices are driven by a convex combination of model-based and model-free values,
    where the arbitration weight is modulated by the participant's anxiety (STAI). A perseveration
    bias at the first stage is also scaled by anxiety.

    Parameters (model_parameters):
    - alpha in [0,1]: Learning rate for value updates at both stages.
    - beta in [0,10]: Inverse temperature for softmax choice at both stages.
    - lam in [0,1]: Eligibility trace strength propagating second-stage RPE to first stage.
    - w_mb in [0,1]: Baseline weight for model-based values at first stage (before anxiety modulation).
    - stickiness in [0,1]: Strength of first-stage perseveration bias (scaled by STAI).

    Inputs:
    - action_1: array of length n_trials with first-stage actions (0 = A, 1 = U).
    - state: array of length n_trials with observed second-stage state (0 = X, 1 = Y).
    - action_2: array of length n_trials with second-stage actions (0/1 for the two aliens on that planet).
    - reward: array of length n_trials with received reward (e.g., 0 or 1).
    - stai: array-like with a single float in [0,1], the participant's anxiety score.
    - model_parameters: array-like with the parameters [alpha, beta, lam, w_mb, stickiness].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices under the model.
    """"""
    alpha, beta, lam, w_mb, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A commonly -> X, U commonly -> Y
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage2 = np.zeros((2, 2))      # Q2[state, action2]
    q_stage1_mf = np.zeros(2)        # model-free Q at stage 1

    # Choice probabilities to accumulate likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration: transform w_mb via logistic shift by (0.5 - stai)
    eps = 1e-10
    w_mb = np.clip(w_mb, eps, 1 - eps)
    logit = np.log(w_mb) - np.log(1 - w_mb)
    w_eff = 1 / (1 + np.exp(-(logit + 2.0 * (0.5 - stai))))  # increases when stai < 0.5, decreases when stai > 0.5
    w_eff = float(np.clip(w_eff, 0.0, 1.0))

    prev_a1 = -1  # for stickiness

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # First-stage action values: MB from transition and current Q2; MF from q_stage1_mf
        max_q2 = np.max(q_stage2, axis=1)                  # best alien per planet
        q1_mb = transition_matrix @ max_q2                 # expected value per spaceship
        q1 = w_eff * q1_mb + (1 - w_eff) * q_stage1_mf     # arbitration

        # Add anxiety-scaled perseveration bias
        if prev_a1 >= 0:
            kappa_eff = stickiness * (0.5 + stai)          # stronger bias with higher anxiety
            bias = np.array([0.0, 0.0])
            bias[prev_a1] += kappa_eff
            q1 = q1 + bias

        # First-stage policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Learning
        # Second-stage update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # First-stage MF update with eligibility trace
        td_to_stage1 = (q_stage2[s, a2] - q_stage1_mf[a1])  # bootstrapping from stage 2
        q_stage1_mf[a1] += alpha * td_to_stage1 + alpha * lam * delta2

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'lam', 'w_mb', 'stickiness']"
iter0_run0_participant14.json,cognitive_model1,542.6041534992905,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated arbitration.
    
    This model blends model-based (MB) and model-free (MF) values at stage 1.
    The arbitration weight toward MB control is modulated by participant anxiety
    (stai). Higher anxiety increases MB control (linear, clamped).
    Stage 2 uses standard MF Q-learning. Stage 1 MF is updated via backup from
    stage-2 value (TD(1)-style).
    
    Parameters (model_parameters):
    - alpha: learning rate for Q updates, in [0,1]
    - beta1: inverse temperature for stage 1 softmax, in [0,10]
    - beta2: inverse temperature for stage 2 softmax, in [0,10]
    - w_base: baseline MB weight (before anxiety modulation), in [0,1]
    - k_anx: anxiety modulation strength of MB weight, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: spaceship A, 1: spaceship U)
    - state: array of reached planet indices (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1; within-planet aliens)
    - reward: array of rewards (coins) obtained on each trial (float)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta1, beta2, w_base, k_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: A->X common (0), U->Y common (1)
    transition_matrix = np.array([[0.7, 0.3],   # from A to [X, Y]
                                  [0.3, 0.7]])  # from U to [X, Y]

    # Initialize choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize Q-values
    q1_mf = np.zeros(2)           # model-free values for spaceships [A, U]
    q2 = np.zeros((2, 2))         # second-stage values per planet and alien

    # Anxiety-modulated arbitration weight (toward MB)
    # Center around medium/high boundary (0.51) and clamp to [0,1]
    w_mb = np.clip(w_base + k_anx * (stai - 0.51), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based stage-1 values via one-step lookahead using fixed transitions
        max_q2 = np.max(q2, axis=1)                 # best alien per planet
        q1_mb = transition_matrix @ max_q2          # expected value per spaceship

        # Hybrid value
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta1 * q1_hybrid
        logits1 -= np.max(logits1)  # numerical stability
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (within reached planet)
        s = state[t]
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 TD update
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update via backup from stage-2 chosen value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta1', 'beta2', 'w_base', 'k_anx']"
iter0_run0_participant14.json,cognitive_model2,453.17402915804837,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned-transition model-based RL with anxiety-modulated perseveration.
    
    This model learns the transition matrix from experience and uses a purely
    model-based (MB) first-stage policy computed from the learned transition and
    second-stage Q-values. Stage 2 uses MF Q-learning.
    
    Anxiety (stai) amplifies a perseveration bias to repeat the previous
    spaceship choice, capturing heightened habit-like inertia under higher
    anxiety while still planning via MB values.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2 updates, in [0,1]
    - alpha_t: transition learning rate, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - stick: base perseveration strength added to stage-1 logits, in [0,1]
    - gamma_anx: anxiety gain on perseveration, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_r, alpha_t, beta, stick, gamma_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition matrix close to uniform (uninformative)
    T = np.ones((2, 2)) * 0.5  # rows: spaceships [A,U], cols: planets [X,Y]
    q2 = np.zeros((2, 2))      # second-stage values per planet-alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    # Anxiety-modulated perseveration
    stick_eff = stick * np.clip(1.0 + gamma_anx * (stai - 0.31), 0.0, 2.0)

    for t in range(n_trials):
        # Compute MB values from learned transitions
        max_q2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        # Add perseveration bias to stage-1 logits
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within reached planet
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Reward learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Transition learning for chosen spaceship a1 toward observed state s
        # One-hot target: 1 for observed state, 0 for the other
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += alpha_t * (target - T[a1])

        # Ensure rows remain normalized and within [0,1] (numerical safety)
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'alpha_t', 'beta', 'stick', 'gamma_anx']"
iter0_run0_participant14.json,cognitive_model3,469.32686669875613,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated uncertainty bonus for directed exploration (MB at stage 1).
    
    Stage 2 is MF Q-learning. An uncertainty bonus (based on inverse visit counts)
    is added to stage-2 action values, encouraging exploration of less-sampled
    aliens. The bonus propagates to stage 1 via a model-based lookahead that uses
    the fixed transition structure. Anxiety reduces the exploration bonus
    (higher stai -> lower directed exploration). A mild perseveration bias is
    also included at stage 1.
    
    Parameters (model_parameters):
    - alpha: learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - bonus_base: baseline uncertainty bonus weight, in [0,1]
    - anx_gain: strength of anxiety suppression of exploration, in [0,1]
    - zeta: perseveration strength (stage 1 logits), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, bonus_base, anx_gain, zeta = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure
    T_fixed = np.array([[0.7, 0.3],   # A -> [X, Y]
                        [0.3, 0.7]])  # U -> [X, Y]

    # Q-values and visit counts for uncertainty
    q2 = np.zeros((2, 2))
    counts = np.zeros((2, 2))  # visit counts per planet-alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    # Anxiety-modulated exploration bonus weight
    # Higher stai reduces bonus linearly, clamped to [0, 1]
    bonus_w = np.clip(bonus_base * (1.0 - anx_gain * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Compute uncertainty as inverse sqrt of visit counts
        # u = 1/sqrt(n+1) in [~0,1], higher when less sampled
        uncertainty = 1.0 / np.sqrt(counts + 1.0)

        # Stage-2 augmented values with uncertainty bonus
        q2_aug = q2 + bonus_w * uncertainty

        # Model-based lookahead for stage 1 using augmented values
        max_q2_aug = np.max(q2_aug, axis=1)   # best augmented alien per planet
        q1_mb = T_fixed @ max_q2_aug

        # Add perseveration to stage-1 logits
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = zeta

        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within reached planet using augmented values
        s = state[t]
        logits2 = beta * q2_aug[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2
        counts[s, a2] += 1.0

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'bonus_base', 'anx_gain', 'zeta']"
iter0_run0_participant15.json,cognitive_model1,565.9408037780487,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    Parameters (all used; total=5):
    - alpha: [0,1] Stage-2 learning rate for rewards.
    - beta: [0,10] Inverse temperature for softmax at both stages.
    - lamb: [0,1] Eligibility trace controlling credit assignment to Stage-1.
    - w0: [0,1] Baseline model-based weight.
    - anx_influence: [0,1] How strongly STAI shifts arbitration toward model-based control.
    
    Anxiety use:
    - The effective model-based weight is w = (1 - anx_influence)*w0 + anx_influence*stai,
      so higher anxiety increases MB control when anx_influence > 0.
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    alpha, beta, lamb, w0, anx_influence = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition: rows are first-stage actions (0=A, 1=U); cols are states (0=X, 1=Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)            # model-free Stage-1 values
    q2 = 0.5 * np.ones((2, 2))     # Stage-2 values initialized neutral

    # Anxiety-modulated arbitration weight (kept constant over trials here)
    w = (1.0 - anx_influence) * w0 + anx_influence * stai
    w = min(1.0, max(0.0, w))

    for t in range(n_trials):
        s = state[t]

        # Model-based Stage-1 values from transition structure and current Stage-2 values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Hybrid Stage-1 values
        q1_hybrid = w * mb_q1 + (1.0 - w) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1_hybrid - np.max(q1_hybrid))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        # TD errors
        delta2 = reward[t] - q2[s, a2]
        delta1 = q2[s, a2] - q1_mf[a1]

        # Update Stage-2 values
        q2[s, a2] += alpha * delta2

        # Update Stage-1 MF with direct TD and eligibility-trace propagation from reward
        q1_mf[a1] += alpha * delta1 + alpha * lamb * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lamb', 'w0', 'anx_influence']"
iter0_run0_participant15.json,cognitive_model2,563.3136803722679,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Attention-weighted learning and anxiety-sensitive arbitration under rare transitions.
    Parameters (all used; total=5):
    - alpha0: [0,1] Base learning rate.
    - beta: [0,10] Inverse temperature for both stages.
    - w_mb: [0,1] Baseline model-based weight.
    - phi: [0,1] Scaling of STAI into learning-rate gain.
    - rho: [0,1] Anxiety-sensitivity to rare transitions (down-weights MB after rare transitions).
    
    Anxiety use:
    - Trial-wise learning rate alpha_t = alpha0 * ((1 - phi) + phi*stai) * g(|delta2|),
      where g(x)=clip(0.5 + 0.5*x, 0, 1) increases learning with surprise.
    - MB weight is suppressed on trials with rare transitions by factor (1 - rho*stai).
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    alpha0, beta, w_mb, phi, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Precompute common destination for each action (argmax over transition probs)
    common_dest = np.argmax(transition_matrix, axis=1)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage-2 policy (use current q2)
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Compute TD at stage 2 to set trial-wise attention/alpha before stage-1 update
        delta2 = reward[t] - q2[s, a2]
        # Surprise-driven gain in [0,1]
        gain = 0.5 + 0.5 * abs(delta2)
        gain = min(1.0, max(0.0, gain))
        # Anxiety-inflated base learning rate
        alpha_t = alpha0 * ((1.0 - phi) + phi * stai)
        alpha_t = max(0.0, min(1.0, alpha_t))
        # Final adaptive learning rate
        alpha_eff = max(0.0, min(1.0, alpha_t * gain))

        # Update stage-2 values
        q2[s, a2] += alpha_eff * delta2

        # Stage-1 model-based and model-free values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_eff * delta1

        # Determine if transition was rare given chosen action
        was_common = (s == common_dest[a1])
        rare = 0 if was_common else 1

        # Anxiety-sensitive arbitration: down-weight MB after rare transitions
        w_t = w_mb * (1.0 - rho * stai * rare)
        w_t = min(1.0, max(0.0, w_t))
        q1_hybrid = w_t * mb_q1 + (1.0 - w_t) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1_hybrid - np.max(q1_hybrid))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'w_mb', 'phi', 'rho']"
iter0_run0_participant15.json,cognitive_model3,558.8496348133491,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Valence-asymmetric learning with anxiety-skew, MB planning at stage-1,
    and anxiety-dependent stickiness and forgetting.
    Parameters (all used; total=5):
    - alpha: [0,1] Base learning rate.
    - beta: [0,10] Inverse temperature for both stages.
    - z: [0,1] Degree of anxiety-skewed asymmetry in learning from positive vs negative outcomes.
    - pi: [0,1] Stickiness strength on Stage-1, reduced by higher anxiety.
    - f: [0,1] Forgetting rate toward 0.5 at Stage-2, amplified by anxiety.
    
    Anxiety use:
    - Learning rates: alpha_plus = alpha * [(1 - z) + z * stai], alpha_minus = alpha * [(1 - z) + z * (1 - stai)].
    - Stickiness bias on previous Stage-1 choice is pi*(1 - stai).
    - Effective forgetting at Stage-2 is f_eff = f * stai.
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    alpha, beta, z, pi, f = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values
    q2 = 0.5 * np.ones((2, 2))

    # Previous Stage-1 choice for stickiness (None -> no bias first trial)
    prev_a1 = None

    # Precompute anxiety-modulated quantities that are constant across trials
    stickiness_weight = pi * (1.0 - stai)
    f_eff = f * stai  # forgetting strength increases with anxiety
    f_eff = min(1.0, max(0.0, f_eff))

    for t in range(n_trials):
        s = state[t]

        # Model-based Stage-1 values from current q2
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Add stickiness bias to chosen action from previous trial
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stickiness_weight

        # Stage-1 policy
        z1 = beta * (mb_q1 + bias - np.max(mb_q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy for observed state
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-2 learning with valence asymmetry and anxiety-dependent forgetting
        r = reward[t]
        pe = r - q2[s, a2]
        # Select learning rate by valence, skewed by anxiety through z
        alpha_plus = alpha * ((1.0 - z) + z * stai)
        alpha_minus = alpha * ((1.0 - z) + z * (1.0 - stai))
        alpha_eff = alpha_plus if pe >= 0.0 else alpha_minus
        alpha_eff = min(1.0, max(0.0, alpha_eff))

        # Apply forgetting toward 0.5 baseline before update
        q2[s, :] = (1.0 - f_eff) * q2[s, :] + f_eff * 0.5

        # Update chosen action value
        q2[s, a2] += alpha_eff * pe

        # Track previous Stage-1 choice for next-trial stickiness
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'z', 'pi', 'f']"
iter0_run0_participant16.json,cognitive_model3,478.637126179872,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive two-step planner. Anxiety increases risk aversion and reduces
    exploitation at stage 2; stage 1 plans over risk-adjusted state values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Received rewards.
    stai : array-like with single float in [0,1]
        Anxiety score. Higher values imply higher risk aversion and lower stage-2 determinism.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for both mean and variance estimates of rewards.
        - beta1 in [0,10]: softmax inverse temperature for first-stage choices.
        - beta2_base in [0,10]: baseline stage-2 inverse temperature.
        - rho_base in [0,1]: baseline risk-aversion weight.
        - kappa in [0,1]: perseveration (stickiness) at first stage.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta1, beta2_base, rho_base, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated parameters
    # Increase risk penalty and reduce stage-2 determinism with anxiety
    gamma = np.clip(rho_base + 0.8 * stai * (1.0 - rho_base), 0.0, 1.0)  # risk penalty weight
    beta2 = max(0.0, beta2_base * (1.0 - 0.5 * stai))

    # Stage-2 estimates: mean and variance of reward per state-action
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.25  # initial uncertainty (Bernoulli variance around 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Risk-adjusted state values for planning: max over risk-adjusted values in each state
        risk_adj = q2_mean - gamma * np.sqrt(np.maximum(q2_var, 1e-8))
        max_risk_adj = np.max(risk_adj, axis=1)  # per state

        q1_mb = T @ max_risk_adj

        # Add first-stage stickiness to logits
        logits1 = beta1 * q1_mb
        if prev_a1 is not None:
            for a in range(2):
                if a == prev_a1:
                    logits1[a] += kappa

        # First-stage softmax
        maxl = np.max(logits1)
        exp_q1 = np.exp(logits1 - maxl)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage softmax uses risk-adjusted values at reached state
        s = state[t]
        logits2 = beta2 * risk_adj[s]
        maxl2 = np.max(logits2)
        exp_q2 = np.exp(logits2 - maxl2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]

        # Update mean with TD error
        delta = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha * delta

        # Update variance as exponential moving average of squared prediction error
        q2_var[s, a2] = (1.0 - alpha) * q2_var[s, a2] + alpha * (delta ** 2)

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta1', 'beta2_base', 'rho_base', 'kappa']"
iter0_run0_participant17.json,cognitive_model1,520.6523988260145,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    The model combines model-based (MB) and model-free (MF) values at stage 1.
    Anxiety down-weights MB control: higher STAI reduces the MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically 0 or 1.
    stai : array-like of float in [0,1]
        Anxiety score (single value array), used to modulate arbitration.
    model_parameters : tuple/list
        Parameters with bounds:
        - alpha: learning rate in [0,1]
        - beta: inverse temperature in [0,10]
        - lam: eligibility trace in [0,1]
        - w0: baseline MB weight in [0,1]
        - anx: anxiety influence on arbitration in [0,1]
          Effective MB weight is w = clip(w0 + anx*(0.5 - stai), 0, 1)
          so higher stai reduces w when anx>0.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, lam, w0, anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)          # model-free values at stage 1
    q2 = np.zeros((2, 2))        # model-free values at stage 2 (per state, per action)

    # Anxiety-modulated arbitration weight (fixed across trials)
    w = w0 + anx * (0.5 - stai)
    w = 0.0 if w < 0.0 else (1.0 if w > 1.0 else w)

    for t in range(n_trials):
        # Stage-1 policy: hybrid MB/MF softmax
        max_q2 = np.max(q2, axis=1)             # values of states X,Y under greedy 2nd-stage
        q1_mb = transition_matrix @ max_q2      # MB action values
        q1 = (1 - w) * q1_mf + w * q1_mb

        # Softmax with stability
        z1 = q1 - np.max(q1)
        exp1 = np.exp(beta * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: purely MF softmax over actions in observed state
        s = state[t]
        q2_s = q2[s]
        z2 = q2_s - np.max(q2_s)
        exp2 = np.exp(beta * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 TD error and update
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF TD error and update with eligibility trace
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1
        q1_mf[a1] += lam * alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lam', 'w0', 'anx']"
iter0_run0_participant17.json,cognitive_model2,482.92309832044623,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric (valence-dependent) learning with anxiety-amplified negative learning,
    hybrid MB/MF stage-1, and anxiety-scaled perseveration bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically 0 or 1.
    stai : array-like of float in [0,1]
        Anxiety score (single value array).
        Used to amplify negative-learning and to scale perseveration.
    model_parameters : tuple/list
        Parameters with bounds:
        - alpha: base learning rate in [0,1]
        - beta: inverse temperature in [0,10]
        - k_neg: anxiety coupling for negative learning in [0,1]
                 alpha_neg = clip(alpha * (1 + k_neg * stai), 0, 1)
                 alpha_pos = alpha
        - persev: base perseveration strength in [0,1]
                  effective stickiness = persev * stai
        - wmb: MB weight at stage 1 in [0,1]
               stage-1 Q = (1 - wmb)*MF + wmb*MB

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, k_neg, persev, wmb = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Effective parameters modulated by anxiety
    alpha_pos = alpha
    alpha_neg = alpha * (1 + k_neg * stai)
    alpha_neg = 1.0 if alpha_neg > 1.0 else (0.0 if alpha_neg < 0.0 else alpha_neg)

    stick = persev * stai  # stronger perseveration under higher anxiety

    prev_a1 = None
    prev_a2 = None
    prev_s = None

    for t in range(n_trials):
        # MB component for stage-1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid Q at stage-1
        q1 = (1 - wmb) * q1_mf + wmb * q1_mb

        # Add perseveration bias at stage-1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        # Stage-1 policy
        z1 = (q1 + bias1) - np.max(q1 + bias1)
        exp1 = np.exp(beta * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration (state-dependent)
        s = state[t]
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2 is not None and prev_s == s:
            bias2[prev_a2] += stick

        z2 = (q2_s + bias2) - np.max(q2_s + bias2)
        exp2 = np.exp(beta * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        delta2 = r - q2[s, a2]
        # Valence-dependent learning at stage-2
        a2_lr = alpha_pos if delta2 >= 0 else alpha_neg
        q2[s, a2] += a2_lr * delta2

        # Stage-1 MF update (bootstrapped from updated q2 and valence-sensitive)
        delta1 = q2[s, a2] - q1_mf[a1]
        a1_lr = alpha_pos if delta1 >= 0 else alpha_neg
        q1_mf[a1] += a1_lr * delta1

        # Update perseveration trackers
        prev_a1 = a1
        prev_a2 = a2
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'k_neg', 'persev', 'wmb']"
iter0_run0_participant17.json,cognitive_model3,520.714906404484,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-sensitive (Pearce-Hall-like) learning with anxiety-scaled gain,
    hybrid MB/MF stage-1, and eligibility trace coupling.
    
    Learning rate adapts from recent surprise: alpha_t = clip((alpha0 + k*|delta_prev|) * g(stai), 0, 1)
    where g(stai) = 0.5 + 0.5*stai increases learning under higher anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically 0 or 1.
    stai : array-like of float in [0,1]
        Anxiety score (single value array) scaling the dynamic learning rate.
    model_parameters : tuple/list
        Parameters with bounds:
        - alpha0: baseline learning rate in [0,1]
        - k: surprise-to-learning coupling in [0,1]
        - beta: inverse temperature in [0,10]
        - wmb: MB weight at stage 1 in [0,1]
        - lam: eligibility trace coupling in [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha0, k, beta, wmb, lam = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Anxiety scaling of learning rate
    g = 0.5 + 0.5 * stai  # in [0.5, 1.0]

    # Initialize previous absolute TD error for dynamic learning
    abs_delta_prev = 0.0

    for t in range(n_trials):
        # Compute dynamic learning rate for this trial
        alpha_t = alpha0 + k * abs_delta_prev
        alpha_t *= g
        alpha_t = 1.0 if alpha_t > 1.0 else (0.0 if alpha_t < 0.0 else alpha_t)

        # Stage-1 hybrid policy
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2
        q1 = (1 - wmb) * q1_mf + wmb * q1_mb

        z1 = q1 - np.max(q1)
        exp1 = np.exp(beta * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        q2_s = q2[s]
        z2 = q2_s - np.max(q2_s)
        exp2 = np.exp(beta * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates with dynamic learning rate
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_t * delta1
        q1_mf[a1] += lam * alpha_t * delta2

        # Update surprise for next trial
        abs_delta_prev = abs(delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha0', 'k', 'beta', 'wmb', 'lam']"
iter0_run0_participant18.json,cognitive_model1,414.05029093008466,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-biased arbitration and stickiness.
    
    Description:
    - Stage 1 choice values are a hybrid of model-based (MB) and model-free (MF) values.
    - The MB weight is interpolated by anxiety: w_mb = (1 - stai) * w_low + stai * w_high,
      so higher anxiety increases the contribution of w_high and lower anxiety favors w_low.
    - A single stickiness parameter biases repeating previous stage-1 and stage-2 actions.
      Its effective strength is scaled by (1 - stai), so lower anxiety yields more stickiness influence.
    - Stage 2 is MF with a softmax policy.
    - MF updates use simple two-step TD with an eligibility trace on stage-1 via lambda fixed to 0.5
      (embedded by propagating the stage-2 value difference once).
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - w_low: MB weight used when stai is low (for stai -> 0) in [0,1]
    - w_high: MB weight used when stai is high (for stai -> 1) in [0,1]
    - stickiness: perseveration strength in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
      (0=W on X or P on Y; 1=S on X or H on Y)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w_low, w_high, stickiness)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, w_low, w_high, stickiness = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition structure (fixed, known)
    # Rows: action (0=A, 1=U), Cols: state (0=X, 1=Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q1_mf = np.zeros(2)            # MF values for stage-1 actions
    Q2 = np.zeros((2, 2))          # MF values for stage-2: state x action

    # Anxiety-modulated arbitration
    w_mb = (1.0 - st) * w_low + st * w_high
    w_mb = min(1.0, max(0.0, w_mb))

    # Stickiness effective strength (more effect for low anxiety)
    stick_eff = stickiness * (1.0 - st)

    # Track previous actions for stickiness
    prev_a1 = None
    prev_a2 = [None, None]  # per state (0=X, 1=Y)

    for t in range(n_trials):
        s = int(state[t])

        # MB estimate at stage 1 from current Q2
        max_Q2 = np.max(Q2, axis=1)     # best action value at each state
        Q1_mb = T @ max_Q2              # expected value under transitions

        # Hybrid stage-1 value with stickiness bias
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Add stickiness bias to stage-1 preferences
        pref1 = Q1_hyb.copy()
        if prev_a1 is not None:
            pref1[prev_a1] += stick_eff

        # Softmax for stage-1
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[int(action_1[t])]

        # Stage-2 policy with stickiness
        pref2 = Q2[s].copy()
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += stick_eff

        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[int(action_2[t])]

        # Observe reward and update values (MF/SARSA-style)
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 TD towards current stage-2 value (eligibility = 0.5)
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * 0.5 * delta1

        # Stage-2 TD to reward
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Optional second eligibility step to propagate reward to stage-1
        Q1_mf[a1] += alpha * 0.5 * delta2

        # Update stickiness trackers
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_low', 'w_high', 'stickiness']"
iter0_run0_participant18.json,cognitive_model2,502.36338991359355,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated temperature and learned transition model (MB-MF hybrid with lambda).
    
    Description:
    - Learns the transition function T from experienced transitions with learning rate eta.
      This captures potential uncertainty or idiosyncratic beliefs.
    - Hybrid MB/MF valuation at stage-1 with fixed mixing 0.5 (implicit through MF+MB sum via softmax).
      We keep MF Q1 separately through eligibility.
    - Inverse temperature is amplified for lower anxiety:
        beta_eff = beta * (1 + k_beta * (1 - stai))
      So lower anxiety sharpens choices; higher anxiety softens them.
    - Eligibility trace lambda is also anxiety-scaled to reflect reduced credit assignment under anxiety:
        lambda_eff = lambda_ * (1 - stai)
    - Stage-2 is MF with softmax.
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: base inverse temperature in [0,10]
    - lambda_: eligibility trace in [0,1]
    - eta: transition learning rate in [0,1]
    - k_beta: temperature gain factor in [0,1] scaling anxiety effect
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship
    - state: array-like of ints in {0,1}; observed planet
    - action_2: array-like of ints in {0,1}; chosen alien at second stage
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, lambda_, eta, k_beta)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, lambda_, eta, k_beta = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize transition model to uninformative (0.5/0.5)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    beta_eff = beta * (1.0 + k_beta * (1.0 - st))
    lambda_eff = lambda_ * (1.0 - st)

    for t in range(n_trials):
        s = int(state[t])

        # MB value via current learned transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Combine MB and MF at stage 1 by summing their normalized contributions
        # Here we use a softmax directly over (Q1_mb + Q1_mf) with equal implicit weighting.
        Q1_comb = 0.5 * Q1_mb + 0.5 * Q1_mf

        # Stage-1 softmax
        exp1 = np.exp(beta_eff * (Q1_comb - np.max(Q1_comb)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[int(action_1[t])]

        # Stage-2 softmax
        pref2 = Q2[s].copy()
        exp2 = np.exp(beta_eff * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[int(action_2[t])]

        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Update transition model T toward observed state for chosen action
        # Move probability mass toward the observed state using eta
        current = T[a1, s]
        T[a1, s] = current + eta * (1.0 - current)
        T[a1, 1 - s] = 1.0 - T[a1, s]

        # MF updates with eligibility
        # Stage-1 towards stage-2 value
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * lambda_eff * delta1

        # Stage-2 towards reward
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Propagate reward further to stage-1 depending on lambda_eff
        Q1_mf[a1] += alpha * lambda_eff * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lambda_', 'eta', 'k_beta']"
iter0_run0_participant19.json,cognitive_model1,354.71718587588896,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and perseveration.
    Parameters:
    - model_parameters = [alpha, beta, w0, kappa, rho]
      alpha: learning rate for value updates (stage 1 and 2), in [0,1]
      beta: inverse temperature for softmax choice, in [0,10]
      w0: baseline weight for model-based control, in [0,1]
      kappa: strength of anxiety modulation of arbitration, in [0,1] (effective weight shifts with stai)
      rho: perseveration strength at stage 1, in [0,1]
    Other inputs:
      action_1: array-like of first-stage choices (0 or 1)
      state: array-like of second-stage states (0 or 1)
      action_2: array-like of second-stage choices (0 or 1)
      reward: array-like of rewards (float)
      stai: array-like with single value in [0,1] indicating anxiety score
    Returns:
      Negative log-likelihood of observed first- and second-stage choices.
    Model details:
      - First-stage action values are a convex combination of model-based and model-free values.
      - Arbitration weight w is modulated by stai: w = clip(w0 + kappa*(stai - 0.5), 0, 1).
      - Perseveration bias added to previously chosen first-stage action scaled by rho and stai.
      - Eligibility-trace-like credit from stage 2 to stage 1 depends on stai: lambda_et = clip(0.5 + 0.5*stai, 0, 1).
      - Transition structure is known (A->X and U->Y common with prob .7).
    """"""
    alpha, beta, w0, kappa, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition matrix: rows are first-stage actions (A=0, U=1), cols are states (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    Q1_mf = np.zeros(2)       # model-free first-stage values
    Q2 = np.zeros((2, 2))     # second-stage state-action values

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration tracker
    last_a1 = None

    # Anxiety-modulated arbitration and eligibility
    w = np.clip(w0 + kappa * (stai - 0.5), 0.0, 1.0)
    lambda_et = np.clip(0.5 + 0.5 * stai, 0.0, 1.0)
    rho_eff = rho * (0.5 + stai)  # stronger perseveration with higher stai

    for t in range(n_trials):
        # Model-based first-stage values: back up from second-stage max Q
        max_Q2 = np.max(Q2, axis=1)              # shape (2,)
        Q1_mb = T @ max_Q2                       # shape (2,)

        # Combine MF and MB with perseveration bias
        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] += rho_eff
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf + bias

        # First-stage policy
        q1 = Q1 - np.max(Q1)
        exp_q1 = np.exp(beta * q1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = int(state[t])
        q2 = Q2[s, :] - np.max(Q2[s, :])
        exp_q2 = np.exp(beta * q2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # TD errors and updates
        # Stage 2 update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage 1 MF update with eligibility from stage 2
        # Add both the bootstrapped difference and a portion of stage-2 prediction error
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1 + alpha * lambda_et * delta2

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'kappa', 'rho']"
iter0_run0_participant19.json,cognitive_model2,356.21193846515405,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive learning with anxiety-modulated volatility and learned transitions.
    Parameters:
    - model_parameters = [alpha0, beta, vsi, alpha_T, pi]
      alpha0: base learning rate for values, in [0,1]
      beta: inverse temperature for softmax choices, in [0,10]
      vsi: volatility sensitivity index (scales |RPE| into learning-rate boosts), in [0,1]
      alpha_T: learning rate for transition model updates, in [0,1]
      pi: first-stage perseveration strength, in [0,1]
    Other inputs:
      action_1: array-like of first-stage choices (0 or 1)
      state: array-like of second-stage states (0 or 1)
      action_2: array-like of second-stage choices (0 or 1)
      reward: array-like of rewards (float)
      stai: array-like with single value in [0,1] indicating anxiety score
    Returns:
      Negative log-likelihood of observed first- and second-stage choices.
    Model details:
      - Learns the transition matrix T_hat online; higher stai -> faster transition learning.
      - Learning rate for values increases with absolute RPE scaled by stai and vsi.
      - First-stage policy uses an equal-weight blend of MB and MF values plus perseveration bias scaling with stai.
      - Second-stage policy is softmax over Q2.
    """"""
    alpha0, beta, vsi, alpha_T, pi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix to neutral (0.5/0.5) for each action
    T_hat = np.full((2, 2), 0.5)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    pi_eff = pi * (0.5 + stai)  # stronger perseveration with anxiety
    lambda_et = 0.5  # fixed trace; keep parameters <=5

    for t in range(n_trials):
        # Model-based using learned transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_hat @ max_Q2

        # Blend MB and MF
        Q1_base = 0.5 * Q1_mb + 0.5 * Q1_mf

        # Perseveration bias
        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] += pi_eff
        Q1 = Q1_base + bias

        # First-stage softmax
        q1 = Q1 - np.max(Q1)
        exp_q1 = np.exp(beta * q1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage softmax
        s = int(state[t])
        q2 = Q2[s, :] - np.max(Q2[s, :])
        exp_q2 = np.exp(beta * q2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # Stage-2 update with adaptive learning-rate
        delta2 = r - Q2[s, a2]
        alpha_t = np.clip(alpha0 + vsi * stai * abs(delta2), 0.0, 1.0)
        Q2[s, a2] += alpha_t * delta2

        # Stage-1 MF update with partial eligibility
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha_t * td1 + alpha_t * lambda_et * delta2

        # Update learned transition model for the taken action
        # Anxiety speeds up transition learning
        aT_eff = np.clip(alpha_T * (0.5 + stai), 0.0, 1.0)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T_hat[a1, :] = (1.0 - aT_eff) * T_hat[a1, :] + aT_eff * target

        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'vsi', 'alpha_T', 'pi']"
iter0_run0_participant19.json,cognitive_model3,436.47994624545004,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric valence learning with anxiety-modulated MB weight and exploration.
    Parameters:
    - model_parameters = [alpha_pos, alpha_neg, beta, mb_weight, gamma]
      alpha_pos: learning rate when RPE is positive, in [0,1]
      alpha_neg: learning rate when RPE is negative, in [0,1]
      beta: inverse temperature baseline, in [0,10]
      mb_weight: baseline weight for model-based control, in [0,1]
      gamma: anxiety sensitivity scaling MB weight and exploration, in [0,1]
    Other inputs:
      action_1: array-like of first-stage choices (0 or 1)
      state: array-like of second-stage states (0 or 1)
      action_2: array-like of second-stage choices (0 or 1)
      reward: array-like of rewards (float)
      stai: array-like with single value in [0,1] indicating anxiety score
    Returns:
      Negative log-likelihood of observed first- and second-stage choices.
    Model details:
      - Uses fixed known transitions (common=0.7).
      - MB/MF hybrid at stage 1; MB weight increases with stai if gamma>0.
      - Exploration increases with anxiety: beta_eff = beta * (1 - 0.5*gamma*stai).
      - Asymmetric learning rates by valence; negative updates amplified by stai.
      - Rare transition credit assignment: when a rare transition occurs, MF value of the unchosen
        first-stage action is nudged toward the alternative MB value, scaled by w and stai.
    """"""
    alpha_pos, alpha_neg, beta, mb_weight, gamma = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration and exploration
    w = np.clip(mb_weight + gamma * (stai - 0.5), 0.0, 1.0)
    beta_eff = max(1e-6, beta * (1.0 - 0.5 * gamma * stai))

    for t in range(n_trials):
        # MB first-stage values
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid first-stage values
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # First-stage policy
        q1 = Q1 - np.max(Q1)
        exp_q1 = np.exp(beta_eff * q1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = int(state[t])
        q2 = Q2[s, :] - np.max(Q2[s, :])
        exp_q2 = np.exp(beta_eff * q2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # Stage-2 update with asymmetric valence and anxiety scaling
        delta2 = r - Q2[s, a2]
        if delta2 >= 0:
            a_eff = np.clip(alpha_pos * (1.0 - 0.5 * stai), 0.0, 1.0)
        else:
            a_eff = np.clip(alpha_neg * (1.0 + stai), 0.0, 1.0)
        Q2[s, a2] += a_eff * delta2

        # Stage-1 MF update towards backed-up value
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += a_eff * td1

        # Rare transition credit assignment to the unchosen first-stage action
        # Common if s == a1 given the task structure; otherwise rare
        is_rare = (s != a1)
        if is_rare:
            alt = 1 - a1
            # Nudge unchosen MF toward the MB value of the alternative action
            target_alt = Q1_mb[alt]
            Q1_mf[alt] += np.clip(w * gamma * stai, 0.0, 1.0) * (target_alt - Q1_mf[alt])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_pos', 'alpha_neg', 'beta', 'mb_weight', 'gamma']"
iter0_run0_participant2.json,cognitive_model1,496.27200429091135,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated arbitration and eligibility traces.
    
    This model combines model-free Q-values with model-based estimates from the known transition
    structure. The arbitration weight favoring model-based control is modulated by STAI (anxiety)
    such that lower anxiety increases model-based weighting. Stage-1 Q-values are updated with an
    eligibility trace from the stage-2 prediction error.
    
    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like, shape (n_trials,)
        Second-stage states (0 = planet X, 1 = planet Y) actually reached after action_1.
    action_2 : array-like, shape (n_trials,)
        Second-stage choices (0/1; X: W/S, Y: P/H).
    reward : array-like, shape (n_trials,)
        Obtained reward (e.g., coins; typically 0/1).
    stai : array-like, shape (1,) or scalar-like
        Participant STAI score in [0,1]. Lower values indicate lower anxiety.
    model_parameters : iterable
        Model parameters (total <= 5):
        - alpha in [0,1]: learning rate for value updates
        - beta1 in [0,10]: inverse temperature at stage 1
        - beta2 in [0,10]: inverse temperature at stage 2
        - lam in [0,1]: eligibility trace strength from stage 2 PE to stage 1 MF value
        - w0 in [0,1]: baseline arbitration weight that is modulated by STAI

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """"""
    alpha, beta1, beta2, lam, w0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(np.asarray(stai)[0])

    # Known transition structure (rows: A,U; cols: X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2)           # model-free stage-1 action values (A,U)
    q2 = np.zeros((2, 2))         # stage-2 state-action values: rows X,Y; cols actions 0/1

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration: lower anxiety -> more MB control.
    # Use a squashed combination of baseline w0 and STAI. Fixed slope ensures STAI is used.
    # w in [0,1]
    # Center STAI around 0.5 and add to a logit formed from w0 to make modulation meaningful.
    eps = 1e-10
    w0 = min(max(w0, 0.0), 1.0)
    # Convert w0 to logit safely
    w0_logit = np.log((w0 + eps) / (1.0 - (w0 + eps)))
    w_logit = w0_logit + 3.0 * (0.5 - stai_val)  # lower STAI -> larger w
    w = 1.0 / (1.0 + np.exp(-w_logit))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values = expected max over stage-2 by transition structure
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = transition_matrix @ max_q2  # shape (2,)

        # Hybrid action values for stage-1
        q1 = (1.0 - w) * q1_mf + w * q1_mb

        # Softmax policy at stage-1
        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1_shift)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Softmax policy at stage-2 (state-specific)
        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta2 * q2_shift)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF TD from stage-2 value and eligibility trace from reward PE
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1
        q1_mf[a1] += alpha * lam * delta2

    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)

","['alpha', 'beta1', 'beta2', 'lam', 'w0']"
iter0_run0_participant20.json,cognitive_model1,375.7929476745039,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated arbitration, eligibility trace, and stickiness.
    
    Parameters
    ----------
    action_1 : 1D array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) per trial.
    state : 1D array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : 1D array-like of int (0 or 1)
        Second-stage choices (alien index within the reached planet) per trial.
    reward : 1D array-like of float (0 or 1)
        Obtained coins per trial.
    stai : 1D array-like of float in [0,1]
        Participant STAI score; higher means higher anxiety. Used to modulate arbitration and stickiness.
    model_parameters : iterable of floats
        Parameters (bounds):
          - alpha in [0,1]: learning rate for action values
          - beta in [0,10]: inverse temperature for softmax
          - w0 in [0,1]: base model-based weight (anxiety reduces it)
          - lam in [0,1]: eligibility trace mixing second-stage PE into first-stage MF update
          - pers in [0,1]: base first-stage choice stickiness strength (scaled up by anxiety)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w0, lam, pers = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities of the actually observed choices (for NLL)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)          # model-free first-stage Q
    q_stage2_mf = np.zeros((2, 2))     # model-free second-stage Q per state
    prev_choice1 = None                # for stickiness

    for t in range(n_trials):

        # Model-based first-stage Q from current second-stage MF values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)     # best alien per planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Anxiety-modulated arbitration and stickiness
        # Higher anxiety reduces reliance on model-based control and increases stickiness
        w_eff = np.clip(w0 * (1.0 - 0.6 * stai), 0.0, 1.0)
        kappa = pers * (0.5 + stai)  # stickiness scales up with anxiety

        # Combine MB and MF values
        q1_combined = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add stickiness bias to first-stage preferences
        pref1 = beta * q1_combined
        if prev_choice1 is not None:
            stick = np.zeros(2)
            stick[prev_choice1] = 1.0
            pref1 = pref1 + kappa * stick

        # First-stage policy and likelihood
        exp_q1 = np.exp(pref1 - np.max(pref1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy and likelihood (state is observed)
        s = state[t]
        pref2 = beta * q_stage2_mf[s]
        exp_q2 = np.exp(pref2 - np.max(pref2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Prediction errors
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]          # bootstrapped MF TD error at stage 1
        delta2 = reward[t] - q_stage2_mf[s, a2]                # reward PE at stage 2

        # Updates
        # Second-stage MF update
        q_stage2_mf[s, a2] += alpha * delta2
        # First-stage MF update with eligibility trace from stage 2
        q_stage1_mf[a1] += alpha * (delta1 + lam * delta2)

        # Book-keeping
        prev_choice1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'lam', 'pers']"
iter0_run0_participant21.json,cognitive_model1,510.29554608697504,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free SARSA(Î») with anxiety-modulated arbitration.
    
    This model blends model-based and model-free values at stage 1. The arbitration
    weight is modulated by the participant's anxiety (stai), consistent with evidence
    that higher anxiety reduces model-based control. Model-free values are updated
    via an eligibility trace from stage 2. Choices at both stages are generated
    via a softmax with inverse temperature beta.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for value updates
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = lam (0 to 1): eligibility trace strength from stage 2 to stage 1
    - model_parameters[3] = w0 (0 to 1): baseline weight on model-based values at stage 1
    - model_parameters[4] = gamma (0 to 1): strength of anxiety modulation on model-based weight
      Effective weight: w_eff = (1 - gamma) * w0 + gamma * (1 - stai), so higher anxiety reduces w

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha, beta, lam, w0, gamma = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Value stores
    q_stage1_mf = np.zeros(2)          # model-free Q at stage 1 (spaceships)
    q_stage2 = np.zeros((2, 2))        # model-free Q at stage 2 (aliens within planet)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight (higher anxiety -> lower MB weight)
    w_eff = (1.0 - gamma) * w0 + gamma * (1.0 - stai_val)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based evaluation for stage 1
        max_q_stage2 = np.max(q_stage2, axis=1)              # best alien per planet
        q_stage1_mb = transition_matrix @ max_q_stage2       # MB value for each spaceship

        # Hybrid action values
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Stage 1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage 2 TD error and update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage 1 model-free update via eligibility trace from stage 2
        # Propagate the stage-2 prediction error backward
        q_stage1_mf[a1] += alpha * lam * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lam', 'w0', 'gamma']"
iter0_run0_participant21.json,cognitive_model3,457.7487207351081,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Kalman-filtered model-based planning with anxiety-modulated volatility and stickiness.
    
    Stage-2 values are tracked with a simple Kalman filter per state-action,
    capturing gradual reward drift. The process noise (volatility) is increased
    by anxiety (stai), yielding larger Kalman gains and faster updating when anxious.
    First-stage choices use model-based planning through the known transition matrix.
    A small perseveration bias at stage 1 is stronger for lower anxiety (assuming
    anxious participants rely less on habits in this variant).

    Parameters (bounds):
    - model_parameters[0] = v_base (0 to 1): baseline process noise (volatility) of rewards
    - model_parameters[1] = r_base (0 to 1): observation noise
    - model_parameters[2] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[3] = kappa (0 to 1): baseline perseveration bias magnitude at stage 1
    - model_parameters[4] = eta (0 to 1): strength of anxiety modulation on process noise
      Effective volatility: v_eff = v_base * (1 + eta * stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    v_base, r_base, beta, kappa, eta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Kalman filter parameters
    v_eff = v_base * (1.0 + eta * stai_val)   # anxiety increases assumed volatility
    r_eff = r_base

    # Means and variances for stage-2 rewards
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 1.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Model-based stage-1 values from current stage-2 means
        max_q2 = np.max(q2_mean, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Perseveration bias stronger for lower anxiety in this variant
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = kappa * (1.0 - stai_val)

        # Stage 1 policy
        q1_eff = q1_mb + bias
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy from current means
        s = state[t]
        q2_s_mean = q2_mean[s]
        exp_q2 = np.exp(beta * (q2_s_mean - np.max(q2_s_mean)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Kalman update for chosen state-action
        prior_var = q2_var[s, a2] + v_eff
        gain = prior_var / (prior_var + r_eff + 1e-12)
        pred = q2_mean[s, a2]
        q2_mean[s, a2] = pred + gain * (r - pred)
        q2_var[s, a2] = (1.0 - gain) * prior_var

        # Optional small diffusion for unchosen pairs to reflect drift
        # (kept small to avoid dominating; ensures volatility meaningfully applied)
        for ss in range(2):
            for aa in range(2):
                if not (ss == s and aa == a2):
                    q2_var[ss, aa] = q2_var[ss, aa] + 0.1 * v_eff

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['v_base', 'r_base', 'beta', 'kappa', 'eta']"
iter0_run0_participant22.json,cognitive_model1,354.06257349705015,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and first-stage stickiness.
    
    This model blends model-free (MF) and model-based (MB) values for the stage-1 choice.
    Anxiety (stai) down-weights the MB contribution via an anx_gain parameter.
    A first-stage perseveration bias (kappa) adds a bonus to repeating the previous stage-1 action.
    Second-stage decisions are purely MF with softmax.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for MF updates at both stages.
    - beta: [0,10] inverse temperature for softmax at both stages.
    - w0: [0,1] baseline MB weight (before anxiety modulation).
    - kappa: [0,1] perseveration strength for repeating the previous stage-1 action.
    - anx_gain: [0,1] scales how much anxiety reduces MB weight; w_eff = clip(w0*(1 - anx_gain*stai)).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array of ints in {0,1}, reached planet (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on that planet (0 vs 1).
    - reward: array of floats, obtained coins per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, w0, kappa, anx_gain].
    
    Returns:
    - Negative log-likelihood of the observed stage-1 and stage-2 choices under the model.
    """"""
    alpha, beta, w0, kappa, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q_stage1_mf = np.zeros(2)          # values for A vs U
    q_stage2_mf = np.zeros((2, 2))     # rows planets X/Y, cols aliens

    # Perseveration memory
    prev_a1 = None

    # Anxiety-modulated arbitration weight
    w_eff = w0 * (1.0 - anx_gain * stai)
    w_eff = min(1.0, max(0.0, w_eff))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Model-based stage-1 Q: expected max MF value at stage-2 via transition model
        max_q_stage2 = np.max(q_stage2_mf, axis=1)          # size 2 (planets)
        q_stage1_mb = transition_matrix @ max_q_stage2      # size 2 (spaceships)

        # Hybrid Q for stage-1
        q1_hybrid = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add perseveration bias for repeating previous stage-1 choice
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += kappa

        # Stage-1 policy
        logits1 = beta * q1_hybrid + bias
        logits1 -= np.max(logits1)  # numerical stability
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure MF)
        logits2 = beta * q_stage2_mf[s, :]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # TD errors
        # Stage-2 update (MF)
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update using a ""bootstrapped"" signal from stage-2 chosen action
        # and an eligibility component from outcome
        # First, immediate bootstrapped update towards the chosen second-stage value
        target1 = q_stage2_mf[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Outcome-eligibility bonus: propagate part of outcome back to stage-1
        q_stage1_mf[a1] += alpha * 0.5 * delta2  # fixed lambda-like factor 0.5

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'kappa', 'anx_gain']"
iter0_run0_participant22.json,cognitive_model2,353.978120373215,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with eligibility traces, anxiety-modulated exploration and stickiness.
    
    This model is a two-stage MF RL with eligibility trace lambda for credit assignment
    from stage-2 outcomes to stage-1 values. Anxiety reduces the effective inverse temperature
    (more exploratory under higher anxiety) and also reduces the effective lambda.
    A perseveration bias applies to both stages.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for stage-2 and stage-1 updates.
    - beta: [0,10] baseline inverse temperature (before anxiety modulation).
    - lam0: [0,1] baseline eligibility trace parameter.
    - gamma_anx: [0,1] scales how much anxiety reduces inverse temperature: beta_eff = beta*(1 - gamma_anx*stai).
    - persev: [0,1] perseveration strength added to the previously chosen action at each stage.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship.
    - state: array of ints in {0,1}, reached planet.
    - action_2: array of ints in {0,1}, chosen alien.
    - reward: array of floats, coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, lam0, gamma_anx, persev].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, lam0, gamma_anx, persev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective inverse temperature and eligibility under anxiety
    beta_eff = beta * (1.0 - gamma_anx * stai)
    # Keep positive range
    beta_eff = max(0.0, min(10.0, beta_eff))
    lam_eff = lam0 * (1.0 - 0.5 * stai)
    lam_eff = min(1.0, max(0.0, lam_eff))

    # Q-values
    q1 = np.zeros(2)          # stage-1 MF Q
    q2 = np.zeros((2, 2))     # stage-2 MF Q by state

    # Probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 policy with perseveration
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += persev
        logits1 = beta_eff * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration (state-specific)
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += persev
        logits2 = beta_eff * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # TD updates
        # Stage-2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 with eligibility trace from outcome
        # Update towards immediate second-stage value and propagate outcome with lambda
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        q1[a1] += alpha * delta1
        q1[a1] += alpha * lam_eff * delta2

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lam0', 'gamma_anx', 'persev']"
iter0_run0_participant22.json,cognitive_model3,354.0625734970504,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid with anxiety-driven transition uncertainty and perseveration.
    
    This model assumes participants may misrepresent the transition structure.
    Anxiety increases transition noise, mixing the true transition matrix with a uniform matrix.
    Stage-1 decisions are a hybrid of model-based (using the noisy transition belief)
    and model-free values, with a fixed MB weight omega. Perseveration bias applies to stage-1.
    Stage-2 is purely MF.
    
    Parameters (model_parameters):
    - alpha: [0,1] MF learning rate.
    - beta: [0,10] inverse temperature for softmax.
    - phi: [0,1] scales anxiety-driven transition noise; noise = phi * stai.
    - omega: [0,1] weight of MB values in the hybrid for stage-1.
    - persev: [0,1] perseveration strength for repeating stage-1 action.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship.
    - state: array of ints in {0,1}, reached planet.
    - action_2: array of ints in {0,1}, chosen alien.
    - reward: array of floats, coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, phi, omega, persev].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, phi, omega, persev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # True transition matrix
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]])
    # Anxiety-driven transition noise: mix with uniform (0.5,0.5) per row
    noise = phi * stai
    noise = min(1.0, max(0.0, noise))
    T_uniform = np.array([[0.5, 0.5],
                          [0.5, 0.5]])
    T_belief = (1.0 - noise) * T_true + noise * T_uniform

    # Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # MB evaluation with noisy transition belief
        max_q2 = np.max(q2_mf, axis=1)      # value of best alien per planet
        q1_mb = T_belief @ max_q2

        # Hybrid Q for stage-1
        q1_h = omega * q1_mb + (1.0 - omega) * q1_mf

        # Perseveration on stage-1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += persev

        # Stage-1 choice
        logits1 = beta * q1_h + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= (np.sum(p1) + 1e-16)
        p_choice_1[t] = p1[a1]

        # Stage-2 choice (MF)
        logits2 = beta * q2_mf[s, :]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= (np.sum(p2) + 1e-16)
        p_choice_2[t] = p2[a2]

        # MF updates
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # First-stage MF update: towards second-stage chosen value + eligibility from outcome
        target1 = q2_mf[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1
        q1_mf[a1] += alpha * 0.5 * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'phi', 'omega', 'persev']"
iter0_run0_participant23.json,cognitive_model1,366.9535833537392,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated arbitration and stickiness.
    
    This model blends a model-based first-stage value with a model-free value and adds
    an anxiety-scaled choice stickiness bias at both stages. Second-stage values are
    learned with a standard TD rule; the first-stage model-free value is updated via
    an eligibility trace from the second stage. Anxiety (stai) reduces the model-based
    weight and increases perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0/1)
        Observed second-stage state (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0/1)
        Second-stage choices within the visited state (two aliens per planet).
    reward : array-like of float
        Rewards obtained on each trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score scaled to [0,1]; only the first element is used.
    model_parameters : list/tuple of floats
        [alpha, beta, w0, lam, stick]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - w0 in [0,1]: baseline model-based weight at stage 1 (reduced by anxiety).
        - lam in [0,1]: eligibility trace weighting from stage 2 to stage 1 MF value.
        - stick in [0,1]: baseline stickiness magnitude (scaled up by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w0, lam, stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # True transition structure (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free action values
    q1_mf = np.zeros(2)        # first-stage MF
    q2_mf = np.zeros((2, 2))   # second-stage MF: state x action

    # Stickiness traces (last choices); initialize to ""no bias""
    last_a1 = None
    last_a2 = {0: None, 1: None}

    # Anxiety-modulated arbitration (reduce MB as anxiety increases)
    w_mb = max(0.0, min(1.0, w0 * (1.0 - stai)))  # stays within [0,1]
    # Anxiety-modulated stickiness scale
    stick_scale = stick * stai

    for t in range(n_trials):
        # Model-based first-stage Q via one-step lookahead
        max_q2 = np.max(q2_mf, axis=1)  # best action per state
        q1_mb = transition_matrix @ max_q2

        # Hybrid Q at stage 1
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add stickiness bias at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stick_scale

        # Softmax policy stage 1
        logits1 = beta * q1_hybrid + bias1
        # numerically stable softmax
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])

        p_choice_1[t] = probs1[a1]

        # Second stage policy: softmax over MF Q with stickiness within state
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += stick_scale

        logits2 = beta * q2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Stage 2 TD update (model-free)
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage 1 MF update via eligibility trace from stage 2 value
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * lam * delta1

        # Update stickiness memories
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'w0', 'lam', 'stick']"
iter0_run0_participant23.json,cognitive_model2,374.1261140730644,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-sensitive hybrid RL with valence-asymmetric learning and anxiety effects.

    This model uses separate learning rates for positive vs. negative second-stage
    prediction errors, and adjusts credit assignment and arbitration depending on
    whether the transition was common or rare. Anxiety reduces model-based arbitration
    and down-weights eligibility after rare transitions.

    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0/1)
        Observed second-stage state (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0/1)
        Second-stage choices within the visited state (two aliens per planet).
    reward : array-like of float
        Rewards obtained on each trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score scaled to [0,1]; only the first element is used.
    model_parameters : list/tuple of floats
        [alpha_pos, alpha_neg, beta, omega0, persev]
        - alpha_pos in [0,1]: learning rate when second-stage PE is positive.
        - alpha_neg in [0,1]: learning rate when second-stage PE is negative.
        - beta in [0,10]: inverse temperature for both stages.
        - omega0 in [0,1]: baseline model-based weight at stage 1.
        - persev in [0,1]: baseline perseveration bias (scaled by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_pos, alpha_neg, beta, omega0, persev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Perseveration traces
    last_a1 = None
    last_a2 = {0: None, 1: None}

    # Anxiety reduces MB arbitration weight
    omega = max(0.0, min(1.0, omega0 * (1.0 - stai)))
    # Perseveration magnitude increases with anxiety
    persev_scale = persev * stai

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Determine if transition was common or rare given the chosen ship
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

        # Model-based planning at stage 1
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid Q1 with anxiety-reduced MB weight
        q1_hybrid = omega * q1_mb + (1.0 - omega) * q1_mf

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += persev_scale

        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second stage policy with perseveration within state
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += persev_scale

        logits2 = beta * q2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Valence-asymmetric second-stage learning
        pe2 = r - q2_mf[s, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        # Anxiety modestly reduces learning rate overall
        lr2_eff = lr2 * (1.0 - 0.5 * stai)
        q2_mf[s, a2] += lr2_eff * pe2

        # Eligibility trace with transition sensitivity and anxiety
        # After rare transitions, anxious participants discount credit assignment
        lam_eff = 1.0 if is_common else (1.0 - stai)
        pe1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += lr2_eff * lam_eff * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_pos', 'alpha_neg', 'beta', 'omega0', 'persev']"
iter0_run0_participant23.json,cognitive_model3,439.2157205985088,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-adjusted transition belief, softmax gain, and forgetting.

    This model assumes the agent plans with an internal transition belief that may be
    biased toward ambiguity with higher anxiety. It also includes anxiety-amplified
    softmax gain and global forgetting. Second-stage values are learned by TD; the
    first-stage action values are computed purely model-based from the believed
    transition matrix.

    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0/1)
        Observed second-stage state (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0/1)
        Second-stage choices within the visited state (two aliens per planet).
    reward : array-like of float
        Rewards obtained on each trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score scaled to [0,1]; only the first element is used.
    model_parameters : list/tuple of floats
        [alpha, beta, k_beta, rho, decay]
        - alpha in [0,1]: TD learning rate at the second stage.
        - beta in [0,10]: base inverse temperature.
        - k_beta in [0,1]: anxiety gain on inverse temperature (beta_eff = beta*(1+k_beta*stai)).
        - rho in [0,1]: confidence in common transitions (maps to p_common belief).
        - decay in [0,1]: global forgetting factor per trial (scaled up by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, k_beta, rho, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-adjusted transition belief:
    # p_common ranges from 0.5 (ambiguous) to 0.7 (true) depending on rho and reduced by anxiety
    p_common = 0.5 + 0.2 * rho * (1.0 - stai)
    p_rare = 1.0 - p_common
    T_belief = np.array([[p_common, p_rare],
                         [p_rare,  p_common]])

    # Anxiety-amplified softmax gain (capped implicitly by problem constraints on beta input)
    beta_eff = beta * (1.0 + k_beta * stai)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Only second-stage is learned; first-stage is computed from believed transitions
    q2 = np.zeros((2, 2))  # state x action

    # Effective forgetting rate increases with anxiety
    forget = decay * (0.5 + 0.5 * stai)

    for t in range(n_trials):
        # Apply global forgetting before acting
        q2 *= (1.0 - forget)

        # Compute MB first-stage Q from transition belief and current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_belief @ max_q2

        # Stage 1 policy
        logits1 = beta_eff * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Second-stage TD update with anxiety-stable learning rate
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'k_beta', 'rho', 'decay']"
iter0_run0_participant24.json,cognitive_model1,455.61359468536637,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated arbitration and eligibility trace.
    
    This model blends a model-based (MB) plan using the known transition matrix with a model-free (MF)
    system. The arbitration weight favoring MB control decreases with anxiety (stai). An eligibility
    trace propagates the reward prediction error from stage 2 back to stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., coins; typically 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, w_base, lam, gamma_anx]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - w_base in [0,1]: baseline MB weight in the hybrid action value.
        - lam in [0,1]: eligibility trace scaling the backpropagation of the stage-2 RPE to stage 1.
        - gamma_anx in [0,1]: strength by which anxiety (stai) shifts control toward MF (reduces MB weight).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w_base, lam, gamma_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q1_mf = np.zeros(2)         # stage-1 MF values for actions A/U
    q2 = np.zeros((2, 2))       # stage-2 MF values for states X/Y and their two actions

    # Anxiety-modulated arbitration weight (more anxiety -> more MF)
    w_mb = np.clip(w_base - gamma_anx * s, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based Q at stage 1: expected max over next-state Q2 under transitions
        max_q2_per_state = np.max(q2, axis=1)              # shape (2,)
        q1_mb = transition_matrix @ max_q2_per_state       # shape (2,)

        # Hybrid Q for stage 1 policy
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy and probability of observed choice
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy conditioned on observed state
        st = state[t]
        exp_q2 = np.exp(beta * (q2[st] - np.max(q2[st])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD error and update
        r = reward[t]
        delta2 = r - q2[st, a2]
        q2[st, a2] += alpha * delta2

        # Stage-1 MF TD error: bootstrap from the chosen stage-2 action value
        delta1 = q2[st, a2] - q1_mf[a1]
        # Backpropagate both via direct TD(0) and eligibility-trace-scaled stage-2 RPE
        q1_mf[a1] += alpha * (delta1 + lam * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_base', 'lam', 'gamma_anx']"
iter0_run0_participant24.json,cognitive_model2,448.9306443519023,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pure model-free SARSA(0) with anxiety-modulated learning asymmetry and choice stickiness.
    
    This model learns MF values for both stages. Learning rates differ for positive vs. negative
    prediction errors, and anxiety shifts the positivity bias (higher anxiety reduces the difference).
    Choice policies at both stages include a perseveration (stickiness) bias that grows with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha_pos, alpha_neg, beta, kappa0, kappa_stai]
        - alpha_pos in [0,1]: base learning rate for positive prediction errors.
        - alpha_neg in [0,1]: base learning rate for negative prediction errors.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa0 in [0,1]: baseline perseveration bias added to the last chosen action.
        - kappa_stai in [0,1]: how much anxiety increases perseveration (stickiness).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_pos, alpha_neg, beta, kappa0, kappa_stai = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # MF action values
    q1 = np.zeros(2)            # stage-1 MF values
    q2 = np.zeros((2, 2))       # stage-2 MF values

    # Perseveration state
    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    # Anxiety-modulated stickiness
    kappa = kappa0 + kappa_stai * s

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Build stickiness biases
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        st = state[t]
        bias2 = np.zeros(2)
        if last_a2_by_state[st] is not None:
            bias2[last_a2_by_state[st]] += kappa

        # Stage-1 policy with stickiness
        prefs1 = q1 + bias1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness (within current state)
        prefs2 = q2[st] + bias2
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning with anxiety-modulated asymmetry:
        # Anxiety s blends the two alphas, reducing positivity bias at higher s
        # Effective alphas for this trial:
        # For positive PE: move alpha_pos toward alpha_neg as s increases
        eff_alpha_pos = (1 - s) * alpha_pos + s * alpha_neg
        # For negative PE: move alpha_neg toward alpha_pos as s increases
        eff_alpha_neg = (1 - s) * alpha_neg + s * alpha_pos

        # Stage-2 update
        r = reward[t]
        pe2 = r - q2[st, a2]
        a2_lr = eff_alpha_pos if pe2 >= 0 else eff_alpha_neg
        q2[st, a2] += a2_lr * pe2

        # Stage-1 update: bootstrap from chosen stage-2 value (pure MF SARSA(0))
        pe1 = q2[st, a2] - q1[a1]
        a1_lr = eff_alpha_pos if pe1 >= 0 else eff_alpha_neg
        q1[a1] += a1_lr * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2_by_state[st] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'kappa0', 'kappa_stai']"
iter0_run0_participant24.json,cognitive_model3,542.0331215464248,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planning with learned transitions, anxiety-modulated reward sensitivity and lapse.
    
    This model learns the first-stage transition probabilities and plans by projecting second-stage
    values through the learned transition model. Anxiety increases subjective reward sensitivity and
    also increases a choice lapse rate (randomness) that mixes the softmax policy with uniform choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha_r, beta, alpha_t, rho0, eps0]
        - alpha_r in [0,1]: learning rate for second-stage value updates.
        - beta in [0,10]: inverse temperature for softmax.
        - alpha_t in [0,1]: learning rate for first-stage transition probabilities.
        - rho0 in [0,1]: baseline reward sensitivity (scales reward).
        - eps0 in [0,1]: baseline lapse rate that is scaled by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_r, beta, alpha_t, rho0, eps0 = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize learned transition model close to uninformative (0.5/0.5)
    T = np.full((2, 2), 0.5)  # rows: actions (A,U), cols: states (X,Y)

    # Second-stage values
    q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    rho = np.clip(rho0 * (0.5 + 0.5 * s), 0.0, 1.0)       # higher anxiety -> higher reward impact
    eps_lapse = np.clip(eps0 * (0.2 + 0.8 * s), 0.0, 0.49)  # higher anxiety -> larger lapse

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based Q at stage 1 from learned transitions
        max_q2 = np.max(q2, axis=1)     # per state
        q1_mb = T @ max_q2

        # Stage-1 softmax with lapse
        logits1 = beta * (q1_mb - np.max(q1_mb))
        exp1 = np.exp(logits1)
        soft1 = exp1 / np.sum(exp1)
        probs1 = (1 - eps_lapse) * soft1 + eps_lapse * np.array([0.5, 0.5])
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with lapse (within current state)
        st = state[t]
        logits2 = beta * (q2[st] - np.max(q2[st]))
        exp2 = np.exp(logits2)
        soft2 = exp2 / np.sum(exp2)
        probs2 = (1 - eps_lapse) * soft2 + eps_lapse * np.array([0.5, 0.5])
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward with anxiety-modulated sensitivity
        r = rho * reward[t]

        # Update second-stage value
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_r * pe2

        # Update transition model for chosen action based on observed state
        # Move probability of observed transition toward 1, the alternative toward 0
        T[a1, st] += alpha_t * (1.0 - T[a1, st])
        other = 1 - st
        T[a1, other] += alpha_t * (0.0 - T[a1, other])

        # Keep rows within [0,1] and normalized (optional normalization to reduce drift)
        T[a1] = np.clip(T[a1], 0.001, 0.999)
        T[a1] /= np.sum(T[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta', 'alpha_t', 'rho0', 'eps0']"
iter0_run0_participant25.json,cognitive_model1,130.9513657427434,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and eligibility traces.
    
    This model blends model-based (MB) and model-free (MF) control at stage 1,
    uses a softmax policy at both stages, and learns with TD(Î»). Anxiety (stai)
    down-weights model-based control and increases perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) on each trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y) on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., alien indices) on each trial.
    reward : array-like of float
        Reward received on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Anxiety score(s). Uses the first element as the participant's anxiety.
        Interpretation in this model:
          - Higher stai reduces model-based weight and increases perseveration strength.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w_mb, lam, kappa)
        - alpha in [0,1]: learning rate.
        - beta in [0,10]: inverse temperature for softmax.
        - w_mb in [0,1]: baseline model-based weight (before anxiety).
        - lam in [0,1]: eligibility trace parameter Î».
        - kappa in [0,1]: baseline perseveration weight.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """"""
    alpha, beta, w_mb, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0]) if hasattr(stai, ""__len__"") else float(stai)

    # Transition: rows are first-stage actions (0=A,1=U), columns are states (0=X,1=Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage1_mf = np.zeros(2)         # MF values for stage-1 actions
    q_stage2_mf = np.zeros((2, 2))    # MF values for stage-2 actions per state

    # For likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration tracking (previous choices)
    prev_a1 = -1
    prev_a2_state0 = -1
    prev_a2_state1 = -1

    # Anxiety-modulated arbitration and perseveration
    # Higher anxiety reduces model-based control
    w_eff = w_mb * (1.0 - stai_val)
    # Higher anxiety increases perseveration influence
    kappa_eff = kappa * (1.0 + stai_val)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]

        # Model-based values for stage 1 via one-step lookahead (using MF stage-2 values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)        # size 2: best value at each state
        q_stage1_mb = transition_matrix @ max_q_stage2     # size 2

        # Hybrid stage-1 values
        q1_hybrid = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa_eff

        # Softmax for stage 1
        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = max(probs1[a1], eps)

        # Stage-2 policy (state-conditional)
        # Add perseveration bias at stage 2 (state-specific)
        bias2 = np.zeros(2)
        prev_a2 = prev_a2_state0 if s == 0 else prev_a2_state1
        if prev_a2 in (0, 1):
            bias2[prev_a2] += kappa_eff

        logits2 = beta * q_stage2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = max(probs2[a2], eps)

        r = reward[t]

        # TD updates
        # Stage-2 update
        q2_old = q_stage2_mf[s, a2]
        delta2 = r - q2_old
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace
        delta1 = q2_old - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1 + lam * delta2)

        # Update perseveration memory
        prev_a1 = a1
        if s == 0:
            prev_a2_state0 = a2
        else:
            prev_a2_state1 = a2

    neg_ll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return float(neg_ll)

","['alpha', 'beta', 'w_mb', 'lam', 'kappa']"
iter0_run0_participant26.json,cognitive_model1,312.57671909504506,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated hybrid model-based/model-free RL with eligibility traces.
    
    Overview:
    - Computes a convex combination of model-based (MB) and model-free (MF) action values at stage 1.
    - MB uses the known transition matrix (common=0.7, rare=0.3).
    - MF values are learned from reward via stage-2 TD errors and an eligibility trace for stage 1.
    - Anxiety (stai) controls the arbitration weight between MB and MF via a sigmoid gate.

    Parameters (model_parameters):
    - alpha: learning rate for value updates in [0,1].
    - beta: inverse temperature for softmax in [0,10].
    - w0: base arbitration bias (pre-sigmoid) in [0,1] mapped through sigmoid to [0,1].
    - k_stai: strength of anxiety modulation of arbitration in [0,1]; larger means anxiety pushes policy more toward MB (positive) or MF (negative if you set w0 accordingly).
    - lam: eligibility trace parameter in [0,1], scales how the stage-2 TD error backs up to stage-1 MF values.

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (0=A, 1=U).
    - state: array of ints in {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}, second-stage choices (0 or 1 for the two aliens on that planet).
    - reward: array of floats (typically 0 or 1), coins returned.
    - stai: array-like with a single float in [0,1], anxiety score for the participant.
    - model_parameters: list or array containing [alpha, beta, w0, k_stai, lam].

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """"""
    alpha, beta, w0, k_stai, lam = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure: A->X common; U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)       # MF values at stage 1 (for A vs U)
    q_stage2_mf = np.zeros((2, 2))  # MF values at stage 2: state in {X=0, Y=1} x action in {0,1}

    # Anxiety-modulated arbitration weight (0..1)
    # w(stai) = sigmoid(w0 + k_stai*(stai - 0.5))
    gate = w0 + k_stai * (st - 0.5)
    w = 1.0 / (1.0 + np.exp(-5.0 * gate))  # steeper sigmoid for sensitivity while respecting bounds

    eps = 1e-12

    for t in range(n_trials):
        # Model-based values at stage 1 from current stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)         # value of best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2     # plan through transitions

        # Arbitration
        q_stage1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # First-stage policy
        a1 = int(action_1[t])
        exp_q1 = np.exp(beta * (q_stage1 - np.max(q_stage1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = int(state[t])
        a2 = int(action_2[t])
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        pe2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * pe2

        # Stage-1 MF bootstrap (SARSA-style) and eligibility trace from stage-2 PE
        # Bootstrap toward observed stage-2 action value
        pe1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * pe1
        # Eligibility: back up a fraction of stage-2 PE to stage1 MF
        q_stage1_mf[a1] += lam * alpha * pe2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

","['alpha', 'beta', 'w0', 'k_stai', 'lam']"
iter0_run0_participant26.json,cognitive_model2,252.53323530600892,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-modulated learning asymmetry and perseveration.
    
    Overview:
    - Learns an internal transition model T_est for each first-stage action and plans model-based values through it.
    - Combines MB and MF values at stage 1 with a simple anxiety-based arbitration: weight on MF increases with anxiety.
    - Stage-2 learning uses valence-asymmetric learning rates that scale with anxiety (more anxious -> stronger learning from non-reward).
    - Includes an anxiety-amplified perseveration bias at stage 1.

    Parameters (model_parameters):
    - alpha: base learning rate in [0,1].
    - beta: inverse temperature for softmax in [0,10].
    - kappa0: base learning rate for updating the transition model in [0,1].
    - phi: perseveration strength in [0,1], bias to repeat previous stage-1 action.
    - a_neg: asymmetry coefficient in [0,1] scaling how anxiety increases learning from negative outcomes.

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (0=A, 1=U).
    - state: array of ints in {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}, second-stage choices on the observed planet.
    - reward: array of floats (typically 0 or 1).
    - stai: array-like with a single float in [0,1], anxiety score for the participant.
    - model_parameters: list/array [alpha, beta, kappa0, phi, a_neg].

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """"""
    alpha, beta, kappa0, phi, a_neg = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize internal transition model with instructed/common structure
    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Perseveration kernel (one-hot of previous action)
    prev_a1 = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-modulated quantities
    # MF weight increases with anxiety; MB weight decreases correspondingly
    w_mf = st
    w_mb = 1.0 - w_mf
    # Transition learning rate increases with anxiety (more anxious -> more volatile belief updates)
    kappa = kappa0 * (0.25 + 0.75 * st)

    # Valence-asymmetric learning rates modulated by anxiety
    alpha_pos = np.clip(alpha * (1.0 - a_neg * st), 0.0, 1.0)
    alpha_neg = np.clip(alpha * (1.0 + a_neg * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 MB values via learned transitions
        max_q2 = np.max(q_stage2, axis=1)      # best alien on each planet
        q1_mb = T_est @ max_q2

        # Perseveration bias (encourages repeating last first-stage choice), amplified by anxiety
        persev_bias = np.zeros(2)
        if prev_a1 is not None:
            persev_bias[prev_a1] = phi * (0.5 + 0.5 * st)

        # Combine MB and MF for stage 1
        q1_total = w_mb * q1_mb + w_mf * q_stage1_mf + persev_bias

        # Policy stage 1
        exp_q1 = np.exp(beta * (q1_total - np.max(q1_total)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Policy stage 2
        q2_vec = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_vec - np.max(q2_vec)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Learn transitions T_est for the chosen first-stage action from observed state
        # Move the chosen row toward one-hot of observed state
        target = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1] = (1.0 - kappa) * T_est[a1] + kappa * target
        # Keep rows normalized (numerical safety)
        T_est[a1] = T_est[a1] / (np.sum(T_est[a1]) + eps)

        # Stage-2 value update with valence asymmetry
        pe2 = r - q_stage2[s, a2]
        lr2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q_stage2[s, a2] += lr2 * pe2

        # Stage-1 MF bootstrap toward updated stage-2 value
        pe1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr2 * pe1

        prev_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

","['alpha', 'beta', 'kappa0', 'phi', 'a_neg']"
iter0_run0_participant27.json,cognitive_model1,417.71779504864594,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-weighted arbitration, learned transitions, and eligibility trace.

    This model learns second-stage Q-values from rewards, a model-free first-stage Q via TD with eligibility,
    and a model-based first-stage value via a learned transition model. The arbitration between model-based
    and model-free control is modulated by anxiety (stai): higher stai shifts control toward model-free.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state each trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (0/1 correspond to the two aliens on that planet).
    reward : array-like of float
        Obtained reward on each trial (e.g., coins; typically 0/1).
    stai : array-like of float
        Array containing a single standardized anxiety score in [0,1]. Higher means more anxious.
    model_parameters : list or array of floats
        [alpha, beta, w_base, lam, k_stai]
        - alpha in [0,1]: learning rate for both transition and value updates.
        - beta in [0,10]: inverse temperature for softmax policies.
        - w_base in [0,1]: baseline weight on model-based control at stage 1.
        - lam in [0,1]: eligibility trace strength to back-propagate reward to stage 1.
        - k_stai in [0,1]: strength with which anxiety shifts arbitration toward model-free.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w_base, lam, k_stai = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model; rows: action1, cols: state
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Action values
    Q1_mf = np.zeros(2)              # model-free first stage
    Q2 = 0.5 * np.ones((2, 2))       # second-stage action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation: expected value of each first-stage action via transition model
        max_Q2 = np.max(Q2, axis=1)  # value of best action at each second-stage state
        Q1_mb = T @ max_Q2

        # Anxiety-weighted arbitration: higher stai -> less model-based weight
        w_mb = np.clip(w_base * (1.0 - k_stai * stai), 0.0, 1.0)
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # First-stage policy
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy at observed state
        s = int(state[t])
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Updates
        # Second-stage TD
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # First-stage model-free with eligibility: combine value prediction error and backpropagated reward
        delta1_val = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * (delta1_val + lam * delta2)

        # Learn transition model from observed transition (chosen a1 led to s)
        # Move T[a1, s] toward 1 and the other toward 0
        T[a1, s] += alpha * (1.0 - T[a1, s])
        T[a1, 1 - s] += alpha * (0.0 - T[a1, 1 - s])

        # Keep each row normalized
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_base', 'lam', 'k_stai']"
iter0_run0_participant27.json,cognitive_model2,430.75265416924856,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric learning with anxiety-modulated arbitration and choice stickiness.

    The model uses separate learning rates for positive vs. negative second-stage prediction errors,
    propagates them to first stage, blends model-based and model-free values with an anxiety-modulated
    weight, and includes an anxiety-scaled perseveration bias on choices at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Rewards each trial.
    stai : array-like of float
        Single anxiety score in [0,1].
    model_parameters : list or array of floats
        [alpha_pos, alpha_neg, beta, w_mb0, stickiness]
        - alpha_pos in [0,1]: learning rate when second-stage PE is positive.
        - alpha_neg in [0,1]: learning rate when second-stage PE is negative.
        - beta in [0,10]: inverse temperature.
        - w_mb0 in [0,1]: baseline model-based weight.
        - stickiness in [0,1]: base perseveration strength; scaled by anxiety for effective bias.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_pos, alpha_neg, beta, w_mb0, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition model (common: 0->0 and 1->1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = 0.5 * np.ones((2, 2))

    # Perseveration traces (previous choices, start neutral)
    last_a1 = None
    last_a2 = np.array([None, None], dtype=object)  # track per-state last second-stage action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated perseveration scale (higher stai -> stronger stickiness)
    kappa = stickiness * (0.25 + stai)  # in [0, 1.25*stickiness]

    for t in range(n_trials):
        # Model-based value at stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Anxiety-modulated model-based weight: higher anxiety reduces MB weight
        w_mb = np.clip(w_mb0 - 0.5 * (stai - 0.5), 0.0, 1.0)

        # Combine values
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Add perseveration bias to stage-1 logits
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with state-specific perseveration bias
        s = int(state[t])
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += kappa

        logits2 = beta * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning rates based on sign of second-stage PE
        pe2 = r - Q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        Q2[s, a2] += alpha2 * pe2

        # Propagate to first stage (use same asymmetry)
        pe1 = Q2[s, a2] - Q1_mf[a1]
        alpha1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        Q1_mf[a1] += alpha1 * pe1

        # Update perseveration traces
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'w_mb0', 'stickiness']"
iter0_run0_participant27.json,cognitive_model3,302.74476126473303,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-sensitive exploration, rare-transition down-weighting, and Pavlovian safety bias.

    This model blends model-free and model-based control with a fixed weight, but:
    - Down-weights model-based control specifically after rare transitions, more so with higher anxiety.
    - Uses anxiety to reduce effective beta (more exploration with higher anxiety).
    - Adds a Pavlovian 'safety' bias toward action 0 at stage 2 that grows with anxiety and uncertainty.
    - Includes value forgetting toward 0.5 to capture drift in unchosen/unstimulated values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Rewards each trial.
    stai : array-like of float
        Single anxiety score in [0,1].
    model_parameters : list or array of floats
        [alpha, beta, mb_weight, bias_safe, forget]
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: base inverse temperature.
        - mb_weight in [0,1]: baseline model-based mixing weight at stage 1.
        - bias_safe in [0,1]: strength of Pavlovian bias favoring action 0 under uncertainty.
        - forget in [0,1]: forgetting rate pulling Qs toward 0.5 each trial.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, mb_weight, bias_safe, forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure for common vs rare inference
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = 0.5 * np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Identify whether the observed transition was common or rare under fixed structure
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Anxiety-dependent down-weighting of model-based control following rare transitions
        mb_w_eff = mb_weight if is_common else mb_weight * (1.0 - 0.7 * stai)
        mb_w_eff = np.clip(mb_w_eff, 0.0, 1.0)

        # Model-based value from fixed transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_fixed @ max_Q2
        Q1 = mb_w_eff * Q1_mb + (1.0 - mb_w_eff) * Q1_mf

        # Anxiety-reduced beta: higher anxiety -> more exploration
        beta_eff = beta * (1.0 - 0.5 * stai)

        # Stage 1 policy
        logits1 = beta_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Pavlovian safety bias at stage 2: favor action 0 when uncertain
        uncertainty = 1.0 - abs(Q2[s, 0] - Q2[s, 1])  # in [0,1]
        bias_vec = np.array([bias_safe * stai * uncertainty, 0.0])

        logits2 = beta_eff * Q2[s] + bias_vec
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Forgetting toward 0.5 before applying TD updates
        Q2 = (1.0 - forget) * Q2 + forget * 0.5
        Q1_mf = (1.0 - forget) * Q1_mf + forget * 0.0  # MF baseline around 0

        # Second-stage TD
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # First-stage MF update with eligibility component
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * (0.5 * delta1 + 0.5 * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'mb_weight', 'bias_safe', 'forget']"
iter0_run0_participant28.json,cognitive_model1,436.165531159023,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and eligibility traces.
    
    This model blends model-based (MB) and model-free (MF) values at stage 1.
    Anxiety (stai) modulates the arbitration weight between MB and MF via kappa,
    shifting control toward MF as anxiety increases (or vice versa depending on kappa),
    and uses an eligibility trace to propagate second-stage TD errors back to the first stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within each state (two aliens per state).
    reward : array-like of float in [0,1]
        Obtained reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; higher means higher anxiety.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, w_base, lam, kappa]
        - alpha (learning rate for MF and second-stage Q; [0,1])
        - beta (inverse temperature for softmax; [0,10])
        - w_base (baseline MB weight at stai=0.5; [0,1])
        - lam (eligibility trace for backpropagating second-stage PE; [0,1])
        - kappa (strength by which stai modulates MB weight; [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, w_base, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure known to participant
    transition_matrix = np.array([[0.7, 0.3],  # A -> (X,Y)
                                 [0.3, 0.7]]) # U -> (X,Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free action values
    q_stage1_mf = np.zeros(2)        # for A,U
    q_stage2 = np.zeros((2, 2))      # for state X,Y and two aliens each

    eps = 1e-10

    for t in range(n_trials):
        # Anxiety-modulated arbitration weight
        # w_eff increases/decreases linearly with stai with slope controlled by kappa
        # w_eff = clip( w_base + kappa * (stai - 0.5), 0, 1 )
        w_eff = w_base + kappa * (stai - 0.5)
        w_eff = 0.0 if w_eff < 0.0 else (1.0 if w_eff > 1.0 else w_eff)

        # Model-based stage-1 value: expected max of stage-2 by transitions
        max_q_stage2 = np.max(q_stage2, axis=1)        # size 2: states
        q_stage1_mb = transition_matrix @ max_q_stage2 # size 2: actions

        # Blend MB and MF for stage-1
        q1 = (1.0 - w_eff) * q_stage1_mf + w_eff * q_stage1_mb

        # Softmax for stage-1
        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in observed state
        s = int(state[t])
        q2_s = q_stage2[s]
        q2_centered = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: second-stage TD error and update
        pe2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        # Model-free first-stage update via eligibility trace using stage-2 PE
        q_stage1_mf[a1] += alpha * lam * pe2

        # Also a direct MF update toward the chosen second-stage value (SARSA(0)-like bootstrapping)
        boot = q_stage2[s, a2]
        pe1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (1.0 - lam) * pe1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'w_base', 'lam', 'kappa']"
iter0_run0_participant28.json,cognitive_model2,404.43474405826066,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with anxiety-scaled perseveration and asymmetric learning rates.
    
    A purely model-free controller learns both stages with separate learning rates
    for positive vs negative RPEs. Choice perseveration (stickiness) biases both
    stages toward repeating the last chosen action; its strength is scaled by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within the current state).
    reward : array-like of float in [0,1]
        Obtained reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; scales perseveration strength.
    model_parameters : list or array-like of 5 floats
        [alpha_pos, alpha_neg, beta, rho, omega]
        - alpha_pos (learning rate for positive RPEs; [0,1])
        - alpha_neg (learning rate for negative RPEs; [0,1])
        - beta (inverse temperature; [0,10])
        - rho (baseline perseveration strength; [0,1])
        - omega (anxiety modulation of perseveration; [0,1])
          effective stickiness = rho * (1 + omega * (stai - 0.5))
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_pos, alpha_neg, beta, rho, omega = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Perseveration traces: last chosen action for each stage/state
    last_a1 = None
    last_a2 = np.array([None, None], dtype=object)  # per state

    eps = 1e-10

    # Anxiety-scaled perseveration coefficient
    rho_eff = rho * (1.0 + omega * (stai - 0.5))

    for t in range(n_trials):
        # Stage-1 preferences include stickiness
        pref1 = q_stage1.copy()
        if last_a1 is not None:
            stick = np.array([0.0, 0.0])
            stick[last_a1] = 1.0
            pref1 = pref1 + rho_eff * stick

        # Softmax for stage-1
        pref1_c = pref1 - np.max(pref1)
        probs_1 = np.exp(beta * pref1_c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 preferences include state-specific stickiness
        s = int(state[t])
        pref2 = q_stage2[s].copy()
        if last_a2[s] is not None:
            stick2 = np.array([0.0, 0.0])
            stick2[int(last_a2[s])] = 1.0
            pref2 = pref2 + rho_eff * stick2

        pref2_c = pref2 - np.max(pref2)
        probs_2 = np.exp(beta * pref2_c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning rates by sign of RPE
        pe2 = r - q_stage2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q_stage2[s, a2] += alpha2 * pe2

        # Bootstrapped MF update of stage-1 toward realized stage-2 value
        pe1 = q_stage2[s, a2] - q_stage1[a1]
        alpha1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        q_stage1[a1] += alpha1 * pe1

        # Update perseveration memories
        last_a1 = a1
        last_a2[s] = a2

    neg_log_lik = -(np.sum(np.log(p_choice_1 + 1e-10)) + np.sum(np.log(p_choice_2 + 1e-10)))
    return neg_log_lik

","['alpha_pos', 'alpha_neg', 'beta', 'rho', 'omega']"
iter0_run0_participant28.json,cognitive_model3,439.16832426364203,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning MB/MF hybrid with anxiety-shaped priors and risk-sensitive utility.
    
    This model learns the transition structure online and uses it for MB planning.
    Anxiety shapes the initial transition beliefs (higher anxiety -> more uncertainty,
    i.e., closer to 0.5/0.5) and increases reward concavity via a risk-sensitivity parameter.
    MB and MF are blended at stage 1 with a deterministic weight 1 - stai (more anxiety -> more MF).
    An eligibility trace propagates second-stage PEs to stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state.
    reward : array-like of float in [0,1]
        Obtained reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; used in priors, risk sensitivity, and arbitration.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, tau, kappa_risk, lam]
        - alpha (learning rate for Q-values; [0,1])
        - beta (inverse temperature; [0,10])
        - tau (transition learning rate; [0,1])
        - kappa_risk (baseline risk-sensitivity exponent; [0,1])
          effective exponent gamma = kappa_risk + stai * (1 - kappa_risk)
        - lam (eligibility trace for MF backpropagation; [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, tau, kappa_risk, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-shaped initial transition beliefs: shrink toward 0.5 as stai increases
    base_common = 0.7
    init_common = (1.0 - stai) * base_common + stai * 0.5
    # Rows: actions (A,U); Cols: states (X,Y). Start with common/rare structure but softened by stai
    trans_est = np.array([[init_common, 1.0 - init_common],   # A -> (X,Y)
                          [1.0 - init_common, init_common]])  # U -> (X,Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Anxiety-modulated risk exponent for utility
    gamma = kappa_risk + stai * (1.0 - kappa_risk)  # in [kappa_risk, 1], higher stai -> more concavity

    eps = 1e-10

    for t in range(n_trials):
        # Model-based Q1 from current transition estimates
        max_q2 = np.max(q_stage2, axis=1)  # per state
        q1_mb = trans_est @ max_q2

        # Arbitration weight: w_mb = 1 - stai (higher anxiety -> more MF)
        w_mb = 1.0 - stai
        q1 = (1.0 - w_mb) * q_stage1_mf + w_mb * q1_mb

        # Stage-1 policy
        q1_c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2_s = q_stage2[s]
        q2_c = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Risk-sensitive utility transformation of reward
        r = reward[t]
        u = r ** gamma

        # Second-stage update
        pe2 = u - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        # MF first-stage updates: eligibility-trace using PE2 and direct bootstrap
        q_stage1_mf[a1] += alpha * lam * pe2
        boot = q_stage2[s, a2]
        pe1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (1.0 - lam) * pe1

        # Transition learning: update estimated row for chosen action toward observed state
        # Simple delta rule on categorical distribution (two states)
        # Move probability mass toward the observed state for chosen action
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        trans_est[a1, :] = trans_est[a1, :] + tau * (target - trans_est[a1, :])

        # Ensure row-stochastic and within [0,1] bounds by normalization
        row_sum = np.sum(trans_est[a1, :])
        if row_sum > 0:
            trans_est[a1, :] = trans_est[a1, :] / row_sum

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik","['alpha', 'beta', 'tau', 'kappa_risk', 'lam']"
iter0_run0_participant29.json,cognitive_model1,402.0684501357661,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    The agent blends a model-based (MB) plan using the known common transitions with a model-free (MF) cached value, and uses an eligibility trace to propagate reward back to the first-stage MF values. Anxiety (stai) reduces reliance on MB planning.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices (alien within state).
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score (this participantâs score).
    - model_parameters: iterable of 5 floats
        learning_rate: [0,1] â reward learning rate for MF Q-values
        beta: [0,10] â inverse temperature for softmax at both stages
        w0: [0,1] â baseline weight on MB planning at stage 1
        lam: [0,1] â eligibility trace for MF update from stage 2 to stage 1
        anx_influence: [0,1] â how strongly anxiety reduces MB weight

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    learning_rate, beta, w0, lam, anx_influence = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (common: 0.7; rare: 0.3)
    # Rows: first-stage action (0=A,1=U). Cols: state (0=X,1=Y).
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices each trial
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)           # for A vs U
    q_stage2_mf = np.zeros((2, 2))      # for states X,Y and two aliens per state

    # Anxiety-modulated MB weight: higher anxiety reduces MB reliance
    w_effective = np.clip(w0 * (1.0 - anx_influence * stai_val), 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):
        # Compute MB action values at stage 1 via one-step lookahead over states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # value of each second-stage state
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid value
        q_stage1 = w_effective * q_stage1_mb + (1.0 - w_effective) * q_stage1_mf

        # Softmax policy for stage 1
        q1 = q_stage1 - np.max(q_stage1)  # stability
        exp_q1 = np.exp(beta * q1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (pure MF at the visited state)
        s = state[t]
        q2_row = q_stage2_mf[s]
        q2c = q2_row - np.max(q2_row)
        exp_q2 = np.exp(beta * q2c)
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD learning
        # Stage 2 update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta2

        # Stage 1 MF update with eligibility trace using the stage-2 prediction error
        # Also allow direct bootstrapping toward the stage-2 chosen action value
        delta1 = (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        q_stage1_mf[a1] += learning_rate * (lam * delta2 + (1.0 - lam) * delta1)

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['learning_rate', 'beta', 'w0', 'lam', 'anx_influence']"
iter0_run0_participant29.json,cognitive_model2,301.45027785951345,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""MF with valence-asymmetric learning and anxiety-modulated perseveration.
    The agent is model-free at both stages, but uses different learning rates for positive and negative outcomes. Anxiety increases sensitivity to negative outcomes and increases choice perseveration (tendency to repeat previous choices).

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha_base: [0,1] â base learning rate
        beta: [0,10] â inverse temperature
        valence_bias: [0,1] â scales difference between positive vs negative learning
        perseveration: [0,1] â base perseveration strength
        anx_gain: [0,1] â how strongly anxiety increases neg. learning and perseveration

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    alpha_base, beta, valence_bias, perseveration, anx_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective learning rates
    # Positive learning rate slightly boosted; negative learning rate boosted by anxiety
    alpha_pos = np.clip(alpha_base * (1.0 + 0.5 * valence_bias), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 + valence_bias * anx_gain * stai_val), 0.0, 1.0)

    # Perseveration terms: bias toward repeating previous actions; anxiety amplifies
    pers1_strength = perseveration * (1.0 + anx_gain * stai_val)
    pers2_strength = perseveration * 0.5 * (1.0 + anx_gain * stai_val)

    # MF Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Last choices for perseveration signals
    last_a1 = None
    last_a2 = np.array([None, None], dtype=object)  # per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    for t in range(n_trials):
        # Stage 1 logits with perseveration bias
        logits1 = q1.copy()
        if last_a1 is not None:
            logits1[last_a1] += pers1_strength

        # Softmax stage 1
        logits1_c = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1_c)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 logits with state-specific perseveration
        s = state[t]
        logits2 = q2[s].copy()
        if last_a2[s] is not None:
            logits2[last_a2[s]] += pers2_strength

        logits2_c = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2_c)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning rates by valence
        pe2 = r - q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        q2[s, a2] += lr2 * pe2

        # Stage 1 MF bootstrapping toward the value of the selected second-stage action
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        q1[a1] += lr1 * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha_base', 'beta', 'valence_bias', 'perseveration', 'anx_gain']"
iter0_run0_participant29.json,cognitive_model3,530.1910487860262,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based with learned transition matrix, lapse-exploration modulated by anxiety.
    The agent learns second-stage rewards (MF at stage 2) and also learns the transition probabilities from first-stage actions to states. Stage-1 decisions are model-based using the learned transition matrix. Anxiety increases lapse/exploration and transition learning rate (assuming higher perceived volatility).

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha_r: [0,1] â reward learning rate for second-stage Q
        beta: [0,10] â inverse temperature
        alpha_t: [0,1] â base transition learning rate
        epsilon0: [0,1] â base lapse probability (uniform choice mixture)
        anx_weight: [0,1] â how strongly anxiety increases alpha_t and epsilon

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    alpha_r, beta, alpha_t, epsilon0, anx_weight = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix T[a, s], rows sum to 1. Start near uniform.
    T = np.full((2, 2), 0.5)

    # Second-stage MF Q-values
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    epsilon = np.clip(epsilon0 * (0.5 + stai_val * anx_weight), 0.0, 1.0)
    alpha_t_eff = np.clip(alpha_t * (1.0 + stai_val * anx_weight), 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):
        # Model-based values at stage 1 using learned transitions
        V_state = np.max(Q2, axis=1)  # value for each state
        Q1_mb = T @ V_state

        # Softmax with lapse at stage 1
        q1c = Q1_mb - np.max(Q1_mb)
        sm1 = np.exp(beta * q1c)
        sm1 /= (np.sum(sm1) + eps)
        # Lapse mixture toward uniform
        probs1 = (1.0 - epsilon) * sm1 + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax with lapse at the visited state (same epsilon)
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        sm2 = np.exp(beta * q2c)
        sm2 /= (np.sum(sm2) + eps)
        probs2 = (1.0 - epsilon) * sm2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model T using delta rule toward one-hot observation
        # Observation o_s = 1 for reached state; 0 for the other. Keep row normalized.
        o = np.array([0.0, 0.0])
        o[s] = 1.0
        T[a1] = T[a1] + alpha_t_eff * (o - T[a1])
        # Ensure numerical stability and row normalization
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        # Update second-stage Q-values
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * pe2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha_r', 'beta', 'alpha_t', 'epsilon0', 'anx_weight']"
iter0_run0_participant3.json,cognitive_model1,415.8381704015121,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-modulated planning, temperature, and perseveration.
    
    This model mixes model-free (first- and second-stage Q-learning with eligibility trace)
    and model-based evaluation of first-stage actions via the known transition matrix. Anxiety
    down-weights planning (model-based), lowers effective inverse temperature (more noise),
    and increases perseveration/stickiness.

    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: array-like of int in {0,1}; chosen spaceship at stage 1 for each trial
    - state:    array-like of int in {0,1}; reached planet (0=X, 1=Y) for each trial
    - action_2: array-like of int in {0,1}; chosen alien at stage 2 for each trial
    - reward:   array-like of float in [0,1]; coins received on each trial
    - stai:     array-like with one float in [0,1]; anxiety score (higher = higher anxiety)
    - model_parameters: tuple/list of five parameters:
        alpha: learning rate for both stages (0..1)
        beta: inverse temperature for softmax (0..10)
        w_base: baseline model-based weight (0..1)
        kappa_base: baseline perseveration strength (0..1)
        lam: eligibility trace scaling credit assignment from stage 2 to stage 1 (0..1)
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w_base, kappa_base, lam = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize value functions
    q1_mf = np.zeros(2)          # model-free stage-1 values for actions {A, U}
    q2 = np.zeros((2, 2))        # second-stage Q-values: states {X,Y} x actions {0,1}

    # Perseveration memory
    prev_a1 = -1
    prev_a2 = -1  # last second-stage action (state-unspecific stickiness for simplicity)

    # Anxiety modulations
    # Higher anxiety -> less planning, more perseveration, more noise
    w = np.clip(w_base * (1.0 - 0.7 * s), 0.0, 1.0)
    kappa = kappa_base * (0.5 + 0.5 * s)
    beta_eff = max(1e-8, beta * (1.0 - 0.4 * s))  # never negative/zero

    for t in range(n_trials):
        # Model-based first-stage values via one-step lookahead
        max_q2 = np.max(q2, axis=1)            # best alien per planet
        q1_mb = transition_matrix @ max_q2     # expected value per spaceship

        # Hybrid action values
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 stickiness bias (one-hot for previous action)
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Policy for the first choice (softmax with stickiness)
        logits1 = beta_eff * q1_hybrid + kappa * stick1
        logits1 -= np.max(logits1)  # numerical stability
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (state-dependent) with stickiness on last second-stage action
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta_eff * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 TD update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

        # Stage-1 model-free TD update toward the realized second-stage action value
        delta1_mf = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1_mf

        # Eligibility trace: propagate stage-2 RPE back to stage-1 choice
        q1_mf[a1] += alpha * lam * delta2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_base', 'kappa_base', 'lam']"
iter0_run0_participant3.json,cognitive_model2,403.04790853338216,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive learning and transition-belief distortion with anxiety.
    
    Anxiety modulates:
    - Perceived transition structure: more anxious participants assume less reliable transitions.
    - Adaptive learning rate: increases with surprise magnitude and anxiety.
    - Perseveration: increased with anxiety.
    - Planning weight: decreases with anxiety.

    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: array-like int {0,1}; first-stage choice per trial
    - state:    array-like int {0,1}; reached second-stage state per trial
    - action_2: array-like int {0,1}; second-stage choice per trial
    - reward:   array-like float [0,1]; reward per trial
    - stai:     array-like with one float [0,1]; anxiety score
    - model_parameters: tuple/list of five parameters:
        alpha0: baseline learning rate (0..1)
        beta: inverse temperature (0..10)
        tau: sensitivity of learning rate to unsigned RPE and anxiety (0..1)
        ps: perseveration baseline strength (0..1)
        zeta: strength of anxiety-induced transition unreliability belief (0..1)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha0, beta, tau, ps, zeta = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Perceived transition reliability (common probability)
    p_common = 0.7 - 0.2 * s * zeta
    p_common = np.clip(p_common, 0.5, 0.7)
    transition_matrix = np.array([[p_common, 1.0 - p_common],
                                  [1.0 - p_common, p_common]])

    # Initialize
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Perseveration and planning weight
    kappa = ps * (0.5 + 0.5 * s)
    w = np.clip(1.0 - s, 0.0, 1.0)  # higher anxiety -> less planning
    beta_eff = beta  # keep base temperature here

    # For adaptive learning rate, track last unsigned RPE magnitude
    last_abs_rpe2 = 0.0

    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Combine MB and MF
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 stickiness
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Policy stage 1
        logits1 = beta_eff * q1 + kappa * stick1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta_eff * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Adaptive learning rate based on surprise and anxiety
        alpha_eff = alpha0 * (1.0 - 0.5 * s) + tau * s * last_abs_rpe2
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Learning updates
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += alpha_eff * delta2

        delta1 = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_eff * delta1

        # Modest eligibility to propagate outcome surprise to stage 1 as well
        q1_mf[a1] += alpha_eff * 0.5 * delta2

        # Update trackers
        last_abs_rpe2 = abs(delta2)
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'tau', 'ps', 'zeta']"
iter0_run0_participant3.json,cognitive_model3,517.285267976067,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Affective utility and anxiety-modulated credit assignment with targeted avoidance bias.
    
    This model assumes anxiety distorts utility and credit assignment:
    - Outcome utility is shifted down by a pessimism baseline proportional to anxiety.
    - Outcome sensitivity is reduced with anxiety (dampened affective impact).
    - Eligibility trace for credit assignment from stage 2 to stage 1 is reduced by anxiety.
    - A targeted avoidance bias at stage 2: with anxiety, participants avoid one alien
      on planet Y (state=1, action=1), implemented as a negative bias on that logit.
    - First-stage decisions mix MB and MF with anxiety-reduced planning weight.

    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: array-like int {0,1}; first-stage choices
    - state:    array-like int {0,1}; second-stage states
    - action_2: array-like int {0,1}; second-stage choices
    - reward:   array-like float [0,1]; obtained rewards
    - stai:     array-like with one float [0,1]; anxiety score
    - model_parameters: tuple/list of five parameters:
        alpha: learning rate (0..1)
        beta: inverse temperature (0..10)
        lam_base: baseline eligibility trace (0..1)
        bias_ref: pessimism reference for utility shift (0..1)
        omega: strength of anxiety-driven avoidance/sensitivity (0..1)
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, lam_base, bias_ref, omega = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed true transition
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Anxiety modulations
    # Utility transformation: pessimistic shift and dampening
    # r_eff = (reward - bias_ref * s) scaled by (1 - 0.5 * s * omega)
    scale = 1.0 - 0.5 * s * omega
    scale = max(scale, 1e-4)
    lam = lam_base * (1.0 - 0.5 * s)  # reduced credit assignment with anxiety
    w = np.clip(1.0 - 0.5 * s, 0.0, 1.0)  # reduced planning with anxiety

    for t in range(n_trials):
        # Model-based Q for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Combine MB and MF
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with targeted avoidance bias on planet Y, action=1
        s_idx = state[t]
        logits2 = beta * q2[s_idx].copy()
        if s_idx == 1:
            # Anxiety-driven avoidance of action 1 on planet Y
            logits2[1] -= beta * omega * s
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Affective utility transformation
        r_raw = reward[t]
        r_eff = scale * (r_raw - bias_ref * s)

        # Learning updates
        delta2 = r_eff - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

        delta1 = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Anxiety-reduced eligibility trace
        q1_mf[a1] += alpha * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'lam_base', 'bias_ref', 'omega']"
iter0_run0_participant30.json,cognitive_model1,485.4208225598213,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with eligibility trace and perseveration.
    
    Anxiety use: Higher STAI reduces model-based control weight (w), biasing toward model-free control.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (spaceship: 0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (planet: 0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien on the planet; index within the planet).
    reward : array-like of float (0 or 1)
        Received reward per trial.
    stai : array-like of float
        Trait anxiety score; stai[0] used here. Interpreted in [0,1].
    model_parameters : array-like of float
        [alpha, beta, w, lambda_, perseveration]
        - alpha in [0,1]: learning rate for Q-value updates.
        - beta in [0,10]: inverse temperature for softmax choice.
        - w in [0,1]: baseline model-based weight; anxiety reduces this.
        - lambda_ in [0,1]: eligibility trace mixing stage-2 PE into stage-1 update.
        - perseveration in [0,1]: tendency to repeat previous action (applied at both stages).
        
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, w, lambda_, perseveration = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: rows are first-stage actions (A,U), cols are states (X,Y).
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action-value tables
    q_stage1_mf = np.zeros(2)           # MF Q for first-stage actions (A,U)
    q_stage2_mf = np.zeros((2, 2))      # MF Q for second-stage actions at each state

    # Perseveration memory (previous actions), initialized to neutral (no bias)
    prev_a1 = None
    prev_a2_by_state = [None, None]

    # Anxiety-modulated model-based weight: higher anxiety reduces MB control.
    w_eff = np.clip(w * (1.0 - 0.8 * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])

        # Compute model-based action values at stage 1 from stage-2 MF values (one-step lookahead)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)      # size 2: best alien at X, Y
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value of A and U

        # Hybrid stage-1 values
        q1 = (1.0 - w_eff) * q_stage1_mf + w_eff * q_stage1_mb

        # Add perseveration bias for repeating previous stage-1 action
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += perseveration

        # Softmax for stage 1
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (softmax over MF Q at reached state), with perseveration bias
        q2 = q_stage2_mf[s].copy()
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += perseveration

        logits2 = beta * q2 + bias2
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # Temporal-difference errors
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        delta2 = r - q_stage2_mf[s, a2]

        # Updates: stage-2 MF
        q_stage2_mf[s, a2] += alpha * delta2

        # Updates: stage-1 MF with eligibility trace (includes both delta1 and lambda*delta2)
        q_stage1_mf[a1] += alpha * (delta1 + lambda_ * delta2)

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w', 'lambda_', 'perseveration']"
iter0_run0_participant31.json,cognitive_model1,441.1391570092653,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-modulated arbitration and eligibility trace.
    
    This model blends a model-free (MF) first-stage value with a model-based (MB) plan,
    with the arbitration weight w increasing or decreasing as a function of anxiety (stai).
    A single learning rate governs updates, and an eligibility trace propagates second-stage
    reward prediction errors (RPEs) back to first-stage MF values. Likelihoods are computed
    from softmax policies at each stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (typically 0 or 1)
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values indicate higher anxiety.
    model_parameters : array-like of floats, length 5
        [alpha, beta, w0, k_stai, lam]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w0 in [0,1]: base model-based weight.
        - k_stai in [0,1]: strength with which anxiety modulates the MB weight.
                           Positive values increase MB weight with higher anxiety if stai>0.5,
                           and decrease it if stai<0.5.
        - lam in [0,1]: eligibility trace parameter for propagating second-stage RPE to stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """"""
    alpha, beta, w0, k_stai, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X and U->Y are common (0.7), cross are rare (0.3)
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)       # MF Q for first-stage actions [A,U]
    q_stage2_mf = np.zeros((2, 2))  # MF Q for second-stage actions in states [X,Y] x [0,1]

    # Anxiety-modulated arbitration weight (kept in [0,1])
    w = w0 + k_stai * (stai - 0.5)
    w = 0.0 if w < 0.0 else (1.0 if w > 1.0 else w)

    for t in range(n_trials):
        # Model-based Q for stage 1: expected max second-stage value under transition model
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [max in X, max in Y]
        q_stage1_mb = transition_matrix @ max_q_stage2  # [A, U]

        # Hybrid value for policy
        q1_hybrid = (1 - w) * q_stage1_mf + w * q_stage1_mb

        # Stage-1 policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)  # stability
        exp_q1 = np.exp(logits1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy within observed state
        s = state[t]
        logits2 = beta * q_stage2_mf[s]
        logits2 -= np.max(logits2)
        exp_q2 = np.exp(logits2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        # TD at stage 1 (toward current second-stage chosen value)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # TD at stage 2 with reward
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Eligibility trace: propagate stage-2 RPE to stage-1 MF value
        q_stage1_mf[a1] += lam * alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'k_stai', 'lam']"
iter0_run0_participant31.json,cognitive_model2,388.10610702251677,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free SARSA(Î») with valence-asymmetric learning and anxiety-modulated exploration and perseveration.

    This purely model-free account allows different effective learning rates for positive
    versus negative RPEs (via a single asymmetry parameter). Anxiety reduces the inverse
    temperature (more exploration with higher anxiety) and increases perseveration (stickiness).
    Likelihoods are from softmax at each stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher scores => more exploration, more perseveration.
    model_parameters : array-like of floats, length 5
        [alpha_base, beta, k_posneg, pi0, k_stai]
        - alpha_base in [0,1]: base learning rate.
        - beta in [0,10]: base inverse temperature.
        - k_posneg in [0,1]: controls asymmetry between positive and negative RPE learning.
                             alpha_pos = alpha_base*(1 + k_posneg), alpha_neg = alpha_base*(1 - k_posneg).
        - pi0 in [0,1]: base perseveration strength added to the last chosen first-stage action.
        - k_stai in [0,1]: anxiety modulation strength; higher stai decreases beta and increases perseveration.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """"""
    alpha_base, beta, k_posneg, pi0, k_stai = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective learning rates for valence
    alpha_pos = alpha_base * (1.0 + k_posneg)
    alpha_neg = alpha_base * (1.0 - k_posneg)
    alpha_pos = 1.0 if alpha_pos > 1.0 else (0.0 if alpha_pos < 0.0 else alpha_pos)
    alpha_neg = 1.0 if alpha_neg > 1.0 else (0.0 if alpha_neg < 0.0 else alpha_neg)

    # Anxiety-modulated exploration: higher stai -> lower beta_eff
    beta_eff = beta * (1.0 + (0.5 - stai) * k_stai * 1.5)
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Anxiety-modulated perseveration: higher stai -> more stickiness
    persev = pi0 + k_stai * stai
    persev = 0.0 if persev < 0.0 else (1.0 if persev > 1.0 else persev)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q1 = np.zeros(2)        # first-stage MF Q
    q2 = np.zeros((2, 2))   # second-stage MF Q

    prev_a1 = None

    for t in range(n_trials):
        # Stage-1 policy with perseveration bias on previous first-stage action
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = persev
        logits1 = beta_eff * q1 + bias
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (no perseveration here)
        s = state[ t ]
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning
        # Stage-1 TD toward second-stage chosen value (SARSA-style)
        delta1 = q2[s, a2] - q1[a1]
        a1_lr = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1[a1] += a1_lr * delta1

        # Stage-2 TD with reward
        r = reward[t]
        delta2 = r - q2[s, a2]
        a2_lr = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += a2_lr * delta2

        # Eligibility trace variant: propagate second-stage RPE to first stage
        q1[a1] += a2_lr * delta2  # uses the valence-appropriate rate

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_base', 'beta', 'k_posneg', 'pi0', 'k_stai']"
iter0_run0_participant31.json,cognitive_model3,426.4805111423017,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-dependent credit assignment (TDL) with anxiety-modulated transition sensitivity and perseveration.

    This model implements a transition-outcome interaction at the first stage:
    rewards after common transitions reinforce the chosen first-stage action,
    whereas the same rewards after rare transitions shift credit toward the unchosen action.
    Anxiety increases or decreases the strength of this transition-dependent credit assignment.
    A standard MF learner operates at stage 2. Likelihoods come from softmax policies.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety modulates the transition sensitivity.
    model_parameters : array-like of floats, length 5
        [alpha, beta, gamma0, k_stai, rho]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - gamma0 in [0,1]: base transition-dependent credit assignment strength.
        - k_stai in [0,1]: modulation of gamma by anxiety: gamma = clip(gamma0 + k_stai*(stai-0.51), 0, 1).
        - rho in [0,1]: perseveration strength at stage 1 (bias toward repeating previous first-stage choice).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """"""
    alpha, beta, gamma0, k_stai, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-modulated transition sensitivity
    gamma = gamma0 + k_stai * (stai - 0.51)
    gamma = 0.0 if gamma < 0.0 else (1.0 if gamma > 1.0 else gamma)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_tdl = np.zeros(2)      # first-stage values shaped by transition-dependent credit assignment
    q2 = np.zeros((2, 2))     # second-stage MF values

    prev_a1 = None

    for t in range(n_trials):
        # Stage-1 softmax with perseveration
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = rho
        logits1 = beta * q1_tdl + bias
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 softmax
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning at stage 2
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Transition-dependent credit assignment at stage 1
        # Common if chosen spaceship typically goes to the observed state (A->X, U->Y)
        is_common = 1 if (a1 == s) else 0
        sign_c = 1 if is_common == 1 else -1

        # Use the immediate second-stage RPE to drive first-stage credit assignment
        pe = delta2  # incorporating outcome surprise
        # Reinforce chosen action on common; shift to unchosen on rare
        q1_tdl[a1] += alpha * gamma * sign_c * pe
        q1_tdl[1 - a1] -= alpha * gamma * sign_c * pe

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'gamma0', 'k_stai', 'rho']"
iter0_run0_participant32.json,cognitive_model1,350.7853621376478,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Hybrid model-based/model-free two-step learner with anxiety-modulated arbitration and stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (for the observed state; 0 or 1) for each trial.
    reward : array-like of float
        Obtained reward (gold coins) on each trial (typically in [0,1]).
    stai : array-like of float
        Anxiety score array; uses stai[0]. Higher values indicate higher anxiety.
    model_parameters : list or array-like of float
        [alpha, beta, omega, phi, lam]
        Bounds:
        - alpha: learning rate in [0,1]
        - beta: softmax inverse temperature in [0,10]
        - omega: model-based weight in [0,1]
        - phi: choice stickiness strength in [0,1]
        - lam: eligibility trace strength in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """"""
    alpha, beta, omega, phi, lam = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize choice likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)          # model-free values for A/U
    q_stage2 = np.zeros((2, 2))        # second-stage Q-values per state (X,Y) x action (0,1)

    # Stickiness (previous choices)
    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    for t in range(n_trials):
        # Model-based evaluation for stage 1
        max_q2 = np.max(q_stage2, axis=1)  # best action per planet
        q_stage1_mb = transition_matrix @ max_q2

        # Anxiety-modulated arbitration: higher anxiety reduces MB control
        omega_eff = np.clip(omega * (1.0 - stai), 0.0, 1.0)

        # Combine MB and MF values
        q1_base = omega_eff * q_stage1_mb + (1.0 - omega_eff) * q_stage1_mf

        # Add anxiety-modulated stickiness at stage 1
        stick1 = phi * stai
        if prev_a1 is not None:
            bias_vec = np.array([1.0 if i == prev_a1 else 0.0 for i in range(2)])
        else:
            bias_vec = np.zeros(2)
        q1_policy = q1_base + stick1 * bias_vec

        # Softmax policy for stage 1
        exp_q1 = np.exp(beta * (q1_policy - np.max(q1_policy)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with stickiness within the reached state
        s = state[t]
        q2_state = q_stage2[s].copy()
        stick2 = phi * stai
        prev_a2 = prev_a2_by_state[s]
        if prev_a2 is not None:
            bias2 = np.array([1.0 if i == prev_a2 else 0.0 for i in range(2)])
        else:
            bias2 = np.zeros(2)
        q2_policy = q2_state + stick2 * bias2

        exp_q2 = np.exp(beta * (q2_policy - np.max(q2_policy)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2 (standard Q-learning)
        r = reward[t]
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Model-free learning at stage 1 with eligibility trace
        # Move Q1 toward the chosen second-stage value and additionally backpropagate reward via lambda
        td_to_q2 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * td_to_q2 + alpha * lam * delta2

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'omega', 'phi', 'lam']"
iter0_run0_participant32.json,cognitive_model2,381.1908741602572,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Pure model-free SARSA(Î»)-style learner with asymmetric learning rates and anxiety-modulated learning/exploration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) for each trial.
    reward : array-like of float
        Obtained reward on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0]. Higher values indicate higher anxiety.
    model_parameters : list or array-like of float
        [alpha_pos, alpha_neg, beta, rho, eta]
        Bounds:
        - alpha_pos: learning rate for positive prediction errors in [0,1]
        - alpha_neg: learning rate for negative prediction errors in [0,1]
        - beta: softmax inverse temperature in [0,10]
        - rho: reward sensitivity scaling in [0,1]
        - eta: anxiety impact strength in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """"""
    alpha_pos, alpha_neg, beta, rho, eta = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Value functions
    q1 = np.zeros(2)         # first-stage MF values
    q2 = np.zeros((2, 2))    # second-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated inverse temperature: higher anxiety -> more stochastic
    beta_eff = beta * (1.0 - 0.5 * eta * stai)

    # Effective SARSA(Î») trace strength as a function of anxiety and eta
    lam_eff = np.clip(0.1 + eta * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Stage 1 policy
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        exp_q2 = np.exp(beta_eff * (q2[s] - np.max(q2[s])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward sensitivity and anxiety-modulated learning rate
        r = reward[t]
        r_eff = rho * r

        # Second-stage update (asymmetric learning rates)
        pe2 = r_eff - q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0 else alpha_neg
        # Anxiety reduces effective learning rate proportionally to eta
        alpha2 *= max(0.0, 1.0 - eta * stai)
        q2[s, a2] += alpha2 * pe2

        # First-stage update via SARSA(Î»): credit assignment from stage 2
        # Target for Q1 is the chosen Q2 value; also backpropagate immediate outcome via lambda
        td1 = q2[s, a2] - q1[a1]
        # Use the same sign-based alpha for consistency
        alpha1 = alpha_pos if td1 >= 0 else alpha_neg
        alpha1 *= max(0.0, 1.0 - eta * stai)
        q1[a1] += alpha1 * td1 + alpha1 * lam_eff * pe2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha_pos', 'alpha_neg', 'beta', 'rho', 'eta']"
iter0_run0_participant32.json,cognitive_model3,511.7811204352659,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Model-based planner with anxiety-modulated transition certainty and epsilon-soft exploration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices in the reached state.
    reward : array-like of float
        Obtained reward on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0]. Higher values indicate higher anxiety.
    model_parameters : list or array-like of float
        [alpha, beta, kappa, epsilon, w2]
        Bounds:
        - alpha: second-stage learning rate baseline in [0,1]
        - beta: softmax inverse temperature in [0,10]
        - kappa: transition certainty weight in [0,1] (higher -> more confident in common transitions)
        - epsilon: baseline epsilon-greedy exploration in [0,1]
        - w2: scaling of alpha for second-stage learning in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """"""
    alpha, beta, kappa, epsilon, w2 = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Anxiety reduces certainty about transitions: p_common pulls toward 0.5 as stai increases
    # Base common prob centered at 0.7, range toward 0.5 controlled by kappa and stai
    # p_common = 0.5 + (0.7 - 0.5) * (1 - stai) * (0.5 + 0.5 * kappa)
    p_common = 0.5 + 0.2 * (1.0 - stai) * (0.5 + 0.5 * kappa)
    p_common = float(np.clip(p_common, 0.5, 0.9))

    transition_matrix = np.array([[p_common, 1.0 - p_common],
                                  [1.0 - p_common, p_common]])

    # Epsilon-greedy level increases with anxiety
    eps_eff = float(np.clip(epsilon * (0.5 + stai), 0.0, 1.0))

    # Second-stage learning rate scaled by w2 and anxiety
    alpha2 = np.clip(alpha * (0.5 + 0.5 * w2) * (0.5 + 0.5 * stai), 0.0, 1.0)

    # Values
    q2 = np.zeros((2, 2))  # state-action values at stage 2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation for stage 1 (expectimax over Q2)
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Softmax for stage 1 then apply epsilon-soft mixing
        logits1 = beta * (q1_mb - np.max(q1_mb))
        sm1 = np.exp(logits1)
        sm1 /= np.sum(sm1)
        probs_1 = (1.0 - eps_eff) * sm1 + eps_eff * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy for reached state
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        sm2 = np.exp(logits2)
        sm2 /= np.sum(sm2)
        probs_2 = (1.0 - eps_eff) * sm2 + eps_eff * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Update second-stage Q-values
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Note: No model-free Q1 learning; planning updates Q1 implicitly via q2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik","['alpha', 'beta', 'kappa', 'epsilon', 'w2']"
iter0_run0_participant33.json,cognitive_model1,396.8598699598498,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated arbitration, eligibility trace, and perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0/1)
        Second-stage state reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0/1)
        Second-stage choices (0/1) on each trial (e.g., W vs S on X; P vs H on Y).
    reward : array-like of float
        Reward received on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Trait anxiety score; use stai[0]. Higher means higher anxiety.
    model_parameters : tuple/list of floats
        (alpha, beta, w_base, lambda_e, persev)
        - alpha in [0,1]: learning rate for value updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w_base in [0,1]: baseline weight on model-based control at stage 1.
        - lambda_e in [0,1]: eligibility trace for backpropagating reward to stage 1 MF.
        - persev in [0,1]: perseveration strength added to the previously chosen first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, w_base, lambda_e, persev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known (fixed) transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage2 = np.zeros((2, 2))     # Q at second stage: state x action
    q_stage1_mf = np.zeros(2)       # Model-free first-stage values

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    prev_a1 = None

    # Anxiety-modulated arbitration: higher anxiety reduces model-based weight
    # w in [0,1]; at stai=1, reduce w_base by 70%
    w = w_base * (1.0 - 0.7 * stai)
    w = max(0.0, min(1.0, w))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values via Bellman backup from stage 2
        mb_values = transition_matrix @ np.max(q_stage2, axis=1)  # shape (2,)

        # Perseveration bias (added to preferences)
        persev_eff = persev * (1.0 + 0.5 * stai)
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += persev_eff

        # Hybrid action values for stage 1
        q1 = w * mb_values + (1.0 - w) * q_stage1_mf + bias

        # Stage-1 policy and likelihood
        q1_shift = q1 - np.max(q1)  # numerical stability
        exp_q1 = np.exp(beta * q1_shift)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy and likelihood
        q2 = q_stage2[s, :].copy()
        q2_shift = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_shift)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        # Second stage TD
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # First stage MF TD toward realized stage-2 value
        delta1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Eligibility trace backprop of reward prediction error to stage 1 MF
        q_stage1_mf[a1] += alpha * lambda_e * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'w_base', 'lambda_e', 'persev']"
iter0_run0_participant34.json,cognitive_model1,403.9299295004612,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-weighted arbitration and eligibility trace.
    
    The agent learns second-stage action values and combines a model-based (MB)
    evaluation of first-stage actions with a model-free (MF) first-stage value.
    Anxiety (STAI) shifts the arbitration weight away from MB control.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial, indexed within the reached state.
    reward : array-like of float
        Reward obtained on each trial (e.g., coins).
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, lam, w0, w_stai]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature (softmax) for both stages.
        - lam in [0,1]: eligibility trace strength propagating reward to first-stage MF value.
        - w0 in [0,1]: baseline MB weight in arbitration.
        - w_stai in [0,1]: linear scaling of how anxiety shifts arbitration
                           (effective MB weight is clipped to [0,1]).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, lam, w0, w_stai = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))  # Q2[state, action]

    for t in range(n_trials):
        # Model-based first-stage action values: expect max Q2 under transitions
        max_q2 = np.max(q_stage2, axis=1)             # shape (2,)
        q_stage1_mb = transition_matrix @ max_q2      # shape (2,)

        # Anxiety-weighted arbitration (higher anxiety -> lower MB weight if w_stai>0)
        w_mb = w0 + w_stai * (0.5 - stai_val)
        w_mb = min(1.0, max(0.0, w_mb))

        # Hybrid first-stage Q
        q1_hybrid = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # First-stage policy
        q1 = q1_hybrid
        q1 = q1 - np.max(q1)
        probs1 = np.exp(beta * q1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        q2_logits = q_stage2[s].copy()
        q2_logits = q2_logits - np.max(q2_logits)
        probs2 = np.exp(beta * q2_logits)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace
        # Bootstrapped update towards Q2 plus additional reward-based eligibility
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + lam * alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'lam', 'w0', 'w_stai']"
iter0_run0_participant34.json,cognitive_model2,380.59447527844577,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pure model-free with anxiety-modulated exploration and first-stage stickiness.
    
    The agent learns model-free action values at both stages. Anxiety reduces the
    effective inverse temperature (more anxious -> more random choices).
    A first-stage choice stickiness bias captures perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha1, beta, k_stai_beta, phi, alpha2]
        - alpha1 in [0,1]: learning rate for first-stage MF value.
        - beta in [0,10]: base inverse temperature.
        - k_stai_beta in [0,1]: scales how much anxiety reduces beta (beta_eff = beta*(1 - k*stai)).
        - phi in [0,1]: first-stage stickiness weight added to the previously chosen action.
        - alpha2 in [0,1]: second-stage learning rate.
        
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha1, beta, k_stai_beta, phi, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective inverse temperature reduced by anxiety
    beta_eff = beta * (1.0 - k_stai_beta * stai_val)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    prev_a1 = None

    for t in range(n_trials):
        # First-stage policy with stickiness
        logits1 = q_stage1.copy()
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            logits1 = logits1 + phi * stick
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta_eff * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (no stickiness)
        s = state[t]
        logits2 = q_stage2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta_eff * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning
        # Stage-2 update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Stage-1 bootstrapped MF update towards the obtained second-stage value
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1[a1]
        q_stage1[a1] += alpha1 * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha1', 'beta', 'k_stai_beta', 'phi', 'alpha2']"
iter0_run0_participant34.json,cognitive_model3,395.376178194364,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid with learned transitions and anxiety-sensitive surprise arbitration.
    
    The agent learns the transition model online and uses it to compute model-based
    first-stage values. Arbitration between MB and MF control is reduced following
    surprising transitions, especially under higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, alpha_t, w0, k_surprise]
        - alpha in [0,1]: learning rate for MF values at both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_t in [0,1]: learning rate for the transition matrix.
        - w0 in [0,1]: baseline MB arbitration weight.
        - k_surprise in [0,1]: scales how much anxiety amplifies surprise-induced reduction
                               of MB control (effective reduction â k_surprise*stai*surprise).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, alpha_t, w0, k_surprise = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transitions T[action, state], rows sum to 1
    T = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Surprise from previous trial (start neutral)
    prev_surprise = 0.0

    for t in range(n_trials):
        # Model-based valuation from currently learned transitions
        max_q2 = np.max(q_stage2, axis=1)     # (2,)
        q_stage1_mb = T @ max_q2              # (2,)

        # Arbitration weight reduced by previous surprise scaled by anxiety
        # w_mb = clip(w0 - k_surprise * stai * prev_surprise, 0, 1)
        w_mb = w0 - k_surprise * stai_val * prev_surprise
        w_mb = min(1.0, max(0.0, w_mb))

        q1_hybrid = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # First-stage policy
        logits1 = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = q_stage2[s] - np.max(q_stage2[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Compute transition surprise BEFORE updating T
        prob_trans = T[a1, s]
        eps = 1e-12
        prev_surprise = -np.log(prob_trans + eps)

        # Learning: transitions
        # Target is one-hot for observed state
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T[a1, s_idx] += alpha_t * (target - T[a1, s_idx])
        # Re-normalize to guard against numerical drift
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, 0] /= row_sum
            T[a1, 1] /= row_sum

        # Learning: values
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 MF update (bootstrapped + full reward eligibility)
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'alpha_t', 'w0', 'k_surprise']"
iter0_run0_participant35.json,cognitive_model1,434.44902006481516,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with eligibility trace and anxiety-weighted arbitration.

    The model combines a model-based (MB) planner that uses the known transition matrix
    with a model-free (MF) learner at the first stage. The arbitration weight placed on
    the MB component decreases with anxiety (stai). Second-stage values are learned via
    TD learning; first-stage MF values receive an eligibility-trace update from the
    second-stage TD error.

    Inputs
    - action_1: array of ints in {0,1}, first-stage choice (0=A, 1=U)
    - state: array of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array of ints in {0,1}, second-stage choice on the reached state
    - reward: array of floats (e.g., 0/1), reward received on each trial
    - stai: array-like of length 1; scalar anxiety score in [0,1]
    - model_parameters: tuple/list of 5 parameters:
        learning_rate: alpha in [0,1] for both stages
        beta1: inverse temperature for first-stage softmax in [0,10]
        beta2: inverse temperature for second-stage softmax in [0,10]
        w_base: baseline MB weight in [0,1]
        lam: eligibility trace parameter in [0,1]

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.

    Notes on anxiety usage:
    - Effective MB weight at stage 1 is w_mb = w_base * (1 - stai), so higher anxiety
      shifts arbitration toward model-free control.
    """"""
    alpha, beta1, beta2, w_base, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition matrix: rows are actions (A=0, U=1), columns are next states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of the chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize value functions
    q_stage1_mf = np.zeros(2)        # MF Q-values for first stage actions
    q_stage2_mf = np.zeros((2, 2))   # MF Q-values for second stage actions per state

    # Anxiety-modulated arbitration
    w_mb = max(0.0, min(1.0, w_base * (1.0 - stai)))  # clamp to [0,1]

    for t in range(n_trials):
        # Model-based first-stage Q: expectation over next-state max Q2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)         # size 2 (states)
        q_stage1_mb = transition_matrix @ max_q_stage2     # size 2 (actions)

        # Hybrid first-stage Q
        q1 = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # First-stage policy
        q1_centered = q1 - np.max(q1)  # numerical stability
        exp_q1 = np.exp(beta1 * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in observed state
        s = int(state[t])
        q2 = q_stage2_mf[s]
        q2_centered = q2 - np.max(q2)
        exp_q2 = np.exp(beta2 * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # TD updates
        r = reward[t]

        # Second-stage update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # First-stage MF update with eligibility trace from second stage
        # Immediate TD term plus bootstrapped credit from the outcome
        # delta1 uses the pre-update Q2 value (as in SARSA-like propagation)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + alpha * lam * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta1', 'beta2', 'w_base', 'lam']"
iter0_run0_participant36.json,cognitive_model1,550.9659583107785,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated arbitration and lapse.
    
    This model maintains second-stage Q-values (Q2) and a model-free first-stage Q (Q1_mf).
    A model-based first-stage value (Q1_mb) is computed via the known transition model.
    Anxiety (stai) down-weights the model-based contribution (more anxiety -> more model-free).
    A small lapse mixes softmax choice with uniform randomness.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., alien index within the planet).
    reward : array-like of float (e.g., 0.0 or 1.0)
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s). We take stai[0] as the participant's score in [0,1] scale.
        Higher stai reduces the model-based arbitration weight and increases the effective lapse impact.
    model_parameters : list or array
        [alpha, beta, w_base, lambda_elig, xi]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax choice.
        - w_base in [0,1]: baseline model-based weight at stai=0.
        - lambda_elig in [0,1]: eligibility trace strength for backing up Q1 from Q2.
        - xi in [0,1]: lapse rate mixing softmax with uniform choice at stage 1.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed sequence of choices under the model.
    """"""
    alpha, beta, w_base, lambda_elig, xi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed, known transition structure: A->X and U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q1_mf = np.zeros(2)           # model-free first-stage values
    Q2 = np.zeros((2, 2))         # second-stage values: state x action

    # Anxiety-modulated arbitration and lapse
    # High anxiety reduces the model-based contribution and slightly boosts the lapse effect.
    w_eff_scale = max(0.0, 1.0 - stai)  # bounded scaling
    xi_eff = np.clip(xi * (1.0 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based first-stage Q from current second-stage values via known transitions
        max_Q2 = np.max(Q2, axis=1)              # best action at each state
        Q1_mb = transition_matrix @ max_Q2       # expected value for each first-stage action

        # Anxiety-adjusted arbitration weight
        w = np.clip(w_base * w_eff_scale, 0.0, 1.0)

        # Combine MB and MF for first-stage decision
        Q1 = (1.0 - w) * Q1_mf + w * Q1_mb

        # First-stage policy with lapse
        logits1 = beta * Q1
        logits1 = logits1 - np.max(logits1)  # numerical stability
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - xi_eff) * probs1 + xi_eff * 0.5  # mix with uniform over 2 actions
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * Q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage 2 TD update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage 1 TD(Î») update towards current second-stage value of chosen branch
        # Back up the chosen second-stage action value into first-stage chosen action
        target1 = Q2[s, a2]
        delta1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * lambda_elig * delta1

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)

","['alpha', 'beta', 'w_base', 'lambda_elig', 'xi']"
iter0_run0_participant37.json,cognitive_model1,465.17757567233025,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-modulated arbitration and eligibility.
    
    This model blends model-based (MB) and model-free (MF) values at stage 1.
    The blend weight is modulated by anxiety (stai). A TD(Î») style eligibility trace
    propagates reward prediction errors from stage 2 to stage 1.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0: planet X, 1: planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1: alien on the current planet) for each trial.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; used to modulate arbitration weight.
    model_parameters : iterable of floats
        [alpha, beta, lambda_, w_base, w_stai]
        - alpha in [0,1]: learning rate for MF values.
        - beta in [0,10]: inverse temperature for both stages.
        - lambda_ in [0,1]: eligibility trace strength from stage 2 to stage 1.
        - w_base in [0,1]: baseline MB weight at stage 1.
        - w_stai in [0,1]: how much anxiety shifts MB weight (positive -> more MB with higher stai).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, lambda_, w_base, w_stai = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    # Storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)          # MF values for spaceships [A,U]
    q_stage2_mf = np.zeros((2, 2))     # MF values for aliens by planet: [X,Y] x [alien0, alien1]

    # Anxiety-modulated MB weight (clipped to [0,1])
    weight_mb = w_base + w_stai * (stai - 0.51)
    weight_mb = min(1.0, max(0.0, weight_mb))

    for t in range(n_trials):
        # MODEL-BASED stage-1 values: expected max value after transition
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [V(X), V(Y)]
        q_stage1_mb = transition_matrix @ max_q_stage2  # [Q_MB(A), Q_MB(U)]

        # Hybrid stage-1 values
        q1 = weight_mb * q_stage1_mb + (1.0 - weight_mb) * q_stage1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy for the reached state
        s = state[t]
        q2 = q_stage2_mf[s, :]
        logits2 = beta * q2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace from stage-2
        delta1_bootstrap = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1_bootstrap + lambda_ * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lambda_', 'w_base', 'w_stai']"
iter0_run0_participant37.json,cognitive_model2,455.73816552787235,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-modulated transition learning and stickiness.
    
    Participants learn the first-stage transition probabilities over time. Anxiety (stai)
    increases assumed volatility: higher stai -> larger transition learning rate. A
    perseveration (stickiness) bias at stage 1 is also scaled by anxiety. Hybrid MB/MF
    control is combined with an anxiety-determined MB weight (no extra parameter).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage action on the reached planet.
    reward : array-like of float
        Coins received each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; used to modulate transition learning and stickiness.
    model_parameters : iterable of floats
        [alpha, beta, alpha_T0, alpha_Tgain, kappa]
        - alpha in [0,1]: reward learning rate for MF Q values.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_T0 in [0,1]: baseline transition learning rate.
        - alpha_Tgain in [0,1]: how much anxiety increases transition learning rate.
        - kappa in [0,1]: baseline perseveration strength (adds bias toward last chosen ship).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, alpha_T0, alpha_Tgain, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T[a, s]
    T = np.full((2, 2), 0.5)  # start uncertain

    # MF values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate
    alpha_T = alpha_T0 + alpha_Tgain * stai
    alpha_T = min(1.0, max(0.0, alpha_T))

    # Anxiety-modulated perseveration magnitude
    stick = kappa * (1.0 + 0.5 * (stai - 0.51))

    # Anxiety-determined MB weight (no extra parameter): higher anxiety -> less MB
    weight_mb = 0.5 + 0.4 * (0.51 - stai)
    weight_mb = min(1.0, max(0.0, weight_mb))

    prev_a1 = -1  # no previous choice initially

    for t in range(n_trials):
        # MB values from learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [V(X), V(Y)]
        q_stage1_mb = T @ max_q_stage2

        # Hybrid stage-1 values
        q1 = weight_mb * q_stage1_mb + (1.0 - weight_mb) * q_stage1_mf

        # Add perseveration bias toward previous choice
        bias = np.zeros(2)
        if prev_a1 >= 0:
            bias[prev_a1] = stick

        logits1 = beta * q1 + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        q2 = q_stage2_mf[s, :]
        logits2 = beta * q2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update learned transitions for the chosen first-stage action
        # Move probability mass toward the observed state
        # T[a, s_obs] += alpha_T*(1 - T[a, s_obs]); T[a, s_other] = 1 - T[a, s_obs]
        T[a1, s] += alpha_T * (1.0 - T[a1, s])
        T[a1, 1 - s] = 1.0 - T[a1, s]

        # MF learning
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Eligibility trace from stage 2 to stage 1: strength = stai (uses anxiety without extra param)
        lam = stai
        delta1_bootstrap = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1_bootstrap + lam * delta2)

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'alpha_T0', 'alpha_Tgain', 'kappa']"
iter0_run0_participant38.json,cognitive_model1,434.478698622927,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MBâMF with anxiety-dependent arbitration and MF eligibility.
    This model blends model-based (MB) and model-free (MF) action values at stage 1,
    with the arbitration weight decreasing as anxiety (stai) increases. Stage-2 values
    are learned via TD; stage-1 MF values use an eligibility trace from the stage-2 TD error.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (two aliens available on each planet).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Participant anxiety score. Higher values indicate higher anxiety.
    model_parameters : iterable of floats
        [alpha, lambd, beta, base_w, anx_gain]
        - alpha: [0,1] learning rate for TD updates at both stages.
        - lambd: [0,1] eligibility trace scaling MF credit from stage-2 back to stage-1.
        - beta: [0,10] inverse temperature for softmax choice at both stages.
        - base_w: [0,1] baseline MB weight in the hybrid value at stage 1.
        - anx_gain: [0,1] how strongly anxiety reduces MB arbitration weight.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, lambd, beta, base_w, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: rows = first-stage actions (A=0, U=1); cols = states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize MF action values
    q1_mf = np.zeros(2)          # stage-1 MF Q for A/U
    q2 = np.zeros((2, 2))        # stage-2 MF Q for each state (X/Y) and action (two aliens)

    for t in range(n_trials):
        # Model-based stage-1 values from current stage-2 MF values
        max_q2 = np.max(q2, axis=1)             # value of best alien on each planet
        q1_mb = transition_matrix @ max_q2      # MB forward planning

        # Anxiety-dependent arbitration weight: higher anxiety -> lower MB weight
        w = base_w * (1.0 - anx_gain) + (1.0 - stai) * anx_gain
        w = min(max(w, 0.0), 1.0)

        # Hybrid stage-1 values
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy and likelihood of observed choice
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy given observed state
        s = state[t]
        exp_q2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # TD updates
        r = reward[t]

        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update: SARSA(Î»)-style credit from stage-2 back to chosen stage-1 action
        # Immediate consistency term plus eligibility trace on reward
        # First a consistency update towards q2 value at visited state-action
        delta1_consistency = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1_consistency
        # Then eligibility credit from reward
        q1_mf[a1] += alpha * lambd * delta2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood

","['alpha', 'lambd', 'beta', 'base_w', 'anx_gain']"
iter0_run0_participant38.json,cognitive_model2,439.56627336185113,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-modulated exploration with anxiety-damped inverse temperature.
    Model-based values drive stage-1 choices; stage-2 values are learned with TD.
    Exploration temperature is reduced by both estimated uncertainty and anxiety.
    Includes choice stickiness at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (two aliens available per state).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score. Higher values increase temperature damping.
    model_parameters : iterable of floats
        [alpha, beta, kappa_u, anx_temp, stickiness]
        - alpha: [0,1] learning rate for updating stage-2 values.
        - beta: [0,10] base inverse temperature.
        - kappa_u: [0,1] uncertainty learning rate (EWMA of squared TD error).
        - anx_temp: [0,1] scales how much anxiety reduces effective beta.
        - stickiness: [0,1] choice perseveration weight added to previously chosen action.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, kappa_u, anx_temp, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 value estimates and uncertainty (per state-action)
    q2 = np.zeros((2, 2))
    unc2 = np.zeros((2, 2))  # running estimate of variance via EWMA of squared TD error

    # For stickiness
    last_a1 = None
    last_a2 = [None, None]  # per state

    for t in range(n_trials):
        # Uncertainty-weighted inverse temperature factor for stage 1:
        # use the expected uncertainty under the transition model, using max-uncertain action per state
        max_q2 = np.max(q2, axis=1)
        # approximate relevant uncertainty for each next-state as uncertainty of its greedy action
        greedy_idx = np.argmax(q2, axis=1)
        unc_next = np.array([unc2[0, greedy_idx[0]], unc2[1, greedy_idx[1]]])
        exp_unc_stage1 = transition_matrix @ unc_next
        # anxiety reduces beta; higher uncertainty further reduces it
        beta1_eff = beta * (1.0 - anx_temp * stai) / (1.0 + np.mean(exp_unc_stage1))

        # MB stage-1 values
        q1_mb = transition_matrix @ max_q2

        # Add stickiness bias to the previously chosen first-stage action
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stickiness

        # Stage-1 policy
        logits1 = beta1_eff * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy given observed state, with uncertainty- and anxiety-damped beta
        s = state[t]
        # local uncertainty for current state based on its greedy action's uncertainty
        g_idx = int(np.argmax(q2[s]))
        local_unc = unc2[s, g_idx]
        beta2_eff = beta * (1.0 - anx_temp * stai) / (1.0 + local_unc)

        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += stickiness

        logits2 = beta2_eff * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observed reward
        r = reward[t]

        # Stage-2 TD error and updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update uncertainty as EWMA of squared TD error
        unc2[s, a2] = (1.0 - kappa_u) * unc2[s, a2] + kappa_u * (delta2 ** 2)

        # Update trackers for stickiness
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood

","['alpha', 'beta', 'kappa_u', 'anx_temp', 'stickiness']"
iter0_run0_participant39.json,cognitive_model1,357.9728385487879,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated planning weight and stickiness.
    
    The agent learns second-stage action values model-free and forms first-stage
    model-based values via the transition model. First-stage choice uses a hybrid
    mix of model-based and model-free values plus a perseveration bias. Anxiety
    (stai) reduces planning weight and increases perseveration strength.

    Parameters
    - action_1: np.array of shape (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state: np.array of shape (n_trials,), second-stage state indices (0=planet X, 1=planet Y)
    - action_2: np.array of shape (n_trials,), second-stage actions (0/1; e.g., alien choices)
    - reward: np.array of shape (n_trials,), outcomes (e.g., coins; scaled 0/1)
    - stai: np.array of shape (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha1: [0,1] learning rate for stage-1 MF values
        alpha2: [0,1] learning rate for stage-2 MF values
        beta:   [0,10] inverse temperature for both stages
        w_mb_base: [0,1] baseline weight on model-based control (before anxiety)
        kappa: [0,1] perseveration strength at stage 1 (bias toward previous a1)
    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha1, alpha2, beta, w_mb_base, kappa = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])  # trait

    # Transition structure: rows = actions (A,U), cols = states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage1_mf = np.zeros(2)          # MF values for A/U
    q_stage2_mf = np.zeros((2, 2))     # MF values for aliens within each planet

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated control weight and perseveration strength
    # Higher anxiety -> less planning weight, more perseveration
    w_mb = np.clip(w_mb_base * (1.0 - stai0), 0.0, 1.0)
    kappa_eff = kappa * (1.0 + stai0)

    prev_a1 = None  # for stickiness

    eps = 1e-10
    for t in range(n_trials):

        # Compute model-based Q at stage 1 from current MF Q at stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # size 2 (states)
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value per action

        # Hybrid Q for stage 1
        q1_hybrid = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # Add perseveration bias toward previous first-stage action
        if prev_a1 is not None:
            stickiness = np.zeros(2)
            stickiness[prev_a1] = 1.0
            q1_hybrid = q1_hybrid + kappa_eff * stickiness

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at realized state
        s2 = state[t]
        q2 = q_stage2_mf[s2]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha2 * pe2

        # Stage-1 MF update via eligibility from stage-2 chosen value (no separate lambda param)
        # TD target uses realized second-stage chosen value
        td_target1 = q_stage2_mf[s2, a2]
        pe1 = td_target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha1 * pe1

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha1', 'alpha2', 'beta', 'w_mb_base', 'kappa']"
iter0_run0_participant39.json,cognitive_model2,371.98209581068437,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free RL with anxiety-modulated associability and eligibility trace.
    
    The agent learns values using a PearceâHall-like associability: the learning
    rate increases with the magnitude of recent prediction errors. Anxiety
    amplifies associability (greater sensitivity to surprising outcomes) and
    reduces credit assignment across stages via a lower eligibility trace.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0/1)
    - state: np.array (n_trials,), second-stage state (0/1)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward: np.array (n_trials,), outcomes (0/1)
    - stai: np.array (1,) or (n_trials,), anxiety in [0,1]
    - model_parameters: iterable of 4 parameters
        alpha_base: [0,1] baseline learning rate
        beta: [0,10] inverse temperature for both stages
        lam_base: [0,1] baseline eligibility trace for backing up to stage 1
        persev: [0,1] perseveration bias strength (applied to both stages)
    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha_base, beta, lam_base, persev = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Model-free values
    q1 = np.zeros(2)         # stage-1 MF values
    q2 = np.zeros((2, 2))    # stage-2 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated eligibility and perseveration
    # Higher anxiety -> lower cross-stage credit assignment, stronger perseveration
    lam = np.clip(lam_base * (1.0 - 0.7 * stai0), 0.0, 1.0)
    persev_eff = persev * (1.0 + stai0)

    # Keep track of previous actions for perseveration
    prev_a1 = None
    prev_a2 = np.array([None, None])  # per state

    eps = 1e-10
    # Running associability scalar (per action/state-action); start at alpha_base
    assoc1 = np.ones(2) * alpha_base
    assoc2 = np.ones((2, 2)) * alpha_base

    for t in range(n_trials):
        s = state[t]

        # Stage-1 policy with perseveration
        q1_bias = q1.copy()
        if prev_a1 is not None:
            q1_bias[prev_a1] += persev_eff

        exp_q1 = np.exp(beta * (q1_bias - np.max(q1_bias)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with within-state perseveration
        q2_bias = q2[s].copy()
        if prev_a2[s] is not None:
            q2_bias[prev_a2[s]] += persev_eff

        exp_q2 = np.exp(beta * (q2_bias - np.max(q2_bias)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 update with PearceâHall associability amplified by anxiety
        pe2 = r - q2[s, a2]
        # associability grows with surprise; anxiety amplifies this growth
        assoc2[s, a2] = np.clip((1 - alpha_base) * assoc2[s, a2] + (alpha_base * (1 + stai0)) * abs(pe2), 0.0, 1.0)
        alpha2_eff = np.clip(alpha_base * (0.5 + assoc2[s, a2]), 0.0, 1.0)
        q2[s, a2] += alpha2_eff * pe2

        # Stage-1 update via eligibility from stage-2 PE
        pe1 = q2[s, a2] - q1[a1]
        # associability for stage 1 also increases with surprise at stage 2
        assoc1[a1] = np.clip((1 - alpha_base) * assoc1[a1] + (alpha_base * (1 + stai0)) * abs(pe1), 0.0, 1.0)
        alpha1_eff = np.clip(alpha_base * (0.5 + assoc1[a1]), 0.0, 1.0)
        q1[a1] += (lam * alpha1_eff) * pe1

        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_base', 'beta', 'lam_base', 'persev']"
iter0_run0_participant39.json,cognitive_model3,480.7783032769023,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid RL with anxiety-modulated transition credit assignment bias.
    
    The agent blends model-based and model-free control at stage 1. Credit
    assignment from stage 2 to stage 1 depends on whether the transition was
    common or rare: anxiety increases the impact of rare transitions on MF
    backups (and reduces it for common transitions), capturing altered learning
    from surprising events.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0/1; A/U)
    - state: np.array (n_trials,), second-stage state (0/1; X/Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward: np.array (n_trials,), outcomes (0/1)
    - stai: np.array (1,) or (n_trials,), anxiety in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha: [0,1] learning rate for MF updates (both stages)
        beta: [0,10] inverse temperature
        w_mb_base: [0,1] baseline weight on model-based control
        lam_base: [0,1] base eligibility for MF backup to stage 1
        b_common: [0,1] bias toward actions whose common destination looks valuable
    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha, beta, w_mb_base, lam_base, b_common = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations:
    # - Reduce reliance on MB control
    w_mb = np.clip(w_mb_base * (1.0 - 0.5 * stai0), 0.0, 1.0)
    # - Adjust eligibility differently for common vs rare transitions
    #   Anxiety increases backup after rare transitions and decreases after common ones
    lam_common = np.clip(lam_base * (1.0 - 0.7 * stai0), 0.0, 1.0)
    lam_rare = np.clip(lam_base * (1.0 + 0.7 * stai0), 0.0, 1.0)
    # - Value-guided bias toward actions whose common destination is currently valuable;
    #   bias decreases with anxiety (less reliance on this heuristic)
    b_eff = b_common * (1.0 - stai0)

    eps = 1e-10
    for t in range(n_trials):
        # Model-based Q from current MF second-stage values
        max_q2 = np.max(q2_mf, axis=1)  # per state
        q1_mb = transition_matrix @ max_q2

        # Value-guided bias: add b_eff to the action whose common planet currently has higher value
        # Common planet for action 0 is state 0; for action 1 is state 1
        bias_vec = np.zeros(2)
        if max_q2[0] > max_q2[1]:
            bias_vec[0] += b_eff
        elif max_q2[1] > max_q2[0]:
            bias_vec[1] += b_eff
        # If equal, no bias added

        # Hybrid Q
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias_vec

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        exp_q2 = np.exp(beta * (q2_mf[s] - np.max(q2_mf[s])))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 MF learning
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Determine transition type (common if action index equals state index here)
        # Action 0 commonly -> state 0; action 1 commonly -> state 1
        is_common = 1 if (a1 == s) else 0
        lam = lam_common if is_common else lam_rare

        # Stage-1 MF update with transition-contingent eligibility
        td_target1 = q2_mf[s, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += (lam * alpha) * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'w_mb_base', 'lam_base', 'b_common']"
iter0_run0_participant4.json,cognitive_model1,446.4415061151748,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-weighted perseveration (stickiness).
    
    This model blends model-based (MB) and model-free (MF) action values at stage 1.
    The MF system uses an eligibility trace to propagate stage-2 prediction errors
    back to stage 1. Anxiety (stai) scales an action perseveration bias that adds
    to the softmax logits at both stages, capturing increased habit/stickiness
    with higher anxiety. Stage-2 learning is simple Rescorla-Wagner.
    
    Inputs
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, observed planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial (planet-specific)
    - reward: array of floats in [0,1], obtained coins per trial
    - stai: array of length 1 with the participant's anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters:
        learning_rate (alpha): [0,1]         -- value learning rate
        beta: [0,10]                          -- inverse temperature for softmax
        lambda_elig (lam): [0,1]              -- eligibility trace for MF credit assignment
        w_mb: [0,1]                           -- weight on MB values at stage 1
        kappa_stick: [0,1]                    -- base perseveration strength; scaled by stai
    
    Returns
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, lam, w_mb, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure (fixed, known): A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to X/Y
                                  [0.3, 0.7]]) # from U to X/Y

    # Initialize value functions
    q1_mf = np.zeros(2)        # MF Q-values at stage 1 (spaceships)
    q2 = np.zeros((2, 2))      # stage-2 Q-values for each planet and alien

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration terms (previous chosen actions), initialized to zero
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)  # one per planet

    # Anxiety-weighted stickiness effective strength
    kappa_eff = kappa * stai

    for t in range(n_trials):
        s = state[t]

        # Model-based action values at stage 1: expect max Q2 on each planet via transitions
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = transition_matrix @ max_q2  # shape (2,)

        # Hybrid stage-1 values
        q1_hyb = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Add perseveration biases to logits (both options get +kappa if they match prev choice)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_eff

        # Stage-1 policy
        logits1 = beta * q1_hyb + bias1
        logits1 -= np.max(logits1)  # numerical stability
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in the reached state, with perseveration on that planet
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa_eff
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning: Stage-2 TD
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # MF backpropagation to stage 1 with eligibility trace
        q1_mf[a1] += alpha * lam * delta2

        # Update perseveration memories
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'lam', 'w_mb', 'kappa']"
iter0_run0_participant4.json,cognitive_model2,452.74893361072577,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-gated learning with anxiety-modulated gain and reduced stickiness.
    
    This model uses a dynamic learning rate driven by surprise (unsigned prediction error)
    tracked with a Pearce-Hallâlike running average. Anxiety (stai) increases the impact
    of surprise on the learning rate, capturing heightened sensitivity to unexpected
    outcomes with higher anxiety. Perseveration is present but attenuated by anxiety.
    Stage-1 values are hybrid MB/MF (with fixed 0.5 mixing).
    
    Inputs
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, observed planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial (planet-specific)
    - reward: array of floats in [0,1], obtained coins per trial
    - stai: array of length 1 with the participant's anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters:
        alpha0: [0,1]               -- base learning rate
        beta: [0,10]                -- inverse temperature
        eta_surprise: [0,1]         -- update rate for running surprise
        lambda_elig (lam): [0,1]    -- eligibility trace to stage 1 for MF
        rho_stick: [0,1]            -- base perseveration strength (reduced by anxiety)
    
    Returns
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha0, beta, eta_surprise, lam, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure for MB planning
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness attenuated by anxiety: higher stai -> lower perseveration
    rho_eff = rho * (1.0 - stai)

    # Running surprise (unsigned PE), initialize moderate
    S = 0.5

    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        s = state[t]

        # MB values via transitions to max second-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid values with fixed mixing 0.5
        q1_hyb = 0.5 * q1_mf + 0.5 * q1_mb

        # Perseveration biases (reduced when anxiety is high)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_eff

        logits1 = beta * q1_hyb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += rho_eff
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Stage-2 PE and dynamic (anxiety-weighted) learning rate
        delta2 = r - q2[s, a2]
        # Update running surprise S in [0,1]
        S = (1.0 - eta_surprise) * S + eta_surprise * abs(delta2)
        # Anxiety gates learning-rate amplification by surprise
        alpha_t = alpha0 * ((1.0 - stai) + stai * S)
        # Clip for safety
        if alpha_t < 0.0:
            alpha_t = 0.0
        if alpha_t > 1.0:
            alpha_t = 1.0

        # Learn stage 2
        q2[s, a2] += alpha_t * delta2

        # Eligibility-trace credit to stage 1 MF
        q1_mf[a1] += alpha_t * lam * delta2

        # Update prev choices
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha0', 'beta', 'eta_surprise', 'lam', 'rho']"
iter0_run0_participant4.json,cognitive_model3,537.6227816510446,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Structure learning of transitions with anxiety-biased arbitration.
    
    This model learns the transition matrix online and uses it for model-based planning.
    Arbitration between MB and MF at stage 1 depends on anxiety: higher anxiety reduces
    reliance on MB (more MF). The initial transition prior is also anxiety-biased:
    low anxiety assumes stronger common transitions (closer to 0.7), high anxiety starts
    more uncertain (closer to 0.5). MF uses an eligibility trace from stage 2 PE.
    
    Inputs
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, observed planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial (planet-specific)
    - reward: array of floats in [0,1], obtained coins per trial
    - stai: array of length 1 with the participant's anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters:
        alphaQ: [0,1]             -- learning rate for Q-values
        beta: [0,10]              -- inverse temperature
        lambda_elig (lam): [0,1]  -- eligibility trace for MF
        w0: [0,1]                 -- baseline MB weight at stage 1 (before anxiety)
        alphaT: [0,1]             -- learning rate for transition probabilities
    
    Returns
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alphaQ, beta, lam, w0, alphaT = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-biased initial transition prior:
    # common prob starts at 0.5 + 0.2*(1 - stai) in [0.5, 0.7]
    p_common_init = 0.5 + 0.2 * (1.0 - stai)
    T = np.array([[p_common_init, 1.0 - p_common_init],   # for A
                  [1.0 - p_common_init, p_common_init]])  # for U

    # Arbitration weight: high anxiety -> more MF (lower MB weight)
    w_mb = np.clip(w0 * (1.0 - stai), 0.0, 1.0)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]

        # MB action values via learned transition matrix
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid stage-1 values
        q1_hyb = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 choice
        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice in observed state
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Stage-2 PE and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alphaQ * delta2

        # MF backprop to stage 1
        q1_mf[a1] += alphaQ * lam * delta2

        # Learn transition matrix row for chosen action a1 from observed state s
        # One-hot target: P(s | a1) = 1 for observed s, 0 for other
        # Update both columns to keep row stochastic
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alphaT * (target - T[a1, sp])
        # Ensure numerical stability of probabilities
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alphaQ', 'beta', 'lam', 'w0', 'alphaT']"
iter0_run0_participant40.json,cognitive_model1,458.54485481508306,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-modulated control and stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha, beta, w, lam, kappa)
        - alpha in [0,1]: learning rate for model-free values at both stages.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: baseline weight on model-based control.
        - lam in [0,1]: eligibility trace to backpropagate value to stage 1.
        - kappa in [0,1]: first-stage choice stickiness strength.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    
    Notes
    -----
    - Anxiety (stai) reduces model-based control: w_eff = w * (1 - stai).
    - Transition structure is assumed known (common: 0.7).
    - Stickiness adds a bias toward repeating the previous first-stage choice.
    """"""
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: rows = actions (A,U), cols = states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize Q-values
    q_stage1_mf = np.zeros(2)         # model-free action values at stage 1
    q_stage2_mf = np.zeros((2, 2))    # model-free values at stage 2: Q[state, action]

    # Anxiety-modulated MB weight
    w_eff = w * (1.0 - stai)

    prev_a1 = None  # for stickiness

    for t in range(n_trials):
        # Compute model-based stage-1 values from stage-2 MF values (myopic MB)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)             # best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2          # MB projection
        # Combine MB and MF with stickiness
        q1_combined = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add first-stage stickiness bias to logits
        stickiness_bias = np.zeros(2)
        if prev_a1 is not None:
            stickiness_bias[prev_a1] = kappa
        logits1 = beta * q1_combined + stickiness_bias
        exp_q1 = np.exp(logits1 - np.max(logits1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        q2 = q_stage2_mf[s2]
        logits2 = beta * q2
        exp_q2 = np.exp(logits2 - np.max(logits2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # TD updates
        # Stage-2 update from reward
        r = reward[t]
        delta2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha * delta2

        # Stage-1 MF update via eligibility trace using value at stage 2
        # Use the bootstrap target as current second-stage value after update
        delta1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * lam * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w', 'lam', 'kappa']"
iter0_run0_participant40.json,cognitive_model2,536.5689535070263,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid model with anxiety-modulated transition updating and lapse.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state visited each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Outcome each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha_r, beta, w, alpha_t, epsilon)
        - alpha_r in [0,1]: learning rate for second-stage rewards (Q-learning).
        - beta in [0,10]: inverse temperature for both stages.
        - w in [0,1]: weight on model-based control at stage 1.
        - alpha_t in [0,1]: base learning rate for transition probabilities.
        - epsilon in [0,1]: lapse probability that mixes uniform random choice.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    
    Notes
    -----
    - The agent learns the actionâstate transition matrix online.
    - Anxiety increases perceived volatility, scaling transition learning:
      alpha_t_eff = alpha_t * (0.5 + stai).
    - Lapse epsilon injects uniform choice noise at both stages.
    """"""
    alpha_r, beta, w, alpha_t, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition beliefs to the canonical 0.7/0.3 structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate
    alpha_t_eff = alpha_t * (0.5 + stai)  # in [0,1.5], but inputs should keep it <=1

    for t in range(n_trials):
        # Model-based projection using current transition beliefs
        max_q2 = np.max(q_stage2_mf, axis=1)
        q1_mb = T @ max_q2
        q1 = w * q1_mb + (1 - w) * q_stage1_mf

        # First-stage policy with lapse
        logits1 = beta * q1
        exp1 = np.exp(logits1 - np.max(logits1))
        soft1 = exp1 / np.sum(exp1)
        probs_1 = (1 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with lapse
        s2 = state[t]
        logits2 = beta * q_stage2_mf[s2]
        exp2 = np.exp(logits2 - np.max(logits2))
        soft2 = exp2 / np.sum(exp2)
        probs_2 = (1 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Update second-stage Q
        delta2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha_r * delta2

        # Stage-1 MF update towards current second-stage value (eligibility = 1 here)
        target1 = q_stage2_mf[s2, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Learn transition probabilities from the observed transition (a1 -> s2)
        # Move the chosen-action row toward a one-hot on the observed state
        for s in (0, 1):
            target = 1.0 if s == s2 else 0.0
            T[a1, s] += alpha_t_eff * (target - T[a1, s])
        # Ensure row normalization (numerical safety)
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'w', 'alpha_t', 'epsilon']"
iter0_run0_participant40.json,cognitive_model3,535.7651771144655,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive hybrid model with anxiety-modulated loss aversion and asymmetric learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state visited each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Outcome each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, w, gamma)
        - alpha_pos in [0,1]: learning rate when second-stage prediction error >= 0.
        - alpha_neg in [0,1]: learning rate when second-stage prediction error < 0.
        - beta in [0,10]: inverse temperature for both stages.
        - w in [0,1]: weight on model-based control at stage 1.
        - gamma in [0,1]: scales anxiety-driven loss aversion: rho = 1 + gamma * stai.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    
    Notes
    -----
    - Utility transform: u(r) = r if r >= 0 else -rho * |r|, with rho increasing in anxiety.
    - Stage-2 updates use asymmetric learning rates based on the sign of the RPE in utility space.
    - Stage-1 MF is updated toward the current second-stage value (eligibility-like update).
    """"""
    alpha_pos, alpha_neg, beta, w, gamma = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss aversion
    rho = 1.0 + gamma * stai  # >= 1

    for t in range(n_trials):
        # Model-based projection
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2
        q1 = w * q1_mb + (1 - w) * q1_mf

        # First-stage policy
        logits1 = beta * q1
        exp1 = np.exp(logits1 - np.max(logits1))
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        logits2 = beta * q2_mf[s2]
        exp2 = np.exp(logits2 - np.max(logits2))
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Utility transform of reward with anxiety-modulated loss aversion
        r = reward[t]
        util = r if r >= 0 else -rho * (-r)

        # Stage-2 update with asymmetric learning rate in utility space
        pe2 = util - q2_mf[s2, a2]
        alpha2 = alpha_pos if pe2 >= 0 else alpha_neg
        q2_mf[s2, a2] += alpha2 * pe2

        # Stage-1 MF update toward current stage-2 value (eligibility-like, no extra param)
        target1 = q2_mf[s2, a2]
        pe1 = target1 - q1_mf[a1]
        # Use the average of alpha_pos/alpha_neg for stage-1 update to keep within param bounds
        alpha1 = 0.5 * (alpha_pos + alpha_neg)
        q1_mf[a1] += alpha1 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_pos', 'alpha_neg', 'beta', 'w', 'gamma']"
iter0_run0_participant41.json,cognitive_model1,278.0849619259301,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-modulated arbitration and perseveration.
    
    This model combines model-free (MF) and model-based (MB) values at the first stage,
    with an eligibility trace propagating second-stage outcomes back to first-stage values.
    Anxiety (stai) increases the reliance on perseveration and decreases the MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (per planet; 0 or 1) for each trial.
    reward : array-like of float
        Obtained reward on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Participant's anxiety score in [0,1]; here, single-element array with stai[0].
        Higher stai reduces model-based control and increases perseveration.
    model_parameters : list or array of floats
        [alpha, lambda_, beta, w_base, pers_base]
        - alpha in [0,1]: learning rate for both stages.
        - lambda_ in [0,1]: eligibility trace; propagates second-stage TD error to first stage.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w_base in [0,1]: baseline MB/MF mixing weight; anxiety reduces this weight.
        - pers_base in [0,1]: baseline perseveration magnitude; anxiety increases its effect.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, lambda_, beta, w_base, pers_base = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage1_mf = np.zeros(2)       # MF values for A/U
    q_stage2_mf = np.zeros((2, 2))  # MF values for second-stage actions in states X/Y

    # Likelihood tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1  # for perseveration bias

    # Anxiety-modulated arbitration and perseveration
    # Higher stai -> lower MB weight (tilt toward MF)
    w_eff_scale = 1.0 - 0.5 * st
    # Bound within [0,1] by construction of scale in [0.5,1]
    w_eff_base = w_base * w_eff_scale

    # Higher stai -> stronger perseveration
    pers_eff = pers_base * (0.5 + 0.5 * st)

    for t in range(n_trials):
        # Compute MB first-stage values via expected max second-stage value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # shape (2,)
        q_stage1_mb = transition_matrix @ max_q_stage2  # shape (2,)

        # Combine MB and MF with anxiety-modulated weight
        w_eff = w_eff_base
        q1_combined = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add perseveration bias to the previously chosen action
        bias = np.zeros(2)
        if prev_a1 >= 0:
            bias[prev_a1] = pers_eff
        q1_policy_vals = q1_combined + bias

        # First-stage policy and likelihood
        exp_q1 = np.exp(beta * (q1_policy_vals - np.max(q1_policy_vals)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy and likelihood
        s = state[t]
        q2_policy_vals = q_stage2_mf[s, :]
        exp_q2 = np.exp(beta * (q2_policy_vals - np.max(q2_policy_vals)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # TD at second stage
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # TD for first-stage MF values:
        # 1) Bootstrapped update towards the second-stage action value (state-action TD)
        delta1_boot = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1_boot

        # 2) Eligibility trace to propagate outcome to first-stage value
        q_stage1_mf[a1] += alpha * lambda_ * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'lambda_', 'beta', 'w_base', 'pers_base']"
iter0_run0_participant42.json,cognitive_model1,323.55282347901164,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free Q-learning with anxiety-modulated arbitration and eligibility trace.
    
    This model combines model-based planning with model-free values at the first stage. The arbitration
    weight on model-based control is reduced as anxiety (stai) increases. Second-stage values are learned
    with a standard delta rule, and a TD(Î») eligibility trace propagates second-stage prediction errors
    back to the first-stage chosen action.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int
        Second-stage state encountered (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int
        Second-stage choices within state (0/1; e.g., aliens W/S on X, P/H on Y).
    reward : array-like of float
        Scalar reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; higher values indicate higher anxiety.
    model_parameters : sequence of floats
        [alpha, beta1, beta2, lam, w_base]
        Bounds:
        - alpha in [0, 1]: learning rate for Q-updates at both stages.
        - beta1 in [0, 10]: inverse temperature for first-stage softmax.
        - beta2 in [0, 10]: inverse temperature for second-stage softmax.
        - lam in [0, 1]: eligibility trace strength to propagate PE to stage-1.
        - w_base in [0, 1]: baseline weight on model-based control (arbitration).
          Effective weight is w = clip(w_base * (1 - stai), 0, 1), decreasing with anxiety.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta1, beta2, lam, w_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free action values
    q_stage1_mf = np.zeros(2)           # for spaceships A/U
    q_stage2_mf = np.zeros((2, 2))      # for planets X/Y and their aliens (0/1)

    for t in range(n_trials):
        # Compute model-based action values by planning over second-stage max-Qs
        max_q_stage2 = np.max(q_stage2_mf, axis=1)         # shape (2,)
        q_stage1_mb = transition_matrix @ max_q_stage2     # shape (2,)

        # Anxiety-modulated arbitration weight
        w = w_base * (1.0 - stai)
        w = min(max(w, 0.0), 1.0)

        # First-stage policy: softmax over hybrid values
        q1_hybrid = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        z1 = np.max(beta1 * q1_hybrid)
        exp_q1 = np.exp(beta1 * q1_hybrid - z1)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy: softmax within encountered state
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        z2 = np.max(beta2 * q2)
        exp_q2 = np.exp(beta2 * q2 - z2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD error and update
        pe2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * pe2

        # Stage-1 TD error relative to realized second-stage action value (model-free backup)
        pe1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # Eligibility trace: propagate a fraction of second-stage PE back to stage-1
        q_stage1_mf[a1] += alpha * (pe1 + lam * pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta1', 'beta2', 'lam', 'w_base']"
iter0_run0_participant42.json,cognitive_model2,410.2468996375999,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-gated model-based control and outcome-contingent learning with anxiety attenuation.
    
    This model blends model-based and model-free values at stage 1, with the model-based weight
    dynamically increased after rare transitions (surprise). Anxiety dampens this surprise-driven
    shift to model-based control. Learning rates at stage 2 depend on outcome valence (rewarded vs.
    unrewarded), and anxiety further reduces these learning rates, modeling disengagement under anxiety.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int
        Second-stage state encountered (0: planet X, 1: planet Y).
    action_2 : array-like of int
        Second-stage choices within state (0/1).
    reward : array-like of float
        Reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1].
    model_parameters : sequence of floats
        [alpha_r, alpha_u, beta, omega, k_surprise]
        Bounds:
        - alpha_r in [0, 1]: learning rate after rewarded outcomes.
        - alpha_u in [0, 1]: learning rate after unrewarded outcomes.
        - beta in [0, 10]: inverse temperature (both stages).
        - omega in [0, 1]: baseline model-based weight at stage 1.
        - k_surprise in [0, 1]: increment to model-based weight following rare transitions.
          Effective increment is scaled by (1 - stai), so higher anxiety blunts surprise usage.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_r, alpha_u, beta, omega, k_surprise = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    for t in range(n_trials):
        # Determine if the transition was common or rare
        a1 = action_1[t]
        s = state[t]
        common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        is_rare = 0 if common else 1

        # Model-based Q via planning
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Surprise-gated arbitration with anxiety attenuation
        w_t = omega + k_surprise * is_rare * (1.0 - stai)
        w_t = min(max(w_t, 0.0), 1.0)

        # Stage-1 policy
        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf
        z1 = np.max(beta * q1)
        probs_1 = np.exp(beta * q1 - z1)
        probs_1 /= np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        a2 = action_2[t]
        q2_s = q2_mf[s].copy()
        z2 = np.max(beta * q2_s)
        probs_2 = np.exp(beta * q2_s - z2)
        probs_2 /= np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Outcome-contingent learning rate with anxiety scaling
        r = reward[t]
        alpha_t = alpha_r if r > 0 else alpha_u
        # Anxiety reduces the effective learning rate
        alpha_eff = alpha_t * (1.0 - 0.5 * stai)

        # Stage-2 update
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha_eff * pe2

        # Stage-1 MF bootstrap from realized second-stage value
        pe1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_eff * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'alpha_u', 'beta', 'omega', 'k_surprise']"
iter0_run0_participant42.json,cognitive_model3,387.5276742389285,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive model-free learning with anxiety-driven exploration-lapse and inverse temperature scaling.
    
    This model is purely model-free. Rewards are transformed by a concave utility function u(r) = r^rho_eff.
    Higher anxiety increases concavity (lower effective marginal utility), making the agent more pessimistic.
    Action selection includes:
    - An inverse temperature scaled down by anxiety (noisier choices as anxiety increases).
    - A lapse probability that increases with anxiety, mixing uniform random choice into the softmax policy.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices (0/1).
    state : array-like of int
        Second-stage state encountered (0/1).
    action_2 : array-like of int
        Second-stage choices (0/1).
    reward : array-like of float
        Reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1].
    model_parameters : sequence of floats
        [alpha, beta, rho_base, eps0, k_beta]
        Bounds:
        - alpha in [0, 1]: learning rate for Q-updates.
        - beta in [0, 10]: base inverse temperature for both stages.
        - rho_base in [0, 1]: baseline utility exponent; rho_eff = rho_base + 0.5*stai*(1 - rho_base).
        - eps0 in [0, 1]: baseline lapse probability; effective lapse = eps0 * stai (increases with anxiety).
        - k_beta in [0, 1]: scales impact of anxiety on inverse temperature:
            beta_eff = beta * (1 - k_beta * stai).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, rho_base, eps0, k_beta = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective parameters shaped by anxiety
    beta_eff = beta * (1.0 - k_beta * stai)
    beta_eff = max(beta_eff, 1e-6)  # avoid zero inverse temperature
    lapse = min(max(eps0 * stai, 0.0), 1.0)
    rho_eff = rho_base + 0.5 * stai * (1.0 - rho_base)  # more concave as anxiety increases

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 policy: softmax with lapse
        z1 = np.max(beta_eff * q1)
        soft1 = np.exp(beta_eff * q1 - z1)
        soft1 /= np.sum(soft1)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: softmax with lapse within state s
        z2 = np.max(beta_eff * q2[s])
        soft2 = np.exp(beta_eff * q2[s] - z2)
        soft2 /= np.sum(soft2)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward
        # For r in [0,1], r**rho_eff is in [0,1]; preserves ordering while changing curvature.
        u = r ** max(rho_eff, 1e-6)

        # Stage-2 update
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 update via bootstrap from realized second-stage chosen value
        pe1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'rho_base', 'eps0', 'k_beta']"
iter0_run0_participant43.json,cognitive_model1,565.6186245508682,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-weighted hybrid model-based/model-free controller with separate stage temperatures.
    
    Idea:
    - Stage-2 values are learned with a TD rule (model-free).
    - Stage-1 choice values combine a model-free estimate and a model-based plan computed
      from a fixed transition structure (A->X, U->Y common).
    - The arbitration weight (omega) is attenuated by anxiety (higher STAI -> less model-based control).
    
    Parameters (all in [0,1] except betas in [0,10]):
    - model_parameters[0]: alpha (0..1), learning rate for value updates
    - model_parameters[1]: beta1 (0..10), inverse temperature for stage-1 choices
    - model_parameters[2]: beta2 (0..10), inverse temperature for stage-2 choices
    - model_parameters[3]: omega_base (0..1), baseline weight on model-based value at stage-1
    - model_parameters[4]: k_anx_mb (0..1), strength of anxiety modulation of omega
         omega_eff = omega_base * (1 - k_anx_mb * stai)
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship on each trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet on each trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on each trial (on reached planet)
    - reward: array of floats, coin outcome on each trial
    - stai: array-like with a single float in [0,1], the participant's STAI score
    - model_parameters: list/array of parameter values as above
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """"""
    alpha, beta1, beta2, omega_base, k_anx_mb = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed two-step transition structure: rows = first-stage action, cols = planet
    # A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)          # model-free values at stage 1 (A,U)
    q2 = np.zeros((2, 2))        # stage-2 MF values: rows=planet (X,Y), cols=alien (W/S at X, P/H at Y)

    eps = 1e-10

    for t in range(n_trials):
        # Compute stage-1 model-based values from current stage-2 values
        max_q2 = np.max(q2, axis=1)               # value of best alien on each planet
        q1_mb = transition_matrix @ max_q2        # plan value for (A,U)

        # Anxiety-weighted arbitration weight
        omega_eff = omega_base * (1.0 - k_anx_mb * stai_val)
        omega_eff = min(1.0, max(0.0, omega_eff))

        # Combined stage-1 action values
        q1 = (1.0 - omega_eff) * q1_mf + omega_eff * q1_mb

        # Stage-1 policy
        # Softmax with temperature beta1
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)  # numerical stability
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (conditional on reached state)
        s = int(state[t])
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        q2_sa_prev = q2[s, a2]
        delta2 = r - q2_sa_prev
        q2[s, a2] = q2_sa_prev + alpha * delta2

        # Stage-1 MF update toward (pre-update) stage-2 chosen value (SARSA-style bootstrap)
        q1_mf[a1] = q1_mf[a1] + alpha * (q2_sa_prev - q1_mf[a1])

    # Negative log-likelihood of observed choices across both stages
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta1', 'beta2', 'omega_base', 'k_anx_mb']"
iter0_run0_participant43.json,cognitive_model2,557.3203778398961,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-asymmetric model-free learner with eligibility and perseveration.
    
    Idea:
    - Pure model-free controller: stage-2 values learned by TD; stage-1 values updated via
      an eligibility trace from the stage-2 prediction error.
    - Anxiety modulates learning asymmetry: higher STAI reduces learning from rewards
      and increases learning from non-rewards (pessimistic updating).
    - Perseveration bias at stage-1 captures action stickiness.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha_base (0..1), baseline learning rate
    - model_parameters[1]: beta (0..10), inverse temperature for both stages
    - model_parameters[2]: lam (0..1), eligibility trace from stage-2 PE to stage-1 value
    - model_parameters[3]: k_anx_asym (0..1), strength of anxiety asymmetry on learning rate
         alpha_pos = alpha_base * (1 - k_anx_asym * stai) for rewards
         alpha_neg = alpha_base * (1 + k_anx_asym * stai), capped to 1, for non-rewards
    - model_parameters[4]: pers (0..1), perseveration bias magnitude at stage-1
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on that planet
    - reward: array of floats, coin outcome
    - stai: array-like with a single float in [0,1], STAI score
    - model_parameters: list/array of parameter values as above
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """"""
    alpha_base, beta, lam, k_anx_asym, pers = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Values
    q1 = np.zeros(2)       # model-free stage-1
    q2 = np.zeros((2, 2))  # stage-2 MF

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        # Perseveration bias vector (additive to logits)
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[int(prev_a1)] = pers

        # Stage-1 policy
        logits1 = beta * q1 + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning rates modulated by anxiety and outcome valence
        r = reward[t]
        if r > 0.5:
            alpha_t = alpha_base * (1.0 - k_anx_asym * stai_val)
        else:
            alpha_t = alpha_base * (1.0 + k_anx_asym * stai_val)
        if alpha_t > 1.0:
            alpha_t = 1.0
        if alpha_t < 0.0:
            alpha_t = 0.0

        # Stage-2 TD update
        q2_sa_prev = q2[s, a2]
        delta2 = r - q2_sa_prev
        q2[s, a2] = q2_sa_prev + alpha_t * delta2

        # Stage-1 MF update via eligibility trace from stage-2 PE
        q1[a1] = q1[a1] + lam * alpha_t * delta2

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_base', 'beta', 'lam', 'k_anx_asym', 'pers']"
iter0_run0_participant43.json,cognitive_model3,559.0606226003662,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Learning transitions with anxiety-driven noise and arbitration.
    
    Idea:
    - The agent learns both transition probabilities and stage-2 values.
    - Model-based plan uses the learned transition matrix to evaluate first-stage actions.
    - Arbitration between model-based and model-free at stage-1.
    - Anxiety increases lapse/noise at stage-2 and reduces both arbitration weight and
      transition learning rate.
    
    Parameters (all in [0,1] except betas in [0,10]):
    - model_parameters[0]: alpha (0..1), learning rate for rewards and transitions
    - model_parameters[1]: beta1 (0..10), inverse temperature for stage-1
    - model_parameters[2]: beta2 (0..10), inverse temperature for stage-2
    - model_parameters[3]: omega_base (0..1), baseline MB weight at stage-1
    - model_parameters[4]: k_anx_noise (0..1), anxiety strength for lapse and arbitration reduction
         eps_lapse = 0.5 * k_anx_noise * stai    (mixed with uniform at stage-2)
         omega_eff = omega_base * (1 - k_anx_noise * stai)
         alpha_trans = alpha * (1 - k_anx_noise * stai)
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on that planet
    - reward: array of floats, coin outcome
    - stai: array-like with a single float in [0,1], STAI score
    - model_parameters: list/array of parameter values as above
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """"""
    alpha, beta1, beta2, omega_base, k_anx_noise = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition model to uniform uncertainty
    # Rows correspond to first-stage actions (A,U), columns to planets (X,Y)
    p_trans = np.full((2, 2), 0.5)

    # Values
    q1_mf = np.zeros(2)       # model-free stage-1 values
    q2 = np.zeros((2, 2))     # stage-2 values

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    omega_eff_base = omega_base * (1.0 - k_anx_noise * stai_val)
    omega_eff_base = min(1.0, max(0.0, omega_eff_base))
    alpha_trans_base = alpha * (1.0 - k_anx_noise * stai_val)
    if alpha_trans_base < 0.0:
        alpha_trans_base = 0.0
    eps_lapse = 0.5 * k_anx_noise * stai_val  # 0..0.5

    eps = 1e-10

    for t in range(n_trials):
        # Current MB first-stage values from learned transitions and stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = p_trans @ max_q2

        # Arbitration
        q1 = (1.0 - omega_eff_base) * q1_mf + omega_eff_base * q1_mb

        # Stage-1 policy
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with anxiety-driven lapse
        s = int(state[t])
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        soft2 = exp2 / (np.sum(exp2) + eps)
        uniform2 = np.array([0.5, 0.5])
        probs2 = (1.0 - eps_lapse) * soft2 + eps_lapse * uniform2
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Reward learning (stage-2 TD)
        r = reward[t]
        q2_sa_prev = q2[s, a2]
        delta2 = r - q2_sa_prev
        q2[s, a2] = q2_sa_prev + alpha * delta2

        # Stage-1 MF update toward (pre-update) stage-2 chosen value
        q1_mf[a1] = q1_mf[a1] + alpha * (q2_sa_prev - q1_mf[a1])

        # Transition learning for chosen first-stage action
        # Move the chosen action's transition row toward the observed state
        onehot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        p_trans[a1, :] = p_trans[a1, :] + alpha_trans_base * (onehot_s - p_trans[a1, :])

        # Ensure rows remain normalized and within [0,1] (should be true by construction)
        # Minor numerical clip
        p_trans[a1, :] = np.clip(p_trans[a1, :], 0.0, 1.0)
        row_sum = p_trans[a1, :].sum()
        if row_sum > 0:
            p_trans[a1, :] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta1', 'beta2', 'omega_base', 'k_anx_noise']"
iter0_run0_participant44.json,cognitive_model1,432.9561473286499,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-weighted arbitration, eligibility trace, and stickiness.
    
    Idea:
    - Stage-1 action values combine model-free (MF) and model-based (MB) values.
    - Arbitration weight w is reduced by anxiety (higher STAI -> more MF).
    - Choice stickiness (perseveration) increases with anxiety.
    - MF values use an eligibility trace to propagate second-stage outcomes to the first stage.

    Parameters (with bounds):
    - learning_rate (alpha) in [0,1]: learning rate for Q updates.
    - beta in [0,10]: inverse temperature for both stages.
    - lambda_etrace in [0,1]: eligibility trace strength from stage 2 to stage 1.
    - w0 in [0,1]: baseline MB weight (anxiety reduces this).
    - kappa in [0,1]: baseline stickiness strength (anxiety amplifies this).

    Inputs:
    - action_1: array-like of ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1} indicating reached planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1} for second-stage choices (aliens W/S on X, P/H on Y).
    - reward: array-like of floats, typically 0/1 coins received.
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, lambda_etrace, w0, kappa].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """"""
    alpha, beta, lambda_etrace, w0, kappa = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_stage1_mf = np.zeros(2)           # MF Q for A/U
    q_stage2_mf = np.zeros((2, 2))      # MF Q for aliens on each planet

    # Anxiety-modulated arbitration and stickiness
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)         # higher anxiety -> lower MB weight
    kappa_eff = kappa * (0.5 + 0.5 * stai)               # higher anxiety -> more stickiness

    prev_a1 = None
    prev_a2 = None

    for t in range(n_trials):
        # Model-based Q at stage 1: T @ max_a Q2(s,a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [X, Y]
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MF/MB
        q1_combined = (1.0 - w_eff) * q_stage1_mf + w_eff * q_stage1_mb

        # Add stage-1 stickiness bias to the previously chosen action
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_eff

        # Stage-1 policy and likelihood
        logits1 = beta * q1_combined + bias1
        logits1 -= np.max(logits1)  # numerical stability
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy given observed state
        s = state[t]
        # Stage-2 stickiness bias
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += kappa_eff

        logits2 = beta * q_stage2_mf[s, :] + bias2
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD updates
        # Stage-1 MF bootstraps on stage-2 action value
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Stage-2 MF learns from reward
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Eligibility trace to propagate outcome up to stage 1
        q_stage1_mf[a1] += alpha * lambda_etrace * delta2

        # Update history for stickiness
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'lambda_etrace', 'w0', 'kappa']"
iter0_run0_participant44.json,cognitive_model2,438.1703757950356,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated temperature, learned transitions, and value spillover.
    
    Idea:
    - Agent learns transition probabilities with a transition learning rate.
    - MB/MF hybrid at stage 1 with anxiety reducing MB weight.
    - Inverse temperature decreases with anxiety (more exploratory).
    - Choice stickiness increases with anxiety.
    - Second-stage learning includes ""spillover"" to the other planet's same action,
      scaled by anxiety (capturing overgeneralization).

    Parameters (with bounds):
    - alpha in [0,1]: learning rate for Q updates.
    - beta in [0,10]: base inverse temperature.
    - w0 in [0,1]: baseline MB arbitration weight (reduced by anxiety).
    - tau in [0,1]: transition learning rate.
    - kappa in [0,1]: baseline stickiness (amplified by anxiety).

    Inputs:
    - action_1: array-like of ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1} indicating reached planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1} for second-stage choices.
    - reward: array-like of floats, typically 0/1.
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, w0, tau, kappa].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """"""
    alpha, beta, w0, tau, kappa = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize transition beliefs (rows: actions A/U, cols: states X/Y)
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Anxiety effects
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)
    beta_eff = beta * (1.0 - 0.5 * stai)  # more anxiety -> lower temperature (more noise)
    kappa_eff = kappa * (0.5 + 0.5 * stai)
    spillover_scale = 0.5 * stai  # fraction of delta applied to other state (anxiety-driven)

    prev_a1 = None
    prev_a2 = None

    for t in range(n_trials):
        # Model-based values: expected max Q2 under learned transitions
        max_q2 = np.max(q2, axis=1)   # [X, Y]
        q1_mb = T @ max_q2

        # Combine MB/MF
        q1 = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Stage-1 stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_eff

        logits1 = beta_eff * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Observe transition and update transition belief
        s = state[t]
        # Exponential moving average update toward one-hot of observed state
        T[a1, :] = (1.0 - tau) * T[a1, :]
        T[a1, s] += tau
        # Renormalize row to guard against drift
        T[a1, :] /= np.sum(T[a1, :])

        # Stage-2 policy with stickiness
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += kappa_eff
        logits2 = beta_eff * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF updates
        # Stage-1 MF bootstraps on stage-2 value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Stage-2 update with spillover to other state's same action (overgeneralization)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2
        other_s = 1 - s
        q2[other_s, a2] += alpha * spillover_scale * delta2

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'w0', 'tau', 'kappa']"
iter0_run0_participant44.json,cognitive_model3,inf,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Prospect-adjusted outcomes, anxiety-driven transition-outcome bias, and hybrid control.
    
    Idea:
    - Rewards are transformed by a concavity parameter rho (risk/utility sensitivity).
    - Anxiety induces pessimistic updating at stage 2 by penalizing uncertain value states.
    - Stage-1 incorporates MB/MF hybrid control with anxiety reducing MB weight.
    - Additionally, an anxiety-weighted transition-outcome bias adds a first-stage stay/switch
      tendency based on whether the previous transition was common or rare and rewarded.

    Parameters (with bounds):
    - alpha in [0,1]: learning rate for Q updates.
    - beta in [0,10]: inverse temperature for both stages.
    - rho in [0,1]: reward sensitivity (concavity; lower => more diminishing returns).
    - w0 in [0,1]: baseline MB weight (reduced by anxiety).
    - gamma in [0,1]: strength of transition-outcome bias at stage 1 (amplified by anxiety).

    Inputs:
    - action_1: array-like of ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1} indicating reached planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1} for second-stage choices.
    - reward: array-like of floats, typically 0/1.
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, rho, w0, gamma].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """"""
    alpha, beta, rho, w0, gamma = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure
    T_fixed = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Anxiety effects
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)
    beta_eff = beta  # keep constant here; other effects captured via biases and updates
    gamma_eff = gamma * stai  # anxiety amplifies transition-outcome bias

    # Keep track of previous trial info for transition-outcome bias
    prev_a1 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        # Model-based component at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Hybrid Q
        q1 = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Transition-outcome bias on repeating previous first-stage action:
        # If previous trial: rewarded+common -> bias to repeat; rewarded+rare -> bias to switch.
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            is_common = None
            if prev_s is not None:
                # Common if A->X or U->Y; rare otherwise
                is_common = int((prev_a1 == 0 and prev_s == 0) or (prev_a1 == 1 and prev_s == 1))
            signed_bias = 0.0
            if prev_r is not None and is_common is not None:
                if prev_r > 0.0:
                    signed_bias = 1.0 if is_common == 1 else -1.0
                else:
                    # If unrewarded, invert the sign (common+no-reward -> switch; rare+no-reward -> stay)
                    signed_bias = -1.0 if is_common == 1 else 1.0
            bias1[prev_a1] += gamma_eff * signed_bias

        # Stage-1 policy
        logits1 = beta_eff * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (no explicit stickiness; bias captured at stage 1 here)
        s = state[t]
        logits2 = beta_eff * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome transformation and anxiety-driven pessimism penalty
        r_raw = reward[t]
        r_eff = (r_raw ** rho)  # concave utility for gains
        # Uncertainty penalty: penalize high local uncertainty q*(1-q), scaled by anxiety
        unc = q2[s, a2] * (1.0 - q2[s, a2])
        r_pess = r_eff - stai * unc

        # MF updates
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        delta2 = r_pess - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update previous trial record for next bias computation
        prev_a1 = a1
        prev_s = s
        prev_r = r_raw

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik","['alpha', 'beta', 'rho', 'w0', 'gamma']"
iter0_run0_participant5.json,cognitive_model1,456.7417439695346,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Hybrid model-based/model-free SARSA(Î») learner with anxiety-weighted arbitration and separate stage temperatures.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = the two aliens available on the planet).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; here stai[0] used. Higher anxiety down-weights model-based control.
    model_parameters : array-like of float
        [alpha, beta1, beta2, w0, lam]
        - alpha in [0,1]: learning rate for both stages.
        - beta1 in [0,10]: inverse temperature for stage 1 softmax.
        - beta2 in [0,10]: inverse temperature for stage 2 softmax.
        - w0 in [0,1]: baseline model-based weight.
        - lam in [0,1]: eligibility trace weighting from stage 2 to stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta1, beta2, w0, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: rows = stage1 actions (A=0, U=1), cols = states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)            # model-free action values at stage 1
    q2 = np.zeros((2, 2))          # model-free action values at stage 2 (state x action)

    # Likelihood tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-weighted arbitration: higher anxiety reduces MB weight
    # w_eff is a convex combination pushing weight toward model-free as stai increases
    w_eff = np.clip((1.0 - stai) * w0, 0.0, 1.0)

    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values = T @ max_a Q2(s,a)
        max_q2 = np.max(q2, axis=1)          # best alien per planet
        q1_mb = transition_matrix @ max_q2   # expected value under transition dynamics

        # Hybrid action values at stage 1
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Policy and likelihood for stage 1
        exp1 = np.exp(beta1 * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Policy and likelihood for stage 2 (state-dependent)
        q2_s = q2[s]
        exp2 = np.exp(beta2 * (q2_s - np.max(q2_s)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 TD error using stage-2 value (SARSA-style bootstrap with chosen a2)
        delta1 = q2[s, a2] - q1_mf[a1]
        # Update stage-1 MF with blend of immediate bootstrap (delta1) and eligibility from outcome (delta2)
        q1_mf[a1] += alpha * ((1.0 - lam) * delta1 + lam * delta2)

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta1', 'beta2', 'w0', 'lam']"
iter0_run0_participant5.json,cognitive_model3,450.6392763419347,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Volatility-adaptive hybrid learner: dynamic learning rate from outcome surprise and anxiety,
    with forgetting for unchosen second-stage actions and anxiety-modulated model-based weight.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = aliens).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; increases volatility sensitivity and reduces model-based arbitration weight.
    model_parameters : array-like of float
        [alpha_base, beta, w0, tau_forget]
        - alpha_base in [0,1]: base learning rate (minimum).
        - beta in [0,10]: inverse temperature for both stages.
        - w0 in [0,1]: baseline model-based weight at stage 1 (down-weighted by anxiety).
        - tau_forget in [0,1]: forgetting rate toward 0.5 for unchosen stage-2 actions.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_base, beta, w0, tau_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X and U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)             # model-free stage-1 values
    q2 = 0.5 * np.ones((2, 2))      # initialize around neutral prior
    # Running volatility proxy per state-action: exponential moving average of squared prediction errors
    vol = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight: higher anxiety reduces reliance on model-based control
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)

    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q at stage 1 from current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid action values
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage 1 policy and likelihood
        exp1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy and likelihood
        exp2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Stage-2 learning with volatility-adaptive rate
        pe2 = r - q2[s, a2]
        # Update volatility estimate for chosen state-action: combine old vol with new PE^2
        # Use anxiety as the update gain for volatility tracking
        vol_gain = np.clip(stai, 0.0, 1.0)
        vol[s, a2] = (1.0 - vol_gain) * vol[s, a2] + vol_gain * (pe2 ** 2)

        # Adaptive learning rate increases with surprise and anxiety
        alpha_t = np.clip(alpha_base + stai * np.sqrt(vol[s, a2]), 0.0, 1.0)
        q2[s, a2] += alpha_t * pe2

        # Forgetting for unchosen aliens in the visited state (toward 0.5)
        unchosen_a2 = 1 - a2
        q2[s, unchosen_a2] += tau_forget * (0.5 - q2[s, unchosen_a2])

        # Stage-1 MF update using carried stage-2 value target
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_t * pe1  # use the same adaptive rate to keep coherence across stages

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha_base', 'beta', 'w0', 'tau_forget']"
iter0_run0_participant6.json,cognitive_model1,522.3978540899312,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with anxiety-modulated control, eligibility traces, and choice stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien index within planet; 0 or 1).
    reward : array-like of float
        Received coins (can be negative or positive).
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0]. Lower is less anxious.
    model_parameters : sequence of floats
        [alpha, beta, w_mb, lam, kappa]
        - alpha (learning rate) in [0,1]
        - beta (inverse temperature) in [0,10]
        - w_mb (base model-based weight) in [0,1]
        - lam (eligibility trace) in [0,1]
        - kappa (first-stage choice stickiness) in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, w_mb, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q_stage2_mf = np.zeros((2, 2))  # MF values at second stage: state x action
    q_stage1_mf = np.zeros(2)       # MF values for first-stage actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness: track last first-stage choice (one-hot)
    last_a1 = np.zeros(2)

    # Anxiety modulation:
    # - Reduce model-based weight with higher anxiety toward 0.5 (less goal-directed extremity)
    w_eff = np.clip(w_mb * (1.0 - stai) + 0.5 * stai, 0.0, 1.0)
    # - Reduce eligibility trace with anxiety (shorter credit assignment window)
    lam_eff = lam * (1.0 - 0.5 * stai)
    # - Reduce stickiness with anxiety (more variable choices)
    kappa_eff = kappa * (1.0 - stai)

    for t in range(n_trials):
        # Model-based first-stage Q from second-stage MF estimates
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # best action at each planet
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value per spaceship

        # Hybrid action values + stickiness for stage 1
        q1_hybrid = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf + kappa_eff * last_a1

        # Softmax policy stage 1
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = int(state[t])
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning
        # Stage 2 TD update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage 1 MF update with eligibility trace using second-stage TD error
        # Classic eligibility-trace style credit assignment: propagate delta2 back
        # and also bootstrap from the reached state's action value.
        # First, compute a TD-like prediction at stage 1 toward the second-stage chosen value
        target1 = q_stage2_mf[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1 + lam_eff * delta2)

        # Update stickiness memory
        last_a1 = np.zeros(2)
        last_a1[a1] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'w_mb', 'lam', 'kappa']"
iter0_run0_participant6.json,cognitive_model2,515.2090419098698,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid RL with learned transition model and anxiety-modulated valence-specific learning.
    
    Key features:
    - Separate learning rates for positive and negative outcomes at stage 2.
    - Learned transition probabilities for model-based planning at stage 1.
    - Anxiety increases sensitivity to negative outcomes (higher alpha_neg) and reduces model-based weighting mildly.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha_pos, alpha_neg, beta, tau, w_mb]
        - alpha_pos in [0,1]: base learning rate for positive outcomes
        - alpha_neg in [0,1]: base learning rate for negative/zero outcomes
        - beta in [0,10]: inverse temperature for both stages
        - tau in [0,1]: learning rate for transitions (Dirichlet-like exponential smoothing)
        - w_mb in [0,1]: base model-based weight
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_pos, alpha_neg, beta, tau, w_mb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T[a, s']: P(state=s' | action=a)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # start from prior consistent with task, update over time

    # Values
    q_stage2 = np.zeros((2, 2))
    q_stage1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation of learning rates and MB weight
    # - Increase learning from negative outcomes with anxiety; decrease for positive
    alpha_pos_eff = np.clip(alpha_pos * (1.0 - 0.5 * stai), 0.0, 1.0)
    alpha_neg_eff = np.clip(alpha_neg * (0.5 + 0.5 * stai), 0.0, 1.0)
    # - Mild reduction of model-based weighting with anxiety
    w_eff = np.clip(w_mb * (1.0 - 0.3 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based first-stage values using learned transitions
        max_q2 = np.max(q_stage2, axis=1)  # best action at each planet
        q1_mb = T @ max_q2

        # Hybrid value
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q_stage1_mf

        # Stage 1 policy
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = int(state[t])
        q2 = q_stage2[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage 2 update with valence-specific learning rate
        delta2 = r - q_stage2[s, a2]
        lr2 = alpha_pos_eff if delta2 > 0 else alpha_neg_eff
        q_stage2[s, a2] += lr2 * delta2

        # MF backup to stage 1
        target1 = q_stage2[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        # Use a blended learning rate reflecting the sign of second-stage PE
        q_stage1_mf[a1] += lr2 * delta1

        # Update learned transition model T with simple exponential smoothing
        # Move probability mass for action a1 toward the observed next state s.
        T[a1, :] = (1.0 - tau) * T[a1, :]
        T[a1, s] += tau
        # Ensure normalization
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_pos', 'alpha_neg', 'beta', 'tau', 'w_mb']"
iter0_run0_participant6.json,cognitive_model3,527.9618331947104,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-bonus model-based RL with anxiety-modulated exploration and common-transition bias.
    
    Key features:
    - Stage-2 values learned with exponential smoothing; track outcome variance per state-action
      to derive an uncertainty bonus for exploration (UCB-like).
    - Model-based planning at stage 1 uses expected values augmented by uncertainty bonus.
    - Anxiety reduces the exploration bonus allocation to uncertain options.
    - Includes a bias toward common transitions (habitual schema) at stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, phi, common_bias, kappa2]
        - alpha in [0,1]: learning rate for stage-2 value and variance estimates
        - beta in [0,10]: inverse temperature
        - phi in [0,1]: base uncertainty bonus weight (UCB coefficient)
        - common_bias in [0,1]: strength of bias favoring the spaceship with higher common transition prob
        - kappa2 in [0,1]: second-stage choice stickiness
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, phi, common_bias, kappa2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition model per task instructions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 estimates: value and running variance per state-action
    q2 = np.zeros((2, 2))
    mean_sq = np.zeros((2, 2))  # running estimate of E[r^2] to derive variance
    # Stickiness at stage 2
    last_a2 = np.zeros((2,))  # last chosen action in previous trial's reached state; encoded for 2 actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation of exploration: lower anxiety -> more bonus from uncertainty
    phi_eff = phi * (1.0 - stai)
    # Prepare common-transition bias term for stage 1 actions:
    # bias vector b[a] = +common_bias for the spaceship with higher common prob to its frequent planet,
    # and -common_bias for the other. Anxiety attenuates reliance on this heuristic.
    bias_scale = (1.0 - 0.5 * stai)
    b = bias_scale * common_bias * np.array([+1.0, +1.0])  # start symmetric
    # Make bias asymmetric to favor ""common"" transitions: assign + to action aligning with its common mapping
    # Here both ships have a common mapping but to different states; to keep it simple,
    # we bias both equally toward their own common destinations by adding the same +bias to both actions.
    # The asymmetry manifests through MB values via T @ ...
    # (Note: b remains a simple action-prior bonus.)

    # Track last second-stage action per state for stickiness
    last_a2_onehot = np.zeros((2, 2))  # state x action one-hot

    for t in range(n_trials):
        # Compute uncertainty (std) per state-action from running moments
        var = np.maximum(mean_sq - q2**2, 0.0)
        std = np.sqrt(var)

        # For each state, get exploration-augmented values
        q2_ucb = q2 + phi_eff * std + kappa2 * last_a2_onehot

        # Stage 1: MB planning using augmented second-stage values
        max_q2_ucb = np.max(q2_ucb, axis=1)
        q1_mb = T @ max_q2_ucb

        # Add common-transition bias prior
        q1_aug = q1_mb + b

        # Stage 1 policy
        exp_q1 = np.exp(beta * (q1_aug - np.max(q1_aug)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state with UCB and stickiness
        s = int(state[t])
        q2_s = q2_ucb[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage 2 learning: update mean and second moment (for variance)
        # Update mean q2[s,a2]
        delta = r - q2[s, a2]
        q2[s, a2] += alpha * delta
        # Update second moment E[r^2] with same rate (tracks nonstationarity)
        r2 = r * r
        mean_sq[s, a2] += alpha * (r2 - mean_sq[s, a2])

        # Update stickiness memory for the reached state
        last_a2_onehot[s, :] = 0.0
        last_a2_onehot[s, a2] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'phi', 'common_bias', 'kappa2']"
iter0_run0_participant7.json,cognitive_model1,505.8598193132119,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-modulated planning weight and eligibility trace.
    
    This model mixes model-based (MB) and model-free (MF) action values at the first stage.
    The mixture weight is reduced as anxiety (stai) increases, modeling reduced reliance on planning
    under higher anxiety. A model-free eligibility trace propagates outcome prediction errors back
    to the first-stage choice. Stage-2 values are learned via TD(0).
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates at both stages.
    - beta: [0,10] inverse temperature for softmax choice at both stages.
    - w_base: [0,1] baseline weight on model-based control at stage 1 (when stai=0).
    - k_stai: [0,1] strength of anxiety modulation of model-based weight (effective w decreases with stai).
    - lam: [0,1] eligibility trace parameter for MF backpropagation to stage 1.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship on each trial (0=A, 1=U).
    - state: array of ints in {0,1}, planet reached on each trial (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on each trial (per planet).
    - reward: array of floats, obtained coins on each trial (can be negative/zero/positive).
    - stai: array-like with a single float in [0,1], participant anxiety score (used to modulate w).
    - model_parameters: tuple/list of 5 params (alpha, beta, w_base, k_stai, lam).
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w_base, k_stai, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (commonly A->X, U->Y)
    p_common = 0.7
    transition_matrix = np.array([[p_common, 1 - p_common],
                                  [1 - p_common, p_common]])

    # Value functions
    q_stage2_mf = np.zeros((2, 2))  # Q(s, a) at stage 2 (MF)
    q_stage1_mf = np.zeros(2)       # MF value for first-stage actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight
    w_eff = max(0.0, min(1.0, w_base * (1.0 - k_stai * stai)))

    for t in range(n_trials):
        # Model-based values at stage 1 from current stage-2 MF values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # per state best alien
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Policy for first choice (hybrid)
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf
        q1_center = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_center)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Policy for second choice (softmax over current state's Q)
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        q2_center = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_center)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning signals
        # Stage-2 TD error and update
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update: bootstrapped from stage-2 chosen value + eligibility to outcome
        boot = q_stage2_mf[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + alpha * lam * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_base', 'k_stai', 'lam']"
iter0_run0_participant7.json,cognitive_model2,534.105519118633,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-modulated transition learning and exploration.
    
    This model learns action-specific transition probabilities online and uses them for
    model-based planning. Anxiety (stai) reduces both the effective transition learning rate
    and the exploitation level (by reducing beta), modeling less confident structure learning
    and more exploratory behavior under higher anxiety. A small MF component (via eligibility)
    complements MB planning at stage 1 with a weight tied to the same modulation to keep
    parameter count constrained.
    
    Parameters (model_parameters):
    - alpha_r: [0,1] reward learning rate for value updates at stage 2 and MF backprop.
    - beta: [0,10] base inverse temperature for softmax choices.
    - alpha_T_base: [0,1] baseline transition learning rate.
    - k_T: [0,1] strength of anxiety modulation; higher stai reduces alpha_T and beta.
    - lam: [0,1] eligibility trace parameter for MF backpropagation to stage 1.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship on each trial (0=A, 1=U).
    - state: array of ints in {0,1}, planet reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on the reached planet.
    - reward: array of floats, obtained coins on each trial.
    - stai: array-like with a single float in [0,1], participant anxiety score.
    - model_parameters: tuple/list of 5 params (alpha_r, beta, alpha_T_base, k_T, lam).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """"""
    alpha_r, beta, alpha_T_base, k_T, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model T[a, s]: P(s | a)
    T = np.full((2, 2), 0.5)  # start uncertain
    # Value functions
    q_stage2_mf = np.zeros((2, 2))
    q_stage1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    alpha_T = max(0.0, min(1.0, alpha_T_base * (1.0 - k_T * stai)))
    beta_eff = max(0.0, beta * (1.0 - 0.6 * k_T * stai))  # stronger modulation of exploration under anxiety
    # Tie MB/MF mixture to the same modulation without adding a new parameter
    w_mb = max(0.0, min(1.0, 1.0 - 0.5 * k_T * stai))  # less MB under higher anxiety
    w_mf = 1.0 - w_mb

    for t in range(n_trials):
        # Model-based values from learned transitions
        max_q2 = np.max(q_stage2_mf, axis=1)  # per state
        q_stage1_mb = T @ max_q2

        # First-stage policy: hybrid using learned T
        q1 = w_mb * q_stage1_mb + w_mf * q_stage1_mf
        q1_center = q1 - np.max(q1)
        exp_q1 = np.exp(beta_eff * q1_center)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        q2_center = q2 - np.max(q2)
        exp_q2 = np.exp(beta_eff * q2_center)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward learning at stage 2
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * delta2

        # MF backprop to stage 1
        boot = q_stage2_mf[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1 + alpha_r * lam * delta2

        # Transition learning for the chosen action: move probability mass toward observed state
        # T[a1, s] increases, T[a1, 1-s] decreases
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            T[a1, ss] += alpha_T * (target - T[a1, ss])
        # Keep rows normalized (they should remain so due to symmetric update, but clip for safety)
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'alpha_T_base', 'k_T', 'lam']"
iter0_run0_participant8.json,cognitive_model1,454.4618707258769,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration, exploration, and perseveration.
    
    This model mixes model-based (MB) and model-free (MF) first-stage values.
    Anxiety (stai) reduces reliance on the model-based controller, increases exploration,
    and increases perseveration toward the previous first-stage choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage states (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = one of the two aliens on the visited planet).
    reward : array-like of float
        Rewards received on each trial (typically 0/1).
    stai : array-like of float
        Anxiety score(s); only stai[0] is used. Must be in [0,1].
    model_parameters : iterable of floats
        [alpha, beta, w_base, lambda_elig, kappa]
        - alpha (learning rate, [0,1])
        - beta (inverse temperature, [0,10])
        - w_base (baseline MB weight, [0,1])
        - lambda_elig (eligibility trace strength for MF credit, [0,1])
        - kappa (perseveration strength, [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, w_base, lambda_elig, kappa = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Anxiety-modulated arbitration and exploration
    w_eff = np.clip(w_base * (1.0 - st), 0.0, 1.0)  # higher anxiety -> less MB
    beta_eff = beta * (1.0 - 0.3 * st)             # higher anxiety -> more exploration

    prev_a1 = None

    for t in range(n_trials):
        # Model-based Q at stage 1 from current MF second-stage values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)                 # value of each state
        q_stage1_mb = transition_matrix @ max_q_stage2             # plan with transitions

        # Combine MB and MF with anxiety-modulated perseveration bias
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        bias = np.zeros(2)
        if prev_a1 is not None:
            # perseveration scaled by anxiety (stronger with higher st)
            bias[prev_a1] += kappa * st
        q1_biased = q1 + bias

        # First-stage policy
        exp_q1 = np.exp(beta_eff * (q1_biased - np.max(q1_biased)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta_eff * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: second-stage MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Learning: first-stage MF with eligibility trace to reflect outcome
        bootstrapped = q_stage2_mf[s, a2]
        td1 = (bootstrapped + lambda_elig * delta2) - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * td1

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_base', 'lambda_elig', 'kappa']"
iter0_run0_participant8.json,cognitive_model2,403.29546722827797,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planner with anxiety-modulated transition trust and valence-asymmetric learning.
    
    This model plans at stage 1 using a convex combination of the true transitions
    and a uniform transition, where anxiety (stai) reduces trust in the transition model.
    At stage 2, it uses valence-asymmetric reward learning rates that are themselves
    modulated by anxiety (higher anxiety -> larger negative learning rate, smaller positive).
    Q-values at stage 2 are softly decayed toward 0.5 with an anxiety-scaled rate.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions within the reached state.
    reward : array-like of float
        Trial outcomes (0/1).
    stai : array-like of float
        Anxiety score(s); only stai[0] is used. Must be in [0,1].
    model_parameters : iterable of floats
        [alpha_pos, alpha_neg, beta, rho_decay, pi_bias]
        - alpha_pos (learning rate for positive PE, [0,1])
        - alpha_neg (learning rate for negative PE, [0,1])
        - beta (inverse temperature, [0,10])
        - rho_decay (decay toward 0.5 at stage 2 per trial, [0,1])
        - pi_bias (first-stage bias strength toward spaceship A, [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_pos, alpha_neg, beta, rho_decay, pi_bias = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Ground-truth transition; planner will attenuate trust based on anxiety
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2 = np.zeros((2, 2)) + 0.5  # initialize neutral expectations

    # Anxiety-modulated trust in transition structure
    # trust=1 uses T_true; trust=0 uses uniform transitions
    trust = np.clip(1.0 - 0.5 * st, 0.0, 1.0)
    Tunif = np.ones_like(T_true) * 0.5
    T_eff = trust * T_true + (1.0 - trust) * Tunif

    # Anxiety-modulated exploration
    beta_eff = beta * (1.0 - 0.3 * st)

    # First-stage static bias toward A, scaled by anxiety
    bias_vec = np.array([+1.0, -1.0]) * (pi_bias * (1.0 + st))

    for t in range(n_trials):
        # MB planning using attenuated transitions
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = T_eff @ max_q2

        # Apply bias
        q1 = q1_mb + bias_vec

        # First-stage policy
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2[s]
        exp_q2 = np.exp(beta_eff * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 learning with anxiety-modulated valence asymmetry
        pe = r - q_stage2[s, a2]
        if pe >= 0:
            alpha_eff = alpha_pos * (1.0 - 0.5 * st)  # anxious: reduce positive updating
        else:
            alpha_eff = alpha_neg * (0.5 + 0.5 * st)  # anxious: enhance negative updating
        q_stage2[s, a2] += alpha_eff * pe

        # Soft decay of all second-stage Q toward 0.5, scaled by anxiety
        decay = rho_decay * (0.5 + 0.5 * st)
        q_stage2 = (1.0 - decay) * q_stage2 + decay * 0.5

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'rho_decay', 'pi_bias']"
iter0_run0_participant8.json,cognitive_model3,490.0705728995879,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-sensitive outcome utility.
    
    The agent learns the transition probabilities online and uses them to plan (MB).
    First-stage choice values are a hybrid of MB and MF values. The MF system learns
    via bootstrapping from stage 2. Anxiety reduces the utility of rewards following
    rare transitions (ambiguity/violation aversion), down-weighting their impact.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float
        Outcomes (0/1).
    stai : array-like of float
        Anxiety score(s); only stai[0] is used. Must be in [0,1].
    model_parameters : iterable of floats
        [alpha_r, beta, alpha_t, w]
        - alpha_r (reward learning rate for Q2 and Q1-MF, [0,1])
        - beta (inverse temperature, [0,10])
        - alpha_t (transition learning rate, [0,1])
        - w (MB weight in first-stage hybrid, [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_r, beta, alpha_t, w = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition probabilities T_learn[a, s']
    # Start neutral: 0.5 to each state
    T_learn = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Anxiety-modulated exploration
    beta_eff = beta * (1.0 - 0.2 * st)

    # Rare-transition utility penalty strength increases with anxiety
    # Utility u = r * (1 - phi*st*rare), where phi in [0,1] is implicitly 1
    phi = 1.0

    for t in range(n_trials):
        # MB planning from learned transitions
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = T_learn @ max_q2

        # Hybrid first-stage value
        q1 = np.clip(w, 0.0, 1.0) * q1_mb + (1.0 - np.clip(w, 0.0, 1.0)) * q_stage1_mf

        # First-stage policy
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2[s]
        exp_q2 = np.exp(beta_eff * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r_raw = reward[t]

        # Determine if transition was rare under the canonical mapping (A->X, U->Y)
        common_state = a1  # 0 for A->X, 1 for U->Y
        rare = 1.0 if s != common_state else 0.0

        # Anxiety-sensitive utility shaping for outcome
        r_util = r_raw * (1.0 - phi * st * rare)

        # Update second-stage values with utility-shaped reward
        pe2 = r_util - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * pe2

        # Update first-stage MF by bootstrapping from second-stage chosen value
        bootstrapped = q_stage2[s, a2]
        pe1 = bootstrapped - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * pe1

        # Update learned transitions T_learn using observed (a1 -> s)
        # Simple delta rule toward one-hot of observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T_learn[a1] += alpha_t * (target - T_learn[a1])

        # Keep rows normalized and within [0,1]
        T_learn[a1] = np.clip(T_learn[a1], 1e-6, 1.0)
        T_learn[a1] /= np.sum(T_learn[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta', 'alpha_t', 'w']"
iter0_run0_participant9.json,cognitive_model1,452.69765321568104,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with eligibility trace; anxiety (stai) modulates MB arbitration.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices: 0=A, 1=U.
    state : array-like of int {0,1}
        Second-stage states reached: 0=planet X, 1=planet Y.
    action_2 : array-like of int {0,1}
        Second-stage choices: for X: 0=W,1=S; for Y: 0=P,1=H.
    reward : array-like of float
        Scalar rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; lower = less anxious. Used to increase planning weight when anxiety is low.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, lam, w_base, w_stai]
        - alpha (learning rate for Q-values): [0,1]
        - beta (inverse temperature): [0,10]
        - lam (eligibility trace from stage-2 PE to stage-1 MF): [0,1]
        - w_base (baseline weight on model-based values at stage 1): [0,1]
        - w_stai (how strongly low anxiety increases MB weight): [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, lam, w_base, w_stai = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])  # rows: action_1 (A,U); cols: states (X,Y)
    # Value functions
    q_stage1_mf = np.zeros(2)       # MF values for first-stage actions A,U
    q_stage2_mf = np.zeros((2, 2))  # MF values for second-stage actions in each state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Arbitration weight increases as anxiety decreases
        w_mb = w_base + w_stai * (1.0 - stai_val)
        w_mb = max(0.0, min(1.0, w_mb))

        # Model-based values for first-stage: expected max over each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # size 2: [max at X, max at Y]
        q_stage1_mb = transition_matrix @ max_q_stage2  # size 2: expected values for choosing A or U

        # Hybrid at stage 1
        q1 = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # Policy for stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Policy for stage 2 (softmax within realized state)
        s = state[t]
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning: stage-2 TD error and update
        r = reward[t]
        pe2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * pe2

        # Backpropagate to stage-1 MF via eligibility trace using observed stage-2 value
        target1 = q_stage2_mf[s, a2]  # after update (SARSA style)
        pe1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * lam * pe1

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))

","['alpha', 'beta', 'lam', 'w_base', 'w_stai']"
iter0_run0_participant9.json,cognitive_model2,400.78530410216115,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-sensitive model-free control with anxiety-modulated surprise and perseveration.
    
    Idea: Learning accelerates after rare transitions (surprise), with sensitivity stronger when anxiety is low.
    Perseveration bias also depends on anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices: 0=A, 1=U.
    state : array-like of int {0,1}
        Second-stage states reached: 0=planet X, 1=planet Y.
    action_2 : array-like of int {0,1}
        Second-stage choices.
    reward : array-like of float
        Rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; lower -> stronger modulation by surprise and stronger perseveration reduction.
    model_parameters : list/tuple of 5 floats
        [alpha1, alpha2, beta, pi_base, surprise_sens]
        - alpha1: [0,1] learning rate for stage-1 MF
        - alpha2: [0,1] learning rate for stage-2 MF
        - beta: [0,10] inverse temperature (both stages)
        - pi_base: [0,1] baseline perseveration strength (applied at both stages)
        - surprise_sens: [0,1] scales how much rare transitions boost alpha1 when anxiety is low
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha1, alpha2, beta, pi_base, surprise_sens = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Model-free Q-values
    q1_mf = np.zeros(2)       # for A,U
    q2_mf = np.zeros((2, 2))  # for (state, action2)

    # Perseveration memory (last chosen action)
    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Determine if transition was common or rare (A->X common, U->Y common)
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        is_rare = 1 - is_common

        # Anxiety-modulated surprise boost to stage-1 learning
        # Low anxiety -> stronger boost on rare transitions
        boost = surprise_sens * (1.0 - stai_val) * is_rare
        alpha1_eff = max(0.0, min(1.0, alpha1 * (1.0 + boost)))

        # Perseveration strength modulated by anxiety (lower anxiety -> less perseveration)
        pi = pi_base * (0.5 + 0.5 * stai_val)  # scales down when stai is low

        # Stage-1 policy with perseveration bias
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += pi
        logits1 = beta * q1_mf + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with within-state perseveration
        bias2 = np.zeros(2)
        la2 = last_a2_by_state[s]
        if la2 is not None:
            bias2[la2] += pi
        logits2 = beta * q2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning stage-2 MF
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha2 * pe2

        # Back up to stage-1 MF (one-step SARSA target is the post-update q2 of the chosen a2)
        target1 = q2_mf[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1_eff * pe1

        # Update perseveration memories
        last_a1 = a1
        last_a2_by_state[s] = a2

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))

","['alpha1', 'alpha2', 'beta', 'pi_base', 'surprise_sens']"
iter0_run0_participant9.json,cognitive_model3,400.84263329489363,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid with learned transition model; anxiety modulates transition learning and MB/MF arbitration.
    
    The agent learns transition probabilities for each first-stage action and uses them
    to compute model-based values. Anxiety reduces both transition learning and MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices.
    state : array-like of int {0,1}
        Observed second-stage states.
    action_2 : array-like of int {0,1}
        Second-stage choices.
    reward : array-like of float
        Rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; higher anxiety reduces planning and transition learning.
    model_parameters : list/tuple of 5 floats
        [alpha_r, beta, w_base, omega_stai, alpha_tr]
        - alpha_r: [0,1] learning rate for second-stage rewards (MF update)
        - beta: [0,10] inverse temperature
        - w_base: [0,1] baseline MB arbitration weight
        - omega_stai: [0,1] how strongly higher anxiety reduces MB weight
        - alpha_tr: [0,1] baseline transition learning rate (for learning P(s|a1))
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_r, beta, w_base, omega_stai, alpha_tr = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Learned transition matrix P(state|action1); initialize to weak prior (common ~0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # MF value functions
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Anxiety-modulated arbitration: higher anxiety -> less MB
        w_mb = w_base * (1.0 - omega_stai * stai_val)
        w_mb = max(0.0, min(1.0, w_mb))

        # Model-based Q at stage 1 from learned transitions and stage-2 MF values
        max_q2 = np.max(q2_mf, axis=1)      # [max at X, max at Y]
        q1_mb = T @ max_q2                  # expected values for A,U

        # Hybrid first-stage values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within state
        logits2 = beta * (q2_mf[s] - np.max(q2_mf[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learn transitions: update row corresponding to chosen action toward observed state (one-hot)
        # Anxiety reduces transition learning rate
        alpha_tr_eff = alpha_tr * (1.0 - 0.75 * stai_val)
        alpha_tr_eff = max(0.0, min(1.0, alpha_tr_eff))
        target_T = np.array([0.0, 0.0])
        target_T[s] = 1.0
        T[a1] = (1.0 - alpha_tr_eff) * T[a1] + alpha_tr_eff * target_T
        # Re-normalize for numerical stability
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        # Learn second-stage MF
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha_r * pe2

        # Update stage-1 MF toward observed post-update q2 value (simple backup)
        target1 = q2_mf[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use same reward learning rate for MF backup to keep parameter count bounded
        q1_mf[a1] += alpha_r * pe1

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))","['alpha_r', 'beta', 'w_base', 'omega_stai', 'alpha_tr']"
iter1_run0_participant0.json,cognitive_model1,512.0880768461035,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning MB-MF hybrid with anxiety-driven transition forgetting and MF eligibility.
    
    Idea:
    - The agent learns its own transition model T(a->s) online and uses it for model-based values.
    - Higher anxiety increases transition forgetting (less reliance on learned structure) and reduces
      MF eligibility propagation from stage 2 to stage 1.
    - Stage 1 choices use a weighted combination of MB and MF values.
    
    Parameters (bounds):
    - alpha: [0,1] stage-2 reward learning rate
    - lambda_tr: [0,1] transition learning/retention (higher = faster learning, lower = more forgetting)
    - w0: [0,1] baseline MB weight at stage 1 before anxiety modulation
    - beta: [0,10] inverse temperature (both stages)
    
    Anxiety usage:
    - Transition learning rate: alpha_tr = lambda_tr * (1 - 0.5*stai)  # higher anxiety => slower consolidation
      plus complementary forgetting toward a uniform transition (more uncertainty when anxious).
    - MF eligibility propagation from stage 2 to stage 1 is scaled by elig = (1 - stai)  # higher anxiety => weaker MF bootstrapping
    - Arbitration weight: w_eff = clip(w0 * (1 - stai), 0, 1)  # higher anxiety => more MF at stage 1
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T with entries in {0,1} except reward in [0,1]
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, lambda_tr, w0, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha, lambda_tr, w0, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Initialize learned transition model T_est[a, s], start uninformative uniform
    T_est = np.full((2, 2), 0.5)

    # Model-free values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Track choice probabilities for NLL
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    alpha_tr = np.clip(lambda_tr * (1.0 - 0.5 * stai_score), 0.0, 1.0)
    w_eff = float(np.clip(w0 * (1.0 - stai_score), 0.0, 1.0))
    elig = float(np.clip(1.0 - stai_score, 0.0, 1.0))  # MF eligibility

    for t in range(n_trials):
        # MB stage-1 values from current transition estimate and Q2
        max_Q2 = np.max(Q2, axis=1)          # best action at each state
        Q1_mb = T_est @ max_Q2               # expected max value given T_est

        # Arbitration: mix MB and MF
        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf

        # Stage-1 policy
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        Q2_s = Q2[s]
        logits2 = beta * (Q2_s - np.max(Q2_s))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model toward observed transition with anxiety-modulated forgetting
        # Target one-hot next-state given a1
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        # Move row a1 toward target with alpha_tr; softly forget toward uniform (0.5,0.5) with weight proportional to stai
        T_est[a1] = (1.0 - alpha_tr) * T_est[a1] + alpha_tr * target
        # Anxiety-driven uncertainty: shrink toward uniform
        T_est[a1] = (1.0 - 0.3 * stai_score) * T_est[a1] + 0.3 * stai_score * np.array([0.5, 0.5])

        # Keep rows normalized
        T_est[a1] = np.clip(T_est[a1], 1e-6, 1.0)
        T_est[a1] /= np.sum(T_est[a1])

        # Stage-2 MF update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Stage-1 MF bootstrapping with anxiety-scaled eligibility
        Q1_mf[a1] += (alpha * elig) * (Q2[s, a2] - Q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'lambda_tr', 'w0', 'beta']"
iter1_run0_participant0.json,cognitive_model2,490.81209104314325,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pure model-free SARSA(0) with risk-sensitive utility, anxiety-modulated curvature,
    and stage-1 perseveration; inverse temperature also shifts with anxiety.
    
    Idea:
    - MF values at stage 2 learn from a risk-transformed utility u(r) = r^rho_eff.
    - Anxiety increases concavity of utility (more risk-averse on gains) and reduces softmax
      inverse temperature (more noise); stage-1 perseveration bias grows with anxiety.
    - Stage 1 learns from bootstrapped stage-2 value (standard two-step MF).
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for value updates
    - rho: [0,1] base utility curvature (0 -> very concave; 1 -> linear)
    - kappa1: [0,1] baseline stage-1 perseveration strength (added to last chosen action's logit)
    - delta_beta: [0,1] strength of anxiety impact on beta (how much beta drops/increases with stai)
    - beta: [0,10] base inverse temperature (both stages)
    
    Anxiety usage:
    - Utility curvature: rho_eff = clip(rho * (1 - 0.5*stai), 0, 1)  # more concave with higher anxiety
    - Inverse temperature: beta_eff = beta * (1 - delta_beta*stai)   # higher anxiety -> more random
    - Perseveration at stage 1: stick1 = kappa1 * (0.5 + stai)       # stronger with higher anxiety
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, rho, kappa1, delta_beta, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha, rho, kappa1, delta_beta, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Values
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Perseveration memory for stage 1
    last_a1 = -1

    # Track choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated params
    rho_eff = np.clip(rho * (1.0 - 0.5 * stai_score), 0.0, 1.0)
    beta_eff = max(1e-6, beta * (1.0 - delta_beta * stai_score))
    stick1 = kappa1 * (0.5 + stai_score)

    for t in range(n_trials):
        # Stage-1 policy with perseveration bonus on previous action
        pref1 = Q1.copy()
        if last_a1 != -1:
            pref1[last_a1] += stick1
        logits1 = beta_eff * (pref1 - np.max(pref1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta_eff * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward (risk sensitivity)
        r = reward[t]
        u = (r + 1e-12) ** rho_eff

        # MF updates
        pe2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Bootstrapped update for stage 1 toward stage-2 value
        Q1[a1] += alpha * (Q2[s, a2] - Q1[a1])

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'rho', 'kappa1', 'delta_beta', 'beta']"
iter1_run0_participant0.json,cognitive_model3,495.21129032811643,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""MB-MF hybrid with directed exploration (UCB) at stage 2 and anxiety-damped exploration.
    
    Idea:
    - Stage 1 uses MB-MF arbitration with a fixed baseline weight that is attenuated by anxiety.
    - Stage 2 adds an uncertainty bonus (UCB style) that decays with visit count; anxiety reduces this bonus,
      capturing reduced directed exploration under higher anxiety.
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for MF value updates
    - bonus0: [0,1] initial uncertainty bonus scale at stage 2
    - tau_decay: [0,1] decay rate of the uncertainty bonus with visits (higher -> faster decay)
    - w_mb0: [0,1] baseline MB arbitration weight at stage 1
    - beta: [0,10] inverse temperature (both stages)
    
    Anxiety usage:
    - Stage-1 arbitration: w_eff = clip(w_mb0 * (1 - 0.5*stai), 0, 1)  # higher anxiety => more MF
    - Stage-2 exploration: bonus_eff = bonus0 * (1 - stai)             # higher anxiety => smaller UCB bonus
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, bonus0, tau_decay, w_mb0, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha, bonus0, tau_decay, w_mb0, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Fixed (known) transition structure for MB planning
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Values and visit counts
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))
    N_visits = np.zeros((2, 2))  # counts for UCB bonus

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    w_eff = float(np.clip(w_mb0 * (1.0 - 0.5 * stai_score), 0.0, 1.0))
    bonus_eff = float(np.clip(bonus0 * (1.0 - stai_score), 0.0, 1.0))
    tau = float(np.clip(tau_decay, 1e-6, 1.0))

    for t in range(n_trials):
        # MB evaluation from known transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_known @ max_Q2

        # Stage-1 arbitration and policy
        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with UCB bonus
        s = state[t]
        # Uncertainty bonus decays with visits: bonus ~ bonus_eff / (1 + tau * N)
        bonus = bonus_eff / (1.0 + tau * N_visits[s])
        pref2 = Q2[s] + bonus
        logits2 = beta * (pref2 - np.max(pref2))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update MF stage-1 toward updated stage-2 value
        Q1_mf[a1] += alpha * (Q2[s, a2] - Q1_mf[a1])

        # Update visit counts after the choice
        N_visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'bonus0', 'tau_decay', 'w_mb0', 'beta']"
iter1_run0_participant1.json,cognitive_model1,291.49990938687085,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 1: Learned transitions with anxiety-modulated surprise bonus and choice stickiness.
    
    Core idea:
    - First-stage policy is purely model-based using a learned transition matrix.
    - Anxiety increases the impact of transition surprise on first-stage preferences.
    - Also includes first-stage choice stickiness (perseveration), scaled by anxiety.
    - Second-stage policy is model-free Q-learning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; aliens).
    reward : array-like of float (0 or 1)
        Coins received each trial.
    stai : array-like of float in [0,1]
        Anxiety score used to scale surprise bonus and stickiness.
    model_parameters : list or array
        [alpha, beta, alpha_t, kappa_stick, phi_anx]
        - alpha in [0,1]: learning rate for second-stage Q-value updates.
        - beta in [0,10]: inverse temperature at both stages.
        - alpha_t in [0,1]: transition learning rate for updating the transition matrix.
        - kappa_stick in [0,1]: strength of first-stage choice perseveration.
        - phi_anx in [0,1]: scales the surprise-to-bonus mapping as a function of anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, alpha_t, kappa_stick, phi_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix with common transitions (A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: actions (A,U), cols: states (X,Y)

    # Model-free second-stage Q-values
    q2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # First-stage stickiness vector; adds bias to previously chosen action
    prev_a1 = None
    stickiness = np.zeros(2, dtype=float)

    for t in range(n_trials):
        # Compute model-based first-stage values: expected max Q2 under learned transitions
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T @ max_q2  # shape (2,)

        # Surprise bonus based on transition prediction error from previous trial (applied to current chosen action)
        # Compute current soft bonus vector; default zeros
        bonus = np.zeros(2, dtype=float)
        if t > 0:
            # Surprise of last transition under then-current T (approximate by current T for tractability)
            a1_prev = action_1[t - 1]
            s2_prev = state[t - 1]
            p_obs = T[a1_prev, s2_prev]
            surprise = 1.0 - p_obs  # higher when transition was rare/unexpected
            # Apply bonus to the action chosen this trial to capture carryover salience
            # Anxiety scales the mapping from surprise to bonus magnitude
            a1_now = action_1[t]
            bonus[a1_now] += phi_anx * stai_val * surprise

        # Update stickiness bias based on previous first-stage action
        if prev_a1 is not None:
            stickiness = np.zeros(2, dtype=float)
            # Anxiety increases perseveration strength
            stickiness[prev_a1] = kappa_stick * (1.0 + stai_val)

        # First-stage decision values with biases
        q1_eff = q1_mb + bonus + stickiness
        q1_eff -= np.max(q1_eff)  # softmax stability
        exp_q1 = np.exp(beta * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage decision
        s2 = state[t]
        q2_s = q2[s2].copy()
        q2_s -= np.max(q2_s)
        exp_q2 = np.exp(beta * q2_s)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Second-stage Q-learning
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # Transition learning: update the row for chosen first-stage action toward observed state
        # Simple delta rule on the two probabilities to preserve row sum 1
        # Move probability mass toward the observed state
        a1_row = T[a1]
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s2 else 0.0
            a1_row[s_idx] += alpha_t * (target - a1_row[s_idx])
        # Normalize for numerical safety
        T[a1] = a1_row / np.sum(a1_row)

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'alpha_t', 'kappa_stick', 'phi_anx']"
iter1_run0_participant10.json,cognitive_model1,264.14702769039377,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid RL with learned transitions and anxiety-weighted arbitration and forgetting.
    
    This model learns the first-stage transition structure online and arbitrates
    between model-based (MB) and model-free (MF) control at stage 1. The arbitration
    weight increases with transition certainty but is down-weighted by anxiety.
    Both stage-1 and stage-2 Q-values undergo anxiety-scaled forgetting.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial on the planet (0/1 for the two aliens).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0] in [0,1]. Higher indicates more anxious.
    model_parameters : array-like of float
        [alpha2, beta, alpha_T, w0, decay]
        - alpha2 in [0,1]: stage-2 MF learning rate.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - alpha_T in [0,1]: learning rate for the transition matrix.
        - w0 in [0,1]: baseline arbitration weight for MB control at stage 1.
        - decay in [0,1]: base forgetting rate applied each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """"""
    alpha2, beta, alpha_T, w0, decay = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T[a, s], start uninformative (0.5/0.5)
    T = np.full((2, 2), 0.5)

    # Q-values
    q1_mf = np.zeros(2)        # model-free stage-1
    q2_mf = np.zeros((2, 2))   # model-free stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-scaled forgetting factor (stronger forgetting with higher anxiety)
    decay_eff = np.clip(decay * (0.5 + 0.5 * np.clip(st, 0.0, 1.0)), 0.0, 1.0)

    for t in range(n_trials):
        # Apply forgetting to both stages before acting
        q1_mf = (1.0 - decay_eff) * q1_mf
        q2_mf = (1.0 - decay_eff) * q2_mf

        # Model-based evaluation using learned transitions and current q2
        max_q2 = np.max(q2_mf, axis=1)      # value of each planet
        q1_mb = T @ max_q2                  # MB value of each spaceship

        # Certainty of transitions (0=uncertain, 1=certain). For action a: c_a=2*|T[a,0]-0.5|
        c_actions = 2.0 * np.abs(T[:, 0] - 0.5)
        certainty = np.mean(c_actions)  # overall certainty in [0,1]

        # Anxiety-weighted arbitration: more anxious (higher st) dampens MB usage when certainty < 1
        # w_eff increases with certainty, but raised to (1-st) to reduce MB under anxiety.
        w_eff = w0 * (certainty ** np.clip(1.0 - st, 0.0, 1.0))
        w_eff = np.clip(w_eff, 0.0, 1.0)

        # Stage-1 policy
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in observed state
        s = int(state[t])
        q2 = q2_mf[s]
        q2_centered = q2 - np.max(q2)
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn transitions from observed (a1 -> s)
        # Move T[a1] toward one-hot for observed s by alpha_T
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_T * (target - T[a1, sp])
        # Keep rows normalized (should already be due to update form, but ensure numerical stability)
        T[a1] = T[a1] / np.sum(T[a1])

        # Stage-2 MF update
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha2 * delta2

        # Stage-1 MF bootstrapping from realized second-stage value
        v2 = q2_mf[s, a2]
        delta1 = v2 - q1_mf[a1]
        q1_mf[a1] += alpha2 * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'alpha_T', 'w0', 'decay']"
iter1_run0_participant11.json,cognitive_model1,376.1072384320429,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-penalized hybrid with anxiety-modulated arbitration and stickiness.
    
    This model blends model-free (MF) and model-based (MB) values at stage 1, and penalizes
    first-stage actions that lead to higher transition uncertainty. Anxiety reduces the
    arbitration weight on MB control and amplifies a stickiness bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed first-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Observed second-stage choices within the encountered state.
    reward : array-like of float
        Rewards received on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Anxiety score in [0,1]; uses stai[0] as scalar.
    model_parameters : array-like of float
        Parameters with bounds:
        - alpha in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - psi in [0,1]: baseline MB arbitration weight.
        - gamma in [0,1]: weight on uncertainty penalty at stage 1.
        - kappa1 in [0,1]: baseline first-stage stickiness strength.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, psi, gamma, kappa1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure: rows are A/U, cols are X/Y
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Precompute action-wise transition entropies H(T[a])
    eps_h = 1e-12
    H = -np.sum(T * np.log(T + eps_h), axis=1)  # shape (2,)

    # Value tables
    q1_mf = np.zeros(2)     # MF values for A,U
    q2 = np.zeros((2, 2))   # Q2[state, action]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness: previous action at stage 1
    prev_a1 = 0

    # Anxiety-modulated arbitration weight (less MB with higher anxiety)
    w_eff = np.clip(psi * (1.0 - 0.6 * stai_val), 0.0, 1.0)
    # Anxiety-modulated stickiness strength
    kappa_eff = kappa1 * (1.0 + stai_val)

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based estimates from stage-2 values
        max_q2 = np.max(q2, axis=1)       # value of X and Y
        q1_mb = T @ max_q2               # MB value for A,U

        # Hybrid action values
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Bias terms for stage 1: stickiness and uncertainty penalty
        bias1 = np.zeros(2)
        bias1[prev_a1] += kappa_eff

        # Uncertainty penalty: higher entropy transitions are penalized more
        # Scaled by anxiety and gamma; we subtract as a bias term
        unc_penalty = gamma * stai_val * H
        logits1 = beta * q1_hybrid + bias1 - unc_penalty
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (no added biases here; beta shared)
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning updates
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # MF first-stage credit assignment via bootstrapped value at stage 2
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'psi', 'gamma', 'kappa1']"
iter1_run0_participant12.json,cognitive_model1,475.8541205317016,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MBâMF with anxiety-modulated arbitration and eligibility trace.
    
    This model combines a model-free (MF) controller that learns second-stage Q-values and
    backs them up to the first stage via an eligibility trace, with a model-based (MB) controller
    that plans using the known transition structure. Anxiety (stai) shifts the arbitration weight
    toward MB or MF control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial at the visited state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration between MB and MF.
    model_parameters : list or array
        [alpha, beta, w0, k_stai, lambd]
        Bounds:
          alpha in [0,1]    : learning rate for MF values
          beta in [0,10]    : inverse temperature for softmax
          w0 in [0,1]       : baseline MB arbitration weight
          k_stai in [0,1]   : sensitivity of MB weight to STAI (positive shifts MB if stai>0.5)
          lambd in [0,1]    : eligibility trace from stage 2 to stage 1

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, w0, k_stai, lambd = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure: rows = actions (A=0, U=1), cols = states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Model-free Q-values
    q1_mf = np.zeros(2)           # first-stage MF values for A/U
    q2_mf = np.zeros((2, 2))      # second-stage MF values for states X/Y and aliens

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration toward MB
    w_eff = w0 + k_stai * (stai - 0.5)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based Q1 computed from current MF second-stage values (planning)
        max_q2_per_state = np.max(q2_mf, axis=1)  # best alien on each planet
        q1_mb = transition_matrix @ max_q2_per_state

        # Hybrid first-stage action values
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage 1 choice probabilities
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 choice probabilities at visited state
        q2s = q2_mf[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2 (MF)
        pe2 = reward[t] - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Eligibility-trace update to stage 1 MF for chosen a1
        q1_mf[a1] += alpha * lambd * pe2

        # Optional direct TD(0) consistency between stages for MF
        td1 = (q2_mf[s, a2] - q1_mf[a1])
        q1_mf[a1] += alpha * (1.0 - lambd) * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'k_stai', 'lambd']"
iter1_run0_participant12.json,cognitive_model2,521.4060212508506,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive model-based controller with anxiety-modulated risk aversion and uncertainty learning.
    
    This purely model-based controller maintains both expected value and outcome variance for each
    second-stage option. Choices maximize a risk-adjusted utility: U = Q - rho * sqrt(Var).
    Anxiety increases risk sensitivity (rho) linearly. Planning uses the known transition structure.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial at the visited state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate risk sensitivity.
    model_parameters : list or array
        [alpha, beta, rho_base, rho_stai, kappa]
        Bounds:
          alpha in [0,1]     : learning rate for mean reward (Q)
          beta in [0,10]     : inverse temperature
          rho_base in [0,1]  : baseline risk sensitivity
          rho_stai in [0,1]  : additional risk sensitivity per unit STAI
          kappa in [0,1]     : learning rate for variance (uncertainty tracking)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, rho_base, rho_stai, kappa = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Maintain mean and variance for each second-stage option
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.25  # start with moderate uncertainty for Bernoulli rewards

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated risk sensitivity
    rho = rho_base + rho_stai * stai

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Risk-adjusted utilities at stage 2
        util2 = q2_mean - rho * np.sqrt(np.maximum(q2_var, 1e-8))

        # Stage 1 MB values via planning over risk-adjusted utilities
        max_util_per_state = np.max(util2, axis=1)
        q1_mb = transition_matrix @ max_util_per_state

        # Stage 1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy at visited state (risk-adjusted)
        u2s = util2[s].copy()
        u2c = u2s - np.max(u2s)
        probs_2 = np.exp(beta * u2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning: update mean and variance of observed second-stage option
        r = reward[t]
        m_old = q2_mean[s, a2]
        q2_mean[s, a2] = m_old + alpha * (r - m_old)

        # Exponential moving average of squared error as variance proxy
        se = (r - m_old) ** 2
        q2_var[s, a2] = (1.0 - kappa) * q2_var[s, a2] + kappa * se

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'rho_base', 'rho_stai', 'kappa']"
iter1_run0_participant12.json,cognitive_model3,519.6131895579329,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planning with anxiety-modulated heuristic (MB-consistent) stay/shift and lapse.
    
    The controller plans using second-stage MF values and the known transition structure (MB).
    Additionally, a heuristic bias at stage 1 promotes MB-consistent stay/shift:
      - After common transitions, reward encourages staying with the previous first-stage choice;
        non-reward encourages switching.
      - After rare transitions, this pattern flips (MB signature).
    Anxiety scales the strength of this heuristic and also increases a lapse (noise) rate.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial at the visited state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Modulates heuristic strength and lapse.
    model_parameters : list or array
        [alpha, beta, eta_base, stai_gain, epsilon_base]
        Bounds:
          alpha in [0,1]        : learning rate for second-stage MF values used by MB planning
          beta in [0,10]        : inverse temperature
          eta_base in [0,1]     : baseline strength of heuristic stay/shift bias
          stai_gain in [0,1]    : scaling of heuristic by STAI (centered around 0.5)
          epsilon_base in [0,1] : baseline lapse rate (mixed with uniform choice)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, eta_base, stai_gain, epsilon_base = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Second-stage MF values that MB planning relies on
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_common = None
    prev_rew = None

    # Anxiety-modulated heuristic strength and lapse
    eta_eff = np.clip(eta_base + stai_gain * (stai - 0.5), 0.0, 1.0)
    epsilon_eff = np.clip(epsilon_base * (0.5 + stai), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # MB planning values at stage 1 from current q2
        max_q2_per_state = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2_per_state

        # Add heuristic bias based on previous trial outcome and transition type
        bias = np.zeros(2)
        if prev_a1 is not None:
            # MB-consistent stay/shift sign: +1 promotes staying with prev_a1, -1 promotes switching
            # After common transition: reward->stay, no reward->switch
            # After rare transition: reward->switch, no reward->stay
            sign = 0.0
            if prev_common is not None and prev_rew is not None:
                if prev_common:
                    sign = 1.0 if prev_rew > 0.0 else -1.0
                else:
                    sign = -1.0 if prev_rew > 0.0 else 1.0
            bias[prev_a1] = eta_eff * sign

        q1_aug = q1_mb + bias

        # Stage 1 softmax with lapse (mixture with uniform)
        q1c = q1_aug - np.max(q1_aug)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        probs_1 = (1.0 - epsilon_eff) * probs_1 + epsilon_eff * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage 2 softmax with lapse at visited state
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        probs_2 = (1.0 - epsilon_eff) * probs_2 + epsilon_eff * 0.5
        p_choice_2[t] = probs_2[a2]

        # Learning: update second-stage values
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Track whether the observed transition was common or rare for the chosen first-stage action
        # Common if (A->X) or (U->Y)
        prev_a1 = a1
        prev_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        prev_rew = reward[t]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'eta_base', 'stai_gain', 'epsilon_base']"
iter1_run0_participant14.json,cognitive_model1,530.320732871516,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 1: Pure model-free SARSA(Î») with anxiety-modulated trace persistence and choice temperature.
    
    This model treats the task as a two-step Markov decision process but learns purely model-free
    Q-values. Stage-1 Q-values are updated using a two-step SARSA(Î») target that blends the
    immediately bootstrapped second-stage value with the terminal reward. Anxiety increases both
    the eligibility trace persistence (Î») and the effective inverse temperature, capturing more
    persistent credit assignment and more deterministic choices under higher anxiety.
    
    Parameters (model_parameters):
    - alpha: learning rate for all Q-updates, in [0,1]
    - beta: base inverse temperature for both stages, in [0,10]
    - lam0: base eligibility trace parameter Î», in [0,1]
    - g_beta: anxiety gain on temperature (scales beta), in [0,1]
    - g_lam: anxiety gain on Î», in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0 or 1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, lam0, g_beta, g_lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-modulated effective parameters
    beta_eff = beta * (1.0 + g_beta * max(0.0, stai - 0.31))
    lam_eff = np.clip(lam0 + g_lam * max(0.0, stai - 0.31), 0.0, 1.0)

    # Model-free Q-values
    q1 = np.zeros(2)        # stage-1 action values for spaceships [A,U]
    q2 = np.zeros((2, 2))   # stage-2 action values per planet [X,Y] x aliens [0,1]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Policy stage 1
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Policy stage 2 (conditioned on observed planet)
        s = state[t]
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Updates
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Two-step SARSA(Î») target for stage-1:
        # target1 = (1-Î»)*Q2(s,a2) + Î»*r
        target1 = (1.0 - lam_eff) * q2[s, a2] + lam_eff * r
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lam0', 'g_beta', 'g_lam']"
iter1_run0_participant14.json,cognitive_model2,567.2220246743557,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 2: Model-based planning with risk-sensitive utility and asymmetric learning.
    
    The model uses a fixed transition structure (AâX common, UâY common) to act in a model-based
    manner at stage 1 (computing expected values from second-stage Q-values). Stage-2 Q-values are
    updated with asymmetric learning rates for positive vs. negative prediction errors. Rewards are
    transformed by a risk/utility curvature parameter Ï. Anxiety modulates the utility curvature,
    capturing increased risk aversion or seeking in value learning and decision making.
    
    Parameters (model_parameters):
    - alpha_pos: learning rate for positive RPEs at stage 2, in [0,1]
    - alpha_neg: learning rate for negative RPEs at stage 2, in [0,1]
    - beta: inverse temperature (both stages), in [0,10]
    - rho0: base utility curvature (0=extremely concave, 1=linear), in [0,1]
    - g_rho: anxiety gain on curvature (adds to rho0), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0 or 1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_pos, alpha_neg, beta, rho0, g_rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions: rows [A,U] to cols [X,Y]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Anxiety-modulated risk curvature
    rho_eff = np.clip(rho0 + g_rho * (stai - 0.51), 0.0, 1.0)

    q2 = np.zeros((2, 2))  # second-stage values per planet/alien
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage values from current second-stage values
        max_q2 = np.max(q2, axis=1)       # best alien per planet
        q1_mb = T @ max_q2               # expected value per spaceship

        # Stage 1 policy
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy conditioned on observed planet
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward and asymmetric learning
        r = reward[t]
        u = r ** rho_eff  # in [0,1], curvature depends on anxiety

        delta2 = u - q2[s, a2]
        alpha = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'rho0', 'g_rho']"
iter1_run0_participant14.json,cognitive_model3,552.3799500624018,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 3: Model-based planning with anxiety-sensitive aversion to previously rare transitions
    and value forgetting.
    
    The model uses fixed transitions for planning at stage 1, but adds a dynamic bias that penalizes
    repeating a spaceship if it most recently produced a rare transition. This bias decays over
    trials and is stronger under higher anxiety. Stage-2 values are learned model-free with
    forgetting (decay toward zero), capturing limited memory. The same decay also shrinks the
    rare-transition bias over time to reuse the parameter efficiently and meaningfully.
    
    Parameters (model_parameters):
    - alpha: reward learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - eta0: base strength of rare-transition aversion bias, in [0,1]
    - g_eta: anxiety gain on rare-transition aversion, in [0,1]
    - decay: forgetting/decay rate applied to Q2 and bias each trial, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0 or 1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, eta0, g_eta, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions: A commonly to X, U commonly to Y
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Anxiety-modulated rare-transition aversion strength
    eta_eff = np.clip(eta0 * (1.0 + g_eta * max(0.0, stai - 0.31)), 0.0, 2.0)

    q2 = np.zeros((2, 2))   # stage-2 values per planet/alien
    bias = np.zeros(2)      # dynamic bias on stage-1 actions [A,U]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Apply decay to values and bias
        q2 *= (1.0 - decay)
        bias *= (1.0 - decay)

        # Model-based q1 from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add dynamic rare-transition bias
        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update Q2 MF with learning rate
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update rare-transition aversion bias based on observed transition rarity
        # Common if (A->X) or (U->Y), rare otherwise
        common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        if common == 0:
            # Penalize repeating the action that just yielded a rare transition
            bias[a1] -= eta_eff
        else:
            # Small rebound toward zero for common transitions (already handled by decay implicitly)

            # Optionally provide a tiny positive nudge to counterbalance decay; keep it zero to
            # avoid adding extra effective parameters.

            pass

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'eta0', 'g_eta', 'decay']"
iter1_run0_participant15.json,cognitive_model1,565.9477822769086,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with learned transition model and anxiety-damped information-seeking.
    
    Idea:
    - Stage-2 values (Q2) are learned model-free from rewards.
    - A transition model T is learned with its own learning rate (alpha_t).
    - Stage-1 values are a mixture of model-based planning via T and model-free bootstrap from Q2.
    - An exploration bonus adds the entropy of the predicted next-state distribution to the MB values.
    - Anxiety decreases both the MB weight and the strength of the exploration bonus.
    
    Parameters (all used; total=5):
    - alpha_r: [0,1] Reward learning rate for Stage-2 values.
    - beta:   [0,10] Inverse temperature for both stages.
    - alpha_t:[0,1] Learning rate for the transition model.
    - w_mb0:  [0,1] Baseline weight on model-based value at Stage-1 (before anxiety).
    - kappa:  [0,1] Coefficient for entropy-based exploration bonus at Stage-1.
    
    Anxiety use:
    - Effective MB weight: w_mb = w_mb0 * (1 - stai).
    - Exploration bonus: bonus_scale = kappa * (1 - stai).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1 (0=A, 1=U).
    - state:    array of ints in {0,1}, reached planet at Stage-2 (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on that planet (0 or 1).
    - reward:   array of floats in [0,1], received coins.
    - stai:     array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [alpha_r, beta, alpha_t, w_mb0, kappa].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    alpha_r, beta, alpha_t, w_mb0, kappa = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition model T(a, s') with weak bias toward 0.5-0.5
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Stage-2 Q-values: Q2[state, action]
    Q2 = 0.5 * np.ones((2, 2), dtype=float)
    # Stage-1 MF values: Q1_mf[action]
    Q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Effective weights modulated by anxiety
    w_mb = w_mb0 * (1.0 - st)
    bonus_scale = kappa * (1.0 - st)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB value at Stage-1 via current learned transitions and Stage-2 max values
        maxQ2 = np.max(Q2, axis=1)  # shape (2,)
        V_mb = T @ maxQ2  # shape (2,)

        # Entropy bonus of next-state prediction for each Stage-1 action
        # H(p) = - sum p log p; encourage actions with uncertain transitions (exploration)
        H = np.zeros(2, dtype=float)
        for a in range(2):
            p = T[a]
            # numerically stable entropy
            p_clip = np.clip(p, 1e-12, 1.0)
            H[a] = -np.sum(p_clip * np.log(p_clip))
        V_mb_bonus = V_mb + bonus_scale * H

        # Blend MB with MF at Stage-1
        Q1 = w_mb * V_mb_bonus + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        z1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: update Q2 (model-free) from reward
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * pe2

        # Update Stage-1 MF by bootstrapping from Stage-2 value (TD)
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_r * pe1

        # Update transition model from observed transition (a1 -> s)
        # Move T[a1] toward one-hot of observed state s
        oh = np.array([0.0, 0.0])
        oh[s] = 1.0
        T[a1] = (1.0 - alpha_t) * T[a1] + alpha_t * oh

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'alpha_t', 'w_mb0', 'kappa']"
iter1_run0_participant15.json,cognitive_model2,576.151891546429,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pure model-free with eligibility trace, anxiety-scaled outcome sensitivity,
    and anxiety-amplified Stage-2 perseveration.
    
    Idea:
    - Both stages are learned via model-free temporal-difference updates.
    - Eligibility trace (gamma_e) credits Stage-1 action with Stage-2 prediction error.
    - Outcome sensitivity is reduced by anxiety via xi, damping learning from both gains and losses.
    - Perseveration at Stage-2 biases repeating the previous Stage-2 action, amplified by anxiety.
    - Stage-1 choice is also model-free (Q1), with its own softmax; anxiety can slightly reduce its precision.
    
    Parameters (all used; total=5):
    - alpha:  [0,1] Base learning rate for value updates.
    - beta:   [0,10] Base inverse temperature.
    - gamma_e:[0,1] Eligibility trace strength propagating Stage-2 PE to Stage-1.
    - psi:    [0,1] Perseveration strength at Stage-2 (amplified by anxiety).
    - xi:     [0,1] Anxiety scaling of outcome sensitivity and Stage-1 precision.
    
    Anxiety use:
    - Outcome sensitivity factor: k_out = 1 - xi * stai, scaling the TD-errors used for learning.
    - Stage-2 perseveration bias magnitude: stick2 = psi * stai.
    - Stage-1 inverse temperature: beta1 = beta * (1 - 0.5 * xi * stai); Stage-2 uses beta.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1 (0=A, 1=U).
    - state:    array of ints in {0,1}, reached planet at Stage-2 (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on that planet (0 or 1).
    - reward:   array of floats in [0,1], received coins.
    - stai:     array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [alpha, beta, gamma_e, psi, xi].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    alpha, beta, gamma_e, psi, xi = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Values
    Q2 = 0.5 * np.ones((2, 2), dtype=float)  # Q2[state, action]
    Q1 = np.zeros(2, dtype=float)            # Q1[action]

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Anxiety effects
    k_out = 1.0 - xi * st
    k_out = max(0.0, min(1.0, k_out))
    stick2 = psi * st
    beta1 = beta * (1.0 - 0.5 * xi * st)

    prev_a2 = None

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 policy (model-free)
        z1 = beta1 * (Q1 - np.max(Q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration bias
        bias2 = np.zeros(2, dtype=float)
        if prev_a2 is not None:
            bias2[prev_a2] = stick2

        z2 = beta * (Q2[s] + bias2 - np.max(Q2[s] + bias2))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * k_out * pe2

        # Eligibility trace to update Stage-1 from Stage-2 PE
        Q1[a1] += alpha * gamma_e * k_out * pe2

        prev_a2 = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'gamma_e', 'psi', 'xi']"
iter1_run0_participant15.json,cognitive_model3,573.9772842417533,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-weighted MB/MF arbitration with anxiety bias and optimistic priors.
    
    Idea:
    - Stage-2 values (Q2) are learned model-free with learning rate alpha.
    - Each state-action also tracks an uncertainty U2 (running absolute PE), updated with alpha_u.
    - Model-based value at Stage-1 comes from a fixed transition model (0.7/0.3).
    - Arbitration weight for MB at Stage-1 decreases with:
        (a) higher predicted successor uncertainty, and
        (b) higher anxiety.
      Thus, w_mb = arb * (1 - predicted_uncertainty) * (1 - stai).
    - Q2 initialized with optimism q0.
    - Stage-1 MF values (Q1_mf) are updated via TD bootstrapping from Stage-2.
    
    Parameters (all used; total=5):
    - alpha:  [0,1] Learning rate for Q2 and Stage-1 MF bootstrap.
    - beta:   [0,10] Inverse temperature for both stages.
    - arb:    [0,1] Overall strength of uncertainty-based arbitration.
    - q0:     [0,1] Optimistic prior for all Stage-2 Q-values.
    - alpha_u:[0,1] Learning rate for uncertainty (U2) updates.
    
    Anxiety use:
    - Effective MB weight: w_mb = arb * (1 - stai) * (1 - U_pred[a]) for each Stage-1 action a.
      Here U_pred[a] is the transition-weighted successor uncertainty.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1 (0=A, 1=U).
    - state:    array of ints in {0,1}, reached planet at Stage-2 (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on that planet (0 or 1).
    - reward:   array of floats in [0,1], received coins.
    - stai:     array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [alpha, beta, arb, q0, alpha_u].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    alpha, beta, arb, q0, alpha_u = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition model (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Initialize optimistic Q2 and zero-mean uncertainties
    Q2 = q0 * np.ones((2, 2), dtype=float)  # Q2[state, action]
    U2 = np.zeros((2, 2), dtype=float)      # running abs PE per state-action
    Q1_mf = np.zeros(2, dtype=float)        # Stage-1 MF values

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based value via transitions and current Q2
        maxQ2 = np.max(Q2, axis=1)  # value for each successor state
        V_mb = T @ maxQ2            # per action at Stage-1

        # Predicted successor uncertainty for each Stage-1 action
        # Define state uncertainty as the uncertainty of its best action
        u_state = np.max(U2, axis=1)  # uncertainty of best option in each state
        U_pred = T @ u_state          # per Stage-1 action

        # Arbitration weight per action: higher uncertainty and higher anxiety -> lower MB weight
        w_mb = arb * (1.0 - st) * (1.0 - U_pred)
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # Final Stage-1 values: action-wise arbitration between MB and MF
        Q1 = w_mb * V_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        z1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning Stage-2 values and uncertainty
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update uncertainty as an exponential moving average of |PE|
        U2[s, a2] = (1.0 - alpha_u) * U2[s, a2] + alpha_u * abs(pe2)
        # Small decay for other entries to avoid stale high uncertainty
        for ss in range(2):
            for aa in range(2):
                if not (ss == s and aa == a2):
                    U2[ss, aa] = (1.0 - 0.1 * alpha_u) * U2[ss, aa]

        # Update Stage-1 MF by bootstrapping from Stage-2 chosen value
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'arb', 'q0', 'alpha_u']"
iter1_run0_participant16.json,cognitive_model1,518.9165513939951,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-weighted arbitration and learned transitions.
    
    Overview
    --------
    - Learns second-stage action values (Q2) from rewards.
    - Learns first-stage model-free values (Q1_MF) via eligibility traces.
    - Learns the transition model T(a -> s) online.
    - Arbitrates between model-based (MB) and MF at stage 1 using a weight that
      decreases with transition surprise, with stronger down-weighting for higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0 or 1 within the reached state).
    reward : array-like of float in [0,1]
        Obtained rewards at the end of trial.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase arbitration sensitivity to surprise.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for Q-values.
        - beta in [0,10]: softmax inverse temperature for both stages.
        - omega_base in [0,1]: baseline MB weight at stage 1 (when no surprise).
        - lambda_ in [0,1]: eligibility trace controlling MF credit assignment to stage 1.
        - tau in [0,1]: transition learning rate for T(a -> s).
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, omega_base, lambda_, tau = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize with instructed/common transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Action values
    q2 = np.zeros((2, 2), dtype=float)     # state x action
    q1_mf = np.zeros(2, dtype=float)       # action at stage 1

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        # Model-based plan using current transition beliefs
        max_q2 = np.max(q2, axis=1)             # best action per state
        q1_mb_all = T @ max_q2                  # MB value for each stage-1 action

        # Arbitration weight shaped by surprise and anxiety
        a1 = int(action_1[t])
        s_obs = int(state[t])
        # Surprise given current transition belief for chosen action
        surprise = 1.0 - float(T[a1, s_obs])
        omega = omega_base * (1.0 - stai * surprise)
        omega = float(np.clip(omega, 0.0, 1.0))

        q1 = omega * q1_mb_all + (1.0 - omega) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s_obs]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning from reward
        r = float(reward[t])

        # Update Q2 at experienced state-action
        delta2 = r - q2[s_obs, a2]
        q2[s_obs, a2] += alpha * delta2

        # MF update for stage 1 with eligibility trace
        # Blend bootstrapped value and reward-based credit
        target_mf = (1.0 - lambda_) * np.max(q2[s_obs]) + lambda_ * r
        q1_mf[a1] += alpha * (target_mf - q1_mf[a1])

        # Update transition model for chosen action towards observed state (row-stochastic)
        # One-hot target for the observed state
        for s_val in (0, 1):
            target = 1.0 if s_val == s_obs else 0.0
            T[a1, s_val] += tau * (target - T[a1, s_val])
        # Numerical cleanup to keep probabilities valid
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'omega_base', 'lambda_', 'tau']"
iter1_run0_participant16.json,cognitive_model2,487.036255762295,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Information-bonus planner with anxiety-amplified exploration and stickiness.
    
    Overview
    --------
    - Tracks both mean and uncertainty (variance) of second-stage rewards.
    - Adds an information bonus proportional to uncertainty when evaluating actions.
    - Anxiety increases the weight on the information bonus and reduces stage-2 inverse temperature.
    - Stage-1 choices plan over uncertainty-augmented values; includes first-stage perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0 or 1).
    reward : array-like of float in [0,1]
        Obtained rewards.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values amplify the information bonus and reduce stage-2 determinism.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for both mean and variance trackers.
        - beta in [0,10]: base inverse temperature (both stages).
        - xi_base in [0,1]: base weight on the information (uncertainty) bonus.
        - kappa in [0,1]: stage-1 perseveration/stickiness for repeating previous action.
        - zeta in [0,1]: anxiety-to-info gain factor.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, xi_base, kappa, zeta = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure known to participant
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Reward belief: mean and variance for each second-stage option
    q2_mean = np.zeros((2, 2), dtype=float)
    q2_var = np.ones((2, 2), dtype=float) * 0.25  # initial uncertainty around 0.5

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = None

    # Anxiety modulation
    xi = float(np.clip(xi_base * (1.0 + zeta * stai), 0.0, 1.0))
    beta2 = max(0.0, beta * (1.0 - 0.4 * stai))  # more anxiety -> more stochastic stage-2

    for t in range(n_trials):
        # Uncertainty bonus
        bonus = xi * np.sqrt(np.maximum(q2_var, 1e-8))
        q2_aug = q2_mean + bonus

        # Plan at stage 1 over augmented values
        max_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_aug
        logits1 = beta * q1_mb

        # Stage-1 perseveration
        if prev_a1 is not None:
            logits1[prev_a1] += kappa

        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy over augmented values, with anxiety-reduced beta
        s = int(state[t])
        logits2 = beta2 * q2_aug[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = float(reward[t])

        # Update mean with delta-rule
        delta = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha * delta
        # Update variance as moving average of squared PE
        q2_var[s, a2] = (1.0 - alpha) * q2_var[s, a2] + alpha * (delta ** 2)

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'xi_base', 'kappa', 'zeta']"
iter1_run0_participant16.json,cognitive_model3,487.9862142464833,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor Representation (SR) first-stage controller with anxiety-tuned weighting
    and reward sensitivity.
    
    Overview
    --------
    - Maintains SR over first-stage actions to second-stage states and learns it online.
    - Combines SR-based planning with a simple MF stage-1 value via anxiety-weighted mixing.
    - Second-stage values learned with standard delta-rule but with reward sensitivity rho
      that increases with anxiety.
    - Includes second-stage perseveration within each state.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage actions (0 or 1).
    reward : array-like of float in [0,1]
        Received rewards.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase SR weight and reward sensitivity, and reduce stage-2 determinism.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for SR and Q-values.
        - beta in [0,10]: inverse temperature (stage 1 and baseline for stage 2).
        - sr_w_base in [0,1]: baseline weight on SR controller at stage 1.
        - rho_base in [0,1]: baseline reward sensitivity.
        - psi in [0,1]: second-stage perseveration within each state.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, sr_w_base, rho_base, psi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize SR rows for each first-stage action over states (rows sum to 1)
    SR = np.array([[0.7, 0.3],
                   [0.3, 0.7]], dtype=float)

    # Second-stage values and stage-1 MF values
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    # Anxiety-modulated weights
    sr_w = float(np.clip(sr_w_base + 0.5 * stai * (1.0 - sr_w_base), 0.0, 1.0))
    rho = float(np.clip(rho_base + 0.7 * stai * (1.0 - rho_base), 0.0, 1.0))
    beta2 = max(0.0, beta * (1.0 - 0.3 * stai))
    lam = 0.3 + 0.5 * stai  # eligibility for MF backprop to stage 1

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Track previous second-stage action per state for perseveration
    prev_a2 = [-1, -1]

    for t in range(n_trials):
        # SR-based first-stage values: expected max Q2 weighted by SR occupancy
        max_q2 = np.max(q2, axis=1)
        q1_sr = SR @ max_q2

        # Combine SR and MF controllers
        q1 = sr_w * q1_sr + (1.0 - sr_w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with within-state perseveration
        s = int(state[t])
        logits2 = beta2 * q2[s]
        if prev_a2[s] != -1:
            logits2[prev_a2[s]] += psi
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = float(reward[t])
        r_subj = rho * r  # reward sensitivity (higher anxiety -> stronger impact)

        # Update Q2
        delta2 = r_subj - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update stage-1 MF with eligibility towards both next-state value and reward
        target_mf = (1.0 - lam) * np.max(q2[s]) + lam * r_subj
        q1_mf[a1] += alpha * (target_mf - q1_mf[a1])

        # Update SR row for chosen action towards the observed state one-hot
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            SR[a1, ss] += alpha * (0.5 + 0.5 * stai) * (target - SR[a1, ss])
        SR[a1] = np.clip(SR[a1], 1e-6, 1.0)
        SR[a1] /= np.sum(SR[a1])

        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'sr_w_base', 'rho_base', 'psi']"
iter1_run0_participant17.json,cognitive_model1,469.26933471052996,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model with anxiety-shaped MB weighting, anxiety-scaled forgetfulness, and state-conditional perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically in {0,1}.
    stai : array-like of float in [0,1]
        Anxiety score (single value array). Used to modulate MB weighting, forgetfulness, and perseveration.
    model_parameters : tuple/list
        Parameters (all in [0,1] except beta in [0,10]):
        - alpha: learning rate for value updates in [0,1]
        - beta: inverse temperature for softmax in [0,10]
        - w_slope: base MB mixing weight; anxiety shifts toward (1 - w_slope) in [0,1]
                   Effective w_mb = (1 - stai)*w_slope + stai*(1 - w_slope)
        - rho_forget: baseline forgetting/decay strength in [0,1]
                      Effective decay factor each trial: decay = 1 - rho_forget * stai
        - tau_stay: state-conditional perseveration strength in [0,1]
                    Bias to repeat previous second-stage action in same state scales with stai

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, w_slope, rho_forget, tau_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (commonly A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Anxiety-shaped MB weight: higher anxiety pushes weight toward (1 - w_slope)
    w_mb = (1.0 - stai) * w_slope + stai * (1.0 - w_slope)
    w_mb = 0.0 if w_mb < 0.0 else (1.0 if w_mb > 1.0 else w_mb)

    # Probability logs
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)        # stage-1 model-free
    q2 = np.zeros((2, 2))      # stage-2 action values

    # Perseveration bookkeeping
    prev_a2 = None
    prev_s = None

    # Decay factor influenced by anxiety
    decay = 1.0 - rho_forget * stai
    if decay < 0.0: decay = 0.0
    if decay > 1.0: decay = 1.0

    for t in range(n_trials):
        # Model-based evaluation for stage 1
        max_q2 = np.max(q2, axis=1)           # best action per state
        q1_mb = T @ max_q2                    # expected value via transition model

        # Hybrid Q for stage-1 decision
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Softmax for stage 1
        z1 = q1 - np.max(q1)
        exp1 = np.exp(beta * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 decision with state-conditional perseveration
        s = state[t]
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2 is not None and prev_s == s:
            bias2[prev_a2] += tau_stay * stai  # stronger stickiness under higher anxiety

        z2 = (q2_s + bias2) - np.max(q2_s + bias2)
        exp2 = np.exp(beta * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Apply global decay (forgetting) before incorporating new information
        q1_mf *= decay
        q2 *= decay

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update bootstrapping off stage-2 value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a2 = a2
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_slope', 'rho_forget', 'tau_stay']"
iter1_run0_participant18.json,cognitive_model2,415.5676844506239,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive utility with fixed MB weight and anxiety-modulated curvature and perseveration.
    
    Description:
    - Rewards are transformed by a concave utility u(r) = r^gamma_eff to capture risk/uncertainty sensitivity.
      Higher anxiety yields more concave utility (stronger diminishing returns).
    - Stage 1 values are a fixed hybrid of model-based (MB) and model-free (MF) values with weight w_mb.
    - A single perseveration parameter biases repeating previous choices at both stages; its influence decreases with anxiety.
    - Stage 2 values are MF and learned from utility-transformed rewards.
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - w_mb: fixed MB weight in [0,1]
    - gamma_base: baseline utility curvature in [0,1] (smaller -> more concave)
    - pi: perseveration strength in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on observed planet
                (0=W on X or P on Y; 1=S on X or H on Y)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w_mb, gamma_base, pi)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, w_mb, gamma_base, pi = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety effects:
    # - Utility more concave with higher anxiety
    gamma_eff = np.clip((1.0 - st) * gamma_base + st * max(1e-6, 0.5 * gamma_base + 0.25), 1e-6, 1.0)
    # - Perseveration weaker with higher anxiety
    pi_eff = pi * (1.0 - st)
    # - Eligibility trace derived from anxiety to propagate outcomes to stage 1
    lam = 0.5 * (1.0 - st)

    prev_a1 = None
    prev_a2 = [None, None]

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # MB stage-1 values from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid stage-1
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Add perseveration bias
        pref1 = Q1_hyb.copy()
        if prev_a1 is not None:
            pref1[prev_a1] += pi_eff

        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        pref2 = Q2[s].copy()
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += pi_eff

        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Utility transform of reward
        u = r ** gamma_eff

        # Learning
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1
        Q1_mf[a1] += alpha * lam * delta2

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_mb', 'gamma_base', 'pi']"
iter1_run0_participant18.json,cognitive_model3,514.8633794722749,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Dual-temperature, dual-learning-rate hybrid with anxiety-driven exploration and forgetting.
    
    Description:
    - Separate learning rates for stage 1 and stage 2 (alpha1, alpha2).
    - Separate inverse temperatures for stage 1 and stage 2 (beta1, beta2).
      Anxiety reduces beta1 (more exploration at stage 1) and mildly reduces beta2.
    - Fixed MB weight omega combines model-based and model-free stage-1 values.
    - Stage 2 includes anxiety-driven forgetting/decay of unchosen action values within the visited state.
    - Stage 1 MF uses an eligibility trace whose strength increases with anxiety (faster credit assignment to outcomes).
    
    Parameters (model_parameters):
    - alpha1: learning rate for stage-1 MF in [0,1]
    - alpha2: learning rate for stage-2 MF in [0,1]
    - beta1: inverse temperature for stage 1 in [0,10]
    - beta2: inverse temperature for stage 2 in [0,10]
    - omega: MB weight in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on observed planet
                (0=W on X or P on Y; 1=S on X or H on Y)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha1, alpha2, beta1, beta2, omega)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha1, alpha2, beta1, beta2, omega = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety effects:
    # - exploration: reduce beta with higher anxiety
    beta1_eff = max(1e-8, beta1 * (1.0 - 0.5 * st))
    beta2_eff = max(1e-8, beta2 * (1.0 - 0.25 * st))
    # - eligibility: faster outcome credit assignment at higher anxiety
    lam = 0.2 + 0.8 * st  # in [0.2,1.0]
    # - forgetting/decay rate for unchosen action in visited state increases with anxiety
    decay = 0.1 * st  # decays toward zero

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # MB estimate from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid stage-1 values
        Q1_hyb = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Stage-1 policy
        pref1 = Q1_hyb
        exp1 = np.exp(beta1_eff * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        pref2 = Q2[s]
        exp2 = np.exp(beta2_eff * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha2 * delta2

        # Anxiety-driven decay of the unchosen action in the visited state
        other = 1 - a2
        Q2[s, other] *= (1.0 - decay)

        # Stage-1 MF updates with eligibility
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha1 * delta1
        Q1_mf[a1] += alpha1 * lam * (r - Q1_mf[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha1', 'alpha2', 'beta1', 'beta2', 'omega']"
iter1_run0_participant19.json,cognitive_model1,208.86194032112527,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pessimistic lookahead with anxiety-modulated loss aversion and forgetting.
    Parameters (model_parameters):
      - alpha: learning rate for Q-value updates at both stages, in [0,1]
      - beta: inverse temperature for both stages, in [0,10]
      - lambda_loss: base loss-aversion coefficient, in [0,1] (effective loss-aversion increases with anxiety)
      - kappa_anx: strength of anxiety-driven pessimism in model-based lookahead, in [0,1]
      - phi_forget: forgetting rate for unchosen actions (value decay), in [0,1]
    Inputs:
      - action_1: array of first-stage choices (0=A, 1=U)
      - state: array of second-stage states (0=X, 1=Y)
      - action_2: array of second-stage choices (0 or 1; e.g., alien index on the planet)
      - reward: array of scalar rewards (can be negative or positive)
      - stai: array-like with a single anxiety score in [0,1]
      - model_parameters: list/tuple as described above
    Returns:
      - Negative log-likelihood of the observed first- and second-stage choices.
    Model summary:
      - Second-stage learning is model-free with loss-averse utility u(r).
      - First-stage decision uses a convex combination of:
          (i) model-free Q1 values bootstrapped from second-stage MF values via eligibility,
          (ii) a pessimistic model-based lookahead over the transition matrix.
        Anxiety increases loss aversion and pessimism, and also increases forgetting of unchosen actions.
    """"""
    alpha, beta, lambda_loss, kappa_anx, phi_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition: rows = first-stage actions (A=0, U=1); cols = states (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    Q1_mf = np.zeros(2)        # model-free values for first-stage actions
    Q2 = np.zeros((2, 2))      # second-stage state-action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated components
    # Effective loss aversion: grows with anxiety (mapping lambda_eff in [0, 2*lambda_loss])
    lambda_eff = lambda_loss * (1.0 + stai)
    # Pessimistic blending within each second-stage state: xi near 1 -> optimistic (max), near 0 -> pessimistic (min)
    xi = np.clip(1.0 - kappa_anx * stai, 0.0, 1.0)
    # Forgetting rate scales with anxiety
    forget = np.clip(phi_forget * (0.5 + stai), 0.0, 1.0)

    eps = 1e-12

    for t in range(n_trials):
        # Construct pessimistic state values from Q2 for model-based lookahead
        vmax = np.max(Q2, axis=1)   # per state
        vmin = np.min(Q2, axis=1)   # per state
        V_state = xi * vmax + (1.0 - xi) * vmin  # pessimism-weighted value per state

        Q1_mb_pess = T @ V_state  # model-based action values under pessimistic evaluation

        # Anxiety-driven arbitration without adding a separate parameter:
        # weight increases with anxiety when loss aversion is low, and vice versa.
        w_eff = np.clip(0.5 + 0.5 * stai * (1.0 - lambda_loss), 0.0, 1.0)

        Q1 = w_eff * Q1_mb_pess + (1.0 - w_eff) * Q1_mf

        # Softmax for first-stage choice
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice policy
        s = int(state[t])
        q2c = Q2[s, :] - np.max(Q2[s, :])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Utility transformation with loss aversion
        r = float(reward[t])
        if r >= 0:
            u = r
        else:
            u = - (1.0 + lambda_eff) * (-r)

        # Learning updates
        # Second-stage MF update
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Forgetting for unchosen second-stage action at current state
        other_a2 = 1 - a2
        Q2[s, other_a2] *= (1.0 - forget)

        # First-stage MF bootstrapping via observed second-stage value (eligibility assumed 1 here)
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1

        # Forget unchosen first-stage action
        other_a1 = 1 - a1
        Q1_mf[other_a1] *= (1.0 - forget)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lambda_loss', 'kappa_anx', 'phi_forget']"
iter1_run0_participant21.json,cognitive_model1,472.20634780833313,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated model-based weighting and dual learning rates.

    The model combines model-free (MF) and model-based (MB) action values at stage 1.
    Stage-2 values are learned with a delta rule. Stage-1 MF values receive a TD-style
    backup from stage-2. The model-based weight is reduced by higher anxiety, capturing
    a shift away from planning under anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha2 (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = alpha1 (0 to 1): learning rate for stage-1 MF Q-values
    - model_parameters[2] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[3] = w_base (0 to 1): baseline weight on model-based values at stage 1
    - model_parameters[4] = anx_MB (0 to 1): strength of anxiety modulation on MB weight
        Effective MB weight: w = clip(w_base + (0.5 - stai) * anx_MB, 0, 1).
        Thus, higher anxiety (stai>0.5) reduces w; lower anxiety increases w.

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha2, alpha1, beta, w_base, anx_MB = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (rows: A,U; cols: X,Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Initialize values
    q1_mf = np.zeros(2)          # stage-1 MF values for A/U
    q2 = np.ones((2, 2)) * 0.5   # stage-2 values per state (X,Y) and action (0,1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight (fixed across trials for parsimony)
    w = w_base + (0.5 - stai_val) * anx_MB
    w = 0.0 if w < 0.0 else (1.0 if w > 1.0 else w)

    for t in range(n_trials):
        # Model-based stage-1 values via lookahead through transitions
        max_q2 = np.max(q2, axis=1)  # best action per planet
        q1_mb = transition_matrix @ max_q2

        # Hybrid stage-1 values
        q1_mix = (1.0 - w) * q1_mf + w * q1_mb

        # Stage-1 policy
        q1c = q1_mix - np.max(q1_mix)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha2 * pe2

        # Stage-1 MF TD backup from stage-2 value at reached state
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] = q1_mf[a1] + alpha1 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'alpha1', 'beta', 'w_base', 'anx_MB']"
iter1_run0_participant21.json,cognitive_model2,515.5043635885355,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planning with anxiety-suppressed directed exploration (uncertainty bonus) and stage-2 perseveration.

    Stage-2 action values are augmented with an uncertainty bonus (UCB-style) to encourage
    directed exploration. Uncertainty is tracked via an exponential moving estimate of reward
    variance per state-action. Anxiety reduces the exploration bonus. A small perseveration
    bias at stage 2 is stronger when anxiety is higher, capturing anxious tendency to repeat.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning/smoothing rate for both mean and variance
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = kappa_base (0 to 1): baseline weight of uncertainty bonus
    - model_parameters[3] = anx_explore (0 to 1): strength of anxiety suppression of bonus
        Effective bonus weight: kappa_eff = kappa_base * (1 - anx_explore * stai)
    - model_parameters[4] = rho2 (0 to 1): baseline perseveration magnitude at stage 2
        Effective perseveration: stick_eff = rho2 * stai

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha, beta, kappa_base, anx_explore, rho2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Track mean and variance of rewards per state-action
    q2_mean = np.ones((2, 2)) * 0.5
    q2_var = np.ones((2, 2)) * 0.05  # small initial uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    kappa_eff = kappa_base * (1.0 - anx_explore * stai_val)
    if kappa_eff < 0.0:
        kappa_eff = 0.0
    stick_eff = rho2 * stai_val  # more anxious -> more perseveration at stage 2

    prev_a2 = [None, None]  # last action taken in each state (X=0,Y=1)

    for t in range(n_trials):
        # Construct UCB-augmented values
        bonus = kappa_eff * np.sqrt(np.maximum(q2_var, 1e-12))
        q2_ucb = q2_mean + bonus

        # Stage-1 model-based values via UCB-propagated expectations
        max_q2_ucb = np.max(q2_ucb, axis=1)
        q1_mb = transition_matrix @ max_q2_ucb

        # Stage-1 softmax
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax with perseveration bias in reached state
        s = state[t]
        q_s = q2_ucb[s].copy()
        if prev_a2[s] is not None:
            q_s[prev_a2[s]] += stick_eff
        q2c = q_s - np.max(q_s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning updates from observed reward
        r = reward[t]
        pe = r - q2_mean[s, a2]
        # Mean update
        q2_mean[s, a2] = q2_mean[s, a2] + alpha * pe
        # Variance update (EWMA of squared error around updated mean)
        # Use one-step approximation: v <- (1-alpha)*v + alpha*(pe^2)
        q2_var[s, a2] = (1.0 - alpha) * q2_var[s, a2] + alpha * (pe ** 2)

        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'kappa_base', 'anx_explore', 'rho2']"
iter1_run0_participant21.json,cognitive_model3,525.5263064157757,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Planning with anxiety-accelerated transition learning and reward forgetting.

    The agent learns the transition model between stage 1 and stage 2 with a delta rule.
    Anxiety increases the transition learning rate, capturing heightened sensitivity to
    environmental changes. Stage-2 reward values are subject to forgetting toward 0.5,
    with forgetting mildly increased by anxiety. Planning uses the learned transition matrix.

    Parameters (bounds):
    - model_parameters[0] = alpha_r (0 to 1): learning rate for stage-2 rewards
    - model_parameters[1] = beta (0 to 10): inverse temperature at both stages
    - model_parameters[2] = alpha_T_base (0 to 1): baseline transition learning rate
    - model_parameters[3] = anx_T (0 to 1): strength of anxiety modulation on transitions and forgetting
        Effective transition LR: alpha_T = clip(alpha_T_base * (1 + anx_T * stai), 0, 1)
        Effective forgetting: d_eff = clip(d_base * (1 + 0.5 * anx_T * stai), 0, 1)
    - model_parameters[4] = d_base (0 to 1): baseline forgetting rate of stage-2 values toward 0.5

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha_r, beta, alpha_T_base, anx_T, d_base = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition model close to canonical structure but learnable
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Stage-2 values
    q2 = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated rates
    alpha_T = alpha_T_base * (1.0 + anx_T * stai_val)
    if alpha_T > 1.0:
        alpha_T = 1.0
    d_eff = d_base * (1.0 + 0.5 * anx_T * stai_val)
    if d_eff > 1.0:
        d_eff = 1.0

    for t in range(n_trials):
        # Planning with learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward learning with forgetting toward 0.5 baseline
        r = reward[t]
        # Apply forgetting to all state-action values each trial
        q2 = (1.0 - d_eff) * q2 + d_eff * 0.5
        # Then update the visited state-action with reward PE
        pe2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha_r * pe2

        # Transition learning: move T[a1] toward the observed next state s
        # One-hot target distribution for observed transition
        for next_s in range(2):
            target = 1.0 if next_s == s else 0.0
            T[a1, next_s] = T[a1, next_s] + alpha_T * (target - T[a1, next_s])

        # Renormalize to avoid numeric drift
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum
        else:
            T[a1] = np.array([0.5, 0.5])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta', 'alpha_T_base', 'anx_T', 'd_base']"
iter1_run0_participant22.json,cognitive_model2,431.3567440330444,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with directed exploration bonus (UCB-style) modulated by anxiety, plus value decay.
    
    The agent is primarily model-free. At stage 2, it uses an uncertainty-driven exploration
    bonus that decreases with experience; higher anxiety reduces the magnitude of this bonus.
    Q-values decay toward a neutral prior to capture nonstationarity.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for Q updates.
    - beta: [0,10] inverse temperature for softmax.
    - bonus0: [0,1] baseline exploration bonus magnitude.
    - anx_bonus: [0,1] scales how much anxiety reduces the bonus: bonus_eff = bonus0*(1 - anx_bonus*stai).
    - rho: [0,1] per-trial decay toward 0.5 for all Q-values (captures drift/nonstationarity).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship each trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien on the reached planet.
    - reward: array-like of floats in [0,1], coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, bonus0, anx_bonus, rho].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, bonus0, anx_bonus, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective directed exploration bonus magnitude
    bonus_eff = bonus0 * (1.0 - anx_bonus * stai)
    bonus_eff = min(1.0, max(0.0, bonus_eff))

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Visit counts for UCB-like uncertainty bonus at stage 2
    n2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 policy (pure MF)
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with UCB-style bonus decreasing with visits
        bonus_vec = np.zeros(2)
        for a in range(2):
            bonus_vec[a] = bonus_eff / np.sqrt(n2[s, a] + 1.0)
        logits2 = beta * (q2[s, :] + bonus_vec)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2
        n2[s, a2] += 1.0

        # Stage-1 TD bootstrapping on stage-2 chosen value
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        q1[a1] += alpha * delta1

        # Per-trial decay toward neutral value 0.5 to capture drifting environment
        if rho > 0:
            q2 = (1.0 - rho) * q2 + rho * 0.5
            q1 = (1.0 - rho) * q1 + rho * 0.5

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'bonus0', 'anx_bonus', 'rho']"
iter1_run0_participant23.json,cognitive_model1,373.59704789296427,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Trust-weighted model-based planning with anxiety-modulated lapse and eligibility.

    Idea
    ----
    The agent relies on a trusted transition model to plan at stage 1, but anxiety
    reduces this trust and introduces a uniform/noisy assumption over transitions.
    The same trust also serves as the model-based arbitration weight (higher trust
    -> more model-based). Choices include an anxiety-driven lapse. Stage-2 values
    are learned with TD; stage-1 model-free values are updated via an eligibility
    trace scaled by eta. All parameters are used and bounded.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int in {0,1}
        Observed second-stage state (0 = planet X, 1 = planet Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices within the visited state (0/1 = the two aliens).
    reward : array-like of float
        Reward on each trial (e.g., 0 or 1).
    stai : array-like of float in [0,1]
        Anxiety score; only stai[0] is used.
    model_parameters : list/tuple of floats
        [alpha2, beta, trust0, lapse0, eta]
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - trust0 in [0,1]: baseline trust in the known transition model (reduced by anxiety).
        - lapse0 in [0,1]: baseline lapse probability (increased by anxiety).
        - eta in [0,1]: eligibility strength to update stage-1 MF from stage-2 value.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha2, beta, trust0, lapse0, eta = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known common transitions: A->X, U->Y (0.7 common, 0.3 rare)
    true_T = np.array([[0.7, 0.3],
                       [0.3, 0.7]])  # rows: action A/U, cols: state X/Y

    # Anxiety reduces trust in the true transition model and increases lapse
    trust = max(0.0, min(1.0, trust0 * (1.0 - stai)))
    lapse = max(0.0, min(1.0, lapse0 * stai))

    # Effective transitions are a blend of true and uniform (agnostic) due to reduced trust
    uniform_T = np.full_like(true_T, 0.5, dtype=float)
    eff_T = trust * true_T + (1.0 - trust) * uniform_T

    # Stage values
    q1_mf = np.zeros(2)        # First-stage model-free Q
    q2_mf = np.zeros((2, 2))   # Second-stage model-free Q: state x action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation via effective transitions
        max_q2 = np.max(q2_mf, axis=1)     # best action per state
        q1_mb = eff_T @ max_q2             # plan over transitions

        # Arbitration: trust also sets the MB weight
        q1_hybrid = trust * q1_mb + (1.0 - trust) * q1_mf

        # First-stage policy with lapse
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 /= np.sum(soft1)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        p_choice_1[t] = probs1[a1]

        # Second-stage policy with lapse
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 /= np.sum(soft2)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning: second stage TD
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha2 * delta2

        # Eligibility-like update for first-stage MF from realized second-stage value
        backup = q2_mf[s, a2]
        delta1 = backup - q1_mf[a1]
        q1_mf[a1] += eta * alpha2 * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha2', 'beta', 'trust0', 'lapse0', 'eta']"
iter1_run0_participant23.json,cognitive_model2,388.5783230575961,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric affective learning with anxiety-modulated negativity bias and confirmation.

    Idea
    ----
    Second-stage learning uses separate learning rates for positive vs. negative
    prediction errors, with anxiety amplifying negative learning and attenuating
    positive learning (negativity bias). A confirmation bias further scales updates:
    when the outcome's sign matches the pre-existing choice advantage, the learning
    rate is increased; otherwise decreased. Stage 1 is model-free and backed up
    via an eligibility term proportional to psi, which also serves as reward
    sensitivity at stage 2.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices.
    state : array-like of int in {0,1}
        Second-stage state per trial.
    action_2 : array-like of int in {0,1}
        Second-stage actions per state.
    reward : array-like of float
        Trial rewards (e.g., 0/1).
    stai : array-like of float in [0,1]
        Anxiety score; only stai[0] is used.
    model_parameters : list/tuple of floats
        [alpha_pos, alpha_neg, beta, psi0, conf0]
        - alpha_pos in [0,1]: base LR for positive PEs (reduced by anxiety).
        - alpha_neg in [0,1]: base LR for negative PEs (increased by anxiety).
        - beta in [0,10]: inverse temperature for both stages.
        - psi0 in [0,1]: reward sensitivity and eligibility strength (reduced by anxiety).
        - conf0 in [0,1]: baseline confirmation bias magnitude (reduced by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_pos, alpha_neg, beta, psi0, conf0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety modulation
    a_pos = max(0.0, min(1.0, alpha_pos * (1.0 - 0.5 * stai)))
    a_neg = max(0.0, min(1.0, alpha_neg * (1.0 + 0.5 * stai)))
    psi = max(0.0, min(1.0, psi0 * (1.0 - stai)))
    conf = max(0.0, min(1.0, conf0 * (1.0 - stai)))

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage choice: pure MF
        logits1 = beta * q1_mf
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        p_choice_1[t] = probs1[a1]

        # Second-stage choice: MF with reward sensitivity in valuation (implicit via learning)
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Effective reward scaled by psi
        r_eff = psi * r

        # Compute PE and sign of prior choice advantage
        chosen_val = q2_mf[s, a2]
        unchosen_val = q2_mf[s, 1 - a2]
        delta2 = r_eff - chosen_val

        # Confirmation factor: if PE sign matches choice advantage sign, amplify; else dampen
        advantage_sign = 0.0
        if chosen_val != unchosen_val:
            advantage_sign = np.sign(chosen_val - unchosen_val)
        pe_sign = np.sign(delta2) if delta2 != 0 else 0.0
        same_sign = (advantage_sign != 0.0) and (pe_sign == advantage_sign)

        conf_scale = (1.0 + conf) if same_sign else (1.0 - conf)

        # Asymmetric LR by PE sign
        lr2 = a_pos if delta2 >= 0.0 else a_neg
        lr2 *= conf_scale
        lr2 = max(0.0, min(1.0, lr2))

        # Update second-stage Q
        q2_mf[s, a2] += lr2 * delta2

        # Eligibility-like backup to stage 1 (scaled by psi)
        backup = q2_mf[s, a2]
        delta1 = backup - q1_mf[a1]
        q1_mf[a1] += psi * lr2 * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_pos', 'alpha_neg', 'beta', 'psi0', 'conf0']"
iter1_run0_participant23.json,cognitive_model3,334.3327949072485,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned transitions with UCB exploration and anxiety-modulated forgetting.

    Idea
    ----
    The agent learns the stage-1 transition probabilities from experience via
    simple Dirichlet counts. Stage-2 action selection includes an uncertainty
    bonus (UCB-style) based on visit counts, with anxiety reducing exploratory
    bonus. First-stage action values combine learned model-based values and
    model-free values; anxiety reduces the model-based blend. Q-values decay
    toward 0 each trial (forgetting), which is stronger under higher anxiety.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices.
    state : array-like of int in {0,1}
        Second-stage state per trial.
    action_2 : array-like of int in {0,1}
        Second-stage actions.
    reward : array-like of float
        Rewards per trial.
    stai : array-like of float in [0,1]
        Anxiety score; only stai[0] is used.
    model_parameters : list/tuple of floats
        [alpha, beta, zeta0, phi0, forget0]
        - alpha in [0,1]: learning rate for Q-values at both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - zeta0 in [0,1]: baseline exploration bonus scale (reduced by anxiety).
        - phi0 in [0,1]: baseline model-based weight at stage 1 (reduced by anxiety).
        - forget0 in [0,1]: baseline per-trial forgetting rate (increased by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, zeta0, phi0, forget0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety modulations
    zeta = max(0.0, min(1.0, zeta0 * (1.0 - stai)))            # less exploration with anxiety
    phi = max(0.0, min(1.0, phi0 * (1.0 - 0.5 * stai)))        # less MB weighting with anxiety
    forget = max(0.0, min(1.0, forget0 * (0.5 + 0.5 * stai)))  # more forgetting with anxiety

    # Initialize learned transitions with symmetric Dirichlet(1,1) priors
    trans_counts = np.ones((2, 2))  # rows: a1 in {0,1}, cols: state in {0,1}
    # Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))
    # Visit counts for UCB at stage 2 (avoid div by zero by starting at 1)
    visit_counts = np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Apply forgetting toward zero
        q1_mf *= (1.0 - forget)
        q2_mf *= (1.0 - forget)

        # Compute learned transition probabilities
        trans_probs = trans_counts / np.sum(trans_counts, axis=1, keepdims=True)  # shape (2,2)

        # Stage-1 MB value from learned transitions and current Q2
        max_q2 = np.max(q2_mf, axis=1)           # best per state
        q1_mb = trans_probs @ max_q2

        # Hybrid value
        q1_hybrid = phi * q1_mb + (1.0 - phi) * q1_mf

        # First-stage policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        p_choice_1[t] = probs1[a1]

        # Stage-2 UCB bonus based on inverse sqrt of visit counts
        bonus = zeta / np.sqrt(visit_counts[s] + 1e-8)
        logits2 = beta * (q2_mf[s] + bonus)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Update counts for UCB
        visit_counts[s, a2] += 1.0

        # Learning: second stage TD
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Eligibility-like backup to stage 1 MF
        backup = q2_mf[s, a2]
        delta1 = backup - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update learned transition counts with observed transition
        trans_counts[a1, s] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'zeta0', 'phi0', 'forget0']"
iter1_run0_participant24.json,cognitive_model1,465.6620651619138,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with learned transition model, entropy-driven exploration, and anxiety-weighted arbitration.
    
    This model learns:
      - Model-free (MF) Q-values at both stages from rewards.
      - A model-based (MB) first-stage value via a learned transition matrix P(s'|a1).
    First-stage choice uses a hybrid of MB and MF values. Arbitration weight (MB share) decreases with anxiety.
    Additionally, an entropy bonus encourages exploration of first-stage actions whose transition models are uncertain;
    this bonus increases with anxiety.
    
    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float in [0,1]
        Reward outcome on each trial.
    stai : array-like of float in [0,1]
        Trait anxiety score; stai[0] is used.
    model_parameters : array-like of 5 floats
        [alpha_mf, alpha_tr, beta, w0, phi_surprise]
        - alpha_mf in [0,1]: MF learning rate for both stages.
        - alpha_tr in [0,1]: learning rate for the transition model P(s'|a1).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w0 in [0,1]: baseline MB weight (share of MB in hybrid).
        - phi_surprise in [0,1]: scale of the entropy-driven exploration bonus at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_mf, alpha_tr, beta, w0, phi_surprise = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Values
    q1_mf = np.zeros(2)          # MF first-stage values
    q2 = np.zeros((2, 2))        # MF second-stage values

    # Learned transition model P(s'|a1); initialize as uninformative (0.5, 0.5)
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage values via learned transition model
        max_q2_per_state = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2_per_state           # shape (2,)

        # Anxiety-weighted arbitration: higher anxiety shifts weight away from MB
        w_mb = (1.0 - s) * w0 + s * 0.2  # floor MB weight to 0.2 when highly anxious
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Entropy-driven exploration bonus over actions (uncertainty in transitions)
        # H(p) = -sum p log p for the two-state transition row of each action
        bonus = np.zeros(2)
        for a in range(2):
            p = np.clip(T[a], 1e-12, 1 - 1e-12)
            H = -(p[0] * np.log(p[0]) + p[1] * np.log(p[1]))
            bonus[a] = phi_surprise * (1.0 + s) * H

        # First-stage policy
        prefs1 = q1_hybrid + bonus
        prefs1 -= np.max(prefs1)
        probs1 = np.exp(beta * prefs1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        st = state[t]
        prefs2 = q2[st].copy()
        prefs2 -= np.max(prefs2)
        probs2 = np.exp(beta * prefs2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # MF learning at stage 2 from reward
        r = reward[t]
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_mf * pe2

        # MF bootstrapping to stage 1 using the obtained second-stage value (SARSA(0))
        pe1 = q2[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_mf * pe1

        # Update transition model for the chosen first-stage action toward observed state
        target = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        T[a1] += alpha_tr * (target - T[a1])

        # Keep rows normalized and within [eps, 1-eps] for stability
        for a in range(2):
            row = np.clip(T[a], 1e-6, 1.0)
            row_sum = np.sum(row)
            if row_sum > 0:
                T[a] = row / row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_mf', 'alpha_tr', 'beta', 'w0', 'phi_surprise']"
iter1_run0_participant24.json,cognitive_model2,452.069168280703,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Utility-transformed MF with eligibility trace modulated by transition rarity and anxiety, plus reward-contingent stickiness.
    
    This is a pure model-free controller that:
      - Applies a concave utility transform to reward (risk sensitivity).
      - Uses an eligibility trace to update first-stage values from second-stage outcomes.
      - Reduces credit assignment after rare transitions, more strongly under higher anxiety.
      - Adds a reward-contingent perseveration bias at stage 1 that grows with anxiety.
    
    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the visited state.
    reward : array-like of float in [0,1]
        Received reward.
    stai : array-like of float in [0,1]
        Trait anxiety score; stai[0] is used.
    model_parameters : array-like of 5 floats
        [alpha, beta, rho0, lam, zeta0]
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - rho0 in [0,1]: baseline risk sensitivity; higher yields more concavity via u = r^(1 - rho).
        - lam in [0,1]: eligibility trace strength for crediting stage-1 after stage-2 outcomes.
        - zeta0 in [0,1]: baseline reward-contingent perseveration at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices over both stages.
    """"""
    alpha, beta, rho0, lam, zeta0 = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Risk sensitivity increases with anxiety
    rho = rho0 + s * (1.0 - rho0)  # moves toward 1 as anxiety increases

    # Reward-contingent perseveration strength increases with anxiety
    zeta = zeta0 * (0.5 + s)  # min 0.5*zeta0 at low anxiety, up to ~1.5*zeta0 at high anxiety

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_reward = 0.0
    last_common = True

    for t in range(n_trials):
        st = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Determine whether the current transition is common or rare given task structure
        # Common: A->X (0->0), U->Y (1->1)
        is_common = (a1 == st)

        # Reward-contingent perseveration bias at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None and last_common and last_reward > 0.0:
            bias1[last_a1] += zeta

        # Stage 1 policy
        prefs1 = q1 + bias1
        prefs1 -= np.max(prefs1)
        probs1 = np.exp(beta * prefs1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        prefs2 = q2[st].copy()
        prefs2 -= np.max(prefs2)
        probs2 = np.exp(beta * prefs2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward
        r = reward[t]
        u = r ** (1.0 - rho)

        # Stage 2 MF update
        pe2 = u - q2[st, a2]
        q2[st, a2] += alpha * pe2

        # Eligibility trace from stage 2 to stage 1.
        # After rare transitions, reduce credit assignment; anxiety amplifies this reduction.
        # eff_lam = lam for common; for rare: lam * (1 - 0.5*s)
        eff_lam = lam if is_common else lam * (1.0 - 0.5 * s)
        pe1 = q2[st, a2] - q1[a1]
        q1[a1] += alpha * eff_lam * pe1

        # Book-keeping for next-trial stickiness
        last_a1 = a1
        last_reward = r
        last_common = is_common

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'rho0', 'lam', 'zeta0']"
iter1_run0_participant24.json,cognitive_model3,461.727618458443,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive learning-rate MF with MB hybrid, driven by uncertainty and anxiety.
    
    This model combines:
      - MF Q-learning at stage 2 with an adaptive learning rate that increases with recent uncertainty (|PE|).
      - MB evaluation at stage 1 using a fixed known transition matrix (common=0.7).
      - Hybrid MB/MF arbitration where anxiety reduces MB reliance.
      - Uncertainty bonus at stage 2 to encourage exploration of uncertain aliens, stronger under anxiety.
    
    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float in [0,1]
        Rewards per trial.
    stai : array-like of float in [0,1]
        Trait anxiety score; stai[0] is used.
    model_parameters : array-like of 5 floats
        [alpha0, beta, kappa, w_mb0, xi_unc]
        - alpha0 in [0,1]: base MF learning rate.
        - beta in [0,10]: inverse temperature for softmax.
        - kappa in [0,1]: uncertainty adaptation rate; also caps contribution to learning-rate modulation.
        - w_mb0 in [0,1]: baseline MB weight in stage-1 hybrid.
        - xi_unc in [0,1]: base scale of uncertainty-driven exploration at stage 2.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha0, beta, kappa, w_mb0, xi_unc = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition matrix (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and uncertainty trackers
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    uncert = np.zeros((2, 2))  # running estimate of uncertainty per state-action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB evaluation for stage 1
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T @ max_q2

        # Anxiety reduces MB reliance
        w_mb = w_mb0 * (1.0 - s) + 0.1 * s
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy
        prefs1 = q1_hybrid.copy()
        prefs1 -= np.max(prefs1)
        probs1 = np.exp(beta * prefs1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with uncertainty-driven exploration
        st = state[t]
        u_bonus_scale = xi_unc * (1.0 + s)  # stronger exploration under anxiety
        prefs2 = q2[st] + u_bonus_scale * uncert[st]
        prefs2 -= np.max(prefs2)
        probs2 = np.exp(beta * prefs2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Adaptive MF learning at stage 2
        pe2 = r - q2[st, a2]
        # Update uncertainty as a leaky average of absolute PE
        uncert[st, a2] = (1.0 - kappa) * uncert[st, a2] + kappa * abs(pe2)
        # Learning rate increases with uncertainty and anxiety
        eff_alpha = np.clip(alpha0 + s * uncert[st, a2], 0.0, 1.0)
        q2[st, a2] += eff_alpha * pe2

        # MF update at stage 1 via bootstrapping
        pe1 = q2[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha0 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha0', 'beta', 'kappa', 'w_mb0', 'xi_unc']"
iter1_run0_participant28.json,cognitive_model1,459.5356313479472,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with learned transition model and anxiety-modulated arbitration.
    
    This model learns both stage-2 action values and the stage-1 transition probabilities.
    First-stage decisions combine model-free and model-based values. The arbitration weight
    on the model-based controller increases or decreases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = alien within the current planet).
    reward : array-like of float in [0,1]
        Reward outcome on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; modulates the arbitration weight toward model-based control.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, w0, eta_T, phi]
        - alpha (learning rate for Q-values; [0,1])
        - beta (inverse temperature; [0,10])
        - w0 (baseline model-based weight; [0,1])
        - eta_T (transition learning rate; [0,1])
        - phi (anxiety modulation strength on model-based weight; [0,1])
          effective weight w = sigmoid(logit(w0) + phi * (stai - 0.5))
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w0, eta_T, phi = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition model rows for actions 0 and 1
    # Start centered on the canonical common/rare structure but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Initialize values
    q1_mf = np.zeros(2)          # model-free first-stage values
    q2 = np.zeros((2, 2))        # second-stage action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper: stable softmax
    eps = 1e-10

    # Anxiety-modulated arbitration weight
    # Map w0 from [0,1] through logit-sigmoid to keep interior and allow smooth modulation
    w0_clipped = min(max(w0, 1e-6), 1 - 1e-6)
    logit_w0 = np.log(w0_clipped) - np.log(1 - w0_clipped)
    w_eff = 1.0 / (1.0 + np.exp(-(logit_w0 + phi * (st - 0.5))))
    w_eff = min(max(w_eff, 0.0), 1.0)

    for t in range(n_trials):
        # Compute model-based first-stage values from current transition model and max second-stage values
        max_q2 = np.max(q2, axis=1)             # shape (2,)
        q1_mb = T @ max_q2                      # propagate through learned transitions

        # Hybrid first-stage preference
        q1_hybrid = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # First-stage choice probability via softmax
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy in the observed state
        s = int(state[t])
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: second-stage TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Model-free first-stage TD update bootstrapping from updated second-stage value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Learn transition probabilities from observed (a1, s)
        # Move row T[a1] toward the one-hot vector for the observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1 - eta_T) * T[a1] + eta_T * target

        # Renormalize to ensure probabilities sum to 1 (avoid drift due to numerical precision)
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'w0', 'eta_T', 'phi']"
iter1_run0_participant28.json,cognitive_model3,450.8507012856297,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-driven exploration (UCB-like) with anxiety-damped exploration and hybrid control.
    
    The agent learns second-stage values and an uncertainty trace per action.
    Choices include an exploration bonus proportional to uncertainty. Anxiety
    suppresses the exploration bonus. First-stage policies combine model-based
    (propagated value+uncertainty) and model-free values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Reward outcomes.
    stai : array-like with one float in [0,1]
        Anxiety score; reduces the exploration bonus.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, w, nu0, xi]
        - alpha (learning rate for values and uncertainty; [0,1])
        - beta (inverse temperature; [0,10])
        - w (weight on model-based vs model-free at stage 1; [0,1])
        - nu0 (baseline exploration-bonus weight; [0,1])
        - xi (anxiety suppression of exploration; [0,1])
          effective bonus weight nu = nu0 * (1 - xi * stai)
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, w, nu0, xi = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)            # model-free stage-1 values
    q2 = np.zeros((2, 2))          # stage-2 values

    # Uncertainty traces for second-stage actions (initialized high)
    u2 = np.ones((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # Anxiety-damped exploration bonus weight
    nu = nu0 * (1.0 - xi * st)
    nu = min(max(nu, 0.0), 1.0)

    for t in range(n_trials):
        # Compute exploration-augmented second-stage preferences
        aug_q2 = q2 + nu * u2
        # Model-based first-stage values by propagating best augmented values per state
        max_aug_q2 = np.max(aug_q2, axis=1)
        q1_mb = T @ max_aug_q2

        # Hybrid first-stage value
        q1_hybrid = (1.0 - w) * q1_mf + w * q1_mb

        # First-stage softmax
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage softmax in current state using augmented preferences
        s = int(state[t])
        pref2 = aug_q2[s].copy()
        pref2c = pref2 - np.max(pref2)
        probs2 = np.exp(beta * pref2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second-stage value update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update uncertainty: decay toward recent unpredictability via absolute PE
        u2[s, a2] = (1.0 - alpha) * u2[s, a2] + alpha * abs(pe2)

        # Model-free first-stage update bootstrapping from value at second stage (without bonus)
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik","['alpha', 'beta', 'w', 'nu0', 'xi']"
iter1_run0_participant29.json,cognitive_model1,460.38068808911015,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated model-based control and eligibility trace.
    The agent uses both model-based (MB) and model-free (MF) values at the first stage.
    Second-stage values are updated with TD learning; a one-step eligibility trace
    propagates second-stage values to the chosen first-stage action.
    Anxiety reduces the reliance on model-based control.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score (higher means more anxious).
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for MF updates at both stages
        beta: [0,10] â inverse temperature used at both stages
        w_mb0: [0,1] â baseline weight on model-based control at stage 1
        elig: [0,1] â eligibility trace weight for backing up stage-2 value to stage-1
        anx_mb_shift: [0,1] â how strongly anxiety reduces model-based weight

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    alpha, beta, w_mb0, elig, anx_mb_shift = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # from A to [X, Y]
                  [0.3, 0.7]]) # from U to [X, Y]

    # Values
    q1_mf = np.zeros(2)       # model-free first-stage action values
    q2 = np.zeros((2, 2))     # second-stage state-action values: state in {X=0, Y=1}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight (higher anxiety => lower MB control)
    # Effective MB weight clipped to [0,1]
    w_mb = np.clip(w_mb0 - anx_mb_shift * stai_val, 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):

        # Compute model-based first-stage action values by planning through T and max over q2
        max_q2 = np.max(q2, axis=1)           # shape (2,)
        q1_mb = T @ max_q2                    # shape (2,)

        # Mix MF and MB for first-stage decision
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in the reached state
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Back up to stage 1:
        # - MF TD toward realized stage-2 action value (eligibility trace controls strength)
        # - We update only the chosen first-stage action
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * elig * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'w_mb0', 'elig', 'anx_mb_shift']"
iter1_run0_participant29.json,cognitive_model2,536.6515629892036,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planning with anxiety-inflated associability and exploration temperature.
    The agent plans at stage 1 via the known transition model and max second-stage values.
    Second-stage learning uses a Pearce-Hall style dynamic learning rate (associability)
    proportional to recent unsigned prediction errors. Anxiety increases associability
    (faster adaptation to changing rewards) and increases exploration (lower effective beta).

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha0: [0,1] â baseline learning rate
        beta0: [0,10] â baseline inverse temperature
        eta: [0,1] â associability gain (scales effect of unsigned PE on learning rate)
        tau0: [0,1] â exploration noise scale (reduces beta)
        anx_temp: [0,1] â anxiety coupling to both associability and exploration

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    alpha0, beta0, eta, tau0, anx_temp = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and associability traces (per state-action)
    q2 = np.zeros((2, 2))
    assoc = np.zeros((2, 2))  # running estimate of unsigned PEs

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated temperature: higher anxiety => more exploration (lower beta)
    temp_scale = 1.0 + tau0 * (1.0 + anx_temp * stai_val)
    beta_eff = beta0 / max(temp_scale, 1e-6)

    eps = 1e-12
    for t in range(n_trials):
        # Stage-1 planning via MB values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        logits1 = beta_eff * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Dynamic learning rate with anxiety-inflated associability
        pe2 = r - q2[s, a2]
        assoc[s, a2] = 0.5 * assoc[s, a2] + 0.5 * abs(pe2)  # smooth recent unsigned PE
        lr_dyn = alpha0 + eta * assoc[s, a2]
        # Anxiety increases learning responsiveness
        lr_eff = np.clip(lr_dyn * (1.0 + anx_temp * stai_val), 0.0, 1.0)

        q2[s, a2] += lr_eff * pe2
        # Note: stage-1 has no MF component; choice remains fully MB via q1_mb

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha0', 'beta0', 'eta', 'tau0', 'anx_temp']"
iter1_run0_participant29.json,cognitive_model3,389.27593551399053,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning MB with anxiety-sensitive stay/switch bias.
    The agent learns the transition probabilities online and plans model-based at stage 1.
    In addition, a choice bias encourages repeating the previous first-stage action;
    this stay bias is attenuated or reversed following rare transitions in proportion
    to anxiety, capturing an anxiety-sensitive response to surprising transitions.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for second-stage action values
        beta: [0,10] â inverse temperature for both stages
        trans_lr: [0,1] â learning rate for transition probabilities
        stay_bias: [0,1] â baseline logit bonus to repeat previous first-stage choice
        anx_rare: [0,1] â strength of anxiety-driven reversal of stay bias after rare transitions

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    alpha, beta, trans_lr, stay_bias, anx_rare = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix T_hat: rows are actions (A=0,U=1), cols are states (X=0,Y=1)
    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_was_rare = False  # rare relative to current T_hat at the time

    eps = 1e-12
    for t in range(n_trials):
        # Model-based first-stage values using learned transitions
        max_q2 = np.max(q2, axis=1)     # best action in each state
        q1_mb = T_hat @ max_q2

        # Construct first-stage logits with stay/switch bias
        logits1 = q1_mb.copy()

        # Apply stay bias on the previously chosen action.
        # After a rare transition, anxiety flips or attenuates the stay bias.
        if last_a1 is not None:
            # Effective bias multiplier: if last was rare, reduce or flip sign depending on anx*anx_rare
            rare_mod = 1.0 - 2.0 * (1.0 * last_was_rare) * (anx_rare * stai_val)
            # rare_mod in [1 - 2*anx_rare, 1]; at max anx_rare=1, stai=1 => rare_mod=-1 (full flip)
            bias_to_add = stay_bias * rare_mod
            logits1[last_a1] += bias_to_add

        # Softmax for stage 1
        logits1_s = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(logits1_s)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update learned transition probabilities for the chosen first-stage action
        # One-hot observation for the realized state
        obs = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        # Delta rule toward the observed state
        T_hat[a1] = T_hat[a1] + trans_lr * (obs - T_hat[a1])
        # Renormalize to avoid numerical drift
        row_sum = np.sum(T_hat[a1])
        if row_sum > 0:
            T_hat[a1] /= row_sum

        # Determine rarity of the observed transition under the pre-update T_hat (approximate):
        # We use the current T_hat after update as a proxy; for small trans_lr this is close.
        prob_to_s = T_hat[a1, s]
        last_was_rare = (prob_to_s < 0.5)
        last_a1 = a1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha', 'beta', 'trans_lr', 'stay_bias', 'anx_rare']"
iter1_run0_participant3.json,cognitive_model1,396.30987464576447,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-gated arbitration with eligibility trace and anxiety-modulated gating.
    
    Idea:
    - Stage-2 values are learned with a constant learning rate.
    - Stage-1 mixes model-free and model-based values. The arbitration weight shifts toward
      model-free when estimated volatility (unsigned RPE) is high; anxiety amplifies this shift.
    - An eligibility trace propagates stage-2 prediction errors back to stage-1 MF values.
    - Perseveration bias affects both stages.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: int array {0,1}, first-stage choices
    - state:    int array {0,1}, reached second-stage state
    - action_2: int array {0,1}, second-stage choices
    - reward:   float array [0,1], reward outcomes
    - stai:     float array [0,1] with single element, anxiety score
    - model_parameters: list/tuple of five values:
        eta    (0..1): stage-2 learning rate
        beta   (0..10): inverse temperature
        rho    (0..1): eligibility trace from stage-2 to stage-1 MF
        kappa  (0..1): perseveration strength
        chi    (0..1): anxiety sensitivity of arbitration to volatility
    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """"""
    eta, beta, rho, kappa, chi = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed (true) transition model used for planning
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Likelihood containers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)          # model-free stage-1 action values
    q2 = np.zeros((2, 2))        # stage-2 state-action values

    # Volatility proxy (EWMA of unsigned stage-2 RPE)
    vol = 0.0
    vol_decay = 0.8  # fixed decay; arbitration sensitivity is governed by chi and anxiety

    # Perseveration memory
    prev_a1 = -1
    prev_a2 = -1

    eps = 1e-12

    for t in range(n_trials):
        # Model-based evaluation from stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Arbitration weight toward model-based control.
        # When volatility is high, weight decreases; anxiety amplifies this effect via chi.
        # Base tendency is 0.7 toward MB; shrink with vol and anxiety.
        w = 0.7 - (vol * (0.5 + 0.5 * s_anx) * chi)
        w = np.clip(w, 0.0, 1.0)

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Perseveration features
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # First-stage policy
        logits1 = beta * q1 + kappa * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Learning
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += eta * delta2

        # Stage-1 MF update via direct TD and eligibility trace from stage-2 RPE
        # Direct TD to move q1_mf toward the value backing the chosen path
        target1 = q2[s_idx, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * delta1
        # Eligibility: propagate a fraction of stage-2 RPE
        q1_mf[a1] += rho * eta * delta2

        # Update volatility proxy (unsigned RPE)
        vol = vol_decay * vol + (1.0 - vol_decay) * abs(delta2)

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta', 'beta', 'rho', 'kappa', 'chi']"
iter1_run0_participant3.json,cognitive_model2,inf,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive utility, transition-dependent choice bias, and lapse; anxiety modulates risk and bias.
    
    Idea:
    - Rewards are transformed by a risk-sensitive utility u(r) = r^gamma_eff.
      Higher anxiety pushes gamma_eff below 1 (more risk-averse for gains).
    - Model-free and model-based are mixed via a fixed weight determined by omega0, with
      anxiety reducing planning weight.
    - Transition-dependent bias: after rewarded common transitions, a first-stage
      stay-bias increases; after rewarded rare transitions, it decreases. Anxiety enhances
      sensitivity to rare events.
    - A small lapse mixes the softmax policy with uniform choice; lapse increases with anxiety.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: int array {0,1}
    - state:    int array {0,1}
    - action_2: int array {0,1}
    - reward:   float array [0,1]
    - stai:     float array [0,1] with one value, anxiety score
    - model_parameters: list/tuple of five values:
        alpha   (0..1): stage-2 learning rate
        beta    (0..10): inverse temperature
        gamma0  (0..1): baseline risk exponent for utility transform
        omega0  (0..1): baseline planning weight (MB vs MF)
        eps0    (0..1): baseline lapse rate
    Returns:
    - Negative log-likelihood over both stages.
    """"""
    alpha, beta, gamma0, omega0, eps0 = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure for planning
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value stores
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Bias state
    prev_a1 = -1
    prev_trans_common = 0
    prev_reward = 0.0

    # Anxiety effects:
    # - Risk exponent shifts down with anxiety (more curvature for gains)
    gamma_eff = np.clip(gamma0 * (1.0 - 0.5 * s_anx), 0.0, 1.0)
    # - Planning weight reduced by anxiety
    w_plan = np.clip(omega0 * (1.0 - 0.7 * s_anx), 0.0, 1.0)
    # - Lapse increases with anxiety
    lapse = np.clip(eps0 * (0.5 + 0.5 * s_anx), 0.0, 1.0)
    eps = 1e-12

    for t in range(n_trials):
        # Current MB estimate
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Transition-dependent stay bias term for first-stage
        # If last trial was rewarded and common, bias toward staying; if rewarded and rare,
        # bias against staying. Anxiety increases the magnitude when rare.
        bias_vec = np.zeros(2)
        if prev_a1 in (0, 1):
            # Compute signed bias magnitude
            sign = 1.0 if prev_trans_common == 1 else -1.0
            # Rare events amplified by anxiety
            amp = 1.0 + s_anx if prev_trans_common == 0 else 1.0 - 0.5 * s_anx
            bias_mag = prev_reward * sign * amp
            bias_vec[prev_a1] = bias_mag  # apply to the previously chosen action

        # Combine MB and MF
        q1 = w_plan * q1_mb + (1.0 - w_plan) * q1_mf

        # First-stage softmax with bias
        logits1 = beta * q1 + bias_vec
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        # Lapse mixture with uniform
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s_idx = state[t]
        logits2 = beta * q2[s_idx]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning with risk-sensitive utility
        r_raw = reward[t]
        r_util = r_raw ** (gamma_eff + eps)  # utility-transformed reward
        delta2 = r_util - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

        # Stage-1 MF bootstrapping toward current stage-2 value
        target1 = q2[s_idx, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update bias state for next trial
        # Determine whether the last transition was common given choice and reached state
        # Common if (A->X) or (U->Y)
        is_common = 1 if ((a1 == 0 and s_idx == 0) or (a1 == 1 and s_idx == 1)) else 0
        prev_trans_common = is_common
        prev_reward = r_raw
        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'gamma0', 'omega0', 'eps0']"
iter1_run0_participant3.json,cognitive_model3,477.6002535124902,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-representation planning with learned transitions; anxiety reduces temporal horizon and transition learning.
    
    Idea:
    - Learn an explicit estimate of action-dependent transition probabilities (p_hat).
    - Use a Successor Representation (SR) at stage 1: Q1_SR[a] = sum_s M[a,s] * V2[s],
      where M[a,s] is the (one-step) successor feature based on learned transitions and discount gamma.
    - Anxiety reduces the effective discount (shorter horizon) and slows transition learning,
      reflecting reduced confidence in learning the structure.
    - Include perseveration at both stages.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: int array {0,1}
    - state:    int array {0,1}
    - action_2: int array {0,1}
    - reward:   float array [0,1]
    - stai:     float array [0,1] with one value, anxiety score
    - model_parameters: list/tuple of five values:
        alpha_r (0..1): stage-2 reward learning rate
        beta    (0..10): inverse temperature
        alpha_T (0..1): transition learning rate (baseline)
        gamma0  (0..1): baseline SR discount factor
        psi     (0..1): anxiety sensitivity scaling for gamma and transition learning
    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha_r, beta, alpha_T, gamma0, psi = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Anxiety effects:
    # - Reduce effective discount (more myopic)
    gamma_eff = np.clip(gamma0 * (1.0 - psi * s_anx), 0.0, 1.0)
    # - Reduce transition learning under anxiety
    alpha_T_eff = np.clip(alpha_T * (1.0 - 0.7 * psi * s_anx), 0.0, 1.0)

    # Initialize learned transition probabilities p_hat[action, state]
    # Start near symmetric but slightly biased toward common structure to aid identifiability
    p_hat = np.array([[0.6, 0.4],
                      [0.4, 0.6]], dtype=float)

    # Value stores
    q2 = np.zeros((2, 2))  # stage-2 state-action values

    # Likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration
    kappa = 0.2 * (0.5 + s_anx)  # small fixed perseveration scaled by anxiety (uses stai)
    prev_a1 = -1
    prev_a2 = -1

    eps = 1e-12

    for t in range(n_trials):
        # Compute V2 from current q2
        V2 = np.max(q2, axis=1)  # shape (2,)

        # SR-like first-stage Q via learned transitions and discount
        # One-step SR for this task: M[a, s] = (1 - gamma_eff) * 0 + gamma_eff * p_hat[a, s]
        # Since there is no immediate reward at stage 1, only future value matters.
        q1_sr = gamma_eff * (p_hat @ V2)

        # Add perseveration
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        logits1 = beta * q1_sr + kappa * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Update stage-2 values
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += alpha_r * delta2

        # Learn transitions p_hat using simple delta rule toward observed state
        # For chosen action a1, move probability mass toward the observed state s_idx.
        for s2 in (0, 1):
            target = 1.0 if s2 == s_idx else 0.0
            p_hat[a1, s2] += alpha_T_eff * (target - p_hat[a1, s2])
        # Normalize to avoid drift
        row_sum = p_hat[a1, 0] + p_hat[a1, 1]
        if row_sum > 0:
            p_hat[a1, :] /= row_sum

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta', 'alpha_T', 'gamma0', 'psi']"
iter1_run0_participant31.json,cognitive_model1,503.6352739994261,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free RL with learned transitions and anxiety-suppressed planning.

    The agent learns second-stage values and also learns the transition structure online.
    First-stage choice values are a convex combination of model-based (using the learned
    transition matrix) and model-free values. Anxiety (stai) reduces the weight placed on
    model-based planning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1; e.g., W/S or P/H).
    reward : array-like of float
        Obtained rewards on each trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety down-weights planning.
    model_parameters : array-like of floats, length 5
        [alpha_mf, alpha_tr, beta, omega_base, k_anx_w]
        Bounds:
        - alpha_mf in [0,1]: learning rate for model-free Q updates.
        - alpha_tr in [0,1]: transition learning rate (for the transition matrix rows).
        - beta in [0,10]: inverse temperature for both stages.
        - omega_base in [0,1]: baseline weight on model-based versus model-free values.
        - k_anx_w in [0,1]: anxiety modulation; effective omega = omega_base * (1 - k_anx_w*stai).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """"""
    alpha_mf, alpha_tr, beta, omega_base, k_anx_w = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective model-based weight reduced by anxiety
    omega_eff = omega_base * (1.0 - k_anx_w * stai)
    if omega_eff < 0.0:
        omega_eff = 0.0
    if omega_eff > 1.0:
        omega_eff = 1.0

    # Initialize learned transition matrix as uniform and learn it
    T = np.full((2, 2), 0.5)

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage action-values computed from learned transitions
        max_q2 = np.max(q2_mf, axis=1)          # best value available on each planet
        q1_mb = T @ max_q2                      # expected value of each spaceship

        # Hybrid first-stage values
        q1_hybrid = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # First-stage policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Update model-free values
        # Stage-1 MF towards the value of the chosen second-stage action
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_mf * delta1

        # Stage-2 MF towards reward
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha_mf * delta2

        # Learn transitions for the chosen spaceship: move its row toward the observed state
        # One-hot target: 1 for observed state, 0 for the other, then renormalize (though update preserves sum)
        for sp in (0, 1):
            if sp == a1:
                # decay toward zero, then add mass to observed state
                T[sp, :] = (1.0 - alpha_tr) * T[sp, :]
                T[sp, s] += alpha_tr
                # numerical cleanup
                row_sum = T[sp, 0] + T[sp, 1]
                if row_sum <= 0:
                    T[sp, :] = 0.5
                else:
                    T[sp, :] /= row_sum
            else:
                # leave unchosen spaceship transition unchanged this trial
                pass

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_mf', 'alpha_tr', 'beta', 'omega_base', 'k_anx_w']"
iter1_run0_participant31.json,cognitive_model2,503.38314109162513,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free RL with anxiety-driven pessimism and a model-based credit assignment (MBCA) stay/switch bias.

    Core values are learned model-free. Anxiety increases pessimism about uncertain second-stage options
    and increases exploration. First-stage decisions are additionally biased by a single-trial
    model-based credit assignment heuristic: after a rewarded common or unrewarded rare transition,
    the model favors staying with the same first-stage action; after a rewarded rare or unrewarded common
    transition, it favors switching. The strength of this heuristic is controlled by mbca.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score; uses stai[0].
    model_parameters : array-like of floats, length 5
        [alpha, beta, psi0, k_anx_beta, mbca]
        Bounds:
        - alpha in [0,1]: learning rate for model-free Q updates at both stages.
        - beta in [0,10]: base inverse temperature for both stages.
        - psi0 in [0,1]: baseline pessimism weight penalizing uncertain second-stage options.
        - k_anx_beta in [0,1]: anxiety modulation; higher stai reduces beta and increases pessimism.
                               beta_eff = beta * (1 - 0.8*k_anx_beta*stai), psi_eff = psi0 + k_anx_beta*stai.
        - mbca in [0,1]: strength of model-based credit assignment stay/switch bias on first-stage choices.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, psi0, k_anx_beta, mbca = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety effects: more exploration and more pessimism
    beta_eff = beta * (1.0 - 0.8 * k_anx_beta * stai)
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0
    psi_eff = psi0 + k_anx_beta * stai
    if psi_eff < 0.0:
        psi_eff = 0.0
    if psi_eff > 1.0:
        psi_eff = 1.0

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        # First-stage policy with MBCA stay/switch bias from previous trial
        bias = np.zeros(2)
        if prev_a1 is not None:
            prev_common = (prev_a1 == 0 and prev_s == 0) or (prev_a1 == 1 and prev_s == 1)
            # Stay preference if (rewarded & common) or (unrewarded & rare); else switch
            stay_pref = 1 if ((prev_common and prev_r > 0.0) or ((not prev_common) and prev_r <= 0.0)) else -1
            if stay_pref == 1:
                bias[prev_a1] += mbca
            else:
                bias[1 - prev_a1] += mbca

        logits1 = beta_eff * q1 + bias
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy with pessimism penalty for uncertainty
        s = state[t]
        q2_row = q2[s].copy()
        # Uncertainty penalty: larger when values are near 0.5
        uncert = 0.5 - np.abs(q2_row - 0.5)
        q2_eff = q2_row - psi_eff * uncert
        logits2 = beta_eff * q2_eff
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Model-free updates
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        prev_a1 = a1
        prev_s = s
        prev_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'psi0', 'k_anx_beta', 'mbca']"
iter1_run0_participant31.json,cognitive_model3,429.64428331714646,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Dual-temperature MF with anxiety-amplified choice kernels and value forgetting.

    This purely model-free account uses separate softmax temperatures for stage 1 and 2,
    adds choice kernels (perseveration) at both stages whose influence scales with anxiety,
    and applies trial-to-trial forgetting of values toward 0.5. This captures increased
    habitual stickiness and reduced reliance on precise values often associated with higher anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Second-stage states encountered.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state.
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety strengthens the choice-kernel influence.
    model_parameters : array-like of floats, length 5
        [alpha, beta1, beta2, k_anx_ck, f_forget]
        Bounds:
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2.
        - k_anx_ck in [0,1]: scales choice-kernel weights with anxiety; kernel weight = k_anx_ck * stai.
        - f_forget in [0,1]: forgetting rate pulling Q-values toward 0.5 each trial
                             and decaying choice kernels (higher => more forgetting/decay).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta1, beta2, k_anx_ck, f_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Value tables
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels (perseveration traces)
    k1 = np.zeros(2)
    k2 = np.zeros((2, 2))

    # Anxiety-scaled kernel weight
    w_ck = k_anx_ck * stai
    if w_ck < 0.0:
        w_ck = 0.0
    if w_ck > 1.0:
        w_ck = 1.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Forgetting pulls Q toward 0.5 and decays kernels
        q1 = (1.0 - f_forget) * q1 + f_forget * 0.5
        q2 = (1.0 - f_forget) * q2 + f_forget * 0.5
        k1 *= (1.0 - f_forget)
        k2 *= (1.0 - f_forget)

        # Stage 1 policy with kernel
        logits1 = beta1 * q1 + w_ck * k1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage 2 policy with kernel
        s = state[t]
        logits2 = beta2 * q2[s] + w_ck * k2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # TD updates
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update kernels after choices
        k1[a1] += 1.0
        k2[s, a2] += 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta1', 'beta2', 'k_anx_ck', 'f_forget']"
iter1_run0_participant32.json,cognitive_model1,375.0863197883133,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Successor-biased hybrid with anxiety-gated temporal horizon and attention-modulated learning.
    
    Idea:
    - Second stage is learned model-free (Q-learning).
    - First-stage action values combine model-free and a successor-like, model-based expectation.
    - The successor/horizon weight is reduced as anxiety increases (shorter planning horizon).
    - Learning rates are modulated on each update by a PearceâHall-like attention term driven by unsigned prediction errors,
      with stronger attention when anxiety is higher.
    - Choice stickiness (perseveration) also increases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the reached planet for each trial.
    reward : array-like of float
        Obtained reward on each trial (typically in [0,1]).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1+], where higher is more anxious.
    model_parameters : list or array-like of float
        [alpha, beta, gamma_sr, psi, chi]
        Bounds:
        - alpha in [0,1]: base learning rate.
        - beta in [0,10]: softmax inverse temperature (both stages share beta).
        - gamma_sr in [0,1]: baseline successor/temporal-horizon weight for model-based contribution at stage 1.
        - psi in [0,1]: choice stickiness strength (converted to additive bias; scaled by anxiety).
        - chi in [0,1]: attention gain for PE-driven learning-rate modulation (scaled by anxiety).
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, gamma_sr, psi, chi = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: rows are first-stage actions (A,U), cols are states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_mf = np.zeros(2)        # model-free values for first-stage actions
    q2 = np.zeros((2, 2))      # second-stage Q-values per state x action

    # Attention traces for adaptive learning-rate (initialized to 1)
    att2 = np.ones((2, 2))
    att1 = np.ones(2)

    # Previous choices for stickiness
    prev_a1 = None
    prev_a2 = {0: None, 1: None}

    # Anxiety effects
    # Planning horizon weight declines with anxiety
    gamma_eff = np.clip(gamma_sr * (1.0 - stai), 0.0, 1.0)
    # Stickiness increases with anxiety
    stick_scale = psi * stai
    # Attention amplification with anxiety
    att_gain = 1.0 + chi * stai

    for t in range(n_trials):
        # Model-based lookahead: expected max Q at second stage
        max_q2 = np.max(q2, axis=1)                 # best action per planet
        q1_mb = transition_matrix @ max_q2          # expected value of each first-stage action

        # Combine MF and successor-like MB via anxiety-gated horizon
        q1_combined = (1.0 - gamma_eff) * q1_mf + gamma_eff * q1_mb

        # Add first-stage stickiness
        if prev_a1 is not None:
            stick_vec1 = np.array([1.0 if i == prev_a1 else 0.0 for i in range(2)])
        else:
            stick_vec1 = np.zeros(2)
        q1_policy = q1_combined + stick_scale * stick_vec1

        # First-stage choice probabilities
        q1c = q1_policy - np.max(q1_policy)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with stickiness within state
        s = state[t]
        if prev_a2[s] is not None:
            stick_vec2 = np.array([1.0 if i == prev_a2[s] else 0.0 for i in range(2)])
        else:
            stick_vec2 = np.zeros(2)
        q2_policy = q2[s] + stick_scale * stick_vec2

        q2c = q2_policy - np.max(q2_policy)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Second-stage learning with attention-modulated learning rate
        pe2 = r - q2[s, a2]
        lr2 = np.clip(alpha * att2[s, a2] * att_gain, 0.0, 1.0)
        q2[s, a2] += lr2 * pe2
        # Update attention by unsigned PE (PearceâHall style)
        att2[s, a2] = np.clip(0.5 * att2[s, a2] + 0.5 * abs(pe2), 0.0, 1.0)

        # First-stage MF learning bootstrapping on updated second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        lr1 = np.clip(alpha * att1[a1] * att_gain, 0.0, 1.0)
        q1_mf[a1] += lr1 * pe1
        att1[a1] = np.clip(0.5 * att1[a1] + 0.5 * abs(pe1), 0.0, 1.0)

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'gamma_sr', 'psi', 'chi']"
iter1_run0_participant32.json,cognitive_model2,509.7854060943266,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Transition-learning planner with stage-specific temperatures and anxiety-biased uncertainty.
    
    Idea:
    - Learns the transition matrix from experience (simple delta rule).
    - First-stage uses model-based planning through the learned transition matrix.
    - Anxiety increases a bias toward uncertain/flattened transitions by interpolating the learned
      transitions with a uniform distribution, reducing planning precision.
    - Stage-specific softmax temperatures (beta1, beta2) allow different exploration at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the reached planet for each trial.
    reward : array-like of float
        Obtained reward on each trial (typically in [0,1]).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1+], where higher is more anxious.
    model_parameters : list or array-like of float
        [alpha, beta1, beta2, gamma, zeta]
        Bounds:
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta1 in [0,10]: softmax inverse temperature for first-stage choices.
        - beta2 in [0,10]: softmax inverse temperature for second-stage choices.
        - gamma in [0,1]: learning rate for transition probabilities.
        - zeta in [0,1]: strength of anxiety-driven uncertainty bias on transitions.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """"""
    alpha, beta1, beta2, gamma, zeta = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition matrix near the canonical structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Uniform transition matrix for uncertainty bias
    Tunif = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    # Anxiety-driven interpolation weight toward uncertainty
    w_unc = np.clip(zeta * stai, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2))  # second-stage Q-values per state x action

    for t in range(n_trials):
        # Anxiety-biased effective transition matrix
        T_eff = (1.0 - w_unc) * T + w_unc * Tunif

        # First-stage model-based action values from current Q2 and T_eff
        max_q2 = np.max(q2, axis=1)          # best action per state
        q1 = T_eff @ max_q2                  # expected value of first-stage actions

        # First-stage choice
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta1 * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta2 * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward and learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Learn transition probabilities for the chosen action: move toward the observed state
        # For chosen a1, update the row to increase probability of observed s
        for dest in range(2):
            target = 1.0 if dest == s else 0.0
            T[a1, dest] += gamma * (target - T[a1, dest])
        # Ensure row normalization
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta1', 'beta2', 'gamma', 'zeta']"
iter1_run0_participant32.json,cognitive_model3,378.74018098724525,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Risk-sensitive model-free learner with anxiety-amplified confirmation and eligibility trace.
    
    Idea:
    - Purely model-free learning across both stages with an eligibility trace that backs up second-stage PEs to first stage.
    - Rewards are transformed by a concave utility capturing risk aversion; anxiety increases effective risk aversion.
    - Confirmation bias: when the transition is common (i.e., expected), the backup to the first stage is amplified,
      and this amplification grows with anxiety (stronger confirmation under anxiety).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the reached planet for each trial.
    reward : array-like of float
        Obtained reward on each trial (typically in [0,1]).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1+], where higher is more anxious.
    model_parameters : list or array-like of float
        [alpha, beta, lam_e, lambda_risk, kappa]
        Bounds:
        - alpha in [0,1]: learning rate.
        - beta in [0,10]: softmax inverse temperature (both stages share beta).
        - lam_e in [0,1]: eligibility trace for backing up second-stage PE to first-stage value.
        - lambda_risk in [0,1]: baseline risk aversion (utility curvature).
        - kappa in [0,1]: strength of anxiety-amplified confirmation bias on the first-stage backup.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """"""
    alpha, beta, lam_e, lambda_risk, kappa = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition matrix to determine whether a transition is common or rare
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)         # first-stage model-free values
    q2 = np.zeros((2, 2))    # second-stage model-free values

    # Effective risk aversion grows with anxiety
    risk_eff = np.clip(lambda_risk + (1.0 - lambda_risk) * 0.5 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Policies (pure MF at both stages)
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Utility-transformed reward (risk-averse concave utility; r in [0,1])
        r_raw = reward[t]
        # Avoid 0^0; clamp
        r_clamped = np.clip(r_raw, 0.0, 1.0)
        # u(r) = r^(1 - risk_eff) with risk_eff in [0,1] -> concave for risk_eff>0
        util = (r_clamped + 1e-12) ** (1.0 - risk_eff)

        # Second-stage learning
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Determine if the transition observed was common for the chosen first-stage action
        prob_common = T[a1, s]
        is_common = 1.0 if prob_common >= 0.5 else 0.0

        # Confirmation bias factor increases first-stage backup on common transitions
        conf = 1.0 + kappa * stai * is_common

        # First-stage learning: TD to Q2 plus eligibility trace of the second-stage PE
        bootstrap = q2[s, a2]
        pe1 = bootstrap - q1[a1]
        q1[a1] += alpha * (pe1 + lam_e * conf * pe2)

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik","['alpha', 'beta', 'lam_e', 'lambda_risk', 'kappa']"
iter1_run0_participant34.json,cognitive_model1,430.3939486354682,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MBâMF with anxiety-dampened transition learning and reliability-weighted arbitration.

    The agent learns:
      - Model-free action values at both stages (MF).
      - A per-action transition model to second-stage states (MB).
    Arbitration between MF and MB at the first stage is driven by the learned
    reliability of the transition model (how far the learned transition is from chance).
    Anxiety (stai) reduces the effective learning rate for transitions, slowing the
    acquisition of a reliable model and thus indirectly lowering MB control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha_mf, beta, tau, k_anx_trans, alpha2]
        - alpha_mf in [0,1]: MF learning rate for first-stage values.
        - beta in [0,10]: inverse temperature for both stages.
        - tau in [0,1]: base learning rate for the transition model p(state|action1).
        - k_anx_trans in [0,1]: anxiety impact on transition learning (tau_eff = tau*(1 - k_anx_trans*stai)).
        - alpha2 in [0,1]: MF learning rate for second-stage values.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_mf, beta, tau, k_anx_trans, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective transition learning rate reduced by anxiety
    tau_eff = tau * (1.0 - k_anx_trans * stai_val)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q_stage1_mf = np.zeros(2)           # for actions A/U
    q_stage2 = np.zeros((2, 2))         # for states X/Y and their two aliens

    # Learned transition model: probability of going to state X given action a
    # Initialize neutral at 0.5 to allow learning
    p_to_X = np.array([0.5, 0.5], dtype=float)

    for t in range(n_trials):
        # Compute MB values using current transition model
        max_q2 = np.max(q_stage2, axis=1)  # [max at X, max at Y]
        # For action 0 (A): P(X)=p_to_X[0], P(Y)=1-p
        q1_mb_A = p_to_X[0] * max_q2[0] + (1.0 - p_to_X[0]) * max_q2[1]
        # For action 1 (U)
        q1_mb_U = p_to_X[1] * max_q2[0] + (1.0 - p_to_X[1]) * max_q2[1]
        q1_mb = np.array([q1_mb_A, q1_mb_U])

        # Reliability of transition model: distance from chance (0.5)
        r_A = 2.0 * abs(p_to_X[0] - 0.5)  # in [0,1]
        r_U = 2.0 * abs(p_to_X[1] - 0.5)
        omega = max(0.0, min(1.0, 0.5 * (r_A + r_U)))  # average reliability, clipped

        # Hybrid first-stage values
        q1_hyb = omega * q1_mb + (1.0 - omega) * q_stage1_mf

        # First-stage policy
        logits1 = q1_hyb - np.max(q1_hyb)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (MF)
        s = state[t]
        logits2 = q_stage2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage MF
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Update first-stage MF toward bootstrapped value
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_mf * delta1

        # Update transition model for the chosen action
        # Move p_to_X[a1] toward indicator(s==X)
        ind_X = 1.0 if s == 0 else 0.0
        p_to_X[a1] += tau_eff * (ind_X - p_to_X[a1])
        # keep within [0,1]
        p_to_X[a1] = max(0.0, min(1.0, p_to_X[a1]))

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_mf', 'beta', 'tau', 'k_anx_trans', 'alpha2']"
iter1_run0_participant34.json,cognitive_model2,424.55712223631144,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with anxiety-modulated uncertainty bonus (UCB-like) and decaying counts.

    The agent learns model-free values at both stages. Exploration at both stages
    is guided by an uncertainty bonus derived from decaying choice counts (pseudo-counts):
    actions chosen less often receive a larger bonus. Anxiety reverses or attenuates this
    bonus: low anxiety -> uncertainty seeking (positive bonus), high anxiety -> uncertainty
    aversion (negative bonus). Counts decay over trials to reflect nonstationarity.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens).
    reward : array-like of float
        Rewards.
    stai : array-like of float
        Anxiety in [0,1]; higher = more anxious. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha1, beta, k_anx_bonus, k_decay, alpha2]
        - alpha1 in [0,1]: MF learning rate for first-stage values.
        - beta in [0,10]: inverse temperature for both stages.
        - k_anx_bonus in [0,1]: scales how strongly anxiety modulates the uncertainty bonus.
        - k_decay in [0,1]: decay factor for pseudo-counts each trial (closer to 1 decays slower).
        - alpha2 in [0,1]: MF learning rate for second-stage values.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha1, beta, k_anx_bonus, k_decay, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Sign and magnitude of bonus from anxiety: in [-k_anx_bonus, +k_anx_bonus]
    # Low anxiety -> positive bonus; high anxiety -> negative
    bonus_weight = k_anx_bonus * (1.0 - 2.0 * stai_val)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Decaying pseudo-counts for uncertainty bonuses
    N1 = np.zeros(2)        # counts for first-stage actions
    N2 = np.zeros((2, 2))   # counts per state-action at second stage

    # Fixed transition structure for expected uncertainty at stage 1
    # A->X common, U->Y common
    P_common = 0.7
    trans = np.array([[P_common, 1 - P_common], [1 - P_common, P_common]])  # rows a1, cols states X,Y

    for t in range(n_trials):
        # Decay counts
        N1 *= k_decay
        N2 *= k_decay

        # Compute uncertainty bonuses
        u1 = 1.0 / np.sqrt(N1 + 1e-6)  # size-2
        u2 = 1.0 / np.sqrt(N2 + 1e-6)  # 2x2

        # Second-stage policy with bonus on current state actions
        s = state[t]
        logits2 = q_stage2[s].copy() + bonus_weight * u2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # First-stage policy with expected bonus based on transition structure
        # For each action, expected bonus approx equals expected max uncertainty at reached state
        max_u2 = np.max(u2, axis=1)  # per state
        exp_bonus_a0 = trans[0, 0] * max_u2[0] + trans[0, 1] * max_u2[1]
        exp_bonus_a1 = trans[1, 0] * max_u2[0] + trans[1, 1] * max_u2[1]
        bonus1 = np.array([exp_bonus_a0, exp_bonus_a1])
        logits1 = q_stage1.copy() + bonus_weight * bonus1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        r = reward[t]

        # Update MF values
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1[a1]
        q_stage1[a1] += alpha1 * delta1

        # Update counts after observing the choices
        N1[a1] += 1.0
        N2[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha1', 'beta', 'k_anx_bonus', 'k_decay', 'alpha2']"
iter1_run0_participant34.json,cognitive_model3,425.4541500753394,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-like model-based control with anxiety-modulated structural reliance.

    The agent combines:
      - Model-free values at both stages (MF).
      - A learned action-to-state mapping (row-stochastic) M[a1, s], updated from experience.
      - A prior structural model M_prior reflecting common transitions (A->X, U->Y).
    Anxiety scales the reliance on the prior structure via a discount-like factor gamma:
    higher anxiety reduces gamma_eff, diminishing the influence of the prior and MB control.
    First-stage values are a convex combination of MF and MB values using gamma_eff.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens).
    reward : array-like of float
        Rewards.
    stai : array-like of float
        Anxiety in [0,1]; higher = more anxious. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, gamma_base, k_anx_gamma, alpha2]
        - alpha in [0,1]: learning rate for MF first-stage values and for M updates.
        - beta in [0,10]: inverse temperature for both stages.
        - gamma_base in [0,1]: base reliance on prior structure and MB control.
        - k_anx_gamma in [0,1]: anxiety reduces gamma (gamma_eff = gamma_base*(1 - k*stai)).
        - alpha2 in [0,1]: MF learning rate for second-stage values.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, gamma_base, k_anx_gamma, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    gamma_eff = gamma_base * (1.0 - k_anx_gamma * stai_val)
    gamma_eff = max(0.0, min(1.0, gamma_eff))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Learned mapping from action to state (rows sum to ~1 via the update rule)
    M = np.full((2, 2), 0.5)  # start neutral
    # Prior structure encoding common transitions
    P_common = 0.7
    M_prior = np.array([[P_common, 1 - P_common], [1 - P_common, P_common]], dtype=float)

    for t in range(n_trials):
        # Blend learned mapping with prior according to gamma_eff
        M_eff = (1.0 - gamma_eff) * M + gamma_eff * M_prior

        # Compute MB first-stage values
        max_q2 = np.max(q_stage2, axis=1)  # per state
        q1_mb = M_eff @ max_q2  # size-2 (for actions A,U)

        # Combine with MF first-stage values using gamma_eff as MB weight
        q1_hyb = gamma_eff * q1_mb + (1.0 - gamma_eff) * q_stage1_mf

        # First-stage policy
        logits1 = q1_hyb - np.max(q1_hyb)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (MF)
        s = state[t]
        logits2 = q_stage2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage MF
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Update first-stage MF toward bootstrapped value
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Update learned mapping M for the chosen action toward observed state
        # One-hot for observed state
        e = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        M[a1, :] += alpha * (e - M[a1, :])
        # Keep row within [0,1]
        M[a1, 0] = max(0.0, min(1.0, M[a1, 0]))
        M[a1, 1] = max(0.0, min(1.0, M[a1, 1]))
        # Optional light re-normalization (not strictly required due to symmetric update)
        row_sum = M[a1, 0] + M[a1, 1]
        if row_sum > 0:
            M[a1, :] = M[a1, :] / row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'gamma_base', 'k_anx_gamma', 'alpha2']"
iter1_run0_participant35.json,cognitive_model1,373.4875912202018,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated hybrid with learned transitions and perseveration.

    This model learns both second-stage action values and the first-stage transition
    matrix online. First-stage decisions combine model-based action values computed
    from the learned transition model with model-free first-stage values. Anxiety
    reduces the reliance on model-based control and shifts learning rates asymmetrically
    across stages. A perseveration bias (choice stickiness) is included and increases
    with anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1) in the reached state
    - reward: array-like of floats in [0,1], reward outcome on each trial
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha1_base: base learning rate for first-stage MF values in [0,1]
        alpha2_base: base learning rate for second-stage MF values in [0,1]
        beta: inverse temperature for both stages in [0,10]
        kappa: base perseveration weight in [0,1] (scaled by anxiety)
        tau: transition learning rate in [0,1] for learning the transition matrix

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.

    Anxiety usage
    - Arbitration weight: w_mb = 1 - stai (higher anxiety => less model-based).
    - Learning rates: alpha1 decreases with anxiety; alpha2 increases with anxiety.
      alpha1_eff = alpha1_base * (1 - 0.3*stai), alpha2_eff = alpha2_base * (1 + 0.5*stai), both clipped to [0,1].
    - Perseveration increases with anxiety: kappa_eff = kappa * stai.
    """"""
    alpha1_base, alpha2_base, beta, kappa, tau = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model; start near the instructed structure but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value tables
    q1_mf = np.zeros(2)          # MF values for stage-1 actions
    q2 = np.zeros((2, 2))        # MF values for stage-2 (state x action)

    # Likelihood recording
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    prev_a1 = -1
    prev_a2 = np.array([-1, -1])  # per-state

    # Anxiety-dependent components
    w_mb = max(0.0, min(1.0, 1.0 - stai))
    alpha1_eff = min(1.0, max(0.0, alpha1_base * (1.0 - 0.3 * stai)))
    alpha2_eff = min(1.0, max(0.0, alpha2_base * (1.0 + 0.5 * stai)))
    kappa_eff = kappa * stai

    for t in range(n_trials):
        # Model-based action values from learned transition and current q2
        max_q2 = np.max(q2, axis=1)     # size 2 for states X,Y
        q1_mb = T @ max_q2              # size 2 for actions A,U

        # Arbitration between MB and MF
        q1_combined = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add perseveration bias to first-stage
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa_eff

        # First-stage policy
        centered_q1 = q1_combined + bias1 - np.max(q1_combined + bias1)
        probs1 = np.exp(beta * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with perseveration bias within the reached state
        s = int(state[t])
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2[s] in (0, 1):
            bias2[prev_a2[s]] += kappa_eff

        centered_q2 = q2_s + bias2 - np.max(q2_s + bias2)
        probs2 = np.exp(beta * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: second-stage MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2_eff * delta2

        # Learning: first-stage MF update towards current state's best value (bootstrap)
        target1 = q2[s, a2]  # on-policy SARSA-like using chosen a2 after its update
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1_eff * delta1

        # Transition learning: move T[a1] toward observed one-hot transition to state s
        o = np.array([1.0 if si == s else 0.0 for si in range(2)])
        T[a1] = (1.0 - tau) * T[a1] + tau * o
        # Normalize to ensure valid probability vector
        T[a1] /= np.sum(T[a1])

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha1_base', 'alpha2_base', 'beta', 'kappa', 'tau']"
iter1_run0_participant35.json,cognitive_model2,401.9628126868606,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planning with anxiety-damped directed exploration (UCB) and reward stickiness.

    First-stage choices are purely model-based, planning through the known transition matrix,
    using second-stage action values augmented with an uncertainty bonus (UCB). Anxiety reduces
    the directed exploration bonus. Second-stage choices also include the bonus. A reward
    stickiness bias increases the tendency to repeat actions that were rewarded on the last
    visit to the same context (state for stage 2; previous trial for stage 1).

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions
    - state: array-like of ints in {0,1}, reached second-stage state
    - action_2: array-like of ints in {0,1}, second-stage actions
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha2: learning rate for second-stage values in [0,1]
        beta1: inverse temperature for first-stage softmax in [0,10]
        beta2: inverse temperature for second-stage softmax in [0,10]
        xi_base: base weight for the UCB exploration bonus in [0,1]
        rho: reward stickiness strength in [0,1]

    Returns
    - Negative log-likelihood of observed choices.

    Anxiety usage
    - Directed exploration bonus weight: xi_eff = xi_base * (1 - stai), so higher anxiety
      reduces the uncertainty bonus at both stages.
    """"""
    alpha2, beta1, beta2, xi_base, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (commonly A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage value function and visit counts for UCB
    q2 = np.zeros((2, 2))
    n_visits = np.ones((2, 2))  # start at 1 to keep bonus finite

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Reward stickiness memory
    prev_a1 = -1
    prev_r1 = 0.0
    prev_a2 = np.array([-1, -1])
    prev_r2 = np.array([0.0, 0.0])

    # Anxiety-damped exploration bonus
    xi_eff = xi_base * (1.0 - stai)

    for t in range(n_trials):
        # Compute UCB bonus per state-action
        bonus = xi_eff / np.sqrt(n_visits)

        # Model-based first-stage Q via planning with bonus-augmented second-stage values
        max_mb = np.max(q2 + bonus, axis=1)  # best action per state including bonus
        q1_mb = T @ max_mb

        # Reward stickiness bias at stage 1: repeat previous first-stage action if last reward was high
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1) and prev_r1 > 0.0:
            bias1[prev_a1] += rho

        centered_q1 = q1_mb + bias1 - np.max(q1_mb + bias1)
        probs1 = np.exp(beta1 * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with UCB bonus and reward stickiness within the reached state
        s = int(state[t])
        q2_s = q2[s] + bonus[s]
        bias2 = np.zeros(2)
        if prev_a2[s] in (0, 1) and prev_r2[s] > 0.0:
            bias2[prev_a2[s]] += rho

        centered_q2 = q2_s + bias2 - np.max(q2_s + bias2)
        probs2 = np.exp(beta2 * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: second-stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Update counts for UCB after observing this choice
        n_visits[s, a2] += 1.0

        # Update stickiness memories
        prev_r1 = r
        prev_a1 = a1
        prev_r2[s] = r
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta1', 'beta2', 'xi_base', 'rho']"
iter1_run0_participant35.json,cognitive_model3,433.1486943262148,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric learning MF+MB hybrid with anxiety-shaped arbitration and noise, plus stage-1 perseveration.

    Second-stage values are learned with separate learning rates for positive vs. negative
    prediction errors. First-stage decisions blend model-based (using the known transition)
    and model-free first-stage values. The MB weight and choice stochasticity depend on
    anxiety: higher anxiety reduces MB arbitration weight and increases choice noise at stage 1.
    A stage-1 perseveration bias encourages repeating the previous first-stage action.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions
    - state: array-like of ints in {0,1}, reached second-stage state
    - action_2: array-like of ints in {0,1}, second-stage actions
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_pos: learning rate for positive TD errors in [0,1]
        alpha_neg: learning rate for negative TD errors in [0,1]
        beta1: inverse temperature base for first-stage in [0,10]
        beta2: inverse temperature for second-stage in [0,10]
        kappa: stage-1 perseveration weight in [0,1]

    Returns
    - Negative log-likelihood of observed choices.

    Anxiety usage
    - MB arbitration: w_mb = 1 - stai (higher anxiety => more MF).
    - Stage-1 noise: beta1_eff = beta1 * (1 - 0.3*stai), increasing noise with anxiety.
    - Perseveration is unaffected by anxiety here; stai already shapes arbitration and noise.
    """"""
    alpha_pos, alpha_neg, beta1, beta2, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure known
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)        # MF first-stage
    q2 = np.zeros((2, 2))      # second-stage

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1

    # Anxiety-shaped arbitration and noise
    w_mb = max(0.0, min(1.0, 1.0 - stai))
    beta1_eff = max(0.0, beta1 * (1.0 - 0.3 * stai))

    for t in range(n_trials):
        # Model-based component from max q2 per state
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MB and MF
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa

        centered_q1 = q1 + bias1 - np.max(q1 + bias1)
        probs1 = np.exp(beta1_eff * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2_s = q2[s]
        centered_q2 = q2_s - np.max(q2_s)
        probs2 = np.exp(beta2 * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second-stage asymmetric learning
        pe2 = r - q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2 * pe2

        # First-stage MF update toward the chosen second-stage value (on-policy)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        alpha1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        q1_mf[a1] += alpha1 * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_pos', 'alpha_neg', 'beta1', 'beta2', 'kappa']"
iter1_run0_participant36.json,cognitive_model1,478.2110820276055,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 1: Asymmetric model-free SARSA(Î») with anxiety-modulated learning and perseveration.
    
    Overview
    --------
    This purely model-free controller learns second-stage action values (Q2) and propagates
    value to the first stage (Q1_mf) via an eligibility trace (lambda). Learning is asymmetric
    for positive vs. negative prediction errors. Anxiety (stai) increases the effective learning
    rate magnitude and strengthens a choice perseveration (stickiness) bias at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher stai increases learning gain and stickiness.
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta, lam, phi]
        - alpha_pos in [0,1]: learning rate for positive prediction errors (PEs).
        - alpha_neg in [0,1]: learning rate for negative PEs.
        - beta in [0,10]: inverse temperature for both stages.
        - lam in [0,1]: eligibility trace strength from Q2 to Q1_mf.
        - phi in [0,1]: baseline perseveration bias magnitude added to chosen action logits.
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_pos, alpha_neg, beta, lam, phi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Probabilities for each stage's observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    Q1_mf = np.zeros(2)          # first-stage MF values over actions A/U
    Q2 = np.zeros((2, 2))        # second-stage values, indexed by state X/Y and alien 0/1

    # Perseveration traces (previous actions), initialized as None
    prev_a1 = None
    prev_a2 = [None, None]  # one for each state

    # Anxiety modulation
    learn_gain = 0.5 + 0.5 * stai               # scales learning rates upward with anxiety
    phi_eff = phi * (0.5 + 0.5 * stai)          # stronger stickiness with higher anxiety

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage 1 policy: softmax over Q1_mf with perseveration bias
        logits1 = beta * Q1_mf.copy()
        if prev_a1 is not None:
            logits1[prev_a1] += phi_eff
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: softmax over Q2[s] with perseveration within state
        logits2 = beta * Q2[s].copy()
        if prev_a2[s] is not None:
            logits2[prev_a2[s]] += phi_eff
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 (asymmetric TD)
        pe2 = r - Q2[s, a2]
        alpha2 = (alpha_pos if pe2 >= 0.0 else alpha_neg) * learn_gain
        Q2[s, a2] += alpha2 * pe2

        # Eligibility-trace update for stage 1 (SARSA(Î»)-like backup from chosen path)
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        alpha1 = (alpha_pos if pe1 >= 0.0 else alpha_neg) * learn_gain
        Q1_mf[a1] += alpha1 * lam * pe1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)

","['alpha_pos', 'alpha_neg', 'beta', 'lam', 'phi']"
iter1_run0_participant36.json,cognitive_model2,534.9327151432908,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 2: Transition-belief model-based planner with MF backup and decay; anxiety blunts transition confidence.
    
    Overview
    --------
    This hybrid controller forms a model-based (MB) first-stage value by planning with a
    subjective transition model whose common-transition probability is not fixed, but is
    biased toward uncertainty (0.5) as anxiety increases. It also maintains a model-free
    first-stage value (Q1_mf). Q2 updates with TD learning and undergoes forgetting/decay
    to capture nonstationarity. Arbitration between MB and MF is weighted by omega, which
    is down-weighted by anxiety (more anxious -> more MF).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher stai pushes subjective transition prob toward 0.5,
        and reduces model-based arbitration weight.
    model_parameters : list or array
        [alpha, beta, omega_base, p_common_base, rho]
        - alpha in [0,1]: TD learning rate for Q2 and MF Q1 backup.
        - beta in [0,10]: inverse temperature for choices at both stages.
        - omega_base in [0,1]: baseline MB arbitration weight (stai=0).
        - p_common_base in [0,1]: baseline belief that each ship commonly reaches its favored planet.
        - rho in [0,1]: forgetting rate applied to all Q2 entries each trial (larger -> faster decay).
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, omega_base, p_common_base, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    # Subjective common-transition probability moves toward 0.5 with anxiety
    p_common = (1.0 - stai) * p_common_base + stai * 0.5
    p_common = np.clip(p_common, 0.0, 1.0)
    # Arbitration weight reduced by anxiety
    omega = np.clip((1.0 - stai) * omega_base, 0.0, 1.0)

    # Construct subjective transition matrix based on p_common
    # Row: action (A=0, U=1), Col: state (X=0, Y=1)
    T = np.array([[p_common, 1.0 - p_common],
                  [1.0 - p_common, p_common]])

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Apply forgetting/decay to all Q2 before computing policy/updates
        Q2 = (1.0 - rho) * Q2

        # Model-based planning for stage 1
        max_Q2 = np.max(Q2, axis=1)   # best alien per planet
        Q1_mb = T @ max_Q2            # expected value per spaceship

        # Arbitration between MB and MF
        Q1 = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Stage 1 policy
        logits1 = beta * Q1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * Q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # TD update at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Model-free first-stage backup from realized second-stage value
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)

","['alpha', 'beta', 'omega_base', 'p_common_base', 'rho']"
iter1_run0_participant36.json,cognitive_model3,546.196358711521,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 3: Surprise-penalized model-based controller with anxiety-amplified avoidance and lapse.
    
    Overview
    --------
    A fully model-based first-stage planner computes expected values via the known common transitions.
    In addition, the model tracks a per-action 'surprise trace' that increases when a rare transition
    occurs after choosing that action. This trace penalizes the corresponding first-stage action
    (reflecting anxiety-driven avoidance of unexpected outcomes). Anxiety amplifies both the penalty
    weight and a small lapse probability at stage 1. Second-stage learning is standard TD.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher stai increases transition-surprise penalty
        and increases lapse mixing at stage 1.
    model_parameters : list or array
        [alpha, beta, eta_surprise, kappa, xi]
        - alpha in [0,1]: TD learning rate for second-stage values.
        - beta in [0,10]: inverse temperature (both stages).
        - eta_surprise in [0,1]: learning rate for updating surprise traces.
        - kappa in [0,1]: baseline weight of surprise penalty applied to first-stage logits.
        - xi in [0,1]: baseline lapse rate for stage 1 (mixed with uniform choice).
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, eta_surprise, kappa, xi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Known true transition structure (common = 0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Second-stage values
    Q2 = np.zeros((2, 2))

    # Surprise traces per first-stage action (A, U)
    z_surprise = np.zeros(2)

    # Anxiety-modulated penalty and lapse
    kappa_eff = kappa * (0.5 + 0.5 * stai)  # stronger penalty with more anxiety
    xi_eff = np.clip(xi * (1.0 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based first-stage values
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Apply surprise penalty to first-stage logits
        logits1 = beta * Q1_mb - kappa_eff * z_surprise
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        # Lapse mix at stage 1
        probs1 = (1.0 - xi_eff) * probs1 + xi_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * Q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # TD update at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update surprise trace for the chosen first-stage action based on whether transition was rare
        # Common if (A->X) or (U->Y); rare otherwise
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        surprisal = 0.0 if is_common else 1.0
        # Exponential moving average of surprisal per action
        z_surprise[a1] = (1.0 - eta_surprise) * z_surprise[a1] + eta_surprise * surprisal

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)","['alpha', 'beta', 'eta_surprise', 'kappa', 'xi']"
iter1_run0_participant38.json,cognitive_model1,431.1316113248854,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MBâMF with learned transitions and anxiety-dampened planning.
    
    This model learns both stage-2 values and the stage-1 transition matrix online.
    Stage-1 choices are driven by a hybrid of model-based (MB) values (computed using
    the learned transition matrix) and model-free (MF) values. Anxiety reduces reliance
    on MB planning and slows transition learning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial at the visited planet.
    reward : array-like of float (e.g., 0 or 1)
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values reduce MB weight and transition learning rate.
    model_parameters : iterable of floats
        [alpha_r, alpha_t, beta, w0]
        - alpha_r: [0,1] learning rate for stage-2 rewards and stage-1 MF value.
        - alpha_t: [0,1] learning rate for updating the transition matrix.
        - beta: [0,10] inverse temperature for softmax choices (both stages).
        - w0: [0,1] baseline MB arbitration weight at stage 1 (dampened by anxiety).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_r, alpha_t, beta, w0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize probabilities and value functions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-1 MF values for A/U
    q1_mf = np.zeros(2)
    # Stage-2 values for each planet (X/Y) and alien (two options)
    q2 = np.zeros((2, 2))
    # Learned transition matrix T[a, s] = P(s | a); start uninformative
    T = np.full((2, 2), 0.5)

    # Anxiety-dampened MB weight and transition learning rate
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)
    alpha_t_eff = np.clip(alpha_t * (1.0 - 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based Q at stage 1 from learned transition and current stage-2 values
        max_q2 = np.max(q2, axis=1)      # value of best alien per planet
        q1_mb = T @ max_q2               # forward planning

        # Hybrid action values
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited planet
        s = state[t]
        exp_q2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # MF update at stage 1 toward the realized stage-2 chosen value
        target1 = q2[s, a2]
        q1_mf[a1] += alpha_r * (target1 - q1_mf[a1])

        # Update learned transition matrix for chosen action using one-hot target for observed state
        # Move row T[a1] toward the observed state s
        T[a1, :] = (1.0 - alpha_t_eff) * T[a1, :]
        T[a1, s] += alpha_t_eff
        # Normalize for numerical stability (should already sum to 1)
        row_sum = T[a1, :].sum()
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood

","['alpha_r', 'alpha_t', 'beta', 'w0']"
iter1_run0_participant38.json,cognitive_model2,386.8070596722222,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric learning with anxiety-weighted negativity and value decay; hybrid control.
    
    Stage-2 learning uses outcome-valence asymmetric learning rates. Anxiety increases
    sensitivity to negative outcomes and reduces reliance on model-based planning.
    Both stage-2 and stage-1 MF values decay over time (forgetting). Stage-1 choice
    uses a hybrid of MB planning (using the known transition structure) and MF values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (two aliens per planet).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values increase negative-learning asymmetry and forgetting,
        and reduce MB arbitration weight.
    model_parameters : iterable of floats
        [alpha, beta, mb_w, asym, decay]
        - alpha: [0,1] base learning rate for TD updates.
        - beta: [0,10] inverse temperature for softmax choices (both stages).
        - mb_w: [0,1] baseline MB weight at stage 1 (reduced by anxiety).
        - asym: [0,1] strength of valence asymmetry in learning rates.
        - decay: [0,1] forgetting rate (toward neutral values) per trial.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, mb_w, asym, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (A->X and U->Y are common)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_mf = np.zeros(2)       # stage-1 MF Q for A/U (initialized neutral at 0)
    q2 = np.full((2, 2), 0.5) # stage-2 Q initialized neutral at 0.5

    # Anxiety-dependent MB weight and decay
    w = mb_w * (1.0 - stai)
    w = np.clip(w, 0.0, 1.0)
    # Decay strength increases with anxiety toward neutral values
    decay_eff_2 = np.clip(decay * (0.5 + 0.5 * stai), 0.0, 1.0)
    decay_eff_1 = np.clip(decay * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # MB plan at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid value
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        exp_q2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Valence-asymmetric TD learning at stage 2 with anxiety-weighted negativity
        delta2 = r - q2[s, a2]
        # Positive and negative learning rates
        alpha_pos = np.clip(alpha * (1.0 + asym * (1.0 - stai)), 0.0, 1.0)
        alpha_neg = np.clip(alpha * (1.0 + asym * stai), 0.0, 1.0)
        lr = alpha_pos if delta2 >= 0 else alpha_neg
        q2[s, a2] += lr * delta2

        # Stage-1 MF learning toward realized stage-2 chosen value
        target1 = q2[s, a2]
        q1_mf[a1] += alpha * (target1 - q1_mf[a1])

        # Forgetting/decay after learning
        # Stage-2 values decay toward 0.5 (neutral)
        q2 = (1.0 - decay_eff_2) * q2 + decay_eff_2 * 0.5
        # Stage-1 MF values decay toward 0 (neutral baseline for advantage)
        q1_mf = (1.0 - decay_eff_1) * q1_mf

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood

","['alpha', 'beta', 'mb_w', 'asym', 'decay']"
iter1_run0_participant39.json,cognitive_model1,495.5478137851685,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 1: Learned-transition model-based planner with anxiety-modulated exploration bonus and MF backup.
    
    The agent learns second-stage rewards model-free and learns the first-stage
    transition model online. First-stage values combine:
      - Model-based (MB): learned transition matrix times max second-stage values
      - Directed exploration bonus: entropy of learned transitions (per action)
      - Model-free (MF) backup: cached TD(0) values for first-stage actions
    Anxiety increases directed exploration and shifts weight toward MF control by
    reducing MB weight through a stai-dependent MF-weight.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes in [0,1]
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha_r: [0,1] reward learning rate at stage 2
        alpha_T_base: [0,1] baseline transition learning rate at stage 1
        beta: [0,10] inverse temperature for both stages
        phi_base: [0,1] baseline weight on exploration bonus (transition entropy)
        omega_mf: [0,1] baseline weight on MF in the hybrid (before anxiety)
    Returns
    - Negative log-likelihood of observed choices (sum over both stages).
    """"""
    alpha_r, alpha_T_base, beta, phi_base, omega_mf = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transition matrix rows to neutral (0.5, 0.5)
    T = np.ones((2, 2)) * 0.5  # rows: actions; cols: states
    # Stage-2 MF Q-values
    q2 = np.zeros((2, 2))
    # Stage-1 MF Q-values
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    alpha_T = np.clip(alpha_T_base * (1.0 + stai0), 0.0, 1.0)  # anxious updates transitions faster
    phi = phi_base * (1.0 + stai0)  # more exploration bonus with higher anxiety
    w_mf = np.clip(omega_mf * stai0, 0.0, 1.0)                 # anxious shifts toward MF
    w_mb = 1.0 - w_mf

    eps = 1e-12
    for t in range(n_trials):
        # Model-based Q1 from learned transitions and current q2
        max_q2 = np.max(q2, axis=1)   # per state
        q1_mb = T @ max_q2            # per action

        # Directed exploration bonus: entropy of transition distribution per action
        H = np.zeros(2)
        for a in range(2):
            p = T[a]
            # entropy with clipping
            H[a] = -np.sum(p * np.log(np.clip(p, eps, 1.0)))
        q1_dir = phi * H

        # Hybrid Q1
        q1 = w_mb * q1_mb + w_mf * q1_mf + q1_dir

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = state[t]
        q2_s = q2[s2]
        q2c = q2_s - np.max(q2_s)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update stage-2 MF values
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_r * pe2

        # Update learned transitions for chosen action using a simple delta rule toward observed state
        onehot_s = np.array([1.0 if i == s2 else 0.0 for i in range(2)])
        T[a1] = (1.0 - alpha_T) * T[a1] + alpha_T * onehot_s
        # Normalize for safety
        T[a1] = np.clip(T[a1], eps, 1.0)
        T[a1] = T[a1] / np.sum(T[a1])

        # Update stage-1 MF via TD(0) bootstrap from current stage-2 value of the chosen alien
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1  # use same reward learning rate to keep params within limit

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'alpha_T_base', 'beta', 'phi_base', 'omega_mf']"
iter1_run0_participant39.json,cognitive_model2,469.5689897531238,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 2: Kalman reward-tracking with anxiety-driven volatility and planet-switch cost.
    
    The agent tracks each second-stage action's reward probability with a
    Kalman filter (state uncertainty -> adaptive learning rate). Anxiety
    increases the assumed volatility, thus larger adaptive learning rates,
    and increases a cognitive cost for switching targeted planet between trials.
    First-stage values combine:
      - Model-based value via fixed transition structure and tracked means
      - A planet-switch aversion term (cost if chosen ship targets a different common planet than the previous observed planet)
      - A simple stage-1 MF cache updated from the chosen second-stage value
    
    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes in [0,1]
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        beta: [0,10] inverse temperature for both stages
        sigma_w_base: [0,1] baseline process noise (volatility) for Kalman filter
        sigma0: [0,1] initial variance for each option
        alpha1_mf: [0,1] learning rate for stage-1 MF cache
        c_switch_base: [0,1] baseline cost for switching targeted planet
    Returns
    - Negative log-likelihood of observed choices (sum over both stages).
    """"""
    beta, sigma_w_base, sigma0, alpha1_mf, c_switch_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed transition matrix (common 0.7)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Kalman parameters
    sigma_w = sigma_w_base * (1.0 + 2.0 * stai0)  # anxious => higher assumed volatility
    sigma_n = 0.25  # observation noise (fixed)

    # Means and variances for each state-action option
    m = np.ones((2, 2)) * 0.5
    v = np.ones((2, 2)) * sigma0

    # Stage-1 MF cache
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Planet-switch cost increases with anxiety
    c_switch = c_switch_base * (1.0 + stai0)

    # Keep track of previous observed planet
    prev_state = None

    eps = 1e-12
    for t in range(n_trials):
        # Model-based value: expected value of best alien on each planet
        max_m = np.max(m, axis=1)  # per planet
        q1_mb = T_fixed @ max_m

        # Planet-switch cost: ships target planet X for A (0), Y for U (1) in common transition
        target_planet = np.array([0, 1])
        switch_cost = np.zeros(2)
        if prev_state is not None:
            for a in range(2):
                # Cost if the targeted common planet differs from the last visited planet
                switch_cost[a] = c_switch if target_planet[a] != prev_state else 0.0

        # Combine MB and MF cache (simple average to limit parameters)
        q1 = 0.5 * q1_mb + 0.5 * q1_mf - switch_cost

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy based on current means
        s2 = state[t]
        q2_s = m[s2]
        q2c = q2_s - np.max(q2_s)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Kalman update for chosen second-stage option
        # Predict step for all options: increase variance by process noise
        v = v + sigma_w

        # Update chosen option
        pred_var = v[s2, a2]
        K = pred_var / (pred_var + sigma_n)  # adaptive learning rate in (0,1)
        m[s2, a2] = m[s2, a2] + K * (r - m[s2, a2])
        v[s2, a2] = (1.0 - K) * pred_var

        # Update stage-1 MF cache toward the current chosen second-stage mean
        target1 = m[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1_mf * pe1

        prev_state = s2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['beta', 'sigma_w_base', 'sigma0', 'alpha1_mf', 'c_switch_base']"
iter1_run0_participant39.json,cognitive_model3,504.76655368620993,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 3: Transition-sensitive win-stay/lose-switch (WSLS) blended with model-based planning.
    
    The first-stage decision blends:
      - Model-based value from the known transition structure and learned second-stage values
      - A WSLS bias that depends on prior reward and whether the prior transition was common vs rare
        (i.e., win-stay after common, win-switch after rare; reverse for losses).
    Anxiety reduces reliance on model-based planning and strengthens WSLS bias.
    Second-stage values are learned with a simple delta rule.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes in [0,1]
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        beta: [0,10] inverse temperature for both stages
        alpha2: [0,1] learning rate for second-stage Q-values
        omega_plan_base: [0,1] baseline weight on model-based component (before anxiety)
        zeta_base: [0,1] baseline weight on transition sensitivity in WSLS (0=ignores transition, 1=fully transition-dependent)
        p_ws_base: [0,1] baseline strength of WSLS bias
    Returns
    - Negative log-likelihood of observed choices (sum over both stages).
    """"""
    beta, alpha2, omega_plan_base, zeta_base, p_ws_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition matrix (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Second-stage Q-values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    omega_plan = np.clip(omega_plan_base * (1.0 - stai0), 0.0, 1.0)   # anxious -> less planning
    zeta = np.clip(zeta_base * (1.0 + stai0), 0.0, 1.0)               # anxious -> more transition sensitivity
    p_ws = p_ws_base * (1.0 + stai0)                                  # stronger WSLS with anxiety

    prev_a1 = None
    prev_r = None
    prev_common = None

    eps = 1e-12
    for t in range(n_trials):
        # Model-based value via expected max second-stage value
        max_q2 = np.max(q2, axis=1)  # per planet
        q1_mb = T @ max_q2

        # WSLS bias vector
        wsls_bias = np.zeros(2)
        if prev_a1 is not None and prev_r is not None and prev_common is not None:
            win = 1.0 if prev_r > 0.0 else 0.0
            # Transition-insensitive WSLS tendency: +1 for win (stay), -1 for loss (switch)
            t_ins = 1.0 if win == 1.0 else -1.0
            # Transition-sensitive adjustment: common -> stay (+1), rare -> switch (-1)
            t_tr = 1.0 if prev_common else -1.0
            # Interpolate by zeta
            t_eff = (1.0 - zeta) * t_ins + zeta * (1.0 if (win == 1.0) else -1.0) * t_tr
            # Apply as a centered bias over actions
            wsls_bias[prev_a1] += p_ws * t_eff
            wsls_bias[1 - prev_a1] -= p_ws * t_eff

        # Combine MB and WSLS
        q1 = omega_plan * q1_mb + (1.0 - omega_plan) * wsls_bias

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = state[t]
        q2_s = q2[s2]
        q2c = q2_s - np.max(q2_s)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn second-stage values
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha2 * pe2

        # Track whether the last transition was common (given the known structure)
        # Common if (A->X) or (U->Y)
        prev_common = ((a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1))
        prev_a1 = a1
        prev_r = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['beta', 'alpha2', 'omega_plan_base', 'zeta_base', 'p_ws_base']"
iter1_run0_participant4.json,cognitive_model1,445.7711417483381,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-surprise arbitration with anxiety-modulated stickiness.
    
    Overview
    - Stage 1 action values are a trial-wise arbitration between model-based (MB)
      planning and model-free (MF) values. The arbitration weight increases with
      transition surprise and participant anxiety.
    - Stage 2 uses standard RescorlaâWagner learning.
    - An anxiety-scaled perseveration (stickiness) bias is added at both stages.
    - Model-free credit assignment from stage 2 to stage 1 uses an eligibility
      trace implicitly set by the participantâs anxiety (higher anxiety -> longer trace).
    
    Parameters (all used; total = 5)
    - alpha: [0,1]          Learning rate for value updates.
    - beta: [0,10]          Inverse temperature for softmax choice at both stages.
    - kappa_trans: [0,1]    Sensitivity of MB arbitration to transition surprise.
    - gamma_anx: [0,1]      Gain mapping anxiety (stai) into baseline MB arbitration.
    - zeta_stick: [0,1]     Base perseveration strength; scaled by stai.
    
    Inputs
    - action_1: int array of shape (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state: int array of shape (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array of shape (n_trials,), chosen alien per trial (0/1).
    - reward: float array of shape (n_trials,), coins obtained per trial in [0,1].
    - stai: float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, kappa_trans, gamma_anx, zeta_stick].
    
    Returns
    - Negative log-likelihood (float) of the observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, kappa_trans, gamma_anx, zeta_stick = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)
    
    # Transition structure: rows = action (A,U), cols = state (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    
    # Value functions
    q1_mf = np.zeros(2)      # model-free Q at stage 1 (spaceships A/U)
    q2 = np.zeros((2, 2))    # stage-2 Q for each planet (X/Y) and alien (0/1)
    
    # Choice probabilities per trial
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    
    # Stickiness (perseveration) memory
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)  # per planet
    
    # Anxiety-scaled stickiness and eligibility
    kappa_eff = zeta_stick * stai
    lam_eff = stai  # eligibility trace: higher anxiety -> more MF backprop
    
    for t in range(n_trials):
        s = state[t]
        
        # Model-based Q at stage 1 from current stage-2 values
        max_q2 = np.max(q2, axis=1)          # shape (2,)
        q1_mb = T @ max_q2                   # shape (2,)
        
        # Trial-wise arbitration weight w_t in [0,1]
        # Surprise = -log p(transition); common=0.7, rare=0.3
        a1_obs = action_1[t]
        p_trans = T[a1_obs, s]
        surprise = -np.log(max(1e-8, p_trans))
        z = (gamma_anx * stai) + (kappa_trans * (surprise - 0.5))  # center ~0.5 nats
        w_t = 1.0 / (1.0 + np.exp(-5.0 * z))  # sharpened logistic to keep in (0,1)
        
        # Hybrid Q at stage 1
        q1 = (1.0 - w_t) * q1_mf + w_t * q1_mb
        
        # Stickiness biases
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_eff
        
        # Stage-1 policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1_obs]
        
        # Stage-2 policy with stickiness
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa_eff
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2_obs = action_2[t]
        p2[t] = probs2[a2_obs]
        
        # Learning updates
        r = reward[t]
        delta2 = r - q2[s, a2_obs]
        q2[s, a2_obs] += alpha * delta2
        # MF back-propagation to chosen stage-1 action
        q1_mf[a1_obs] += alpha * lam_eff * delta2
        
        # Update stickiness memory
        prev_a1 = a1_obs
        prev_a2[s] = a2_obs
    
    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['alpha', 'beta', 'kappa_trans', 'gamma_anx', 'zeta_stick']"
iter1_run0_participant4.json,cognitive_model2,505.9368554568574,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-adaptive learning with anxiety-inflated volatility and stage-2 stickiness.
    
    Overview
    - Stage 2 learning rate adapts online to estimated reward volatility (delta^2) with
      anxiety inflating perceived volatility, thus increasing learning rate under anxiety.
    - Stage 1 is purely model-based (no fixed MB/MF weight), using current stage-2 values.
    - A stage-2 perseveration bias scales with anxiety.
    
    Parameters (all used; total = 5)
    - alpha0: [0,1]        Base learning rate (minimum) for stage-2 Q.
    - beta: [0,10]         Inverse temperature for softmax at both stages.
    - k_vol: [0,1]         Sensitivity of learning rate to estimated volatility.
    - psi_anx: [0,1]       How strongly anxiety increases effective volatility.
    - kappa2: [0,1]        Base stage-2 stickiness, scaled by anxiety.
    
    Inputs
    - action_1: int array (n_trials,), chosen spaceship (0/1).
    - state: int array (n_trials,), reached planet (0/1).
    - action_2: int array (n_trials,), chosen alien (0/1).
    - reward: float array (n_trials,), coins obtained [0,1].
    - stai: float array length-1, anxiety score in [0,1].
    - model_parameters: [alpha0, beta, k_vol, psi_anx, kappa2].
    
    Returns
    - Negative log-likelihood (float) of observed choices at both stages.
    """"""
    alpha0, beta, k_vol, psi_anx, kappa2 = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)
    
    # Transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    
    # Stage-2 Q-values and volatility estimates per state-action
    q2 = np.zeros((2, 2))
    vol = np.zeros((2, 2))  # running estimate of variance of prediction error
    
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    
    # Stickiness memory for stage-2 only
    prev_a2 = np.array([None, None], dtype=object)
    kappa2_eff = kappa2 * stai
    
    for t in range(n_trials):
        s = state[t]
        
        # Stage-1 purely model-based from current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        
        # Softmax for stage 1
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1_obs = action_1[t]
        p1[t] = probs1[a1_obs]
        
        # Stage-2 softmax with stickiness
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa2_eff
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2_obs = action_2[t]
        p2[t] = probs2[a2_obs]
        
        # Outcome and adaptive learning
        r = reward[t]
        delta2 = r - q2[s, a2_obs]
        
        # Update volatility estimate with parameterized decay
        # decay in [0.5,1.0] ensures smoother vol when k_vol small
        decay = 1.0 - 0.5 * k_vol
        vol[s, a2_obs] = decay * vol[s, a2_obs] + (1.0 - decay) * (delta2 ** 2)
        
        # Anxiety-inflated effective volatility
        vol_eff = vol[s, a2_obs] * (1.0 + psi_anx * stai)
        # Map to effective learning rate in [0,1]
        alpha_eff = alpha0 + k_vol * vol_eff
        if alpha_eff > 1.0:
            alpha_eff = 1.0
        
        # Update stage-2 Q
        q2[s, a2_obs] += alpha_eff * delta2
        
        # Update stickiness memory
        prev_a2[s] = a2_obs
    
    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['alpha0', 'beta', 'k_vol', 'psi_anx', 'kappa2']"
iter1_run0_participant4.json,cognitive_model3,532.6742952205512,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-representation-inspired planning with anxiety-shortened horizon and common-transition preference.
    
    Overview
    - Stage 1 values blend a model-based component with a successor-representation (SR)
      approximation: SR reduces to a mixture between MF bootstrapping and one-step MB,
      controlled by a discount gamma that shortens with anxiety.
    - A preference bias toward the spaceship that commonly reaches the currently
      better planet is modulated by anxiety, capturing structure-seeking tendencies.
    - Stage 2 uses standard RescorlaâWagner learning.
    
    Parameters (all used; total = 5)
    - alpha: [0,1]          Learning rate for value updates.
    - beta: [0,10]          Inverse temperature for softmax at both stages.
    - gamma_sr: [0,1]       Base SR discount controlling weight on one-step MB within SR.
    - w_sr: [0,1]           Weight of SR-computed values relative to pure MB at stage 1.
    - pref_common: [0,1]    Strength of bias toward the spaceship with the better common destination.
    
    Inputs
    - action_1: int array (n_trials,), chosen spaceship (0=A, 1=U).
    - state: int array (n_trials,), reached planet (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien (0/1) within planet.
    - reward: float array (n_trials,), coins obtained [0,1].
    - stai: float array length-1, anxiety score in [0,1].
    - model_parameters: [alpha, beta, gamma_sr, w_sr, pref_common].
    
    Returns
    - Negative log-likelihood (float) of observed choices.
    """"""
    alpha, beta, gamma_sr, w_sr, pref_common = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)
    
    # Transition matrix (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    
    # Values
    q1_mf = np.zeros(2)   # MF at stage 1
    q2 = np.zeros((2, 2)) # stage-2 values
    
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    
    # Anxiety-shortened planning horizon: higher stai -> smaller gamma_eff
    gamma_eff = gamma_sr * (1.0 - 0.5 * stai)  # stays within [0,1]
    
    for t in range(n_trials):
        s = state[t]
        
        # Compute standard MB value from current stage-2 values
        max_q2 = np.max(q2, axis=1)   # best alien per planet
        q1_mb = T @ max_q2
        
        # SR-approx value: interpolate MF and one-step MB via gamma_eff
        # q1_sr = (1 - gamma) * MF + gamma * (one-step MB)
        q1_sr = (1.0 - gamma_eff) * q1_mf + gamma_eff * q1_mb
        
        # Final stage-1 value: blend SR with pure MB using w_sr
        q1 = (1.0 - w_sr) * q1_mb + w_sr * q1_sr
        
        # Common-transition preference bias toward currently better planet
        # If planet X is better, bias spaceship A (index 0); else bias spaceship U (index 1).
        better_is_X = (max_q2[0] >= max_q2[1])
        bias1 = np.zeros(2)
        pref_eff = pref_common * stai
        if better_is_X:
            bias1[0] += pref_eff
        else:
            bias1[1] += pref_eff
        
        # Stage-1 policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1_obs = action_1[t]
        p1[t] = probs1[a1_obs]
        
        # Stage-2 policy (no stickiness here to keep parameters within budget)
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2_obs = action_2[t]
        p2[t] = probs2[a2_obs]
        
        # Learning from outcome
        r = reward[t]
        delta2 = r - q2[s, a2_obs]
        q2[s, a2_obs] += alpha * delta2
        
        # MF update at stage 1 via bootstrapped TD from stage 2
        q1_mf[a1_obs] += alpha * delta2
    
    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)","['alpha', 'beta', 'gamma_sr', 'w_sr', 'pref_common']"
iter1_run0_participant40.json,cognitive_model1,531.1631109525459,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety- and surprise-modulated control and two-stage stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha, beta, omega0, eta, phi)
        - alpha in [0,1]: learning rate for model-free values at both stages.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - omega0 in [0,1]: baseline weight on model-based (MB) control.
        - eta in [0,1]: surprise sensitivity; after rare transitions MB weight is reduced
          by eta * stai.
        - phi in [0,1]: stickiness strength at stage 2 (bias to repeat the last alien
          chosen in the same planet).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Transition structure is assumed known (common: 0.7).
    - Effective MB weight on each trial is w_t = clip(omega0 - eta * stai * I_rare, 0, 1).
      Thus, higher anxiety amplifies MB down-weighting after surprising (rare) transitions.
    - Stage-1 also has a simple MF component learned by TD from the observed stage-2 value.
    - Stickiness at stage 2 is state-dependent; a bias is added to the previously chosen
      alien in that planet.
    """"""
    alpha, beta, omega0, eta, phi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: rows = actions (A,U), cols = states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)         # Q^MF at stage 1 over actions (A,U)
    q_stage2_mf = np.zeros((2, 2))    # Q^MF at stage 2: Q[state, action]

    # For stickiness at stage 2 (state-dependent)
    prev_a2 = [None, None]
    prev_a1 = None

    for t in range(n_trials):
        # MB projection using current MF estimates of second stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)       # best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2    # MB action values at stage 1

        # Combine MB and MF with dynamic, anxiety- and surprise-modulated weight
        # Surprise indicator (rare transition detection uses previous trial's choice/state after they occur)
        a1 = int(action_1[t])
        s2 = int(state[t])

        # Compute choice probabilities for stage 1 using last trial's w (before seeing s2)
        # We need the policy before seeing s2, so use a provisional w based on expected rarity.
        # Approximate with baseline omega0 attenuated by average surprise probability weighted by anxiety.
        w_prov = np.clip(omega0 - eta * stai * 0.3, 0.0, 1.0)
        q1_combined = w_prov * q_stage1_mb + (1.0 - w_prov) * q_stage1_mf

        # Add first-stage stickiness toward repeating last action (use phi/2 as mild, symmetric bias)
        stick1 = np.zeros(2)
        if prev_a1 is not None:
            stick1[prev_a1] = phi / 2.0
        logits1 = beta * q1_combined + stick1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with state-dependent stickiness
        a2 = int(action_2[t])
        bias2 = np.zeros(2)
        if prev_a2[s2] is not None:
            bias2[prev_a2[s2]] = phi
        logits2 = beta * q_stage2_mf[s2] + bias2
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Observe transition rarity for this trial and adjust weight for learning update
        # Rare if A->Y or U->X
        is_rare = 1 if ((a1 == 0 and s2 == 1) or (a1 == 1 and s2 == 0)) else 0
        w_t = np.clip(omega0 - eta * stai * is_rare, 0.0, 1.0)

        # Learning updates
        r = reward[t]

        # Stage-2 MF update
        delta2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha * delta2

        # Stage-1 MF update bootstrapping from updated stage-2 value
        target1 = q_stage2_mf[s2, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Update memory for stickiness
        prev_a2[s2] = a2
        prev_a1 = a1

        # Optionally, we could use w_t for a post-hoc blended update; here w_t influences only control,
        # which we already applied in policy via w_prov and through future trials.

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega0', 'eta', 'phi']"
iter1_run0_participant40.json,cognitive_model2,453.7462281507113,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based control with learned transitions; anxiety dampens transition learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, tau, kappa1)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - tau in [0,1]: transition learning rate for updating P(state | action).
        - kappa1 in [0,1]: first-stage stickiness strength (bias to repeat previous spaceship).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - The agent learns the transition matrix T (rows = actions, cols = states) online.
    - Anxiety reduces transition learning: tau_eff = tau * (1 - stai).
    - Stage-1 policy is fully model-based using the learned T and current stage-2 values.
    - Stage-2 values are learned via simple TD with learning rate alpha2.
    """"""
    alpha2, beta, tau, kappa1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize transition beliefs to uniform (uncertain)
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage values
    Q2 = np.zeros((2, 2))

    prev_a1 = None

    # Effective transition learning rate reduced by anxiety
    tau_eff = tau * (1.0 - stai_val)

    for t in range(n_trials):
        # Compute MB Q for stage 1: expected max Q2 under learned transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # First-stage stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = kappa1

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]

        # Update T row for chosen action toward observed state (EMA ensuring row-stochastic)
        if tau_eff > 0.0:
            # Decay entire row
            T[a1, :] = (1.0 - tau_eff) * T[a1, :]
            # Add mass to observed state
            T[a1, s2] += tau_eff
            # Numerical renormalization (guard)
            row_sum = np.sum(T[a1, :])
            if row_sum > 0:
                T[a1, :] /= row_sum

        # Update second-stage Q values
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'tau', 'kappa1']"
iter1_run0_participant40.json,cognitive_model3,547.6986165657328,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based with uncertainty-driven exploration bonus modulated by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha, beta, kappa, alpha_u)
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - kappa in [0,1]: exploration bonus weight applied to uncertainty.
        - alpha_u in [0,1]: learning rate for uncertainty estimates.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Fixed, known transitions (common: 0.7).
    - The agent tracks per-alien uncertainty u[state, action] and adds a bonus
      b = kappa * (1 - stai) * u to encourage exploration; higher anxiety reduces
      exploration bonus.
    - Uncertainty u is updated via an error-driven rule toward |reward prediction error|.
    - Stage-1 choice is purely model-based using the bonus-augmented stage-2 values.
    """"""
    alpha, beta, kappa, alpha_u = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 values and uncertainty
    Q2 = np.zeros((2, 2))
    U2 = np.ones((2, 2))  # initialize high uncertainty

    bonus_scale = kappa * (1.0 - stai_val)

    for t in range(n_trials):
        # Build bonus-augmented values for each state
        Q2_plus = Q2 + bonus_scale * U2

        # Model-based projection to stage 1 using augmented values
        max_Q2_plus = np.max(Q2_plus, axis=1)
        Q1_MB = transition_matrix @ max_Q2_plus

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited state using augmented values
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2_plus[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # TD error on raw value (without bonus) for learning Q2
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha * delta2

        # Uncertainty update toward absolute prediction error |delta2|
        U2[s2, a2] += alpha_u * (abs(delta2) - U2[s2, a2])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'kappa', 'alpha_u']"
iter1_run0_participant42.json,cognitive_model2,539.5435419515989,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-bonus model with anxiety-modulated exploration and model-based lookahead.
    
    Second-stage action values carry an exploration bonus proportional to a learned uncertainty
    signal per alien. First-stage values are computed via model-based lookahead using the transition
    structure and the bonus-augmented second-stage values. Anxiety reduces the effective inverse
    temperature (i.e., increases exploration) via a scaling factor.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int
        Second-stage state encountered (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int
        Second-stage choices within state (0/1; e.g., aliens W/S on X, P/H on Y).
    reward : array-like of float
        Scalar reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; higher values indicate higher anxiety. Used to reduce beta.
    model_parameters : sequence of floats
        [alpha, beta_base, xi, tau_u, c_anx]
        Bounds:
        - alpha in [0, 1]: learning rate for Q-updates at second stage.
        - beta_base in [0, 10]: baseline inverse temperature.
        - xi in [0, 1]: weight of the uncertainty bonus added to Q-values.
        - tau_u in [0, 1]: learning rate for the uncertainty signal update.
        - c_anx in [0, 1]: strength by which anxiety reduces inverse temperature,
          beta_eff = beta_base * (1 - c_anx * stai).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta_base, xi, tau_u, c_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition matrix (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Storage for probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values and uncertainty per alien
    q2 = np.zeros((2, 2))   # per state and alien
    u2 = np.ones((2, 2)) * 0.5  # start moderately uncertain

    # First-stage MF cache not used; we compute MB values each trial
    eps = 1e-12

    # Anxiety-modulated temperature
    beta_eff = beta_base * (1.0 - c_anx * stai_val)
    beta_eff = max(0.0, min(beta_eff, 10.0))

    for t in range(n_trials):
        # Construct bonus-augmented second-stage values
        q2_aug = q2 + xi * u2

        # Model-based first-stage values: expected max over each planet
        max_q2_aug = np.max(q2_aug, axis=1)     # shape (2,)
        q1_mb = T @ max_q2_aug                  # shape (2,)

        # Stage 1 policy
        logits1 = beta_eff * q1_mb
        z1 = np.max(logits1)
        probs1 = np.exp(logits1 - z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta_eff * q2_aug[s]
        z2 = np.max(logits2)
        probs2 = np.exp(logits2 - z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update uncertainty signal first (volatility-like PE magnitude)
        pe2_pred = r - q2[s, a2]
        u2[s, a2] = (1.0 - tau_u) * u2[s, a2] + tau_u * abs(pe2_pred)

        # Update second-stage Q-values
        q2[s, a2] += alpha * pe2_pred

        # No explicit first-stage Q since we use model-based lookahead each trial

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta_base', 'xi', 'tau_u', 'c_anx']"
iter1_run0_participant42.json,cognitive_model3,410.28959960785255,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planner with learned transition model, Q-value decay, and anxiety-biased transition beliefs.
    
    The agent learns both the second-stage values and the first-stage transition matrix from experience.
    Anxiety biases transition beliefs toward uncontrollable (uniform) dynamics, reducing the confidence in
    learned transitions. Additionally, Q-values decay toward zero over trials to capture forgetting.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int
        Second-stage state encountered (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int
        Second-stage choices within state (0/1; e.g., aliens W/S on X, P/H on Y).
    reward : array-like of float
        Scalar reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; higher values indicate higher anxiety. Increases transition-uniformity bias.
    model_parameters : sequence of floats
        [alpha, beta1, beta2, decay, bias_base]
        Bounds:
        - alpha in [0, 1]: learning rate for both transition model and Q-updates.
        - beta1 in [0, 10]: inverse temperature for first-stage softmax.
        - beta2 in [0, 10]: inverse temperature for second-stage softmax.
        - decay in [0, 1]: per-trial decay rate for all Q-values toward zero.
        - bias_base in [0, 1]: magnitude of anxiety-induced bias that pulls transitions toward uniform.
          Effective bias = bias_base * stai.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta1, beta2, decay, bias_base = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize transition estimates with known prior (common/rare)
    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Second-stage Q-values and first-stage MF (for bootstrapping/regularization)
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-biased transition prior strength
    bias = max(0.0, min(1.0, bias_base * stai_val))

    for t in range(n_trials):
        # Apply decay to values (forgetting)
        q2 *= (1.0 - decay)
        q1_mf *= (1.0 - decay)

        # Bias transition estimates toward uniform according to anxiety
        T_eff = (1.0 - bias) * T_est + bias * 0.5

        # Model-based stage-1 values from T_eff and current q2
        max_q2 = np.max(q2, axis=1)          # best value on each planet
        q1_mb = T_eff @ max_q2               # expected value per spaceship

        # Combine MB and a small MF regularizer (implicit via q1_mf)
        q1_comb = 0.8 * q1_mb + 0.2 * q1_mf  # fixed mixture to stabilize; both are learned/updated

        # Stage 1 policy
        logits1 = beta1 * q1_comb
        z1 = np.max(logits1)
        probs1 = np.exp(logits1 - z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta2 * q2[s]
        z2 = np.max(logits2)
        probs2 = np.exp(logits2 - z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model for the chosen first-stage action using alpha
        one_hot = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1] = (1.0 - alpha) * T_est[a1] + alpha * one_hot

        # Second-stage learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First-stage MF bootstrap from obtained second-stage value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta1', 'beta2', 'decay', 'bias_base']"
iter1_run0_participant43.json,cognitive_model1,534.1634991099011,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Hybrid MBâMF with anxiety-modulated transition learning.

    Idea:
    - Stage-2 values are learned model-free from reward.
    - Stage-1 uses a hybrid of model-free Q1 and model-based evaluation using a learned transition model.
    - The transition model T(s'|a) is learned online; anxiety reduces transition learning, making the agent rely
      more on prior/common-transition beliefs (i.e., slower to update surprises).
    
    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha_v (0..1), value learning rate for stage-2 MF and stage-1 TD backup
    - model_parameters[1]: beta (0..10), inverse temperature used for both stages
    - model_parameters[2]: alpha_tr (0..1), base learning rate for transitions T(s'|a)
    - model_parameters[3]: omega (0..1), weight of model-based vs model-free at stage-1 (Q = (1-omega)*MF + omega*MB)
    - model_parameters[4]: k_anx_tr (0..1), scales reduction of transition learning with STAI
         alpha_tr_eff = alpha_tr * (1 - k_anx_tr * stai)
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on the reached planet
    - reward: array of floats (e.g., 0.0 or 1.0)
    - stai: array-like with a single float in [0,1], STAI score
    - model_parameters: list/array of parameter values as above
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """"""
    alpha_v, beta, alpha_tr, omega, k_anx_tr = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize choice probability storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 MF values: Q2[state, action]
    q2 = np.zeros((2, 2))
    # Stage-1 MF values: Q1[action]
    q1_mf = np.zeros(2)

    # Learned transition model T(s'|a)
    # Start with a prior reflecting the task's common transitions (0.7/0.3)
    # Implemented via pseudo-counts; these set the initial probabilities.
    prior_strength = 2.0  # fixed constant prior strength
    T = np.array([
        [0.7, 0.3],  # P(X|A), P(Y|A)
        [0.3, 0.7],  # P(X|U), P(Y|U)
    ], dtype=float)
    # Represent T via soft counts to allow simple incremental updates
    counts = T * prior_strength

    eps = 1e-10

    for t in range(n_trials):
        # Compute model-based Q at stage-1 from current T and max Q2 per state
        max_q2 = np.max(q2, axis=1)  # size 2: [max on X, max on Y]
        q1_mb = T @ max_q2  # size 2: expected value of each spaceship

        # Hybrid Q at stage-1
        q1 = (1.0 - omega) * q1_mf + omega * q1_mb

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Observe transition
        s = int(state[t])

        # Stage-2 policy in reached state
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update stage-2 MF values
        delta2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha_v * delta2

        # TD backup to stage-1 MF from the realized state-action value (eligibility via one-step backup)
        # Use the chosen stage-2 action value as the outcome for stage-1 MF
        backup_val = q2[s, a2]
        q1_mf[a1] = q1_mf[a1] + alpha_v * (backup_val - q1_mf[a1])

        # Update transition model counts and probabilities; anxiety reduces the step size
        alpha_tr_eff = alpha_tr * (1.0 - k_anx_tr * stai_val)
        if alpha_tr_eff < 0.0:
            alpha_tr_eff = 0.0
        # Convert counts to probabilities for the chosen action via an incremental update
        # First, form the current probs for chosen action
        curr_probs = counts[a1] / (np.sum(counts[a1]) + eps)
        # Target one-hot for observed next state
        target = np.zeros(2)
        target[s] = 1.0
        # Incremental probability update toward target
        new_probs = (1 - alpha_tr_eff) * curr_probs + alpha_tr_eff * target
        # Map back to counts by preserving total mass
        total = np.sum(counts[a1])
        counts[a1] = new_probs * total
        # Update T from counts for both actions
        T = (counts.T / (np.sum(counts, axis=1) + eps)).T

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_v', 'beta', 'alpha_tr', 'omega', 'k_anx_tr']"
iter1_run0_participant43.json,cognitive_model2,565.619789329795,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Risk-sensitive model-based planner with anxiety-inflated risk aversion.

    Idea:
    - Track for each alien the mean and variance of reward using exponential recency weighting.
    - Use a risk-sensitive utility: U = mean - phi * sqrt(variance), where phi increases with STAI.
    - Stage-1 choices are model-based: compute expected utility of each spaceship via the known transition matrix.
    - Stage-2 choices use the utilities directly.

    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha_val (0..1), learning rate for reward mean per alien
    - model_parameters[1]: beta (0..10), inverse temperature for softmax at both stages
    - model_parameters[2]: alpha_var (0..1), learning rate for reward variance per alien
    - model_parameters[3]: rho_base (0..1), baseline risk aversion weight
    - model_parameters[4]: k_anx_risk (0..1), increase of risk aversion with STAI
         phi = rho_base + k_anx_risk * stai

    Inputs:
    - action_1: array of ints in {0,1} (0=A, 1=U)
    - state: array of ints in {0,1} (0=X, 1=Y)
    - action_2: array of ints in {0,1} (alien index within planet)
    - reward: array of floats (e.g., 0/1)
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of the 5 parameters above

    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """"""
    alpha_val, beta, alpha_var, rho_base, k_anx_risk = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (common transitions)
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Mean and variance trackers for each alien within each planet
    m = np.zeros((2, 2))
    v = np.ones((2, 2)) * 0.25  # initial variance guess for Bernoulli outcome

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # Risk aversion coefficient
    phi = rho_base + k_anx_risk * stai_val
    if phi < 0.0:
        phi = 0.0
    if phi > 1.0:
        phi = 1.0

    for t in range(n_trials):
        # Stage-2 utilities (risk-adjusted)
        U2 = m - phi * np.sqrt(np.maximum(v, 1e-8))  # shape (2,2)

        # Stage-1 model-based value = expected max utility over reached states
        max_u2 = np.max(U2, axis=1)  # per state
        q1_mb = T @ max_u2

        # Stage-1 policy
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = int(state[t])
        logits2 = beta * U2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        # Update mean for the chosen alien
        m_prev = m[s, a2]
        m[s, a2] = m_prev + alpha_val * (r - m_prev)

        # Update variance using exponential smoothing of squared deviation
        # v <- v + alpha_var * ( (r - m_prev)^2 - v ), use pre-update m_prev for stability
        sq_err = (r - m_prev) ** 2
        v[s, a2] = v[s, a2] + alpha_var * (sq_err - v[s, a2])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_val', 'beta', 'alpha_var', 'rho_base', 'k_anx_risk']"
iter1_run0_participant43.json,cognitive_model3,534.0895903903483,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    MF learner with anxiety-modulated lapse and directed exploration bonus.

    Idea:
    - Pure model-free TD learning at stage-2; stage-1 receives a one-step TD backup from stage-2.
    - Add a directed exploration bonus at stage-2 based on visit-count uncertainty (eta / sqrt(n+1)).
    - Incorporate an anxiety-modulated lapse probability epsilon:
        final choice prob = (1 - epsilon) * softmax + epsilon * uniform.
      Higher anxiety increases lapses.

    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha (0..1), learning rate for MF value updates
    - model_parameters[1]: beta (0..10), inverse temperature used for both stages
    - model_parameters[2]: epsilon_base (0..1), baseline lapse probability
    - model_parameters[3]: k_anx_lapse (0..1), slope by which STAI increases lapse
         epsilon = clip(epsilon_base + k_anx_lapse * stai, 0, 1)
    - model_parameters[4]: eta (0..1), strength of directed exploration bonus at stage-2

    Inputs:
    - action_1: array of ints in {0,1} (spaceship)
    - state: array of ints in {0,1} (planet)
    - action_2: array of ints in {0,1} (alien on planet)
    - reward: array of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of 5 parameters as above

    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, epsilon_base, k_anx_lapse, eta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated lapse
    epsilon = epsilon_base + k_anx_lapse * stai_val
    if epsilon < 0.0:
        epsilon = 0.0
    if epsilon > 1.0:
        epsilon = 1.0

    # Model-free values
    q1 = np.zeros(2)        # stage-1 MF
    q2 = np.zeros((2, 2))   # stage-2 MF

    # Visit counts for directed exploration bonus at stage-2
    n_visits = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        # Stage-1 policy (with lapse)
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 = soft1 / (np.sum(soft1) + eps)
        probs1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s = int(state[t])

        # Stage-2 policy: add directed exploration bonus
        bonus = eta / np.sqrt(n_visits[s] + 1.0)
        logits2 = beta * (q2[s] + bonus)
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 = soft2 / (np.sum(soft2) + eps)
        probs2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        # Update stage-2 MF
        delta2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha * delta2

        # Increment visit count after choice for exploration statistics
        n_visits[s, a2] += 1.0

        # One-step TD backup to stage-1
        q1[a1] = q1[a1] + alpha * (q2[s, a2] - q1[a1])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'epsilon_base', 'k_anx_lapse', 'eta']"
iter1_run0_participant44.json,cognitive_model1,435.8422761747105,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-weighted arbitration, perseveration, and rarity aversion.
    
    Idea:
    - Learn the stage-1 transition model online; compute model-based (MB) action values from learned transitions.
    - Arbitration between model-based and model-free (MF) values is state/action-specific and depends on transition uncertainty.
      Anxiety increases MF reliance by down-weighting MB when uncertainty is present.
    - Perseveration is amplified by anxiety.
    - ""Rarity aversion"" bias favors actions expected to lead to common transitions; anxiety amplifies this bias.
    
    Parameters (bounds):
    - alpha in [0,1]: learning rate for Q updates (both stages).
    - beta in [0,10]: inverse temperature for both stages.
    - eta_T in [0,1]: transition learning rate for updating the stage-1 transition model.
    - xi in [0,1]: perseveration strength (anxiety amplifies it).
    - phi in [0,1]: rarity-aversion strength (anxiety amplifies it).
    
    Inputs:
    - action_1: array-like ints {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like ints {0,1} reached planet (0=X, 1=Y).
    - action_2: array-like ints {0,1} second-stage choices (aliens per planet).
    - reward: array-like floats, typically {0.0,1.0}.
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, eta_T, xi, phi].
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, eta_T, xi, phi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model (rows: actions A/U, cols: states X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Qs
    q1_mf = np.zeros(2)           # stage-1 MF values for spaceships
    q2_mf = np.zeros((2, 2))      # stage-2 MF values for aliens on each planet

    # Anxiety-modulated coefficients
    xi_eff = xi * (0.5 + 0.5 * stai)          # more anxiety -> more perseveration
    phi_eff = phi * stai                      # more anxiety -> stronger rarity aversion

    prev_a1 = None
    prev_a2 = None

    for t in range(n_trials):
        # Compute model-based Q1 from learned transitions
        max_q2 = np.max(q2_mf, axis=1)          # [best on X, best on Y]
        q1_mb = T @ max_q2                      # MB Q for each spaceship

        # Arbitration weight per action based on transition uncertainty (entropy), reduced by anxiety
        # For each action a, use w_a = (1 - stai) * (1 - H(T[a]))
        # Entropy H in [0,1] after normalization by log(2)
        w = np.zeros(2)
        for a in range(2):
            p = T[a, :]
            # avoid extreme zeros
            p_clip = np.clip(p, 1e-8, 1 - 1e-8)
            H = -np.sum(p_clip * np.log(p_clip)) / np.log(2.0)  # normalized entropy in [0,1]
            w[a] = (1.0 - stai) * (1.0 - H)
        q1_combined = (1.0 - w) * q1_mf + w * q1_mb

        # Rarity aversion bias toward actions with higher ""commonness"" (max transition prob)
        rarity_bias = np.array([np.max(T[0, :]), np.max(T[1, :])]) - 0.5
        rarity_bias *= phi_eff

        # perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += xi_eff

        # First-stage choice probabilities
        logits1 = beta * q1_combined + bias1 + rarity_bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probabilities with perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += xi_eff
        logits2 = beta * q2_mf[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Transition learning update for T given chosen a1 and observed s
        # Move row a1 toward one-hot of observed state
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += eta_T * (target - T[a1, sp])
        # Renormalize row for safety
        T[a1, :] = np.clip(T[a1, :], 1e-8, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

        # Model-free TD updates
        # Stage-1 MF bootstraps from chosen second-stage value
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Stage-2 update from reward
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_ll

","['alpha', 'beta', 'eta_T', 'xi', 'phi']"
iter1_run0_participant44.json,cognitive_model2,455.41932945546387,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Valence-asymmetric MF with anxiety-modulated stay/switch bias from transition-reward interaction and stage-2 stickiness.
    
    Idea:
    - Pure model-free learning with separate learning rates for positive vs negative prediction errors.
    - Anxiety amplifies a dynamic ""stay"" bias that depends on whether last trial's transition was common/rare and rewarded:
      replicate the classical two-step interaction as a bias on repeating the previous first-stage action.
    - Stage-2 perseveration included and amplified by anxiety.
    
    Parameters (bounds):
    - alpha_pos in [0,1]: learning rate for positive TD errors.
    - alpha_neg in [0,1]: learning rate for negative TD errors.
    - beta in [0,10]: inverse temperature for both stages.
    - psi in [0,1]: strength of the stay/switch bias from transition-reward interaction (scaled by anxiety).
    - theta in [0,1]: stage-2 perseveration strength (scaled by anxiety).
    
    Inputs:
    - action_1: array-like ints {0,1} first-stage choices.
    - state: array-like ints {0,1} reached planet.
    - action_2: array-like ints {0,1} second-stage choices.
    - reward: array-like floats {0,1}.
    - stai: array-like with a single float in [0,1].
    - model_parameters: [alpha_pos, alpha_neg, beta, psi, theta]
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """"""
    alpha_pos, alpha_neg, beta, psi, theta = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Qs
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Keep track of previous trial to shape the dynamic bias
    prev_a1 = None
    prev_s = None
    prev_r = None
    prev_transition_common = None
    prev_a2 = None

    # Common transition structure assumed known for bias computation only (0->X common, 1->Y common)
    # This is not used for MB planning, only to compute the stay/switch bias term.
    common_map = np.array([0, 1])  # action 0 commonly -> state 0; action 1 commonly -> state 1

    theta_eff = theta * (0.5 + 0.5 * stai)

    for t in range(n_trials):
        # Dynamic stay/switch bias at stage 1 based on previous trial
        # If last transition was common and rewarded OR rare and unrewarded -> positive bias to repeat.
        # Otherwise -> negative bias to repeat.
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            sign = 0.0
            if prev_transition_common is not None and prev_r is not None:
                if (prev_transition_common and prev_r > 0.0) or ((not prev_transition_common) and prev_r <= 0.0):
                    sign = 1.0
                else:
                    sign = -1.0
            bias_strength = psi * stai * sign
            bias1[prev_a1] += bias_strength

        # First-stage policy: softmax over MF Q with dynamic bias
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += theta_eff
        logits2 = beta * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]
        r = reward[t]

        # MF updates with valence-asymmetric learning rates
        delta2 = r - q2[s, a2]
        alpha2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2 * delta2

        # Stage-1 bootstraps from chosen second-stage value
        delta1 = q2[s, a2] - q1[a1]
        alpha1 = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1[a1] += alpha1 * delta1

        # Update prev info for next trial's bias
        prev_transition_common = (s == common_map[a1])
        prev_a1 = a1
        prev_a2 = a2
        prev_s = s
        prev_r = r

    eps = 1e-12
    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_ll

","['alpha_pos', 'alpha_neg', 'beta', 'psi', 'theta']"
iter1_run0_participant5.json,cognitive_model1,409.1915032307151,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-weighted hybrid learner with stickiness and transition-surprise gating.
    
    Core ideas:
    - Stage 1 uses a hybrid of model-free (MF) and model-based (MB) values.
    - Anxiety down-weights MB arbitration weight and increases choice stickiness (perseveration).
    - Rare transitions gate credit assignment to stage-1 MF values more strongly when anxiety is high.
    - Stage 2 uses standard Q-learning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = alien within the visited planet).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]. Higher values:
        - reduce the model-based weight at stage 1,
        - increase choice stickiness,
        - increase gating (down-weight) of MF credit assignment after rare transitions.
    model_parameters : array-like of float
        [alpha, beta, w_base, kappa]
        - alpha in [0,1]: learning rate for Q-value updates.
        - beta in [0,10]: inverse temperature for both stages.
        - w_base in [0,1]: baseline MB weight (reduced by anxiety).
        - kappa in [0,1]: baseline stickiness; anxiety scales it up.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, w_base, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: rows = action at stage 1, cols = next state
    # A commonly -> X, U commonly -> Y
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)        # stage-1 model-free
    q2 = 0.5 * np.ones((2, 2)) # stage-2 action values (both aliens on each planet)

    # Choice stickiness biases (applied as additive bias to the chosen action from previous trial)
    bias1 = np.zeros(2)
    bias2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    w_eff = np.clip(w_base * (1.0 - 0.7 * stai), 0.0, 1.0)
    kappa_eff = kappa * (0.2 + 0.8 * stai)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    prev_a1 = None
    prev_s = None
    prev_a2 = None

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute MB value at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid stage-1 values with stickiness
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf + bias1

        # Stage-1 policy
        exp1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with within-state stickiness
        q2_with_bias = q2[s] + bias2[s]
        exp2 = np.exp(beta * (q2_with_bias - np.max(q2_with_bias)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Identify whether the transition was common vs rare for gating
        # Common if (a1=0 -> s=0) or (a1=1 -> s=1)
        is_common = (a1 == s)
        # Gate MF credit assignment at stage 1 more strongly on rare transitions when anxiety is high
        gate_mf = 1.0 if is_common else (1.0 - 0.7 * stai)
        gate_mf = np.clip(gate_mf, 0.0, 1.0)

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF learning (bootstrapping from obtained stage-2 value)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += (alpha * gate_mf) * pe1

        # Update stickiness biases for next trial
        bias1[:] = 0.0
        bias1[a1] += kappa_eff

        bias2[:] = 0.0
        bias2[s, a2] += kappa_eff

        prev_a1, prev_s, prev_a2 = a1, s, a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'w_base', 'kappa']"
iter1_run0_participant5.json,cognitive_model2,437.57364069931816,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Directed exploration with anxiety-modulated bonus and valence-dependent learning rates.

    Core ideas:
    - Stage 2 action selection includes an uncertainty bonus derived from recent outcome volatility.
    - Anxiety reduces directed exploration (smaller bonus) but increases learning rates.
    - Learning uses separate rates for positive vs negative prediction errors.
    - Stage 1 is model-based (planning via transition matrix) from current stage-2 values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = aliens).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - reduces directed exploration bonus,
        - increases effective learning rate magnitude,
        - indirectly affects stage-1 policy via updated Q-values.
    model_parameters : array-like of float
        [alpha_pos, alpha_neg, beta, bonus0, decay]
        - alpha_pos in [0,1]: base learning rate for positive PEs.
        - alpha_neg in [0,1]: base learning rate for negative PEs.
        - beta in [0,10]: inverse temperature for both stages.
        - bonus0 in [0,1]: base scale of uncertainty bonus at stage 2.
        - decay in [0,1]: volatility tracking rate (EMA of squared PEs).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_pos, alpha_neg, beta, bonus0, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = 0.5 * np.ones((2, 2))  # stage-2 action values
    var2 = np.zeros((2, 2))     # volatility proxy: EMA of squared PEs

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration bonus scale
    bonus_scale = np.clip(bonus0 * (1.0 - stai), 0.0, 1.0)
    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage 1 model-based planning from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Stage-1 policy (pure MB)
        exp1 = np.exp(beta * (q1_mb - np.max(q1_mb)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus based on volatility
        bonus = bonus_scale * np.sqrt(np.maximum(var2[s], 0.0))
        q2_bonus = q2[s] + bonus
        exp2 = np.exp(beta * (q2_bonus - np.max(q2_bonus)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 with valence-dependent, anxiety-amplified rates
        pe2 = r - q2[s, a2]
        alpha_val = alpha_pos if pe2 >= 0.0 else alpha_neg
        # Anxiety increases responsiveness to outcomes
        alpha_eff = np.clip(alpha_val * (0.5 + 0.5 * stai), 0.0, 1.0)

        # Update Q and volatility proxy
        q2[s, a2] += alpha_eff * pe2
        var2[s, a2] = (1.0 - decay) * var2[s, a2] + decay * (pe2 ** 2)

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha_pos', 'alpha_neg', 'beta', 'bonus0', 'decay']"
iter1_run0_participant5.json,cognitive_model3,451.1987242015752,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Risk-sensitive hybrid with anxiety-modulated lapse and inverse temperature.

    Core ideas:
    - Stage 2 uses risk-sensitive utility: action values are penalized by recent outcome volatility.
    - Lapse rate increases with anxiety; inverse temperature decreases with anxiety.
    - Stage 1 is a hybrid of MB and MF values, with MB weight decreasing with anxiety.
    - Credit assignment from stage 2 to stage 1 uses an eligibility that increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = aliens).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - increases lapse rate,
        - lowers inverse temperature (more noise),
        - increases risk sensitivity (stronger variance penalty),
        - increases credit assignment from stage 2 to stage 1 (eligibility).
    model_parameters : array-like of float
        [alpha, beta0, lapse0, rho]
        - alpha in [0,1]: learning rate for Q updates.
        - beta0 in [0,10]: baseline inverse temperature.
        - lapse0 in [0,1]: baseline lapse rate (scaled by anxiety).
        - rho in [0,1]: baseline risk-sensitivity scaling for variance penalty.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta0, lapse0, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values and volatility trackers
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))
    var2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    w_mb = np.clip(1.0 - stai, 0.0, 1.0)                # arbitration: more anxiety -> less MB
    beta_eff = np.clip(beta0 * (1.0 - 0.5 * stai), 0.0, 10.0)  # more anxiety -> lower beta
    lapse_eff = np.clip(lapse0 * stai, 0.0, 1.0)        # more anxiety -> more lapses
    risk_eff = np.clip(rho * (0.5 + 0.5 * stai), 0.0, 1.0)     # more anxiety -> stronger variance penalty
    elig = np.clip(0.3 + 0.7 * stai, 0.0, 1.0)          # credit assignment from stage 2 to stage 1

    eps = 1e-12
    decay_var = 0.2  # fixed EMA rate for volatility; not a parameter

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based planning at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid stage-1 values
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy with lapse
        exp1 = np.exp(beta_eff * (q1_hybrid - np.max(q1_hybrid)))
        soft1 = exp1 / (np.sum(exp1) + eps)
        probs1 = (1.0 - lapse_eff) * soft1 + lapse_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 risk-sensitive policy: penalize by volatility (uncertainty)
        util2 = q2[s] - risk_eff * np.maximum(var2[s], 0.0)
        exp2 = np.exp(beta_eff * (util2 - np.max(util2)))
        soft2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - lapse_eff) * soft2 + lapse_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2
        var2[s, a2] = (1.0 - decay_var) * var2[s, a2] + decay_var * (pe2 ** 2)

        # Stage-1 MF update via eligibility-weighted bootstrapping
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += (alpha * elig) * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha', 'beta0', 'lapse0', 'rho']"
iter1_run0_participant6.json,cognitive_model1,522.9538781389597,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 1: Model-based uncertainty bonus (UCB) with anxiety-modulated exploration and perseveration.

    Concept
    -------
    - Uses a model-based planner at stage 1 that looks ahead to the best action on each planet.
    - Adds an uncertainty bonus (UCB) to second-stage action values to encourage exploration.
    - Uncertainty is learned via an error-driven proxy that increases when prediction errors are large.
    - Anxiety reduces directed exploration (lower UCB weight) and increases perseveration (stickiness).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage planets (0=X, 1=Y) actually reached.
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices (0 or 1).
    reward : array-like of float
        Received coins (e.g., -1, 0, 1).
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha_q, alpha_unc, beta, w_ucb, stickiness]
        - alpha_q in [0,1]: learning rate for second-stage Q-values.
        - alpha_unc in [0,1]: learning rate for uncertainty proxy (driven by absolute prediction error).
        - beta in [0,10]: inverse temperature used for both stages.
        - w_ucb in [0,1]: base weight on uncertainty bonus (directed exploration).
        - stickiness in [0,1]: base perseveration strength at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_q, alpha_unc, beta, w_ucb, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage value and uncertainty estimates
    q2 = np.zeros((2, 2))
    unc2 = np.ones((2, 2))  # start maximally uncertain

    # Perseveration bias at stage 1
    last_a1 = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations:
    # - Lower exploration when anxiety is higher
    w_ucb_eff = np.clip(w_ucb * (1.0 - stai), 0.0, 1.0)
    # - Higher perseveration when anxiety is higher
    stick_eff = np.clip(stickiness * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Compute model-based values for stage 1 using UCB-augmented stage 2
        bonus = w_ucb_eff * np.sqrt(np.maximum(unc2, 0.0))
        q2_ucb = q2 + bonus
        max_q2_ucb = np.max(q2_ucb, axis=1)
        q1_mb = T @ max_q2_ucb  # expected best value on each planet

        # Add perseveration bias at stage 1
        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] = stick_eff

        # Stage 1 policy
        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state with UCB-augmented values
        s = int(state[t])
        logits2 = beta * q2_ucb[s]
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Outcome and updates
        r = reward[t]
        pe = r - q2[s, a2]
        # Update value
        q2[s, a2] += alpha_q * pe
        # Update uncertainty proxy by tracking absolute prediction error
        unc2[s, a2] = (1.0 - alpha_unc) * unc2[s, a2] + alpha_unc * abs(pe)

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_q', 'alpha_unc', 'beta', 'w_ucb', 'stickiness']"
iter1_run0_participant7.json,cognitive_model1,555.9438793428242,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive model-based learner with anxiety-modulated loss aversion and asymmetric learning.
    
    Idea:
    - The agent is fully model-based at stage 1: it computes the expected value of each spaceship
      using the known transition structure and the current second-stage values.
    - Outcomes are evaluated through a loss-averse utility function. Anxiety increases sensitivity
      to losses relative to gains.
    - Learning at stage 2 is asymmetric: separate learning rates for positive vs. negative utility
      prediction errors. Stage 1 uses the model-based projection of second-stage values.
    
    Parameters (model_parameters):
    - alpha_pos: [0,1] learning rate used when the second-stage utility prediction error >= 0
    - alpha_neg: [0,1] learning rate used when the second-stage utility prediction error < 0
    - beta: [0,10] inverse temperature for softmax at both stages
    - loss_base: [0,1] baseline loss aversion factor (transformed internally to [1,5])
    - k_loss: [0,1] anxiety modulation strength for loss aversion (higher stai -> higher loss aversion)
    
    Inputs:
    - action_1: int array in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: int array in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: int array in {0,1}, chosen alien per planet per trial
    - reward: float array, obtained coins per trial (can be negative/zero/positive)
    - stai: array-like with single float in [0,1], participant anxiety score
    - model_parameters: tuple/list of 5 parameters in the order above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_pos, alpha_neg, beta, loss_base, k_loss = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (common=0.7)
    p_common = 0.7
    transition_matrix = np.array([[p_common, 1 - p_common],
                                  [1 - p_common, p_common]])

    # Second-stage value (in utility space)
    q_stage2 = np.zeros((2, 2))  # Q(s, a)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss aversion; map [0,1] -> [1,5]
    loss_aversion = 1.0 + 4.0 * max(0.0, min(1.0, loss_base + k_loss * stai))

    for t in range(n_trials):
        # Stage-1 model-based values from current second-stage estimates
        max_q_stage2 = np.max(q_stage2, axis=1)  # best alien per planet in utility space
        q1_mb = transition_matrix @ max_q_stage2

        # Stage-1 policy
        q1_center = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1_center)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        q2 = q_stage2[s].copy()
        q2_center = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2_center)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome utility with anxiety-modulated loss aversion
        r = reward[t]
        util = r if r >= 0 else -loss_aversion * (-r)

        # Stage-2 learning with asymmetric learning rate in utility space
        delta2 = util - q_stage2[s, a2]
        alpha_use = alpha_pos if delta2 >= 0 else alpha_neg
        q_stage2[s, a2] += alpha_use * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'loss_base', 'k_loss']"
iter1_run0_participant7.json,cognitive_model2,555.1683233332742,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based learner with anxiety-modulated directed exploration via uncertainty bonus (UCB-like).
    
    Idea:
    - The agent maintains second-stage value estimates and a simple uncertainty proxy per alien
      based on recent absolute prediction errors (volatility-like).
    - An exploration bonus proportional to this uncertainty is added to action values during choice.
    - Anxiety reduces the exploration bonus, modeling reduced directed exploration under higher anxiety.
    - Stage 1 choices are model-based, projecting the uncertainty-bonus-augmented second-stage values
      through the known transition structure.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for second-stage value updates
    - beta: [0,10] inverse temperature for softmax at both stages
    - alpha_u: [0,1] learning rate for updating the uncertainty proxy (EMA of |PE|)
    - b_base: [0,1] baseline strength of exploration bonus
    - k_b: [0,1] anxiety modulation of exploration bonus (effective bonus b decreases with stai)
    
    Inputs:
    - action_1: int array in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: int array in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: int array in {0,1}, chosen alien per planet per trial
    - reward: float array, obtained coins per trial
    - stai: array-like with single float in [0,1], participant anxiety score
    - model_parameters: tuple/list of 5 parameters in the order above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, alpha_u, b_base, k_b = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (common=0.7)
    p_common = 0.7
    transition_matrix = np.array([[p_common, 1 - p_common],
                                  [1 - p_common, p_common]])

    # Second-stage values and uncertainty proxy
    q_stage2 = np.zeros((2, 2))  # Q(s, a)
    u_stage2 = np.zeros((2, 2))  # uncertainty proxy (EMA of |prediction error|)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration bonus strength
    b_eff = max(0.0, min(1.0, b_base * (1.0 - k_b * stai)))

    for t in range(n_trials):
        # Stage-2 decision values include an uncertainty bonus
        q2_aug = q_stage2 + b_eff * u_stage2

        # Stage-1 model-based values computed from augmented second-stage values
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = transition_matrix @ max_q2_aug

        # Stage-1 policy
        q1_center = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1_center)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in reached state using augmented values
        s = state[t]
        q2_s_aug = q2_aug[s].copy()
        q2_center = q2_s_aug - np.max(q2_s_aug)
        probs_2 = np.exp(beta * q2_center)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning from outcome (update base values and uncertainty proxy)
        r = reward[t]
        delta2 = r - q_stage2[s, a2]
        # Value update
        q_stage2[s, a2] += alpha * delta2
        # Uncertainty proxy update (EMA of absolute PE)
        u_stage2[s, a2] = (1.0 - alpha_u) * u_stage2[s, a2] + alpha_u * abs(delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'alpha_u', 'b_base', 'k_b']"
iter1_run0_participant7.json,cognitive_model3,505.896363523025,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid arbitration with learned transitions and anxiety-modulated planning reliance.
    
    Idea:
    - The agent learns transition probabilities online and uses them to compute model-based (MB)
      first-stage values. Model-free (MF) first-stage values are learned via bootstrapping from
      second-stage values (TD).
    - Arbitration weight on MB control increases with transition certainty but is reduced by anxiety.
      This yields dynamic reliance on planning that reflects how well the transition model is learned
      and the participant's anxiety.
    
    Parameters (model_parameters):
    - alpha_r: [0,1] learning rate for reward/value updates at stage 2 and MF stage 1
    - beta: [0,10] inverse temperature for softmax at both stages
    - alpha_T_base: [0,1] baseline learning rate for updating transition probabilities
    - k_Tstai: [0,1] anxiety modulation of transition learning rate (higher stai -> slower T learning)
    - w_bias: [0,1] baseline arbitration weight scale favoring MB when transitions are certain
    
    Inputs:
    - action_1: int array in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: int array in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: int array in {0,1}, chosen alien per planet per trial
    - reward: float array, obtained coins per trial
    - stai: array-like with single float in [0,1], participant anxiety score
    - model_parameters: tuple/list of 5 parameters in the order above
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_r, beta, alpha_T_base, k_Tstai, w_bias = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T[a, s], rows sum to 1
    T = np.full((2, 2), 0.5)

    # Value functions
    q_stage2 = np.zeros((2, 2))  # Q(s, a) at stage 2
    q_stage1_mf = np.zeros(2)    # MF first-stage Q(a)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate
    alpha_T = max(0.0, min(1.0, alpha_T_base * (1.0 - k_Tstai * stai)))

    for t in range(n_trials):
        # Model-based first-stage values computed from learned transitions
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = T @ max_q2

        # Transition certainty in [0,1]: 0.0 when uniform (0.5), 1.0 when deterministic (0 or 1)
        certainty = np.mean(np.abs(T[:, 0] - 0.5)) * 2.0
        certainty = max(0.0, min(1.0, certainty))

        # Anxiety-reduced MB weight; higher stai lowers reliance on MB
        w = w_bias * certainty * (1.0 - stai)
        w = max(0.0, min(1.0, w))

        # Arbitration between MB and MF at stage 1
        q1 = w * q1_mb + (1.0 - w) * q_stage1_mf

        # Stage-1 policy
        q1_center = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_center)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2 = q_stage2[s].copy()
        q2_center = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2_center)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update values
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * delta2

        # Stage-1 MF TD update bootstrapping from updated second-stage value
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Update learned transition matrix row for chosen action based on observed next state
        # One-hot target for transition
        target = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1, :] = (1.0 - alpha_T) * T[a1, :] + alpha_T * target
        # Ensure numerical stability and row normalization
        row_sum = T[a1, :].sum()
        if row_sum > 0:
            T[a1, :] = T[a1, :] / row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta', 'alpha_T_base', 'k_Tstai', 'w_bias']"
iter1_run0_participant8.json,cognitive_model1,362.544450574257,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated stickiness and transition-contingent stay/switch bias.
    
    Summary
    -------
    - Stage 2: model-free Q-learning of alien values.
    - Stage 1: hybrid value = w * model-based + (1 - w) * model-free.
      Model-based uses fixed transition matrix.
    - Perseveration (choice stickiness) and a stay/switch bias depend on the previous trialâs
      transition (common vs rare). Anxiety amplifies both biases and reduces the MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [alpha2, beta, w0, kappa_stick, xi_trans]
        - alpha2: learning rate for stage-2 Q-learning, used also for stage-1 MF bootstrapping [0,1]
        - beta: inverse temperature for both stages [0,10]
        - w0: baseline MB weight at stage 1 before anxiety modulation [0,1]
        - kappa_stick: strength of perseveration (repeat previous a1) bias [0,1]
        - xi_trans: strength of transition-contingent stay/switch bias [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha2, beta, w0, kappa_stick, xi_trans = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition matrix: rows = action1 (A,U), cols = planet (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values (planet x alien)
    q2 = np.zeros((2, 2)) + 0.5  # neutral initialization

    # Stage-1 MF Q-values for choosing A/U
    q1_mf = np.zeros(2) + 0.0

    # Bias trackers
    prev_a1 = None
    prev_trans_common = None

    # Anxiety-modulated parameters
    # Higher anxiety reduces MB weight and increases perseveration and transition-driven bias
    w_eff_base = np.clip(w0 * (1.0 - 0.4 * st), 0.0, 1.0)
    kappa_eff = kappa_stick * (0.5 + 0.5 * st)
    xi_eff = xi_trans * (0.5 + 0.5 * st)

    for t in range(n_trials):
        # Model-based evaluation: expected max Q2 under transitions
        max_q2 = np.max(q2, axis=1)  # size 2 for planets X,Y
        q1_mb = T @ max_q2           # size 2 for actions A,U

        # Combine MB and MF
        q1 = w_eff_base * q1_mb + (1.0 - w_eff_base) * q1_mf

        # Add perseveration bias (stay with previous a1)
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += kappa_eff

        # Add transition-contingent stay/switch bias on the first-stage policy:
        # - If previous transition was common: bias to stay.
        # - If previous transition was rare: bias to switch.
        if (prev_a1 is not None) and (prev_trans_common is not None):
            if prev_trans_common:
                bias[prev_a1] += xi_eff
            else:
                bias[1 - prev_a1] += xi_eff

        # First-stage action probabilities
        logits1 = q1 + bias
        logits1 = logits1 - np.max(logits1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage action probabilities (within reached state)
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]

        # Stage 2 TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage 1 MF bootstrapping toward the experienced stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Update bias memory and transition type
        # Determine whether the experienced transition was common given a1->s
        prob_to_s = T[a1, s]
        prev_trans_common = (prob_to_s >= 0.5)
        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'w0', 'kappa_stick', 'xi_trans']"
iter1_run0_participant8.json,cognitive_model2,509.60995454069814,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Kalman-uncertainty planner with anxiety-boosted uncertainty exploration.
    
    Summary
    -------
    - Stage 2: Each alien's value is tracked by a simple Kalman filter (mean and variance).
      Process noise scales with an anxiety-modulated volatility parameter.
    - Choice at stage 2 uses a softmax over mean + exploration bonus proportional to uncertainty.
      Anxiety increases the exploration bonus.
    - Stage 1: Model-based evaluation from fixed transitions over the current
      uncertainty-bonused alien values, mixed by an MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [w_mb, beta, kappa_bonus, alpha_vol, sigma0]
        - w_mb: weight of MB evaluation at stage 1 [0,1]
        - beta: inverse temperature for both stages [0,10]
        - kappa_bonus: scale of uncertainty exploration bonus [0,1]
        - alpha_vol: process noise scale (effective volatility) [0,1]
        - sigma0: initial uncertainty (variance) of each alien value [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    w_mb, beta, kappa_bonus, alpha_vol, sigma0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Kalman means and variances per planet x alien
    m = np.zeros((2, 2)) + 0.5
    v = np.zeros((2, 2)) + sigma0

    # Anxiety effects:
    # - Increase process noise (volatility) with anxiety.
    # - Increase exploration bonus with anxiety.
    q_process = alpha_vol * (0.1 + 0.9 * st)  # in [0,1]
    r_meas = 0.25  # observation noise (fixed)
    bonus_scale = kappa_bonus * (0.5 + 0.5 * st)

    for t in range(n_trials):
        # Build exploration-bonused values for both planets
        bonus = bonus_scale * np.sqrt(np.maximum(v, 1e-8))
        q2_eff = m + bonus

        # Stage 1 MB value is expected max over aliens on each planet
        max_q2_eff = np.max(q2_eff, axis=1)
        q1_mb = T @ max_q2_eff

        # No explicit MF here; use MB only, scaled by w_mb and centered for stability
        q1 = w_mb * q1_mb

        # First-stage policy
        logits1 = q1 - np.max(q1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy within the reached planet
        s = state[t]
        logits2 = q2_eff[s] - np.max(q2_eff[s])
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Kalman update for visited alien
        r = reward[t]
        # Time update (add process noise to all aliens; simple diffusion)
        v = v + q_process

        # Measurement update only for visited (s, a2)
        pred_var = v[s, a2]
        K = pred_var / (pred_var + r_meas + 1e-10)  # Kalman gain
        m[s, a2] = m[s, a2] + K * (r - m[s, a2])
        v[s, a2] = (1.0 - K) * pred_var

        # Clamp for numerical stability
        m[s, a2] = np.clip(m[s, a2], 0.0, 1.0)
        v[s, a2] = np.clip(v[s, a2], 1e-8, 1.0)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['w_mb', 'beta', 'kappa_bonus', 'alpha_vol', 'sigma0']"
iter1_run0_participant8.json,cognitive_model3,362.41596483345666,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive planner with learned transitions and anxiety-weighted loss aversion.
    
    Summary
    -------
    - Learns transition probabilities T(a1 -> planet) over time via simple delta-rule.
    - Learns second-stage Q-values via delta-rule.
    - Utility is risk-sensitive: u(r) = r for gains and = -lambda*r for losses, where
      lambda (loss aversion) increases with anxiety.
    - Stage 1 is purely model-based using the learned transitions and the current stage-2 Qs.
      A small perseveration term at stage 1 captures inertia and is reduced by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [eta_r, beta, alpha_T, lambda0, gamma_prev]
        - eta_r: reward learning rate at stage 2 [0,1]
        - beta: inverse temperature for both stages [0,10]
        - alpha_T: transition learning rate [0,1]
        - lambda0: baseline loss aversion (multiplier on negative utility) [0,1]
        - gamma_prev: perseveration strength to repeat previous first-stage action [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    eta_r, beta, alpha_T, lambda0, gamma_prev = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transitions close to uniform to allow learning
    # T[a, s] rows sum to 1
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values (planet x alien)
    q2 = np.zeros((2, 2)) + 0.5

    prev_a1 = None

    # Anxiety-modulated parameters:
    # - Loss aversion increases with anxiety.
    # - Perseveration decreases with anxiety (more anxious -> less inertia).
    lambda_loss = lambda0 * (0.5 + 1.0 * st)  # can exceed 1 if lambda0 near 1 and st high (still within reason)
    persev = gamma_prev * (1.0 - 0.6 * st)

    for t in range(n_trials):
        # Stage-1 model-based evaluation using learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add perseveration bias toward previous a1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += persev

        # First-stage choice
        logits1 = q1_mb + bias
        logits1 = logits1 - np.max(logits1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice within reached planet
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and utility transformation (risk-sensitive)
        r = reward[t]
        # Utility: if r >= current expectation -> gain; else treated as loss relative to 0
        # Since rewards are 0/1, we define utility as:
        # u(1) = 1, u(0) = -lambda_loss
        u = 1.0 if r >= 0.5 else (-lambda_loss)

        # Stage-2 update toward utility
        pe2 = u - q2[s, a2]
        q2[s, a2] += eta_r * pe2
        q2[s, a2] = np.clip(q2[s, a2], -lambda_loss, 1.0)

        # Transition learning: update the chosen row of T toward the observed state
        # One-hot target for reached state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - alpha_T) * T[a1] + alpha_T * target
        # Ensure normalization
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['eta_r', 'beta', 'alpha_T', 'lambda0', 'gamma_prev']"
iter1_run0_participant9.json,cognitive_model2,378.8161490817656,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free controller with anxiety-modulated forgetting and WSLS bias that depends on reward and transition type.

    Core idea:
    - Pure MF Q-learning at both stages with trial-by-trial forgetting (decay).
    - A first-stage win-stay/lose-shift (WSLS) bias influences action selection:
        - After rewarded-common or unrewarded-rare transitions, bias to repeat is stronger when anxiety is low.
        - After unrewarded-common or rewarded-rare transitions, bias to switch is stronger when anxiety is high.
    - Anxiety also increases forgetting (higher decay when stai is high).

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices: 0=A, 1=U.
    state : array-like of int {0,1}
        Reached second-stage state: 0=X, 1=Y.
    action_2 : array-like of int {0,1}
        Second-stage choices.
    reward : array-like of float
        Rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; higher -> more forgetting and stronger lose-shift after certain outcomes.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, decay, wsls_gain, xi]
        - alpha: [0,1] learning rate for both stages
        - beta: [0,10] inverse temperature (both stages)
        - decay: [0,1] baseline forgetting rate applied each trial to all Q-values
        - wsls_gain: [0,1] magnitude of WSLS bias on first-stage logits
        - xi: [0,1] mixes reward vs transition contributions to the WSLS signal
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, decay, wsls_gain, xi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    q1 = np.zeros(2)        # first-stage MF values
    q2 = np.zeros((2, 2))   # second-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Keep track of previous trial info for WSLS bias
    last_a1 = None
    last_reward = 0.0
    last_is_common = 0  # 1 common, 0 rare

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply forgetting (decay) stronger for higher anxiety
        decay_eff = decay * (0.5 + 0.5 * stai_val)
        q1 *= (1.0 - decay_eff)
        q2 *= (1.0 - decay_eff)

        # WSLS bias for first-stage logits
        bias1 = np.zeros(2)
        if last_a1 is not None:
            # Signal that combines reward and transition type
            # reward_term: +1 if rewarded, -1 if not
            reward_term = (2.0 * np.clip(last_reward, 0.0, 1.0)) - 1.0
            # transition_term: +1 if common, -1 if rare
            trans_term = 1.0 if last_is_common == 1 else -1.0

            # Blend terms; xi controls weighting; anxiety flips emphasis toward losing/rare
            # Effective signed bias to repeat last action:
            # - Low anxiety emphasizes reward-term on common transitions (classic WSLS).
            # - High anxiety emphasizes switching after losses and rare transitions.
            base_signal = xi * reward_term + (1.0 - xi) * trans_term
            anxiety_gain = (0.5 + 0.5 * stai_val)  # scales switching on adverse signals
            signed_signal = base_signal * (1.0 - stai_val) - reward_term * (anxiety_gain - (1.0 - stai_val))

            # Convert to bias on last chosen action's logit
            bias_strength = wsls_gain * signed_signal
            # Positive -> repeat bias; negative -> switch bias implemented by negative bias on last action
            bias1[last_a1] += bias_strength

        # Stage 1 policy
        logits1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (pure MF)
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Compute whether current transition is common given the task structure:
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

        # MF learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Update WSLS bookkeeping
        last_a1 = a1
        last_reward = r
        last_is_common = is_common

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))

","['alpha', 'beta', 'decay', 'wsls_gain', 'xi']"
iter2_run0_participant0.json,cognitive_model1,495.2112903281163,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety- and uncertainty-gated arbitration with shared learning and adaptive temperature.

    Idea:
    - Stage 2 learned with a single learning rate (alpha).
    - Stage 1 uses a mixture of model-free (MF) and model-based (MB) values.
    - The arbitration weight (w_eff) is reduced by both anxiety and current uncertainty
      about stage-2 values.
    - Inverse temperature beta is reduced by anxiety (capturing more exploration).

    Parameters (bounds):
    - alpha: [0,1] learning rate for stage-2 Q and MF backprop to stage 1
    - omega0: [0,1] baseline MB arbitration weight at stage 1
    - beta0: [0,10] baseline inverse temperature
    - anx_temp: [0,1] how strongly anxiety reduces beta (beta_eff = beta0*(1 - anx_temp*stai))
    - zeta_unc: [0,1] how strongly current uncertainty reduces MB weight

    Anxiety usage:
    - Arbitration: w_eff = clip(omega0 * (1 - 0.5*stai) * (1 - zeta_unc * U), 0, 1)
      where U is a running uncertainty index derived from stage-2 value dispersion.
    - Temperature: beta_eff = beta0 * (1 - anx_temp * stai)

    Inputs:
    - action_1: int array of length T in {0,1} (spaceship choices A/U)
    - state: int array of length T in {0,1} (planet X/Y reached)
    - action_2: int array of length T in {0,1} (alien choices W/S on X, P/H on Y)
    - reward: float array of length T (received coins)
    - stai: array-like with one element in [0,1] (anxiety score)
    - model_parameters: [alpha, omega0, beta0, anx_temp, zeta_unc]

    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha, omega0, beta0, anx_temp, zeta_unc = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Fixed transition structure: A->X and U->Y are common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Initialize values
    q1_mf = np.zeros(2)           # MF values for stage-1 actions (A, U)
    q2 = np.zeros((2, 2))         # Q-values at stage 2 for each state (X,Y) and actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Uncertainty proxy U_t: average entropy-like measure across states
    # We maintain a running value updated from q2.
    U = 1.0

    # Effective temperature (same at both stages)
    beta_eff = max(1e-6, beta0 * (1.0 - anx_temp * stai_score))

    for t in range(n_trials):
        # Model-based Q at stage 1: expected max value on each planet given transitions
        max_q2 = np.max(q2, axis=1)  # size 2: best action per planet
        q1_mb = transition_matrix @ max_q2

        # Compute current uncertainty from q2 dispersion (softmax entropy across states)
        # For each state, compute softmax over q2 with a modest temperature
        temp_unc = 2.0  # fixed internal temperature for uncertainty readout
        entropies = []
        for s_idx in range(2):
            logits = temp_unc * (q2[s_idx] - np.max(q2[s_idx]))
            probs = np.exp(logits)
            probs /= np.sum(probs) if np.sum(probs) > 0 else 1.0
            # Binary entropy
            p = probs[0]
            h = - (p * np.log(p + 1e-12) + (1 - p) * np.log(1 - p + 1e-12))
            # Normalize max entropy for binary is ln(2)
            entropies.append(h / np.log(2.0))
        U = 0.5 * (entropies[0] + entropies[1])

        # Arbitration: reduce MB with anxiety and uncertainty
        w_eff = omega0 * (1.0 - 0.5 * stai_score) * (1.0 - zeta_unc * U)
        w_eff = float(np.clip(w_eff, 0.0, 1.0))

        # Mixed policy at stage 1
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1_sum = np.sum(probs1)
        if probs1_sum <= 0:
            probs1 = np.array([0.5, 0.5])
        else:
            probs1 /= probs1_sum

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy at observed state
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2_sum = np.sum(probs2)
        if probs2_sum <= 0:
            probs2 = np.array([0.5, 0.5])
        else:
            probs2 /= probs2_sum

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # Stage-2 TD
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF bootstrapping from the actually visited state's chosen action
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'omega0', 'beta0', 'anx_temp', 'zeta_unc']"
iter2_run0_participant0.json,cognitive_model2,491.6021251700745,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Common-rare sensitive arbitration with anxiety-amplified perseveration at stage 1.

    Idea:
    - Stage 2 learned with single learning rate (alpha).
    - Stage 1 arbitration weight depends on anxiety and whether the previous trial
      was a common or rare transition (capturing reliance on MB after common vs. rare).
    - Stage 1 has a perseveration term that increases with anxiety (habit-like repeat).
    - Model-based values come from known transition matrix (no transition learning).

    Parameters (bounds):
    - alpha: [0,1] learning rate for stage-2 Q and MF backprop
    - w0: [0,1] baseline MB arbitration weight
    - phi_common: [0,1] added MB weight after a common transition, reduced after rare
    - pers1: [0,1] baseline stage-1 perseveration strength
    - beta: [0,10] inverse temperature (both stages)

    Anxiety usage:
    - Arbitration: w_eff_t = clip(w0 * (1 - stai) + phi_common * I_common_prev - phi_common * stai * I_rare_prev, 0, 1)
      where I_common_prev/I_rare_prev indicate whether previous transition was common or rare.
      This captures reduced MB reliance after rare, amplified by anxiety.
    - Perseveration: pers_eff = pers1 * (1 + stai)  # more stickiness with anxiety

    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, w0, phi_common, pers1, beta]

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha, w0, phi_common, pers1, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    last_a1 = -1  # for perseveration
    was_common_prev = 0  # indicator for previous trial

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    pers_eff = pers1 * (1.0 + stai_score)
    beta_eff = max(1e-6, beta)

    for t in range(n_trials):
        # MB computation from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Determine current arbitration weight based on previous trial's transition type
        I_common_prev = 1.0 if was_common_prev == 1 else 0.0
        I_rare_prev = 1.0 if was_common_prev == -1 else 0.0
        w_eff = w0 * (1.0 - stai_score) + phi_common * I_common_prev - phi_common * stai_score * I_rare_prev
        w_eff = float(np.clip(w_eff, 0.0, 1.0))

        # Stage-1 choice with perseveration bonus toward last chosen action
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        if last_a1 in [0, 1]:
            q1[last_a1] += pers_eff

        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        sum1 = np.sum(probs1)
        probs1 = probs1 / sum1 if sum1 > 0 else np.array([0.5, 0.5])

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice at reached state
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        sum2 = np.sum(probs2)
        probs2 = probs2 / sum2 if sum2 > 0 else np.array([0.5, 0.5])

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # MF backprop to stage 1
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update markers for next trial
        # Common if A->X or U->Y, rare otherwise
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        was_common_prev = 1 if is_common else -1
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'w0', 'phi_common', 'pers1', 'beta']"
iter2_run0_participant0.json,cognitive_model3,474.32487014162575,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pearce-Hall associability with anxiety-scaled learning, MF eligibility, and stage-2 stickiness.

    Idea:
    - Stage 2 uses a volatility-adaptive learning rate via Pearce-Hall associability A_t.
    - Anxiety increases the gain on associability, speeding learning when surprised.
    - Stage 1 uses purely MF values updated with an eligibility-like trace.
    - Stage 2 includes action stickiness (per-state) to capture habitual repetition.

    Parameters (bounds):
    - alpha0: [0,1] base learning rate
    - kappa_ph: [0,1] associability gain factor (scaled by anxiety)
    - trace_tau: [0,1] MF eligibility factor for stage 1 update magnitude
    - stick2: [0,1] stage-2 stickiness strength for repeating last action in a state
    - beta: [0,10] inverse temperature (both stages)

    Anxiety usage:
    - Effective learning rate: alpha_t = clip(alpha0 + kappa_ph * stai * |PE2_{t-1}|, 0, 1)
      operationalized via an exponentially smoothed associability A_t driven by |PE|.
    - Stickiness: stick2_eff = stick2 * (1 + 0.5*stai)  # more repetition with anxiety

    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha0, kappa_ph, trace_tau, stick2, beta]

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha0, kappa_ph, trace_tau, stick2, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Transition matrix used only to compute a model-based preview for stage 1 if desired,
    # but in this model we use pure MF at stage 1 to highlight associability effects.
    # We still compute MB preview with zeros; effectively stage 1 choice is governed by MF only.
    # (Leaving transition structure available if extensions needed.)
    # transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    last_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Pearce-Hall associability A initialized moderately
    A = 0.5
    beta_eff = max(1e-6, beta)
    stick2_eff = stick2 * (1.0 + 0.5 * stai_score)

    prev_abs_pe2 = 0.0

    for t in range(n_trials):
        # Stage-1 policy (pure MF with optional soft normalization)
        logits1 = beta_eff * (q1_mf - np.max(q1_mf))
        probs1 = np.exp(logits1)
        sum1 = np.sum(probs1)
        probs1 = probs1 / sum1 if sum1 > 0 else np.array([0.5, 0.5])

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        s = state[t]
        q2_s = q2[s].copy()
        if last_a2[s] != -1:
            q2_s[last_a2[s]] += stick2_eff

        logits2 = beta_eff * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        sum2 = np.sum(probs2)
        probs2 = probs2 / sum2 if sum2 > 0 else np.array([0.5, 0.5])

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Update associability from previous absolute PE with anxiety gain
        # A_t = (1 - trace_tau)*A_{t-1} + trace_tau*|PE2_{t-1}|; alpha_t = clip(alpha0 + kappa_ph*stai*A_t, 0, 1)
        A = (1.0 - trace_tau) * A + trace_tau * prev_abs_pe2
        alpha_t = alpha0 + kappa_ph * stai_score * A
        alpha_t = float(np.clip(alpha_t, 0.0, 1.0))

        # Stage-2 TD update with alpha_t
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * pe2

        # Stage-1 MF update with eligibility-like factor toward updated stage-2 value
        # q1_mf[a1] moves toward q2[s, a2] with step size alpha_t * trace_tau
        q1_mf[a1] += (alpha_t * trace_tau) * (q2[s, a2] - q1_mf[a1])

        # Bookkeeping
        last_a2[s] = a2
        prev_abs_pe2 = abs(pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha0', 'kappa_ph', 'trace_tau', 'stick2', 'beta']"
iter2_run0_participant1.json,cognitive_model1,539.1398788019834,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF RL with learned transitions, eligibility trace, and anxiety-gated arbitration.
    
    This model learns second-stage rewards and first-stage transition probabilities. The first-stage policy
    arbitrates between model-based (MB) and model-free (MF) values. The arbitration weight is reduced by
    uncertainty about transitions and further down-weighted by anxiety, making anxious participants rely
    more on MF under uncertain transitions. An eligibility trace propagates stage-2 prediction errors to
    stage-1 MF values. A small lapse handles occasional random responding.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha: learning rate for reward values and MF updates (0..1).
    - alphaT: learning rate for transition probabilities (0..1).
    - beta: inverse temperature for both stages (0..10).
    - w0: baseline arbitration weight (MB share at stage 1; 0..1).
    - xi_stai: strength by which anxiety gates the arbitration under uncertainty (0..1).
    - lam: eligibility trace from stage 2 PE to stage 1 MF (0..1).
    - epsilon: lapse rate for both stages (0..1).

    Args:
        action_1: 1D array of first-stage actions, 0/1.
        state: 1D array of second-stage states reached, 0/1.
        action_2: 1D array of second-stage actions in the reached state, 0/1.
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element: participant's anxiety score in [0,1].
        model_parameters: list/tuple of parameters in the order specified above.

    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """"""
    alpha, alphaT, beta, w0, xi_stai, lam, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition probabilities (rows: actions A,U; cols: states X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q2 = np.zeros((2, 2))        # second-stage state-action values
    Q1_mf = np.zeros(2)          # first-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1  # for optional stickiness if needed; not used here beyond MF trace

    eps = 1e-12
    ln2 = np.log(2.0)

    for t in range(n_trials):
        # Compute MB first-stage values from learned transitions and current Q2
        max_q2 = np.max(Q2, axis=1)      # max per second-stage state
        Q1_mb = T @ max_q2

        # Uncertainty over transitions: average row entropy normalized to [0,1]
        ent_rows = []
        for a in range(2):
            p = T[a].clip(eps, 1.0)
            p = p / np.sum(p)
            ent_rows.append(-(p * np.log(p)).sum())
        uncert = float(np.mean(ent_rows) / ln2)  # 0..1

        # Anxiety-gated arbitration: higher STAI and higher uncertainty push toward MF
        w_eff = w0 * (1.0 - xi_stai * stai * uncert)
        w_eff = min(max(w_eff, 0.0), 1.0)

        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf

        # First-stage choice probability with lapse
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability with lapse
        s = state[t]
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]

        # Stage-2 TD update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Eligibility trace to stage-1 MF: use the max next value difference as TD target
        # Here we leverage the experienced second-stage value update pe2 to update Q1_mf
        Q1_mf[a1] += alpha * lam * pe2

        # Learn transitions from experienced state (Dirichlet-like exponential averaging)
        oh = np.array([1.0 if j == s else 0.0 for j in range(2)])
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'alphaT', 'beta', 'w0', 'xi_stai', 'lam', 'epsilon']"
iter2_run0_participant1.json,cognitive_model2,552.0448242304026,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive MF with win-stay/lose-shift heuristic modulated by anxiety.
    
    Stage 2 uses risk-sensitive utility and asymmetric learning rates for positive vs negative outcomes.
    A win-stay/lose-shift (WSLS) heuristic contributes to second-stage choices; anxiety strengthens
    reliance on this heuristic. Stage 1 combines a bootstrapped MF value from learned Q2 via the fixed
    transition structure with standard softmax choice. Perseveration at stage 2 is included.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha_pos: learning rate for positive outcomes at stage 2 (0..1).
    - alpha_neg: learning rate for negative outcomes at stage 2 (0..1).
    - beta: inverse temperature for both stages (0..10).
    - phi: risk/utility curvature for rewards, u(r)=r^phi (0..1, with smaller -> more concave).
    - zeta0: baseline weight of WSLS heuristic at stage 2 (0..1, translates to additive bias).
    - xi_stai: anxiety modulation of heuristic weight; zeta_eff = zeta0*(1 + xi_stai*stai) (0..1).
    - kappa2: second-stage perseveration bias for repeating last action in the same state (0..1).

    Args:
        action_1: 1D array of first-stage actions, 0/1.
        state: 1D array of second-stage states reached, 0/1.
        action_2: 1D array of second-stage actions in the reached state, 0/1.
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element: participant's anxiety score in [0,1].
        model_parameters: list/tuple of parameters in the order specified above.

    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """"""
    alpha_pos, alpha_neg, beta, phi, zeta0, xi_stai, kappa2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed known transition structure (common 0.7)
    T_fix = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))  # second-stage values
    Q1 = np.zeros(2)       # stage-1 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # For WSLS and perseveration
    prev_a2 = np.array([-1, -1])  # last action taken in each state
    prev_r2 = np.array([-1.0, -1.0])  # last reward observed in each state (scalar per state: for WSLS rule)

    eps = 1e-12

    for t in range(n_trials):
        # Compute bootstrapped stage-1 MF values from Q2 via fixed transitions
        max_q2 = np.max(Q2, axis=1)
        Q1 = T_fix @ max_q2

        # First-stage choice
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice: value + WSLS heuristic + perseveration
        s = state[t]

        # Base value logits from Q2
        logits2_val = beta * Q2[s]

        # WSLS heuristic: prefer repeating if previous reward in this state was 1, else switch
        zeta_eff = zeta0 * (1.0 + xi_stai * stai)
        heuristic_bias = np.zeros(2)
        if prev_a2[s] != -1 and prev_r2[s] != -1:
            if prev_r2[s] >= 0.5:
                # win-stay: add bias to previous action
                heuristic_bias[prev_a2[s]] += zeta_eff
            else:
                # lose-shift: add bias to the alternate action
                heuristic_bias[1 - prev_a2[s]] += zeta_eff

        # Perseveration bias to repeat last action in the same state
        persev_bias = np.zeros(2)
        if prev_a2[s] != -1:
            persev_bias[prev_a2[s]] += kappa2

        logits2 = logits2_val + heuristic_bias + persev_bias
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]
        # Risk-sensitive utility transform
        u = (r ** max(phi, eps)) if r >= 0.0 else -(((-r) ** max(phi, eps)))
        # Asymmetric learning rate
        alpha_eff = alpha_pos if u >= Q2[s, a2] else alpha_neg
        pe2 = u - Q2[s, a2]
        Q2[s, a2] += alpha_eff * pe2

        # Update WSLS memory
        prev_a2[s] = a2
        prev_r2[s] = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'phi', 'zeta0', 'xi_stai', 'kappa2']"
iter2_run0_participant1.json,cognitive_model3,541.5052184559797,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Subjective-transition MB with anxiety-driven dynamic lapse and value forgetting.
    
    The agent uses a subjective common-transition probability to form model-based first-stage values.
    Second-stage values are learned with exponential forgetting toward a neutral prior. A volatility
    signal (unsigned PE) drives a dynamic lapse rate; anxiety amplifies this volatility-to-lapse mapping,
    causing more random choices during volatile periods for anxious participants. First-stage perseveration
    is included.

    Parameters (all in [0,1] except beta in [0,10]):
    - alphaR: learning rate for second-stage rewards (0..1).
    - beta: inverse temperature for both stages (0..10).
    - tau: forgetting rate toward 0.5 for Q2 on each trial (0..1).
    - p_common: subjective probability that A->X and U->Y are common (0..1).
    - delta_anx: anxiety gain mapping volatility to lapse (0..1).
    - epsilon0: baseline lapse rate (0..1).
    - kappa1: first-stage perseveration bias for repeating same spaceship (0..1).

    Args:
        action_1: 1D array of first-stage actions, 0/1.
        state: 1D array of second-stage states reached, 0/1.
        action_2: 1D array of second-stage actions, 0/1.
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element: participant's anxiety score in [0,1].
        model_parameters: list/tuple of parameters in the order specified above.

    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """"""
    alphaR, beta, tau, p_common, delta_anx, epsilon0, kappa1 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Subjective transition matrix based on p_common
    T_subj = np.array([[p_common, 1.0 - p_common],
                       [1.0 - p_common, p_common]], dtype=float)

    Q2 = np.zeros((2, 2))  # second-stage values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    # Volatility estimate from unsigned PE
    v = 0.0

    eps = 1e-12

    for t in range(n_trials):
        # Dynamic lapse: baseline + anxiety-amplified volatility
        epsilon_t = epsilon0 + delta_anx * stai * v
        epsilon_t = min(max(epsilon_t, 0.0), 1.0)

        # MB first-stage values using subjective transitions
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T_subj @ max_q2

        # First-stage perseveration
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        logits1 = beta * Q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon_t) * probs1 + epsilon_t * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice with the same dynamic lapse
        s = state[t]
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon_t) * probs2 + epsilon_t * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        # Exponential forgetting toward 0.5 baseline
        Q2 = (1.0 - tau) * Q2 + tau * 0.5

        # TD learning at the encountered state-action
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Update volatility estimate v from unsigned PE (bounded in [0,1])
        v = (1.0 - tau) * v + tau * abs(pe2)
        v = min(max(v, 0.0), 1.0)

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alphaR', 'beta', 'tau', 'p_common', 'delta_anx', 'epsilon0', 'kappa1']"
iter2_run0_participant12.json,cognitive_model1,414.6487263365806,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free arbitration with anxiety-modulated model-based weight and stage-1 stickiness.

    This model combines a model-based (MB) planner with a model-free (MF) learner at stage 1.
    Stage 2 is learned model-free. The arbitration weight between MB and MF at stage 1 is shifted
    by the participant's anxiety (STAI). Additionally, there is a stage-1 choice perseveration
    (stickiness) term whose strength is also scaled by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien on that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1 in [0,1]. Used to modulate arbitration and stickiness.
    model_parameters : list or array
        [alpha, beta, w_base, psi_stai, kappa]
        Bounds:
          alpha in [0,1]     : MF learning rate for both stages
          beta in [0,10]     : inverse temperature for softmax at both stages
          w_base in [0,1]    : baseline model-based weight at stage 1
          psi_stai in [0,1]  : how strongly STAI shifts the MB weight (0=no effect)
          kappa in [0,1]     : baseline stage-1 perseveration strength

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, w_base, psi_stai, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common (0.7/0.3)
    transition_matrix = np.array([[0.7, 0.3],  # from A to X/Y
                                  [0.3, 0.7]]) # from U to X/Y

    # Values
    q1_mf = np.zeros(2)          # model-free values at stage 1
    q2_mf = np.zeros((2, 2))     # model-free values at stage 2: states x actions

    # Likelihood tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration and stickiness
    stai_signed = 2.0 * stai - 1.0
    w_eff = np.clip(w_base + psi_stai * stai_signed, 0.0, 1.0)  # MB weight
    # Stickiness strength increases or decreases with anxiety
    stick_strength = np.clip(kappa * (1.0 + 0.5 * stai_signed), 0.0, 1.0)

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based stage-1 Q via planning through learned stage-2 values
        max_q2 = np.max(q2_mf, axis=1)  # for each state, best alien
        q1_mb = transition_matrix @ max_q2

        # Combine MB and MF; add perseveration bias for previous stage-1 choice
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            q1 = q1 + stick_strength * stick

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        p1 = np.exp(beta * q1c)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (model-free)
        q2s = q2_mf[s].copy()
        q2c = q2s - np.max(q2s)
        p2 = np.exp(beta * q2c)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning (MF)
        r = reward[t]
        # Stage 2 update
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Stage 1 MF update toward the experienced stage-2 value (simple eligibility)
        td1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_base', 'psi_stai', 'kappa']"
iter2_run0_participant12.json,cognitive_model2,472.7741246637519,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated exploration (beta) and lapse (epsilon) with MF learning and eligibility.

    This purely model-free controller learns second-stage values and backs them up to stage 1
    through an eligibility trace. Anxiety jointly lowers inverse temperature (more exploration)
    and increases a soft lapse (epsilon-greedy mixture).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1 in [0,1]. Used to modulate beta and epsilon.
    model_parameters : list or array
        [alpha, beta_base, epsilon_base, chi_stai, trace]
        Bounds:
          alpha in [0,1]        : learning rate
          beta_base in [0,10]   : baseline inverse temperature
          epsilon_base in [0,1] : baseline lapse/epsilon (mixed with uniform policy)
          chi_stai in [0,1]     : sensitivity of beta/epsilon to STAI (0=no effect)
          trace in [0,1]        : eligibility strength for backing up from stage 2 to stage 1

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta_base, epsilon_base, chi_stai, trace = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Signed anxiety factor in [-1,1]
    z = 2.0 * stai - 1.0
    # Anxiety reduces beta and increases epsilon
    beta = np.clip(beta_base * (1.0 - 0.5 * chi_stai * z), 0.0, 10.0)
    epsilon = np.clip(epsilon_base * (1.0 + chi_stai * z), 0.0, 1.0)

    # Values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage 1 softmax
        q1c = q1 - np.max(q1)
        p1_soft = np.exp(beta * q1c)
        p1_soft /= np.sum(p1_soft)
        # Epsilon-greedy mixture
        p1 = (1.0 - epsilon) * p1_soft + epsilon * 0.5
        p_choice_1[t] = p1[a1]

        # Stage 2 softmax
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        p2_soft = np.exp(beta * q2c)
        p2_soft /= np.sum(p2_soft)
        p2 = (1.0 - epsilon) * p2_soft + epsilon * 0.5
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility backup to stage 1
        q1[a1] += trace * alpha * pe2

        # Optional direct TD correction of stage-1 toward current stage-2 value
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += (1.0 - trace) * alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta_base', 'epsilon_base', 'chi_stai', 'trace']"
iter2_run0_participant14.json,cognitive_model2,454.86733513178297,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-sensitive learning and anxiety-amplified transition vigilance.
    
    This model uses a fixed transition structure (common=0.7, rare=0.3) but
    modulates the stage-2 learning rate on each trial based on transition
    surprise: alpha_eff = alpha * (1 + g_surprise * stai * surprise_t),
    where surprise_t = 1 - P(planet | chosen spaceship).
    
    Stage 1 combines MB and MF via a constant mixing weight omega (no anxiety
    in the mixing to avoid overlap with model1), and includes a perseveration
    bias toward repeating the previous spaceship.
    
    Parameters (model_parameters):
    - alpha: base learning rate for MF Q updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - g_surprise: gain on surprise- and anxiety-driven learning modulation, in [0,1]
    - omega: MB weight in stage-1 hybrid value, in [0,1]
    - stick1: perseveration strength at stage 1 (added to previous choice logit), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, g_surprise, omega, stick1 = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # MB Q1 from expected best alien values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid Q1
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Perseveration bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick1

        # Stage-1 policy
        logits1 = beta * q1 + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Surprise based on fixed transition structure
        p_common = T[a1, s]
        surprise = 1.0 - p_common  # 0.3 for common, 0.7 for rare
        # Anxiety-amplified surprise increases the effective learning rate
        alpha_eff = alpha * (1.0 + g_surprise * st * surprise)
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * delta2

        # Stage-1 MF update by eligibility from stage 2 observed value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'g_surprise', 'omega', 'stick1']"
iter2_run0_participant14.json,cognitive_model3,518.4340094459534,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-adaptive MF learning with MB guidance and anxiety-scaling.
    
    This model tracks reward prediction error volatility at stage 2 and adapts
    the learning rate accordingly. A running estimate of squared PE is kept:
      v_t = (1 - rho) * v_{t-1} + rho * delta2_t^2
    The effective learning rate is:
      alpha_eff = clip(alpha0 + k_vol * stai * v_t, 0, 1)
    capturing the idea that higher anxiety increases sensitivity to volatility.
    
    Stage 1 uses a hybrid of MB and MF values with a fixed mixing weight omega.
    Stage 2 uses the volatility-adaptive alpha_eff.
    
    Parameters (model_parameters):
    - alpha0: base learning rate (lower bound), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - k_vol: gain from volatility (and anxiety) to learning rate, in [0,1]
    - rho: volatility tracker update rate (EWMA), in [0,1]
    - omega: MB weight in stage-1 hybrid value, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha0, beta, k_vol, rho, omega = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    v = 0.0  # initial volatility estimate of delta2^2

    for t in range(n_trials):
        # MB Q1 from expected best alien values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid Q1
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Compute PE then update volatility estimate
        delta2 = r - q2[s, a2]
        v = (1.0 - rho) * v + rho * (delta2 ** 2)

        # Anxiety-scaled volatility-adaptive learning rate
        alpha_eff = alpha0 + k_vol * st * v
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Update stage-2 values
        q2[s, a2] += alpha_eff * delta2

        # Stage-1 MF update toward realized stage-2 chosen value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha0 * delta1  # keep base rate for MF eligibility

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha0', 'beta', 'k_vol', 'rho', 'omega']"
iter2_run0_participant15.json,cognitive_model1,574.1718935646347,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-weighted arbitration and learned transitions.
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for values (both stages) and MF Q1.
    - beta: [0,10] Inverse temperature for both stages.
    - w_base: [0,1] Arbitration weight toward model-based control at low anxiety.
    - w_anx: [0,1] Arbitration weight toward model-based control at high anxiety.
    - eta_t: [0,1] Transition learning rate base, amplified by anxiety.
    
    Anxiety use:
    - Arbitration weight: w = (1 - stai) * w_base + stai * w_anx (interpolates between low- and high-anxiety MB weights).
    - Transition learning: alpha_t = eta_t * (0.5 + 0.5 * stai) (higher anxiety speeds up learning of transitions).
    
    Model summary:
    - Learns state-action values at Stage-2 (Q2).
    - Learns MF Stage-1 values (Q1_mf) by bootstrapping from realized Stage-2 values.
    - Learns the transition matrix T(a -> s) online.
    - Stage-1 action values are a weighted sum of MB plan (T @ max(Q2)) and MF Q1.
    - Returns negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    alpha, beta, w_base, w_anx, eta_t = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Start with agnostic transitions (rows sum to 1)
    T = np.full((2, 2), 0.5)

    # Values
    Q2 = 0.5 * np.ones((2, 2))
    Q1_mf = np.zeros(2)

    # Anxiety-weighted arbitration and transition learning rate
    w = (1.0 - stai) * w_base + stai * w_anx
    w = min(1.0, max(0.0, w))
    alpha_t = eta_t * (0.5 + 0.5 * stai)
    alpha_t = min(1.0, max(0.0, alpha_t))

    for t in range(n_trials):
        s = state[t]

        # Model-based Q at Stage-1 using current T and Q2
        mb_q1 = T @ np.max(Q2, axis=1)
        # Hybrid Q1
        q1 = w * mb_q1 + (1.0 - w) * Q1_mf

        # Policy Stage-1
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Policy Stage-2
        z2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transitions based on observed state given chosen a1
        obs = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = (1.0 - alpha_t) * T[a1] + alpha_t * obs
        # Numerical guard to keep rows normalized
        T[a1] = T[a1] / max(1e-12, np.sum(T[a1]))

        # Update Q2 with reward prediction error
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update MF Q1 by bootstrapping from the realized Stage-2 value
        target_q1 = Q2[s, a2]
        pe1 = target_q1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_base', 'w_anx', 'eta_t']"
iter2_run0_participant15.json,cognitive_model2,560.7902993720422,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""MF with eligibility traces, anxiety-amplified outcome sensitivity, and anxious stickiness.
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for both stages.
    - beta: [0,10] Inverse temperature for both stages.
    - lam: [0,1] Eligibility trace parameter for credit assignment from Stage-2 to Stage-1.
    - phi: [0,1] Base stickiness strength on Stage-1; expressed more strongly with anxiety.
    - c: [0,1] Outcome sensitivity scaling factor; increases effective reward with anxiety.
    
    Anxiety use:
    - Effective reward scaling: r_eff = r * (1 + c * stai).
    - Stickiness bias on previous Stage-1 choice: stick = phi * stai.
    
    Model summary:
    - Pure model-free control: Q2 updated from reward; Q1 updated via eligibility trace from Q2's prediction error.
    - Stage-1 policy includes a choice stickiness bias that grows with anxiety.
    - Returns negative log-likelihood of observed choices.
    """"""
    alpha, beta, lam, phi, c = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = 0.5 * np.ones((2, 2))
    Q1 = np.zeros(2)

    prev_a1 = None
    stick = phi * stai

    for t in range(n_trials):
        s = state[t]

        # Stage-1 policy with stickiness bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick

        z1 = beta * (Q1 + bias - np.max(Q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure MF)
        z2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Anxiety-amplified outcome sensitivity
        r_eff = reward[t] * (1.0 + c * stai)

        # MF updates with eligibility trace
        pe2 = r_eff - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Credit assignment to Stage-1 choice
        Q1[a1] += alpha * lam * pe2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lam', 'phi', 'c']"
iter2_run0_participant15.json,cognitive_model3,561.0253508498078,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Directed exploration with uncertainty bonus (reduced by anxiety) and MB/MF mix.
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for Q2 and MF Q1.
    - beta: [0,10] Inverse temperature for both stages.
    - b: [0,1] Directed exploration bonus strength at Stage-2 (dampened by anxiety).
    - w_mix: [0,1] Mix between MB (1) and MF (0) at Stage-1.
    - psi: [0,1] Perseveration strength on Stage-1, reduced by anxiety.
    
    Anxiety use:
    - Directed exploration bonus is scaled by (1 - stai): b_eff = b * (1 - stai).
    - Perseveration is reduced with anxiety: stick = psi * (1 - stai).
    
    Model summary:
    - Tracks uncertainty for each alien via visit counts; bonus ~ 1/sqrt(n+1).
    - Stage-2 policy adds an uncertainty bonus to Q2 before softmax (does not bias learning target).
    - Stage-1 values are a mixture of MB plan (fixed transitions) and MF Q1, plus perseveration.
    - Returns negative log-likelihood of observed choices.
    """"""
    alpha, beta, b, w_mix, psi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Fixed known transitions (common-rare structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q2 = 0.5 * np.ones((2, 2))
    Q1_mf = np.zeros(2)

    # Uncertainty via visit counts
    N2 = np.zeros((2, 2))

    b_eff = b * (1.0 - stai)
    stick_strength = psi * (1.0 - stai)

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]

        # Model-based plan at Stage-1
        mb_q1 = T @ np.max(Q2, axis=1)
        # MF component from cached Q1
        q1_base = w_mix * mb_q1 + (1.0 - w_mix) * Q1_mf

        # Add perseveration bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_strength

        z1 = beta * (q1_base + bias - np.max(q1_base + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 directed exploration: bonus decreases with visits
        u = 1.0 / np.sqrt(N2[s] + 1.0)
        q2_bonus = Q2[s] + b_eff * u

        z2 = beta * (q2_bonus - np.max(q2_bonus))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update counts and Q2 learning (no bonus in the target)
        N2[s, a2] += 1.0
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update MF Q1 toward realized Stage-2 value
        target_q1 = Q2[s, a2]
        pe1 = target_q1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'b', 'w_mix', 'psi']"
iter2_run0_participant16.json,cognitive_model1,516.6457473604935,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated arbitration between model-based and model-free control
    with eligibility trace credit assignment.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state after first-stage choice (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Received rewards at the end of the trial.
    stai : array-like with single float in [0,1]
        Anxiety score. Higher values shift arbitration toward model-free control
        and increase eligibility trace strength.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for Q-values.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2.
        - w_mb_base in [0,1]: baseline model-based weight at stage 1.
        - lambda_base in [0,1]: baseline eligibility trace strength.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta1, beta2, w_mb_base, lambda_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed, known transition structure (common 0.7/rare 0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)         # model-free cached values for first-stage actions
    q2 = np.zeros((2, 2))       # second-stage Q-values per state and action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety attenuates model-based weight and increases eligibility trace
    w_mb = np.clip(w_mb_base * (1.0 - 0.6 * stai), 0.0, 1.0)
    lam = np.clip(lambda_base + 0.5 * stai * (1.0 - lambda_base), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based action values from planning over second-stage max values
        v2 = np.max(q2, axis=1)             # best achievable value per state
        q1_mb = T @ v2                      # plan by forward model

        # Arbitration between MB and MF
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Eligibility-based credit to stage-1 MF value (anxiety increases lam)
        # Use the immediate TD error from stage 2 as the teaching signal.
        q1_mf[a1] += alpha * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta1', 'beta2', 'w_mb_base', 'lambda_base']"
iter2_run0_participant16.json,cognitive_model2,493.10054665914265,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty- and surprise-driven exploration with anxiety amplification.
    Rewards are learned model-free; exploration bonuses stem from reward
    uncertainty and transition surprise. Anxiety increases exploration bonuses
    and reduces determinism, especially after surprising transitions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state after first-stage choice (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Received rewards at the end of the trial.
    stai : array-like with single float in [0,1]
        Anxiety score. Higher values increase uncertainty and surprise bonuses
        and reduce softmax inverse temperature.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for Q-values (means).
        - beta_base in [0,10]: baseline inverse temperature for both stages.
        - eta_base in [0,1]: baseline uncertainty bonus weight.
        - phi_base in [0,1]: baseline surprise modulation of perseveration at stage 1.
        - kappa in [0,1]: first-stage perseveration strength.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta_base, eta_base, phi_base, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition kernel (known to participant)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Reward means (Q2) and visit counts for uncertainty estimation
    q2 = np.zeros((2, 2))
    n_visits = np.zeros((2, 2))  # for beta-binomial variance proxy

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety scaling
    # Lower determinism with anxiety
    beta = max(0.0, beta_base * (1.0 - 0.5 * stai))
    # Higher uncertainty-driven exploration with anxiety
    eta = np.clip(eta_base + 0.8 * stai * (1.0 - eta_base), 0.0, 1.0)
    # Surprise sensitivity shaping perseveration attenuation
    phi = np.clip(phi_base + 0.8 * stai * (1.0 - phi_base), 0.0, 1.0)

    prev_a1 = None
    prev_surprise = 0.0  # computed from previous trial transition

    for t in range(n_trials):
        # Reward-uncertainty bonus per state-action via beta-binomial variance proxy
        # var ~ mean*(1-mean)/(n+1); use sqrt for bonus scale
        var_sa = q2 * (1.0 - q2) / (n_visits + 1.0)
        bonus_sa = eta * np.sqrt(np.maximum(var_sa, 1e-12))

        # State values as optimistic estimates: best action including bonus
        v_state = np.max(q2 + bonus_sa, axis=1)

        # First-stage exploration bonus from state uncertainty (propagated via T)
        # Use the state's maximal action variance as its uncertainty summary.
        u_state = np.max(var_sa, axis=1)
        U_a = T @ np.sqrt(np.maximum(u_state, 1e-12))  # uncertainty routed through transitions
        U_a *= eta

        # Stage-1 perseveration, attenuated after surprising transitions (more surprise -> less stickiness)
        stick = np.zeros(2)
        if prev_a1 is not None:
            kappa_eff = kappa * (1.0 - phi * prev_surprise)
            stick[prev_a1] += kappa_eff

        # Stage 1 policy: value + exploration bonus + stickiness
        q1 = T @ v_state
        logits1 = beta * q1 + U_a + stick
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy: value + local uncertainty bonus
        s = state[t]
        logits2 = beta * (q2[s] + bonus_sa[s])
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning from reward
        r = reward[t]
        n_visits[s, a2] += 1.0
        q2[s, a2] += alpha * (r - q2[s, a2])

        # Update surprise for next trial based on current transition
        prev_surprise = 1.0 - T[a1, s]  # large when a rare transition occurred
        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta_base', 'eta_base', 'phi_base', 'kappa']"
iter2_run0_participant16.json,cognitive_model3,507.8639801416241,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pessimistic temporal-difference learning with anxiety-enhanced forgetting.
    Stage-1 plans over pessimistic state values; stage-2 includes global
    value decay toward 0.5 that increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state after first-stage choice (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Received rewards at the end of the trial.
    stai : array-like with single float in [0,1]
        Anxiety score. Higher values increase pessimism and forgetting, and reduce
        second-stage determinism.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2_base in [0,10]: baseline inverse temperature at stage 2.
        - psi_base in [0,1]: baseline pessimism weight (0=max-only, 1=min-only).
        - omega in [0,1]: forgetting strength toward 0.5 (scaled by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta1, beta2_base, psi_base, omega = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition kernel
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations
    psi = np.clip(psi_base + 0.7 * stai * (1.0 - psi_base), 0.0, 1.0)  # more pessimism with anxiety
    beta2 = max(0.0, beta2_base * (1.0 - 0.6 * stai))
    # Forgetting rate toward 0.5 increases with anxiety
    forget_rate = np.clip(omega * stai * 0.5, 0.0, 1.0)

    for t in range(n_trials):
        # Pessimistic state values: convex combination of max and min
        v_max = np.max(q2, axis=1)
        v_min = np.min(q2, axis=1)
        v_pess = (1.0 - psi) * v_max + psi * v_min

        # Stage 1 policy via planning over pessimistic state values
        q1 = T @ v_pess
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with anxiety-reduced determinism
        s = state[t]
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning with pessimistic TD and forgetting toward 0.5 baseline
        r = reward[t]
        q2[s, a2] += alpha * (r - q2[s, a2])

        # Global forgetting toward 0.5, stronger with anxiety
        if forget_rate > 0.0:
            q2 = (1.0 - forget_rate) * q2 + forget_rate * 0.5

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta1', 'beta2_base', 'psi_base', 'omega']"
iter2_run0_participant3.json,cognitive_model1,510.6338831464843,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-weighted predictability preference.
    
    Core ideas:
    - Learn the first-stage transition matrix online (simple delta rule).
    - Compute model-based (MB) first-stage values from learned transitions and current stage-2 values.
    - Mix MB and model-free (MF) values at stage 1 with a fixed arbitration weight.
    - Anxiety increases preference for predictable actions at stage 1 by penalizing actions
      whose transition distributions are less peaked (higher expected surprise).
    
    Parameters (all arrays are 1D of equal length n_trials):
    - action_1: int array of {0,1}, first-stage choices (spaceships A=0, U=1).
    - state:    int array of {0,1}, reached second-stage state (planets X=0, Y=1).
    - action_2: int array of {0,1}, second-stage choices (aliens W/P=0, S/H=1 depending on planet).
    - reward:   float array in [0,1], reward outcome (gold coins).
    - stai:     float array with a single element in [0,1], anxiety score.
    - model_parameters: 5-tuple/list of:
        eta  in [0,1]: learning rate for stage-2 Q updates (MF) and MF stage-1 bootstrap
        beta in [0,10]: inverse temperature for both stages
        pi   in [0,1]: arbitration weight on MB values at stage 1 (1=MB, 0=MF)
        phi  in [0,1]: learning rate for transition matrix updates
        nu   in [0,1]: strength of anxiety-driven predictability preference at stage 1
    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """"""
    eta, beta, pi, phi, nu = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize learned transition matrix; start with weak prior near the task structure
    T = np.array([[0.65, 0.35],
                  [0.35, 0.65]], dtype=float)

    # Value tables
    q2 = np.zeros((2, 2), dtype=float)   # stage-2 Q(s,a)
    q1_mf = np.zeros(2, dtype=float)     # stage-1 MF values

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first-stage values from learned transitions and current q2
        max_q2 = np.max(q2, axis=1)  # best second-stage action per state
        q1_mb = T @ max_q2

        # Predictability (lower surprise) bonus: penalize actions with flatter transition distributions
        # Expected surprise per action = 1 - max(T[a])
        expected_surprise = 1.0 - np.max(T, axis=1)
        predictability_penalty = nu * s_anx * expected_surprise  # higher anxiety -> stronger penalty

        # Arbitration between MB and MF for stage 1
        q1 = pi * q1_mb + (1.0 - pi) * q1_mf
        logits1 = beta * q1 - predictability_penalty  # subtract penalty
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s_idx = int(state[t])
        logits2 = beta * q2[s_idx]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Stage-2 MF update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += eta * delta2

        # Stage-1 MF bootstrap toward realized second-stage action value
        target1 = q2[s_idx, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * delta1

        # Transition learning update for the chosen action toward the observed state
        # Move T[a1, :] toward one-hot of s_idx
        for s in (0, 1):
            targ = 1.0 if s == s_idx else 0.0
            T[a1, s] += phi * (targ - T[a1, s])

        # Renormalize to guard against numerical drift
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta', 'beta', 'pi', 'phi', 'nu']"
iter2_run0_participant3.json,cognitive_model3,520.8687464179164,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Arbitrated MF/MB with anxiety-amplified forgetting and rare-transition credit.
    
    Core ideas:
    - Stage-2 MF learning with forgetting toward a neutral prior for unvisited/unchosen options.
    - Stage-1 arbitration between MB and MF values; anxiety reduces the MB weight
      and increases forgetting.
    - After rare transitions, propagate a larger portion of the stage-2 PE to stage-1 MF,
      scaled by anxiety (biased credit assignment).
    
    Parameters:
    - action_1: int array {0,1}, first-stage choices.
    - state:    int array {0,1}, reached second-stage state.
    - action_2: int array {0,1}, second-stage choices.
    - reward:   float array [0,1], outcomes.
    - stai:     float array with single element in [0,1], anxiety score.
    - model_parameters: 5-tuple/list of:
        eta  in [0,1]: learning rate for stage-2 MF and stage-1 MF bootstrap
        beta in [0,10]: inverse temperature for both stages
        pi   in [0,1]: baseline MB weight at stage 1 (before anxiety modulation)
        f0   in [0,1]: baseline forgetting rate toward 0.5 for unchosen/uncharted Q-values
        b    in [0,1]: anxiety coupling factor for both forgetting and rare-transition credit
    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """"""
    eta, beta, pi, f0, b = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed task transitions (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.full((2, 2), 0.5, dtype=float)  # initialize around neutral prior
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # Stage-1 MB evaluation from fixed transitions and current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Anxiety reduces MB weight
        w = pi * (1.0 - 0.5 * b * s_anx)
        w = np.clip(w, 0.0, 1.0)
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # First-stage policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s_idx = int(state[t])
        logits2 = beta * q2[s_idx]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Stage-2 update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += eta * delta2

        # Apply anxiety-amplified forgetting toward 0.5 for all other entries
        f_eff = np.clip(f0 * (1.0 + b * s_anx), 0.0, 1.0)
        # Decay unchosen action in the visited state
        other_a = 1 - a2
        q2[s_idx, other_a] = (1.0 - f_eff) * q2[s_idx, other_a] + f_eff * 0.5
        # Decay both actions in the unvisited state
        other_s = 1 - s_idx
        q2[other_s, 0] = (1.0 - f_eff) * q2[other_s, 0] + f_eff * 0.5
        q2[other_s, 1] = (1.0 - f_eff) * q2[other_s, 1] + f_eff * 0.5

        # Stage-1 MF update with rare-transition credit amplification
        # Compute rarity of the observed transition given chosen action
        p_trans = T[a1, s_idx]  # probability of observed transition under fixed structure
        rarity = 1.0 - p_trans  # 0.3 for rare, 0.0 for impossible (not applicable), 0.3 rare, 0.7 common -> rarity small
        # Bootstrap update
        target1 = q2[s_idx, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * delta1
        # Additional credit proportional to stage-2 PE, rarity, and anxiety
        q1_mf[a1] += (b * s_anx) * rarity * eta * delta2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['eta', 'beta', 'pi', 'f0', 'b']"
iter2_run0_participant18.json,cognitive_model1,415.4793843301761,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned-transition hybrid with anxiety-modulated arbitration, temperature, and perseveration.
    
    Description:
    - The agent learns the first-stage transition structure T_hat online with a delta rule.
    - Stage-1 action values are a hybrid of model-based planning using T_hat and model-free Q1.
    - Arbitration weight, softmax temperature, and perseveration are modulated by anxiety (stai).
      Higher anxiety reduces MB weight, increases exploration (lower beta), and reduces perseveration.
    - Stage-2 values are learned model-free. Stage-1 MF uses a TD and an eligibility propagation from stage-2.
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature base in [0,10]
    - kappa_T: transition learning rate in [0,1]
    - w0_mb: base MB weight in [0,1]
    - perseveration: action repetition strength in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, kappa_T, w0_mb, perseveration)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, kappa_T, w0_mb, perseveration = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize transition estimate (rows sum to 1); start agnostic at 0.5/0.5
    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)       # model-free values for stage-1 actions
    Q2 = np.zeros((2, 2))     # stage-2 MF values: state x action

    # Anxiety-modulated arbitration and temperature
    w_mb = w0_mb * (1.0 - st)                    # higher anxiety reduces MB weighting
    w_mb = min(1.0, max(0.0, w_mb))
    beta_eff = beta * (1.0 - 0.5 * st)           # higher anxiety -> more exploration (lower beta)
    beta_eff = max(1e-6, beta_eff)

    # Perseveration scaled down by anxiety
    pers_eff = perseveration * (1.0 - st)

    prev_a1 = None
    prev_a2 = [None, None]

    lambda_elig = 0.5  # fixed eligibility trace for propagating stage-2 PE to stage-1

    for t in range(n_trials):
        s = int(state[t])

        # Model-based Q at stage-1 via current transition estimate
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_hat @ max_Q2

        # Hybrid Q
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Add perseveration bias at stage 1
        pref1 = Q1_hyb.copy()
        if prev_a1 is not None:
            pref1[prev_a1] += pers_eff

        # Softmax for stage 1
        pref1_centered = pref1 - np.max(pref1)
        exp1 = np.exp(beta_eff * pref1_centered)
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration within state
        pref2 = Q2[s].copy()
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += pers_eff

        pref2_centered = pref2 - np.max(pref2)
        exp2 = np.exp(beta_eff * pref2_centered)
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Update transition estimate for the chosen first-stage action based on observed state
        # Delta rule toward one-hot outcome with row sum preserved
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T_hat[a1, sp] += kappa_T * (target - T_hat[a1, sp])
        # Ensure numerical stability of row
        row_sum = np.sum(T_hat[a1])
        if row_sum > 0:
            T_hat[a1] /= row_sum

        # TD learning
        # Stage-1 MF TD toward the observed stage-2 value (bootstrapped)
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Stage-2 MF TD toward reward
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Eligibility propagation from stage-2 to stage-1 for the taken a1
        Q1_mf[a1] += alpha * lambda_elig * delta2

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'kappa_T', 'w0_mb', 'perseveration']"
iter2_run0_participant18.json,cognitive_model2,456.3608356530242,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Valence-asymmetric MF learning with MB planning and anxiety-shaped asymmetry and policy bias.
    
    Description:
    - Stage-1 action values are a weighted combination of MB (fixed transition) and MF values.
    - MF learning uses valence-asymmetric learning rates: alpha_pos for positive PEs and
      alpha_neg = alpha_pos * (1 + k_neg * f(stai)), where f(stai) increases with anxiety.
      Thus, higher anxiety increases sensitivity to worse-than-expected outcomes.
    - A state-contingent choice bias favors the spaceship whose common destination matches the
      last rewarded planet; the bias is dampened by anxiety.
    - Stage-2 is MF with softmax policy.
    
    Parameters (model_parameters):
    - alpha: base learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - k_neg: multiplier (0-1) controlling how much larger alpha_neg is vs alpha_pos, [0,1]
    - omega0: base MB weight in [0,1]
    - bias: state-contingent first-stage bias strength in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}
    - state: array-like of ints in {0,1}
    - action_2: array-like of ints in {0,1}
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]
    - model_parameters: tuple/list (alpha, beta, k_neg, omega0, bias)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, k_neg, omega0, bias = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed true transition structure (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated MB weight: higher anxiety reduces MB reliance
    omega = omega0 * (1.0 - st)
    omega = min(1.0, max(0.0, omega))

    # Valence asymmetry as a function of anxiety
    # Higher anxiety -> larger alpha_neg relative to alpha_pos
    alpha_pos = alpha
    alpha_neg = alpha * (1.0 + k_neg * st)
    alpha_neg = min(1.0, max(0.0, alpha_neg))

    # State-contingent bias toward spaceship commonly leading to last rewarded planet
    last_rewarded_state = None
    bias_eff = bias * (1.0 - st)

    for t in range(n_trials):
        s = int(state[t])

        # MB at stage-1 from stage-2 values
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid Q
        Q1_hyb = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Apply state-contingent bias if last rewarded state is known:
        # Favor action whose common destination is that state
        pref1 = Q1_hyb.copy()
        if last_rewarded_state is not None:
            # Common destinations: action 0->state 0, action 1->state 1
            common_dest = [0, 1]
            for a in range(2):
                if common_dest[a] == last_rewarded_state:
                    pref1[a] += bias_eff

        # Softmax at stage-1
        pref1_centered = pref1 - np.max(pref1)
        exp1 = np.exp(beta * pref1_centered)
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax (pure MF)
        pref2 = Q2[s].copy()
        pref2_centered = pref2 - np.max(pref2)
        exp2 = np.exp(beta * pref2_centered)
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD updates with valence asymmetry
        # Stage-1 MF bootstraps from stage-2 chosen action value
        delta1 = Q2[s, a2] - Q1_mf[a1]
        lr1 = alpha_pos if delta1 >= 0 else alpha_neg
        Q1_mf[a1] += lr1 * delta1

        # Stage-2 MF toward reward
        delta2 = r - Q2[s, a2]
        lr2 = alpha_pos if delta2 >= 0 else alpha_neg
        Q2[s, a2] += lr2 * delta2

        # Update last rewarded state when reward is obtained (threshold at 0.5)
        if r >= 0.5:
            last_rewarded_state = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'k_neg', 'omega0', 'bias']"
iter2_run0_participant18.json,cognitive_model3,457.0777266984459,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Directed exploration bonus at stage-2 with anxiety damping and eligibility-trace MF + MB mix.
    
    Description:
    - Stage-2 incorporates a count-based uncertainty bonus: b_eff / sqrt(N[state, action] + 1).
      Anxiety reduces directed exploration by scaling b_eff = bonus * (1 - stai).
    - Stage-1 uses a hybrid MB/MF value; MB weight is reduced by anxiety and further reduced
      after rare transitions on the previous trial (more MF after surprising events).
    - MF learning uses an eligibility trace (trace parameter) to propagate reward back to stage-1.
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - bonus: base directed-exploration strength in [0,1]
    - trace: eligibility trace lambda in [0,1]
    - mix: base MB weight at stage-1 in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}
    - state: array-like of ints in {0,1}
    - action_2: array-like of ints in {0,1}
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]
    - model_parameters: tuple/list (alpha, beta, bonus, trace, mix)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, bonus, trace, mix = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Visit counts for stage-2 actions to compute bonus
    N2 = np.zeros((2, 2), dtype=float)

    # Anxiety effects
    b_eff = bonus * (1.0 - st)  # higher anxiety -> smaller directed exploration
    w_base = mix * (1.0 - st)   # higher anxiety -> lower MB weight at baseline
    w_base = min(1.0, max(0.0, w_base))

    prev_a1 = None
    prev_s = None

    for t in range(n_trials):
        s = int(state[t])

        # Compute model-based Q1 from current Q2 and known transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Adapt MB weight further based on whether previous transition was rare
        w_mb = w_base
        if prev_a1 is not None and prev_s is not None:
            # Determine if previous observed state was a rare outcome for prev_a1
            prob_prev = T[prev_a1, prev_s]
            was_rare = 1.0 if prob_prev < 0.5 else 0.0
            # After rare transitions, rely more on MF (reduce MB), amplified by anxiety
            w_mb = w_base * (1.0 - st * was_rare)
        w_mb = min(1.0, max(0.0, w_mb))

        # Hybrid stage-1 Q
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        pref1 = Q1_hyb.copy()
        pref1_centered = pref1 - np.max(pref1)
        exp1 = np.exp(beta * pref1_centered)
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus
        # Add UCB-like bonus
        bonus_vec = b_eff / np.sqrt(N2[s] + 1.0)
        pref2 = Q2[s] + bonus_vec
        pref2_centered = pref2 - np.max(pref2)
        exp2 = np.exp(beta * pref2_centered)
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Update counts after observing the chosen action at stage-2
        N2[s, a2] += 1.0

        # TD updates
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Eligibility trace to propagate reward PE back to stage-1 choice
        Q1_mf[a1] += alpha * trace * delta2

        prev_a1 = a1
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'bonus', 'trace', 'mix']"
iter2_run0_participant21.json,cognitive_model2,564.770655585618,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Bayesian reward tracking with uncertainty bonus tempered by anxiety.

    Overview:
    - Each planet-alien option has a Beta-Bernoulli posterior over coin probability.
    - The decision value is the posterior mean plus an uncertainty bonus proportional
      to posterior standard deviation. Anxiety suppresses this bonus (less uncertainty seeking).
    - Stage-1 planning is fully model-based through the known transitions using these stage-2 values.

    Parameters (bounds):
    - model_parameters[0] = beta (0..10): inverse temperature for softmax at both stages
    - model_parameters[1] = tau_unc (0..1): base weight on uncertainty bonus (exploration)
    - model_parameters[2] = m0 (0..1): baseline prior mean of coin probability
    - model_parameters[3] = s0 (0..1): baseline prior strength (mapped to pseudo-counts)
    - model_parameters[4] = anx_pess (0..1): anxiety-driven shift of prior mean (higher anxiety -> more pessimistic if >0)

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on reached planet
    - reward: array of floats (0/1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters within the bounds above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    beta, tau_unc, m0, s0, anx_pess = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transitions
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Map prior settings to Beta pseudo-counts
    # Prior strength in [1, 21], to be neither too weak nor too strong
    strength = 1.0 + 20.0 * s0
    # Anxiety shifts prior mean: higher anxiety reduces mean if anx_pess > 0.5, increases if < 0.5
    signed = 2.0 * anx_pess - 1.0  # in [-1,1]
    m_eff = np.clip(m0 - 0.3 * signed * (stai_val - 0.5), 1e-3, 1 - 1e-3)

    a_counts = np.ones((2, 2)) * (strength * m_eff)
    b_counts = np.ones((2, 2)) * (strength * (1.0 - m_eff))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Posterior means and uncertainties at stage 2
        mean_p = a_counts / (a_counts + b_counts)
        var_p = (a_counts * b_counts) / (((a_counts + b_counts) ** 2) * (a_counts + b_counts + 1.0) + eps)
        std_p = np.sqrt(var_p)

        # Anxiety-tempered uncertainty bonus: less bonus when anxiety is higher
        bonus_scale = tau_unc * (1.0 - 0.8 * stai_val)
        q2 = mean_p + bonus_scale * std_p

        # Stage-1 MB planning
        max_q2 = np.max(q2, axis=1)
        q1 = transition_matrix @ max_q2

        # Stage-1 choice likelihood
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 choice likelihood on reached planet
        s = state[t]
        q2_s = q2[s]
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update Beta counts
        r = reward[t]
        # Constrain r to [0,1] in case of noise
        r_clipped = 1.0 if r >= 0.5 else 0.0
        a_counts[s, a2] += r_clipped
        b_counts[s, a2] += 1.0 - r_clipped

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['beta', 'tau_unc', 'm0', 's0', 'anx_pess']"
iter2_run0_participant22.json,cognitive_model1,431.4072360606932,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with learned transition model and anxiety-modulated MB weight.
    
    This model learns:
      - Stage-2 model-free Q-values for each alien on each planet.
      - A transition model T[a, s] for each spaceship a to each planet s.
    First-stage decisions use a hybrid of model-free and model-based values, with the
    model-based weight increased or decreased by anxiety (stai).
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for Q-value updates at both stages.
    - beta: [0,10] inverse temperature for softmax choice at both stages.
    - w0: [0,1] baseline weight on model-based control at stage 1.
    - eta_trans: [0,1] learning rate for updating the transition model T.
    - anx_mb: [0,1] strength by which anxiety shifts the MB weight:
               w_eff = clip(w0 + anx_mb*(stai - 0.5), 0, 1).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U).
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien per trial (0/1 on each planet).
    - reward: array of floats (e.g., 0/1), coins obtained per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, w0, eta_trans, anx_mb].
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, w0, eta_trans, anx_mb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize values
    q1_mf = np.zeros(2)        # model-free Q at stage 1 (spaceships)
    q2 = np.zeros((2, 2))      # model-free Q at stage 2 (planets x aliens)

    # Initialize transition model T[a, s]; start from the instructed structure.
    T = np.array([[0.7, 0.3],  # A -> X common, Y rare
                  [0.3, 0.7]]) # U -> X rare,  Y common

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight (fixed across trials for parsimony)
    w_eff = w0 + anx_mb * (stai - 0.5)
    w_eff = max(0.0, min(1.0, w_eff))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Model-based Q for stage 1 from current transition model and stage-2 values
        max_q2 = np.max(q2, axis=1)                 # best alien per planet
        q1_mb = T @ max_q2                          # expected value per spaceship

        # Hybrid Q for policy at stage 1
        q1_hyb = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Stage-1 policy (softmax)
        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax on q2 for reached state)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Learning: stage 2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning: stage 1 MF update (bootstrapping from reached state/action)
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update transition model T for the chosen spaceship using the observed state
        # Simple exponential recency-weighted update toward one-hot observation
        # Decay current row
        T[a1, :] = (1.0 - eta_trans) * T[a1, :]
        # Increment observed transition
        T[a1, s] += eta_trans
        # Renormalize to ensure it's a proper probability vector
        T[a1, :] /= (np.sum(T[a1, :]) + 1e-16)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'eta_trans', 'anx_mb']"
iter2_run0_participant22.json,cognitive_model2,452.9409603425504,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive model-based planning with anxiety-modulated risk aversion and forgetting.
    
    This model:
      - Learns stage-2 reward probabilities (means) model-free.
      - Uses a risk-sensitive utility u = q - rho_eff * q*(1-q) that penalizes uncertainty.
      - Plans at stage 1 using the fixed transition structure and the risk-sensitive utilities
        of the best alien on each planet.
      - Applies uniform forgetting to all stage-2 Q-values each trial.
      - Anxiety (stai) increases risk aversion: rho_eff = clip(risk0 + anx_risk*stai, 0,1).
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for stage-2 Q updates.
    - beta: [0,10] inverse temperature for softmax at both stages.
    - k_forget: [0,1] forgetting/decay rate applied to all stage-2 Q-values each trial.
    - risk0: [0,1] baseline risk aversion.
    - anx_risk: [0,1] anxiety leverage on risk aversion.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial.
    - state: array of ints in {0,1}, reached planet per trial.
    - action_2: array of ints in {0,1}, chosen alien per trial.
    - reward: array of floats (e.g., 0/1), coins obtained per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, k_forget, risk0, anx_risk].
    
    Returns:
    - Negative log-likelihood of observed choices (both stages).
    """"""
    alpha, beta, k_forget, risk0, anx_risk = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition matrix per task instructions
    T = np.array([[0.7, 0.3],  # A -> X common, Y rare
                  [0.3, 0.7]]) # U -> X rare,  Y common

    # Stage-2 mean reward estimates (interpreted as coin probabilities)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated risk aversion
    rho_eff = risk0 + anx_risk * stai
    rho_eff = max(0.0, min(1.0, rho_eff))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Apply uniform forgetting to stage-2 Qs
        q2 *= (1.0 - k_forget)

        # Compute risk-sensitive utilities for each planet's aliens
        # For Bernoulli, variance estimate ~ q*(1-q)
        var2 = q2 * (1.0 - q2)
        u2 = q2 - rho_eff * var2

        # Stage-2 policy uses risk-sensitive utilities
        logits2 = beta * u2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Model-based planning at stage 1 uses the best utility on each planet
        best_u_per_planet = np.max(u2, axis=1)
        q1_mb = T @ best_u_per_planet

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Learning: update mean reward at stage 2 with the actual reward
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'k_forget', 'risk0', 'anx_risk']"
iter2_run0_participant22.json,cognitive_model3,381.1981028434682,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Reliability-based arbitration between MB and MF with anxiety-modulated arbitration
    sensitivity and choice stickiness at both stages.
    
    Core ideas:
      - Stage-2 MF learning of alien values.
      - Stage-1 uses a dynamic weight w_t to mix MB and MF values:
            Q1 = (1 - w_t)*Q1_MF + w_t*Q1_MB.
      - Arbitration weight w_t depends on relative reliabilities of MB vs MF on the previous trial:
            rel_MB_t  = 0.7 if transition was common, 0.3 if rare (inverse of surprise).
            rel_MF_t  = 1 - |delta2_t| where delta2_t is the stage-2 prediction error.
            w_{t+1}   = sigmoid(kappa_eff * (rel_MB_t - rel_MF_t)).
      - Anxiety increases arbitration sensitivity: kappa_eff = kappa0*(1 + anx_arbit*stai).
      - A stickiness term biases repeating the previous action at each stage.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for MF updates.
    - beta: [0,10] inverse temperature for softmax at both stages.
    - kappa0: [0,1] baseline arbitration sensitivity.
    - anx_arbit: [0,1] anxiety leverage on arbitration sensitivity.
    - stick: [0,1] stickiness added to the previously chosen action's logit (per stage).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial.
    - state: array of ints in {0,1}, reached planet per trial.
    - action_2: array of ints in {0,1}, chosen alien per trial.
    - reward: array of floats, coins obtained per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, kappa0, anx_arbit, stick].
    
    Returns:
    - Negative log-likelihood of observed choices (both stages).
    """"""
    alpha, beta, kappa0, anx_arbit, stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Arbitration sensitivity with anxiety
    kappa_eff = kappa0 * (1.0 + anx_arbit * stai)
    # Map to a reasonable range for sigmoid sensitivity by scaling to [0,10] softly
    kappa_eff = max(0.0, min(10.0, 10.0 * kappa_eff))

    # Initialize arbitration weight using neutral prior
    w_prev = 0.5

    # Stickiness trackers
    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 MB values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid Q with previous trial's arbitration weight
        q1_hyb = (1.0 - w_prev) * q1_mf + w_prev * q1_mb

        # Stage-1 policy with stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick
        logits1 = beta * q1_hyb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick
        logits2 = beta * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning at stage 1 (MF)
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Compute reliabilities to update arbitration weight for next trial
        # Transition common if reached planet equals chosen spaceship index
        is_common = 1 if s == a1 else 0
        rel_mb = 0.7 if is_common == 1 else 0.3
        rel_mf = 1.0 - min(1.0, abs(delta2))

        # Update arbitration weight for next trial
        diff_rel = rel_mb - rel_mf
        w_prev = 1.0 / (1.0 + np.exp(-kappa_eff * diff_rel))
        w_prev = max(0.0, min(1.0, w_prev))

        # Update stickiness trackers
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'kappa0', 'anx_arbit', 'stick']"
iter2_run0_participant24.json,cognitive_model1,536.7406725511375,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 1: Model-based planning with learned transition dynamics and anxiety-dampened exploration bonus.
    
    Overview:
    - Learns second-stage Q-values from reward.
    - Learns first-stage transition probabilities from observed transitions (separately per spaceship).
    - First-stage policy is model-based: Q_MB(a1) = sum_s P(s|a1) * max_a2 Q2[s, a2].
    - Adds an uncertainty-directed, count-based exploration bonus to first-stage preferences that decreases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha_q, beta, omega_bonus, alpha_T]
        - alpha_q in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega_bonus in [0,1]: scale of the first-stage exploration bonus.
        - alpha_T in [0,1]: transition learning rate for P(state | action_1).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_q, beta, omega_bonus, alpha_T = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize learned transition probabilities P(state | action1)
    # Rows: action1 in {0,1}; Cols: state in {0,1}; rows sum to 1.
    T = np.full((2, 2), 0.5)

    # Second-stage action values
    q2 = np.zeros((2, 2))

    # Count-based exploration at stage 1
    N1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage values using current transition estimates
        max_q2_by_state = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2_by_state          # shape (2,)

        # Anxiety-dampened UCB-style exploration bonus for less tried first-stage actions
        # Bonus decays with sqrt of counts; higher anxiety reduces the bonus amplitude.
        bonus_scale = omega_bonus * (1.0 - s)
        bonus1 = bonus_scale / np.sqrt(1.0 + N1)

        # First-stage policy
        prefs1 = q1_mb + bonus1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (pure MF)
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]

        # Update second-stage Q-values (MF)
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_q * pe2

        # Update transition model for the chosen first-stage action
        # Move the probability mass toward the observed state using alpha_T
        oh = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        T[a1] += alpha_T * (oh - T[a1])

        # Update counts for exploration bonus
        N1[a1] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_q', 'beta', 'omega_bonus', 'alpha_T']"
iter2_run0_participant24.json,cognitive_model2,457.682707921267,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 2: Hybrid MB/MF with anxiety- and surprise-gated model-based control.
    
    Overview:
    - Combines model-free (MF) and model-based (MB) action values at the first stage.
    - MB uses the known transition structure (common = 0.7, rare = 0.3).
    - The MB weight is reduced by (a) higher anxiety and (b) surprising (rare) transitions.
    - Second-stage policy is MF; MF values are updated via SARSA(0)-style backup from stage 2 to stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, omega_base, zeta_surprise]
        - alpha in [0,1]: learning rate for MF Q-values (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega_base in [0,1]: baseline MB weight when anxiety and surprise are zero.
        - zeta_surprise in [0,1]: how much rare transitions suppress MB control.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, omega_base, zeta_surprise = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T_known = np.array([[0.7, 0.3],  # from A
                        [0.3, 0.7]]) # from U

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB values computed from known transitions and current q2
        max_q2_by_state = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2_by_state

        # Determine surprise (rare transition) for current trial
        a1 = action_1[t]
        st = state[t]
        # common if (A and X) or (U and Y), rare otherwise
        is_common = (a1 == 0 and st == 0) or (a1 == 1 and st == 1)
        surprise = 0.0 if is_common else 1.0

        # Anxiety- and surprise-gated MB weight
        # Higher anxiety and surprise reduce omega_t multiplicatively.
        omega_t = omega_base * (1.0 - s) * (1.0 - zeta_surprise * surprise)
        omega_t = max(0.0, min(1.0, omega_t))

        # First-stage policy from hybrid values
        q1_hybrid = omega_t * q1_mb + (1.0 - omega_t) * q1_mf
        exp1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (MF)
        a2 = action_2[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha * pe2

        # Stage-1 MF update via SARSA(0) style using obtained second-stage value
        pe1 = q2[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega_base', 'zeta_surprise']"
iter2_run0_participant24.json,cognitive_model3,543.356979611113,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 3: Adaptive (PearceâHall) learning rate and anxiety-modulated exploration temperature.
    
    Overview:
    - Second-stage learning uses an adaptive learning rate that increases with unsigned prediction error.
    - Anxiety increases sensitivity to surprise during learning (boosts the adaptive component)
      and reduces inverse temperature (more random choices).
    - First-stage policy is model-based using the known transition matrix.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha0, phi0, beta, nu_anx]
        - alpha0 in [0,1]: baseline learning rate for second-stage values.
        - phi0 in [0,1]: gain on the adaptive (unsigned-PE-based) increment to the learning rate.
        - beta in [0,10]: baseline inverse temperature for softmax.
        - nu_anx in [0,1]: how strongly anxiety reduces the effective beta.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha0, phi0, beta, nu_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Known transition structure
    T_known = np.array([[0.7, 0.3],  # from A
                        [0.3, 0.7]]) # from U

    # Second-stage values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration (lower beta -> more random)
    beta_eff = beta * (1.0 - nu_anx * s)
    if beta_eff < 0.0:
        beta_eff = 0.0

    for t in range(n_trials):
        # First-stage MB values: expected best second-stage value per action
        max_q2_by_state = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2_by_state

        # First-stage policy
        exp1 = np.exp(beta_eff * (q1_mb - np.max(q1_mb)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (MF with same beta_eff)
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta_eff * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning with adaptive rate at stage 2
        r = reward[t]
        pe2 = r - q2[st, a2]
        # Anxiety increases the impact of surprise on learning rate
        alpha_t = alpha0 + (1.0 + s) * phi0 * abs(pe2)
        # clip to [0,1]
        if alpha_t < 0.0:
            alpha_t = 0.0
        if alpha_t > 1.0:
            alpha_t = 1.0
        q2[st, a2] += alpha_t * pe2

        # No separate MF update at stage 1; first-stage control is purely MB in this model.

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha0', 'phi0', 'beta', 'nu_anx']"
iter2_run0_participant28.json,cognitive_model1,459.53559199919175,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with learned transitions and anxiety-gated arbitration.

    A hybrid controller combines a model-free (MF) TD learner with a model-based (MB)
    planner that uses a learned transition model. Arbitration between MB and MF
    depends on the current uncertainty of the learned transition model, and anxiety
    up- or down-regulates the MB weight in proportion to that uncertainty.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states observed (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 for the two aliens on that planet).
    reward : array-like of float in [0,1]
        Obtained reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; modulates arbitration based on transition uncertainty.
    model_parameters : list or array-like of 5 floats
        [alpha_mf, alpha_T, beta, zeta, psi]
        - alpha_mf (0..1): learning rate for MF Q-values (both stages).
        - alpha_T  (0..1): learning rate for transition probabilities T(a->s).
        - beta     (0..10): inverse temperature for softmax (both stages).
        - zeta     (0..1): baseline weight on MB action values.
        - psi      (0..1): anxiety modulation strength of MB weight w.r.t.
                           transition uncertainty. Effective MB weight
                           w_eff = clip(zeta + psi * (stai-0.5) * U, 0, 1),
                           where U is normalized transition uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_mf, alpha_T, beta, zeta, psi = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Initialize learned transition model T[action, state]; start near task structure but learnable
    # Prior slightly informative toward common transitions: A->X, U->Y.
    T = np.array([[0.65, 0.35],
                  [0.35, 0.65]], dtype=float)

    # MF Q-values
    Q1_mf = np.zeros(2)         # stage-1 MF action values
    Q2 = np.zeros((2, 2))       # stage-2 MF action values: Q2[state, action]

    p1 = np.zeros(n)
    p2 = np.zeros(n)
    eps = 1e-10

    for t in range(n):
        # Model-based Q1 from learned transitions and current Q2
        max_Q2 = np.max(Q2, axis=1)            # value of each planet
        Q1_mb = T @ max_Q2                     # MB estimate over states

        # Arbitration weight: scale baseline zeta by transition uncertainty and anxiety.
        # Compute mean normalized entropy over actions as uncertainty U in [0,1].
        # Entropy H(p) normalized by log(2): Hn = -sum p log p / log2
        # Avoid log(0) with small epsilon.
        Hs = []
        for a in range(2):
            pa = T[a]
            pa = np.clip(pa, 1e-6, 1.0)
            Ha = -(pa[0]*np.log(pa[0]) + pa[1]*np.log(pa[1])) / np.log(2.0)
            Hs.append(Ha)
        U = max(0.0, min(1.0, 0.5*(Hs[0] + Hs[1])))

        w_eff = zeta + psi * (st - 0.5) * U
        w_eff = max(0.0, min(1.0, w_eff))

        # Hybrid Q for stage 1
        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf

        # First-stage policy
        pref1 = Q1 - np.max(Q1)
        probs1 = np.exp(beta * pref1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p1[t] = probs1[a1]

        # Second-stage policy (pure MF at stage 2)
        s = int(state[t])
        pref2 = Q2[s] - np.max(Q2[s])
        probs2 = np.exp(beta * pref2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        r = float(reward[t])

        # Update transition model T using observed transition (a1 -> s)
        # Simple delta rule toward 1 for observed state and 0 for the other, with normalization.
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_T * (target - T[a1, sp])
        # Ensure row-stochastic within numerical tolerance
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        # Stage-2 MF update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_mf * pe2

        # Stage-1 MF update (bootstrapped from the realized second-stage value)
        pe1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha_mf * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return neg_ll

","['alpha_mf', 'alpha_T', 'beta', 'zeta', 'psi']"
iter2_run0_participant28.json,cognitive_model2,517.2315381490629,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive learning via volatility with anxiety-coupled gain and stickiness.

    A model-based planner selects first-stage actions using the known task structure,
    while second-stage values are learned by a volatility-adaptive TD rule. The learning
    rate at stage 2 increases with estimated volatility (tracked from squared PE), and
    anxiety scales the volatility gain. Anxiety also modulates a mild second-stage
    perseveration bias.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; scales volatility sensitivity and stickiness.
    model_parameters : list or array-like of 5 floats
        [alpha0, kappa_vol, beta, tau, chi]
        - alpha0 (0..1): base learning rate for Q updates.
        - kappa_vol (0..1): volatility update rate from squared PEs.
        - beta (0..10): inverse temperature for both stages.
        - tau (0..1): baseline second-stage perseveration strength.
        - chi (0..1): anxiety modulation strength applied to
                      both volatility gain and perseveration.
                      Effective learning rate: alpha_t = clip(alpha0 + chi*(stai-0.5)*sqrt(v)),
                      Effective stickiness: rho = tau * (1 + chi*(stai-0.5)).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha0, kappa_vol, beta, tau, chi = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Known task transition structure for MB planning
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)  # will be used to propagate MF prediction to stage 1 (optional bootstrapping)
    v = np.zeros((2, 2))  # volatility proxy per state-action (squared PE EWMA)

    p1 = np.zeros(n)
    p2 = np.zeros(n)
    eps = 1e-10

    # Perseveration memory for stage 2 per state
    last_a2 = np.array([None, None], dtype=object)
    rho = tau * (1.0 + chi * (st - 0.5))

    for t in range(n):
        # Model-based Q1 from known transitions and current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_known @ max_Q2

        # First-stage softmax
        pref1 = Q1_mb - np.max(Q1_mb)
        probs1 = np.exp(beta * pref1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p1[t] = probs1[a1]

        # Second-stage softmax with stickiness
        s = int(state[t])
        pref2 = Q2[s].copy()
        if last_a2[s] is not None:
            stick_vec = np.array([0.0, 0.0])
            stick_vec[int(last_a2[s])] = 1.0
            pref2 = pref2 + rho * stick_vec
        pref2 = pref2 - np.max(pref2)
        probs2 = np.exp(beta * pref2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        # Outcomes
        r = float(reward[t])

        # Volatility-adaptive learning rate at stage 2
        pe2 = r - Q2[s, a2]
        # Update volatility proxy (EWMA of squared PEs)
        v[s, a2] = (1.0 - kappa_vol) * v[s, a2] + kappa_vol * (pe2 * pe2)
        # Anxiety scales learning gain via sqrt volatility to keep in [0,1] range
        alpha_t = alpha0 + chi * (st - 0.5) * np.sqrt(max(0.0, v[s, a2]))
        alpha_t = max(0.0, min(1.0, alpha_t))

        # Update Q2
        Q2[s, a2] += alpha_t * pe2

        # Optional MF bootstrapping at stage 1 to capture some MF variance in choices
        pe1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha0 * pe1  # uses base rate; MB policy dominates at stage 1

        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return neg_ll

","['alpha0', 'kappa_vol', 'beta', 'tau', 'chi']"
iter2_run0_participant28.json,cognitive_model3,454.40087455521507,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""MF with outcome-by-transition bias at stage 1 and anxiety-tuned temperatures.

    A model-free learner updates action values at both stages. Stage-1 choices additionally
    include a model-based-like bias that depends on whether the previous transition was
    common vs rare and whether it was rewarded (classic stay/switch interaction).
    Anxiety tunes the strength of this bias and also modulates separate inverse temperatures
    at stage 1 and stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; scales bias strength and exploration temperatures.
    model_parameters : list or array-like of 5 floats
        [alpha, beta1, beta2, theta, mu]
        - alpha (0..1): learning rate for MF Q-values at both stages.
        - beta1 (0..10): base inverse temperature at stage 1.
        - beta2 (0..10): base inverse temperature at stage 2.
        - theta (0..1): baseline strength of the outcome-by-transition stay/switch bias.
        - mu (0..1): anxiety modulation strength.
          Effective bias: b = theta * (1 + mu*(stai-0.5)).
          Effective temperatures:
            beta1_eff = beta1 * (1 + mu*(stai-0.5))
            beta2_eff = beta2 * (1 - mu*(stai-0.5))

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta1, beta2, theta, mu = model_parameters
    n = len(action_1)
    st = float(stai[0])

    beta1_eff = beta1 * (1.0 + mu * (st - 0.5))
    beta2_eff = beta2 * (1.0 - mu * (st - 0.5))
    # Clip to valid ranges to be safe
    beta1_eff = max(0.0, min(10.0, beta1_eff))
    beta2_eff = max(0.0, min(10.0, beta2_eff))

    b_eff = theta * (1.0 + mu * (st - 0.5))

    # MF Q-values
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p1 = np.zeros(n)
    p2 = np.zeros(n)
    eps = 1e-10

    # Memory of previous trial to compute bias
    last_a1 = None
    last_state = None
    last_reward = None

    for t in range(n):
        # Stage-1 preferences from MF plus outcome-by-transition bias toward stay/switch
        pref1 = Q1.copy()

        if last_a1 is not None:
            # Determine whether previous transition was common vs rare based on task structure:
            # A commonly -> X; U commonly -> Y.
            was_common = ((last_a1 == 0 and last_state == 0) or (last_a1 == 1 and last_state == 1))
            c = 1.0 if was_common else -1.0
            rsgn = 1.0 if float(last_reward) > 0.5 else -1.0
            bias_val = b_eff * c * rsgn
            # Apply symmetric bias to prefer staying vs switching
            bias_vec = np.array([0.0, 0.0])
            bias_vec[int(last_a1)] += bias_val
            bias_vec[1 - int(last_a1)] -= bias_val
            pref1 = pref1 + bias_vec

        # Stage-1 policy
        pref1_c = pref1 - np.max(pref1)
        probs1 = np.exp(beta1_eff * pref1_c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p1[t] = probs1[a1]

        # Stage-2 policy (MF) without additional bias
        s = int(state[t])
        pref2 = Q2[s] - np.max(Q2[s])
        probs2 = np.exp(beta2_eff * pref2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        r = float(reward[t])

        # MF updates
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        pe1 = Q2[s, a2] - Q1[a1]
        Q1[a1] += alpha * pe1

        # Update memory for next-trial bias
        last_a1 = a1
        last_state = s
        last_reward = r

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return neg_ll","['alpha', 'beta1', 'beta2', 'theta', 'mu']"
iter2_run0_participant29.json,cognitive_model2,514.435021829644,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Utility-based model with anxiety-amplified loss aversion, pruned model-based planning, and Pavlovian approach/avoid.
    The agent learns second-stage utilities with asymmetric valuation (loss aversion).
    First-stage choices are purely model-based but use a pruning transform that
    overweights common transitions; pruning grows with anxiety. Pavlovian approach/avoid
    biases at stage 2 push toward options with higher expected utility and away from
    lower ones, scaled by anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        eta: [0,1] â learning rate for second-stage utilities
        beta: [0,10] â inverse temperature for both stages
        lambda_loss: [0,1] â base loss aversion (transformed internally)
        pav_bias: [0,1] â strength of Pavlovian approach/avoid bias at stage 2
        anx_gain: [0,1] â how strongly anxiety increases loss aversion and pruning

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    eta, beta, lambda_loss, pav_bias, anx_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (common 0.7, rare 0.3)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Transform loss aversion to >1 range and modulate by anxiety
    lam = 1.0 + 4.0 * lambda_loss
    lam_eff = lam * (1.0 + anx_gain * stai_val)

    # Second-stage expected utilities
    U2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # First-stage model-based planning with pruning (overweight common transitions)
        prune = np.clip(anx_gain * stai_val, 0.0, 1.0)  # 0..1
        power = 1.0 + 2.0 * prune  # raise probs to >1 to accentuate common
        # State values as max over second-stage utilities
        V = np.max(U2, axis=1)
        Q1_mb = np.zeros(2, dtype=float)
        for a in range(2):
            p = T_fixed[a].copy()
            p_trans = p**power
            p_trans /= (np.sum(p_trans) + eps)
            Q1_mb[a] = np.dot(p_trans, V)

        logits1 = Q1_mb - np.max(Q1_mb)
        probs1 = np.exp(beta * logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice with Pavlovian bias toward higher-utility option
        s = state[t]
        centered = U2[s] - np.mean(U2[s])
        pav_strength = pav_bias * (1.0 + anx_gain * stai_val)
        logits2 = U2[s] + pav_strength * centered
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome utility with loss aversion
        r = reward[t]
        # Treat r=1 as gain 1, r=0 as loss magnitude lam_eff (relative to 0 baseline)
        u = 1.0 if r >= 0.5 else -lam_eff

        # Update second-stage utilities (simple delta rule)
        pe2 = u - U2[s, a2]
        U2[s, a2] += eta * pe2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['eta', 'beta', 'lambda_loss', 'pav_bias', 'anx_gain']"
iter2_run0_participant29.json,cognitive_model3,354.77025754180653,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with anxiety-modulated volatility-sensitive temperature, forgetting, and transition-sensitive stickiness.
    The agent is model-free at both stages. A running estimate of reward volatility
    (absolute prediction error) adjusts the softmax temperature: higher volatility
    and anxiety reduce beta (more exploration). Values decay toward 0.5 via forgetting,
    and a stage-1 stickiness bias encourages repeating the previous first-stage choice,
    but this stickiness is weakened after surprising (rare) transitions, especially
    under higher anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for Q-values and volatility running average
        beta_mu: [0,10] â baseline inverse temperature
        rho_forget: [0,1] â forgetting rate toward 0.5 each trial
        stick0: [0,1] â baseline first-stage stickiness strength
        anx_vol: [0,1] â how strongly anxiety and volatility reduce beta and stickiness

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    alpha, beta_mu, rho_forget, stick0, anx_vol = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transitions for surprise computation
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Model-free Q-values
    Q1 = np.zeros(2, dtype=float)
    Q2 = np.zeros((2, 2), dtype=float)

    # Volatility estimate (running abs PE at stage 2)
    vol = 0.0

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    last_a1 = None
    last_surprise = 0.0  # from previous trial's transition

    eps = 1e-12

    for t in range(n_trials):
        # Temperature adjusted by volatility and anxiety
        beta_t = beta_mu / (1.0 + anx_vol * stai_val * vol)
        beta_t = max(beta_t, eps)

        # Stage-1 stickiness modulated by previous transition surprise and anxiety
        stick_strength = stick0 * (1.0 - anx_vol * stai_val * last_surprise)
        stick_strength = np.clip(stick_strength, 0.0, 1.0)

        logits1 = Q1.copy()
        if last_a1 is not None:
            logits1[last_a1] += stick_strength

        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta_t * logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice
        s = state[t]
        logits2 = Q2[s] - np.max(Q2[s])
        probs2 = np.exp(beta_t * logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2
        # Update volatility estimate with absolute PE
        vol = (1.0 - alpha) * vol + alpha * abs(pe2)

        # Stage-1 update (bootstraps from realized Q2)
        target1 = Q2[s, a2]
        pe1 = target1 - Q1[a1]
        Q1[a1] += alpha * pe1

        # Forgetting toward 0.5
        Q1 = (1.0 - rho_forget) * Q1 + rho_forget * 0.5
        Q2 = (1.0 - rho_forget) * Q2 + rho_forget * 0.5

        # Update for next trial: last choice and surprise based on transition rarity
        # Surprise = 1 - P(s | a1) under fixed transition matrix
        last_a1 = a1
        last_surprise = 1.0 - T_fixed[a1, s]

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha', 'beta_mu', 'rho_forget', 'stick0', 'anx_vol']"
iter2_run0_participant31.json,cognitive_model1,503.9693065874553,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with learned transitions and anxiety-modulated model-based control and transition volatility.

    This model learns second-stage Q-values and also learns the first-stage transition probabilities online.
    The first-stage policy is a mixture of model-free and model-based values, with the mixing weight
    increased or decreased by the participant's anxiety (stai). Anxiety also increases the effective
    transition learning rate (perceived environmental volatility), making transition beliefs update faster.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the encountered planet (0/1; W/S on X, P/H on Y).
    reward : array-like of float
        Rewards obtained on each trial, typically in [0,1].
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values reduce MB control and increase transition volatility.
    model_parameters : array-like of floats, length 5
        [alpha, beta, omega0, k_anx_omega, eta_tr]
        - alpha in [0,1]: learning rate for Q-values (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega0 in [0,1]: baseline weight of model-based control at stage 1.
        - k_anx_omega in [0,1]: strength by which anxiety modulates omega (MB weight).
                                 omega_eff = clip(omega0 + k_anx_omega*(stai - 0.51), 0, 1).
        - eta_tr in [0,1]: baseline learning rate for transition probabilities T(a->s).
                           Anxiety increases it: eta_eff = clip(eta_tr * (1 + (stai - 0.51)), 0, 1).

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, omega0, k_anx_omega, eta_tr = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition probabilities T[a, s]
    # Start from the canonical common/rare pattern but allow learning.
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)       # model-free first-stage Q
    q2 = np.zeros((2, 2))     # second-stage Q for each state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    omega_eff = omega0 + k_anx_omega * (stai - 0.51)
    omega_eff = 0.0 if omega_eff < 0.0 else (1.0 if omega_eff > 1.0 else omega_eff)

    eta_eff = eta_tr * (1.0 + (stai - 0.51))
    eta_eff = 0.0 if eta_eff < 0.0 else (1.0 if eta_eff > 1.0 else eta_eff)

    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    for t in range(n_trials):
        # Compute model-based Q at stage 1 using current learned transitions
        max_q2 = np.max(q2, axis=1)          # value of each second-stage state
        q1_mb = T @ max_q2                   # plan via transitions

        # Mixture of MB and MF values
        q1_mix = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # First-stage policy
        logits1 = beta_eff * q1_mix
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy conditional on observed state
        s = state[t]
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # TD updates
        # Stage-1 MF update bootstraps on the chosen stage-2 action value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Stage-2 update from reward
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learn transition probabilities T[a1] toward the observed state 's'
        # Simple exponential recency-weighted update toward one-hot(s)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - eta_eff) * T[a1] + eta_eff * target
        # Keep rows normalized (they should remain normalized by construction, but renormalize for safety)
        T[a1] = T[a1] / (np.sum(T[a1]) + 1e-12)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega0', 'k_anx_omega', 'eta_tr']"
iter2_run0_participant31.json,cognitive_model2,451.24147534680947,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive model-free SARSA with second-stage perseveration; anxiety increases outcome curvature.

    Rewards are transformed through a risk-/loss-sensitive utility function centered at 0.5.
    Higher anxiety increases the curvature (greater sensitivity near losses), which changes learning signals.
    First-stage values are updated by propagating the stage-2 utility prediction error (one-step SARSA back-up).
    A perseveration bias at the second stage captures stickiness of the alien choice within each planet.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1).
    reward : array-like of float
        Obtained rewards in [0,1].
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher stai increases outcome curvature phi.
    model_parameters : array-like of floats, length 5
        [alpha, beta, phi0, k_anx_phi, pi2]
        - alpha in [0,1]: learning rate for Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - phi0 in [0,1]: baseline curvature of utility transformation.
        - k_anx_phi in [0,1]: how strongly anxiety increases curvature.
                              phi_eff = clip(phi0 + k_anx_phi * stai, 0, 1).
        - pi2 in [0,1]: second-stage perseveration strength (stickiness) per state.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, phi0, k_anx_phi, pi2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective parameters
    phi_eff = phi0 + k_anx_phi * stai
    phi_eff = 0.0 if phi_eff < 0.0 else (1.0 if phi_eff > 1.0 else phi_eff)

    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Second-stage perseveration: maintain last chosen action per state
    last_a2 = [None, None]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage policy (no perseveration here)
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy with within-state perseveration bias
        s = state[t]
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] = pi2
        logits2 = beta_eff * q2[s] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Risk-/loss-sensitive utility transformation centered at 0.5
        r = reward[t]
        x = r - 0.5
        # Curvature: more curvature (higher phi) down-weights large magnitudes,
        # signs preserve gain vs loss relative to 0.5.
        util = np.sign(x) * (np.abs(x) ** (1.0 + phi_eff))

        # TD updates with utility as outcome
        delta2 = util - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Back up to stage-1 value
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'phi0', 'k_anx_phi', 'pi2']"
iter2_run0_participant31.json,cognitive_model3,392.06024449777226,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with first-stage choice kernel and anxiety-weighted surprise gating of MB control.

    The first-stage policy mixes model-based and model-free values, plus a choice kernel (habitual
    propensity independent of value). On trials with surprising transitions (rare given the fixed
    0.7/0.3 structure), model-based control is transiently down-weighted. This down-weighting is stronger
    for higher anxiety, capturing reduced reliance on planning under surprise.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1).
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety increases surprise-based down-weighting of MB control.
    model_parameters : array-like of floats, length 5
        [alpha, beta, kappa, k_anx_surprise, omega]
        - alpha in [0,1]: learning rate for Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - kappa in [0,1]: learning rate for first-stage choice kernel (propensities).
        - k_anx_surprise in [0,1]: scales how much anxiety gates MB weight after surprising transitions.
                                   omega_t = omega * (1 - k_anx_surprise * stai * surprise_t).
        - omega in [0,1]: baseline MB weight in the value mixture at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, kappa, k_anx_surprise, omega = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure for surprise detection (common=0.7)
    # A -> X common, U -> Y common
    common_prob = 0.7

    # Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernel (tendency to repeat first-stage actions)
    K = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]

        # Determine whether the observed transition was rare (surprising)
        # A(0) common->X(0); U(1) common->Y(1)
        if a1 == 0:
            p_trans = common_prob if s == 0 else (1 - common_prob)
        else:
            p_trans = common_prob if s == 1 else (1 - common_prob)
        surprise = 1.0 if p_trans < 0.5 else 0.0  # rare when probability < 0.5

        # Compute MB Q from fixed transition model
        max_q2 = np.max(q2, axis=1)
        q1_mb = np.array([
            common_prob * max_q2[0] + (1 - common_prob) * max_q2[1],  # A
            (1 - common_prob) * max_q2[0] + common_prob * max_q2[1]   # U
        ])

        # Anxiety-weighted gating of MB contribution on surprising trials
        omega_t = omega * (1.0 - k_anx_surprise * stai * surprise)
        omega_t = 0.0 if omega_t < 0.0 else (1.0 if omega_t > 1.0 else omega_t)

        # Mixed first-stage value plus choice kernel bias
        q1_mix = omega_t * q1_mb + (1.0 - omega_t) * q1_mf

        logits1 = beta_eff * q1_mix + K
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        p_choice_1[t] = p1[a1]

        # Second-stage choice policy
        a2 = action_2[t]
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # TD updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update choice kernel toward the chosen first-stage action
        K = (1.0 - kappa) * K
        K[a1] += kappa

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'kappa', 'k_anx_surprise', 'omega']"
iter2_run0_participant32.json,cognitive_model1,394.049497074653,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Model 1: Anxiety-dampened structure use with lapse and perseveration (pure model-based planning).
    
    Idea
    ----
    The agent plans using the known two-step structure, but anxiety (stai) reduces reliance
    on the common/rare structure by shrinking the assumed common-transition probability toward 0.5.
    Additionally, there is a lapse rate (random choice mixture) and choice perseveration at both
    stages that increases with anxiety.
    
    Parameters (all used)
    ---------------------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=Planet X, 1=Planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices for the reached state (0 or 1) per trial.
    reward : array-like of float
        Obtained reward on each trial, typically in [0,1].
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list-like of float
        [alpha, beta, delta, phi, eps]
        Bounds:
        - alpha in [0,1]: learning rate for second-stage Q-values
        - beta in [0,10]: softmax inverse temperature
        - delta in [0,1]: anxiety sensitivity that flattens the transition structure
        - phi in [0,1]: choice perseveration strength
        - eps in [0,1]: lapse rate (mix with uniform choice)
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, delta, phi, eps = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Anxiety-modulated transition matrix: common prob moves toward 0.5 as st increases
    p_common = 0.7 * (1.0 - delta * st) + 0.5 * (delta * st)
    p_common = float(np.clip(p_common, 0.5, 0.7))
    transition_matrix = np.array([[p_common, 1.0 - p_common],
                                  [1.0 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage Q-values for each planet (rows) and action (cols)
    q2 = np.zeros((2, 2))

    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    for t in range(n_trials):
        # Model-based values for stage 1 from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Perseveration terms (increase with anxiety)
        stick1 = phi * st
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0

        q1_policy = q1_mb + stick1 * bias1

        # Softmax with lapse
        q1c = q1_policy - np.max(q1_policy)
        probs_1_soft = np.exp(beta * q1c)
        probs_1_soft = probs_1_soft / np.sum(probs_1_soft)
        probs_1 = (1.0 - eps) * probs_1_soft + eps * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in reached state with perseveration
        s = state[t]
        q2_state = q2[s].copy()
        stick2 = phi * st
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] = 1.0

        q2_policy = q2_state + stick2 * bias2
        q2c = q2_policy - np.max(q2_policy)
        probs_2_soft = np.exp(beta * q2c)
        probs_2_soft = probs_2_soft / np.sum(probs_2_soft)
        probs_2 = (1.0 - eps) * probs_2_soft + eps * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps_num = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps_num)) + np.sum(np.log(p_choice_2 + eps_num)))
    return float(neg_log_lik)

","['alpha', 'beta', 'delta', 'phi', 'eps']"
iter2_run0_participant32.json,cognitive_model2,394.2325867340334,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Model 2: Directed exploration via uncertainty bonus with leaky counts and MF fusion.
    
    Idea
    ----
    The agent maintains leaky Beta-Bernoulli counts for each second-stage action to estimate
    mean reward and its uncertainty, and uses an Upper-Confidence-Bound-like bonus to explore.
    Anxiety reduces directed exploration. A model-free (MF) value is also learned and fused
    with the uncertainty-augmented estimate. First-stage choices are model-based by propagating
    second-stage values through the fixed transition structure. Perseveration is included.
    
    Parameters (all used)
    ---------------------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=Planet X, 1=Planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choice per trial.
    reward : array-like of float
        Reward on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list-like of float
        [alpha, beta, c_bonus, pers, decay]
        Bounds:
        - alpha in [0,1]: mixing weight between MF and uncertainty-based estimates (also MF learning rate)
        - beta in [0,10]: inverse temperature
        - c_bonus in [0,1]: strength of directed exploration bonus
        - pers in [0,1]: choice perseveration strength
        - decay in [0,1]: leak factor for Beta counts (higher -> more forgetting); interacts with stai
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, c_bonus, pers, decay = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Beta-Bernoulli pseudo-counts for each state-action: successes a, failures b
    a_counts = np.ones((2, 2))  # prior a=1
    b_counts = np.ones((2, 2))  # prior b=1

    # Model-free Q for second-stage
    q2_mf = np.zeros((2, 2))

    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    for t in range(n_trials):
        # Compute mean and uncertainty (variance) for each state-action
        n_sa = a_counts + b_counts
        mu = a_counts / n_sa
        var = (mu * (1.0 - mu)) / (n_sa + 1.0)

        # Anxiety-dampened exploration bonus
        bonus_scale = c_bonus * (1.0 - st)
        q2_unc = mu + bonus_scale * np.sqrt(var)

        # Fuse uncertainty-based estimate with MF using alpha as mixing weight
        q2_fused = (1.0 - alpha) * q2_unc + alpha * q2_mf

        # First-stage MB values via transition
        max_q2 = np.max(q2_fused, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Perseveration terms scale with anxiety
        stick1 = pers * (0.5 + 0.5 * st)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        q1_policy = q1_mb + stick1 * bias1

        q1c = q1_policy - np.max(q1_policy)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second stage policy in reached state with perseveration
        s = state[t]
        q2_state = q2_fused[s].copy()
        stick2 = pers * (0.5 + 0.5 * st)
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] = 1.0
        q2_policy = q2_state + stick2 * bias2

        q2c = q2_policy - np.max(q2_policy)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update leaky counts and MF Q
        r = reward[t]

        # Leaky counts: decay toward prior with anxiety-dependent forgetting
        leak = 1.0 - decay * (0.5 + 0.5 * st)
        a_counts *= leak
        b_counts *= leak
        # Add new observation to reached state-action
        a_counts[s, a2] += r
        b_counts[s, a2] += (1.0 - r)

        # MF update
        q2_mf[s, a2] += alpha * (r - q2_mf[s, a2])

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps_num = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps_num)) + np.sum(np.log(p_choice_2 + eps_num)))
    return float(neg_log_lik)

","['alpha', 'beta', 'c_bonus', 'pers', 'decay']"
iter2_run0_participant32.json,cognitive_model3,390.303541097114,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Model 3: Surprise-gated learning and arbitration with anxiety modulation and forgetting.
    
    Idea
    ----
    Learning rates adapt to transition surprise: rare transitions boost learning.
    Anxiety amplifies this surprise gating (more reactive to surprising transitions).
    Stage-1 policy blends model-based and model-free values with an anxiety-sensitive weight.
    Additionally, there is anxiety-scaled forgetting of unchosen values.
    
    Parameters (all used)
    ---------------------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=Planet X, 1=Planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward outcome on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list-like of float
        [alpha_base, beta, eta_surprise, rho_bias, kappa_forget]
        Bounds:
        - alpha_base in [0,1]: baseline learning rate
        - beta in [0,10]: inverse temperature
        - eta_surprise in [0,1]: strength of surprise-gated learning-rate boost
        - rho_bias in [0,1]: modulates MB arbitration and transition-credit asymmetry
        - kappa_forget in [0,1]: forgetting strength for unchosen values (scales with anxiety)
    
    Returns
    -------
    float
        Negative log-likelihood of choices.
    """"""
    alpha_base, beta, eta_surprise, rho_bias, kappa_forget = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free first-stage and second-stage values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Compute MB first-stage value from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Anxiety-weighted arbitration toward MB
        w_mb = 0.5 + 0.5 * (1.0 - st) * rho_bias
        q1_val = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy
        q1c = q1_val - np.max(q1_val)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state
        s = state[t]
        q2_state = q2[s].copy()
        q2c = q2_state - np.max(q2_state)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Determine transition surprise (relative to chosen a1 and reached s)
        # Compute probability of reached state given chosen action under fixed transitions
        p_to_s = transition_matrix[a1, s]
        surprise = 1.0 - p_to_s  # 0.3 for rare, 0.7 for common -> larger for rarer transitions

        # Surprise-gated learning rate (amplified by anxiety)
        alpha_t = alpha_base + eta_surprise * surprise * (0.5 + 0.5 * st)
        alpha_t = float(np.clip(alpha_t, 0.0, 1.0))

        # Stage 2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * delta2

        # Stage 1 MF update with eligibility-like and transition-asymmetry component
        # TD toward updated second-stage value
        td_to_q2 = q2[s, a2] - q1_mf[a1]

        # Transition asymmetry term: credit assignment differs for common vs rare
        is_common = 1.0 if p_to_s >= 0.5 else 0.0
        trans_factor = (1.0 if is_common > 0.5 else -1.0)  # +1 common, -1 rare
        asym_term = alpha_t * rho_bias * (1.0 - st) * trans_factor * delta2

        q1_mf[a1] += alpha_t * td_to_q2 + asym_term

        # Anxiety-scaled forgetting of unchosen values
        forget_rate = kappa_forget * (0.5 + 0.5 * st)
        if forget_rate > 0.0:
            # Decay all Q-values slightly toward zero, except the updated entries (to avoid double hit)
            # Stage 1: decay the unchosen action
            other_a1 = 1 - a1
            q1_mf[other_a1] *= (1.0 - 0.1 * forget_rate)
            # Stage 2: decay all except the updated state-action
            for ss in (0, 1):
                for aa in (0, 1):
                    if not (ss == s and aa == a2):
                        q2[ss, aa] *= (1.0 - 0.1 * forget_rate)

    eps_num = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps_num)) + np.sum(np.log(p_choice_2 + eps_num)))
    return float(neg_log_lik)","['alpha_base', 'beta', 'eta_surprise', 'rho_bias', 'kappa_forget']"
iter2_run0_participant34.json,cognitive_model1,429.7170670735991,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-modulated valence-asymmetric learning.
    
    Idea:
    - First-stage decisions are a weighted mixture of model-based (MB) and model-free (MF) values.
    - Second-stage values are learned model-free (per state and alien).
    - Learning is valence-asymmetric (different rates for positive vs. negative prediction errors).
    - Anxiety increases learning from negative prediction errors (threat-sensitive learning) and
      slightly dampens learning from positive prediction errors.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; aliens) for each trial.
    reward : array-like of float
        Reward received on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : list/tuple of 5 floats
        [alpha_pos, beta, alpha_neg, k_anx_valence, w_mb]
        - alpha_pos in [0,1]: base learning rate for positive prediction errors.
        - beta in [0,10]: inverse temperature for softmax choice.
        - alpha_neg in [0,1]: base learning rate for negative prediction errors.
        - k_anx_valence in [0,1]: how strongly anxiety boosts negative learning and dampens positive.
        - w_mb in [0,1]: base weight for model-based influence at first stage.
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_pos, beta, alpha_neg, k_anx_valence, w_mb = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective valence learning rates shaped by anxiety
    # - Anxiety increases sensitivity to negative PEs and slightly reduces positive PE learning
    alpha_pos_eff = alpha_pos * (1.0 - 0.5 * k_anx_valence * stai_val)
    alpha_neg_eff = alpha_neg * (1.0 + 1.0 * k_anx_valence * stai_val)
    # clip to [0,1]
    alpha_pos_eff = max(0.0, min(1.0, alpha_pos_eff))
    alpha_neg_eff = max(0.0, min(1.0, alpha_neg_eff))

    # MB weight with a mild anxiety-driven reduction (threat narrows planning)
    w_mb_eff = w_mb * (1.0 - 0.5 * k_anx_valence * stai_val)
    w_mb_eff = max(0.0, min(1.0, w_mb_eff))

    # Fixed transition structure (commonly A->X, U->Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Prob storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)        # MF values for first-stage choices A/U
    q_stage2 = np.zeros((2, 2))      # MF values for aliens within each state

    for t in range(n_trials):
        # Compute model-based first-stage values as expectation over next-state max Q2
        max_q2 = np.max(q_stage2, axis=1)       # length 2: value of each state if optimal at stage 2
        q_stage1_mb = transition_matrix @ max_q2

        # Mix MF and MB for first-stage policy
        q1_mix = (1.0 - w_mb_eff) * q_stage1_mf + w_mb_eff * q_stage1_mb

        # First-stage policy
        logits1 = q1_mix - np.max(q1_mix)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (state-dependent softmax on Q2)
        s = state[t]
        logits2 = q_stage2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update with valence-asymmetric, anxiety-modulated learning rates
        pe2 = r - q_stage2[s, a2]
        if pe2 >= 0.0:
            q_stage2[s, a2] += alpha_pos_eff * pe2
        else:
            q_stage2[s, a2] += alpha_neg_eff * pe2

        # Stage-1 MF bootstrapping from updated stage-2 value
        boot = q_stage2[s, a2]
        pe1 = boot - q_stage1_mf[a1]
        if pe1 >= 0.0:
            q_stage1_mf[a1] += alpha_pos_eff * pe1
        else:
            q_stage1_mf[a1] += alpha_neg_eff * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_pos', 'beta', 'alpha_neg', 'k_anx_valence', 'w_mb']"
iter2_run0_participant34.json,cognitive_model2,432.4924309676929,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-adaptive exploration with anxiety-gated temperature control (pure MF).
    
    Idea:
    - The agent learns model-free values at both stages.
    - It tracks expected uncertainty (absolute PE) per second-stage option via an EWMA (nu).
    - Anxiety amplifies the influence of uncertainty on exploration (reducing beta when uncertainty is high).
    - First-stage temperature is reduced based on the expected uncertainty of the next-state options
      under the fixed transition model; second-stage temperature is reduced based on current-state uncertainty.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) per trial.
    reward : array-like of float
        Reward received per trial.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : list/tuple of 4 floats
        [alpha, beta, nu, k_anx_temp]
        - alpha in [0,1]: learning rate for MF values (both stages).
        - beta in [0,10]: base inverse temperature.
        - nu in [0,1]: smoothing for uncertainty tracker (EWMA of |PE|).
        - k_anx_temp in [0,1]: scales how much anxiety converts uncertainty into extra exploration.
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, nu, k_anx_temp = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition model used only to compute expected uncertainty at stage 1
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Prob storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Uncertainty estimates per second-stage action: EWMA of absolute PEs
    u2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Compute expected second-stage uncertainty for each first-stage action.
        # For each state, consider the more valuable (exploited) alien's uncertainty.
        best_idx = np.argmax(q2, axis=1)              # length 2: best alien per state
        u_state_best = u2[np.arange(2), best_idx]     # uncertainty tied to best alien in each state
        u_exp = T @ u_state_best                      # expected uncertainty if choosing A or U

        # Anxiety-gated temperature reduction due to uncertainty
        # beta_eff1[a] = beta / (1 + k * stai * u_exp[a])
        beta_eff1_vec = beta / (1.0 + k_anx_temp * stai_val * (u_exp + 1e-12))

        # First-stage policy with action-specific beta
        # Implement by scaling logits per action: softmax with heterogeneous temperatures
        # We approximate by computing per-action logits scaled by its beta_eff.
        logits1_raw = q1 - np.max(q1)
        scaled1 = np.array([beta_eff1_vec[0] * logits1_raw[0],
                            beta_eff1_vec[1] * logits1_raw[1]])
        # Stabilize
        scaled1 = scaled1 - np.max(scaled1)
        probs1 = np.exp(scaled1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy: beta reduced by current state's uncertainty
        s = state[t]
        u_here = u2[s].copy()
        # Use the chosen-action's uncertainty influence on beta by proxy through max u_here
        u_scale = np.max(u_here) if np.isfinite(np.max(u_here)) else 0.0
        beta_eff2 = beta / (1.0 + k_anx_temp * stai_val * (u_scale + 1e-12))

        logits2 = q2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta_eff2 * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning at second stage
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2
        # Update uncertainty tracker as EWMA of absolute PEs
        u2[s, a2] = (1.0 - nu) * u2[s, a2] + nu * abs(pe2)

        # Back up to first-stage MF value
        boot = q2[s, a2]
        pe1 = boot - q1[a1]
        q1[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'nu', 'k_anx_temp']"
iter2_run0_participant34.json,cognitive_model3,463.2782280640648,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned transitions with anxiety-amplified rare-transition aversion (pure MB at stage 1).
    
    Idea:
    - Learn first-stage transition probabilities from experience.
    - Use learned transitions to compute model-based first-stage values (expected max Q2).
    - Apply a bias against repeating the previous first-stage action if the last transition
      was subjectively 'rare' under the learned model; anxiety amplifies this aversion (threat bias).
    - Second-stage values learned model-free.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) per trial.
    reward : array-like of float
        Reward obtained per trial.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : list/tuple of 5 floats
        [alpha_T, beta, rho, k_anx_threat, alpha_Q]
        - alpha_T in [0,1]: learning rate for transition probabilities.
        - beta in [0,10]: inverse temperature.
        - rho in [0,1]: base magnitude of rare-transition aversion bias on first-stage logits.
        - k_anx_threat in [0,1]: scales how much anxiety increases rare-transition aversion and transition learning.
        - alpha_Q in [0,1]: learning rate for second-stage values and first-stage bootstrapping.
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_T, beta, rho, k_anx_threat, alpha_Q = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective parameters modulated by anxiety
    rho_eff = rho * (1.0 + k_anx_threat * stai_val)
    rho_eff = max(0.0, min(1.0, rho_eff))
    alpha_T_eff = alpha_T * (1.0 + 0.5 * k_anx_threat * stai_val)
    alpha_T_eff = max(0.0, min(1.0, alpha_T_eff))

    # Initialize learned transition probabilities: each row sums to 1
    T = np.ones((2, 2)) * 0.5

    # Values
    q2 = np.zeros((2, 2))
    q1_mb = np.zeros(2)  # computed each trial from T and q2

    # Likelihood arrays
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous action and whether last transition was rare
    prev_a1 = None
    prev_rare = False

    for t in range(n_trials):
        # Compute MB first-stage values from learned transitions
        max_q2 = np.max(q2, axis=1)   # best alien per planet
        q1_mb = T @ max_q2

        # Construct a bias vector penalizing the previous action if last transition was rare
        bias = np.zeros(2)
        if prev_a1 is not None and prev_rare:
            bias[prev_a1] -= rho_eff

        # First-stage policy (MB + bias)
        logits1 = q1_mb + bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = q2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and learn Q2
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_Q * pe2

        # Update transitions for the chosen action based on observed next state
        # Target is one-hot on observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        # Row update: T[a1] += alpha * (target - T[a1]); then renormalize (still sums to 1)
        T[a1] = T[a1] + alpha_T_eff * (target - T[a1])
        # Numerical guard to keep probabilities in [1e-6, 1-1e-6]
        T[a1] = np.clip(T[a1], 1e-6, 1.0 - 1e-6)
        T[a1] = T[a1] / np.sum(T[a1])

        # Determine whether the observed transition was rare under current T BEFORE update for next-trial bias
        # Use the pre-update T for rarity check: we approximate by reconstructing prob from post-update inverse step.
        # Instead, for clarity, compute rarity using the probability after update but before next trial:
        # If probability of observed s under chosen a1 is below 0.5, we mark it as rare (subjective criterion).
        prev_rare = (T[a1, s] < 0.5)
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha_T', 'beta', 'rho', 'k_anx_threat', 'alpha_Q']"
iter2_run0_participant35.json,cognitive_model1,322.7769397945243,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated hybrid with eligibility trace and lapse.

    This model blends model-based (MB) and model-free (MF) control at the first stage,
    uses an eligibility trace to propagate second-stage prediction errors back to the
    first stage, and includes an anxiety-dependent lapse rate and temperature.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1) in the reached state
    - reward: array-like of floats in [0,1], reward outcome on each trial
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_r: base learning rate for second-stage MF values in [0,1]
        beta: inverse temperature for both stages in [0,10]
        omega_base: base model-based weight for stage-1 in [0,1]
        lam_base: base eligibility trace from stage-2 to stage-1 in [0,1]
        eps_base: base lapse rate (random choice) in [0,1]

    Bounds
    - alpha_r, omega_base, lam_base, eps_base in [0,1]
    - beta in [0,10]

    Anxiety usage
    - MB weight decreases with anxiety: omega_eff = clip(omega_base * (1 - 0.6*stai))
    - Eligibility increases with anxiety: lambda_eff = clip(lam_base * (1 + 0.5*stai))
    - Lapse increases with anxiety: eps_eff = clip(eps_base * stai)
    - Temperature decreases with anxiety: beta_eff = beta * (1 - 0.5*stai)
    - Learning rate at stage-2 increases with anxiety: alpha2_eff = clip(alpha_r * (1 + 0.3*stai))
    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_r, beta, omega_base, lam_base, eps_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)        # stage-1 MF values for actions A/U
    q2 = np.zeros((2, 2))      # stage-2 MF values for states X/Y and actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    omega_eff = min(1.0, max(0.0, omega_base * (1.0 - 0.6 * stai)))
    lambda_eff = min(1.0, max(0.0, lam_base * (1.0 + 0.5 * stai)))
    eps_eff = min(1.0, max(0.0, eps_base * stai))
    beta_eff = max(0.0, beta * (1.0 - 0.5 * stai))
    alpha2_eff = min(1.0, max(0.0, alpha_r * (1.0 + 0.3 * stai)))

    for t in range(n_trials):

        # Model-based first-stage action values from current second-stage values
        max_q2 = np.max(q2, axis=1)    # best value in each state
        q1_mb = T @ max_q2             # action values: expect next-state value via transition

        # Combine MB and MF for stage-1 decision
        q1_comb = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # Stage-1 policy with softmax and lapse
        centered_q1 = q1_comb - np.max(q1_comb)
        logits1 = beta_eff * centered_q1
        exp1 = np.exp(logits1)
        soft1 = exp1 / np.sum(exp1)
        probs1 = (1.0 - eps_eff) * soft1 + eps_eff * 0.5

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with softmax and lapse in reached state
        s = int(state[t])
        q2_s = q2[s].copy()
        centered_q2 = q2_s - np.max(q2_s)
        logits2 = beta_eff * centered_q2
        exp2 = np.exp(logits2)
        soft2 = exp2 / np.sum(exp2)
        probs2 = (1.0 - eps_eff) * soft2 + eps_eff * 0.5

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2_eff * delta2

        # Propagate second-stage PE back to stage-1 via eligibility trace
        q1_mf[a1] += alpha2_eff * lambda_eff * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'omega_base', 'lam_base', 'eps_base']"
iter2_run0_participant35.json,cognitive_model2,329.99148269453144,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-driven first-stage bias with anxiety-amplified transition sensitivity.

    This model is primarily model-free for values but augments first-stage choice
    with a dynamic surprise bonus that tracks how surprising recent transitions were.
    Rare transitions produce a positive surprise signal; common transitions produce
    a negative signal. The bonus is action-specific and decays over time.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha2: learning rate for second-stage MF values in [0,1]
        beta: inverse temperature for both stages in [0,10]
        eta_surprise: base magnitude of surprise bonus update in [0,1]
        gamma_decay: decay rate for surprise bonus in [0,1]
        xi_stick: base first-stage perseveration weight in [0,1]

    Bounds
    - alpha2, eta_surprise, gamma_decay, xi_stick in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Surprise sensitivity increases with anxiety: eta_eff = clip(eta_surprise * (1 + stai))
    - Perseveration increases with anxiety: xi_eff = xi_stick * (1 + 0.5*stai)
    - Temperature decreases with anxiety at stage-1: beta1 = beta * (1 - 0.4*stai); stage-2 uses beta unchanged
    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha2, beta, eta_surprise, gamma_decay, xi_stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values (MF)
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Surprise bonus per first-stage action
    bonus = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eta_eff = min(1.0, max(0.0, eta_surprise * (1.0 + stai)))
    xi_eff = xi_stick * (1.0 + 0.5 * stai)
    beta1 = max(0.0, beta * (1.0 - 0.4 * stai))
    beta2 = beta

    prev_a1 = -1

    for t in range(n_trials):

        # First-stage policy: MF values + surprise bonus + perseveration
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += xi_eff

        q1_decision = q1_mf + bonus + bias1
        centered_q1 = q1_decision - np.max(q1_decision)
        probs1 = np.exp(beta1 * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy: MF softmax
        s = int(state[t])
        centered_q2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta2 * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Reward learning at stage-2
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Update first-stage MF towards second-stage chosen value (bootstrapped)
        target1 = q2[s, a2]
        q1_mf[a1] += alpha2 * (target1 - q1_mf[a1])

        # Transition surprise update for the chosen action
        # Determine whether the observed transition was rare (prob < 0.5) or common
        p_to_s = T[a1, s]
        is_rare = 1.0 if p_to_s < 0.5 else 0.0
        # Surprise signal: +1 for rare, -1 for common
        surprise_signal = 2.0 * is_rare - 1.0

        # Decay bonus for both actions
        bonus = (1.0 - gamma_decay) * bonus
        # Update bonus for chosen action toward surprise signal
        bonus[a1] += gamma_decay * eta_eff * surprise_signal

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'eta_surprise', 'gamma_decay', 'xi_stick']"
iter2_run0_participant35.json,cognitive_model3,319.0001407701328,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric learning with anxiety-modulated loss aversion and planet preference.

    This model uses asymmetric learning rates for positive vs. negative prediction errors
    at the second stage (prospect-like), propagates value to the first stage via MF bootstrapping,
    and blends in a small model-based component whose weight is reduced by anxiety. An anxiety-
    modulated planet preference bias pushes choices toward the spaceship that is more likely
    to reach the currently preferred planet.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_base: base learning rate in [0,1] (split into pos/neg by anxiety and zeta)
        beta: inverse temperature for both stages in [0,10]
        zeta_loss: scales how much anxiety increases loss-weighted learning in [0,1]
        chi_pref: base magnitude of planet preference bias in [0,1]
        rho_mb: base model-based mixture weight in [0,1]

    Bounds
    - alpha_base, zeta_loss, chi_pref, rho_mb in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Asymmetric learning:
        alpha_pos = clip(alpha_base * (1 - 0.3*stai))
        alpha_neg = clip(alpha_base * (1 + zeta_loss*stai))
      Negative PEs are learned faster as anxiety and zeta_loss increase.
    - Model-based weight decreases with anxiety: w_mb = clip(rho_mb * (1 - 0.5*stai))
    - Planet preference bias increases with anxiety and targets a planet:
        pref_strength = chi_pref * (2*stai - 1)  (negative favors Y when stai<0.5, positive favors X when stai>0.5)
        state_desirability = [pref_strength, -pref_strength] for [X, Y]
        action_bias = T @ state_desirability added to stage-1 decision values.
    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha_base, beta, zeta_loss, chi_pref, rho_mb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    alpha_pos = min(1.0, max(0.0, alpha_base * (1.0 - 0.3 * stai)))
    alpha_neg = min(1.0, max(0.0, alpha_base * (1.0 + zeta_loss * stai)))
    w_mb = min(1.0, max(0.0, rho_mb * (1.0 - 0.5 * stai)))

    # Planet preference bias mapped through transitions into action space
    pref_strength = chi_pref * (2.0 * stai - 1.0)   # [-chi_pref, +chi_pref]
    desirability = np.array([pref_strength, -pref_strength])  # [X, Y]
    action_pref_bias = T @ desirability  # bias for actions A/U

    for t in range(n_trials):

        # Model-based first-stage values from second-stage
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MF and MB and add planet preference bias
        q1_decision = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + action_pref_bias

        # Stage-1 policy
        centered_q1 = q1_decision - np.max(q1_decision)
        probs1 = np.exp(beta * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        centered_q2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning with asymmetric rates at stage-2
        r = reward[t]
        pe2 = r - q2[s, a2]
        alpha2_eff = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2_eff * pe2

        # Propagate updated second-stage value to stage-1 MF
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        alpha1_eff = alpha_pos if pe1 >= 0.0 else alpha_neg
        q1_mf[a1] += alpha1_eff * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_base', 'beta', 'zeta_loss', 'chi_pref', 'rho_mb']"
iter2_run0_participant36.json,cognitive_model1,547.2388517316143,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 1: Pure model-based controller with learned transitions, anxiety-weighted exploration, and lapse.
    
    Overview
    --------
    This model learns second-stage action values (Q2) from reward and learns the state transition
    probabilities T(s | a1). First-stage choices are purely model-based: they value each spaceship
    by the expected value of the best alien on the planet it leads to under the learned transitions.
    Anxiety increases exploration (reduces effective beta) and increases lapse probability.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher scores increase exploration and lapses.
    model_parameters : list or array
        [alpha_r, alpha_T, beta, gamma_anx, lapse]
        - alpha_r in [0,1]: learning rate for second-stage reward values (Q2).
        - alpha_T in [0,1]: learning rate for transition probabilities T(s | a1).
        - beta in [0,10]: inverse temperature.
        - gamma_anx in [0,1]: scales how strongly anxiety reduces beta (exploration gain).
        - lapse in [0,1]: base lapse probability mixed with uniform choice; increased by anxiety.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_r, alpha_T, beta, gamma_anx, lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T(s | a1). Start at symmetric prior (0.5/0.5).
    T = np.ones((2, 2)) * 0.5  # rows: action (A,U), cols: next state (X,Y)
    # Initialize Q-values at second stage
    Q2 = np.zeros((2, 2))  # state x alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective parameters modulated by anxiety
    beta_eff = beta * (1.0 - gamma_anx * stai)  # higher anxiety -> lower beta (more exploration)
    beta_eff = max(beta_eff, 1e-6)              # avoid exactly zero
    lapse_eff = lapse * (0.5 + 0.5 * stai)      # higher anxiety -> more lapses
    alpha_T_eff = alpha_T * (0.5 + 0.5 * stai)  # anxiety accelerates learning of transitions

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # First-stage policy: model-based evaluation using learned transitions
        max_Q2 = np.max(Q2, axis=1)  # value of best alien in each state: [V(X), V(Y)]
        Q1_mb = np.array([
            T[0, 0] * max_Q2[0] + T[0, 1] * max_Q2[1],  # value of spaceship A
            T[1, 0] * max_Q2[0] + T[1, 1] * max_Q2[1],  # value of spaceship U
        ])
        logits1 = beta_eff * (Q1_mb - np.max(Q1_mb))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        # Apply lapse mixture with uniform choice over 2 actions
        probs1 = (1.0 - lapse_eff) * probs1 + lapse_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Second-stage policy: softmax over Q2 in the visited state
        logits2 = beta_eff * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - lapse_eff) * probs2 + lapse_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Learn transitions T(s | a1) by simple exponential moving average (one-hot target)
        # Target vector: 1 for observed s, 0 for other
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_T_eff * (target - T[a1, sp])
        # Keep rows normalized (they will remain normalized under this update, but guard numerical drift)
        T[a1] = T[a1] / np.sum(T[a1])

        # Learn second-stage Q-values from reward
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * pe2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)

","['alpha_r', 'alpha_T', 'beta', 'gamma_anx', 'lapse']"
iter2_run0_participant36.json,cognitive_model2,468.76394656241075,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 2: Hybrid MB-MF with counterfactual learning, anxiety-reduced model-based weight, and forgetting.
    
    Overview
    --------
    The model learns:
      - Q2(s, a2) from reward for the chosen alien (chosen learning) and also updates the unchosen alien
        in the same state via counterfactual learning.
      - A model-free first-stage value Q1_mf via a TD backup from Q2(s, a2).
      - A fixed transition model (common A->X, U->Y with 0.7/0.3) for model-based evaluation.
    First-stage decisions combine model-based and model-free values with weight w. Anxiety reduces
    the effective model-based weight and increases forgetting (decay to a neutral prior).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher scores reduce MB weight and increase forgetting.
    model_parameters : list or array
        [alpha_chosen, alpha_unchosen, beta, w_hybrid, f_forget]
        - alpha_chosen in [0,1]: learning rate for chosen second-stage action.
        - alpha_unchosen in [0,1]: counterfactual learning rate for unchosen second-stage action (same state).
        - beta in [0,10]: inverse temperature for both stages.
        - w_hybrid in [0,1]: baseline weight on model-based values at stage 1.
        - f_forget in [0,1]: forgetting rate toward neutral 0.5 for Q2 and toward 0 for Q1_mf.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_c, alpha_u, beta, w_hybrid, f_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (common=0.7)
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    Q2 = np.ones((2, 2)) * 0.5  # start at neutral 0.5 to align with forgetting anchor
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects: reduce MB weight; increase forgetting; reduce counterfactual learning
    w_eff = np.clip(w_hybrid * (1.0 - 0.5 * stai), 0.0, 1.0)
    f_eff = f_forget * (0.5 + 0.5 * stai)
    alpha_u_eff = alpha_u * (1.0 - 0.5 * stai)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Forgetting toward anchors before updates
        Q2 = (1.0 - f_eff) * Q2 + f_eff * 0.5
        Q1_mf = (1.0 - f_eff) * Q1_mf  # forget toward 0

        # Model-based evaluation for stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2  # expected value under fixed transitions

        # Hybrid Q for stage 1
        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf

        # Stage 1 policy
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Q2 chosen update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_c * pe2

        # Q2 unchosen counterfactual update within same state, using same outcome
        a2_unchosen = 1 - a2
        pe2_cf = r - Q2[s, a2_unchosen]
        Q2[s, a2_unchosen] += alpha_u_eff * pe2_cf

        # Model-free Q1 update via TD backup from observed Q2
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_c * pe1

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)

","['alpha_c', 'alpha_u', 'beta', 'w_hybrid', 'f_forget']"
iter2_run0_participant36.json,cognitive_model3,483.42828066087776,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 3: Rare-transition sensitivity with anxiety-modulated noise, internal transition belief, and repetition bias.
    
    Overview
    --------
    This model combines:
      - A simple model-free learner at both stages (Q2 learned from reward; Q1_mf backed up from Q2).
      - A model-based planner for stage 1 using an internal belief about common transition probability t_common.
      - A repetition (stickiness) bias that promotes repeating the last action, scaled by anxiety.
      - Rare-transition sensitivity: after rare transitions, the second-stage choice policy becomes noisier
        with strength set by zeta_rare and amplified by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher scores intensify rare-transition noise and stickiness.
    model_parameters : list or array
        [alpha, beta, zeta_rare, t_common, rep]
        - alpha in [0,1]: learning rate for Q updates (both stages).
        - beta in [0,10]: base inverse temperature.
        - zeta_rare in [0,1]: strength of beta reduction after rare transitions.
        - t_common in [0,1]: internal belief that chosen spaceship makes a common transition (A->X, U->Y).
        - rep in [0,1]: baseline repetition bias added to the last chosen action's logit.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, zeta_rare, t_common, rep = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_a2 = [None, None]  # track per-state repetition

    # Anxiety-scaled repetition bias
    rep_eff = rep * (0.5 + 0.5 * stai)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q1 using internal transition belief t_common
        # Belief: A commonly -> X, U commonly -> Y with prob t_common
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = np.array([
            t_common * max_Q2[0] + (1.0 - t_common) * max_Q2[1],  # spaceship A
            (1.0 - t_common) * max_Q2[0] + t_common * max_Q2[1],  # spaceship U
        ])
        # Hybrid with simple MF Q1
        Q1 = 0.5 * (Q1_mb + Q1_mf)  # fixed 0.5-0.5 mixture to keep parameter budget; MF updated below.

        # Stage 1 policy with repetition bias
        logits1 = beta * Q1
        if last_a1 is not None:
            logits1[last_a1] += rep_eff
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Determine if current transition was common or rare under task structure
        was_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Rare-transition sensitivity: reduce beta at stage 2 when rare occurs
        beta2 = beta * (1.0 - zeta_rare * stai * (1 - was_common))
        beta2 = max(beta2, 1e-6)

        # Stage 2 policy with per-state repetition bias
        logits2 = beta2 * Q2[s]
        if last_a2[s] is not None:
            logits2[last_a2[s]] += rep_eff
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        # Update repetition trackers
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)","['alpha', 'beta', 'zeta_rare', 't_common', 'rep']"
iter2_run0_participant39.json,cognitive_model1,491.45356452859505,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned-transition hybrid RL with anxiety-modulated planning and transition learning.

    Overview:
    - Learns second-stage (planet-alien) values model-free with forgetting.
    - Learns the first-stage transition model (spaceship -> planet) from experience.
    - Forms model-based first-stage values from the learned transition model and combines
      them with model-free first-stage values via a hybrid weight.
    - Anxiety (stai) decreases planning weight and increases transition learning and forgetting.

    Parameters (bounds):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage state indices (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; e.g., aliens)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), trait/state anxiety in [0,1]
    - model_parameters: iterable of 5 values:
        eta_v        [0,1]   base learning rate for second-stage Q-values
        beta         [0,10]  inverse temperature for both stages
        psi_T_base   [0,1]   base learning rate for transition model (spaceship->planet)
        mix_mb_base  [0,1]   baseline weight on model-based control at stage 1
        phi_decay    [0,1]   forgetting rate for second-stage values per trial

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    eta_v, beta, psi_T_base, mix_mb_base, phi_decay = model_parameters
    n_trials = len(action_1)

    # Storage for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Learned transition model (rows: spaceship A/U; cols: planet X/Y)
    T = np.full((2, 2), 0.5)

    # Model-free values
    q1_mf = np.zeros(2)        # stage-1 MF values for A/U
    q2_mf = np.zeros((2, 2))   # stage-2 MF values per planet and alien

    eps = 1e-10

    for t in range(n_trials):
        st = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        stai_t = float(stai[min(t, len(stai) - 1)])

        # Anxiety-modulated parameters
        # Higher anxiety -> less planning, faster transition learning, stronger forgetting
        w_mb = np.clip(mix_mb_base * (1.0 - stai_t), 0.0, 1.0)
        psi_T = np.clip(psi_T_base * (1.0 + 0.5 * stai_t), 0.0, 1.0)
        phi_eff = np.clip(phi_decay * (1.0 + 0.5 * stai_t), 0.0, 1.0)

        # Decay second-stage values slightly each trial to track drifting rewards
        q2_mf = (1.0 - phi_eff) * q2_mf

        # Model-based first-stage values from learned transitions
        max_q2 = np.max(q2_mf, axis=1)         # size 2: best alien on each planet
        q1_mb = T @ max_q2                     # expected value of spaceships

        # Hybrid value for first-stage choice
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage policy
        q1_center = q1 - np.max(q1)
        p1 = np.exp(beta * q1_center)
        p1 /= (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        q2 = q2_mf[st]
        q2_center = q2 - np.max(q2)
        p2 = np.exp(beta * q2_center)
        p2 /= (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Transition learning: update row of chosen spaceship toward observed planet
        T[a1] = (1.0 - psi_T) * T[a1]
        T[a1, st] += psi_T
        # Re-normalize row (numerical safety)
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Second-stage MF update
        pe2 = r - q2_mf[st, a2]
        q2_mf[st, a2] += eta_v * pe2

        # Stage-1 MF TD backup with the realized second-stage value
        td_target1 = q2_mf[st, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += eta_v * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta_v', 'beta', 'psi_T_base', 'mix_mb_base', 'phi_decay']"
iter2_run0_participant39.json,cognitive_model2,472.0191808686935,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Valence-asymmetric SARSA with anxiety-modulated loss aversion and stage-2 noise.

    Overview:
    - Purely model-free SARSA-style learning at both stages.
    - Separate learning rates for positive vs negative prediction errors.
    - Utility is asymmetric: losses (omissions) are weighted more heavily than gains,
      with loss aversion increasing with anxiety.
    - Anxiety also increases decision noise specifically at stage 2.

    Parameters (bounds):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety in [0,1]
    - model_parameters: iterable of 5 values:
        a_pos       [0,1]   learning rate when prediction error is positive
        a_neg       [0,1]   learning rate when prediction error is negative
        beta        [0,10]  baseline inverse temperature
        pav_base    [0,1]   Pavlovian approach bias toward repeating rewarded a2 in same state
        temp2_scale [0,1]   anxiety scaling that reduces stage-2 beta

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    a_pos, a_neg, beta, pav_base, temp2_scale = model_parameters
    n_trials = len(action_1)

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Pavlovian memory per state: last action and whether it was rewarded
    last_a2 = np.full(2, -1, dtype=int)
    last_rew = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        stai_t = float(stai[min(t, len(stai) - 1)])

        # Anxiety-modulated stage-2 temperature (higher anxiety -> lower beta2)
        beta2 = beta * max(0.0, 1.0 - temp2_scale * stai_t)

        # Stage-1 policy (no explicit MB; purely MF)
        q1_c = q1 - np.max(q1)
        p1 = np.exp(beta * q1_c)
        p1 /= (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 Pavlovian approach bias: repeat rewarded action in this state
        pav_eff = pav_base * (1.0 + stai_t)  # anxiety increases approach bias to recent reward
        pav_bias = np.zeros(2)
        if last_a2[s] != -1:
            sign = 1.0 if last_rew[s] > 0.5 else -1.0
            pav_bias[last_a2[s]] = pav_eff * sign

        q2_pref = q2[s] + pav_bias
        q2_c = q2_pref - np.max(q2_pref)
        p2 = np.exp(beta2 * q2_c)
        p2 /= (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Utility with anxiety-modulated loss aversion
        # Treat r in {0,1}; transform via piecewise utility
        k_loss = 1.0 + stai_t  # higher anxiety -> stronger loss weighting
        u = r if r >= q2[s, a2] else -k_loss * (q2[s, a2] - r)

        # Second-stage update (valence-asymmetric)
        pe2 = u - q2[s, a2]
        lr2 = a_pos if pe2 >= 0.0 else a_neg
        q2[s, a2] += lr2 * pe2

        # Stage-1 SARSA backup using current second-stage estimate (MF only)
        td_target1 = q2[s, a2]
        pe1 = td_target1 - q1[a1]
        lr1 = a_pos if pe1 >= 0.0 else a_neg
        q1[a1] += lr1 * pe1

        # Update Pavlovian memory for this state
        last_a2[s] = a2
        last_rew[s] = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['a_pos', 'a_neg', 'beta', 'pav_base', 'temp2_scale']"
iter2_run0_participant39.json,cognitive_model3,466.80000460350675,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-weighted learning (adaptive step size) with anxiety-scaled exploration.

    Overview:
    - Second-stage learning uses an adaptive learning rate proportional to estimated
      outcome uncertainty per (state, action). Uncertainty is updated from recent
      prediction errors and decays over time.
    - First-stage values are a hybrid of model-based (fixed transition 0.7/0.3)
      and model-free backup, both updated using the same adaptive learning rate
      derived from the realized second-stage (state, action).
    - Anxiety increases the gain on uncertainty (larger step sizes) and reduces
      stage-2 inverse temperature (more exploration under anxiety).

    Parameters (bounds):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety in [0,1]
    - model_parameters: iterable of 5 values:
        beta          [0,10] baseline inverse temperature (used at stage 1)
        xi2_base      [0,1]  scales reduction of stage-2 beta with anxiety
        kappa_u_base  [0,1]  gain mapping uncertainty to learning rate
        nu_init       [0,1]  initial uncertainty per (state, action)
        omega_decay   [0,1]  decay rate for uncertainty; higher -> tracks recent variability

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    beta, xi2_base, kappa_u_base, nu_init, omega_decay = model_parameters
    n_trials = len(action_1)

    # Fixed transition model as in task description
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and uncertainty
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    U = np.full((2, 2), nu_init)  # uncertainty per (state, action), in [0,1]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        stai_t = float(stai[min(t, len(stai) - 1)])

        # Stage-1 model-based component from fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid first-stage value: here weight is implicit via MF update; selection uses MF+MB sum
        q1 = q1_mb + q1_mf

        # First-stage softmax with baseline beta
        q1_c = q1 - np.max(q1)
        p1 = np.exp(beta * q1_c)
        p1 /= (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 softmax with anxiety-reduced beta
        beta2 = beta * max(0.0, 1.0 - xi2_base * stai_t)
        q2_c = q2[s] - np.max(q2[s])
        p2 = np.exp(beta2 * q2_c)
        p2 /= (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Adaptive learning rate from uncertainty
        kappa_eff = kappa_u_base * (1.0 + stai_t)          # anxiety increases sensitivity to uncertainty
        alpha_adapt = np.clip(kappa_eff * U[s, a2], 0.0, 1.0)

        # Second-stage update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_adapt * pe2

        # Update uncertainty from absolute PE with decay
        U[s, a2] = (1.0 - omega_decay) * U[s, a2] + omega_decay * min(1.0, abs(pe2))

        # First-stage MF backup using same adaptive step size
        td_target1 = q2[s, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha_adapt * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['beta', 'xi2_base', 'kappa_u_base', 'nu_init', 'omega_decay']"
iter2_run0_participant4.json,cognitive_model1,457.6605810769664,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated lapse, transition-contingent repetition bias, and value forgetting.

    Overview
    - Stage-2 values learned via RescorlaâWagner, with valence asymmetry induced by anxiety.
      Positive prediction errors are amplified when anxiety is high; negative ones dampened, and vice versa.
    - Stage-1 choice uses model-free values shaped by an anxiety-scaled, transition-contingent repetition bias:
      after rewarded common transitions the agent tends to repeat; after rewarded rare transitions the agent tends to switch.
    - A small lapse probability grows with anxiety, mixing softmax with uniform choice.
    - Values at both stages undergo forgetting toward 0.5 each trial.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha: [0,1]          Base learning rate for value updates.
    - beta:  [0,10]         Inverse temperature for softmax at both stages.
    - omega_trans: [0,1]    Strength of transition-contingent repetition bias at stage 1.
    - phi_lapse: [0,1]      Scales lapse probability with anxiety.
    - xi_forget: [0,1]      Per-trial forgetting rate toward 0.5.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, omega_trans, phi_lapse, xi_forget].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, omega_trans, phi_lapse, xi_forget = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)

    # Fixed transition structure (rows: action A/U; cols: state X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    q1_mf = np.zeros(2) + 0.5           # Stage-1 MF values for A/U
    q2 = np.zeros((2, 2)) + 0.5         # Stage-2 values for states X/Y and actions 0/1

    # Track likelihoods
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Transition-contingent repetition bias (based on previous trial)
    prev_a1 = None
    prev_common = None
    prev_rew = None

    # Lapse grows with anxiety
    lapse = min(0.25, phi_lapse * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Forgetting toward 0.5
        q2 = (1.0 - xi_forget) * q2 + xi_forget * 0.5
        q1_mf = (1.0 - xi_forget) * q1_mf + xi_forget * 0.5

        # Stage-1 bias term: transition-contingent repetition effect
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_common is not None and prev_rew is not None:
            # Repeat bias after rewarded common; switch bias after rewarded rare
            # Signed effect: (+) to repeat when common&rewarded, (-) when rare&rewarded; zero when no reward
            sign = (1.0 if prev_common else -1.0) * (2.0 * prev_rew - 1.0)  # in {-1, 0, +1}
            bias1[prev_a1] += omega_trans * stai * sign

        # Stage-1 policy (MF baseline + bias; we do not compute MB values here to keep mechanism distinct)
        logits1 = beta * q1_mf + bias1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p1[t] = probs1[a1]

        # Stage-2 policy (standard softmax from current state's Q)
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p2[t] = probs2[a2]

        # Stage-2 update with valence asymmetry modulated by anxiety
        pe2 = r - q2[s, a2]
        # Anxiety-induced asymmetry: amplify learning more for the valence that aligns with anxiety
        # Map stai in [0,1] to asymmetry factor in [0.5, 1.5] around 1.0
        asym = 1.0 + (stai - 0.5)  # in [0.5, 1.5]
        a_pos = alpha * asym
        a_neg = alpha * (2.0 - asym)  # mirrors so average remains ~alpha
        eff_alpha2 = a_pos if pe2 >= 0 else a_neg
        q2[s, a2] += eff_alpha2 * pe2

        # Stage-1 MF credit assignment from stage-2 PE (eligibility is implicit via MF only)
        q1_mf[a1] += alpha * pe2

        # Prepare previous trial markers for next trial's bias
        prev_a1 = a1
        prev_rew = r
        prev_common = T[a1, s] >= 0.5

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['alpha', 'beta', 'omega_trans', 'phi_lapse', 'xi_forget']"
iter2_run0_participant4.json,cognitive_model2,417.50983378365356,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-gated exploration and choice-kernel biases, with anxiety scaling exploration.

    Overview
    - Stage-2 learning via RescorlaâWagner.
    - A running estimate of outcome volatility (squared PE) modulates exploration:
      higher volatility reduces effective beta, and this effect scales with anxiety.
    - Choice kernels (perseveration) for both stages bias toward previously chosen options.
    - Uses a small lapse that also scales with volatility and anxiety.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha: [0,1]          Learning rate for Q updates.
    - beta:  [0,10]         Base inverse temperature.
    - eta_explore: [0,1]    Strength of volatility-driven exploration (reduces beta).
    - kappa_ck: [0,1]       Choice-kernel learning rate/strength for perseveration.
    - alpha_var: [0,1]      Learning rate for updating volatility estimate from squared PE.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, eta_explore, kappa_ck, alpha_var].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, eta_explore, kappa_ck, alpha_var = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)

    # Transition matrix (used only to define common/rare if needed for kernels â here not used)
    # Kept for completeness; model policy does not plan over transitions.
    # T = np.array([[0.7, 0.3],
    #               [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2) + 0.5
    q2 = np.zeros((2, 2)) + 0.5

    # Choice kernels for perseveration (stage 1 and per state at stage 2)
    ck1 = np.zeros(2)
    ck2 = np.zeros((2, 2))

    # Volatility estimate (scalar), initialized small
    vol = 0.05

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Effective beta reduced by volatility and anxiety
        # vol in [0, ~1]; clamp for safety
        v = max(0.0, min(1.0, vol))
        beta_t = beta * (1.0 - eta_explore * stai * v)
        beta_t = max(1e-3, beta_t)

        # Lapse also grows with volatility and anxiety (small)
        lapse = min(0.2, 0.1 * stai * v)

        # Stage-1 policy: MF values + choice kernel bias
        logits1 = beta_t * q1_mf + ck1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p1[t] = probs1[a1]

        # Stage-2 policy: state-specific values + choice kernel
        logits2 = beta_t * q2[s] + ck2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p2[t] = probs2[a2]

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Backpropagate to stage-1 MF value of chosen action
        q1_mf[a1] += alpha * pe2

        # Update volatility estimate from squared PE
        vol = (1.0 - alpha_var) * vol + alpha_var * (pe2 * pe2)

        # Update choice kernels (exponential recency)
        ck1 = (1.0 - kappa_ck) * ck1
        ck1[a1] += kappa_ck

        ck2[s] = (1.0 - kappa_ck) * ck2[s]
        ck2[s, a2] += kappa_ck

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['alpha', 'beta', 'eta_explore', 'kappa_ck', 'alpha_var']"
iter2_run0_participant40.json,cognitive_model1,471.7912435972711,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and first-stage stickiness.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien on visited planet).
    reward : array-like of float
        Reward (gold coins), can be negative, zero, or positive.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha2, beta, omega0, xi, kappa1)
        - alpha2 in [0,1]: learning rate for second-stage Q-values and MF bootstrapping to stage 1.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega0 in [0,1]: baseline MB weight in first-stage arbitration.
        - xi in [0,1]: strength with which anxiety shifts MB weight (higher anxiety reduces MB if xi>0).
                        Effective weight: w_eff = clip(omega0 + xi*(0.5 - stai), 0, 1).
        - kappa1 in [0,1]: first-stage choice stickiness; bias to repeat previous spaceship.
                           Stickiness is dampened by anxiety: bias = kappa1 * (1 - stai).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.

    Notes
    -----
    - Uses fixed transition structure (common = 0.7): A->X, U->Y.
    - Stage-1 value is a convex combination of model-based (MB) and model-free (MF) values.
    - Anxiety modulates the arbitration weight (w_eff) and reduces stickiness bias.
    """"""
    alpha2, beta, omega0, xi, kappa1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition matrix: rows=actions(A,U), cols=states(X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values: Q2[state, action]
    Q2 = np.zeros((2, 2))
    # Stage-1 model-free Q-values
    Q1_MF = np.zeros(2)

    prev_a1 = None

    # Anxiety-modulated arbitration weight and stickiness scale
    w_eff = omega0 + xi * (0.5 - stai_val)
    w_eff = 0.0 if w_eff < 0.0 else (1.0 if w_eff > 1.0 else w_eff)
    stickiness_scale = 1.0 - stai_val  # higher anxiety -> less stickiness

    for t in range(n_trials):
        # MB component: expected max value after transition
        max_Q2 = np.max(Q2, axis=1)  # per state
        Q1_MB = T @ max_Q2

        # Combine MB and MF
        Q1 = w_eff * Q1_MB + (1.0 - w_eff) * Q1_MF

        # Stickiness bias for stage-1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = kappa1 * stickiness_scale

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Stage-1 MF bootstrapping from experienced second-stage action value
        target1 = Q2[s2, a2]
        delta1 = target1 - Q1_MF[a1]
        Q1_MF[a1] += alpha2 * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'omega0', 'xi', 'kappa1']"
iter2_run0_participant40.json,cognitive_model2,547.6986165657328,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-bonus exploration with anxiety-modulated bonus strength (both stages).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien on visited planet).
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha2, beta, phi, z)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - phi in [0,1]: base strength of an optimism/uncertainty bonus.
        - z in [0,1]: anxiety modulation of bonus; effective scale:
                      scale = clip(1 + z*(0.5 - stai), 0, 2), bonus = phi * scale / sqrt(N + 1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.

    Notes
    -----
    - Uses fixed transitions (common = 0.7).
    - Maintains visit counts N[state, action]; rarely visited options carry a larger bonus.
    - Bonus augments action values at stage 2 and propagates to stage 1 via the MB backup.
    - Higher anxiety (stai > 0.5) reduces the bonus when z > 0; lower anxiety increases it.
    """"""
    alpha2, beta, phi, z = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))
    N = np.zeros((2, 2))  # visit counts for uncertainty bonus

    # Anxiety-modulated bonus scale
    scale = 1.0 + z * (0.5 - stai_val)
    if scale < 0.0:
        scale = 0.0
    if scale > 2.0:
        scale = 2.0
    phi_eff = phi * scale

    for t in range(n_trials):
        # Compute per-option bonus from visit counts for both states
        bonus_all = np.zeros((2, 2))
        for s in range(2):
            for a in range(2):
                bonus_all[s, a] = phi_eff / np.sqrt(N[s, a] + 1.0)

        # Stage-1 MB values use augmented second-stage utilities (Q2 + bonus)
        max_aug = np.max(Q2 + bonus_all, axis=1)  # per state
        Q1_MB = T @ max_aug

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with bonus
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * (Q2[s2] + bonus_all[s2])
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        N[s2, a2] += 1.0
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'phi', 'z']"
iter2_run0_participant40.json,cognitive_model3,inf,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-like transition learning with anxiety-modulated discount and learning rate.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien on visited planet).
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha2, beta, gamma0, zeta, eta_sr)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - gamma0 in [0,1]: baseline discount controlling transition sharpening.
        - zeta in [0,1]: anxiety modulation of discount:
                         gamma_eff = clip(gamma0 * (1 - zeta * stai), 0, 1).
                         Lower gamma_eff (e.g., high anxiety with zeta>0) sharpens reliance
                         on the most likely transition.
        - eta_sr in [0,1]: learning rate for the action-to-state transition row (SR-like update).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.

    Notes
    -----
    - Learns a per-action transition row T_sr[a,:] online (initialized uniform).
    - First-stage MB value uses a sharpened transition distribution:
          T_used[a, s] â T_sr[a, s] ** k, where k = 1 + 4 * (1 - gamma_eff).
      Thus, smaller gamma_eff (e.g., higher anxiety if zeta>0) increases k and
      emphasizes the modal transition.
    - Stage-2 values are learned with TD.
    """"""
    alpha2, beta, gamma0, zeta, eta_sr = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transitions per action as uniform
    T_sr = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))

    # Anxiety-modulated discount and sharpening exponent
    gamma_eff = gamma0 * (1.0 - zeta * stai_val)
    if gamma_eff < 0.0:
        gamma_eff = 0.0
    if gamma_eff > 1.0:
        gamma_eff = 1.0
    k = 1.0 + 4.0 * (1.0 - gamma_eff)

    for t in range(n_trials):
        # Build sharpened transition matrix for decision
        T_used = np.zeros_like(T_sr)
        for a in range(2):
            row = T_sr[a] ** k
            ssum = np.sum(row)
            if ssum > 0:
                row = row / ssum
            else:
                row = np.array([0.5, 0.5])
            T_used[a] = row

        # MB value from learned transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T_used @ max_Q2

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(logits2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Update transition row for chosen action toward observed state (SR-like)
        # Move probability mass toward the observed state while keeping normalization
        T_sr[a1, :] = (1.0 - eta_sr) * T_sr[a1, :]
        T_sr[a1, s2] += eta_sr
        # Normalize for numerical stability
        row_sum = np.sum(T_sr[a1, :])
        if row_sum > 0:
            T_sr[a1, :] /= row_sum

        # Update stage-2 values
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha2', 'beta', 'gamma0', 'zeta', 'eta_sr']"
iter2_run0_participant42.json,cognitive_model1,360.90735590266206,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned-transition hybrid with anxiety-modulated model-based weight and choice stickiness.

    This model learns the first-stage transition matrix online and arbitrates between
    a model-based (MB) planner and a model-free (MF) first-stage value. The arbitration
    weight decreases with both anxiety and transition uncertainty. A choice-stickiness
    bias (increasing with anxiety) captures perseverative tendencies often linked to anxiety.
    Second-stage values are learned with a simple delta rule.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int
        Reached second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state (0/1; X: W/S, Y: P/H).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; higher means higher anxiety.
    model_parameters : sequence of floats
        [alpha, beta, k_stick, eta_T, w_base]
        Bounds:
        - alpha in [0,1]: learning rate for Q-updates.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - k_stick in [0,1]: baseline stickiness strength; effective stickiness scales with anxiety.
        - eta_T in [0,1]: learning rate for the transition matrix.
        - w_base in [0,1]: baseline MB arbitration weight; effective weight reduced by anxiety and transition uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, k_stick, eta_T, w_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix with non-informative prior (uniform rows).
    T = np.full((2, 2), 0.5)

    # Value functions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Previous first-stage action for stickiness
    prev_a1 = None

    for t in range(n_trials):
        # Compute transition uncertainty via row entropies (0=low, 1=high)
        # Normalize entropy of a binary distribution by dividing by log(2)=1
        epsH = 1e-12
        ent = -np.sum(T * np.log(T + epsH), axis=1)  # in nats
        ent = ent / np.log(2.0)  # normalize to [0,1]
        trans_uncert = float(np.mean(ent))

        # MB values: expectation over learned transitions of max second-stage Q
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2           # shape (2,)

        # Arbitration weight w: lower with higher anxiety and higher transition uncertainty
        w = w_base * (1.0 - stai) * (1.0 - trans_uncert)
        w = min(max(w, 0.0), 1.0)

        # Stickiness bias increases with anxiety
        stick_eff = k_stick * stai

        # Combine MB and MF
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Add stickiness as an additive bias to the chosen-last action
        pref1 = beta * q1
        if prev_a1 is not None:
            pref1[prev_a1] += stick_eff

        # First-stage choice probability
        z1 = np.max(pref1)
        exp1 = np.exp(pref1 - z1)
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (softmax on q2 at reached state)
        s = state[t]
        pref2 = beta * q2[s].copy()
        z2 = np.max(pref2)
        exp2 = np.exp(pref2 - z2)
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning
        # Update second-stage Q
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update first-stage MF toward realized second-stage value (bootstrapped)
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update learned transitions for taken first-stage action using observed state
        # One-hot target distribution for observed transition
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1.0 - eta_T) * T[a1, :] + eta_T * target
        # Normalize for numerical safety
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'k_stick', 'eta_T', 'w_base']"
iter2_run0_participant42.json,cognitive_model2,315.8431972034641,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric model-free SARSA with anxiety-modulated lapse and endogenous eligibility.

    Purely model-free learning with separate learning rates for gains and losses at the
    second stage. A TD(Î») credit assignment to the first stage uses Î» = 1 - stai, so higher
    anxiety shortens the eligibility span. An anxiety-modulated lapse (random choice mixing)
    is applied at both stages.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int
        Reached second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state (0/1).
    reward : array-like of float
        Obtained reward per trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher means higher anxiety.
    model_parameters : sequence of floats
        [alpha_gain, alpha_loss, beta1, beta2, lapse_base]
        Bounds:
        - alpha_gain in [0,1]: learning rate when second-stage PE is positive.
        - alpha_loss in [0,1]: learning rate when second-stage PE is negative or zero.
        - beta1 in [0,10]: inverse temperature for first-stage softmax.
        - beta2 in [0,10]: inverse temperature for second-stage softmax.
        - lapse_base in [0,1]: base lapse mixture; effective lapse increases with anxiety:
          lapse_eff = min(0.5, lapse_base * stai).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_g, alpha_l, beta1, beta2, lapse_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective eligibility and lapse from anxiety
    lam = 1.0 - stai
    lam = min(max(lam, 0.0), 1.0)
    lapse = min(0.5, max(0.0, lapse_base * stai))

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage policy with lapse
        pref1 = beta1 * q1
        z1 = np.max(pref1)
        exp1 = np.exp(pref1 - z1)
        soft1 = exp1 / np.sum(exp1)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with lapse
        s = state[t]
        pref2 = beta2 * q2[s]
        z2 = np.max(pref2)
        exp2 = np.exp(pref2 - z2)
        soft2 = exp2 / np.sum(exp2)
        probs_2 = (1.0 - lapse) * soft2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning rates depend on the sign of the second-stage PE
        pe2 = r - q2[s, a2]
        alpha2 = alpha_g if pe2 > 0.0 else alpha_l
        q2[s, a2] += alpha2 * pe2

        # Stage-1 update with TD(lambda): mixture of bootstrapped difference and second-stage PE
        pe1 = q2[s, a2] - q1[a1]
        # Use same asymmetric rate for credit assignment
        q1[a1] += alpha2 * (pe1 + lam * pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_g', 'alpha_l', 'beta1', 'beta2', 'lapse_base']"
iter2_run0_participant43.json,cognitive_model1,533.8014866082563,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Hybrid model-based/model-free learner with anxiety-gated arbitration and action stickiness.

    Idea:
    - Stage-2: standard model-free TD learning of alien values.
    - Stage-1: combines model-free values with model-based evaluation via the transition matrix.
    - The arbitration weight omega between MF and MB at stage-1 is gated by STAI:
        omega(stai) = omega_low + (omega_high - omega_low) * stai
      Higher STAI can push the agent toward more MB or more MF depending on parameters.
    - Action stickiness (same parameter) applies to both stages to capture perseveration.

    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha (0..1), TD learning rate for value updates
    - model_parameters[1]: beta (0..10), inverse temperature for softmax at both stages
    - model_parameters[2]: omega_low (0..1), arbitration weight at STAI=0
    - model_parameters[3]: omega_high (0..1), arbitration weight at STAI=1
    - model_parameters[4]: zeta (0..1), strength of action stickiness bias

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U)
    - state: array of ints in {0,1}, observed planet (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on the planet (0 or 1)
    - reward: array of floats, obtained coins
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of 5 parameters as above

    Returns:
    - Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, omega_low, omega_high, zeta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Transition structure: rows are spaceships (A=0, U=1), cols are planets (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)            # stage-1 model-free
    q2 = np.zeros((2, 2))          # stage-2 model-free (state x action)

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Previous choices for stickiness
    prev_a1 = -1
    prev_a2 = -1
    eps = 1e-10

    # Compute arbitration weight from STAI
    omega = omega_low + (omega_high - omega_low) * stai_val
    if omega < 0.0:
        omega = 0.0
    if omega > 1.0:
        omega = 1.0

    for t in range(n_trials):
        # Model-based evaluation at stage-1 from current q2
        max_q2 = np.max(q2, axis=1)                 # best alien per planet
        q1_mb = transition_matrix @ max_q2          # expected values per spaceship

        # Hybrid action values at stage-1
        q1_hybrid = (1.0 - omega) * q1_mf + omega * q1_mb

        # Stickiness bias for stage-1
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += 1.0  # favor repeating last stage-1 choice

        # Softmax for stage-1
        logits1 = beta * (q1_hybrid + zeta * bias1)
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        probs1 = soft1 / (np.sum(soft1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in observed state
        s = int(state[t])
        bias2 = np.zeros(2)
        if prev_a2 in (0, 1):
            bias2[prev_a2] += 1.0  # favor repeating last stage-2 choice (global stickiness)
        logits2 = beta * (q2[s] + zeta * bias2)
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        probs2 = soft2 / (np.sum(soft2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update (model-free)
        delta2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha * delta2

        # Stage-1 MF update: bootstrapped toward realized stage-2 value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] = q1_mf[a1] + alpha * delta1

        # Update stickiness memory
        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega_low', 'omega_high', 'zeta']"
iter2_run0_participant43.json,cognitive_model2,533.6315232431598,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Model-free learner with anxiety-modulated learning rate and temperature,
    plus a transition-dependent choice kernel at stage-1.

    Idea:
    - Pure model-free TD value learning at both stages.
    - STAI increases/decreases learning rate and choice noise:
        alpha_eff = clip(alpha_base + k_anx_alpha * stai, 0, 1)
        beta_eff  = beta_base / (1 + 9 * k_anx_temp * stai)  (higher STAI -> more noise if k_anx_temp>0)
    - A transition-dependent kernel (gamma_tr) biases repeating the previous stage-1 choice:
        After reward: common transition encourages repetition; rare discourages it.
        After no reward: rare encourages repetition; common discourages it.
      This captures a model-based-like transition-outcome interaction via a simple bias term.

    Parameters (all in [0,1] except beta_base in [0,10]):
    - model_parameters[0]: alpha_base (0..1), baseline learning rate
    - model_parameters[1]: k_anx_alpha (0..1), slope of STAI effect on learning rate
    - model_parameters[2]: beta_base (0..10), baseline inverse temperature
    - model_parameters[3]: k_anx_temp (0..1), slope of STAI effect on temperature
    - model_parameters[4]: gamma_tr (0..1), strength of transition-dependent bias

    Inputs:
    - action_1: array of ints in {0,1}
    - state: array of ints in {0,1}
    - action_2: array of ints in {0,1}
    - reward: array of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of 5 parameters as above

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha_base, k_anx_alpha, beta_base, k_anx_temp, gamma_tr = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated parameters
    alpha_eff = alpha_base + k_anx_alpha * stai_val
    if alpha_eff < 0.0:
        alpha_eff = 0.0
    if alpha_eff > 1.0:
        alpha_eff = 1.0

    beta_eff = beta_base / (1.0 + 9.0 * (k_anx_temp * stai_val))
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Values
    q1 = np.zeros(2)          # stage-1 model-free
    q2 = np.zeros((2, 2))     # stage-2 model-free

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # For transition-dependent kernel, track previous trial info
    have_prev = False
    prev_a1 = 0
    prev_state = 0
    prev_reward = 0.0

    for t in range(n_trials):
        # Transition-dependent bias for stage-1 logits based on previous trial
        bias1 = np.zeros(2)
        if have_prev:
            # Determine if previous transition was common or rare
            # Common: A->X or U->Y; Rare otherwise
            was_common = ((prev_a1 == 0 and prev_state == 0) or
                          (prev_a1 == 1 and prev_state == 1))

            # Define signed signal for repeating previous action
            # After reward: +1 if common, -1 if rare
            # After no reward: -1 if common, +1 if rare
            rewarded = prev_reward > 0.0
            if rewarded:
                sign = 1.0 if was_common else -1.0
            else:
                sign = -1.0 if was_common else 1.0

            # Apply bias symmetrically toward repeating previous choice
            b = gamma_tr * sign
            if prev_a1 == 0:
                bias1[0] += b
                bias1[1] -= b
            else:
                bias1[1] += b
                bias1[0] -= b

        # Stage-1 policy
        logits1 = beta_eff * (q1 + bias1)
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        probs1 = soft1 / (np.sum(soft1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at observed state
        s = int(state[t])
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        probs2 = soft2 / (np.sum(soft2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha_eff * delta2

        # Stage-1 TD update toward realized stage-2 value
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] = q1[a1] + alpha_eff * delta1

        # Save current trial info for next trial's bias
        have_prev = True
        prev_a1 = a1
        prev_state = s
        prev_reward = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_base', 'k_anx_alpha', 'beta_base', 'k_anx_temp', 'gamma_tr']"
iter2_run0_participant43.json,cognitive_model3,528.7382378952112,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Risk-sensitive model-free learner with anxiety-modulated risk aversion and value decay.

    Idea:
    - Stage-2 maintains exponentially weighted estimates of both mean and second moment for each alien.
      Utility is mean - rho * std, where std is derived from the EW second moment.
    - Stage-1 is purely model-free, bootstrapping toward the realized stage-2 utility.
    - STAI modulates risk aversion rho:
        rho = clip(rho_base + k_anx_rho * stai, 0, 1)
      Higher STAI can increase (or decrease) risk aversion depending on k_anx_rho.
    - A decay parameter shrinks unchosen values toward zero, capturing forgetting/instability.

    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha (0..1), EW update rate for mean and second moment
    - model_parameters[1]: beta (0..10), inverse temperature at both stages
    - model_parameters[2]: rho_base (0..1), baseline risk aversion
    - model_parameters[3]: k_anx_rho (0..1), slope of STAI effect on risk aversion
    - model_parameters[4]: decay (0..1), per-trial decay applied to unchosen values

    Inputs:
    - action_1: array of ints in {0,1}
    - state: array of ints in {0,1}
    - action_2: array of ints in {0,1}
    - reward: array of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of 5 parameters as above

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha, beta, rho_base, k_anx_rho, decay = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated risk aversion
    rho = rho_base + k_anx_rho * stai_val
    if rho < 0.0:
        rho = 0.0
    if rho > 1.0:
        rho = 1.0

    # Stage-2 statistics: EW means and second moments per (state,action)
    m = np.zeros((2, 2))     # EW mean reward
    m2 = np.zeros((2, 2))    # EW mean of squared reward

    # Stage-1 MF values
    q1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        # Compute risk-sensitive utility for each state-action
        var = np.maximum(m2 - m**2, 0.0)
        std = np.sqrt(var + 1e-12)
        u = m - rho * std  # utility

        # Stage-1 policy: use current MF q1
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        probs1 = soft1 / (np.sum(soft1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in observed state using utility
        s = int(state[t])
        logits2 = beta * u[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        probs2 = soft2 / (np.sum(soft2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = reward[t]

        # Learning at stage-2: EW updates for mean and second moment
        # Decay unchosen alien at this state and also decay other state's means slightly
        other_a2 = 1 - a2
        m[s, other_a2] = (1.0 - decay) * m[s, other_a2]
        m2[s, other_a2] = (1.0 - decay) * m2[s, other_a2]
        other_state = 1 - s
        m[other_state, 0] = (1.0 - decay) * m[other_state, 0]
        m[other_state, 1] = (1.0 - decay) * m[other_state, 1]
        m2[other_state, 0] = (1.0 - decay) * m2[other_state, 0]
        m2[other_state, 1] = (1.0 - decay) * m2[other_state, 1]

        # Update chosen alien stats at observed state
        m[s, a2] = m[s, a2] + alpha * (r - m[s, a2])
        m2[s, a2] = m2[s, a2] + alpha * (r*r - m2[s, a2])

        # Recompute utility for the realized (s, a2) after the update
        var_sa = max(m2[s, a2] - m[s, a2]**2, 0.0)
        u_sa = m[s, a2] - rho * np.sqrt(var_sa + 1e-12)

        # Stage-1 learning: MF TD toward risk-sensitive utility
        # Decay unchosen stage-1 action
        q1[1 - a1] = (1.0 - decay) * q1[1 - a1]
        delta1 = u_sa - q1[a1]
        q1[a1] = q1[a1] + alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'rho_base', 'k_anx_rho', 'decay']"
iter2_run0_participant44.json,cognitive_model1,416.8822895596396,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated transition learning, MF/MB arbitration, forgetting, and temperature shift.

    Idea:
    - The agent learns the transition matrix online and uses it for model-based (MB) evaluation.
    - Anxiety increases transition learning rate (hypervigilance) and increases forgetting of Q values.
    - Anxiety decreases inverse temperature (more exploration).
    - First-stage policy arbitrates between MB and model-free (MF) values based on current transition uncertainty and anxiety.
    - MF credit assignment from stage 2 to stage 1 uses an eligibility trace equal to the participant's anxiety (no extra parameter).

    Parameters (with bounds):
    - alpha in [0,1]: learning rate for second-stage Q updates; also used for first-stage MF TD.
    - beta in [0,10]: base inverse temperature.
    - phi_trans in [0,1]: base transition learning rate; anxiety scales it upward.
    - eta_forget in [0,1]: base forgetting rate; anxiety scales it upward (more forgetting under anxiety).
    - xi_anxTemp in [0,1]: strength of anxiety-driven temperature reduction.

    Inputs:
    - action_1: array-like ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like ints in {0,1} for reached planet (0=X, 1=Y).
    - action_2: array-like ints in {0,1} for second-stage choices (0/1 for the two aliens on that planet).
    - reward: array-like floats (typically 0/1).
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, phi_trans, eta_forget, xi_anxTemp].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """"""
    alpha, beta, phi_trans, eta_forget, xi_anxTemp = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize transition matrix estimate; start with common=0.7 prior
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Effective parameters modulated by anxiety
    beta_eff = max(1e-3, beta * (1.0 - xi_anxTemp * stai))  # higher anxiety -> lower beta
    phi_eff = np.clip(phi_trans * (1.0 + 0.5 * stai), 0.0, 1.0)  # higher anxiety -> faster transition learning
    forget_eff = np.clip(eta_forget * stai, 0.0, 1.0)  # higher anxiety -> more forgetting
    lambda_anx = np.clip(stai, 0.0, 1.0)  # eligibility trace equals anxiety

    # Value tables
    q1_mf = np.zeros(2)          # MF Q for A/U
    q2 = np.zeros((2, 2))        # Q for aliens on each planet

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Compute MB values from current transition estimate
        max_q2 = np.max(q2, axis=1)  # best value on each planet
        q1_mb = T @ max_q2

        # Arbitration weight: less MB when anxiety high and when transitions are uncertain (high entropy)
        # Measure average transition uncertainty (mean row entropy, normalized to [0,1])
        row_ent = []
        for r in range(2):
            p = T[r]
            # entropy base 2, max entropy = 1 for binary
            h = 0.0
            for x in p:
                if x > 0:
                    h -= x * (np.log(x) / np.log(2))
            row_ent.append(h)  # in [0,1] for binary
        ent_mean = 0.5 * (row_ent[0] + row_ent[1])  # 0=certainty, 1=max uncertainty
        w_mb = np.clip((1.0 - stai) * (1.0 - ent_mean), 0.0, 1.0)

        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # First-stage policy
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Second-stage policy (no additional bias)
        logits2 = beta_eff * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD updates
        # Forgetting applied to all Qs (global drift), stronger under anxiety
        q1_mf = (1.0 - forget_eff) * q1_mf
        q2 = (1.0 - forget_eff) * q2

        # Stage-1 MF TD toward realized second-stage action value (SARSA-style)
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Stage-2 TD with received reward
        td2 = r - q2[s, a2]
        q2[s, a2] += alpha * td2

        # Eligibility trace: propagate outcome back to stage 1, scaled by anxiety
        q1_mf[a1] += alpha * lambda_anx * td2

        # Transition learning: update only the chosen first-stage action's row toward observed state
        # One-hot for observed transition
        obs = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = (1.0 - phi_eff) * T[a1, :] + phi_eff * obs

        # Keep rows normalized and numerically safe
        T[a1, :] = np.clip(T[a1, :], 1e-6, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'phi_trans', 'eta_forget', 'xi_anxTemp']"
iter2_run0_participant5.json,cognitive_model1,443.48465118009847,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Transition-learning hybrid with anxiety-weighted arbitration and uncertainty bonus.

    Core ideas:
    - Learns the state-transition function online (rather than assuming it's fixed).
    - Stage 1 uses a hybrid of model-based (via learned transitions) and model-free values.
    - Anxiety increases the transition learning rate and reduces MB arbitration weight.
    - Adds a directed exploration bonus at stage 1 proportional to transition uncertainty, scaled by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited planet (0 or 1).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - increases transition learning rate,
        - reduces reliance on model-based arbitration at stage 1,
        - increases directed exploration bonus driven by transition uncertainty.
    model_parameters : array-like of float
        [alpha_q, beta, alpha_T, w_mb0, phi_unc]
        Bounds:
        - alpha_q in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_T in [0,1]: learning rate for transition probability learning.
        - w_mb0 in [0,1]: baseline MB arbitration weight at stage 1.
        - phi_unc in [0,1]: scaling of uncertainty (entropy) bonus at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_q, beta, alpha_T, w_mb0, phi_unc = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition probabilities T[a, s'] as rows that sum to 1
    # Start with a mild prior favoring common transitions (0.6/0.4)
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)          # model-free values at stage 1
    q2 = 0.5 * np.ones((2, 2))   # stage-2 action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective parameters modulated by anxiety
    alpha_T_eff = np.clip(alpha_T * (0.5 + 0.5 * stai), 0.0, 1.0)  # higher stai -> faster transition learning
    w_mb = np.clip(w_mb0 * (1.0 - 0.5 * stai), 0.0, 1.0)           # higher stai -> less MB reliance
    phi_eff = phi_unc * stai                                        # higher stai -> stronger uncertainty bonus

    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute MB action values at stage 1 from learned transitions and max stage-2 values
        max_q2 = np.max(q2, axis=1)      # value of each planet
        q1_mb = T @ max_q2               # expected value of choosing each spaceship

        # Add directed exploration bonus from transition uncertainty (entropy of each row)
        # Entropy H(p) = -sum p log p, max at uniform (uncertain transitions)
        row_ent = -(T * (np.log(T + eps))).sum(axis=1)
        q1_aug = q1_mb + phi_eff * row_ent

        # Hybrid arbitration
        q1 = w_mb * q1_aug + (1.0 - w_mb) * q1_mf

        # Stage-1 choice probabilities
        x1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(x1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probabilities at reached state
        q2_s = q2[s]
        x2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(x2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Transition learning: update row for chosen action toward observed state (one-hot target)
        target = np.zeros(2)
        target[s] = 1.0
        T[a1] = (1.0 - alpha_T_eff) * T[a1] + alpha_T_eff * target
        # Renormalize to avoid drift
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Stage-1 MF update toward obtained stage-2 action value (SARSA(0)-style backup)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha_q', 'beta', 'alpha_T', 'w_mb0', 'phi_unc']"
iter2_run0_participant5.json,cognitive_model2,435.6970956742449,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-gated eligibility traces with surprise amplification and perseveration.

    Core ideas:
    - Stage-2 uses standard TD learning.
    - Stage-1 model-free values are updated via eligibility traces that persist across trials.
    - Anxiety increases the eligibility trace persistence (lambda) and amplifies credit assignment after rare transitions.
    - Includes choice stickiness at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited planet (0 or 1).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - increases eligibility persistence (lambda),
        - increases the amplification of credit assignment following rare transitions,
        - increases choice stickiness.
    model_parameters : array-like of float
        [alpha, beta, lambda0, xi, stick0]
        Bounds:
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: inverse temperature for both stages.
        - lambda0 in [0,1]: baseline eligibility trace persistence.
        - xi in [0,1]: magnitude of surprise amplification on eligibility after rare transitions.
        - stick0 in [0,1]: baseline stickiness; scaled up by anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, lambda0, xi, stick0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure for common vs. rare classification
    # Common if a1 == state, rare otherwise (given task mapping).
    # We'll use this to compute surprise.
    q1 = np.zeros(2)           # stage-1 MF values
    q2 = 0.5 * np.ones((2, 2)) # stage-2 values

    # Eligibility trace over stage-1 actions
    e1 = np.zeros(2)

    # Stickiness biases
    bias1 = np.zeros(2)
    bias2 = np.zeros((2, 2))
    stick_eff = stick0 * (0.3 + 0.7 * stai)  # higher anxiety -> more stickiness

    # Anxiety-gated lambda
    lam_eff = np.clip(lambda0 * (0.3 + 0.7 * stai), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    prev_a1 = None
    prev_s = None
    prev_a2 = None

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 policy with stickiness
        logits1 = q1 + bias1
        x1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(x1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        logits2 = q2[s] + bias2[s]
        x2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(x2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Compute surprise: 0 common, 1 rare
        is_common = (a1 == s)
        surprise = 0.0 if is_common else 1.0

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update eligibility trace for stage-1 actions
        # Decay and add current chosen action; amplify by surprise scaled by anxiety
        e1 *= lam_eff
        e1[a1] += 1.0 * (1.0 + xi * stai * surprise)
        # Normalize e1 to avoid blow-up (optional but stabilizing)
        norm_e = max(1.0, np.sum(np.abs(e1)))
        e1 = e1 / norm_e

        # Back up stage-1 values toward realized second-stage value via eligibility
        target1 = q2[s, a2]
        pe1_vec = target1 - q1
        q1 += alpha * e1 * pe1_vec

        # Update stickiness biases
        bias1[:] = 0.0
        bias1[a1] += stick_eff
        bias2[:] = 0.0
        bias2[s, a2] += stick_eff

        prev_a1, prev_s, prev_a2 = a1, s, a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'lambda0', 'xi', 'stick0']"
iter2_run0_participant5.json,cognitive_model3,438.8822338094147,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-weighted negative-outcome sensitivity with value forgetting and pure MB planning.

    Core ideas:
    - Stage 2 learns action values with asymmetric sensitivity to negative outcomes.
      Outcomes below a neutral baseline (0.5) are up-weighted by anxiety.
    - Q-values undergo forgetting toward a neutral prior (0.5), stronger with anxiety.
    - Stage 1 uses model-based planning from a fixed transition model; no free MB weight parameter.
      Anxiety also reduces effective choice precision (softmax temperature scaling).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited planet (0 or 1).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - increases sensitivity to negative outcomes (below 0.5),
        - increases forgetting toward neutral value at stage 2,
        - reduces choice precision (lower effective beta).
    model_parameters : array-like of float
        [alpha, beta, gamma0, zeta0]
        Bounds:
        - alpha in [0,1]: learning rate for Q-value updates.
        - beta in [0,10]: baseline inverse temperature for both stages.
        - gamma0 in [0,1]: baseline forgetting rate toward 0.5 for stage-2 values.
        - zeta0 in [0,1]: baseline amplification of negative-outcome sensitivity.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, gamma0, zeta0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 values initialized at neutral prior 0.5
    q2 = 0.5 * np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Effective forgetting toward 0.5 increases with anxiety
    gamma_eff = np.clip(gamma0 * (0.3 + 0.7 * stai), 0.0, 1.0)
    # Negative-outcome sensitivity increases with anxiety
    zeta_eff = zeta0 * (0.5 + 0.5 * stai)
    # Choice precision reduced with anxiety
    beta_eff = beta * (0.6 + 0.4 * (1.0 - stai))

    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values from fixed transitions and current q2
        max_q2 = np.max(q2, axis=1)  # planet values
        q1_mb = T @ max_q2

        # Stage-1 softmax
        x1 = beta_eff * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(x1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax
        x2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(x2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Stage-2 forgetting toward neutral prior
        q2[s] = (1.0 - gamma_eff) * q2[s] + gamma_eff * 0.5

        # Stage-2 update with anxiety-weighted negative-outcome sensitivity around baseline 0.5
        outcome = r - 0.5
        if outcome >= 0.0:
            adj_outcome = outcome  # positive or neutral
        else:
            adj_outcome = outcome * (1.0 + zeta_eff)  # amplify negative outcome

        pe2 = (0.5 + adj_outcome) - q2[s, a2]
        q2[s, a2] += alpha * pe2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha', 'beta', 'gamma0', 'zeta0']"
iter2_run0_participant6.json,cognitive_model1,516.8787490748282,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Partially planned TD model with anxiety-modulated foresight and transition confidence.
    
    Idea:
    - Stage-2 values are learned with simple TD.
    - Stage-1 uses a partially model-based backup that blends max and mean action values at stage 2.
    - Anxiety reduces foresight (mix toward mean) and lowers confidence in the transition structure.
    - An eligibility-like backpropagation (eta) updates a stage-1 MF value from the realized stage-2 value.
    
    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Received coins (can be negative or positive).
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, mix, eta, conf]
        - alpha in [0,1]: learning rate for Q-values (both stages)
        - beta in [0,10]: inverse temperature for both stages
        - mix in [0,1]: base weight on max vs mean for constructing stage-2 state values
                         V2 = mix*max(Q2) + (1-mix)*mean(Q2)
        - eta in [0,1]: eligibility-like backprop weight for updating stage-1 MF from realized stage-2 value
        - conf in [0,1]: confidence in known transition structure (1 uses nominal 0.7/0.3; 0 uses uniform 0.5/0.5)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """"""
    alpha, beta, mix, eta, conf = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Nominal transition structure (rows: A, U; cols: X, Y)
    T_nom = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)
    T_unif = np.array([[0.5, 0.5],
                       [0.5, 0.5]], dtype=float)

    # Anxiety reduces confidence in transitions (more uncertainty) and foresight (less max, more mean)
    conf_eff = np.clip(conf * (1.0 - 0.6 * stai_val), 0.0, 1.0)
    T = conf_eff * T_nom + (1.0 - conf_eff) * T_unif

    mix_eff = np.clip(mix * (1.0 - 0.5 * stai_val), 0.0, 1.0)  # higher anxiety -> rely more on mean than max
    eta_eff = np.clip(eta * (1.0 - 0.3 * stai_val), 0.0, 1.0)  # slightly dampen credit assignment under anxiety

    # Value tables
    q2 = np.zeros((2, 2))     # Q at stage 2: states X/Y x actions {0,1}
    q1_mf = np.zeros(2)       # Model-free Q at stage 1 for A/U

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        # Construct state values at stage 2 using mix of max and mean
        max_q2 = np.max(q2, axis=1)
        mean_q2 = np.mean(q2, axis=1)
        v2 = mix_eff * max_q2 + (1.0 - mix_eff) * mean_q2

        # Partially planned stage-1 values from transitions
        q1_mb = T @ v2

        # Combine with MF using a simple average (no extra parameter): avoids duplicating w_mb mechanism tried before
        q1 = 0.5 * q1_mb + 0.5 * q1_mf

        # Stage-1 policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy for realized state
        s = int(state[t])
        q2_s = q2[s]
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update via eligibility-like backprop from realized Q2 of chosen a2
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * eta_eff * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'mix', 'eta', 'conf']"
iter2_run0_participant6.json,cognitive_model3,515.3788614351784,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-outcome interaction with perseveration and anxiety-modulated parameters.
    
    Idea:
    - Stage-2 uses standard TD learning.
    - Stage-1 value includes a transition-outcome interaction signal that captures the classic
      two-step effect: reward after common transition encourages repeating the same first-stage choice,
      reward after rare transition encourages switching (and vice versa for non-reward).
    - Include first- and second-stage choice perseveration kernels.
    - Anxiety increases perseveration and reduces learning rate; also adds a baseline bias toward spaceship A for low anxiety.
    
    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, rho, xi, bias0]
        - alpha in [0,1]: learning rate for stage-2 Q
        - beta in [0,10]: inverse temperature for both stages
        - rho in [0,1]: base perseveration strength (choice kernels at both stages)
        - xi in [0,1]: weight of transition-outcome interaction signal on stage-1 decision values
        - bias0 in [0,1]: baseline bias toward spaceship A when anxiety is low (bias decreases with anxiety)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """"""
    alpha, beta, rho, xi, bias0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known mapping: A commonly -> X (0), U commonly -> Y (1)
    common_state = np.array([0, 1], dtype=int)

    # Values and kernels
    q2 = np.zeros((2, 2))
    k1 = np.zeros(2)  # first-stage choice kernel
    k2 = np.zeros(2)  # second-stage choice kernel
    q1_mf = np.zeros(2)

    # Anxiety-modulated parameters
    alpha_eff = np.clip(alpha * (1.0 - 0.3 * stai_val), 0.0, 1.0)
    rho_eff = np.clip(rho * (0.5 + 0.5 * stai_val), 0.0, 1.0)
    xi_eff = np.clip(xi * (1.0 + 0.2 * stai_val), 0.0, 1.0)
    # Bias toward A (action 0) stronger at low anxiety
    bias_eff = (0.5 - stai_val) * bias0  # can be negative if anxiety > 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Variables to carry previous trial info for interaction term
    prev_a1 = None
    prev_common = None
    prev_reward = 0.0

    eps = 1e-10

    for t in range(n_trials):
        # Transition-outcome interaction signal based on previous trial
        interact = np.zeros(2)
        if prev_a1 is not None:
            # Common if reached state equals common_state[prev_a1]
            sign = 1.0 if prev_common else -1.0
            # Add to the previously chosen action a signed amount proportional to previous reward
            # and subtract from the alternative to keep relative code balanced
            val = xi_eff * sign * prev_reward
            interact[prev_a1] += val
            interact[1 - prev_a1] -= val

        # Stage-1 values: MF plus interaction, choice kernel, and baseline bias
        v1 = q1_mf + interact + k1 + np.array([bias_eff, -bias_eff])
        v1_center = v1 - np.max(v1)
        probs_1 = np.exp(beta * v1_center)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = int(state[t])
        v2 = q2[s] + k2
        v2_center = v2 - np.max(v2)
        probs_2 = np.exp(beta * v2_center)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # TD update at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * delta2

        # Simple MF bootstrapping for stage-1 Q using realized state-action value
        target1 = q2[s, a2]
        q1_mf[a1] += alpha_eff * (target1 - q1_mf[a1])

        # Update choice kernels (perseveration)
        # Decay toward zero, then add to chosen action
        k1 *= (1.0 - rho_eff)
        k1[a1] += rho_eff
        k2 *= (1.0 - rho_eff)
        k2[a2] += rho_eff

        # Record info for next trial's interaction term
        prev_a1 = a1
        prev_common = (s == common_state[a1])
        prev_reward = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'rho', 'xi', 'bias0']"
iter2_run0_participant7.json,cognitive_model1,535.4256290065945,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning model-based control with anxiety-modulated uncertainty bonus and stickiness.
    
    Overview:
    - Learns transition probabilities T(a1 -> s) online and uses them for model-based valuation at stage 1.
    - Learns stage-2 Q-values via TD(0).
    - Adds an exploration bonus proportional to the transition uncertainty (entropy of next-state distribution).
    - Includes first-stage choice stickiness (perseveration).
    - Anxiety (stai) increases sensitivity to transition uncertainty (exploration bonus) and slightly speeds up transition learning,
      while decreasing perseveration.

    Parameters (model_parameters):
    - alpha_r: [0,1] learning rate for stage-2 reward values.
    - beta: [0,10] inverse temperature for softmax choice at both stages.
    - alpha_T: [0,1] base transition learning rate (modulated upward by stai internally).
    - eta: [0,1] exploration bonus weight for transition uncertainty (scaled up by stai internally).
    - pers: [0,1] baseline first-stage choice stickiness (scaled down by stai internally).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choice (0=A, 1=U).
    - state: array of ints {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array of ints {0,1}, second-stage choice (0/1 within the reached planet).
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety score.
    - model_parameters: tuple/list of 5 params (alpha_r, beta, alpha_T, eta, pers).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_r, beta, alpha_T, eta, pers = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Learned transition matrix T[a1, s] initialized near common/rare but neutral (0.5/0.5)
    T = np.ones((2, 2)) * 0.5

    # Stage-2 Q-values: Q[s, a2]
    Q2 = np.zeros((2, 2))

    # Storage for choice likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations
    alpha_T_eff = np.clip(alpha_T * (1.0 + 0.5 * stai), 0.0, 1.0)
    eta_eff = eta * (0.5 + 0.5 * stai)  # more anxiety => stronger uncertainty bonus
    pers_eff = pers * (1.0 - 0.7 * stai)  # more anxiety => less stickiness

    # Stickiness memory (last chosen action at stage 1)
    last_a1 = -1

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first-stage Q via learned transitions and current second-stage Q
        max_Q2 = np.max(Q2, axis=1)  # best alien per planet
        Q1_MB = T @ max_Q2  # expected best outcome via transitions

        # Uncertainty (entropy) bonus per first-stage action
        # H(p) = -sum p log p, with 2 states so bounded in [0, log 2]
        ent = np.zeros(2)
        for a in range(2):
            p_x, p_y = T[a, 0], T[a, 1]
            ent[a] = -(p_x * np.log(p_x + eps) + p_y * np.log(p_y + eps))
        bonus = eta_eff * ent

        # Perseveration bonus for repeating last first-stage choice
        stick = np.zeros(2)
        if last_a1 in (0, 1):
            stick[last_a1] = pers_eff

        Q1 = Q1_MB + bonus + stick

        # First-stage policy
        Q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * Q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in reached state
        s = state[t]
        Q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * Q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcomes
        r = reward[t]

        # Update transition model for the chosen first-stage action (supervised/Dirichlet-like update)
        # Move probability mass toward the observed state
        T[a1, :] = (1.0 - alpha_T_eff) * T[a1, :]
        T[a1, s] += alpha_T_eff
        # Ensure normalization (floating error guard)
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

        # Stage-2 TD(0) learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * delta2

        # Update stickiness memory
        last_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'alpha_T', 'eta', 'pers']"
iter2_run0_participant7.json,cognitive_model2,505.896363523025,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric learning with anxiety-modulated loss aversion and repetition bias.
    
    Overview:
    - Uses separate learning rates for positive and negative outcomes at stage 2.
    - Transforms outcomes via loss aversion: negative outcomes are weighted by rho(stai) > 1.
    - Propagates learned value to first-stage via model-free backup (bootstrapping from Q2).
    - Includes a repetition bias applied to both stages, reduced by higher anxiety.

    Parameters (model_parameters):
    - alpha_pos: [0,1] learning rate when the outcome utility is positive.
    - alpha_neg: [0,1] learning rate when the outcome utility is negative.
    - beta: [0,10] inverse temperature for both stages.
    - rho_base: [0,1] baseline loss aversion scaling factor; effective loss aversion is 1 + 4*rho_base*(0.5 + stai).
    - rep: [0,1] baseline repetition bias magnitude (applied to previous choices), scaled by (1 - stai).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choice (0=A,1=U).
    - state: array of ints {0,1}, second-stage state reached (0=X,1=Y).
    - action_2: array of ints {0,1}, second-stage choice.
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha_pos, alpha_neg, beta, rho_base, rep).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_pos, alpha_neg, beta, rho_base, rep = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Stage-2 and Stage-1 MF values
    Q2 = np.zeros((2, 2))   # Q(s, a2)
    Q1 = np.zeros(2)        # MF value for first-stage actions

    # Repetition memory
    last_a1 = -1
    last_a2 = -1
    last_s = -1

    # Anxiety-modulated parameters
    rho = 1.0 + 4.0 * rho_base * (0.5 + stai)  # maps rho_base in [0,1] to approx [1,3] as stai increases
    rep_eff = rep * (1.0 - stai)               # less repetition under higher anxiety

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Repetition bonuses
        stick1 = np.zeros(2)
        if last_a1 in (0, 1):
            stick1[last_a1] = rep_eff

        # First-stage policy (pure MF + stickiness)
        Q1_eff = Q1 + stick1
        q1c = Q1_eff - np.max(Q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with repetition within the reached state
        s = state[t]
        stick2 = np.zeros(2)
        if last_s == s and last_a2 in (0, 1):
            stick2[last_a2] = rep_eff

        Q2_eff = Q2[s] + stick2
        q2c = Q2_eff - np.max(Q2_eff)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome with loss aversion transform for learning
        r = reward[t]
        util = r if r >= 0.0 else -rho * abs(r)

        # Stage-2 learning with asymmetric alphas
        pe2 = util - Q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0 else alpha_neg
        Q2[s, a2] += alpha2 * pe2

        # Stage-1 MF backup using the post-update value as bootstrap
        boot = Q2[s, a2]
        pe1 = boot - Q1[a1]
        alpha1 = alpha_pos if pe1 >= 0 else alpha_neg
        Q1[a1] += alpha1 * pe1

        # Update repetition memory
        last_a1 = a1
        last_a2 = a2
        last_s = s

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'rho_base', 'rep']"
iter2_run0_participant7.json,cognitive_model3,504.4091588564712,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated decisional noise and value decay (forgetting).
    
    Overview:
    - Mixture of model-based (fixed known transitions) and model-free values at stage 1.
    - Stage-2 values learned via TD(0); stage-1 MF values learn via bootstrapping from stage-2.
    - Global value decay simulates forgetting/drift, countering nonstationarity.
    - Anxiety reduces effective inverse temperature (more noise) while leaving planning weight fixed.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates (both stages).
    - beta0: [0,10] base inverse temperature (when stai=0).
    - w: [0,1] weight on model-based control at stage 1 (fixed across trials).
    - k_beta: [0,1] strength of anxiety-induced temperature reduction; beta_eff = beta0*(1 - k_beta*stai).
    - decay: [0,1] per-trial value decay factor applied to all Q-values (higher means more decay).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choice.
    - state: array of ints {0,1}, second-stage state.
    - action_2: array of ints {0,1}, second-stage choice.
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha, beta0, w, k_beta, decay).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta0, w, k_beta, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common=0.7)
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Values
    Q2 = np.zeros((2, 2))  # stage-2 Q(s, a2)
    Q1_mf = np.zeros(2)    # stage-1 MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated beta (avoid <=0)
    beta_eff = max(1e-6, beta0 * (1.0 - k_beta * stai))

    eps = 1e-12

    for t in range(n_trials):
        # Decay (forgetting) applied each trial before decisions
        Q2 *= (1.0 - decay)
        Q1_mf *= (1.0 - decay)

        # Model-based first-stage values from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid action values
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # First-stage policy
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # MF stage-1 update via bootstrap from stage-2 chosen action value (post-update)
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + 1e-12)) + np.sum(np.log(p_choice_2 + 1e-12)))
    return nll","['alpha', 'beta0', 'w', 'k_beta', 'decay']"
iter2_run0_participant8.json,cognitive_model1,448.08135352121997,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated arbitration between model-based and model-free control via state-uncertainty.
    
    Summary
    -------
    - Learns second-stage Q-values with a single learning rate.
    - Maintains a model-free first-stage value via TD backup from second-stage values.
    - Computes a model-based first-stage value using the fixed transition structure (0.7 common).
    - Arbitration weight w between MB and MF depends on:
        (i) base propensity omega0,
        (ii) current state-uncertainty (low discriminability at stage 2 increases MB reliance),
        (iii) participant anxiety (higher anxiety shifts arbitration via phi_anx).
    - Choice at each stage uses softmax with inverse temperature beta.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [alpha, beta, omega0, kappa_unc, phi_anx]
        - alpha: learning rate for value updates [0,1]
        - beta: inverse temperature for both stages [0,10]
        - omega0: baseline model-based weight (pre-sigmoid prior) [0,1]
        - kappa_unc: sensitivity of arbitration to state-uncertainty [0,1]
        - phi_anx: sensitivity of arbitration to anxiety [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, omega0, kappa_unc, phi_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values (states x actions), initialize neutral
    q2 = np.zeros((2, 2)) + 0.5
    # Stage-1 model-free Q-values over actions
    q1_mf = np.zeros(2)

    # Helper: transform omega0 in [0,1] to a logit prior
    eps = 1e-8
    logit_omega0 = np.log((omega0 + eps) / (1.0 - omega0 + eps))

    for t in range(n_trials):
        # Compute current state-uncertainty from discriminability in each state
        diff0 = abs(q2[0, 0] - q2[0, 1])
        diff1 = abs(q2[1, 0] - q2[1, 1])
        # Higher diff => more certainty; define uncertainty in [0,1]
        uncert = 1.0 - 0.5 * (diff0 + diff1)
        uncert = float(np.clip(uncert, 0.0, 1.0))

        # Model-based first-stage values from current q2
        max_q2 = np.max(q2, axis=1)  # value per state
        q1_mb = T @ max_q2

        # Arbitration weight via logistic transform; anxiety and uncertainty push w
        z = logit_omega0 + phi_anx * (st - 0.5) + kappa_unc * uncert
        w = 1.0 / (1.0 + np.exp(-z))

        # First-stage softmax over combined value
        q1 = w * q1_mb + (1.0 - w) * q1_mf
        logits1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage softmax within reached state
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD updates
        # Stage-2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 model-free update: back up the obtained second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'omega0', 'kappa_unc', 'phi_anx']"
iter2_run0_participant8.json,cognitive_model2,479.8702867296673,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-representation planner with anxiety-shaped planning horizon and WSLS bias.
    
    Summary
    -------
    - Uses a successor representation (SR) over planets to evaluate first-stage actions.
      SR depends on a discount gamma that is increased by anxiety (longer planning horizon).
    - Learns second-stage action values with utility scaling rho (reduced by anxiety).
    - Adds a win-stay/lose-shift bias at stage 2, attenuated by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [eta, beta, gamma0, rho0, kappa_wsls]
        - eta: learning rate for stage-2 Q updates [0,1]
        - beta: inverse temperature for both stages [0,10]
        - gamma0: baseline SR discount parameter [0,1]
        - rho0: baseline reward sensitivity (utility scaling) [0,1]
        - kappa_wsls: strength of win-stay/lose-shift bias at stage 2 [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    eta, beta, gamma0, rho0, kappa_wsls = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values
    q2 = np.zeros((2, 2)) + 0.5

    # Track previous second-stage choice and outcome per state for WSLS
    prev_a2 = np.array([-1, -1], dtype=int)
    prev_r = np.array([0.0, 0.0], dtype=float)

    # Anxiety-modulated parameters
    # Planning horizon increases with anxiety (cap at 0.99 to keep (I - gamma*T) invertible)
    gamma = float(np.clip(gamma0 * (0.6 + 0.8 * st), 0.0, 0.99))
    # Reward sensitivity decreases with anxiety
    rho = float(np.clip(rho0 * (0.8 + 0.4 * (1.0 - st)), 0.0, 1.0))
    # WSLS bias attenuated by anxiety
    k_wsls_eff = float(kappa_wsls * (1.0 - 0.5 * st))

    # Precompute SR kernel inverse term per trial since gamma is fixed here
    I = np.eye(2)
    inv_term = np.linalg.inv(I - gamma * T)  # 2x2

    for t in range(n_trials):
        # SR-based first-stage values: v_a = T[a] @ (I - gamma T)^-1 @ max_q2
        max_q2 = np.max(q2, axis=1)  # value per state
        sr_values = np.zeros(2)
        for a in range(2):
            occ = T[a] @ inv_term  # discounted occupancy over states after choosing action a
            sr_values[a] = occ @ max_q2

        logits1 = beta * (sr_values - np.max(sr_values))
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with WSLS bias for the reached state
        s = state[t]
        base_logits2 = q2[s] - np.max(q2[s])

        wsls_bias = np.zeros(2)
        if prev_a2[s] != -1:
            if prev_r[s] > 0.0:
                wsls_bias[prev_a2[s]] += k_wsls_eff
            else:
                wsls_bias[prev_a2[s]] -= k_wsls_eff

        logits2 = beta * (base_logits2 + wsls_bias)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        u = rho * r  # utility-scaled outcome

        # Update second-stage Q-values
        pe2 = u - q2[s, a2]
        q2[s, a2] += eta * pe2

        # Update WSLS memory for this state
        prev_a2[s] = a2
        prev_r[s] = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['eta', 'beta', 'gamma0', 'rho0', 'kappa_wsls']"
iter2_run0_participant8.json,cognitive_model3,365.1058372489481,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-gated credit assignment with anxiety-shaped learning rate and perseveration.
    
    Summary
    -------
    - Uses fixed transitions (0.7 common) to compute model-based Q1 and model-free Q1 (TD from stage 2).
    - Surprise (rare transition) gates how much MF credit is assigned back to the chosen first-stage action.
    - Learning rate at stage 2 increases with both surprise and anxiety.
    - Arbitration between MB and MF at stage 1 depends on anxiety (higher anxiety -> more MF).
    - First-stage perseveration bias increases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [eta0, beta, chi_surprise, pi_rep, zeta_anx]
        - eta0: base learning rate [0,1]
        - beta: inverse temperature for both stages [0,10]
        - chi_surprise: strength of surprise gating on MF credit and learning rate [0,1]
        - pi_rep: perseveration bias to repeat previous first-stage action [0,1]
        - zeta_anx: anxiety sensitivity affecting both arbitration and learning rate [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    eta0, beta, chi_surprise, pi_rep, zeta_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2)

    # Anxiety-modulated arbitration weight: higher anxiety => lower MB weight
    w_mb = 1.0 / (1.0 + np.exp(zeta_anx * (st - 0.5) * 4.0))  # smooth, in (0,1)

    prev_a1 = None

    for t in range(n_trials):
        # Model-based Q1 from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # First-stage softmax with perseveration bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            # Perseveration increases with anxiety
            bias[prev_a1] += pi_rep * (0.4 + 0.8 * st)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias
        logits1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Determine whether the observed transition was common or rare (surprise)
        # Common mapping: A->X (0->0), U->Y (1->1)
        common = (a1 == s)
        p_common = 0.7 if common else 0.3
        surprise = 1.0 - p_common  # 0.3 for common, 0.7 for rare

        # Anxiety and surprise shape the effective learning rate at stage 2
        eta_t = eta0 * (0.5 + 0.5 * zeta_anx * st + chi_surprise * surprise)
        eta_t = float(np.clip(eta_t, 0.0, 1.0))

        # Stage-2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta_t * pe2

        # Surprise-gated model-free credit assignment back to stage 1
        # Rare transitions reduce MF credit (classic two-step signature),
        # and anxiety increases this attenuation via zeta_anx.
        gate = 1.0 - chi_surprise * surprise * (0.6 + 0.4 * st * zeta_anx)
        gate = float(np.clip(gate, 0.0, 1.0))
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += eta0 * gate * pe1

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['eta0', 'beta', 'chi_surprise', 'pi_rep', 'zeta_anx']"
iter3_run0_participant0.json,cognitive_model1,491.2736973520752,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-gated learning with anxiety-weighted arbitration (MB vs. MF).

    This model learns stage-2 action values and their uncertainty, uses an
    uncertainty- and anxiety-dependent effective learning rate at stage 2,
    and arbitrates at stage 1 between model-based (MB) and model-free (MF)
    values using an arbitration weight that decreases with both anxiety and
    global uncertainty.

    Parameters (bounds):
    - alpha0: [0,1] base learning rate for stage-2 value updates
    - kappa_unc: [0,1] update rate for tracking uncertainty (PE^2) at stage 2
    - w_anx: [0,1] strength by which anxiety down-weights model-based arbitration
    - beta: [0,10] inverse temperature for both stages

    Inputs:
    - action_1: array of length T with first-stage choices (0 or 1)
    - state: array of length T with observed second-stage state (0=X, 1=Y)
    - action_2: array of length T with second-stage choices (0 or 1)
    - reward: array of length T with rewards (e.g., 0/1)
    - stai: array-like with one element in [0,1] indicating anxiety score
    - model_parameters: [alpha0, kappa_unc, w_anx, beta]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha0, kappa_unc, w_anx, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Value and uncertainty for stage 2
    q2 = np.zeros((2, 2))
    var2 = np.ones((2, 2)) * 0.25  # initial uncertainty

    # Model-free values at stage 1
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Global normalized uncertainty (0..1-ish) based on current var2
        u = np.mean(var2)
        u_norm = u / (u + 0.1)

        # Arbitration weight: higher anxiety and higher uncertainty -> less MB
        w_mb = np.clip((1.0 - w_anx * stai_score) * (1.0 - u_norm), 0.0, 1.0)

        # Combine MB and MF for stage 1 decision values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (state-conditioned)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s, a2]

        # Update uncertainty tracker (variance-like)
        var2[s, a2] = (1.0 - kappa_unc) * var2[s, a2] + kappa_unc * (pe2 * pe2)

        # Uncertainty- and anxiety-gated learning rate
        alpha_eff = alpha0 / (1.0 + var2[s, a2])
        alpha_eff *= (1.0 + stai_score)  # more anxious -> faster updating
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Update stage-2 value
        q2[s, a2] += alpha_eff * pe2

        # Model-free backup to stage-1 action value
        q1_mf[a1] += alpha_eff * (q2[s, a2] - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'kappa_unc', 'w_anx', 'beta']"
iter3_run0_participant0.json,cognitive_model2,509.3530575069468,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based control with learned transition probabilities, anxiety-boosted
    transition learning.

    This model learns both second-stage action values and the first-stage
    transition structure. Stage-1 choices are purely model-based using the
    learned transitions. Transition learning rate increases with anxiety, capturing
    heightened sensitivity to contingency changes.

    Parameters (bounds):
    - alpha_r: [0,1] learning rate for stage-2 reward values
    - alpha_tr_base: [0,1] base learning rate for transition probabilities
    - anx_tr_gain: [0,1] multiplicative gain determining how much anxiety increases transition LR
    - beta: [0,10] inverse temperature for both stages

    Inputs:
    - action_1: array of length T with first-stage choices (0 or 1)
    - state: array of length T with observed second-stage state (0=X, 1=Y)
    - action_2: array of length T with second-stage choices (0 or 1)
    - reward: array of length T with rewards (e.g., 0/1)
    - stai: array-like with one element in [0,1] indicating anxiety score
    - model_parameters: [alpha_r, alpha_tr_base, anx_tr_gain, beta]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_r, alpha_tr_base, anx_tr_gain, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Learned transition matrix T[action, state], initialize uniform
    T = np.ones((2, 2)) * 0.5

    # Stage-2 Q-values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate
    alpha_tr = alpha_tr_base * (1.0 + anx_tr_gain * stai_score)
    alpha_tr = np.clip(alpha_tr, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based value using learned transitions
        max_q2 = np.max(q2, axis=1)  # over actions at each second-stage state
        q1_mb = T @ max_q2

        # Optional mild anxiety-dependent exploration: reduce precision with anxiety
        beta_eff = beta * (1.0 - 0.3 * stai_score)
        beta_eff = max(beta_eff, 0.0)

        # Stage 1 policy (pure MB)
        logits1 = beta_eff * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Update transitions for the chosen first-stage action toward the observed state
        # Move probability mass toward the observed state s
        T[a1, :] = (1.0 - alpha_tr) * T[a1, :]
        T[a1, s] += alpha_tr
        # Ensure numeric normalization (should be normalized by construction)
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

        # Stage-2 reward learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'alpha_tr_base', 'anx_tr_gain', 'beta']"
iter3_run0_participant0.json,cognitive_model3,494.3154514840153,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive omission aversion with anxiety-driven lapse and stage-1 perseveration.

    This model learns utility-weighted values at stage 2, where missed rewards
    (omissions) are penalized via a loss-aversion-like factor that grows with anxiety.
    It includes an anxiety-driven lapse at both stages and a stage-1 choice
    perseveration that strengthens with anxiety. Stage-1 values combine MB and MF
    without adding extra parameters.

    Parameters (bounds):
    - alpha: [0,1] learning rate for value updates
    - lambda0: [0,1] baseline omission aversion (higher -> stronger penalty for 0 reward)
    - eps0: [0,1] baseline lapse scaling; effective lapse grows with anxiety
    - pers1: [0,1] baseline perseveration strength at stage 1
    - beta: [0,10] inverse temperature for both stages

    Anxiety usage:
    - Omission aversion: lambda_eff = clip(lambda0 * (1 + stai), 0, 1)
    - Lapse: epsilon = clip(eps0 * stai, 0, 0.5)
    - Perseveration: pers1_eff = pers1 * stai
    - Arbitration weight (no extra parameter): w_mb = 0.25 + 0.5 * (1 - stai)

    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, lambda0, eps0, pers1, beta]

    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha, lambda0, eps0, pers1, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Stage-2 and stage-1 MF values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    # Effective parameters modulated by anxiety
    lambda_eff = np.clip(lambda0 * (1.0 + stai_score), 0.0, 1.0)
    epsilon = np.clip(eps0 * stai_score, 0.0, 0.5)
    pers1_eff = pers1 * stai_score
    w_mb = 0.25 + 0.5 * (1.0 - stai_score)  # in [0.25, 0.75]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = -1

    for t in range(n_trials):
        # Model-based at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Combine MB/MF
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add stage-1 perseveration bias for repeating last choice
        bias1 = np.zeros(2)
        if last_a1 != -1:
            bias1[last_a1] += pers1_eff

        # Stage 1 policy with lapse
        logits1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 /= np.sum(probs1)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with lapse
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 /= np.sum(probs2)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Utility-transformed outcome: omission penalized
        r = reward[t]
        util = r - lambda_eff * (1.0 - r)

        # Update stage-2 values
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update stage-1 MF via bootstrapped backup
        q1_mf[a1] += alpha * (q2[s, a2] - q1_mf[a1])

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'lambda0', 'eps0', 'pers1', 'beta']"
iter3_run0_participant1.json,cognitive_model1,538.0996334365784,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MBâMF with anxiety-gated arbitration and first-stage perseveration.
    
    This model combines a model-based (MB) evaluation that uses the known common/rare transition
    structure with a model-free (MF) first-stage value learned from second-stage outcomes.
    Anxiety increases reliance on the MF system (lower MB arbitration weight), implementing
    a shift toward habitual control in high anxiety. A first-stage perseveration term biases
    repeating the previous spaceship. A small lapse applies to both stages.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha: learning rate for second-stage MF values and backpropagation to first-stage MF (0..1).
    - beta: inverse temperature for both stages (0..10).
    - omega0: baseline arbitration weight for MB contribution at stage 1 (0..1).
    - phi_stai: strength by which anxiety reduces MB weight: omega = clip(omega0 - phi_stai*stai, 0, 1).
    - kappa1: first-stage perseveration strength (adds to the logit of the previously chosen spaceship; 0..1).
    - epsilon: lapse rate applied to both stages (0..1).

    Args:
        action_1: 1D int array of first-stage actions (0/1) chosen on each trial.
        state: 1D int array of encountered second-stage states (0 for X, 1 for Y).
        action_2: 1D int array of second-stage actions within the encountered state (0/1).
        reward: 1D float array of rewards (typically 0/1).
        stai: 1D array with a single float anxiety score in [0,1].
        model_parameters: iterable of parameters in the order above.

    Returns:
        Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, omega0, phi_stai, kappa1, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Model-free values
    Q2 = np.zeros((2, 2))      # second-stage action values
    Q1_mf = np.zeros(2)        # first-stage model-free values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    prev_a1 = -1  # for perseveration

    # Anxiety-gated arbitration weight
    omega = omega0 - phi_stai * stai
    omega = max(0.0, min(1.0, omega))
    beta_eff = max(beta, 1e-6)

    for t in range(n_trials):
        # Model-based forecast for each spaceship: value = T @ max_a Q2(s, a)
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Hybrid first-stage value
        Q1_hyb = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # First-stage perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        # Stage 1 policy
        logits1 = beta_eff * Q1_hyb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (within encountered state)
        s = state[t]
        logits2 = beta_eff * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Learning at stage 2 (MF)
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Backpropagate to first stage MF using the realized second-stage value
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega0', 'phi_stai', 'kappa1', 'epsilon']"
iter3_run0_participant1.json,cognitive_model2,529.302198816806,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pure model-free with asymmetric learning, anxiety-driven pessimistic utility, and Q decay.
    
    This model assumes choices are governed by model-free values only. Second-stage action values
    are learned with asymmetric learning rates for positive vs. negative prediction errors.
    Anxiety adds pessimism by penalizing zero-reward outcomes: r_eff = r - psi*stai*(1 - r),
    making omissions feel worse under higher anxiety. Values slowly decay toward a neutral prior
    (0.5) to capture forgetfulness/drift. Perseveration at the second stage biases repeating
    the previous alien choice within a planet.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha_pos: learning rate when prediction error is positive (0..1).
    - alpha_neg: learning rate when prediction error is negative (0..1).
    - beta: inverse temperature for both stages (0..10).
    - psi_anx: anxiety-driven pessimism strength (0..1).
    - decay: per-trial decay toward 0.5 for Q2 values (0..1).
    - kappa2: second-stage perseveration strength (0..1).

    Args:
        action_1: 1D int array of first-stage actions (0/1).
        state: 1D int array of second-stage states (0/1).
        action_2: 1D int array of second-stage actions within state (0/1).
        reward: 1D float array of rewards (0/1).
        stai: 1D array with a single float anxiety score in [0,1].
        model_parameters: iterable of parameters in the order above.

    Returns:
        Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_pos, alpha_neg, beta, psi_anx, decay, kappa2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])
    beta_eff = max(beta, 1e-6)
    eps = 1e-12

    # Initialize values
    Q2 = np.full((2, 2), 0.5)   # start with neutral prior to align with decay anchor
    Q1 = np.zeros(2)            # first-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a2 = np.array([-1, -1], dtype=int)  # previous a2 per state

    for t in range(n_trials):
        # Stage 1 policy from MF value only
        logits1 = beta_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = 0.99 * probs1 + 0.01 * 0.5  # small built-in lapse to avoid degeneracy

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy within encountered state, with perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa2

        logits2 = beta_eff * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = 0.99 * probs2 + 0.01 * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome with anxiety-driven pessimism for omission
        r = reward[t]
        r_eff = r - psi_anx * stai * (1.0 - r)  # decreases value of 0 outcomes; leaves 1 unchanged

        # Learning at stage 2 with asymmetry and decay
        pe2 = r_eff - Q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        # Decay for all Q2 in this state toward 0.5
        Q2[s] = (1.0 - decay) * Q2[s] + decay * 0.5
        # Update chosen action after decay
        Q2[s, a2] += lr2 * pe2

        # Backpropagate to stage 1 MF using observed second-stage action value (SARSA(1))
        pe1 = Q2[s, a2] - Q1[a1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        Q1[a1] += lr1 * pe1

        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'psi_anx', 'decay', 'kappa2']"
iter3_run0_participant1.json,cognitive_model3,538.9988032820114,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning MB with anxiety-driven structure reset and dual perseveration.

    The agent learns transition probabilities from experience and second-stage rewards.
    Anxiety increases a hazard of 'structural reset': with probability zeta_reset*stai each trial,
    the current row of the transition matrix for the chosen spaceship is partially reset toward
    an uninformative prior (0.5, 0.5), capturing lapses in maintaining the task structure.
    Both stages include perseveration biases. A lapse parameter applies to both stages.

    Parameters (all in [0,1] except beta in [0,10]):
    - alphaR: learning rate for second-stage rewards (0..1).
    - alphaT: learning rate for transition probabilities (0..1).
    - beta: inverse temperature for both stages (0..10).
    - zeta_reset: strength of anxiety-driven transition reset (0..1).
    - kappa1: first-stage perseveration strength (0..1).
    - kappa2: second-stage perseveration strength within state (0..1).
    - epsilon: lapse rate applied to both stages (0..1).

    Args:
        action_1: 1D int array of first-stage actions (0/1).
        state: 1D int array of observed second-stage states (0/1).
        action_2: 1D int array of second-stage actions within state (0/1).
        reward: 1D float array of rewards (0/1).
        stai: 1D array with a single float anxiety score in [0,1].
        model_parameters: iterable of parameters in the order above.

    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """"""
    alphaR, alphaT, beta, zeta_reset, kappa1, kappa2, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])
    beta_eff = max(beta, 1e-6)
    eps = 1e-12

    # Initialize transition beliefs to the canonical common/rare structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)
    # Second-stage values
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        # MB first-stage values from current transition belief
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # First-stage perseveration
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        # Stage 1 policy
        logits1 = beta_eff * Q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with perseveration within state
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa2

        logits2 = beta_eff * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome learning
        r = reward[t]
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Transition learning for the chosen spaceship
        oh = np.array([1.0 if j == s else 0.0 for j in range(2)], dtype=float)
        # Standard exponential averaging toward observed state
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        # Anxiety-driven partial reset toward uninformative prior (0.5, 0.5)
        reset_strength = zeta_reset * stai
        if reset_strength > 0.0:
            T[a1] = (1.0 - reset_strength) * T[a1] + reset_strength * 0.5

        # Renormalize the chosen row to ensure it remains a proper distribution
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / (row_sum + eps)

        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alphaR', 'alphaT', 'beta', 'zeta_reset', 'kappa1', 'kappa2', 'epsilon']"
iter3_run0_participant12.json,cognitive_model1,479.61332743035166,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free controller with anxiety-modulated arbitration
    and nonstationarity-aware decay at the second stage.

    This model blends a fixed-transition model-based (MB) controller with a model-free (MF)
    controller, where the arbitration weight is shifted by anxiety (STAI). Second-stage values
    are allowed to drift toward a neutral prior each trial to capture gradual nonstationarity.
    Anxiety also scales the effective learning rate used to back up second-stage values to stage 1.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien in that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration and learning.
    model_parameters : list or array
        [alpha, beta, w_base, xi_stai, tau]
        Bounds:
          alpha in [0,1]     : base learning rate for value updates
          beta in [0,10]     : inverse temperature for softmax
          w_base in [0,1]    : baseline MB weight in stage-1 arbitration
          xi_stai in [0,1]   : sensitivity of arbitration and MF backup to STAI
          tau in [0,1]       : nonstationarity decay toward 0.5 for second-stage Q-values

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w_base, xi_stai, tau = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)            # model-free first-stage values
    q2 = np.full((2, 2), 0.5)      # second-stage action values, start at neutral prior 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight and MF backup rate
    w = np.clip(w_base + xi_stai * (stai - 0.5), 0.0, 1.0)
    # Effective MF backup rate increases/decreases with STAI
    alpha_back = np.clip(alpha * (1.0 + xi_stai * (stai - 0.5)), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Nonstationarity-aware decay of second-stage values toward 0.5
        if tau > 0.0:
            q2 = (1.0 - tau) * q2 + tau * 0.5

        # Stage-1 MB values: expectation over next-state max-Q
        max_q2 = np.max(q2, axis=1)                    # per state: best second-stage value
        q1_mb = transition_matrix @ max_q2             # expected value for each first-stage action

        # Hybrid arbitration
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2_s = q2[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # MF backup to stage 1 (eligibility-like)
        q1_mf[a1] += alpha_back * (q2[s, a2] - q1_mf[a1])

        # Optional small direct TD correction toward realized second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * 0.0 * td1  # kept zero to avoid extra implicit parameterization

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'w_base', 'xi_stai', 'tau']"
iter3_run0_participant12.json,cognitive_model2,484.02732319358427,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-modulated transition learning and uncertainty bonuses.

    The agent learns the first-stage transition matrix from experience and combines a learned
    model-based controller with a model-free controller. Anxiety increases the transition learning
    rate and boosts directed exploration via an uncertainty bonus at the second stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien in that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate transition learning and exploration bonus.
    model_parameters : list or array
        [alpha, beta, omega, kappa_stai, novelty]
        Bounds:
          alpha in [0,1]        : learning rate for value updates and MF backup
          beta in [0,10]        : inverse temperature
          omega in [0,1]        : weight on model-based controller at stage 1
          kappa_stai in [0,1]   : scaling of transition learning rate by STAI
          novelty in [0,1]      : base weight of uncertainty bonus at stage 2

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, omega, kappa_stai, novelty = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition probabilities; start agnostic
    T = np.full((2, 2), 0.5)  # T[a, s] = P(state=s | action=a)
    # Second-stage Q-values
    q2 = np.full((2, 2), 0.5)
    # Model-free stage-1 values
    q1_mf = np.zeros(2)

    # Visit counters for uncertainty bonus
    visits = np.zeros((2, 2))  # visits[state, action2]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate and exploration weight
    lr_T = np.clip((0.1 + 0.9 * kappa_stai * stai), 0.0, 1.0)
    bonus_scale = novelty * (0.5 + 0.5 * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Uncertainty bonus for current state's second-stage actions
        # Smaller with more visits: 1/sqrt(N+1)
        bonus_s = bonus_scale / np.sqrt(visits[s] + 1.0)
        q2_aug = q2.copy()
        q2_aug[s] = q2[s] + bonus_s

        # Compute MB values using learned transitions and augmented q2
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_q2_aug

        # Stage-1 arbitration
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy uses augmented q2 in the observed state
        q2_s_aug = q2_aug[s]
        q2_centered = q2_s_aug - np.max(q2_s_aug)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Transition learning: update T[a1, :]
        # Move probability mass toward the observed state with lr_T
        T[a1] = (1.0 - lr_T) * T[a1]
        T[a1, s] += lr_T
        # Ensure numerical stability (re-normalize)
        T[a1] = T[a1] / np.sum(T[a1])

        # Second-stage learning
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update visit counts after choice
        visits[s, a2] += 1.0

        # Model-free backup to stage 1, slightly amplified by anxiety
        lambd_eff = 0.5 + 0.5 * stai
        q1_mf[a1] += alpha * lambd_eff * (q2[s, a2] - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'omega', 'kappa_stai', 'novelty']"
iter3_run0_participant12.json,cognitive_model3,517.7561900668777,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive utility with anxiety-modulated lapse and within-state perseveration.

    The agent is model-based at stage 1 using the known transition structure but evaluates
    outcomes through a concave utility function, producing risk sensitivity that depends on a
    risk parameter. Anxiety increases a choice lapse rate (randomness independent of value)
    and induces within-state perseveration at stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien in that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate lapse rate and perseveration.
    model_parameters : list or array
        [alpha, beta, lambda_risk, lapse_base, phi_stai]
        Bounds:
          alpha in [0,1]        : learning rate for utility-based value updates
          beta in [0,10]        : inverse temperature for softmax
          lambda_risk in [0,1]  : base risk sensitivity for utility curvature
          lapse_base in [0,1]   : baseline lapse probability
          phi_stai in [0,1]     : sensitivity of lapse and perseveration to STAI

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, lambda_risk, lapse_base, phi_stai = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Second-stage values (in utility space)
    q2 = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated lapse rate and perseveration strength
    lapse = np.clip(lapse_base + phi_stai * (stai - 0.5), 0.0, 1.0)
    stick2_strength = phi_stai * stai  # bias toward repeating previous a2 within state

    prev_a2 = [-1, -1]  # previous second-stage action for each state

    # Risk sensitivity: concave utility u(r) = r^(1 - lambda_eff)
    lambda_eff = np.clip(lambda_risk * (0.5 + stai), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage-1 MB values via expected max utility-based second-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2
        q1 = q1_mb.copy()

        # Stage-1 policy with lapse mixture
        q1_centered = q1 - np.max(q1)
        soft_1 = np.exp(beta * q1_centered)
        soft_1 = soft_1 / np.sum(soft_1)
        probs_1 = (1.0 - lapse) * soft_1 + lapse * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with within-state perseveration and lapse
        q2_s = q2[s].copy()
        if prev_a2[s] in (0, 1):
            bias = np.zeros(2)
            bias[prev_a2[s]] = stick2_strength
            q2_s = q2_s + bias

        q2_centered = q2_s - np.max(q2_s)
        soft_2 = np.exp(beta * q2_centered)
        soft_2 = soft_2 / np.sum(soft_2)
        probs_2 = (1.0 - lapse) * soft_2 + lapse * 0.5
        p_choice_2[t] = probs_2[a2]

        # Utility-transformed reward
        r = reward[t]
        # Map 0->0, 1->1 with curvature; for r in {0,1}, preserves bounds
        u = r ** (1.0 - lambda_eff)

        # Update second-stage values in utility space
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'lambda_risk', 'lapse_base', 'phi_stai']"
iter3_run0_participant14.json,cognitive_model1,532.3143540213958,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated planning weight and eligibility tracing.
    
    This model blends model-free (MF) and model-based (MB) action values at the
    first stage. MB values are computed from a fixed transition model and the
    current second-stage Q-values. MF values at stage 1 are updated via an
    eligibility trace that backs up second-stage values. Anxiety increases or
    decreases the reliance on MB planning via a linear gain on the MB weight.
    
    Parameters (model_parameters):
    - alpha_q: learning rate for Q-value updates (both stages), in [0,1]
    - beta: inverse temperature for both stages' softmax, in [0,10]
    - w0: baseline weight on MB contribution at stage 1, in [0,1]
    - g_w: anxiety gain for MB weight (positive -> more planning with higher anxiety), in [0,1]
    - elig: eligibility strength for backing up stage-2 value into stage-1 MF, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (0/1)
    - stai: array-like with single float anxiety score in [0,1]
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_q, beta, w0, g_w, elig = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: rows ships [A,U], cols planets [X,Y]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)        # stage-1 MF values for [A,U]
    q2 = np.zeros((2, 2))      # stage-2 values per planet x alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight
    w_mb = w0 + g_w * (stai - 0.5)  # center at 0.5 for symmetry
    w_mb = np.clip(w_mb, 0.0, 1.0)

    for t in range(n_trials):
        # Compute MB action values from current Q2 and known transitions
        max_q2 = np.max(q2, axis=1)     # best alien per planet
        q1_mb = T @ max_q2

        # Hybrid first-stage values
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy for observed planet
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * delta2

        # Back up stage-2 value to stage-1 MF via eligibility
        # Move q1_mf toward current second-stage action value for the chosen path.
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_q * elig * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_q', 'beta', 'w0', 'g_w', 'elig']"
iter3_run0_participant14.json,cognitive_model2,554.9416249452654,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pessimistic-planning model-based RL with anxiety-modulated risk attitude.
    
    This model uses a purely model-based (MB) first-stage evaluation with a
    pessimistic aggregation at the second stage: instead of using the max over
    second-stage options, it blends the best and worst second-stage Q-values.
    Anxiety modulates the pessimism parameter, shifting weight toward the worst
    outcome under higher anxiety. Stage 2 uses standard MF Q-learning.
    
    Parameters (model_parameters):
    - alpha: reward learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - z0: baseline optimism (z=1 is max-only, z=0 is min-only), in [0,1]
    - g_z: anxiety gain on pessimism (negative values push toward min with higher stai), in [0,1]
      Effective z = clip(z0 + g_z*(0.5 - stai), 0, 1) so higher stai reduces z if g_z>0.
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (0/1)
    - stai: array-like with single float anxiety score in [0,1]
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, z0, g_z = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))  # planet x alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated pessimism: higher stai -> lower z (more weight on worst)
    z = z0 + g_z * (0.5 - stai)
    z = np.clip(z, 0.0, 1.0)

    for t in range(n_trials):
        # Pessimistic aggregator per planet
        planet_best = np.max(q2, axis=1)
        planet_worst = np.min(q2, axis=1)
        planet_val = z * planet_best + (1.0 - z) * planet_worst

        # MB first-stage values via expected planet_val under transitions
        q1_mb = T @ planet_val

        # Stage-1 policy
        logits1 = beta * (q1_mb - np.max(q1_mb))
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (standard softmax on q2 at reached planet)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Update second-stage values
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'z0', 'g_z']"
iter3_run0_participant14.json,cognitive_model3,565.2118717552677,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Directed-exploration bonus with anxiety-modulated curiosity and count decay.
    
    This model uses MB action values at stage 1 computed from second-stage
    values augmented by an uncertainty-driven exploration bonus. Uncertainty is
    estimated from simple visit counts per planet-alien, with optional decay to
    capture non-stationarity. Anxiety modulates the curiosity strength: higher
    anxiety reduces the bonus (less directed exploration).
    
    Stage 2 choices also receive the same bonus, encouraging exploration when
    uncertainty is high. Learning at stage 2 is MF Q-learning.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - xi0: baseline exploration bonus strength, in [0,1]
    - g_xi: anxiety gain on exploration bonus (negative with higher stai if positive), in [0,1]
      Effective xi = clip(xi0 * (1.0 - g_xi * stai), 0, 1)
    - decay_n: count decay per trial toward zero (0=no decay, 1=full forgetting), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (0/1)
    - stai: array-like with single float anxiety score in [0,1]
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_r, beta, xi0, g_xi, decay_n = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # planet x alien Q-values
    n = np.zeros((2, 2))    # visit counts per planet x alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration strength
    xi = xi0 * (1.0 - g_xi * stai)
    xi = np.clip(xi, 0.0, 1.0)

    for t in range(n_trials):
        # Compute uncertainty bonus from counts (higher when fewer visits)
        # Use 1/sqrt(n+1) scaling for directed exploration
        bonus = xi * (1.0 / np.sqrt(n + 1.0))

        # Stage-1 MB values computed from (Q2 + bonus) on each planet
        q2_plus = q2 + bonus
        planet_best_aug = np.max(q2_plus, axis=1)
        q1_mb = T @ planet_best_aug

        # Stage-1 policy
        logits1 = beta * (q1_mb - np.max(q1_mb))
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy uses augmented values to capture exploration at choice
        s = state[t]
        logits2 = beta * (q2_plus[s] - np.max(q2_plus[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update counts with decay
        n *= (1.0 - decay_n)
        n[s, a2] += 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta', 'xi0', 'g_xi', 'decay_n']"
iter3_run0_participant15.json,cognitive_model1,578.211679847747,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""MB planning with anxiety-damped learning, UCB exploration at Stage-2, and surprise-driven bias at Stage-1.
    
    Mechanisms:
    - Stage-1: Model-based values from transition model; adds a surprise-driven bias that uses the previous trial's rare transition
      to favor the action that commonly leads to the last visited state. Surprise influence scales with anxiety.
    - Stage-2: Upper Confidence Bound (UCB) exploration bonus based on visit counts; exploration is reduced by higher anxiety.
    - Learning: Stage-2 MF learning with an anxiety-damped learning rate.
    
    Parameters (all used; total=5):
    - alpha_b: [0,1] Base learning rate for Stage-2 MF updates; reduced by anxiety.
    - beta: [0,10] Inverse temperature for both stages.
    - ucb: [0,1] Magnitude of exploration bonus at Stage-2; down-weighted by anxiety.
    - surpr: [0,1] Strength of surprise-driven Stage-1 bias after rare transitions; scaled by anxiety.
    - q0: [0,1] Initial Q-value for Stage-2 action values.
    
    Inputs:
    - action_1: array-like (n_trials,) First-stage choices (0=A, 1=U).
    - state: array-like (n_trials,) Observed planet index (0=X, 1=Y).
    - action_2: array-like (n_trials,) Second-stage choices (0 or 1; alien on that planet).
    - reward: array-like (n_trials,) Observed reward (e.g., 0 or 1).
    - stai: array-like (1,) Anxiety score in [0,1].
    - model_parameters: iterable of the five parameters above in order.
    
    Returns:
    - Negative log-likelihood of the observed Stage-1 and Stage-2 choices.
    """"""
    alpha_b, beta, ucb, surpr, q0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: rows = actions (A=0, U=1), cols = states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Prob tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value and count estimates
    q2 = q0 * np.ones((2, 2))  # Q-values at Stage-2
    n_sa = np.zeros((2, 2))    # visit counts for UCB

    # Anxiety effects
    alpha_eff_scale = 1.0 - 0.5 * stai
    alpha_eff_scale = max(0.0, min(1.0, alpha_eff_scale))
    alpha = alpha_b * alpha_eff_scale

    ucb_eff = ucb * (1.0 - stai)  # less directed exploration with higher anxiety
    # Surprise bias magnitude grows with anxiety (more sensitivity to surprise)
    surpr_eff = surpr * stai

    # Surprise memory from previous trial
    prev_rare = 0.0
    prev_state = None

    for t in range(n_trials):
        s = state[t]

        # Stage-1 model-based values from current q2
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Surprise-driven bias: after a rare transition on previous trial,
        # favor the action that commonly leads to the previously visited state.
        bias = np.zeros(2)
        if prev_state is not None and prev_rare > 0.0:
            a_common_for_prev_state = prev_state  # A(0)->X(0) common, U(1)->Y(1) common
            bias[a_common_for_prev_state] += surpr_eff * prev_rare

        # Stage-1 policy
        z1 = beta * (mb_q1 + bias - np.max(mb_q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 UCB-augmented policy in the observed state
        bonus = ucb_eff / np.sqrt(n_sa[s] + 1.0)
        q2_bonus = q2[s] + bonus
        z2 = beta * (q2_bonus - np.max(q2_bonus))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update visit counts for UCB
        n_sa[s, a2] += 1.0

        # Compute whether current transition is rare for next-trial bias
        # Rare if (A->Y) or (U->X)
        is_common = (a1 == s)  # since A(0)->X(0) common, U(1)->Y(1) common
        prev_rare = 1.0 - float(is_common)
        prev_state = s

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_b', 'beta', 'ucb', 'surpr', 'q0']"
iter3_run0_participant15.json,cognitive_model2,562.1253661926931,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated volatility learning (PearceâHall) with perseveration and MB planning.
    
    Mechanisms:
    - Volatility estimate v_t tracks absolute PEs and modulates both learning rate and choice stochasticity.
    - Stage-2 learning rate: alpha_t = alpha_min + (gain * stai) * |PE_t|, clipped to [0,1].
    - Softmax temperature: beta_eff = beta * (1 - stai * v_t), reducing determinism under high volatility and anxiety.
    - Stage-1 values: Model-based from transition model, plus anxiety-scaled perseveration bias toward previous a1.
    
    Parameters (all used; total=5):
    - alpha_min: [0,1] Baseline learning rate floor.
    - beta: [0,10] Base inverse temperature for both stages (modulated to beta_eff).
    - vol0: [0,1] Initial volatility estimate v_0.
    - gain: [0,1] Volatility/gain parameter controlling sensitivity to |PE|; scaled by stai.
    - persever: [0,1] Strength of perseveration bias at Stage-1; scaled by stai.
    
    Inputs:
    - action_1: array-like (n_trials,) First-stage choices (0=A, 1=U).
    - state: array-like (n_trials,) Observed planet index (0=X, 1=Y).
    - action_2: array-like (n_trials,) Second-stage choices (0 or 1).
    - reward: array-like (n_trials,) Observed reward.
    - stai: array-like (1,) Anxiety score in [0,1].
    - model_parameters: iterable of the five parameters above in order.
    
    Returns:
    - Negative log-likelihood of the observed Stage-1 and Stage-2 choices.
    """"""
    alpha_min, beta, vol0, gain, persever = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))

    # Volatility state and anxiety couplings
    v = float(vol0)
    gain_eff = gain * stai
    persever_eff = persever * stai  # stronger perseveration with higher anxiety

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]

        # Effective inverse temperature decreases with volatility and anxiety
        beta_eff = beta * (1.0 - stai * v)
        beta_eff = max(0.0, beta_eff)

        # Stage-1 MB values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Perseveration bias on previous a1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = persever_eff

        # Stage-1 policy
        z1 = beta_eff * (mb_q1 + bias - np.max(mb_q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and PE
        r = reward[t]
        pe2 = r - q2[s, a2]

        # PearceâHall style learning rate
        alpha_t = alpha_min + gain_eff * abs(pe2)
        if alpha_t < 0.0:
            alpha_t = 0.0
        if alpha_t > 1.0:
            alpha_t = 1.0

        # Update Stage-2 values
        q2[s, a2] += alpha_t * pe2

        # Update volatility estimate with leaky integration of |PE|
        v = (1.0 - gain_eff) * v + gain_eff * abs(pe2)
        v = min(1.0, max(0.0, v))

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_min', 'beta', 'vol0', 'gain', 'persever']"
iter3_run0_participant16.json,cognitive_model1,498.7486593912788,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free learner with anxiety-modulated loss sensitivity and arbitration.
    
    Idea:
    - Stage-2 values are learned via asymmetric learning rates (losses loom larger when anxiety is high).
    - Stage-1 action values combine model-based planning with model-free SARSA backup.
    - Anxiety increases the weight on model-based control and amplifies negative prediction-error learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 within reached state).
    reward : array-like of float in [0,1]
        Rewards received at the end of each trial.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase loss sensitivity and model-based arbitration weight.
    model_parameters : iterable of 5 floats
        - alpha_base in [0,1]: baseline learning rate for value updates.
        - beta1 in [0,10]: softmax inverse temperature for stage-1.
        - beta2 in [0,10]: softmax inverse temperature for stage-2.
        - w_base in [0,1]: baseline weight on model-based control at stage-1.
        - phi_base in [0,1]: baseline loss-sensitivity factor (scales negative PE learning).
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_base, beta1, beta2, w_base, phi_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition matrix: A->X common, U->Y common
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated arbitration and asymmetric learning
    # MB weight moves toward 1 with higher anxiety
    w = np.clip(w_base + 0.6 * stai * (1.0 - w_base), 0.0, 1.0)
    # Asymmetric learning rates: negative PE increased with anxiety
    alpha_pos = np.clip(alpha_base * (1.0 - 0.4 * stai * phi_base), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 + 0.6 * stai * phi_base), 0.0, 1.0)
    # Use a small amount of eligibility at stage-1 that also scales with anxiety via phi
    lam = np.clip(0.2 + 0.6 * stai * phi_base, 0.0, 1.0)

    # Value functions
    q1_mf = np.zeros(2)        # model-free Q at stage-1
    q2 = np.zeros((2, 2))      # stage-2 Q per state and action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based lookahead: value of first-stage action = expected best second-stage value
        max_q2 = np.max(q2, axis=1)    # best within each state
        q1_mb = T @ max_q2

        # Hybrid policy at stage-1
        q1 = w * q1_mb + (1.0 - w) * q1_mf
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 update with asymmetric learning
        pe2 = r - q2[s, a2]
        if pe2 >= 0.0:
            q2[s, a2] += alpha_pos * pe2
        else:
            q2[s, a2] += alpha_neg * pe2

        # Stage-1 model-free SARSA(Î») backup using the chosen second-stage value as target
        # TD target for stage-1 is the pre-update q2 value of the chosen second-stage action
        # Use the current q2 (post-update) but reduce with lambda to avoid over-crediting
        target1 = (1.0 - lam) * q1_mf[a1] + lam * q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use a balanced learning rate: average of pos/neg to keep scale consistent
        alpha1 = 0.5 * (alpha_pos + alpha_neg)
        q1_mf[a1] += alpha1 * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_base', 'beta1', 'beta2', 'w_base', 'phi_base']"
iter3_run0_participant16.json,cognitive_model2,527.7058774124544,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Exploration-bonus planner with anxiety-driven directed exploration and uncertainty tracking.
    
    Idea:
    - Stage-2 maintains both mean rewards and uncertainty (variance proxy) per option.
    - Anxiety increases directed exploration via an uncertainty bonus and speeds uncertainty adaptation.
    - Stage-1 plans over uncertainty-bonus-adjusted values (optimism under uncertainty), while stage-2
      choices use the same bonus in their softmax.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 within reached state).
    reward : array-like of float in [0,1]
        Rewards received at the end of each trial.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase the exploration bonus and uncertainty adaptation.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for mean reward estimates at stage-2.
        - beta1 in [0,10]: softmax inverse temperature for stage-1.
        - beta2 in [0,10]: softmax inverse temperature for stage-2.
        - xi_base in [0,1]: baseline coefficient for directed exploration bonus.
        - tau_base in [0,1]: baseline uncertainty-adaptation rate (variance update gain).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta1, beta2, xi_base, tau_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transitions
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated exploration bonus and uncertainty adaptation
    xi = np.clip(xi_base + 0.8 * stai * (1.0 - xi_base), 0.0, 1.0)      # exploration bonus weight
    tau = np.clip(tau_base + 0.5 * stai * (1.0 - tau_base), 0.0, 1.0)   # uncertainty update gain

    # Stage-2 statistics
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.25  # initial uncertainty near Bernoulli(0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Risk/uncertainty-seeking planning via bonus on uncertain options
        bonus = xi * np.sqrt(np.maximum(q2_var, 1e-8))
        augmented = q2_mean + bonus

        # Stage-1 model-based values are expectations over best augmented value in each state
        max_aug = np.max(augmented, axis=1)
        q1_mb = T @ max_aug

        logits1 = beta1 * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = beta2 * (q2_mean[s] + bonus[s])
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update means
        delta = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha * delta

        # Update uncertainty proxy (variance-like), adapting faster with higher anxiety (via tau)
        # EWMA of squared prediction error
        q2_var[s, a2] = (1.0 - tau) * q2_var[s, a2] + tau * (delta ** 2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta1', 'beta2', 'xi_base', 'tau_base']"
iter3_run0_participant16.json,cognitive_model3,485.92988656973864,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Kalman filter learner with anxiety-modulated volatility and stage-1 perseveration.
    
    Idea:
    - Stage-2 values are learned with a simple Kalman filter per state-action, maintaining
      both means and uncertainties. Anxiety increases assumed process noise (volatility),
      which increases the Kalman gain (faster adaptation). Anxiety also slightly reduces
      stage-2 determinism, and we include a stage-1 perseveration term.
    - Stage-1 plans model-based over current mean estimates.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 within reached state).
    reward : array-like of float in [0,1]
        Rewards received at the end of each trial.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase volatility and reduce stage-2 inverse temperature.
    model_parameters : iterable of 5 floats
        - beta1 in [0,10]: softmax inverse temperature for stage-1.
        - beta2_base in [0,10]: baseline softmax inverse temperature for stage-2.
        - nu_base in [0,1]: baseline process noise (volatility) for Kalman prediction step.
        - zeta_base in [0,1]: baseline observation noise for Kalman update step.
        - kappa in [0,1]: stage-1 perseveration weight added to the previously chosen action.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    beta1, beta2_base, nu_base, zeta_base, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transitions
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated parameters
    # Higher anxiety -> higher process noise (more volatility), slightly lower beta2 (less exploitation)
    nu = np.clip(nu_base + 0.7 * stai * (1.0 - nu_base), 0.0, 1.0)
    zeta = np.clip(zeta_base, 1e-6, 1.0)
    beta2 = max(0.0, beta2_base * (1.0 - 0.4 * stai))

    # Kalman statistics for stage-2 rewards per state-action
    m = np.zeros((2, 2))        # mean
    v = np.ones((2, 2)) * 0.2   # initial uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Stage-1 planning over means
        max_m = np.max(m, axis=1)
        q1_mb = T @ max_m

        # Perseveration at stage-1
        logits1 = beta1 * q1_mb
        if prev_a1 is not None:
            for a in range(2):
                if a == prev_a1:
                    logits1[a] += kappa
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice
        s = state[t]
        logits2 = beta2 * m[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Kalman prediction step: increase uncertainty due to process noise
        v[s, a2] = np.clip(v[s, a2] + nu, 1e-6, 10.0)

        # Kalman update step
        S = v[s, a2] + zeta  # innovation variance
        K = v[s, a2] / S     # Kalman gain in [0,1]
        pe = r - m[s, a2]
        m[s, a2] += K * pe
        v[s, a2] = (1.0 - K) * v[s, a2]

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['beta1', 'beta2_base', 'nu_base', 'zeta_base', 'kappa']"
iter3_run0_participant18.json,cognitive_model1,457.07772669844735,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-biased transition-sensitivity hybrid with forgetting.

    Idea:
    - Stage 1 values are a hybrid of model-based (MB) and model-free (MF).
    - The arbitration weight on each trial depends on the previous trial's transition
      commonness (common vs. rare) and is attenuated by anxiety (STAI).
      Low anxiety => stronger sensitivity to previous transition type.
    - Stage 2 uses MF learning and softmax.
    - Unchosen values decay (forgetting), controlling stability vs. flexibility.

    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - w0: baseline MB weight in [0,1]
    - phi: transition-sensitivity of MB weight in [0,1] (scaled by 1 - stai)
    - decay: forgetting rate for unchosen Q-values in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien (0=W/P, 1=S/H depending on planet)
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w0, phi, decay)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, w0, phi, decay = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    Q1_mf = np.zeros(2)       # MF values at stage 1 (actions A/U)
    Q2 = np.zeros((2, 2))     # MF values at stage 2 (state x action)

    # Track previous transition commonness to modulate next trial arbitration
    # prev_common = +1 for common, -1 for rare, 0 for undefined at t=0
    prev_common_flag = 0

    for t in range(n_trials):
        # Compute model-based action values from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Trial-wise arbitration weight depends on the previous transition's commonness
        # Low anxiety (small st) => larger modulation; High anxiety => smaller modulation
        w_t = w0 + (1.0 - st) * phi * prev_common_flag * 0.5  # scale so modulation stays modest
        w_t = min(1.0, max(0.0, w_t))

        # Hybrid values and stage-1 policy
        Q1_hyb = w_t * Q1_mb + (1.0 - w_t) * Q1_mf
        pref1 = Q1_hyb
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy given observed state
        s = int(state[t])
        pref2 = Q2[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD learning
        # Stage-1 MF bootstraps from Q2 at observed state-action
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Stage-2 MF update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Forgetting (decay) for unchosen options toward a neutral baseline (0.5)
        other_a1 = 1 - a1
        Q1_mf[other_a1] = (1 - decay) * Q1_mf[other_a1] + decay * 0.5
        other_a2 = 1 - a2
        Q2[s, other_a2] = (1 - decay) * Q2[s, other_a2] + decay * 0.5

        # Determine current trial's transition commonness to affect next trial's w_t
        # Common if action matches its more likely state: A->X or U->Y
        is_common = (a1 == s)
        prev_common_flag = 1 if is_common else -1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'phi', 'decay']"
iter3_run0_participant18.json,cognitive_model2,528.3765582527504,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Stage-specific temperatures and anxiety-driven arbitration gain.

    Idea:
    - Stage 1 action values are a hybrid of MB and MF.
    - The MB weight increases (or decreases) with anxiety via a linear gain.
      w_mb = clip(w_base + k_anx * stai, 0, 1)
    - Different inverse temperatures for stage 1 and stage 2 choices (beta1, beta2).
    - TD(Î») with Î» determined by anxiety: higher STAI strengthens credit assignment
      from outcome back to stage-1 (eligibility trace Î» = stai).

    Parameters (model_parameters):
    - alpha: learning rate for Q-values in [0,1]
    - beta1: inverse temperature at stage 1 in [0,10]
    - beta2: inverse temperature at stage 2 in [0,10]
    - w_base: base MB weight (at stai=0) in [0,1]
    - k_anx: anxiety gain on MB weight in [0,1] (effective addition scaled by stai)

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien (0=W/P, 1=S/H)
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta1, beta2, w_base, k_anx)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta1, beta2, w_base, k_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated arbitration and trace
    w_mb = w_base + k_anx * st
    w_mb = min(1.0, max(0.0, w_mb))
    lam = st  # eligibility trace strength increases with anxiety

    for t in range(n_trials):
        # Model-based evaluation from Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Stage-1 policy (hybrid)
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        pref1 = Q1_hyb
        exp1 = np.exp(beta1 * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        pref2 = Q2[s]
        exp2 = np.exp(beta2 * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD updates with eligibility trace from stage-2 PE to stage-1 choice
        # Stage-2
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF bootstrapping
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Trace-based backpropagation of outcome to stage-1
        Q1_mf[a1] += alpha * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta1', 'beta2', 'w_base', 'k_anx']"
iter3_run0_participant18.json,cognitive_model3,472.1069274220613,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Directed exploration at stage 2, anxiety-scaled, with action bias at stage 1.

    Idea:
    - Stage 1 uses MB/MF hybrid values with a fixed MB weight.
    - Stage 1 preference includes a bias toward spaceship A vs. U that scales with anxiety
      (e.g., anxious participants may prefer a ""safer"" option). The bias magnitude is kappa_bias.
    - Stage 2 uses MF values plus a directed exploration bonus based on the running
      magnitude of recent prediction errors (as a proxy for uncertainty/volatility).
      The exploration bonus is down-weighted by anxiety: bonus_eff = bonus_u * (1 - stai).

    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - w_mb: MB weight for stage 1 in [0,1]
    - bonus_u: magnitude of directed exploration bonus in [0,1]
    - kappa_bias: stage-1 action bias magnitude in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien (0=W/P, 1=S/H)
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w_mb, bonus_u, kappa_bias)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, w_mb, bonus_u, kappa_bias = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Running estimate of uncertainty via moving average of absolute PE per state-action
    U = np.zeros((2, 2))  # uncertainty proxy in [0,1] approximately

    # Anxiety-scaled bonus (higher anxiety => less directed exploration)
    bonus_eff = bonus_u * (1.0 - st)

    for t in range(n_trials):
        # Model-based evaluation from Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Stage-1 bias: favor A (index 0) vs U (index 1) proportional to anxiety
        # bias vector: [+1, -1] scaled by kappa_bias * st
        bias_vec = np.array([1.0, -1.0]) * (kappa_bias * st)

        # Hybrid Q and policy
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        pref1 = Q1_hyb + bias_vec
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus
        s = int(state[t])
        pref2 = Q2[s] + bonus_eff * U[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD learning at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update uncertainty proxy (moving average of |PE|)
        U[s, a2] = (1 - alpha) * U[s, a2] + alpha * abs(pe2)

        # Stage-1 MF bootstrapping and modest outcome backpropagation
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1
        # Small direct outcome influence to stage-1 scaled by (1 - w_mb):
        Q1_mf[a1] += alpha * (1.0 - w_mb) * pe2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'w_mb', 'bonus_u', 'kappa_bias']"
iter3_run0_participant21.json,cognitive_model1,399.0163175416353,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free control with anxiety-modulated planning weight and bi-stage perseveration.

    Overview:
    - Stage-2 action values are learned via incremental reward prediction errors.
    - Stage-1 uses a hybrid of model-based (via known transitions) and model-free values.
    - The model-based weight is modulated by anxiety (stai).
    - Perseveration (choice stickiness) acts at both stages and is weaker with higher anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 values and stage-1 bootstrapping
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = omega0 (0 to 1): baseline weight on model-based value at stage 1
    - model_parameters[3] = anx_mod (0 to 1): strength of anxiety modulation on model-based weight
       Effective MB weight: omega_eff = clip(omega0 + anx_mod * (0.5 - stai), 0, 1).
       Higher anxiety (stai>0.5) reduces model-based weight.
    - model_parameters[4] = pi (0 to 1): baseline perseveration magnitude; applied as pi*(1 - stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha, beta, omega0, anx_mod, pi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)        # model-free stage-1 values
    q2 = np.zeros((2, 2))      # stage-2 values: states x actions

    # Choice probabilities to accumulate
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration bookkeeping
    prev_a1 = None
    prev_a2_by_state = [None, None]

    # Anxiety-modulated weights
    omega_eff = np.clip(omega0 + anx_mod * (0.5 - stai_val), 0.0, 1.0)
    stick_strength = pi * (1.0 - stai_val)  # less stickiness with higher anxiety

    for t in range(n_trials):
        # Model-based Q at stage 1 via transition and current stage-2 max values
        max_q2 = np.max(q2, axis=1)                  # shape (2,)
        q1_mb = transition_matrix @ max_q2           # shape (2,)

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick_strength

        # Hybrid Q at stage 1
        q1_net = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf + bias1

        # Softmax for stage 1
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy: add perseveration in the reached state
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick_strength

        q2_net = q2[s] + bias2
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 bootstrapping toward stage-2 value (model-free component)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega0', 'anx_mod', 'pi']"
iter3_run0_participant21.json,cognitive_model2,415.6306659162683,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free control with anxiety-modulated rare-transition credit assignment and perseveration.

    Overview:
    - Pure model-free values at both stages.
    - After rare transitions, the credit assigned from stage-2 to stage-1 is discounted.
    - The discount for rare transitions increases with anxiety (stai), simulating reduced trust in rare outcomes.
    - Perseveration bias is stronger with higher anxiety in this model (distinct assumption).

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate
    - model_parameters[1] = beta (0 to 10): inverse temperature at both stages
    - model_parameters[2] = zeta0 (0 to 1): baseline rare-transition credit discount
    - model_parameters[3] = anx_zeta (0 to 1): strength of anxiety modulation on rare credit
        zeta_eff = clip(zeta0 + anx_zeta * stai, 0, 1)
        Credit weight w_credit = 1 - zeta_eff if common, else zeta_eff
    - model_parameters[4] = phi (0 to 1): perseveration magnitude scaled by stai (phi * stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha, beta, zeta0, anx_zeta, phi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Model-free values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_by_state = [None, None]

    zeta_eff = np.clip(zeta0 + anx_zeta * stai_val, 0.0, 1.0)
    stick_strength = phi * stai_val  # here, higher anxiety => stronger perseveration

    for t in range(n_trials):
        # Perseveration at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick_strength

        q1_net = q1 + bias1
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        # Perseveration at stage 2 within the reached state
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick_strength

        q2_net = q2[s] + bias2
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Determine if transition is common or rare given action_1 -> state
        # Common if A->X (0->0) or U->Y (1->1)
        common = (action_1[t] == state[t])
        w_credit = (1.0 - zeta_eff) if common else zeta_eff

        # Stage-1 update with rare-transition credit modulation
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * w_credit * delta1

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'zeta0', 'anx_zeta', 'phi']"
iter3_run0_participant21.json,cognitive_model3,524.0059937215901,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planning with anxiety-modulated forgetting and uncertainty bonus.

    Overview:
    - Stage-2 values are updated with learning rate alpha and also decay (forget) each trial.
    - A state-action uncertainty measure (based on visit counts) provides an exploration bonus.
    - Both the forgetting rate and the exploration bonus scale with anxiety (stai).
    - Stage-1 choices are fully model-based: compute expected values via transitions from
      the uncertainty-bonused stage-2 values.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = f_base (0 to 1): baseline forgetting rate per trial
    - model_parameters[3] = anx_forget (0 to 1): anxiety modulation of forgetting
        f_eff = clip(f_base + anx_forget * stai, 0, 1)
    - model_parameters[4] = bonus0 (0 to 1): baseline uncertainty bonus strength
        bonus_eff = bonus0 * (1 + stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha, beta, f_base, anx_forget, bonus0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Stage-2 values and visit counts for uncertainty
    q2 = np.zeros((2, 2))
    visits = np.zeros((2, 2))  # counts per state-action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    f_eff = np.clip(f_base + anx_forget * stai_val, 0.0, 1.0)
    bonus_eff = bonus0 * (1.0 + stai_val)

    for t in range(n_trials):
        # Decay (forgetting) before computing policy
        q2 *= (1.0 - f_eff)

        # Uncertainty bonus derived from visit counts; higher for less visited actions
        unc = 1.0 / np.sqrt(visits + 1.0)  # shape (2,2); max 1.0 initially

        # Policy at stage 1 via model-based planning using bonused Q2
        q2_bonused = q2 + bonus_eff * unc
        max_q2_bonused = np.max(q2_bonused, axis=1)
        q1_mb = transition_matrix @ max_q2_bonused

        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state using bonused Q2
        s = state[t]
        q2c = q2_bonused[s] - np.max(q2_bonused[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning update at stage 2 (after observing reward)
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update visit counts (after choice)
        visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'f_base', 'anx_forget', 'bonus0']"
iter3_run0_participant22.json,cognitive_model1,460.6525015027928,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB-MF with learned transitions and anxiety-modulated arbitration.
    
    This two-step model learns stage-2 values (MF), learns the action->state transition
    probabilities, and arbitrates between model-based (MB) and model-free (MF) values
    at stage 1. Anxiety down-weights the MB contribution.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for stage-2 values and TD update of stage-1 MF values.
    - beta: [0,10] inverse temperature for both stages (before any modulation).
    - tau_trans: [0,1] learning rate for actionâstate transition probabilities.
    - w0: [0,1] baseline weight on MB values at stage 1.
    - anx_down: [0,1] strength by which anxiety reduces MB weight: w_eff = clip(w0*(1 - anx_down*stai)).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (A=0, U=1).
    - state: array of ints in {0,1}, reached planet (X=0, Y=1).
    - action_2: array of ints in {0,1}, chosen alien at reached state.
    - reward: array of floats in [0,1], coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha, beta, tau_trans, w0, anx_down].
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, tau_trans, w0, anx_down = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Learned transition model: rows = actions (A=0,U=1), cols = next states (X=0,Y=1)
    # Initialize neutral (0.5/0.5) and learn from experience.
    T = np.ones((2, 2)) * 0.5

    # MF values
    q2 = np.zeros((2, 2))   # stage-2 values by state
    q1_mf = np.zeros(2)     # stage-1 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB arbitration weight (trial-constant here)
    w_eff = w0 * (1.0 - anx_down * stai)
    w_eff = min(1.0, max(0.0, w_eff))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # MB Q at stage 1: use current transition model and greedy stage-2 values
        max_q2 = np.max(q2, axis=1)  # value of each state if acted optimally
        q1_mb = T @ max_q2           # action value = expected state value under learned transitions

        # Hybrid Q at stage 1
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Policy stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Policy stage 2 (MF)
        logits2 = beta * (q2[s, :] - np.max(q2[s, :]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Transition learning: move probability mass of chosen action toward observed state
        # Simple delta rule toward one-hot next state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1 - tau_trans) * T[a1, :] + tau_trans * target
        # Renormalize to avoid drift
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

        # MF learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update via bootstrapping on reached state value (greedy continuation)
        boot = np.max(q2[s, :])
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'tau_trans', 'w0', 'anx_down']"
iter3_run0_participant22.json,cognitive_model2,358.9662545451192,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with asymmetric learning rates and anxiety-amplified loss learning plus perseveration.
    
    This two-stage MF RL uses separate learning rates for positive and negative prediction errors.
    Anxiety selectively amplifies the negative-error learning rate (heightened sensitivity to losses).
    A perseveration term biases repeating the previous choice at each stage.
    
    Parameters (model_parameters):
    - alpha_pos: [0,1] learning rate for positive PEs (both stages).
    - alpha_neg0: [0,1] baseline learning rate for negative PEs (both stages) before anxiety.
    - beta: [0,10] inverse temperature for both stages.
    - stick: [0,1] perseveration strength added to the previously chosen action's logit at each stage.
    - anx_loss: [0,1] scales how much anxiety increases negative learning: alpha_neg = clip(alpha_neg0*(1 + anx_loss*stai)).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship.
    - state: array of ints in {0,1}, reached planet.
    - action_2: array of ints in {0,1}, chosen alien.
    - reward: array of floats, coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha_pos, alpha_neg0, beta, stick, anx_loss].
    
    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha_pos, alpha_neg0, beta, stick, anx_loss = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-modulated negative learning rate
    alpha_neg = alpha_neg0 * (1.0 + anx_loss * stai)
    alpha_neg = min(1.0, max(0.0, alpha_neg))

    # MF Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage 1 policy with perseveration
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick
        logits1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with perseveration (state-specific)
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick
        logits2 = beta * (q2[s, :] - np.max(q2[s, :])) + bias2
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Stage-2 MF update (asymmetric learning rates)
        pe2 = r - q2[s, a2]
        a2_lr = alpha_pos if pe2 >= 0 else alpha_neg
        q2[s, a2] += a2_lr * pe2

        # Stage-1 MF update with full-outcome credit assignment (lambda=1 style)
        pe1 = r - q1[a1]
        a1_lr = alpha_pos if pe1 >= 0 else alpha_neg
        q1[a1] += a1_lr * pe1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg0', 'beta', 'stick', 'anx_loss']"
iter3_run0_participant22.json,cognitive_model3,534.2190298914747,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Utility-transformed outcomes with model-based planning and anxiety-modulated lapses.
    
    Stage-2 learning uses a concave/convex utility transform of rewards to capture
    risk sensitivity. Stage-1 choices are purely model-based using known transitions
    (A->X common, U->Y common), and both stages include an epsilon-lapse that increases
    with anxiety (more random responding).
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for utility-based updates at stage 2.
    - beta: [0,10] inverse temperature for softmax policies (pre-lapse).
    - omega: [0,1] risk-sensitivity on rewards: utility u = r**omega (omega<1 -> risk seeking).
    - eps0: [0,1] baseline lapse probability.
    - anx_lapse: [0,1] how strongly anxiety increases lapse: eps = clip(eps0 + anx_lapse*stai).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship.
    - state: array of ints in {0,1}, reached planet.
    - action_2: array of ints in {0,1}, chosen alien.
    - reward: array of floats in [0,1], coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha, beta, omega, eps0, anx_lapse].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, omega, eps0, anx_lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],  # action A
                  [0.3, 0.7]]) # action U

    # Utility-transformed MF stage-2 values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated lapse
    eps_lapse = eps0 + anx_lapse * stai
    eps_lapse = min(1.0, max(0.0, eps_lapse))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage 1 model-based Q via planning over T and current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Softmax with lapse at stage 1
        logits1 = beta * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        probs1 = (1.0 - eps_lapse) * probs1 + eps_lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with lapse
        logits2 = beta * (q2[s, :] - np.max(q2[s, :]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        probs2 = (1.0 - eps_lapse) * probs2 + eps_lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward update at stage 2
        u = r ** max(1e-8, omega)  # guard tiny omega -> well-defined
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'omega', 'eps0', 'anx_lapse']"
iter3_run0_participant24.json,cognitive_model1,458.0588388588425,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and eligibility backup.

    This model learns model-free (MF) Q-values for both stages and uses a fixed
    transition structure to compute model-based (MB) action values at stage 1.
    It arbitrates between MB and MF at stage 1 with a dynamically changing weight
    that depends on (i) current uncertainty in second-stage preferences (entropy)
    and (ii) anxiety (stai). An eligibility-like backup passes a fraction of the
    immediate reward directly to stage-1 MF values.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Reward received each trial (e.g., 0/1).
    stai : array-like of float
        Anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [eta, beta, tau, psi0, psi_anx]
        - eta in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax policies.
        - tau in [0,1]: eligibility backup strength from immediate reward to stage-1 MF.
        - psi0 in [0,1]: baseline arbitration bias toward MB (via logistic).
        - psi_anx in [0,1]: how much anxiety tilts arbitration toward MB when stai is low
                            and toward MF when stai is high.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    eta, beta, tau, psi0, psi_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # shape (2 actions, 2 states)

    # MF Q-values
    q1_mf = np.zeros(2)        # stage-1 MF
    q2_mf = np.zeros((2, 2))   # stage-2 MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    ln2 = np.log(2.0)

    for t in range(n_trials):
        # Compute MB action values at stage 1 from current MF stage-2 Q
        max_q2 = np.max(q2_mf, axis=1)          # best within each state
        q1_mb = T @ max_q2                      # expected best via transitions

        # Compute uncertainty (entropy) of stage-2 policies across states
        # Use current Q2 to form softmax policies, then average normalized entropy
        Hs = []
        for st in (0, 1):
            prefs2 = q2_mf[st]
            e2 = np.exp(beta * (prefs2 - np.max(prefs2)))
            p2 = e2 / np.sum(e2)
            H = -np.sum(p2 * (np.log(p2 + 1e-12)))
            Hs.append(H / ln2)  # normalize to [0,1]
        Hbar = 0.5 * (Hs[0] + Hs[1])

        # Arbitration weight toward MB; anxiety reduces MB weighting when high (if psi_anx > 0)
        # w in (0,1) via logistic; center stai at 0.5 so (0.5 - s) increases w for lower anxiety.
        z = (psi0 - 0.5) + psi_anx * (0.5 - s) - Hbar
        w = 1.0 / (1.0 + np.exp(-5.0 * z))  # steep logistic; bounded (uses psi0, psi_anx, stai, entropy)

        # Hybrid action values at stage 1
        q1_hyb = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy and likelihood
        e1 = np.exp(beta * (q1_hyb - np.max(q1_hyb)))
        p1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy and likelihood
        st = state[t]
        prefs2 = q2_mf[st]
        e2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        p2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]

        # Stage-2 MF update (TD(0))
        pe2 = r - q2_mf[st, a2]
        q2_mf[st, a2] += eta * pe2

        # Stage-1 MF update using bootstrapped value from visited state-action (SARSA-style)
        pe1 = q2_mf[st, a2] - q1_mf[a1]
        q1_mf[a1] += eta * pe1

        # Eligibility backup: a fraction of the immediate reward directly to stage 1
        q1_mf[a1] += eta * tau * (r - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta', 'beta', 'tau', 'psi0', 'psi_anx']"
iter3_run0_participant24.json,cognitive_model2,542.0339773758749,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planning with learned transitions and anxiety-modulated directed exploration.

    This model learns:
      - Second-stage MF Q-values from reward.
      - First-stage transition probabilities P(state | action_1).
    The stage-1 MB values are computed from the learned transition model and current second-stage values.
    Directed exploration bonuses are added at the second stage based on action uncertainty (count-based),
    and are propagated to stage-1 via the MB backup. Anxiety modulates the exploration bonus magnitude.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the visited state.
    reward : array-like of float
        Reward received each trial (e.g., 0/1).
    stai : array-like of float
        Anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha_r, alpha_p, beta, zeta0, zeta_stai]
        - alpha_r in [0,1]: learning rate for reward values at the second stage.
        - alpha_p in [0,1]: learning rate for transition probabilities P(s|a1).
        - beta in [0,10]: inverse temperature for both stages.
        - zeta0 in [0,1]: baseline directed exploration bonus scale.
        - zeta_stai in [0,1]: how much anxiety reduces exploration bonus (higher stai -> smaller bonus).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_r, alpha_p, beta, zeta0, zeta_stai = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Learned transition model initialized neutral (0.5/0.5)
    T = np.ones((2, 2)) * 0.5  # rows: a1 in {0,1}; cols: state in {0,1}

    # Second-stage MF Q-values
    q2 = np.zeros((2, 2))

    # Count-based uncertainty for directed exploration
    N2 = np.zeros((2, 2))  # visit counts per state-action

    # Anxiety-modulated exploration scale: higher anxiety -> smaller directed exploration
    zeta = zeta0 + zeta_stai * (1.0 - s)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Directed exploration bonus at stage 2 (UCB-style): larger when less visited
        bonus2 = zeta / np.sqrt(N2 + 1.0)

        # MB action values for stage 1: expect max(Q2 + bonus) under learned transitions
        q2_plus = q2 + bonus2
        max_q2_plus = np.max(q2_plus, axis=1)  # best action in each state
        q1_mb = T @ max_q2_plus

        # Stage-1 policy
        e1 = np.exp(beta * (q1_mb - np.max(q1_mb)))
        p1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with directed exploration bonus in the visited state
        st = state[t]
        prefs2 = q2[st] + bonus2[st]
        e2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        p2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]

        # Update second-stage Q with reward learning
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_r * pe2

        # Update transition model using a simple delta rule toward the observed state
        onehot = np.array([1.0 if st == k else 0.0 for k in (0, 1)])
        T[a1] = T[a1] + alpha_p * (onehot - T[a1])
        # Ensure normalization (numerically stable)
        T[a1] = T[a1] / np.sum(T[a1])

        # Update visitation count for exploration
        N2[st, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'alpha_p', 'beta', 'zeta0', 'zeta_stai']"
iter3_run0_participant24.json,cognitive_model3,482.0801775311818,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk- and valence-sensitive model-free learner with anxiety-shaped PE asymmetry and forgetting.

    Pure model-free learning at both stages. A single learning rate updates values, but
    positive vs. negative prediction errors (PEs) are scaled asymmetrically by an anxiety-dependent
    factor. Additionally, values undergo per-trial forgetting toward an uninformative baseline (0.5),
    capturing drift in latent contingencies.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the visited state.
    reward : array-like of float
        Reward received each trial (e.g., 0/1).
    stai : array-like of float
        Anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, sigma0, sigma_stai, rho]
        - alpha in [0,1]: base learning rate for value updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - sigma0 in [0,1]: base asymmetry factor for scaling PEs (risk/valence sensitivity).
        - sigma_stai in [0,1]: how much anxiety increases negative-PE sensitivity and reduces positive-PE sensitivity.
        - rho in [0,1]: forgetting rate toward 0.5 applied each trial to all Q-values.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, sigma0, sigma_stai, rho = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # MF Q-values
    q1 = np.zeros(2) + 0.5
    q2 = np.zeros((2, 2)) + 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-shaped asymmetry factor: higher anxiety -> stronger weighting of negative PEs
    sigma = min(1.0, max(0.0, sigma0 + sigma_stai * s))

    for t in range(n_trials):
        # Policies
        e1 = np.exp(beta * (q1 - np.max(q1)))
        p1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        st = state[t]
        e2 = np.exp(beta * (q2[st] - np.max(q2[st])))
        p2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Forgetting toward 0.5 baseline (applied each trial before updates)
        q1 = (1.0 - rho) * q1 + rho * 0.5
        q2 = (1.0 - rho) * q2 + rho * 0.5

        # Stage-2 update with anxiety-shaped PE asymmetry
        pe2 = r - q2[st, a2]
        scale2 = (1.0 - sigma) if pe2 >= 0.0 else (1.0 + sigma)
        q2[st, a2] += alpha * scale2 * pe2

        # Stage-1 update bootstrapping from updated second-stage value (SARSA(0) style)
        pe1 = q2[st, a2] - q1[a1]
        scale1 = (1.0 - sigma) if pe1 >= 0.0 else (1.0 + sigma)
        q1[a1] += alpha * scale1 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'sigma0', 'sigma_stai', 'rho']"
iter3_run0_participant28.json,cognitive_model1,459.53559199919175,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    
    A hybrid controller combines model-free (MF) and model-based (MB) values at stage 1.
    The MB component uses the known transition structure. The arbitration weight is
    modulated by anxiety: higher anxiety can tilt the balance via a gain parameter.
    An eligibility trace propagates stage-2 value prediction errors to stage-1 MF values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=A, 1=U.
    state : array-like of int (0 or 1)
        Second-stage states: 0=X, 1=Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Reward outcome.
    stai : array-like with one float in [0,1]
        Anxiety score; modulates arbitration (MB weight).
    model_parameters : list or array-like of 5 floats
        [alpha, beta, mix0, gamma, trace]
        - alpha: learning rate for MF value updates at both stages [0,1]
        - beta: inverse temperature (shared across stages) [0,10]
        - mix0: baseline MB weight in [0,1]
        - gamma: anxiety gain on MB weight in [0,1]; w_eff = clip(mix0 * (1 + gamma*(stai-0.5)), 0, 1)
        - trace: eligibility trace from stage-2 to stage-1 MF values [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, mix0, gamma, trace = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (common = 0.7)
    T = np.array([[0.7, 0.3],  # P(state | action=A)
                  [0.3, 0.7]]) # P(state | action=U)

    # Probabilities for choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)       # stage-1 actions
    q2 = np.zeros((2, 2))     # stage-2 (state, action)

    eps = 1e-10

    # Anxiety-modulated MB weight (fixed per subject, applied each trial)
    w_eff = mix0 * (1.0 + gamma * (stai_val - 0.5))
    if w_eff < 0.0:
        w_eff = 0.0
    if w_eff > 1.0:
        w_eff = 1.0

    for t in range(n_trials):
        # Compute MB action values at stage 1 from current stage-2 values
        max_q2 = np.max(q2, axis=1)           # value of best action in each state
        q1_mb = T @ max_q2                    # expected value over transitions

        # Hybrid values for decision
        q1_hybrid = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Stage-1 policy
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (model-free within reached state)
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 update (MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update with eligibility trace using realized stage-2 chosen value
        target1 = q2[s, a2]  # bootstrap from updated stage-2 value
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * trace * pe1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + 1e-10)) + np.sum(np.log(p_choice_2 + 1e-10)))
    return neg_log_lik

","['alpha', 'beta', 'mix0', 'gamma', 'trace']"
iter3_run0_participant28.json,cognitive_model2,470.22652515319567,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based with learned transitions and anxiety-modulated lapses.
    
    The agent learns the transition function online and uses it to compute model-based
    action values at stage 1. A small stimulus-independent lapse mixes the softmax with
    uniform choice; lapse probability increases with anxiety. Stage-2 values are learned
    model-free. One inverse temperature governs both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=A, 1=U.
    state : array-like of int (0 or 1)
        Second-stage states: 0=X, 1=Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Reward outcome.
    stai : array-like with one float in [0,1]
        Anxiety score; increases lapse rate.
    model_parameters : list or array-like of 5 floats
        [alpha_Q, alpha_M, beta, xi0, delta]
        - alpha_Q: learning rate for stage-2 Q updates [0,1]
        - alpha_M: learning rate for transition probabilities [0,1]
        - beta: inverse temperature for both stages [0,10]
        - xi0: baseline lapse probability (choice noise) [0,1]
        - delta: anxiety gain on lapse; xi = clip(xi0 * (1 + delta*(stai-0.5)), 0, 1)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_Q, alpha_M, beta, xi0, delta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition model T[a, s]
    T = np.full((2, 2), 0.5)

    # Stage-2 Q-values
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    # Anxiety-modulated lapse
    xi = xi0 * (1.0 + delta * (stai_val - 0.5))
    if xi < 0.0:
        xi = 0.0
    if xi > 1.0:
        xi = 1.0

    for t in range(n_trials):
        # Compute MB values using learned transitions
        max_q2 = np.max(Q2, axis=1)     # value of best action in each state
        q1_mb = T @ max_q2              # expected value over learned transitions

        # Stage-1 policy with lapse
        q1c = q1_mb - np.max(q1_mb)
        ps1 = np.exp(beta * q1c)
        ps1 = ps1 / (np.sum(ps1) + eps)
        ps1 = (1.0 - xi) * ps1 + xi * 0.5  # mix with uniform
        a1 = int(action_1[t])
        p_choice_1[t] = ps1[a1]

        # Stage-2 policy with lapse within the reached state
        s = int(state[t])
        q2c = Q2[s] - np.max(Q2[s])
        ps2 = np.exp(beta * q2c)
        ps2 = ps2 / (np.sum(ps2) + eps)
        ps2 = (1.0 - xi) * ps2 + xi * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = ps2[a2]

        r = reward[t]

        # Update transitions for the chosen action a1 toward observed state s
        # One-hot target for state
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_M * (target - T[a1, sp])
        # Keep rows normalized within numeric limits
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, :] = T[a1, :] / row_sum

        # Stage-2 MF learning
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_Q * pe2

    neg_log_lik = -(np.sum(np.log(p_choice_1 + 1e-10)) + np.sum(np.log(p_choice_2 + 1e-10)))
    return neg_log_lik

","['alpha_Q', 'alpha_M', 'beta', 'xi0', 'delta']"
iter3_run0_participant28.json,cognitive_model3,354.583717443301,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-curvature utility, anxiety-shaped asymmetry, and forgetting.
    
    A model-free learner updates stage-2 values using a nonlinear utility function
    u(r) = r^curv (concave if curv<1). Learning is asymmetric for positive vs negative
    prediction errors, with asymmetry scaled by anxiety. Values undergo forgetting
    toward a neutral prior (0.5). A single inverse temperature governs both stages,
    and its effective strength is reduced for higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=A, 1=U.
    state : array-like of int (0 or 1)
        Second-stage states: 0=X, 1=Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Reward outcome.
    stai : array-like with one float in [0,1]
        Anxiety score; increases learning asymmetry and softens choice.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, c_asym, forget, curv]
        - alpha: base learning rate [0,1]
        - beta: inverse temperature baseline [0,10]
        - c_asym: scales asymmetry via anxiety: 
                  alpha_plus = alpha * (1 + c_asym*stai), 
                  alpha_minus = alpha * (1 - c_asym*stai) [0,1]
        - forget: forgetting rate toward 0.5 for all Qs each trial [0,1]
        - curv: utility curvature for rewards in [0,1], u = r**curv [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, c_asym, forget, curv = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective inverse temperature decreases with anxiety (no extra parameter)
    beta_eff = beta * (1.0 - 0.5 * (stai_val - 0.5))
    if beta_eff < 1e-6:
        beta_eff = 1e-6

    # Q-values
    Q1 = np.zeros(2)        # MF stage-1 values
    Q2 = np.zeros((2, 2))   # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    # Compute asymmetric learning rates as functions of stai
    alpha_plus = alpha * (1.0 + c_asym * stai_val)
    alpha_minus = alpha * (1.0 - c_asym * stai_val)
    if alpha_plus > 1.0:
        alpha_plus = 1.0
    if alpha_minus < 0.0:
        alpha_minus = 0.0

    for t in range(n_trials):
        # Apply forgetting toward 0.5 prior
        Q1 = (1.0 - forget) * Q1 + forget * 0.5
        Q2 = (1.0 - forget) * Q2 + forget * 0.5

        # Stage-1 policy (MF)
        q1c = Q1 - np.max(Q1)
        ps1 = np.exp(beta_eff * q1c)
        ps1 = ps1 / (np.sum(ps1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = ps1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2c = Q2[s] - np.max(Q2[s])
        ps2 = np.exp(beta_eff * q2c)
        ps2 = ps2 / (np.sum(ps2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = ps2[a2]

        # Utility-transformed reward
        r = reward[t]
        u = r ** curv

        # Stage-2 update with asymmetry
        pe2 = u - Q2[s, a2]
        a2_lr = alpha_plus if pe2 >= 0.0 else alpha_minus
        Q2[s, a2] += a2_lr * pe2

        # Stage-1 bootstrapped MF update from realized stage-2 value
        pe1 = Q2[s, a2] - Q1[a1]
        a1_lr = alpha_plus if pe1 >= 0.0 else alpha_minus
        Q1[a1] += a1_lr * pe1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + 1e-10)) + np.sum(np.log(p_choice_2 + 1e-10)))
    return neg_log_lik","['alpha', 'beta', 'c_asym', 'forget', 'curv']"
iter3_run0_participant29.json,cognitive_model1,409.18994713781615,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid planning with anxiety-modulated temperature and transition-contingent bias.
    The agent blends model-free and model-based values at the first stage. Anxiety increases choice determinism (inverse temperature)
    and amplifies a transition-contingent bias to repeat after common transitions and switch after rare transitions.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state (0/1).
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like containing one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for both stages
        beta_base: [0,10] â base inverse temperature
        plan_gain: [0,1] â weight on model-based (transition-expected) values at stage 1
        rare_bias: [0,1] â strength of transition-contingent perseveration (+after common, âafter rare)
        anx_beta: [0,1] â degree to which anxiety scales beta (higher stai -> higher beta)

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta_base, plan_gain, rare_bias, anx_beta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure: rows are actions (A=0, U=1); cols are states (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Transition-contingent perseveration memory
    last_a1 = None
    last_common = None  # whether the last transition was common (True) or rare (False)

    eps = 1e-12
    for t in range(n_trials):
        # Effective inverse temperature with anxiety scaling
        beta = beta_base * (1.0 + anx_beta * stai_val)

        # Model-based component at stage 1: expect max over second-stage actions under transitions
        max_q2 = np.max(q2, axis=1)  # values of states X and Y
        q1_mb = T @ max_q2  # expected values for A and U

        # Combine MF and MB at stage 1
        q1 = (1.0 - plan_gain) * q1_mf + plan_gain * q1_mb

        # Transition-contingent perseveration bias from previous trial
        bias = np.zeros(2)
        if last_a1 is not None and last_common is not None:
            # If last transition was common, bias to repeat; if rare, bias to switch
            signed_bias = rare_bias * (1.0 + stai_val) * (1.0 if last_common else -1.0)
            bias[last_a1] += signed_bias

        # Stage 1 policy
        logits1 = q1 + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(beta * logits1)
        denom1 = np.sum(probs1) + eps
        probs1 /= denom1
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (pure MF on current state's Q-values)
        s = state[t]
        logits2 = q2[s].copy()
        logits2 -= np.max(logits2)
        probs2 = np.exp(beta * logits2)
        denom2 = np.sum(probs2) + eps
        probs2 /= denom2
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Learning updates
        # Stage 2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward the obtained second-stage action value (SARSA-style backup)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update memory for transition type
        # Common if (A->X) or (U->Y); rare otherwise
        last_common = (a1 == s)  # because A=0->X=0, U=1->Y=1
        last_a1 = a1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta_base', 'plan_gain', 'rare_bias', 'anx_beta']"
iter3_run0_participant29.json,cognitive_model2,333.9359893644844,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""PearceâHall associability with anxiety-modulated learning rate and stickiness.
    The agent adapts its learning rate trial-by-trial based on recent surprise (absolute prediction error).
    Anxiety increases the associability gain and decreases effective temperature (more randomness).
    A choice stickiness term biases repeating the previous action at each stage.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state (0/1).
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like containing one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha_min: [0,1] â baseline learning rate floor
        beta: [0,10] â inverse temperature when stai=0
        k_ph: [0,1] â associability gain scaling (how much surprise boosts learning rate)
        stickiness: [0,1] â tendency to repeat previous choices at each stage
        anx_temp: [0,1] â how much anxiety reduces effective beta (increases randomness)

    Returns
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha_min, beta_base, k_ph, stickiness, anx_temp = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Associability (surprise trace) per second-stage state
    assoc = np.zeros(2)  # for states X and Y

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness memory
    last_a1 = None
    last_a2 = np.array([None, None], dtype=object)

    eps = 1e-12
    for t in range(n_trials):
        # Effective temperature decreases with anxiety (more randomness): beta_eff = beta / (1 + anx_temp*stai)
        beta_eff = beta_base / (1.0 + anx_temp * stai_val)

        # Stage 1 logits with stickiness
        logits1 = q1.copy()
        if last_a1 is not None:
            logits1[last_a1] += stickiness
        logits1 -= np.max(logits1)
        probs1 = np.exp(beta_eff * logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 logits with stickiness within current state
        s = state[t]
        logits2 = q2[s].copy()
        if last_a2[s] is not None:
            logits2[last_a2[s]] += stickiness * 0.5  # weaker stickiness at stage 2
        logits2 -= np.max(logits2)
        probs2 = np.exp(beta_eff * logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Surprise (PE) at stage 2
        pe2 = r - q2[s, a2]
        # Update associability trace for current state (slow decay and accumulation of |PE|)
        assoc[s] = 0.9 * assoc[s] + 0.1 * abs(pe2)

        # Trial-specific learning rate with anxiety-amplified associability
        lr_t = alpha_min + k_ph * assoc[s] * (1.0 + stai_val)
        lr_t = 0.0 if lr_t < 0.0 else (1.0 if lr_t > 1.0 else lr_t)

        # Stage 2 update
        q2[s, a2] += lr_t * pe2

        # Stage 1 update towards the obtained second-stage value (SARSA-style)
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += lr_t * pe1

        # Update stickiness memory
        last_a1 = a1
        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha_min', 'beta_base', 'k_ph', 'stickiness', 'anx_temp']"
iter3_run0_participant29.json,cognitive_model3,461.08698099338545,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive strategy gating: anxiety and reward-rate modulated MB/MF mixture.
    The agent mixes model-based and model-free control at stage 1 with a dynamic gate w_t in [0,1].
    The gate increases with recent reward rate and with anxiety (anxious participants rely more on the structured model).
    Stage 2 is model-free. Reward rate is tracked by an exponential moving average.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state (0/1).
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like containing one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for MF values
        beta: [0,10] â inverse temperature
        gate_bias: [0,1] â baseline tendency toward model-based control (mapped to logits)
        rr_sensitivity: [0,1] â weight of reward-rate on the gate
        anx_gate: [0,1] â how strongly anxiety shifts the gate toward model-based

    Returns
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, gate_bias, rr_sensitivity, anx_gate = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Reward-rate tracker
    rr = 0.5  # initialize at neutral 0.5
    rr_decay = 0.9  # fixed forgetting (outside parameter budget)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    for t in range(n_trials):
        # Compute model-based action values
        max_q2 = np.max(q2, axis=1)  # state values
        q1_mb = T @ max_q2

        # Gate combining MF and MB at stage 1: w_t = sigmoid(logit(gate_bias) + rr_sensitivity*(rr-0.5) + anx_gate*stai)
        # Map gate_bias in [0,1] to logit space safely
        gb = min(max(gate_bias, eps), 1.0 - eps)
        logit_gb = np.log(gb) - np.log(1.0 - gb)
        gate_logit = logit_gb + rr_sensitivity * (rr - 0.5) + anx_gate * stai_val
        w_t = 1.0 / (1.0 + np.exp(-gate_logit))
        w_t = 0.0 if w_t < 0.0 else (1.0 if w_t > 1.0 else w_t)

        q1 = (1.0 - w_t) * q1_mf + w_t * q1_mb

        # Stage 1 policy
        logits1 = q1 - np.max(q1)
        probs1 = np.exp(beta * logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (MF)
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # MF learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update reward-rate estimate
        rr = rr_decay * rr + (1.0 - rr_decay) * r

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha', 'beta', 'gate_bias', 'rr_sensitivity', 'anx_gate']"
iter3_run0_participant3.json,cognitive_model1,395.9978102312673,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-damped model-based control with learned transitions and MF backup.

    Core ideas:
    - Stage-2 values (Q2) are learned via TD(0).
    - The transition model T(a1 -> s2) is learned with a dedicated learning rate.
    - Stage-1 combines model-based (using learned T) and model-free Q-values.
    - Anxiety reduces reliance on model-based control (linear damping of MB weight).
    - Perseveration bias at both stages.

    Parameters and bounds:
    - action_1: int array of shape (n_trials,), values in {0,1}; first-stage choices
    - state:    int array of shape (n_trials,), values in {0,1}; reached second-stage state
    - action_2: int array of shape (n_trials,), values in {0,1}; second-stage choices
    - reward:   float array of shape (n_trials,), rewards in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        eta   in [0,1]: learning rate for stage-2 values and stage-1 MF backup
        beta  in [0,10]: inverse temperature for softmax choice at both stages
        tau_T in [0,1]: learning rate for the transition model T
        omega in [0,1]: baseline weight on model-based control at stage-1
        kappa in [0,1]: perseveration strength added to the previously chosen action

    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """"""
    eta, beta, tau_T, omega, kappa = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize learned transition matrix (rows: first-stage action, cols: second-stage state)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Action probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)       # model-free Q at stage-1
    q2 = np.zeros((2, 2))     # stage-2 Q(s2, a2)

    prev_a1 = -1
    prev_a2 = -1
    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q at stage-1 via learned transitions and current Q2
        max_q2 = np.max(q2, axis=1)          # value of each second-stage state
        q1_mb = T @ max_q2                   # MB projection to stage-1 actions

        # Anxiety reduces MB reliance: omega_eff is damped by anxiety
        # Higher anxiety -> lower omega_eff
        omega_eff = omega * (1.0 - 0.7 * s_anx)
        omega_eff = np.clip(omega_eff, 0.0, 1.0)

        q1 = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # Perseveration features
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Stage-1 policy
        logits1 = beta * q1 + kappa * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0

        logits2 = beta * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model for the chosen first-stage action
        # Move that action's row toward the observed next state (one-hot)
        onehot_s = np.array([1.0 if i == s_idx else 0.0 for i in range(2)])
        T[a1] = (1.0 - tau_T) * T[a1] + tau_T * onehot_s
        # Keep normalization numerically stable
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Stage-2 TD update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += eta * delta2

        # Stage-1 MF backup:
        # (i) bootstrap toward the reached second-stage action value
        delta1_mf = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += eta * delta1_mf
        # (ii) propagate stage-2 RPE as an eligibility-like term (fixed strength)
        q1_mf[a1] += eta * delta2

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta', 'beta', 'tau_T', 'omega', 'kappa']"
iter3_run0_participant3.json,cognitive_model2,inf,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive utility with decay and anxiety-driven lapses (pure model-based planning).

    Core ideas:
    - Rewards are transformed by a concave/convex utility with curvature shaped by anxiety.
    - Stage-2 values are learned on utility, not raw reward.
    - Forgetting/decay pulls Q2 toward zero each trial to capture volatility.
    - Choices are softmax with an epsilon-lapse component that increases with anxiety.
    - Stage-1 is purely model-based using fixed transition structure.

    Parameters and bounds:
    - action_1: int array (n_trials,), {0,1}
    - state:    int array (n_trials,), {0,1}
    - action_2: int array (n_trials,), {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        alpha in [0,1]: learning rate for stage-2 on utility outcomes
        beta  in [0,10]: inverse temperature for softmax at both stages
        decay in [0,1]: forgetting factor applied to Q2 each trial
        theta in [0,1]: base utility curvature; combined with anxiety to set utility exponent
        lapse in [0,1]: base lapse rate; effective lapse increases with anxiety

    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """"""
    alpha, beta, decay, theta, lapse = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2))

    # Utility exponent shaped by anxiety:
    # As anxiety rises, curvature shifts toward (1 - theta), ensuring sensitivity changes with stai.
    gamma = theta * (1.0 - s_anx) + (1.0 - theta) * s_anx
    # Keep gamma in a safe numeric range
    gamma = float(np.clip(gamma, 0.05, 1.5))

    # Effective lapse increases with anxiety
    eps_lapse = lapse * (0.5 + 0.5 * s_anx)
    eps_lapse = float(np.clip(eps_lapse, 0.0, 0.5))  # keep lapse modest

    eps = 1e-12

    for t in range(n_trials):
        # Stage-1 MB values from current Q2 (no MF component)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 policy with lapse
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        softmax1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        probs1 = (1.0 - eps_lapse) * softmax1 + eps_lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapse
        s_idx = state[t]
        logits2 = beta * q2[s_idx]
        logits2 -= np.max(logits2)
        softmax2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        probs2 = (1.0 - eps_lapse) * softmax2 + eps_lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Apply forgetting to all Q2 values (toward 0)
        q2 *= (1.0 - decay)

        # Utility-transformed reward and TD update
        r = reward[t]
        u = (r ** gamma)
        delta2 = u - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'decay', 'theta', 'lapse']"
iter3_run0_participant3.json,cognitive_model3,402.07813550526566,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-representation hybrid with anxiety-driven novelty bonus at stage-2.

    Core ideas:
    - Learn a 2x2 successor-like mapping M(a1 -> s2) via simple prediction learning
      (approximates the transition structure but allows gradual adaptation).
    - Stage-1 values blend SR-derived values and MF values with weight w_sr.
    - Stage-2 values are learned via TD(0).
    - An anxiety-scaled novelty bonus encourages exploration of infrequently chosen
      second-stage actions: bonus ~ xi * stai / sqrt(visit_count+1).
    - Perseveration at stage-1.

    Parameters and bounds:
    - action_1: int array (n_trials,), {0,1}
    - state:    int array (n_trials,), {0,1}
    - action_2: int array (n_trials,), {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        alpha in [0,1]: learning rate for both SR (M) and stage-2 Q-values
        beta  in [0,10]: inverse temperature for softmax at both stages
        w_sr in [0,1]: weight on SR-derived values at stage-1 (vs MF)
        xi   in [0,1]: scale for the novelty/exploration bonus at stage-2
        pi   in [0,1]: perseveration strength at stage-1

    Returns:
    - Negative log-likelihood of observed first- and second-stage actions.
    """"""
    alpha, beta, w_sr, xi, pi = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize SR-like mapping with the nominal transitions
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: a1, cols: s2

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    visit = np.zeros((2, 2))  # counts for novelty at stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    eps = 1e-12

    for t in range(n_trials):
        # SR-based stage-1 value by projecting max Q2 through M
        max_q2 = np.max(q2, axis=1)   # value of states
        q1_sr = M @ max_q2

        # Blend SR and MF values
        q1 = w_sr * q1_sr + (1.0 - w_sr) * q1_mf

        # Stage-1 perseveration
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        logits1 = beta * q1 + pi * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with anxiety-driven novelty bonus
        s_idx = state[t]
        bonus = np.zeros(2)
        # Bonus is larger for less-visited actions and scales with anxiety
        for a in range(2):
            bonus[a] = xi * s_anx / np.sqrt(visit[s_idx, a] + 1.0)

        logits2 = beta * q2[s_idx] + bonus
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update SR mapping M for chosen a1 toward observed state
        onehot_s = np.array([1.0 if i == s_idx else 0.0 for i in range(2)])
        M[a1] = (1.0 - alpha) * M[a1] + alpha * onehot_s
        # Renormalize row
        M[a1] = M[a1] / (np.sum(M[a1]) + eps)

        # Stage-2 TD update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

        # Update MF stage-1 with both bootstrap and an eligibility-like term
        delta1 = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1
        q1_mf[a1] += alpha * delta2

        # Update visit counts after observing the choice
        visit[s_idx, a2] += 1.0

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'w_sr', 'xi', 'pi']"
iter3_run0_participant31.json,cognitive_model1,497.6759437934444,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB-MF with learned transitions and anxiety-weighted uncertainty aversion at stage 1.

    This model learns a model-free (MF) Q at both stages and a model-based (MB) planner that
    uses a learned transition matrix. First-stage action values are a convex combination
    of MF and MB values. Anxiety simultaneously (a) lowers reliance on MB control and
    (b) adds an uncertainty-avoidance penalty proportional to the transition entropy of
    each first-stage action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; W/S on X; P/H on Y).
    reward : array-like of float
        Obtained rewards (typically 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values reduce MB weighting and increase
        uncertainty aversion at stage 1.
    model_parameters : array-like of floats, length 5
        [alpha, beta, omega0, chi_anx, nu]
        - alpha in [0,1]: learning rate for MF Q-values (both stages).
        - beta in [0,10]: inverse temperature for both stages' softmax.
        - omega0 in [0,1]: baseline MB weight at stage 1 before anxiety effects.
        - chi_anx in [0,1]: strength with which anxiety reduces MB weight and increases
                            uncertainty aversion.
        - nu in [0,1]: learning rate for the transition matrix (row for chosen action).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, omega0, chi_anx, nu = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective inverse temperature bounded
    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Anxiety-modulated MB weight: higher anxiety -> less MB
    omega_eff = omega0 * (1.0 - chi_anx * stai_val)
    if omega_eff < 0.0:
        omega_eff = 0.0
    if omega_eff > 1.0:
        omega_eff = 1.0

    # Uncertainty aversion weight applied to transition entropy at stage 1
    u_weight = chi_anx * stai_val  # in [0,1]

    # Initialize learned transition matrix T[a, s'] (rows sum to 1); start with a mild prior toward common mapping
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float) * 0.5 + 0.5 * np.array([[0.5, 0.5],
                                                                     [0.5, 0.5]], dtype=float)

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage value via current learned transitions
        max_q2 = np.max(q2_mf, axis=1)  # shape (2,)
        q1_mb = T @ max_q2  # shape (2,)

        # Transition uncertainty (Shannon entropy per action)
        # H(p) = -sum p log p; clamp to avoid log(0)
        pA = T[0]
        pU = T[1]
        eps = 1e-12
        H_A = -(pA[0] * np.log(pA[0] + eps) + pA[1] * np.log(pA[1] + eps))
        H_U = -(pU[0] * np.log(pU[0] + eps) + pU[1] * np.log(pU[1] + eps))
        H_vec = np.array([H_A, H_U])

        # Combine MB and MF with uncertainty aversion penalty
        q1_comb = (1.0 - omega_eff) * q1_mf + omega_eff * q1_mb - u_weight * H_vec

        # Stage 1 policy
        logits1 = beta_eff * q1_comb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (pure MF)
        s = int(state[t])
        logits2 = beta_eff * q2_mf[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # MF learning
        # TD for stage 1 toward second-stage value (SARSA style bootstrapping on chosen a2 within visited state)
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # TD for stage 2 toward reward
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Learn transitions for chosen action toward observed state
        # One-hot target for observed next state
        target = np.array([1.0 if k == s else 0.0 for k in range(2)])
        T[a1] = (1.0 - nu) * T[a1] + nu * target
        # Normalize to maintain stochasticity
        T[a1] = T[a1] / (np.sum(T[a1]) + 1e-12)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega0', 'chi_anx', 'nu']"
iter3_run0_participant31.json,cognitive_model2,442.4442084687777,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free SARSA(Î») with anxiety-amplified loss aversion in the utility of outcomes.

    This purely model-free account uses an eligibility-trace to propagate second-stage
    reward prediction errors to the first-stage values. Rewards are transformed through
    a loss-averse utility; anxiety increases the loss-aversion parameter, effectively
    making unrewarded outcomes more punishing.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; W/S on X; P/H on Y).
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values increase loss aversion.
    model_parameters : array-like of floats, length 5
        [alpha, beta, lam, zeta_anx, rho]
        - alpha in [0,1]: learning rate for Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - lam in [0,1]: eligibility trace mixing of second-stage PE into first-stage value update.
        - zeta_anx in [0,1]: strength by which anxiety increases loss aversion.
        - rho in [0,1]: baseline loss-aversion for zero outcomes in utility transform.

        Utility transform:
          u(r) = r - rho_eff * (1 - r),  where rho_eff = rho + zeta_anx * stai.
        Thus u(1) = 1; u(0) = -rho_eff.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, lam, zeta_anx, rho = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    rho_eff = rho + zeta_anx * stai_val
    if rho_eff < 0.0:
        rho_eff = 0.0
    if rho_eff > 1.0:
        rho_eff = 1.0

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Policies
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        s = int(state[t])
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Utility-transformed reward
        r_raw = float(reward[t])
        u = r_raw - rho_eff * (1.0 - r_raw)

        # Second-stage TD
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # First-stage TD with eligibility trace lam mixing
        # Combine immediate bootstrapping term (q2 value) and the experienced utility via delta2
        # Equivalent to standard SARSA(Î») with two steps:
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * ((1.0 - lam) * delta1 + lam * delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lam', 'zeta_anx', 'rho']"
iter3_run0_participant31.json,cognitive_model3,532.112467062205,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Distorted-model planner with anxiety-modulated second-stage decisional noise.

    First-stage choices are computed from a model-based planner using a transition model
    that blends the objective common-rare structure with the learned MF values downstream.
    Anxiety increases reliance on a 'canonical' common-rare prior (i.e., more distortion toward
    0.7/0.3 transitions), and separately increases second-stage decisional noise.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state.
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values increase second-stage noise and
        increase distortion toward canonical transitions at stage 1.
    model_parameters : array-like of floats, length 5
        [alpha, beta1, beta2, k_anx_temp, mu]
        - alpha in [0,1]: learning rate for second-stage MF Q-values.
        - beta1 in [0,10]: inverse temperature for first-stage softmax.
        - beta2 in [0,10]: baseline inverse temperature for second-stage softmax.
        - k_anx_temp in [0,1]: anxiety scaling that reduces effective beta2 (more noise with anxiety).
        - mu in [0,1]: strength of anxiety-driven distortion of the transition model toward
                       the canonical common-rare structure at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta1, beta2, k_anx_temp, mu = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Clamp temperatures
    beta1_eff = beta1
    if beta1_eff < 0.0:
        beta1_eff = 0.0
    if beta1_eff > 10.0:
        beta1_eff = 10.0

    beta2_eff = beta2 * (1.0 - k_anx_temp * stai_val)
    if beta2_eff < 0.0:
        beta2_eff = 0.0
    if beta2_eff > 10.0:
        beta2_eff = 10.0

    # Canonical transition matrix for two-step task
    T_canonical = np.array([[0.7, 0.3],
                            [0.3, 0.7]], dtype=float)

    # Start with a neutral learned transition (equal) but we will not learn it here; instead distort toward canonical
    T_neutral = np.array([[0.5, 0.5],
                          [0.5, 0.5]], dtype=float)

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Anxiety-driven distortion coefficient toward canonical structure
        c = mu * stai_val  # in [0,1]
        # Distorted transition used for planning
        T_tilde = (1.0 - c) * T_neutral + c * T_canonical

        # First-stage MB values based on expected max Q at each planet
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T_tilde @ max_q2

        # Stage 1 policy
        logits1 = beta1_eff * q1_mb
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Stage 2 policy (anxiety increases noise)
        s = int(state[t])
        logits2 = beta2_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Second-stage learning from reward
        r = float(reward[t])
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta1', 'beta2', 'k_anx_temp', 'mu']"
iter3_run0_participant32.json,cognitive_model2,337.42724078618596,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Asymmetric (win/loss) learning with anxiety-modulated forgetting and choice persistence.
    
    Core idea:
    - Model-free learner with separate learning rates for positive vs. negative second-stage prediction errors.
    - Anxiety increases global forgetting of Q-values (toward a neutral prior), reflecting reduced confidence/maintenance.
    - Persistence (choice stickiness) at both stages, scaled up by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices (alien) per trial.
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [mu_win, mu_loss, beta, z_forget, psi_persist]
        Bounds:
        - mu_win: [0,1] learning rate used when the second-stage prediction error is positive.
        - mu_loss: [0,1] learning rate used when the second-stage prediction error is negative.
        - beta: [0,10] inverse temperature for both stages.
        - z_forget: [0,1] forgetting strength per trial, scaled by anxiety.
        - psi_persist: [0,1] baseline stickiness (persistence) strength, scaled by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    mu_win, mu_loss, beta, z_forget, psi_persist = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Model-free values
    q1_mf = np.zeros(2)        # values for first-stage actions A/U
    q2 = np.zeros((2, 2))      # values for second-stage state x action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_state = {0: None, 1: None}

    # Neutral prior to forget toward
    prior_q1 = 0.5
    prior_q2 = 0.5

    for t in range(n_trials):
        # Anxiety-scaled persistence
        stick = psi_persist * (0.5 + 0.5 * stai)

        # Stage 1 policy with stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        logits1 = beta * (q1_mf - np.max(q1_mf)) + stick * bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with within-state stickiness
        s = state[t]
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2_state[s] is not None:
            bias2[prev_a2_state[s]] = 1.0
        logits2 = beta * (q2_s - np.max(q2_s)) + stick * bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 with asymmetric rates
        r = reward[t]
        pe2 = r - q2[s, a2]
        lr2 = mu_win if pe2 >= 0.0 else mu_loss
        q2[s, a2] += lr2 * pe2

        # Update stage 1 model-free value via bootstrapping from second-stage value
        pe1 = q2[s, a2] - q1_mf[a1]
        lr1 = mu_win if pe1 >= 0.0 else mu_loss
        q1_mf[a1] += lr1 * pe1

        # Anxiety-modulated forgetting toward neutral priors
        f = np.clip(z_forget * (0.5 + 0.5 * stai), 0.0, 1.0)
        q1_mf = (1.0 - f) * q1_mf + f * prior_q1
        q2 = (1.0 - f) * q2 + f * prior_q2

        prev_a1 = a1
        prev_a2_state[s] = a2

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['mu_win', 'mu_loss', 'beta', 'z_forget', 'psi_persist']"
iter3_run0_participant32.json,cognitive_model3,382.3012899529935,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Hybrid MB/MF with anxiety-modulated subjective transition bias and surprise-controlled temperature.
    
    Core idea:
    - First-stage values combine model-based and model-free components.
    - Subjective transition matrix has a tunable 'common-transition' bias reduced by anxiety.
    - Softmax temperature dynamically increases on surprising (rare) transitions, more so with higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Reward on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha, beta_base, kappa_temp, nu_mb, bias_common]
        Bounds:
        - alpha: [0,1] learning rate for both stages (shared).
        - beta_base: [0,10] baseline inverse temperature.
        - kappa_temp: [0,1] gain for increasing temperature on surprising transitions.
        - nu_mb: [0,1] weight on model-based values (1.0 => purely MB).
        - bias_common: [0,1] strength of subjective bias toward common transitions at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta_base, kappa_temp, nu_mb, bias_common = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Subjective transition matrix (fixed form but tunable ""commonness"")
    # Anxiety reduces the bias toward common transitions (moves toward 0.5/0.5)
    c = 0.7 + 0.2 * bias_common * (1.0 - stai)  # in [0.7, 0.9] scaled down by anxiety
    c = np.clip(c, 0.5, 0.99)
    T_subj = np.array([[c, 1.0 - c],
                       [1.0 - c, c]])

    # Values
    q1_mf = np.zeros(2)      # model-free values for A/U
    q2 = np.zeros((2, 2))    # second-stage values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_subj @ max_q2

        # Combine MB and MF for policy
        q1 = nu_mb * q1_mb + (1.0 - nu_mb) * q1_mf

        # Determine surprise of current transition to modulate temperature
        # Use probability of the actually reached state under subjective transitions.
        a1 = action_1[t]
        s = state[t]
        p_obs = T_subj[a1, s]
        surprise = 1.0 - p_obs  # large when rare transition occurs
        beta_t = beta_base * np.exp(kappa_temp * surprise * (1.0 + stai))

        # Stage 1 choice probability
        logits1 = beta_t * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (use same beta_t for coherence with surprise/arousal)
        q2_s = q2[s]
        logits2 = beta_t * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # Stage 2 delta
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage 1 model-free update bootstrapping from current second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik","['alpha', 'beta_base', 'kappa_temp', 'nu_mb', 'bias_common']"
iter3_run0_participant34.json,cognitive_model1,465.9702060550933,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based choice with anxiety-amplified uncertainty aversion at planning time.
    
    The agent plans using the known transition structure but subtracts a penalty
    for uncertainty in each second-stage state. Uncertainty is tracked online as
    an exponential moving estimate of reward variance for each alien. Anxiety
    increases aversion to this uncertainty, shifting choices away from first-stage
    actions that commonly lead to uncertain states. Second-stage behavior is
    standard softmax. A model-free first-stage value is still learned (alpha)
    though planning here relies on the uncertainty-penalized model-based values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, xi0, k_anx_xi, alpha2]
        - alpha in [0,1]: learning rate for first-stage model-free value (bootstraps from stage 2).
        - beta in [0,10]: inverse temperature (shared across stages).
        - xi0 in [0,1]: baseline uncertainty aversion weight.
        - k_anx_xi in [0,1]: how strongly anxiety increases uncertainty aversion.
        - alpha2 in [0,1]: learning rate for second-stage values and uncertainty.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, xi0, k_anx_xi, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective uncertainty aversion weight, grows with anxiety
    xi_eff = xi0 + k_anx_xi * stai_val
    if xi_eff < 0.0:
        xi_eff = 0.0

    # Fixed transition structure: rows are actions (A=0, U=1); columns are states (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Q-values
    q1_mf = np.zeros(2, dtype=float)         # First-stage model-free values
    q2 = np.zeros((2, 2), dtype=float)       # Second-stage action values: state x action

    # Exponential moving estimate of reward variance per alien
    var2 = np.zeros((2, 2), dtype=float)

    for t in range(n_trials):
        # Compute uncertainty per state as the max alien uncertainty in that state
        # Use sqrt(var) (i.e., std) as uncertainty measure to keep units close to reward scale
        std_state = np.sqrt(np.maximum(0.0, np.max(var2, axis=1)))  # shape (2,)

        # Model-based first-stage values: expected max Q2 minus uncertainty penalty
        v_state_penalized = np.max(q2, axis=1) - xi_eff * std_state  # shape (2,)
        q1_mb = T @ v_state_penalized  # shape (2,)

        # First-stage policy (purely model-based with uncertainty penalty)
        logits1 = q1_mb.copy()
        logits1 -= np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (standard softmax)
        s = int(state[t])
        logits2 = q2[s].copy()
        logits2 -= np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = float(reward[t])

        # Second stage TD learning and uncertainty tracking
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Update variance estimate with squared TD error as innovation
        var2[s, a2] = (1.0 - alpha2) * var2[s, a2] + alpha2 * (delta2 ** 2)

        # First-stage model-free bootstrap from chosen second-stage action
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'xi0', 'k_anx_xi', 'alpha2']"
iter3_run0_participant34.json,cognitive_model2,431.9424737784908,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with learned transitions and anxiety-sensitive surprise gating.
    
    The agent learns transition probabilities from experience and blends model-based
    and model-free action values at the first stage. After a surprising transition
    on the previous trial, high-anxiety agents rely less on model-based control on
    the next trial (surprise gating). Second-stage choices are softmax over Q-values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, kappa, w_base, eta_anx]
        - alpha in [0,1]: TD learning rate (used for both stages).
        - beta in [0,10]: inverse temperature (shared across stages).
        - kappa in [0,1]: learning rate for transition probabilities P(state | action).
        - w_base in [0,1]: baseline weight on model-based control at stage 1.
        - eta_anx in [0,1]: strength by which anxiety reduces MB weight after a rare transition.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, kappa, w_base, eta_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Learned transition model initialized to uniform uncertainty
    T_hat = np.full((2, 2), 0.5, dtype=float)

    # Ground-truth common transitions to determine surprise (rare vs common)
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    q1_mf = np.zeros(2, dtype=float)
    q2 = np.zeros((2, 2), dtype=float)

    # Surprise from previous trial (0 for first trial)
    prev_rare = 0.0

    for t in range(n_trials):
        # Compute model-based first-stage values using learned transitions
        max_q2 = np.max(q2, axis=1)        # value of each state
        q1_mb = T_hat @ max_q2             # shape (2,)

        # Surprise-gated MB weight for this trial
        w_eff = w_base - eta_anx * stai_val * prev_rare
        if w_eff < 0.0: 
            w_eff = 0.0
        if w_eff > 1.0:
            w_eff = 1.0

        # Hybrid first-stage policy
        q1_combined = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        logits1 = q1_combined - np.max(q1_combined)
        probs1 = np.exp(beta * logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = int(state[t])
        logits2 = q2[s].copy()
        logits2 -= np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Learning: second stage
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning: first stage model-free bootstrap
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Learn transition model for chosen action using simple delta rule
        # Move probability of observed state toward 1, other state toward 0
        T_hat[a1, s] += kappa * (1.0 - T_hat[a1, s])
        other_s = 1 - s
        T_hat[a1, other_s] += kappa * (0.0 - T_hat[a1, other_s])

        # Compute whether current transition was rare under true structure
        # Rare if went to the uncommon state for the chosen action
        is_rare = 1.0 if ((a1 == 0 and s == 1) or (a1 == 1 and s == 0)) else 0.0
        prev_rare = is_rare  # used to gate MB weight on the next trial

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'kappa', 'w_base', 'eta_anx']"
iter3_run0_participant34.json,cognitive_model3,426.91359222617723,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""MB/MF mixture with anxiety-specific exploration at stage 2 and second-stage stickiness.
    
    The agent blends model-based and model-free values at the first stage using a
    fixed mixture weight. At the second stage, anxiety selectively increases choice
    randomness (lower inverse temperature) while choices also exhibit within-state
    stickiness (tendency to repeat the same alien the next time that planet is reached).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, z_stai, psi2, chi]
        - alpha in [0,1]: TD learning rate for both stages.
        - beta in [0,10]: base inverse temperature.
        - z_stai in [0,1]: scales how much anxiety reduces second-stage beta:
                          beta2 = beta * (1 - z_stai * stai).
        - psi2 in [0,1]: second-stage stickiness weight added to the previously
                         chosen action in that state (perseveration bias).
        - chi in [0,1]: weight on model-based control in first-stage decision (MB/MF mix).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, z_stai, psi2, chi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition model for planning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Effective second-stage inverse temperature decreases with anxiety
    beta2 = beta * (1.0 - z_stai * stai_val)
    if beta2 < 1e-6:
        beta2 = 1e-6  # avoid zero or negative

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    q1_mf = np.zeros(2, dtype=float)
    q2 = np.zeros((2, 2), dtype=float)

    # Track last chosen action for each state to implement stickiness
    prev_a2 = np.full(2, -1, dtype=int)

    for t in range(n_trials):
        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)   # value of each state
        q1_mb = T @ max_q2

        # Mixed first-stage values
        q1_mix = chi * q1_mb + (1.0 - chi) * q1_mf

        # First-stage policy with base beta
        logits1 = q1_mix - np.max(q1_mix)
        probs1 = np.exp(beta * logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with anxiety-modulated beta and stickiness
        s = int(state[t])
        logits2 = q2[s].copy()
        # Add within-state perseveration bias
        if prev_a2[s] >= 0:
            logits2[prev_a2[s]] += psi2
        logits2 -= np.max(logits2)
        probs2 = np.exp(beta2 * logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Learning at second stage
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning at first stage (model-free bootstrap from second stage)
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update stickiness memory
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'z_stai', 'psi2', 'chi']"
iter3_run0_participant35.json,cognitive_model1,445.3344470208739,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-bonus MF-MB hybrid with anxiety-modulated exploration and forgetting.

    Core ideas
    - Second-stage values are learned model-free with an uncertainty-tracking bonus (UCB-like).
    - First-stage mixes model-based (via known transition matrix) and model-free values.
    - Anxiety increases directed exploration (larger uncertainty bonus) and increases forgetting.
      Anxiety also reduces reliance on model-based planning.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage action on each trial (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage action on each trial (aliens per state)
    - reward: array-like of floats in [0,1], obtained reward each trial
    - stai: array-like length-1, trait anxiety score in [0,1]
    - model_parameters: tuple/list of 5 parameters
        alpha0: base learning rate for Q2 in [0,1]
        beta: inverse temperature for both stages in [0,10]
        kappa_ucb: base weight of uncertainty bonus at stage 2 in [0,1]
        trust_base: base reliance on model-based evaluation at stage 1 in [0,1]
        decay: base forgetting rate per trial in [0,1]

    How anxiety is used
    - Directed exploration bonus scales as: kappa_eff = kappa_ucb * (1 + stai)
      (more anxious -> more exploration)
    - Model-based reliance reduced by anxiety:
      w_mb = clip(trust_base * (1 - 0.5*stai), 0, 1)
    - Forgetting increases with anxiety:
      decay_eff = clip(decay * (0.5 + 0.5*stai), 0, 1)

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha0, beta, kappa_ucb, trust_base, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)           # model-free first-stage values
    q2 = np.zeros((2, 2))         # second-stage action values for each state

    # Uncertainty tracker for Q2 (per state-action), initialized moderately uncertain
    u2 = np.full((2, 2), 0.5, dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated settings
    w_mb = min(1.0, max(0.0, trust_base * (1.0 - 0.5 * stai)))
    kappa_eff = kappa_ucb * (1.0 + stai)
    decay_eff = min(1.0, max(0.0, decay * (0.5 + 0.5 * stai)))

    for t in range(n_trials):
        # Compute model-based first-stage values from current Q2 (no bonus in planning)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Mix MB and MF for stage-1 decision values
        q1_decision = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Softmax for stage 1
        centered_q1 = q1_decision - np.max(q1_decision)
        logits1 = beta * centered_q1
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2: add UCB-like uncertainty bonus to chosen state's action values
        s = int(state[t])
        bonus2 = kappa_eff * u2[s]  # per-action exploration bonus
        q2_with_bonus = q2[s] + bonus2

        centered_q2 = q2_with_bonus - np.max(q2_with_bonus)
        logits2 = beta * centered_q2
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]

        # Second stage update with learning rate boosted by uncertainty (bounded in [0,1])
        alpha2_eff = max(0.0, min(1.0, alpha0 + kappa_ucb * u2[s, a2]))
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2_eff * pe2

        # Update uncertainty: reduction where we updated, gentle diffusion elsewhere
        # Reduction proportional to learning (more learning -> less uncertainty)
        u2[s, a2] = (1.0 - alpha2_eff) * u2[s, a2]
        # Diffusion toward higher uncertainty for all entries (bounded)
        u2 = (1.0 - decay_eff) * u2 + decay_eff * 0.01

        # First stage MF bootstrapping toward realized second-stage action value (no bonus)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use same alpha base for MF update to keep parameter economy
        q1_mf[a1] += alpha0 * pe1

        # Forgetting on Q-values
        q2 *= (1.0 - decay_eff)
        q1_mf *= (1.0 - decay_eff)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'kappa_ucb', 'trust_base', 'decay']"
iter3_run0_participant35.json,cognitive_model2,422.16014569214894,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning MB-MF arbitration with anxiety-modulated volatility and stickiness.

    Core ideas
    - Learns both reward values (Q2) and the transition matrix (T) online.
    - Model-based strength increases with transition confidence, but anxiety dampens MB reliance.
    - Anxious participants are assumed to perceive higher transition volatility, thus learn T faster.
    - Perseveration (action stickiness) increases with anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage action (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage action
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, anxiety score in [0,1]
    - model_parameters: tuple/list of 5 parameters
        alpha_r: reward learning rate for Q2 in [0,1]
        beta: inverse temperature for both stages in [0,10]
        alpha_T: base transition learning rate in [0,1]
        w_conf_base: base factor scaling MB arbitration by transition confidence in [0,1]
        stick_base: base perseveration strength in [0,1]

    How anxiety is used
    - Transition learning rate: alpha_T_eff = clip(alpha_T * (1 + 0.5*stai), 0, 1)
    - MB weight: w_mb = clip(w_conf_base * conf * (1 - 0.4*stai), 0, 1)
      where conf is the mean deviation of learned T from 0.5 (higher = more confident).
    - Stickiness increases with anxiety: stick = stick_base * (1 + 0.5*stai)

    Returns
    - Negative log-likelihood of the observed choices.
    """"""
    alpha_r, beta, alpha_T, w_conf_base, stick_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize transition model close to known structure but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # For stickiness
    last_a1 = None
    stick = stick_base * (1.0 + 0.5 * stai)
    alpha_T_eff = min(1.0, max(0.0, alpha_T * (1.0 + 0.5 * stai)))

    for t in range(n_trials):
        # Compute confidence from transition model: average |p - 0.5|
        conf = 0.5 * (abs(T[0, 0] - 0.5) + abs(T[1, 1] - 0.5))
        w_mb = min(1.0, max(0.0, w_conf_base * conf * (1.0 - 0.4 * stai)))

        # Model-based values via current learned T
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add perseveration bias to stage-1 logits
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stick

        q1_decision = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias1

        centered_q1 = q1_decision - np.max(q1_decision)
        logits1 = beta * centered_q1
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (no stickiness)
        s = int(state[t])
        centered_q2 = q2[s] - np.max(q2[s])
        logits2 = beta * centered_q2
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Reward update
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # First-stage MF update toward realized Q2
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

        # Learn transition probabilities via EMA toward observed next state
        # If we chose a1 and observed s, move T[a1] toward one-hot of s
        if s == 0:
            T[a1, 0] = (1.0 - alpha_T_eff) * T[a1, 0] + alpha_T_eff * 1.0
            T[a1, 1] = 1.0 - T[a1, 0]
        else:
            T[a1, 1] = (1.0 - alpha_T_eff) * T[a1, 1] + alpha_T_eff * 1.0
            T[a1, 0] = 1.0 - T[a1, 1]

        # Keep rows normalized and within bounds
        T[a1] = np.clip(T[a1], 0.01, 0.99)
        T[a1] /= np.sum(T[a1])

        # Update last action
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'alpha_T', 'w_conf_base', 'stick_base']"
iter3_run0_participant36.json,cognitive_model1,466.1908050233647,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 1: Volatility-adaptive hybrid with anxiety-modulated exploration and perseveration.
    
    Overview
    --------
    The agent learns second-stage action values (Q2) with a standard TD rule and backs up a model-free
    first-stage value (Q1_mf). A fixed transition model (0.7 common, 0.3 rare) supports model-based
    evaluation (Q1_mb). First-stage choice uses a hybrid of MB and MF values.
    
    Novel mechanisms:
      - Volatility-adaptive inverse temperature: beta is down-regulated as estimated reward volatility
        increases, and further reduced by anxiety (stai).
      - Perseveration bias at both stages to capture choice stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int {0,1}
        Second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the observed state.
    reward : array-like of float
        Rewards (typically 0 or 1).
    stai : array-like of float
        Anxiety score; stai[0] in [0,1]. Higher anxiety reduces effective exploration temperature.
    model_parameters : list or array
        [alpha, beta, k_vol, w_MB, stickiness]
        - alpha in [0,1]: learning rate for Q updates (both stages).
        - beta in [0,10]: base inverse temperature for both stages.
        - k_vol in [0,1]: volatility learning rate controlling sensitivity to reward PE variance.
        - w_MB in [0,1]: weight on model-based values at stage 1 (1=fully MB).
        - stickiness in [0,1]: strength of choice perseveration bias at both stages.
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta_base, k_vol, w_MB, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (A->X, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Volatility estimator (exponential moving average of squared PEs)
    v = 0.0

    # Previous choices for perseveration
    prev_a1 = 0
    prev_a2 = 0

    # Anxiety-modulated exploration: higher stai reduces baseline beta; scale in [0,1]
    beta_base_eff = beta_base * (1.0 - 0.5 * stai)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based evaluation for stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid combination
        Q1 = w_MB * Q1_mb + (1.0 - w_MB) * Q1_mf

        # Volatility-adaptive beta (down-regulate when v high, and with anxiety)
        beta_t = beta_base_eff / (1.0 + v)

        # Add perseveration biases (additive to logits)
        bias1 = np.array([0.0, 0.0])
        bias1[prev_a1] += stickiness

        # Stage-1 policy
        logits1 = beta_t * (Q1 - np.max(Q1)) + bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration bias
        bias2 = np.array([0.0, 0.0])
        bias2[prev_a2] += stickiness
        logits2 = beta_t * (Q2[s] - np.max(Q2[s])) + bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Second stage TD update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update volatility estimate with squared PE
        v = (1.0 - k_vol) * v + k_vol * (pe2 * pe2)

        # First stage MF backup from realized second-stage value
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)

","['alpha', 'beta_base', 'k_vol', 'w_MB', 'stickiness']"
iter3_run0_participant39.json,cognitive_model1,486.9297487812456,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid RL with learned transitions and anxiety-weighted surprise credit assignment.
    
    The agent learns:
      - Second-stage Q-values model-free.
      - First-stage transition probabilities from experience.
      - First-stage action values from both model-based planning using the learned transitions
        and model-free bootstrapping. The latter is boosted by transition surprise (how
        unexpected the reached planet was), and this surprise effect increases with anxiety.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), second-stage states (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; e.g., alien choices)
    - reward:   np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha_val:   [0,1] learning rate for second-stage values and stage-1 MF values
        alpha_tr:    [0,1] learning rate for transition model
        beta:        [0,10] inverse temperature for both stages
        w_plan0:     [0,1] baseline weight on model-based control
        surpr0:      [0,1] baseline weight on surprise-boosted MF credit at stage 1

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_val, alpha_tr, beta, w_plan0, surpr0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transition model T[a, s'].
    # Start neutral at 0.5/0.5 for each action.
    T = np.full((2, 2), 0.5)

    # Model-free Q-values
    q1_mf = np.zeros(2)         # first-stage MF values for A/U
    q2_mf = np.zeros((2, 2))    # second-stage MF values within each planet

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects:
    # - Reduce planning weight (higher anxiety -> less planning).
    w_plan = np.clip(w_plan0 * (1.0 - stai0), 0.0, 1.0)
    # - Increase surprise impact on MF credit.
    surpr_w = np.clip(surpr0 * (0.5 + 0.5 * stai0), 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):
        a1 = int(action_1[t])

        # Model-based first-stage values via current transition model and max second-stage values
        max_q2 = np.max(q2_mf, axis=1)           # shape (2,)
        q1_mb = T @ max_q2                       # shape (2,)

        # Hybrid first-stage action values
        q1 = w_plan * q1_mb + (1.0 - w_plan) * q1_mf

        # Softmax policy for first-stage
        q1c = q1 - np.max(q1)  # stability
        p1 = np.exp(beta * q1c)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Second-stage policy at observed state
        s2 = int(state[t])
        a2 = int(action_2[t])

        q2 = q2_mf[s2]
        q2c = q2 - np.max(q2)
        p2 = np.exp(beta * q2c)
        p2 = p2 / (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Outcome
        r = float(reward[t])

        # Transition model update for the chosen first-stage action using a simple delta rule
        # Move probability mass toward observed state, away from the other.
        for sp in (0, 1):
            target = 1.0 if sp == s2 else 0.0
            T[a1, sp] += alpha_tr * (target - T[a1, sp])
        # Renormalize against numerical drift
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

        # Second-stage MF update
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha_val * pe2

        # First-stage MF update with surprise-weighted bootstrapping target
        # Surprise is 1 - P(reached state | chosen action) using pre-update T estimate.
        # We approximated with post-update T; to keep the signal bounded and meaningful,
        # compute surprise from the previous probabilities by inverting the update step:
        # As a practical proxy, we use surpr = 1 - T[a1, s2] after update; it remains in [0,1].
        surpr = 1.0 - T[a1, s2]
        td_target1 = q2_mf[s2, a2] * (1.0 + surpr_w * surpr)
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha_val * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_val', 'alpha_tr', 'beta', 'w_plan0', 'surpr0']"
iter3_run0_participant39.json,cognitive_model2,453.8908906399199,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-representation hybrid with anxiety-modulated trace and second-stage stickiness.
    
    The agent:
      - Learns a simple successor map M[a1, s2] (probability of landing on each planet
        given a spaceship) and uses it to compute model-based values q1_sr = M @ max(Q2).
      - Also learns model-free values at both stages, with an eligibility trace that
        propagates second-stage prediction errors to first-stage MF values.
      - First-stage choice uses a mix of SR-based and MF values.
      - Second-stage choices include a perseveration bias that increases with anxiety.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha:    [0,1] learning rate for values and SR map
        beta:     [0,10] inverse temperature for both stages
        lambda0:  [0,1] baseline eligibility trace from stage 2 PE to stage 1 MF
        w_sr0:    [0,1] baseline weight on SR-based planning at stage 1
        stick2:   [0,1] baseline perseveration strength for second-stage choices

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, lambda0, w_sr0, stick2 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Successor representation for one-step environment ~ transition model
    M = np.full((2, 2), 0.5)  # start neutral

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    # - SR/map learning slightly reduced with anxiety
    alpha_sr = np.clip(alpha * (1.0 - 0.5 * stai0), 0.0, 1.0)
    # - Planning weight reduced with anxiety
    w_sr = np.clip(w_sr0 * (1.0 - stai0), 0.0, 1.0)
    # - Eligibility trace increased with anxiety (faster MF credit assignment)
    lam = np.clip(lambda0 * (1.0 + stai0), 0.0, 1.0)
    # - Second-stage perseveration increased with anxiety
    k2 = stick2 * (1.0 + stai0)

    prev_a2 = None

    eps = 1e-12
    for t in range(n_trials):
        a1 = int(action_1[t])

        # SR-based planning value (uses current map M)
        max_q2 = np.max(q2_mf, axis=1)   # shape (2,)
        q1_sr = M @ max_q2               # shape (2,)

        # First-stage hybrid value
        q1 = w_sr * q1_sr + (1.0 - w_sr) * q1_mf

        # First-stage policy
        q1c = q1 - np.max(q1)
        p1 = np.exp(beta * q1c)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Second-stage policy with perseveration bias
        s2 = int(state[t])
        a2 = int(action_2[t])

        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] = 1.0
        q2 = q2_mf[s2] + k2 * bias2

        q2c = q2 - np.max(q2)
        p2 = np.exp(beta * q2c)
        p2 = p2 / (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Outcome
        r = float(reward[t])

        # Update SR map row for chosen a1 toward observed state (akin to transitions)
        for sp in (0, 1):
            target = 1.0 if sp == s2 else 0.0
            M[a1, sp] += alpha_sr * (target - M[a1, sp])
        # Renormalize
        row_sum = M[a1].sum()
        if row_sum > 0:
            M[a1] /= row_sum

        # Second-stage MF update
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # First-stage MF eligibility-trace update driven by second-stage PE
        q1_mf[a1] += alpha * lam * pe2

        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'lambda0', 'w_sr0', 'stick2']"
iter3_run0_participant39.json,cognitive_model3,498.12940199746845,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-bonus exploration with anxiety-modulated lapse and forgetting.
    
    The agent:
      - Learns second-stage values model-free with learning rate alpha.
      - Tracks per-alien volatility via running unsigned prediction errors and adds an
        exploration bonus proportional to volatility (bonus decreases with anxiety).
      - Uses fixed, known transitions (common .7) to plan model-based first-stage values
        from second-stage values plus exploration bonus.
      - Includes a lapse component in both stages that increases with anxiety.
      - Applies value forgetting/decay that grows with anxiety.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha:       [0,1] learning rate for value and volatility traces
        beta:        [0,10] inverse temperature for both stages
        lapse0:      [0,1] baseline lapse probability (uniform-choice mixing)
        decay0:      [0,1] baseline forgetting toward priors
        vol_bonus0:  [0,1] baseline weight on volatility bonus

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, lapse0, decay0, vol_bonus0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition structure
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Values and volatility traces
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))
    vol = np.zeros((2, 2))  # running unsigned PEs for each alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    lapse = np.clip(lapse0 * (0.2 + 0.8 * stai0), 0.0, 0.5)  # cap lapse to <=0.5 for identifiability
    decay = np.clip(decay0 * (1.0 + 0.5 * stai0), 0.0, 1.0)
    vol_w = np.clip(vol_bonus0 * (1.0 - stai0), 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):
        # Apply forgetting before choice
        # Stage-2 values decay toward 0.5 prior; stage-1 MF toward 0 prior.
        q2_mf = (1.0 - decay) * q2_mf + decay * 0.5
        q1_mf = (1.0 - decay) * q1_mf

        # Model-based value from second-stage values plus exploration bonus
        max_q2_bonus = np.max(q2_mf + vol_w * vol, axis=1)  # shape (2,)
        q1_mb = T_known @ max_q2_bonus

        # Combine MB with MF (simple average weighted by confidence in MF via 1 - decay)
        w_mb = 0.5 + 0.5 * (1.0 - stai0)  # more planning when less anxious
        w_mb = np.clip(w_mb, 0.0, 1.0)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage action probabilities with lapse
        q1c = q1 - np.max(q1)
        p1_soft = np.exp(beta * q1c)
        p1_soft = p1_soft / (np.sum(p1_soft) + eps)
        p1 = (1.0 - lapse) * p1_soft + lapse * 0.5
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Second-stage action probabilities with volatility bonus and lapse
        s2 = int(state[t])
        q2b = q2_mf[s2] + vol_w * vol[s2]
        q2c = q2b - np.max(q2b)
        p2_soft = np.exp(beta * q2c)
        p2_soft = p2_soft / (np.sum(p2_soft) + eps)
        p2 = (1.0 - lapse) * p2_soft + lapse * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Outcome and learning
        r = float(reward[t])

        # Second-stage PE and updates
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # Volatility trace update (running unsigned PE)
        vol[s2, a2] = (1.0 - alpha) * vol[s2, a2] + alpha * abs(pe2)

        # Simple MF update at stage 1 using the updated second-stage value as bootstrap
        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'lapse0', 'decay0', 'vol_bonus0']"
iter3_run0_participant4.json,cognitive_model1,537.7533451068626,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-modulated exploration with anxiety-sensitive temperature and eligibility trace.

    Overview
    - Stage-2 values learned via RescorlaâWagner (RW).
    - Maintains an uncertainty estimate per state-action from running absolute PEs.
    - Anxiety scales down the effective inverse temperature in proportion to uncertainty (risk sensitivity).
    - An uncertainty-driven optimism/exploration bonus is added to Q-values, attenuated by anxiety.
    - Stage-1 uses a combination of model-based evaluation (via known transitions) and a model-free
      value that is updated by an eligibility trace from stage-2 prediction errors.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:      [0,1]   Learning rate for Q updates (RW) and uncertainty trace.
    - beta:       [0,10]  Base inverse temperature for softmax choice.
    - lambda_e:   [0,1]   Eligibility trace strength for propagating stage-2 PE to stage-1 MF values.
    - tau_ent:    [0,1]   Weight of uncertainty-based exploration bonus added to Q-values.
    - xi_anx:     [0,1]   Strength with which anxiety and uncertainty reduce effective beta.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, lambda_e, tau_ent, xi_anx].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, lambda_e, tau_ent, xi_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common=0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # actions (A,U) x states (X,Y)

    # Value and uncertainty initialization
    q2 = np.zeros((2, 2)) + 0.5       # stage-2 Q(s, a2)
    u2 = np.zeros((2, 2)) + 0.25      # stage-2 uncertainty proxy (running |PE|); start moderately uncertain
    q1_mf = np.zeros(2) + 0.0         # stage-1 model-free values

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Uncertainty-driven exploration bonus, attenuated by anxiety
        bonus2 = tau_ent * (1.0 - stai) * u2  # per state-action
        # Effective temperature reduced by anxiety-weighted uncertainty (state-average for stability)
        mean_u_state = float(np.mean(u2[s]))
        beta2_eff = beta * (1.0 - xi_anx * stai * mean_u_state)
        beta2_eff = max(1e-3, beta2_eff)

        # Stage-2 policy
        logits2 = beta2_eff * (q2[s] + bonus2[s])
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Stage-1 model-based evaluation uses expected max over second stage including bonus
        max_q2_with_bonus = np.max(q2 + bonus2, axis=1)  # per state
        q1_mb = transition_matrix @ max_q2_with_bonus

        # Stage-1 effective temperature reduced by expected uncertainty along transitions
        exp_u = transition_matrix @ np.mean(u2, axis=1)
        beta1_eff = beta * (1.0 - xi_anx * stai * float(exp_u[a1]))
        beta1_eff = max(1e-3, beta1_eff)

        # Combine MB and MF (equal weight, no extra parameter) for choice
        q1_comb = 0.5 * q1_mb + 0.5 * q1_mf
        logits1 = beta1_eff * q1_comb
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Learning updates
        pe2 = r - q2[s, a2]
        # Stage-2 RW update
        q2[s, a2] += alpha * pe2
        # Update uncertainty proxy with running absolute PE
        u2[s, a2] = (1.0 - alpha) * u2[s, a2] + alpha * abs(pe2)
        # Eligibility trace to stage-1 MF
        q1_mf[a1] += alpha * lambda_e * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['alpha', 'beta', 'lambda_e', 'tau_ent', 'xi_anx']"
iter3_run0_participant4.json,cognitive_model2,430.7671738644001,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-weighted arbitration between model-based and model-free control with forgetting and repetition bias.

    Overview
    - Learns stage-2 values via RW with value forgetting toward 0.5.
    - Stage-1 combines MB and MF values using a dynamic weight w_t computed from:
        â¢ MB reliability (contrast between planets' best options).
        â¢ Transition surprise (rare vs common).
      Anxiety increases the impact of surprise (shifting toward MF) and reduces the impact of MB reliability.
    - Repetition bias (choice kernel) for both stages.
    
    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:       [0,1]   Learning rate for Q updates.
    - beta:        [0,10]  Inverse temperature for softmax.
    - chi_forget:  [0,1]   Forgetting rate pulling Q2 toward 0.5 each trial.
    - phi_arbit:   [0,1]   Strength of arbitration modulation by reliability and surprise with anxiety scaling.
    - kappa_rep:   [0,1]   Repetition bias strength for both stages.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, chi_forget, phi_arbit, kappa_rep].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, chi_forget, phi_arbit, kappa_rep = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2) + 0.0

    # Repetition kernels (logit bias)
    rep1 = np.zeros(2)
    rep2 = np.zeros((2, 2))
    last_a1 = None
    last_a2 = np.array([-1, -1])  # per state

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply forgetting toward 0.5 to stage-2
        q2 = (1.0 - chi_forget) * q2 + chi_forget * 0.5

        # Repetition biases (decay old bias slightly)
        rep1 *= (1.0 - 0.5 * kappa_rep)
        rep2[s] *= (1.0 - 0.5 * kappa_rep)
        if last_a1 is not None:
            rep1[last_a1] += kappa_rep
        if last_a2[s] in (0, 1):
            rep2[s, last_a2[s]] += kappa_rep

        # Stage-2 policy
        logits2 = beta * q2[s] + rep2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Model-based evaluation for stage-1
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = transition_matrix @ max_q2

        # Arbitration weight w_t in [0,1]
        reliability = abs(max_q2[0] - max_q2[1])  # contrast between planets
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        surprise = 1 - is_common  # 1 if rare

        # Compute logit for w with opposing effects of reliability and surprise, anxiety-weighted
        base_logit = 0.0  # centered at 0 -> w ~ 0.5 when no signals
        logit_w = base_logit \
                  + phi_arbit * (1.0 - stai) * (reliability - 0.5) \
                  - phi_arbit * stai * surprise
        w_t = 1.0 / (1.0 + np.exp(-logit_w))
        w_t = min(1.0, max(0.0, w_t))

        # Combine MB and MF for stage-1 choice
        q1_comb = w_t * q1_mb + (1.0 - w_t) * q1_mf
        logits1 = beta * q1_comb + rep1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2
        q1_mf[a1] += alpha * pe2  # eligibility propagation

        # Update last choices
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['alpha', 'beta', 'chi_forget', 'phi_arbit', 'kappa_rep']"
iter3_run0_participant4.json,cognitive_model3,556.493237474409,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Valence-asymmetric learning with anxiety-modulated negativity bias and adaptive transition-belief plus lapse.

    Overview
    - Separate learning rates for positive vs. negative PEs at stage 2.
    - Anxiety increases learning from negative outcomes and reduces learning from positive outcomes.
    - Maintains an adaptive belief about transition commonness for each first-stage action (p_common^A, p_common^U),
      updated from observed transitions; model-based values use these beliefs.
    - Includes an anxiety-scaled lapse at both stages.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha_pos:  [0,1]   Base learning rate for positive PEs.
    - alpha_neg:  [0,1]   Base learning rate for negative PEs.
    - beta:       [0,10]  Inverse temperature for softmax.
    - theta_trans:[0,1]   Learning rate for updating transition commonness beliefs.
    - rho_lapse:  [0,1]   Base lapse rate; effective lapse scales with anxiety.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha_pos, alpha_neg, beta, theta_trans, rho_lapse].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha_pos, alpha_neg, beta, theta_trans, rho_lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Stage-2 values and stage-1 MF values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2) + 0.0

    # Transition belief: p_common for each first-stage action
    # For action A: P(X|A) = p_common_A, P(Y|A) = 1 - p_common_A
    # For action U: P(Y|U) = p_common_U, P(X|U) = 1 - p_common_U
    p_common_A = 0.7
    p_common_U = 0.7

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Anxiety-modulated valence asymmetry
        alpha_pos_eff = alpha_pos * (1.0 - 0.5 * stai)
        alpha_neg_eff = alpha_neg * (1.0 + 0.5 * stai)
        alpha_pos_eff = min(1.0, max(0.0, alpha_pos_eff))
        alpha_neg_eff = min(1.0, max(0.0, alpha_neg_eff))

        # Lapse proportional to anxiety
        lapse = min(0.2, rho_lapse * stai)

        # Build current transition matrix from beliefs
        # Row 0 corresponds to A, row 1 to U; columns X(0), Y(1)
        trans_A = np.array([p_common_A, 1.0 - p_common_A])
        trans_U = np.array([1.0 - p_common_U, p_common_U])
        transition_matrix = np.vstack([trans_A, trans_U])

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p2[t] = probs2[a2]

        # Stage-1 model-based values using current beliefs
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2
        # Combine MB and MF without extra parameter by simple averaging
        q1_comb = 0.5 * q1_mb + 0.5 * q1_mf

        logits1 = beta * q1_comb
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p1[t] = probs1[a1]

        # Learning updates
        pe2 = r - q2[s, a2]
        if pe2 >= 0.0:
            q2[s, a2] += alpha_pos_eff * pe2
            q1_mf[a1] += alpha_pos_eff * pe2
        else:
            q2[s, a2] += alpha_neg_eff * pe2
            q1_mf[a1] += alpha_neg_eff * pe2

        # Update transition belief for chosen first-stage action based on whether transition was common
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        target = float(is_common)  # 1 if common observed, 0 if rare
        if a1 == 0:
            p_common_A = (1.0 - theta_trans) * p_common_A + theta_trans * target
        else:
            p_common_U = (1.0 - theta_trans) * p_common_U + theta_trans * target

        # Keep beliefs within [0.05, 0.95] to avoid degeneracy
        p_common_A = min(0.95, max(0.05, p_common_A))
        p_common_U = min(0.95, max(0.05, p_common_U))

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)","['alpha_pos', 'alpha_neg', 'beta', 'theta_trans', 'rho_lapse']"
iter3_run0_participant40.json,cognitive_model1,471.7590529749811,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-gated arbitration with anxiety-weighted exploration and perseveration.
    
    The agent learns second-stage values (Q2) and uses a model-based (MB) plan at stage 1
    using the known transition structure. It also learns a model-free (MF) first-stage value (Q1_MF).
    Uncertainty in second-stage outcomes is tracked per state-action as an exponential running variance.
    Anxiety (stai) increases exploration under uncertainty by reducing the effective inverse temperature
    proportionally to the predicted uncertainty for each action. Stage-1 action values are a convex
    combination of MB and MF values, plus a perseveration bias that is stronger at lower anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, eta, zeta, rho)
        Bounds:
        - alpha2 in [0,1]: learning rate for second-stage Q-values and stage-1 MF backup.
        - beta in [0,10]: inverse temperature baseline for softmax.
        - eta in [0,1]: arbitration weight; 1=model-based, 0=model-free at stage 1.
        - zeta in [0,1]: uncertainty sensitivity scaling the anxiety-gated exploration.
        - rho in [0,1]: perseveration strength at stage 1; is down-weighted by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha2, beta, eta, zeta, rho = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    Q2 = np.zeros((2, 2))       # second-stage values per state-action
    Q1_MF = np.zeros(2)         # model-free first-stage values

    # Running statistics for uncertainty (variance) per state-action
    m = np.zeros((2, 2))        # running mean of rewards
    v = np.zeros((2, 2))        # running variance proxy of rewards

    prev_a1 = None

    for t in range(n_trials):
        # Model-based stage-1 values via transition model and current Q2
        max_Q2 = np.max(Q2, axis=1)         # value of each state by best alien
        Q1_MB = T @ max_Q2                  # expected value for each spaceship

        # Arbitration between MB and MF
        Q1_mix = eta * Q1_MB + (1.0 - eta) * Q1_MF

        # Anxiety-weighted perseveration bias (stronger when anxiety is low)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = rho * (1.0 - stai_val)

        # Uncertainty-dependent temperature at stage 1:
        # uncertainty for each action = expected state-uncertainty (max action var per state)
        state_unc = np.max(v, axis=1)  # per-state uncertainty summary
        unc_a = T @ state_unc          # expected uncertainty per first-stage action
        beta1_eff_per_action = beta / (1.0 + zeta * stai_val * (unc_a + 1e-8))

        # Softmax with per-action temperatures -> implement as action-specific scaling of Q
        # Equivalently, compute logits_i = beta_i * Q_i + bias_i
        logits1 = beta1_eff_per_action * Q1_mix + bias1
        # stabilize
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: uncertainty-gated temperature at visited state
        s2 = int(state[t])
        a2 = int(action_2[t])

        beta2_eff = beta / (1.0 + 0.5 * zeta * stai_val * (np.max(v[s2]) + 1e-8))
        logits2 = beta2_eff * Q2[s2]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage values
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Update running mean and variance proxy for uncertainty tracking
        # Mean update with alpha2
        m_old = m[s2, a2]
        m[s2, a2] = (1.0 - alpha2) * m_old + alpha2 * r
        # Variance proxy using exponential smoothing of squared deviation
        dev = r - m_old
        v[s2, a2] = (1.0 - alpha2) * v[s2, a2] + alpha2 * (dev * dev)

        # Model-free first-stage backup from realized second-stage chosen value
        target1 = Q2[s2, a2]
        delta1 = target1 - Q1_MF[a1]
        Q1_MF[a1] += alpha2 * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'eta', 'zeta', 'rho']"
iter3_run0_participant5.json,cognitive_model1,413.47720589471874,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-modulated hybrid with eligibility trace and perseveration.

    Idea:
    - Stage 1 mixes model-based (MB) and model-free (MF) values.
    - Anxiety diminishes MB arbitration weight and enhances perseveration.
    - Credit assignment to stage-1 MF values is eligibility-trace-like and
      further reduced after rare transitions, especially under anxiety.
    - Stage 2 uses standard Q-learning.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received (e.g., 0 or 1).
    stai : array-like of float in [0,1]
        Participant anxiety score; higher values reduce MB weight and increase
        perseveration and rare-transition gating.
    model_parameters : array-like of float
        [alpha0, beta, mix0, elig, persever0]
        - alpha0 in [0,1]: baseline learning rate for Q updates (both stages).
        - beta in [0,10]: inverse temperature (both stages).
        - mix0 in [0,1]: baseline MB arbitration weight (scale reduced by anxiety).
        - elig in [0,1]: eligibility-trace intensity for credit assignment to stage-1 MF.
        - persever0 in [0,1]: baseline perseveration strength (scaled up by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha0, beta, mix0, elig, persever0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common.
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    # Values
    q1_mf = np.zeros(2)          # stage-1 MF values for A/U
    q2 = 0.5 * np.ones((2, 2))   # stage-2 values for aliens on planets X/Y

    # Perseveration biases
    bias1 = np.zeros(2)
    bias2 = np.zeros((2, 2))

    # Anxiety-adjusted arbitration and perseveration
    # Higher anxiety reduces MB weight and increases perseveration.
    w_mb = np.clip(mix0 * (1.0 - 0.6 * st), 0.0, 1.0)
    kappa = persever0 * (0.3 + 0.7 * st)
    # Eligibility trace intensity (scaled mildly up by anxiety)
    e_eff = (0.1 + 0.9 * elig) * (0.7 + 0.3 * st)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    last_a1 = None
    last_s = None
    last_a2 = None

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 MB action-values via one-step lookahead
        max_q2 = np.max(q2, axis=1)             # best alien on each planet
        q1_mb = transition_matrix @ max_q2      # expected value from each spaceship

        # Combine MB and MF plus stickiness
        q1_comb = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias1

        # Stage-1 policy
        logits1 = beta * (q1_comb - np.max(q1_comb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage-2 policy (with within-state perseveration)
        q2_biased = q2[s] + bias2[s]
        logits2 = beta * (q2_biased - np.max(q2_biased))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Determine whether the observed transition was common or rare
        # For A=0 common->X=0, U=1 common->Y=1
        is_common = (a1 == s)

        # Update Stage-2 values (standard Q-learning)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha0 * pe2

        # Stage-1 MF credit assignment uses an eligibility-like gate
        # Rare transitions down-weight the back-propagation more when anxiety is high.
        rare_gate = (1.0 - 0.8 * st) if (not is_common) else 1.0
        alpha1_eff = alpha0 * e_eff * np.clip(rare_gate, 0.0, 1.0)

        target1 = q2[s, a2]  # bootstrap from attained state-action value
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1_eff * pe1

        # Update perseveration biases
        bias1[:] = 0.0
        bias1[a1] += kappa

        bias2[:] = 0.0
        bias2[s, a2] += kappa

        last_a1, last_s, last_a2 = a1, s, a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha0', 'beta', 'mix0', 'elig', 'persever0']"
iter3_run0_participant5.json,cognitive_model2,442.2346591593499,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Learned-transitions with UCB exploration, anxiety-weighted.

    Idea:
    - The agent learns both stage-2 rewards and stage-1 transition probabilities.
    - Stage-1 is purely model-based using the learned transition matrix.
    - Stage-2 adds a directed exploration (UCB) bonus inversely related to anxiety.
    - Anxiety reduces prior trust that transitions are strongly common (A->X, U->Y),
      and reduces the exploration bonus magnitude.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher anxiety lowers prior trust in common transitions
        and dampens UCB exploration.
    model_parameters : array-like of float
        [alpha_r, beta, ucb0, trust0]
        - alpha_r in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature (both stages).
        - ucb0 in [0,1]: base strength of UCB exploration bonus at stage 2.
        - trust0 in [0,1]: scales prior trust that A->X and U->Y are common.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha_r, beta, ucb0, trust0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Stage-2 values and visitation counts for UCB
    q2 = 0.5 * np.ones((2, 2))
    n_sa = np.zeros((2, 2))  # visit counts per state-action

    # Dirichlet-like counts for transition learning: counts[action, state]
    counts = np.zeros((2, 2))
    # Anxiety-weighted prior: trust reduces with anxiety (flattening towards 0.5/0.5)
    trust_eff = np.clip(trust0 * (1.0 - 0.8 * st), 0.0, 1.0)
    # Map trust to prior probability for ""common"" transitions: 0.5 -> 0.5, 1.0 -> 0.7
    p_common_prior = 0.5 + 0.2 * trust_eff
    base = 2.0  # symmetric pseudo-count mass
    # For A=0, common state is X=0; for U=1, common state is Y=1
    counts[0, 0] = base * p_common_prior
    counts[0, 1] = base * (1.0 - p_common_prior)
    counts[1, 1] = base * p_common_prior
    counts[1, 0] = base * (1.0 - p_common_prior)

    # Anxiety-weighted UCB scale (anxious explore less)
    ucb_scale = ucb0 * (0.3 + 0.7 * (1.0 - st))

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Update transition counts from previous choice and observed state
        counts[a1, s] += 1.0
        # Current learned transition matrix
        T = counts / np.clip(np.sum(counts, axis=1, keepdims=True), 1.0, None)

        # Stage-1 purely MB using learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        logits1 = beta * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage-2: Q + UCB exploration bonus
        # UCB bonus ~ sqrt(log(t+2) / (n_sa + 1))
        n_sa_term = n_sa[s] + 1.0
        bonus = ucb_scale * np.sqrt(np.log(t + 2.0) / n_sa_term)
        q2_bonus = q2[s] + bonus

        logits2 = beta * (q2_bonus - np.max(q2_bonus))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Update Q2 and visitation counts
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2
        n_sa[s, a2] += 1.0

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha_r', 'beta', 'ucb0', 'trust0']"
iter3_run0_participant5.json,cognitive_model3,407.040604844778,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Meta-control via anxiety-gated temperature, MB weight, and reward forgetting.

    Idea:
    - Stage 1 uses a hybrid of MB and MF, with MB weight higher when anxiety is low.
    - Softmax temperature increases when anxiety is low (more exploitation).
    - Stage 2 values decay (forget) over trials; decay grows with anxiety,
      modeling difficulty maintaining stable reward beliefs under high anxiety.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce beta (more randomness), reduce MB weight,
        and increase forgetting of stage-2 values.
    model_parameters : array-like of float
        [alpha, beta_base, forget, mbw0]
        - alpha in [0,1]: learning rate for Q updates.
        - beta_base in [0,10]: baseline inverse temperature.
        - forget in [0,1]: base forgetting factor for stage-2 values.
        - mbw0 in [0,1]: baseline MB weight at stage 1 (boosted when anxiety is low).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta_base, forget, mbw0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition model (fixed known structure)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Anxiety-gated meta-control
    # Lower anxiety => higher MB weight and higher beta (more deterministic).
    w_mb = np.clip(mbw0 + (1.0 - st) * (1.0 - mbw0) * 0.5, 0.0, 1.0)
    beta_eff = np.clip(beta_base * (0.5 + 0.5 * (1.0 - st)), 0.0, 10.0)
    # Forgetting increases with anxiety
    forget_eff = np.clip(forget * (0.5 + 0.5 * st), 0.0, 1.0)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 MB values from expected max Q2 per planet
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Stage-2 forgetting before update (decay towards 0.5 baseline)
        q2 = (1.0 - forget_eff) * q2 + forget_eff * 0.5

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF learning from attained stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Mild anxiety-driven reduction of stage-1 learning
        alpha1 = alpha * (0.8 + 0.2 * (1.0 - st))
        q1_mf[a1] += alpha1 * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)","['alpha', 'beta_base', 'forget', 'mbw0']"
iter3_run0_participant6.json,cognitive_model1,509.208678065532,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-like planning with MF integration, anxiety-modulated arbitration, and memory decay.

    Core idea:
    - Stage 1 values blend a successor-like (transition-based) planner and a model-free cache.
    - Low anxiety increases planning weight and a structure-based stay/shift bias tied to prior common/rare transitions.
    - Soft forgetting (decay) prevents overcommitment to stale values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, omega, zeta, bias_prev]
        - alpha in [0,1]: learning rate for value updates (both stages).
        - beta in [0,10]: inverse temperature for both stages.
        - omega in [0,1]: base weight on transition-based planning at stage 1.
        - zeta in [0,1]: forgetting rate toward zero for unrefreshed Q-values.
        - bias_prev in [0,1]: magnitude of structure-based stay/shift bias at stage 1:
            after a common rewarded trial, bias to repeat; after a rare punished trial, bias to switch.
            Anxiety reduces the impact of this bias.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, omega, zeta, bias_prev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common = 0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2)        # model-free cache for stage 1 actions
    q2 = np.zeros((2, 2))      # stage 2 action values per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated weights
    omega_eff = np.clip(omega * (1.0 - 0.4 * stai), 0.0, 1.0)
    bias_prev_eff_base = np.clip(bias_prev * (1.0 - stai), 0.0, 1.0)  # lower with anxiety

    # Keep track of previous trial for bias
    a1_prev = None
    s_prev = None
    r_prev = None

    for t in range(n_trials):
        # Planning component from successor-like expectation (transition-based)
        max_q2_by_state = np.max(q2, axis=1)     # best attainable value on each planet
        q1_plan = T @ max_q2_by_state           # expected value of each spaceship

        # Structure-based stay/shift bias from prior trial outcome and transition type
        pref = np.zeros(2)
        if a1_prev is not None:
            was_common = 1 if ((a1_prev == 0 and s_prev == 0) or (a1_prev == 1 and s_prev == 1)) else 0
            sign = 1.0 if r_prev > 0 else (-1.0 if r_prev < 0 else 0.0)
            # Rewarded-common or punished-rare encourages repeating; reverse otherwise
            bias = bias_prev_eff_base * sign * (1.0 if was_common else -1.0)
            # Apply bias as a preference to repeat or switch previous action
            pref[a1_prev] += bias
            pref[1 - a1_prev] -= bias

        # Hybrid stage-1 value
        q1 = omega_eff * q1_plan + (1.0 - omega_eff) * q1_mf + pref

        # Policy stage 1
        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Policy stage 2 (based only on the current state's Q-values)
        s = int(state[t])
        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # Soft forgetting (global) before updates
        q2 *= (1.0 - zeta)
        q1_mf *= (1.0 - zeta)

        # Stage 2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage 1 MF update bootstrapping from the obtained second-stage action value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Bookkeeping for next-trial bias
        a1_prev, s_prev, r_prev = a1, s, r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'omega', 'zeta', 'bias_prev']"
iter3_run0_participant7.json,cognitive_model1,505.90257449903936,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-weighted arbitration with learned transitions, anxiety-modulated planning and lapse.

    Core ideas:
    - Learn second-stage Q-values model-free and first-stage model-free via bootstrapping from stage-2.
    - Learn the transition matrix online; per-action arbitration weight is 1 - entropy(row), i.e., rely more on MB when the row is certain.
    - Anxiety reduces reliance on model-based control when transitions are uncertain and increases lapse (irreducible noise).
    
    Parameters (model_parameters):
    - alpha_r: [0,1] reward learning rate (for both stages and bootstrapping).
    - beta: [0,10] inverse temperature for both stages.
    - alpha_T0: [0,1] base transition learning rate (per-trial delta-rule on T).
    - k_unc: [0,1] anxiety sensitivity; effective MB weight is scaled by (1 - k_unc * stai).
    - lapse: [0,1] base lapse rate; effective lapse increases with anxiety.
    
    Inputs:
    - action_1: int array {0,1}, first-stage choices (0=A, 1=U).
    - state: int array {0,1}, observed second-stage state (0=X, 1=Y).
    - action_2: int array {0,1}, second-stage choices (0=left/alien0, 1=right/alien1).
    - reward: float array, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 floats (alpha_r, beta, alpha_T0, k_unc, lapse).
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_r, beta, alpha_T0, k_unc, lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix (rows: first-stage actions; cols: states)
    T = np.full((2, 2), 0.5)  # start uncertain, will learn toward 0.7/0.3 structure via experience
    # Stage-2 and stage-1 MF values
    Q2 = np.zeros((2, 2))  # Q2[state, action]
    Q1_mf = np.zeros(2)    # MF value for first-stage actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    # Effective transition learning rate can be dampened by anxiety (more anxious -> slower to learn structure)
    alpha_T = alpha_T0 * (1.0 - 0.5 * stai)

    # Effective lapse increases with anxiety
    lapse_eff = min(1.0, lapse * (0.5 + stai))

    for t in range(n_trials):
        # Compute per-action transition uncertainty via entropy (normalized to [0,1])
        # H(p) = -sum p log p / log(2); if p ~ [0.5,0.5] then H=1; if p ~ [1,0] then H=0.
        H_rows = np.zeros(2)
        for a in range(2):
            p_row = np.clip(T[a], eps, 1.0)
            H = -np.sum(p_row * np.log(p_row)) / np.log(2.0)
            H_rows[a] = H  # already in [0,1] for binary
        
        # Model-based Q1 via current transition beliefs
        max_Q2 = np.max(Q2, axis=1)  # best at each state
        Q1_mb = T @ max_Q2

        # Arbitration weight per action: rely more on MB when entropy low; anxiety reduces MB reliance
        w_vec = (1.0 - H_rows) * (1.0 - k_unc * stai)  # element-wise per action
        w_vec = np.clip(w_vec, 0.0, 1.0)
        Q1 = w_vec * Q1_mb + (1.0 - w_vec) * Q1_mf

        # First-stage policy with lapse
        q1c = Q1 - np.max(Q1)
        probs_1_soft = np.exp(beta * q1c)
        probs_1_soft = probs_1_soft / np.sum(probs_1_soft)
        probs_1 = (1.0 - lapse_eff) * probs_1_soft + lapse_eff * 0.5
        a1 = action_1[t]
        p_choice_1[t] = np.clip(probs_1[a1], eps, 1.0)

        # Second-stage policy with lapse
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        probs_2_soft = np.exp(beta * q2c)
        probs_2_soft = probs_2_soft / np.sum(probs_2_soft)
        probs_2 = (1.0 - lapse_eff) * probs_2_soft + lapse_eff * 0.5
        a2 = action_2[t]
        p_choice_2[t] = np.clip(probs_2[a2], eps, 1.0)

        # Outcomes
        r = reward[t]

        # Learn transitions from observed a1 -> s
        # Move the chosen action's row toward one-hot of observed state.
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - alpha_T) * T[a1] + alpha_T * target
        # Ensure normalization (should already hold, but safeguard numerical drift)
        T[a1] = T[a1] / np.sum(T[a1])

        # TD learning at stage 2
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * delta2

        # Stage-1 MF bootstraps from updated stage-2 value
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha_r * delta1

    nll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return nll

","['alpha_r', 'beta', 'alpha_T0', 'k_unc', 'lapse']"
iter3_run0_participant7.json,cognitive_model2,531.6350848358394,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-shaped utility curvature and choice stickiness.

    Core ideas:
    - Stage-2 values learned with a utility transform u(r) = sign(r) * |r|^rho_eff.
    - Anxiety increases concavity (reduces rho_eff), making the agent more risk-averse/sensitive.
    - Stage-1 decision is a mixture of model-based (fixed transitions) and model-free values,
      where MB weight w = 1 - stai (higher anxiety -> less planning).
    - Choice stickiness bias encourages repeating previous actions; stickiness grows with anxiety.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for both stages.
    - beta0: [0,10] base inverse temperature.
    - rho0: [0,1] baseline utility curvature exponent.
    - k_rho: [0,1] strength of anxiety impact on curvature; rho_eff = rho0 * (1 - k_rho * stai).
    - stick: [0,1] baseline stickiness coefficient added to logits; scaled by (1 + stai).

    Inputs:
    - action_1: int array {0,1}, first-stage choice.
    - state: int array {0,1}, second-stage state.
    - action_2: int array {0,1}, second-stage choice.
    - reward: float array, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha, beta0, rho0, k_rho, stick).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta0, rho0, k_rho, stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed known transition structure (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    rho_eff = max(1e-6, rho0 * (1.0 - k_rho * stai))
    w = np.clip(1.0 - stai, 0.0, 1.0)  # MB weight
    stick_eff = stick * (1.0 + stai)   # stronger repetition with anxiety

    # Track previous actions for stickiness
    prev_a1 = -1
    prev_a2_by_state = np.array([-1, -1])

    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid Q1
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # Add stickiness bias to first-stage logits
        bias1 = np.zeros(2)
        if prev_a1 in [0, 1]:
            bias1[prev_a1] += stick_eff

        q1_logits = beta0 * (Q1 - np.max(Q1)) + bias1
        probs_1 = np.exp(q1_logits - np.max(q1_logits))
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = np.clip(probs_1[a1], eps, 1.0)

        # Second stage policy with state-dependent stickiness
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] in [0, 1]:
            bias2[prev_a2_by_state[s]] += stick_eff

        q2_logits = beta0 * (Q2[s] - np.max(Q2[s])) + bias2
        probs_2 = np.exp(q2_logits - np.max(q2_logits))
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = np.clip(probs_2[a2], eps, 1.0)

        # Observe reward; transform via utility
        r = reward[t]
        sign_r = 1.0 if r >= 0.0 else -1.0
        u = sign_r * (abs(r) ** rho_eff)

        # TD learning at stage 2 (on utility)
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF update via bootstrapped utility value
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    nll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return nll

","['alpha', 'beta0', 'rho0', 'k_rho', 'stick']"
iter3_run0_participant7.json,cognitive_model3,505.89636352460815,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive learning via Pearce-Hall associability, anxiety-coupled, with MB/MF hybrid and repetition bias.

    Core ideas:
    - Learning rates adapt to recent surprise: alpha_t = clip(alpha0 + k_pe * stai * |delta2_prev|, 0,1).
      Anxiety amplifies PE-driven associability, enabling faster adaptation under surprising outcomes.
    - Stage-1 decision is a fixed hybrid of MB (known transitions) and MF.
    - Action repetition bias at both stages captures perseveration independent of value.

    Parameters (model_parameters):
    - alpha0: [0,1] base learning rate when no surprise.
    - beta: [0,10] inverse temperature for both stages.
    - k_pe: [0,1] scale of PE-driven associability; multiplied by stai.
    - w_mb: [0,1] fixed weight on model-based control at stage 1.
    - bias_rep: [0,1] repetition bias coefficient added to logits.

    Inputs:
    - action_1: int array {0,1}, first-stage choice.
    - state: int array {0,1}, second-stage state.
    - action_2: int array {0,1}, second-stage choice.
    - reward: float array, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha0, beta, k_pe, w_mb, bias_rep).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha0, beta, k_pe, w_mb, bias_rep = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Repetition memory
    prev_a1 = -1
    prev_a2_by_state = np.array([-1, -1])

    # Previous unsigned PE for associability
    prev_abs_pe = 0.0

    eps = 1e-12

    for t in range(n_trials):
        # Dynamic learning rate from previous surprise (Pearce-Hall style)
        alpha_t = alpha0 + k_pe * stai * prev_abs_pe
        alpha_t = np.clip(alpha_t, 0.0, 1.0)

        # Hybrid stage-1 value
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # First-stage logits with repetition bias
        bias1 = np.zeros(2)
        if prev_a1 in [0, 1]:
            bias1[prev_a1] += bias_rep
        q1_logits = beta * (Q1 - np.max(Q1)) + bias1
        probs_1 = np.exp(q1_logits - np.max(q1_logits))
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = np.clip(probs_1[a1], eps, 1.0)

        # Second-stage logits with state-specific repetition bias
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] in [0, 1]:
            bias2[prev_a2_by_state[s]] += bias_rep
        q2_logits = beta * (Q2[s] - np.max(Q2[s])) + bias2
        probs_2 = np.exp(q2_logits - np.max(q2_logits))
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = np.clip(probs_2[a2], eps, 1.0)

        # Outcome and learning
        r = reward[t]
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_t * delta2

        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha_t * delta1

        # Update memories
        prev_abs_pe = abs(delta2)
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    nll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return nll","['alpha0', 'beta', 'k_pe', 'w_mb', 'bias_rep']"
iter3_run0_participant8.json,cognitive_model1,368.67498324256405,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-shaped arbitration and information-seeking bonus.

    Summary
    -------
    - Stage 2 values (aliens) learned via delta rule.
    - Stage 1 uses a hybrid of model-based (MB) and model-free (MF) values.
    - An information-seeking bonus (uncertainty bonus) is added to the MB value to favor
      planets whose alien rewards are uncertain; this bonus is maximal for medium anxiety.
    - Arbitration weight between MB and MF depends on anxiety (higher anxiety pushes weight
      toward a neutral 0.5, reducing reliance on either extreme).
    - First-stage perseveration bias is reduced by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet (0/1).
    reward : array-like of float
        Trial outcomes (e.g., 0 or 1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [alpha, beta, omega0, tau_info, kappa_pers]
        Bounds:
        - alpha: stage-2 reward learning rate [0,1]
        - beta: inverse temperature for both stages [0,10]
        - omega0: base MB weight for stage-1 arbitration [0,1]
        - tau_info: base strength of information-seeking bonus [0,1]
        - kappa_pers: first-stage perseveration strength [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, omega0, tau_info, kappa_pers = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 action values (planet x alien)
    q2 = np.zeros((2, 2)) + 0.5

    # Stage-1 MF values (for spaceships A/U)
    q1_mf = np.zeros(2)

    prev_a1 = None

    # Anxiety-modulated arbitration:
    # As anxiety increases, pull omega toward 0.5 (less extreme reliance on MB/MF).
    omega = omega0 * (1.0 - st) + 0.5 * st
    omega = np.clip(omega, 0.0, 1.0)

    # Information-seeking bonus: emphasize uncertainty around medium anxiety.
    # Gaussian bump centered at ~0.41 (mid of medium range .31-.51) with width ~0.08.
    bump = np.exp(-((st - 0.41) ** 2) / (2.0 * (0.08 ** 2)))
    info_scale = tau_info * (0.5 + 0.5 * bump)  # in [0,1]

    # Perseveration reduced by anxiety
    persev = kappa_pers * (1.0 - st)

    for t in range(n_trials):
        # Compute per-planet uncertainty bonus: Bernoulli variance q*(1-q) per alien,
        # then take the max within planet to capture targetable information.
        var_per_planet = q2 * (1.0 - q2)
        unc_bonus = np.max(var_per_planet, axis=1)  # shape (2,)

        # Model-based action values via transitions and info bonus
        max_q2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T @ (max_q2 + info_scale * unc_bonus)

        # Combine MB and MF
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += persev

        # Stage 1 policy
        logits1 = q1 + bias1
        logits1 = logits1 - np.max(logits1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update stage-2 values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update stage-1 MF from stage-2 values (one-step bootstrapping)
        # TD target: value of observed second-stage choice
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega0', 'tau_info', 'kappa_pers']"
iter3_run0_participant8.json,cognitive_model2,513.637719600209,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-gated transition learning with anxiety-weighted stochasticity and stage-2 stickiness.

    Summary
    -------
    - Learns transition probabilities T(a1 -> planet) from experience.
    - Stage 1 is purely model-based using learned T and current stage-2 values.
    - Surprise on observed transitions accelerates transition learning and lowers effective
      choice precision; both effects are amplified by anxiety.
    - Stage 2 has a perseveration bias (stickiness) that increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0 or 1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [eta, beta, alpha_T0, zeta, psi_st2]
        Bounds:
        - eta: stage-2 reward learning rate [0,1]
        - beta: base inverse temperature [0,10]
        - alpha_T0: base transition learning rate [0,1]
        - zeta: surprise gain applied to both transition learning and stochasticity [0,1]
        - psi_st2: base second-stage perseveration strength [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    eta, beta, alpha_T0, zeta, psi_st2 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize transitions to uniform uncertainty
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2)) + 0.5

    # Stage-2 perseveration (increases with anxiety)
    stick2 = psi_st2 * (1.0 + 0.5 * st)
    prev_a2_by_state = {0: None, 1: None}
    prev_r_by_state = {0: None, 1: None}  # not used in policy but could be extended

    for t in range(n_trials):
        # Expected impurity over transitions (higher = more uncertainty)
        # Use mean Gini impurity across actions to derive a global beta1_eff
        gini_rows = 1.0 - np.sum(T ** 2, axis=1)  # shape (2,)
        avg_impurity = 0.5 * (gini_rows[0] + gini_rows[1])
        beta1_eff = beta * (1.0 - zeta * st * avg_impurity)
        beta1_eff = max(beta1_eff, 1e-3)

        # Model-based Q at stage 1 using learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 policy
        logits1 = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta1_eff * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = q2[s].copy()
        # Add stage-2 perseveration bias to the previously chosen alien in this state
        if prev_a2_by_state[s] is not None:
            logits2[prev_a2_by_state[s]] += stick2
        logits2 = logits2 - np.max(logits2)
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update stage-2 values
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta * pe2

        # Compute surprise on the observed transition and update transitions
        surpr = 1.0 - T[a1, s]  # high when transition was unlikely
        alpha_T_eff = alpha_T0 + (1.0 - alpha_T0) * (zeta * st * surpr)
        # Move T[a1] toward one-hot of observed state with rate alpha_T_eff
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - alpha_T_eff) * T[a1] + alpha_T_eff * target
        # Normalize to be safe
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        prev_a2_by_state[s] = a2
        prev_r_by_state[s] = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta', 'beta', 'alpha_T0', 'zeta', 'psi_st2']"
iter3_run0_participant8.json,cognitive_model3,447.87052178624685,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Robust (pessimistic) planner with anxiety-weighted robustness, value forgetting, and WSLS at stage 2.

    Summary
    -------
    - Stage 2 values learned via delta rule with decay toward 0.5 (forgetting).
    - Stage 1 uses robust planning that mixes best- and worst-alien values in each planet:
      V_planet = (1 - xi)*max(Q2) + xi*min(Q2), where xi grows with anxiety (more pessimistic).
    - Fixed transition structure (common=0.7, rare=0.3) used for model-based planning.
    - Stage 2 includes a win-stay/lose-shift bias: with higher anxiety, lose-shift strengthens,
      while win-stay weakens (approach-avoidance asymmetry).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0 or 1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [alpha, beta, xi0, rho, kappa_wsls]
        Bounds:
        - alpha: stage-2 reward learning rate [0,1]
        - beta: inverse temperature [0,10]
        - xi0: base robustness/pessimism weight [0,1]
        - rho: forgetting rate toward 0.5 at stage 2 [0,1]
        - kappa_wsls: base WSLS strength at stage 2 [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, xi0, rho, kappa_wsls = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2)) + 0.5

    # Anxiety-weighted pessimism (robustness)
    xi = np.clip(xi0 * st, 0.0, 1.0)

    # WSLS parameters: attenuate win-stay with anxiety, amplify lose-shift with anxiety
    ws_gain = kappa_wsls * (1.0 - st)   # win-stay bias strength
    ls_gain = kappa_wsls * st           # lose-shift bias strength

    prev_a2_by_state = {0: None, 1: None}
    prev_r_by_state = {0: None, 1: None}

    for t in range(n_trials):
        # Stage-1 robust planet values
        max_q = np.max(q2, axis=1)
        min_q = np.min(q2, axis=1)
        v_planet = (1.0 - xi) * max_q + xi * min_q

        q1 = T @ v_planet

        # Stage-1 policy
        logits1 = q1 - np.max(q1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with WSLS bias
        s = state[t]
        logits2 = q2[s].copy()
        if prev_a2_by_state[s] is not None and prev_r_by_state[s] is not None:
            last_a2 = prev_a2_by_state[s]
            last_r = prev_r_by_state[s]
            if last_r >= 0.5:
                # Win-stay: increase logit for previously chosen action
                logits2[last_a2] += ws_gain
            else:
                # Lose-shift: decrease logit for previously chosen action
                logits2[last_a2] -= ls_gain
        logits2 = logits2 - np.max(logits2)
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 forgetting toward 0.5
        q2 = (1.0 - rho) * q2 + rho * 0.5

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        prev_a2_by_state[s] = a2
        prev_r_by_state[s] = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'xi0', 'rho', 'kappa_wsls']"
iter4_run0_participant0.json,cognitive_model1,486.30380780352493,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Controllability-weighted arbitration with learned transitions and stage-1 perseveration.
    
    Idea:
    - The agent learns second-stage Q-values from reward (MF).
    - It also learns the transition structure T_hat(a -> s) via a simple delta rule.
    - Arbitration at stage 1 blends model-based (MB, via T_hat and max Q on each planet)
      and model-free (MF, TD-propagated from stage 2) with a weight that depends on
      perceived controllability and anxiety:
        w = clip((1 - stai) * mean_action_determinism, 0, 1)
      where determinism per action is |T_hat(a, X) - 0.5| * 2.
      Thus, higher anxiety and less predictable transitions reduce MB control.
    - There is stage-1 perseveration: repeating the previous spaceship receives a bonus.
    
    Parameters (bounds):
    - alpha2: [0,1] learning rate for stage-2 Q-learning and stage-1 MF backup
    - gamma_trans: [0,1] learning rate for learning the transition matrix T_hat
    - pers1: [0,1] strength of stage-1 perseveration bonus for repeating last action
    - beta: [0,10] inverse temperature for both stages
    
    Inputs:
    - action_1: int array of length T with values in {0,1} (0=A, 1=U)
    - state: int array of length T with values in {0,1} (0=X, 1=Y)
    - action_2: int array of length T with values in {0,1} (aliens per planet)
    - reward: float array of length T with values in [0,1]
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha2, gamma_trans, pers1, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha2, gamma_trans, pers1, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Initialize learned transition model T_hat: rows=actions(A,U), cols=states(X,Y)
    T_hat = np.full((2, 2), 0.5)

    # MF values
    q1_mf = np.zeros(2)           # stage-1 MF values for A,U
    q2 = np.zeros((2, 2))         # stage-2 values for each state and action

    # For stage-1 perseveration
    last_a1 = -1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation: value of each action is expectation over learned transitions
        max_q2_per_state = np.max(q2, axis=1)  # [X_best, Y_best]
        q1_mb = T_hat @ max_q2_per_state       # size 2 (A,U)

        # Arbitration weight from controllability and anxiety
        determinism = np.abs(T_hat[:, 0] - 0.5) * 2.0  # per action in [0,1]
        w = np.clip((1.0 - stai_score) * np.mean(determinism), 0.0, 1.0)

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 perseveration bonus
        logits1 = q1.copy()
        if last_a1 != -1:
            logits1[last_a1] += pers1

        # Softmax for stage 1
        l1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(l1)
        probs1 /= np.sum(probs1)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (within the observed state)
        s = state[t]
        logits2 = q2[s].copy()
        l2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(l2)
        probs2 /= np.sum(probs2)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: stage 2 Q-learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # MF backup to stage 1 (TD(1) using obtained second-stage value)
        target1 = q2[s, a2]
        q1_mf[a1] += alpha2 * (target1 - q1_mf[a1])

        # Learn transitions T_hat via delta rule toward the realized state
        # For chosen action a1, update probability of observed state s toward 1, other toward 0
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T_hat[a1, sp] += gamma_trans * (target - T_hat[a1, sp])

        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'gamma_trans', 'pers1', 'beta']"
iter4_run0_participant0.json,cognitive_model2,458.544369619271,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-split temperatures and approach-to-reward bias at stage 2 with MB/MF arbitration from anxiety.
    
    Idea:
    - Stage-2 values learned by MF TD.
    - Stage-1 combines MB (using known transition structure) and MF with a weight w = 1 - stai
      so higher anxiety yields more MF reliance.
    - Anxiety also splits choice stochasticity across stages:
        beta1 = beta * (1 - chi * stai)   (more exploratory at stage 1 with anxiety)
        beta2 = beta * (1 + chi * stai)   (more exploitative at stage 2 with anxiety)
      with clipping to remain positive.
    - Stage-2 approach bias: tend to repeat the last stage-2 action that produced reward within
      each state. The bias magnitude increases with anxiety.
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for stage-2 Q and MF propagation to stage 1
    - beta: [0,10] base inverse temperature
    - chi: [0,1] temperature split gain modulated by anxiety
    - app2: [0,1] baseline stage-2 approach bias toward last rewarded action in a state
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, beta, chi, app2]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha, beta, chi, app2 = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Known transition structure
    T_known = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Arbitration weight
    w = np.clip(1.0 - stai_score, 0.0, 1.0)

    # Temperature split
    beta1 = max(1e-6, beta * (1.0 - chi * stai_score))
    beta2 = max(1e-6, beta * (1.0 + chi * stai_score))

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Track last rewarded action per state (-1 if none yet)
    last_rew_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB at stage 1 from known transitions and current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage 1 softmax
        l1 = beta1 * (q1 - np.max(q1))
        probs1 = np.exp(l1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 with approach-to-reward bias
        s = state[t]
        logits2 = q2[s].copy()
        # Anxiety-scaled bias bonus for the last rewarded action in this state
        if last_rew_a2[s] != -1:
            bias_mag = app2 * (0.5 + 0.5 * stai_score)
            logits2[last_rew_a2[s]] += bias_mag

        l2 = beta2 * (logits2 - np.max(logits2))
        probs2 = np.exp(l2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update last rewarded action memory
        if r > 0.0:
            last_rew_a2[s] = a2

        # MF propagation to stage 1 (TD(1))
        q1_mf[a1] += alpha * (q2[s, a2] - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'chi', 'app2']"
iter4_run0_participant0.json,cognitive_model3,441.1210835208236,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-dependent MF credit with anxiety-driven spillover and ship-A bias.
    
    Idea:
    - Standard stage-2 MF learning from rewards.
    - Stage-1 MF receives TD backup from obtained second-stage value.
    - Additionally, after rare transitions, a fraction of the TD signal is misassigned
      to the unchosen first-stage action (transition-dependent MF). This fraction grows
      with anxiety, capturing stress-driven miscrediting.
    - Stage-2 ""spillover"" generalization: the reward also partially updates the same-index
      alien in the unvisited planet; spillover increases with anxiety.
    - Stage-1 policy includes a baseline bias toward spaceship A.
    - Stage-1 choice value is a mixture of MB (known transitions) and MF with weight
      w = 1 - stai (more MF under higher anxiety).
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for all TD updates
    - spill: [0,1] baseline fraction of reward that generalizes to the other state's same action
    - rare_bias: [0,1] baseline fraction of TD signal credited to the unchosen first-stage action after rare transitions
    - beta: [0,10] inverse temperature for both stages
    - biasA: [0,1] additive bias toward choosing spaceship A at stage 1
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, spill, rare_bias, beta, biasA]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """"""
    alpha, spill, rare_bias, beta, biasA = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Known transitions (A->X common, U->Y common)
    T_known = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Arbitration weight
    w = np.clip(1.0 - stai_score, 0.0, 1.0)

    # Anxiety-scaled effects
    spill_eff = spill * (0.5 + 0.5 * stai_score)
    rare_eff = rare_bias * (0.5 + 0.5 * stai_score)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB values for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Add explicit bias toward spaceship A (action index 0)
        logits1 = q1.copy()
        logits1[0] += biasA

        # Stage-1 softmax
        l1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(l1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = q2[s].copy()
        l2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(l2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Determine if the transition was common or rare
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Spillover: update same-index action in the other state
        other_s = 1 - s
        q2[other_s, a2] += alpha * spill_eff * (r - q2[other_s, a2])

        # Stage-1 MF backup from obtained second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Transition-dependent miscrediting to unchosen first-stage action after rare transitions
        if not is_common:
            a1_other = 1 - a1
            # Use the same TD target magnitude but attenuated
            q1_mf[a1_other] += alpha * rare_eff * (q2[s, a2] - q1_mf[a1_other])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'spill', 'rare_bias', 'beta', 'biasA']"
iter4_run0_participant1.json,cognitive_model2,534.4205227784328,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""MB planning at stage 1 with anxiety-amplified UCB exploration at stage 2 and stickiness.
    
    The model computes stage-1 action values via a fixed transition model and the max second-stage values.
    At stage 2, choices are driven by a softmax over Q2 plus an uncertainty bonus (UCB-like) that
    decays with visit count. Anxiety increases the exploration bonus, encouraging sampling under uncertainty.
    A second-stage perseveration bias is included. Lapse affects both stages.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - alphaR: learning rate for second-stage Q2 (0..1).
    - beta: inverse temperature for both stages (0..10).
    - b0: baseline UCB exploration bonus scale (0..1).
    - zeta_anx: anxiety amplification of exploration, b_eff = b0 * (1 + zeta_anx * stai) (0..1).
    - kappa2: second-stage perseveration (repeat same alien within a state) (0..1).
    - epsilon: lapse rate (0..1).
    
    Args:
        action_1: 1D array of first-stage choices (0/1).
        state: 1D array of encountered second-stage states (0/1).
        action_2: 1D array of second-stage actions (0/1).
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element in [0,1]: anxiety score.
        model_parameters: tuple/list in order (alphaR, beta, b0, zeta_anx, kappa2, epsilon).
    
    Returns:
        Negative log-likelihood of observed choices at both stages.
    """"""
    alphaR, beta, b0, zeta_anx, kappa2, epsilon = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    p1 = np.zeros(n)
    p2 = np.zeros(n)

    # Visit counts for UCB at stage 2
    N = np.zeros((2, 2))  # counts per state-action
    prev_a2 = np.array([-1, -1])  # last action per state

    # Effective exploration bonus scale with anxiety
    b_eff = b0 * (1.0 + zeta_anx * stai)

    eps = 1e-12

    for t in range(n):
        # Stage-1 MB values from current Q2
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        logits1 = beta * Q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        a1 = int(action_1[t])
        p1[t] = probs1[a1]

        # Stage-2 with UCB and perseveration
        s = int(state[t])
        bonus = b_eff / np.sqrt(N[s] + 1.0)  # diminishing uncertainty bonus
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa2

        logits2 = beta * Q2[s] + bonus + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        r = reward[t]

        # Learning stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Update counters and history
        N[s, a2] += 1.0
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll

","['alphaR', 'beta', 'b0', 'zeta_anx', 'kappa2', 'epsilon']"
iter4_run0_participant1.json,cognitive_model3,540.030333016162,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-gated rare-transition sensitivity and counterfactual update.
    
    The agent learns both transition probabilities and second-stage rewards.
    At stage 1, it blends model-based (MB) values (from learned transitions) with model-free (MF) values.
    The MB weight increases after rarer-than-expected transitions, and this rare-transition sensitivity
    is amplified by anxiety. It also performs counterfactual updating at stage 2 on the unchosen alien,
    scaled by anxiety. First-stage perseveration and a lapse are included.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - alphaR: learning rate for second-stage Q2 (0..1).
    - alphaT: learning rate for transition probabilities (row-wise exponential averaging) (0..1).
    - beta: inverse temperature for both stages (0..10).
    - psi_rare: base sensitivity of MB weight to rarity (0..1).
    - chi_anx: scales both (a) amplification of rarity sensitivity by anxiety and 
               (b) counterfactual learning rate at stage 2 (0..1).
    - kappa1: first-stage perseveration (0..1).
    - epsilon: lapse rate applied to both stages (0..1).
    
    Args:
        action_1: 1D array of first-stage choices (0/1).
        state: 1D array of second-stage states encountered (0/1).
        action_2: 1D array of second-stage choices (0/1).
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element in [0,1]: anxiety score.
        model_parameters: tuple/list in order (alphaR, alphaT, beta, psi_rare, chi_anx, kappa1, epsilon).
    
    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """"""
    alphaR, alphaT, beta, psi_rare, chi_anx, kappa1, epsilon = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Initialize transitions near the canonical structure, but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p1 = np.zeros(n)
    p2 = np.zeros(n)

    prev_a1 = -1
    eps = 1e-12

    for t in range(n):
        # Stage-1 MB values from learned transitions
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Compute rarity of the realized transition under current T BEFORE updating it
        a1 = int(action_1[t])
        s = int(state[t])
        p_trans = T[a1, s]  # probability assigned to the realized state
        # Rarity signal: positive when rarer-than-0.5, negative when more common-than-0.5
        rarity_signal = 0.5 - p_trans
        # Anxiety-amplified MB weight adjustment; map to [0,1] with clipping
        w_mb = 0.5 + psi_rare * rarity_signal * (1.0 + chi_anx * stai)
        w_mb = float(np.clip(w_mb, 0.0, 1.0))

        # Combine MB and MF with first-stage perseveration bias
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        p1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 learning (chosen action)
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Counterfactual update on unchosen action, scaled by anxiety via chi_anx
        a2_cf = 1 - a2
        cf_rate = alphaR * (chi_anx * stai)
        if cf_rate > 0.0:
            pe2_cf = r - Q2[s, a2_cf]
            Q2[s, a2_cf] += cf_rate * pe2_cf

        # Stage-1 MF update using outcome as a teaching signal (simple delta toward reward)
        Q1_mf[a1] += alphaR * (r - Q1_mf[a1])

        # Transition learning: move chosen row toward the observed state one-hot, then renormalize
        oh = np.array([1.0 if j == s else 0.0 for j in range(2)])
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        prev_a1 = a1

    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll","['alphaR', 'alphaT', 'beta', 'psi_rare', 'chi_anx', 'kappa1', 'epsilon']"
iter4_run0_participant12.json,cognitive_model1,476.0942180068855,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated model-based weight, perceived transition, and novelty bonus.
    
    This model blends model-based (MB) and model-free (MF) control at stage 1. Anxiety (stai) shifts:
      1) the arbitration weight toward/away from MB control,
      2) the agent's perceived transition structure (more anxious => perceives more rare transitions),
      3) the novelty/exploration bonus at stage 2 (more anxious => reduced novelty seeking).
    
    Parameters
    ----------
    action_1 : array-like of int in {0, 1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0, 1}
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int in {0, 1}
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float in {0, 1}
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1, used to modulate MB weight, perceived transitions, and novelty.
        Interpretation in this task:
          low < 0.31, medium in [0.31, 0.51], high > 0.51.
    model_parameters : list or array
        [alpha, beta, omega0, xi_stai, b0]
        Bounds:
          alpha   in [0, 1]   : learning rate for MF updates
          beta    in [0, 10]  : inverse-temperature for both stages
          omega0  in [0, 1]   : baseline MB arbitration weight
          xi_stai in [0, 1]   : sensitivity of parameters to stai (modulates MB weight, perceived transitions, novelty)
          b0      in [0, 1]   : baseline novelty bonus magnitude
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, omega0, xi_stai, b0 = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Model-free values
    q1_mf = np.zeros(2)          # stage-1 MF action values for spaceships A/U
    q2_mf = np.zeros((2, 2))     # stage-2 MF action values per planet/alien

    # Novelty tracking (visit counts for each alien per planet)
    visits = np.zeros((2, 2))

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated perceived transition matrix:
    # Base common prob = 0.7. Anxious agents perceive more rare transitions (lower common probability).
    delta_common = 0.2 * xi_stai * (stai - 0.5)  # stai>0.5 => reduce common prob; stai<0.5 => increase
    p_common_eff = np.clip(0.7 - delta_common, 0.5, 0.9)
    T = np.array([[p_common_eff, 1.0 - p_common_eff],
                  [1.0 - p_common_eff, p_common_eff]])

    # Anxiety-modulated MB weight and novelty
    w_mb = np.clip(omega0 + xi_stai * (stai - 0.5), 0.0, 1.0)
    bonus_eff = max(0.0, b0 * (1.0 - xi_stai * stai))  # more anxiety => smaller novelty bonus

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Novelty-augmented second-stage values for planning
        bonus_state = np.zeros(2)
        for a in range(2):
            bonus_state[a] = bonus_eff / np.sqrt(1.0 + visits[s, a])
        q2_aug = q2_mf + bonus_eff / np.sqrt(1.0 + visits)  # used for MB backup across states

        # Model-based computation for stage 1 using max over augmented q2 per next state
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_q2_aug

        # Hybrid stage-1 values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy uses novelty-augmented values in the visited state
        q2s_aug = q2_mf[s].copy() + bonus_state
        q2c = q2s_aug - np.max(q2s_aug)
        probs_2 = np.exp(beta * q2c) / np.sum(np.exp(beta * q2c))
        p_choice_2[t] = probs_2[a2]

        # Update counts and MF values
        visits[s, a2] += 1.0

        # Stage-2 MF update
        pe2 = reward[t] - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Stage-1 MF backup from realized second-stage action value (no extra lambda parameter)
        td1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega0', 'xi_stai', 'b0']"
iter4_run0_participant12.json,cognitive_model2,519.3542983872802,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive model-based control with anxiety-modulated risk aversion.
    
    The agent learns both the mean and uncertainty (variance) of each alien's payout.
    Choices are based on a risk-sensitive utility: U = mean - gamma * stdev.
    Anxiety increases risk aversion (gamma) and thus promotes avoiding uncertain aliens.
    Stage 1 uses pure model-based planning over risk-sensitive second-stage values.
    
    Parameters
    ----------
    action_1 : array-like of int in {0, 1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0, 1}
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int in {0, 1}
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float in {0, 1}
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1, used to modulate risk aversion.
    model_parameters : list or array
        [alpha, beta, gamma0, phi_stai]
        Bounds:
          alpha    in [0, 1]   : learning rate for mean/variance updates
          beta     in [0, 10]  : inverse temperature for both stages
          gamma0   in [0, 1]   : baseline risk-aversion coefficient
          phi_stai in [0, 1]   : scaling of risk aversion with stai
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, gamma0, phi_stai = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition matrix (true/task-level)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Track mean and variance per alien per planet
    m = np.zeros((2, 2)) + 0.5    # initialize around chance
    v = np.zeros((2, 2)) + 0.25   # initial uncertainty (stdev ~ 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated risk aversion
    gamma = np.clip(gamma0 * (1.0 + phi_stai * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Risk-sensitive second-stage utilities
        sd = np.sqrt(np.maximum(v, 1e-12))
        u2 = m - gamma * sd

        # Model-based stage-1 values from risk-sensitive utilities
        max_u2 = np.max(u2, axis=1)
        q1_mb = T @ max_u2

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        u2s = u2[s].copy()
        u2c = u2s - np.max(u2s)
        probs_2 = np.exp(beta * u2c) / np.sum(np.exp(beta * u2c))
        p_choice_2[t] = probs_2[a2]

        # Updates: mean and variance (incremental, unbiased for bounded alpha)
        err = reward[t] - m[s, a2]
        m[s, a2] += alpha * err
        # Update variance toward squared error (online variance tracking)
        v[s, a2] += alpha * ((err ** 2) - v[s, a2])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'gamma0', 'phi_stai']"
iter4_run0_participant12.json,cognitive_model3,486.26287311953524,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Confidence-weighted arbitration between MB and MF with anxiety-modulated bias and stage-2 perseveration.
    
    This model learns model-free values at both stages and computes model-based values via the transition model.
    An arbitration weight w_t favors the controller (MB vs MF) with higher confidence (lower recent surprise).
    Anxiety shifts arbitration toward model-free control and increases stage-2 perseveration.
    
    Confidence proxies:
      - MF surprise: moving average of absolute stage-2 prediction error |PE2|.
      - MB surprise: moving average of transition surprise (rare=1, common=0).
    Larger surprise => lower confidence.
    
    Parameters
    ----------
    action_1 : array-like of int in {0, 1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0, 1}
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int in {0, 1}
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float in {0, 1}
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1, used to bias arbitration toward MF and to scale perseveration.
    model_parameters : list or array
        [alpha, beta, arb0, z_stai, kappa_ps]
        Bounds:
          alpha    in [0, 1]   : learning rate (both stages)
          beta     in [0, 10]  : inverse temperature for both stages
          arb0     in [0, 1]   : baseline arbitration bias for MB (higher => more MB)
          z_stai   in [0, 1]   : strength of stai impact on arbitration (higher stai => more MF)
          kappa_ps in [0, 1]   : baseline stage-2 perseveration (stickiness) magnitude
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, arb0, z_stai, kappa_ps = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition matrix (task)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Confidence trackers (exponentially weighted moving averages)
    mf_surprise = 0.5  # initialize mid-range
    mb_surprise = 0.3
    conf_alpha = 0.2   # smoothing for confidence signals (fixed, not a free parameter)

    # Likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration at stage 2
    prev_a2 = None
    # Anxiety increases perseveration
    kappa_eff = kappa_ps * (1.0 + z_stai * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based Q for stage 1 from MF stage-2 values
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2

        # Arbitration weight based on confidence
        # Higher surprise => lower confidence; map to confidence in [0,1]
        conf_mf = 1.0 - np.clip(mf_surprise, 0.0, 1.0)
        conf_mb = 1.0 - np.clip(mb_surprise, 0.0, 1.0)
        # Baseline MB bias (arb0), anxiety tilts away from MB toward MF as stai increases
        arb_bias = np.clip(arb0 - z_stai * stai, 0.0, 1.0)
        # Combine: normalized MB weight
        if (conf_mb + conf_mf) > 1e-8:
            w_mb = np.clip(arb_bias * (conf_mb / (conf_mb + conf_mf)), 0.0, 1.0)
        else:
            w_mb = arb_bias  # fallback

        # Hybrid stage-1 values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with perseveration
        q2s = q2_mf[s].copy()
        if prev_a2 is not None:
            stick = np.zeros(2)
            stick[prev_a2] = 1.0
            q2s = q2s + kappa_eff * stick
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c) / np.sum(np.exp(beta * q2c))
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        # Stage-2 MF update
        pe2 = reward[t] - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Stage-1 MF update with modest eligibility (fixed lambda=0.5)
        td1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1 * 0.5

        # Update confidence trackers
        mf_surprise = (1.0 - conf_alpha) * mf_surprise + conf_alpha * np.abs(pe2)

        # Transition surprise: 0 if common, 1 if rare
        # Identify whether transition was common for the chosen a1
        common_for_a1 = (s == 0 and a1 == 0) or (s == 1 and a1 == 1)  # A->X, U->Y are common
        trans_surprise = 0.0 if common_for_a1 else 1.0
        mb_surprise = (1.0 - conf_alpha) * mb_surprise + conf_alpha * trans_surprise

        prev_a2 = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'arb0', 'z_stai', 'kappa_ps']"
iter4_run0_participant14.json,cognitive_model1,538.0324051726791,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-gated rare-transition learning boost.
    
    This model combines model-based (MB) and model-free (MF) values at stage 1,
    uses MF Q-learning at stage 2, and modulates the learning rate specifically
    after rare transitions. Higher anxiety amplifies learning from rare events,
    capturing heightened sensitivity to surprising outcomes.
    
    Parameters (model_parameters):
    - alpha: base learning rate for Q-updates (stage 2 and TD backup to stage 1), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - w_h: hybrid weight for MB at stage 1 (0: pure MF, 1: pure MB), in [0,1]
    - k_rare: multiplicative boost applied to alpha on rare transitions (as factor-1), in [0,1]
              Effective alpha on rare trials â alpha * (1 + k_rare * anx_gain)
    - g_anx: anxiety gain scaling the rare-transition boost by stai, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (typically 0/1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, w_h, k_rare, g_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure for two-step task
    T = np.array([[0.7, 0.3],  # A -> X (common), Y (rare)
                  [0.3, 0.7]]) # U -> X (rare),   Y (common)

    # Value structures
    q2 = np.zeros((2, 2))   # Stage-2 values per planet and alien
    q1_mf = np.zeros(2)     # Stage-1 model-free cached values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated multiplier applied only on rare transitions
    rare_boost_base = np.clip(1.0 + (k_rare * g_anx * stai), 0.0, 2.0)

    for t in range(n_trials):
        # Model-based estimate for stage 1 from current q2
        max_q2 = np.max(q2, axis=1)          # best alien per planet
        q1_mb = T @ max_q2                   # expected value per spaceship

        # Hybrid stage-1 action values
        q1 = w_h * q1_mb + (1.0 - w_h) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (based on reached planet)
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Determine whether the observed transition was common or rare
        is_rare = 1 if T[a1, s] < 0.5 else 0
        # Effective learning rate: boosted on rare transitions, scaled by anxiety
        alpha_eff = alpha * (rare_boost_base if is_rare == 1 else 1.0)
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * pe2

        # Stage-1 MF TD backup toward immediate post-transition value (without extra params)
        # Use the chosen alien's updated Q as target for the chosen spaceship
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_eff * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_h', 'k_rare', 'g_anx']"
iter4_run0_participant15.json,cognitive_model2,571.8069346560247,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Directed exploration at Stage-2 with anxiety-dampened novelty bonus, MB planning at Stage-1.
    
    Mechanism:
    - Stage-2 MF values (Q2) learned with delta-rule.
    - A directed exploration (novelty) bonus is added to Stage-2 values: bonus = nu_eff / sqrt(N[s,a] + 1),
      where N is a decaying visit count. This increases choice probability for less-tried aliens.
    - Stage-1 uses MB planning from bonus-augmented Stage-2 values.
    
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for Q2 updates.
    - beta: [0,10] Inverse temperature for both stages.
    - nu: [0,1] Base strength of directed exploration bonus.
    - sigma: [0,1] Decay rate for visit counts per trial (higher = faster forgetting of visits).
    - psi: [0,1] Anxiety coupling: effective bonus nu_eff = nu * (1 - psi * stai).
    
    Anxiety use:
    - Higher STAI reduces directed exploration via nu_eff = nu * (1 - psi * stai).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1.
    - state: array of ints in {0,1}, observed planet (X=0, Y=1).
    - action_2: array of ints in {0,1}, chosen alien at Stage-2 within observed state.
    - reward: array of floats (e.g., 0 or 1), coins received.
    - stai: array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [alpha, beta, nu, sigma, psi]
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    alpha, beta, nu, sigma, psi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))
    # Decaying visit counts for directed exploration
    N = np.zeros((2, 2))

    nu_eff = nu * (1.0 - psi * stai)
    # Ensure non-negativity
    nu_eff = max(0.0, nu_eff)

    for t in range(n_trials):
        s = state[t]

        # Compute exploration bonuses for current state
        bonus_state = nu_eff / np.sqrt(N[s] + 1.0)

        # Stage-1 model-based planning uses bonus-augmented Q2
        mb_q1 = transition_matrix @ np.max(q2 + np.vstack([bonus_state, bonus_state]), axis=1)

        # Stage-1 policy
        z1 = beta * (mb_q1 - np.max(mb_q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with bonus in the observed state
        q2b = q2[s] + bonus_state
        z2 = beta * (q2b - np.max(q2b))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update Q2
        pe = r - q2[s, a2]
        q2[s, a2] += alpha * pe

        # Decay visit counts globally for the visited state, then increment chosen
        N[s, :] = (1.0 - sigma) * N[s, :]
        N[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'nu', 'sigma', 'psi']"
iter4_run0_participant15.json,cognitive_model3,573.8950948699847,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive learning-rate via learned volatility with anxiety-scaling and temperature dampening.
    
    Mechanism:
    - Tracks a local volatility estimate per state-action from absolute prediction errors.
    - Trial-wise learning rate alpha_t increases with estimated volatility (faster adaptation).
    - Anxiety increases effective volatility and reduces choice temperature (more noise).
    - Stage-1 values are model-based from current Stage-2 values.
    
    Parameters (all used; total=5):
    - a0: [0,1] Base learning-rate floor; when volatility is low, alpha_t ~ a0.
    - beta: [0,10] Baseline inverse temperature.
    - vol: [0,1] Initial volatility per state-action.
    - eta: [0,1] Update rate for volatility from absolute PE.
    - tmult: [0,1] Anxiety scaling of temperature: beta_eff = beta * (1 - tmult * stai).
    
    Anxiety use:
    - Volatility inflation: vol_eff = vol_est * (0.5 + 0.5 * stai) increases learning rate under higher anxiety.
    - Temperature dampening: beta_eff decreases linearly with stai via tmult.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1.
    - state: array of ints in {0,1}, observed planet (X=0, Y=1).
    - action_2: array of ints in {0,1}, chosen alien at Stage-2 within observed state.
    - reward: array of floats (e.g., 0 or 1), coins received.
    - stai: array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [a0, beta, vol, eta, tmult]
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    a0, beta, vol, eta, tmult = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))
    vol_est = vol * np.ones((2, 2))  # local volatility estimate per state-action

    # Anxiety-dampened temperature
    beta_eff = beta * (1.0 - tmult * stai)
    beta_eff = max(0.0, beta_eff)  # ensure non-negative

    for t in range(n_trials):
        s = state[t]

        # Stage-1 model-based using current Q2
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Stage-1 policy
        z1 = beta_eff * (mb_q1 - np.max(mb_q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # PE and volatility update
        pe = r - q2[s, a2]
        vol_est[s, a2] = (1.0 - eta) * vol_est[s, a2] + eta * abs(pe)

        # Anxiety-inflated volatility and adaptive learning rate
        vol_eff = vol_est[s, a2] * (0.5 + 0.5 * stai)
        vol_eff = min(1.0, max(0.0, vol_eff))
        alpha_t = a0 * (1.0 - vol_eff) + vol_eff  # interpolates between a0 and 1 with volatility
        alpha_t = min(1.0, max(0.0, alpha_t))

        # Q2 update with adaptive alpha
        q2[s, a2] += alpha_t * pe

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['a0', 'beta', 'vol', 'eta', 'tmult']"
iter4_run0_participant16.json,cognitive_model2,509.542099716896,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-tuned transition-learning model with stage-2 perseveration.
    
    Idea
    ----
    The agent learns the transition matrix online (which spaceship leads to which planet).
    Anxiety (stai) increases the transition learning rate (hypervigilance) and slightly
    reduces choice determinism. Stage-2 choices also exhibit perseveration (stickiness).
    Stage-1 is purely model-based using the learned transitions; Stage-2 is MF.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached planet (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices.
    reward : array-like of float in [0,1]
        Received coins.
    stai : array-like with a single float in [0,1]
        Anxiety score. Higher -> faster transition learning, slightly lower beta.
    model_parameters : iterable of 5 floats
        - alpha_r in [0,1]: stage-2 reward learning rate.
        - beta in [0,10]: baseline inverse temperature for both stages.
        - alpha_T in [0,1]: baseline transition learning rate.
        - kappa2 in [0,1]: stage-2 perseveration weight.
        - zeta in [0,1]: anxiety sensitivity controlling how much stai speeds transition learning and reduces beta.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_r, beta, alpha_T, kappa2, zeta = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Initialize learned transitions as uniform
    # T[a, s] = P(state=s | first-action=a), rows each sum to 1
    T = np.ones((2, 2)) * 0.5

    # MF values at stage 2
    q2 = np.zeros((2, 2))

    # Logs
    p_choice_1 = np.zeros(n)
    p_choice_2 = np.zeros(n)

    # Effective parameters shaped by anxiety
    # Higher anxiety -> higher transition learning rate, slightly softer policies
    alpha_T_eff = np.clip(alpha_T + zeta * stai * (1.0 - alpha_T), 0.0, 1.0)
    beta_eff = max(0.0, beta * (1.0 - 0.3 * zeta * stai))

    # Stage-2 perseveration trace per state
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n):
        # Stage-1 model-based Q via learned transitions and max second-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 softmax with beta_eff
        logits1 = beta_eff * q1_mb
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with state-dependent perseveration
        s = int(state[t])
        logits2 = beta_eff * q2[s].copy()
        if prev_a2[s] is not None:
            # Add perseveration bias toward repeating previous a2 in this state
            logits2[prev_a2[s]] += kappa2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Update transitions from observed (a1 -> s)
        # Simple delta rule toward one-hot observation
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = (1.0 - alpha_T_eff) * T[a1] + alpha_T_eff * oh
        # Ensure numeric stability (renormalize)
        T[a1] = T[a1] / np.sum(T[a1])

        # Update MF stage-2 values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Update perseveration trace
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_r', 'beta', 'alpha_T', 'kappa2', 'zeta']"
iter4_run0_participant16.json,cognitive_model3,518.3940194544574,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Entropy-regularized exploration with anxiety-driven entropy and lapse rates.
    
    Idea
    ----
    Choices maximize a combination of value and entropy (soft actor). Anxiety (stai)
    increases an entropy bonus (tau) and a value-free lapse rate (epsilon), promoting
    exploration, particularly at stage 2 where immediate uncertainty is higher.
    Stage 1 uses MB planning from fixed transitions blended with a small MF trace.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached planet (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices.
    reward : array-like of float in [0,1]
        Received coins.
    stai : array-like with a single float in [0,1]
        Anxiety score; higher -> more entropy bonus and higher lapse probability.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: MF learning rate for both stages.
        - beta1 in [0,10]: baseline inverse temperature for stage 1.
        - beta2 in [0,10]: baseline inverse temperature for stage 2.
        - tau_base in [0,1]: baseline entropy weight; anxiety increases it.
        - eps_base in [0,1]: baseline lapse probability; anxiety increases it.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta1, beta2, tau_base, eps_base = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Fixed transitions (known structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n)
    p_choice_2 = np.zeros(n)

    # Anxiety-shaped parameters
    # Entropy bonus increases with stai; lapse also increases with stai
    tau1 = np.clip(tau_base + 0.6 * stai * (1.0 - tau_base), 0.0, 1.0)
    tau2 = np.clip(tau_base + 0.9 * stai * (1.0 - tau_base), 0.0, 1.0)
    eps1 = np.clip(eps_base + 0.3 * stai * (1.0 - eps_base), 0.0, 1.0)
    eps2 = np.clip(eps_base + 0.6 * stai * (1.0 - eps_base), 0.0, 1.0)

    for t in range(n):
        # Model-based value for stage 1 from max stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        # Small MF trace blended in to capture habitual tendencies
        w_mf = 0.2 + 0.2 * stai  # more anxiety -> slightly more MF carryover
        q1 = (1 - w_mf) * q1_mb + w_mf * q1_mf

        # Stage 1: entropy-regularized softmax is equivalent to softmax with adjusted logits.
        # We implement entropy-regularized policy by mixing softmax with a uniform prior via tau.
        logits1 = beta1 * q1
        logits1 = logits1 - np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 = soft1 / np.sum(soft1)
        uni = np.array([0.5, 0.5])
        pi1 = (1 - tau1) * soft1 + tau1 * uni

        # Apply value-free lapse: final policy is mixture with uniform
        pi1 = (1 - eps1) * pi1 + eps1 * uni
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage 2: same entropy-regularized approach
        s = int(state[t])
        logits2 = beta2 * q2[s]
        logits2 = logits2 - np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 = soft2 / np.sum(soft2)
        pi2 = (1 - tau2) * soft2 + tau2 * uni
        pi2 = (1 - eps2) * pi2 + eps2 * uni
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        r = float(reward[t])

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF trace bootstraps from current state/action value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta1', 'beta2', 'tau_base', 'eps_base']"
iter4_run0_participant18.json,cognitive_model1,437.3280358520241,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-bonus exploration with valence-asymmetric learning and anxiety-modulated stickiness.

    Description:
    - Stage 2 action values (Q2) are learned with separate learning rates for positive/negative prediction errors.
    - An information bonus (U2) encourages exploration toward uncertain options; its strength decreases with anxiety.
    - Stage 1 uses a hybrid of model-based (MB; using fixed transitions) and model-free (MF) values with MB weight w_mb = 1 - stai.
    - Stickiness (perseveration) biases repeating previous actions; its effective strength increases with anxiety.
    - Eligibility propagation from stage 2 to stage 1 is included via a fixed lambda = 0.5.

    Parameters (model_parameters):
    - alpha_pos: learning rate for positive RPEs at stage 2 in [0,1]
    - alpha_neg: learning rate for negative RPEs at stage 2 in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - xi_info: information-bonus scale in [0,1] (effective bonus = xi_info * (1 - stai))
    - tau_stick: stickiness scale in [0,1] (effective stickiness = tau_stick * stai)

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha_pos, alpha_neg, beta, xi_info, tau_stick)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_pos, alpha_neg, beta, xi_info, tau_stick = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    Q1_mf = np.zeros(2)      # stage-1 MF values
    Q2 = np.zeros((2, 2))    # stage-2 values: state x action

    # Uncertainty estimates per state-action for stage 2
    U2 = np.ones((2, 2))     # start uncertain
    decay_u = 0.9            # fixed uncertainty decay

    # Anxiety-modulated components
    w_mb = max(0.0, min(1.0, 1.0 - st))             # MB weight for stage 1
    bonus_scale = xi_info * (1.0 - st)              # exploration bonus diminished by anxiety
    stick_eff = tau_stick * st                       # stickiness stronger with anxiety

    prev_a1 = None
    prev_a2 = [None, None]

    for t in range(n_trials):
        s = int(state[t])

        # Stage-1 model-based values via planning over Q2 (no info bonus in planning)
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid S1
        pref1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        if prev_a1 is not None:
            pref1[prev_a1] += stick_eff

        # Softmax S1
        z1 = pref1 - np.max(pref1)
        probs1 = np.exp(beta * z1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 preferences with information bonus and stickiness
        pref2 = Q2[s] + bonus_scale * U2[s]
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += stick_eff

        z2 = pref2 - np.max(pref2)
        probs2 = np.exp(beta * z2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD errors
        delta1 = Q2[s, a2] - Q1_mf[a1]
        delta2 = r - Q2[s, a2]

        # MF updates
        Q1_mf[a1] += 0.5 * delta1  # bootstrap with fixed lambda=0.5
        # Valence-asymmetric update at stage 2
        lr = alpha_pos if delta2 >= 0.0 else alpha_neg
        Q2[s, a2] += lr * delta2
        # Eligibility propagation from reward to S1
        Q1_mf[a1] += 0.5 * lr * delta2

        # Uncertainty update: decay and add surprise magnitude
        U2[s, a2] = decay_u * U2[s, a2] + (1.0 - decay_u) * abs(delta2)

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'xi_info', 'tau_stick']"
iter4_run0_participant18.json,cognitive_model2,475.1979525277487,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned transition model with anxiety-dependent risk sensitivity and arbitration.

    Description:
    - The agent learns the transition matrix T via delta-rule updates (alpha_T).
    - Stage 2 values learn expected utility, where utility has curvature gamma = 1 - k_risk * stai
      (higher anxiety -> more concave utility -> risk aversion for variable payouts).
    - Stage 1 is a hybrid of model-based (using learned T) and model-free values, with MB weight
      w_mb(stai) = (1 - stai)*w_mb0 + stai*(1 - w_mb0) (anxiety shifts arbitration away from baseline).
    - A single inverse temperature beta governs both stages.

    Parameters (model_parameters):
    - alpha_q: learning rate for Q-updates at stage 2 in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - alpha_T: learning rate for transition learning in [0,1]
    - w_mb0: baseline model-based weight in [0,1]
    - k_risk: utility curvature gain in [0,1] (gamma = 1 - k_risk * stai)

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha_q, beta, alpha_T, w_mb0, k_risk)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_q, beta, alpha_T, w_mb0, k_risk = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T (rows sum to 1)
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    w_mb = (1.0 - st) * w_mb0 + st * (1.0 - w_mb0)
    w_mb = max(0.0, min(1.0, w_mb))
    gamma = 1.0 - k_risk * st
    gamma = max(0.0, min(1.0, gamma))

    for t in range(n_trials):
        s = int(state[t])

        # Stage-1 model-based values via learned T
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid preference
        pref1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Softmax S1
        z1 = pref1 - np.max(pref1)
        probs1 = np.exp(beta * z1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Softmax S2
        pref2 = Q2[s]
        z2 = pref2 - np.max(pref2)
        probs2 = np.exp(beta * z2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])
        # Utility transform of reward with anxiety-dependent curvature
        u = r ** gamma

        # TD errors and updates
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha_q * delta1

        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha_q * delta2

        # Transition learning for the chosen action a1 toward observed state s
        # Move the a1-th row toward the one-hot vector of observed state
        target = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1] = (1.0 - alpha_T) * T[a1] + alpha_T * target
        # Ensure row normalization (should already hold but keep numerically stable)
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_q', 'beta', 'alpha_T', 'w_mb0', 'k_risk']"
iter4_run0_participant18.json,cognitive_model3,457.0777266984446,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-aware arbitration: anxiety reduces MB control after rare transitions.

    Description:
    - Stage 1 choice is a hybrid of MB and MF values. The baseline MB weight is omega_base,
      but after rare transitions it is multiplicatively down-weighted by (1 - k_rare * stai).
    - Rare vs. common is defined by the fixed task structure: A->X and U->Y are common.
    - Stage 2 is MF.
    - Stage 1 MF receives eligibility-trace credit from stage 2 reward with elig weight
      that is larger after common transitions and further reduced after rare transitions
      in proportion to anxiety and k_rare.

    Parameters (model_parameters):
    - alpha: learning rate for value updates in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - omega_base: baseline model-based arbitration weight in [0,1]
    - k_rare: penalty strength for MB after rare transitions in [0,1]
    - lambda_elig: eligibility trace base weight for propagating reward to stage 1 in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, omega_base, k_rare, lambda_elig)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, omega_base, k_rare, lambda_elig = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition matrix for MB planning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = int(state[t])

        # Compute MB values for stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Determine common vs rare transition given chosen a1 and observed s
        a1 = int(action_1[t])
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        rare = 0 if is_common else 1

        # Anxiety-dependent arbitration: reduce MB after rare transitions
        omega_t = omega_base
        if rare == 1:
            omega_t = omega_t * (1.0 - k_rare * st)
        omega_t = max(0.0, min(1.0, omega_t))

        # Hybrid preference for S1
        pref1 = omega_t * Q1_mb + (1.0 - omega_t) * Q1_mf

        # Softmax S1
        z1 = pref1 - np.max(pref1)
        probs1 = np.exp(beta * z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax on MF values
        a2 = int(action_2[t])
        pref2 = Q2[s]
        z2 = pref2 - np.max(pref2)
        probs2 = np.exp(beta * z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD errors
        delta1 = Q2[s, a2] - Q1_mf[a1]
        delta2 = r - Q2[s, a2]

        # MF updates
        Q2[s, a2] += alpha * delta2
        Q1_mf[a1] += alpha * delta1

        # Eligibility trace to stage 1 depends on transition type and anxiety
        # After rare transitions, reduce the eligibility by (1 - k_rare * st)
        elig_t = lambda_elig * (1.0 if is_common else (1.0 - k_rare * st))
        elig_t = max(0.0, min(1.0, elig_t))
        Q1_mf[a1] += alpha * elig_t * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'omega_base', 'k_rare', 'lambda_elig']"
iter4_run0_participant21.json,cognitive_model1,465.9219126065457,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""TD(lambda) model-free control with anxiety-modulated eligibility and lapse.

    Overview:
    - Stage-2 values are learned with standard TD updates.
    - Stage-1 values are updated via SARSA(Î»)-style eligibility that propagates the stage-2 TD error.
    - Anxiety increases random lapses (epsilon-greedy mixture) and reduces credit assignment span (Î»).

    Parameters (bounds):
    - model_parameters[0] = alpha2 (0 to 1): learning rate for stage-2 action values
    - model_parameters[1] = alpha1 (0 to 1): learning rate for stage-1 values
    - model_parameters[2] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[3] = trace0 (0 to 1): baseline eligibility trace strength
    - model_parameters[4] = anx_lapse (0 to 1): how strongly anxiety increases lapse (random choice) rate

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha2, alpha1, beta, trace0, anx_lapse = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: rows are spaceships A,U; columns are planets X,Y
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Action values
    q1 = np.zeros(2)        # stage-1 model-free values for A,U
    q2 = np.zeros((2, 2))   # stage-2 values for planets X,Y and their two aliens

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated lapse (random choice mixing)
    # Higher anxiety => higher lapse probability at both stages
    eps_lapse = np.clip(anx_lapse * stai_val, 0.0, 0.5)

    for t in range(n_trials):
        # Stage-1 policy (softmax over MF q1, with lapse)
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        # Mix with lapse uniform policy
        probs_1 = (1.0 - eps_lapse) * probs_1 + eps_lapse * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]

        # Stage-2 policy (softmax over q2 at reached planet, with lapse)
        q2_net = q2[s]
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        probs_2 = (1.0 - eps_lapse) * probs_2 + eps_lapse * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD updates
        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Stage-1 update with eligibility trace Î»
        # Anxiety reduces the depth of credit assignment (smaller Î»)
        lambda_eff = np.clip(trace0 * (1.0 - 0.6 * stai_val), 0.0, 1.0)

        # Bootstrapped update includes both immediate stage-2 value and the stage-2 TD error
        # This implements a SARSA(Î»)-like update using the propagated delta2
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        q1[a1] += alpha1 * (delta1 + lambda_eff * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'alpha1', 'beta', 'trace0', 'anx_lapse']"
iter4_run0_participant21.json,cognitive_model2,532.6207929611533,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planning with anxiety-skewed transition beliefs and reward compression.

    Overview:
    - Stage-2 values learned via TD.
    - Stage-1 choices derive purely from model-based planning that uses a biased transition model.
    - Anxiety reduces confidence in the canonical transition structure (skews toward the opposite structure).
    - Anxiety also compresses rewards toward neutrality, dampening learning from outcomes.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = psi0 (0 to 1): baseline trust in canonical transitions (higher => more canonical)
    - model_parameters[3] = anx_skew (0 to 1): how strongly anxiety shifts beliefs away from canonical transitions
      Effective trust: tau_eff = clip(psi0 + anx_skew * (0.5 - stai), 0, 1).
    - model_parameters[4] = zeta_r (0 to 1): reward compression toward 0.5, amplified by anxiety

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha, beta, psi0, anx_skew, zeta_r = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Canonical transition structure
    T_canon = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Opposite (flipped) structure swaps common/rare per spaceship
    T_flip = np.array([[0.3, 0.7],
                       [0.7, 0.3]])

    # Anxiety-modulated trust in canonical transitions
    tau_eff = np.clip(psi0 + anx_skew * (0.5 - stai_val), 0.0, 1.0)
    T_eff = tau_eff * T_canon + (1.0 - tau_eff) * T_flip

    # Stage-2 Q-values
    q2 = np.zeros((2, 2))
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Reward compression: anxiety pushes rewards toward 0.5 baseline
    comp = np.clip(zeta_r * (0.5 + 0.5 * stai_val), 0.0, 1.0)  # effective compression weight in [0,1]

    for t in range(n_trials):
        # Model-based stage-1 values from current q2 and biased transitions
        max_q2 = np.max(q2, axis=1)            # best action value per planet
        q1_mb = T_eff @ max_q2                 # expected value per spaceship

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at reached planet
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Compressed reward signal
        r = reward[t]
        r_tilde = (1.0 - comp) * r + comp * 0.5

        # Stage-2 learning
        delta2 = r_tilde - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'psi0', 'anx_skew', 'zeta_r']"
iter4_run0_participant21.json,cognitive_model3,431.8193146197932,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive-temperature hybrid control driven by volatility, modulated by anxiety.

    Overview:
    - Stage-2 values learned via TD.
    - Stage-1 combines model-based and model-free values with a weight that decreases with estimated volatility.
    - Decision noise (1/beta) increases when recent outcome volatility is high, especially under anxiety.
    - Includes small stage-1 perseveration that weakens with anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 and stage-1 MF values
    - model_parameters[1] = beta0 (0 to 10): baseline inverse temperature
    - model_parameters[2] = nu_vol (0 to 1): strength of volatility impact on temperature
    - model_parameters[3] = anx_beta (0 to 1): how strongly anxiety amplifies volatility-induced noise
    - model_parameters[4] = stick1 (0 to 1): baseline stage-1 perseveration magnitude

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha, beta0, nu_vol, anx_beta, stick1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q1_mf = np.zeros(2)       # model-free stage-1
    q2 = np.zeros((2, 2))     # stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    # Volatility tracker: exponentially-weighted mean absolute TD error at stage 2
    vol = 0.0

    for t in range(n_trials):
        # Model-based component from current q2 and known transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Volatility-dependent mixture weight (higher vol => more MF reliance)
        w_mb = 1.0 - np.clip(vol, 0.0, 1.0)              # in [0,1]
        w_mb *= (1.0 - 0.5 * stai_val)                   # anxiety reduces MB weighting further (without extra param)
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # Perseveration bias (reduced by anxiety)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick1 * (1.0 - stai_val)

        # Combine MB and MF with bias
        q1_net = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias1

        # Adaptive temperature based on volatility and anxiety
        beta_eff = beta0 / (1.0 + nu_vol * vol * (1.0 + anx_beta * stai_val))
        beta_eff = np.clip(beta_eff, 1e-6, 10.0)

        # Stage-1 policy
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at reached planet with same beta_eff
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update volatility estimate (faster adaptation under anxiety)
        vol_lr = 0.3 + 0.5 * stai_val  # in [0.3,0.8]
        vol = (1.0 - vol_lr) * vol + vol_lr * abs(delta2)
        vol = np.clip(vol, 0.0, 1.0)

        # Stage-1 MF bootstrapping from the obtained stage-2 value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta0', 'nu_vol', 'anx_beta', 'stick1']"
iter4_run0_participant22.json,cognitive_model1,415.04187129378784,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with learned transitions and anxiety-modulated arbitration.
    
    This model learns second-stage values model-free and first-stage transition
    structure model-based. First-stage choices are guided by a weighted blend
    of model-based (MB) and model-free (MF) action values. The arbitration
    weight depends on the entropy (uncertainty) of the currently chosen action's
    transition row and on the participant's anxiety level.
    
    Parameters (model_parameters):
    - alpha_mf: [0,1] model-free learning rate for Q at both stages.
    - beta: [0,10] inverse temperature for both stages.
    - trans_lr: [0,1] learning rate for updating the transition matrix rows.
    - arbit_slope: [0,1] slope controlling sensitivity of arbitration to uncertainty and anxiety.
    - mb_bias: [0,1] baseline bias toward MB control (mapped to [-2,2] internally).
    
    Inputs:
    - action_1: int array of length T; 0 or 1 (spaceship A vs U).
    - state: int array of length T; 0 or 1 (planet X vs Y) actually reached.
    - action_2: int array of length T; 0 or 1 (alien choice on that planet).
    - reward: float array of length T; received coins (assumed in [0,1]).
    - stai: array-like length 1; anxiety score in [0,1].
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_mf, beta, trans_lr, arbit_slope, mb_bias = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix near the task's structure but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Model-free values
    q1_mf = np.zeros(2)        # first-stage MF values
    q2 = np.zeros((2, 2))      # second-stage MF values per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Eligibility trace at stage-1 is modulated by anxiety (not an extra parameter)
    # Higher anxiety reduces bootstrapping credit assignment.
    lam_eff = max(0.0, min(1.0, 0.6 - 0.4 * stai))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Model-based first-stage Q via planning through learned transitions
        max_q2 = np.max(q2, axis=1)       # best attainable value at each planet
        q1_mb = T @ max_q2                # plan over transitions

        # Uncertainty (entropy) of each action's transition row
        # H(p) = -sum p log p; compute per action and take the chosen action's value.
        # Use it to compute an arbitration weight: higher uncertainty and higher anxiety -> more MF
        row_entropies = np.zeros(2)
        for a in range(2):
            p_row = T[a]
            # numerical safety
            p_row = np.clip(p_row, 1e-12, 1.0)
            row_entropies[a] = -np.sum(p_row * np.log(p_row))
        H_chosen = row_entropies[a1]

        # Map mb_bias in [0,1] to centered bias in [-2,2] to allow a wide range of weights
        bias_centered = 4.0 * (mb_bias - 0.5)

        # Arbitration signal: positive favors MB; uncertainty and anxiety push toward MF
        # w in [0,1]
        arb_signal = bias_centered - 3.0 * arbit_slope * (H_chosen + stai)
        w = 1.0 / (1.0 + np.exp(-arb_signal))
        w = min(1.0, max(0.0, w))

        # Hybrid Q for first-stage policy
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf

        # First-stage choice probability (softmax)
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability (softmax on q2 at reached state)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Learning: update model-free values
        # Stage-2 TD
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_mf * delta2

        # Stage-1 MF bootstrapping with eligibility from stage-2 PE
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha_mf * delta1
        q1_mf[a1] += alpha_mf * lam_eff * delta2

        # Learn the transition matrix row for the chosen first-stage action from observed state
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = (1.0 - trans_lr) * T[a1, :] + trans_lr * one_hot_s
        # Renormalize for safety
        T[a1, :] = np.clip(T[a1, :], 1e-8, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_mf', 'beta', 'trans_lr', 'arbit_slope', 'mb_bias']"
iter4_run0_participant22.json,cognitive_model2,428.02515203321457,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with Pearce-Hall associability, anxiety-weighted negative PEs, and risk-sensitive utility.
    
    This model is purely model-free at both stages. The effective learning rate
    is dynamically adapted via an associability term (Pearce-Hall): recent absolute
    prediction errors increase associability and hence learning rate. Anxiety
    selectively amplifies negative prediction errors. Rewards are transformed by
    a risk-sensitive utility that becomes more concave with higher anxiety.
    
    Parameters (model_parameters):
    - alpha0: [0,1] baseline learning-rate scalar multiplied by associability.
    - beta: [0,10] inverse temperature for both stages.
    - phi: [0,1] associability update rate (how quickly associability tracks |PE|).
    - anx_neg: [0,1] scales amplification of negative PEs as (1 + anx_neg*stai).
    - risk_sens: [0,1] risk-sensitivity strength; higher makes utility more concave with anxiety.
    
    Inputs:
    - action_1: int array of length T; 0 or 1 (spaceship).
    - state: int array of length T; 0 or 1 (planet).
    - action_2: int array of length T; 0 or 1 (alien).
    - reward: float array of length T; coins in [0,1].
    - stai: array-like length 1; anxiety score in [0,1].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha0, beta, phi, anx_neg, risk_sens = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Associability terms per state-action at stage-2, and a single term for stage-1 actions
    A1 = np.ones(2) * 0.5
    A2 = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Utility transform exponent: more concave with higher anxiety and risk_sens
    # exponent in (0,1]: 1 means linear; lower means more risk-averse
    util_exp = max(0.05, 1.0 - 0.7 * risk_sens * stai)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r_raw = reward[t]

        # Risk-sensitive utility
        r = (r_raw + 1e-12) ** util_exp

        # Policies
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Stage-2 TD update with associability and anxiety-weighted negative PE
        delta2 = r - q2[s, a2]
        delta2_eff = delta2 if delta2 >= 0 else (1.0 + anx_neg * stai) * delta2
        A2[s, a2] = (1.0 - phi) * A2[s, a2] + phi * abs(delta2_eff)
        alpha2_eff = alpha0 * np.clip(A2[s, a2], 0.0, 1.0)
        q2[s, a2] += alpha2_eff * delta2_eff

        # Stage-1 bootstrapped TD using current state-action value and associability at stage-1
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        # Update stage-1 associability with absolute TD
        A1[a1] = (1.0 - phi) * A1[a1] + phi * abs(delta1)
        alpha1_eff = alpha0 * np.clip(A1[a1], 0.0, 1.0)
        q1[a1] += alpha1_eff * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'phi', 'anx_neg', 'risk_sens']"
iter4_run0_participant22.json,cognitive_model3,530.5255738532702,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planning with confirmation-biased transition learning modulated by anxiety.
    
    This model plans using a learned transition matrix and second-stage model-free values.
    The transition learning rate exhibits confirmation bias: if the observed transition
    matches the most likely transition under current beliefs, the update rate differs from
    when it contradicts expectations. Anxiety increases this asymmetry.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for second-stage Q-values.
    - beta: [0,10] inverse temperature for both stages.
    - trans_lr: [0,1] baseline transition learning rate.
    - conf0: [0,1] baseline confirmation bias strength (0=no asymmetry, 1=strong).
    - anx_conf: [0,1] scales how much anxiety amplifies confirmation bias.
    
    Inputs:
    - action_1: int array of length T; 0 or 1 (spaceship).
    - state: int array of length T; 0 or 1 (planet).
    - action_2: int array of length T; 0 or 1 (alien).
    - reward: float array of length T; coins in [0,1].
    - stai: array-like length 1; anxiety score in [0,1].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, trans_lr, conf0, anx_conf = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize transitions with moderate prior; learn from experience
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    q2 = np.zeros((2, 2))  # model-free second-stage values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Model-based first-stage Q by planning through T and q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # First-stage policy
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Update second-stage values (MF)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Confirmation-biased transition learning for the chosen action
        # Determine whether the observed state matches current expectation
        expected_state = int(np.argmax(T[a1, :]))
        is_confirming = 1 if s == expected_state else 0

        # Effective bias magnitude (0 to ~2*conf) with anxiety amplification
        bias_mag = conf0 * (1.0 + anx_conf * stai)
        # Adjust learning rate: increase for confirming updates, decrease for disconfirming
        # Map to multiplier in [1 - bias_mag, 1 + bias_mag]
        lr_mult = 1.0 + (2 * is_confirming - 1) * bias_mag
        lr_mult = max(0.0, lr_mult)  # avoid negative
        lr_eff = max(0.0, min(1.0, trans_lr * lr_mult))

        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = (1.0 - lr_eff) * T[a1, :] + lr_eff * one_hot_s
        # Renormalize and clip
        T[a1, :] = np.clip(T[a1, :], 1e-8, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'trans_lr', 'conf0', 'anx_conf']"
iter4_run0_participant29.json,cognitive_model2,527.7861340786236,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive model with anxiety-modulated loss aversion and directed exploration.
    The agent encodes utility with asymmetric weighting of misses vs. gains; anxiety increases loss sensitivity.
    Additionally, it uses a directed exploration bonus that decreases with visit count; anxiety dampens exploration.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for second-stage utilities and first-stage MF backup
        beta: [0,10] â inverse temperature for both stages
        kappa_loss: [0,1] â baseline weight on losses (1 - reward)
        phi_bonus: [0,1] â baseline directed exploration bonus scale
        anx_risk: [0,1] â anxiety gain scaling for loss aversion; also reduces exploration

    Returns
    - Negative log-likelihood of observed action_1 and action_2 sequences.
    """"""
    alpha, beta, kappa_loss, phi_bonus, anx_risk = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective parameters
    kappa_eff = np.clip(kappa_loss * (1.0 + anx_risk * stai_val), 0.0, 1.0)
    phi_eff = phi_bonus * (1.0 - 0.7 * stai_val * anx_risk)  # anxiety reduces directed exploration

    # Utility-based Q for second stage
    q2 = np.zeros((2, 2))
    # First-stage MF values (utility domain)
    q1_mf = np.zeros(2)

    # Fixed transition model (commonly 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Visit counts for directed exploration bonus
    visit_counts = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Directed exploration bonus per state-action
        bonus = phi_eff / np.sqrt(1.0 + visit_counts)

        # Model-based evaluation of first stage: expected max of (q2 + bonus) in each state
        mb_state_values = np.max(q2 + bonus, axis=1)  # for X and Y
        q1_mb = transition_matrix @ mb_state_values

        # Use purely MB at first stage but also allow MF backup to shape learning (not decision)
        q1_eval = q1_mb

        # First-stage choice probability
        logits1 = beta * (q1_eval - np.max(q1_eval))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second stage choice probability with bonus
        s = state[t]
        logits2 = beta * ((q2[s] + bonus[s]) - np.max(q2[s] + bonus[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Update counts and values
        visit_counts[s, a2] += 1.0

        # Risk-sensitive utility transform: reward vs miss (1 - r)
        r = reward[t]
        util = r - kappa_eff * (1.0 - r)  # in [-kappa_eff, 1]; encourages avoiding misses under high anxiety

        # Second-stage update in utility space
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First-stage MF backup (not used for decision this model but learned for completeness)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'kappa_loss', 'phi_bonus', 'anx_risk']"
iter4_run0_participant29.json,cognitive_model3,527.2448365844375,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-Representation-inspired first-stage evaluation with anxiety-modulated discounting and forgetting.
    The agent learns second-stage MF values and a compact state-occupancy predictor for first-stage actions.
    Anxiety reduces the effective planning horizon (discount) and increases forgetting of learned values.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for Q-values (both stages)
        beta_base: [0,10] â base inverse temperature for both stages
        gamma0: [0,1] â baseline discount factor for SR-style evaluation
        sr_forget: [0,1] â base forgetting rate applied to Q-values
        anx_gamma: [0,1] â how strongly anxiety reduces the discount and increases forgetting

    Returns
    - Negative log-likelihood of observed action_1 and action_2 sequences.
    """"""
    alpha, beta_base, gamma0, sr_forget, anx_gamma = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated discount and forgetting
    gamma_eff = np.clip(gamma0 * (1.0 - 0.6 * anx_gamma * stai_val), 0.0, 1.0)
    forget_eff = np.clip(sr_forget * (1.0 + 0.8 * anx_gamma * stai_val), 0.0, 1.0)

    beta = beta_base

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    # Successor-like predictor: occupancy of states from first-stage actions (rows: actions; cols: states)
    # Initialize with the known common transitions
    M = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Also track transitions via counts to refine M
    trans_counts = np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Compute transition probabilities
        trans_probs = trans_counts / np.sum(trans_counts, axis=1, keepdims=True)

        # Update SR-like matrix mildly toward empirical transitions (one-step SR with discount)
        # Here SR reduces to discounted immediate transitions
        M = (1.0 - alpha) * M + alpha * trans_probs

        # First-stage values via SR: discounted expected planet value
        v_states = np.max(q2, axis=1)
        q1_sr = gamma_eff * (M @ v_states)

        # First-stage choice
        logits1 = beta * (q1_sr - np.max(q1_sr))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Apply anxiety-modulated forgetting to Q-values before update
        q2 *= (1.0 - forget_eff)

        # Second-stage MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Transition counts update to refine M
        trans_counts[a1, s] += 1.0

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha', 'beta_base', 'gamma0', 'sr_forget', 'anx_gamma']"
iter4_run0_participant3.json,cognitive_model1,509.2382656868491,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-uncertainty aversion with learned transitions and nonlinear utility.

    Core mechanisms:
    - Learn a transition model T(a1 -> s2) online.
    - Learn stage-2 Q-values via TD(0) using a nonlinear utility of reward.
    - First-stage is purely model-based, but penalizes actions whose transition row is uncertain
      via an entropy penalty on T (uncertainty aversion).
    - Anxiety increases the weight of the uncertainty penalty and also increases concavity
      of the reward utility (risk sensitivity).

    Parameters and bounds:
    - action_1: int array (n_trials,), values in {0,1}; first-stage choices taken by the participant.
    - state:    int array (n_trials,), values in {0,1}; second-stage state reached (X=0, Y=1).
    - action_2: int array (n_trials,), values in {0,1}; second-stage action (alien) chosen.
    - reward:   float array (n_trials,), in [0,1]; obtained coins.
    - stai:     float array with a single float in [0,1]; anxiety score.
    - model_parameters: iterable of five parameters:
        alpha_v in [0,1]: learning rate for stage-2 Q-values
        beta    in [0,10]: inverse temperature for softmax at both stages
        tau_M   in [0,1]: learning rate for the transition model T
        chi_u   in [0,1]: baseline weight on transition uncertainty (entropy) penalty at stage-1
        rho_r   in [0,1]: baseline exponent for utility u(r)=r^rho (concavity)

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_v, beta, tau_M, chi_u, rho_r = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize learned transition model; start from common/rare prior
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q2(s2, a2) values
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    # Anxiety effects:
    # - stronger aversion to transition uncertainty
    chi_eff = chi_u * (0.5 + 0.5 * s_anx)  # scales up to chi_u for s_anx=1
    # - more concave utility with anxiety: interpolate rho toward 0.5 with anxiety
    rho_eff = 0.5 * s_anx + (1.0 - s_anx) * max(1e-6, rho_r)

    for t in range(n_trials):

        # Compute model-based Q1 from learned transitions and current Q2
        vmax = np.max(Q2, axis=1)  # value of each second-stage state

        # Entropy of transition rows (uncertainty)
        # H(p) = -sum p log p
        row_entropy = np.zeros(2)
        for a1_idx in range(2):
            p_row = np.clip(T[a1_idx], eps, 1.0)
            row_entropy[a1_idx] = -(p_row[0] * np.log(p_row[0]) + p_row[1] * np.log(p_row[1]))

        q1_mb = T @ vmax
        # Penalize actions with uncertain transitions
        q1 = q1_mb - chi_eff * row_entropy

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = state[t]
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and apply nonlinear utility
        r_raw = reward[t]
        # Ensure r in [0,1], then apply u(r) = r^rho_eff
        r_u = max(0.0, min(1.0, r_raw)) ** rho_eff

        # Update transition model T with simple delta rule toward one-hot observed state
        oh = np.array([1.0 if i == s2 else 0.0 for i in range(2)], dtype=float)
        T[a1] = (1.0 - tau_M) * T[a1] + tau_M * oh
        # Renormalize row
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # TD(0) update for Q2
        delta2 = r_u - Q2[s2, a2]
        Q2[s2, a2] += alpha_v * delta2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_v', 'beta', 'tau_M', 'chi_u', 'rho_r']"
iter4_run0_participant3.json,cognitive_model2,398.351608844604,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-gated asymmetric learning with eligibility and stickiness; hybrid MB/MF arbitration.

    Core mechanisms:
    - Stage-2 Q-values learned via TD(0) with asymmetric learning rates for negative vs positive
      prediction errors, where anxiety amplifies negative-learning rate.
    - Stage-1 MF values receive eligibility-based credit assignment from stage-2 TD error.
    - Stage-1 decision is a hybrid: weighted combination of model-based (fixed transitions) and
      model-free values. Arbitration weight decreases with anxiety (more anxious -> less MB).
    - Action stickiness (perseveration) at both stages.

    Parameters and bounds:
    - action_1: int array (n_trials,), values in {0,1}
    - state:    int array (n_trials,), values in {0,1}
    - action_2: int array (n_trials,), values in {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with a single float in [0,1]
    - model_parameters: iterable of five parameters:
        alpha0   in [0,1]: baseline learning rate
        beta     in [0,10]: inverse temperature (both stages)
        neg_boost in [0,1]: scales extra learning from negative PEs, amplified by anxiety
        trace_w in [0,1]: eligibility weight from stage-2 PE to stage-1 MF
        stick   in [0,1]: strength of action stickiness

    Returns:
    - Negative log-likelihood of the observed choices.
    """"""
    alpha0, beta, neg_boost, trace_w, stick = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure (known common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = -1
    eps = 1e-12

    for t in range(n_trials):

        # Model-based projection from Q2 via fixed transitions
        vmax = np.max(Q2, axis=1)
        Q1_mb = T @ vmax

        # Anxiety reduces MB reliance
        w_mb = max(0.0, min(1.0, 1.0 - s_anx))
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stickiness/logit bias
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        logits1 = beta * Q1 + stick * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        s2 = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0

        logits2 = beta * Q2[s2] + stick * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD error at stage 2
        delta2 = r - Q2[s2, a2]

        # Asymmetric, anxiety-gated learning rate
        if delta2 < 0.0:
            alpha_eff = alpha0 * (1.0 + neg_boost * s_anx)
        else:
            alpha_eff = alpha0 * (1.0 - 0.5 * neg_boost * s_anx)
        alpha_eff = max(0.0, min(1.0, alpha_eff))

        # Update Q2
        Q2[s2, a2] += alpha_eff * delta2

        # Eligibility assignment to stage-1 MF value of chosen action
        Q1_mf[a1] += trace_w * alpha_eff * delta2

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'neg_boost', 'trace_w', 'stick']"
iter4_run0_participant3.json,cognitive_model3,516.7233714535346,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-sensitive exploration and transition-type avoidance shaped by anxiety.

    Core mechanisms:
    - Learn Q2 via TD(0) and track per-state volatility via running estimate of absolute PE.
    - Stage-1 hybrid MB/MF choice: MB uses expected Q2 but subtracts a volatility penalty
      (discouraging transitions to volatile states), where anxiety strengthens the penalty.
    - Anxiety also promotes avoidance of repeating an action after a rare transition (post-rare
      avoidance bias at stage-1).
    - Fixed transition structure (common=0.7), with optional MF mixture at stage-1.

    Parameters and bounds:
    - action_1: int array (n_trials,), values in {0,1}
    - state:    int array (n_trials,), values in {0,1}
    - action_2: int array (n_trials,), values in {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with a single float in [0,1]
    - model_parameters: iterable of five parameters:
        alpha    in [0,1]: learning rate for Q2 (and volatility tracker)
        beta     in [0,10]: inverse temperature for softmax
        gamma    in [0,1]: baseline penalty strength for volatility in MB values
        w_mix    in [0,1]: weight of model-based control at stage-1 (1=MB only)
        bias_pr  in [0,1]: strength of post-rare avoidance bias at stage-1

    Returns:
    - Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, gamma, w_mix, bias_pr = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    # Volatility estimate per second-stage state (higher means more surprising outcomes)
    vol = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # For post-rare avoidance bias, track previous trial's transition rarity and action
    prev_a1 = -1
    prev_was_rare = False

    eps = 1e-12

    for t in range(n_trials):

        # Compute MB action values as expected vmax minus volatility penalty
        vmax = np.max(Q2, axis=1)
        # Anxiety-strengthened volatility penalty
        gamma_eff = gamma * (0.5 + 0.5 * s_anx)
        # Expected volatility cost per action
        vol_cost = T @ vol
        Q1_mb = (T @ vmax) - gamma_eff * vol_cost

        # Hybrid with MF
        Q1 = w_mix * Q1_mb + (1.0 - w_mix) * Q1_mf

        # Post-rare avoidance bias: if last transition was rare, bias against repeating prev_a1
        bias_vec = np.zeros(2)
        if prev_a1 in (0, 1) and prev_was_rare:
            # Anxiety scales the bias strength
            bias_vec[prev_a1] = -bias_pr * (0.5 + 0.5 * s_anx)

        # Stage-1 policy
        logits1 = beta * Q1 + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = state[t]
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 TD update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha * delta2

        # Update volatility estimate with running absolute PE
        vol[s2] += alpha * (abs(delta2) - vol[s2])

        # MF backup to Q1 (simple SARSA-style from observed Q2)
        Q1_mf[a1] += alpha * (Q2[s2, a2] - Q1_mf[a1])

        # Determine if current transition was rare for next-trial bias
        # Common if (A->X) or (U->Y)
        is_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        prev_was_rare = (not is_common)
        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'gamma', 'w_mix', 'bias_pr']"
iter4_run0_participant31.json,cognitive_model1,502.07017297563493,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with risk-sensitive utility modulated by anxiety.

    Core ideas:
    - Stage-2 learning is model-free but uses a concave utility transform u(r) = r^(1 - rho_eff).
      Higher anxiety increases effective risk aversion (larger rho_eff), making utilities more
      concave and dampening learning from high rewards.
    - Stage-1 action values are a mixture of model-based (via transitions) and model-free values,
      weighted by omega.
    - Policies at both stages use a softmax with inverse temperature beta.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choice within state (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Obtained rewards (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety increases risk aversion in utility.
    model_parameters : array-like of floats, length 5
        [alpha, beta, omega, rho0, k_anx_risk]
        - alpha in [0,1]: learning rate for MF values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - omega in [0,1]: weight on model-based value at stage 1.
        - rho0 in [0,1]: baseline risk aversion shaping the utility exponent.
        - k_anx_risk in [0,1]: how much anxiety increases risk aversion.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, omega, rho0, k_anx_risk = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition model: rows are A,U; columns are planets X,Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)       # MF for spaceships A,U
    q_stage2_mf = np.zeros((2, 2))  # MF for aliens on planet X (row 0) and Y (row 1)

    # Anxiety-modulated risk parameter
    rho_eff = rho0 + k_anx_risk * stai
    if rho_eff < 0.0:
        rho_eff = 0.0
    if rho_eff > 1.0:
        rho_eff = 1.0

    # bounded beta
    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    for trial in range(n_trials):

        # Model-based evaluation at stage 1 from current MF stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)              # best alien expected value per planet
        q_stage1_mb = transition_matrix @ max_q_stage2          # expected value per spaceship via transitions

        # Hybrid stage-1 value
        q1_hybrid = omega * q_stage1_mb + (1.0 - omega) * q_stage1_mf

        # policy for the first choice
        logits1 = beta_eff * q1_hybrid
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[trial]
        p_choice_1[trial] = p1[a1]

        # policy for the second choice
        s = state[trial]
        logits2 = beta_eff * q_stage2_mf[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[trial]
        p_choice_2[trial] = p2[a2]

        # Utility-transform the reward (risk-sensitive, anxiety-modulated)
        r = reward[trial]
        # Ensure input is within [0,1]; then apply concave utility
        if r < 0.0:
            r = 0.0
        if r > 1.0:
            r = 1.0
        util = r ** (1.0 - rho_eff)

        # Learning updates
        # Stage-1 MF bootstraps from the chosen stage-2 action value (SARSA-style)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Stage-2 MF learns from utility-transformed outcome
        delta2 = util - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss

","['alpha', 'beta', 'omega', 'rho0', 'k_anx_risk']"
iter4_run0_participant31.json,cognitive_model2,417.00652978671894,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-adaptive exploration with anxiety scaling and choice perseveration (purely MF).

    Core ideas:
    - A purely model-free SARSA learner at both stages.
    - The inverse temperature used to generate choices is adapted on each trial based on the
      previous trial's surprise |r - Q|. Greater surprise leads to more exploration (lower beta).
    - Anxiety amplifies the effect of surprise on exploration.
    - A choice perseveration kernel adds a bias toward repeating the last chosen action at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S or P/H).
    reward : array-like of float
        Obtained rewards (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety increases surprise-driven exploration.
    model_parameters : array-like of floats, length 5
        [alpha, beta, chi_surp, pi, xi_anx]
        - alpha in [0,1]: learning rate for Q.
        - beta in [0,10]: base inverse temperature.
        - chi_surp in [0,1]: strength of surprise impact on exploration.
        - pi in [0,1]: perseveration bias magnitude added to last chosen action at each stage.
        - xi_anx in [0,1]: scales how strongly anxiety amplifies surprise-driven exploration.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, chi_surp, pi, xi_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    prev_a1 = None
    prev_a2 = [None, None]  # separate memory per state for stage-2

    # Surprise from previous trial initializes to moderate level
    prev_surprise = 0.5

    for t in range(n_trials):

        # Trial-wise inverse temperature: larger surprise => more exploration (smaller beta)
        # beta_t = beta * exp(-chi_surp * (1 + xi_anx*stai) * prev_surprise)
        scale = chi_surp * (1.0 + xi_anx * stai) * prev_surprise
        if scale < 0.0:
            scale = 0.0
        beta_t = beta * np.exp(-scale)
        if beta_t < 0.0:
            beta_t = 0.0
        if beta_t > 10.0:
            beta_t = 10.0

        # Stage-1 softmax with perseveration
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += pi
        logits1 = beta_t * q1 + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 softmax with state-specific perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += pi
        logits2 = beta_t * q2[s] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning updates (SARSA-style bootstrapping)
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update memories for perseveration
        prev_a1 = a1
        prev_a2[s] = a2

        # Compute surprise to inform next trial's exploration
        # Use absolute RPE magnitude at stage-2 as surprise proxy
        prev_surprise = abs(delta2)
        if prev_surprise > 1.0:
            prev_surprise = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'chi_surp', 'pi', 'xi_anx']"
iter4_run0_participant31.json,cognitive_model3,394.2602534591813,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Leaky-belief model-based planner with anxiety-modulated volatility and stickiness.

    Core ideas:
    - Maintain belief-like values over second-stage options that drift toward an uninformative prior
      via a leak term (captures assumed volatility). Anxiety increases effective leak (more volatility).
    - Choices are purely model-based at stage 1: value of a spaceship is the transition-weighted
      max over the planet's current alien beliefs.
    - A stickiness bias encourages repeating the previous choices at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S or P/H).
    reward : array-like of float
        Obtained rewards (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety increases effective leak (perceived volatility).
    model_parameters : array-like of floats, length 5
        [alpha, beta, zeta_leak, k_anx_vol, stick]
        - alpha in [0,1]: learning rate toward observed reward at stage 2.
        - beta in [0,10]: inverse temperature for softmax policies (both stages).
        - zeta_leak in [0,1]: baseline leak toward 0.5 prior on every trial before learning.
        - k_anx_vol in [0,1]: how much anxiety increases the leak (volatility).
        - stick in [0,1]: stickiness bias added to the last chosen action at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices at both stages.
    """"""
    alpha, beta, zeta_leak, k_anx_vol, stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition model: rows are A,U; columns are planets X,Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Belief-like values for aliens on each planet (initialized to neutral 0.5)
    q2 = np.ones((2, 2)) * 0.5
    q1_mf_placeholder = np.zeros(2)  # not used for learning, but needed for delta form below

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2 = [None, None]

    # Effective leak increases with anxiety
    leak_eff = zeta_leak * (1.0 + k_anx_vol * stai)
    if leak_eff < 0.0:
        leak_eff = 0.0
    if leak_eff > 1.0:
        leak_eff = 1.0

    # Bound beta
    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    for t in range(n_trials):

        # Model-based evaluation at stage 1 using current beliefs
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Stage-1 policy with stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick
        logits1 = beta_eff * q1_mb + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with stickiness within the encountered state
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick
        logits2 = beta_eff * q2[s] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Leak toward prior (0.5) to capture volatility assumptions (anxiety-modulated)
        q2 = (1.0 - leak_eff) * q2 + leak_eff * 0.5

        # Learning: move the chosen second-stage value toward the outcome
        r = reward[t]
        if r < 0.0:
            r = 0.0
        if r > 1.0:
            r = 1.0
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Optional placeholder to maintain delta structure (not updating q1 values, purely MB)
        _ = q1_mf_placeholder  # ensures the placeholder is used

        # Update stickiness memories
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'zeta_leak', 'k_anx_vol', 'stick']"
iter4_run0_participant32.json,cognitive_model1,351.22296948861555,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Hybrid MB/MF with anxiety-weighted arbitration and volatility tracking.

    Idea:
    - Learns second-stage action values (model-free) with a single learning rate.
    - Computes model-based first-stage values via known transition matrix and current second-stage values.
    - Arbitration weight w_t between MB and MF depends on baseline w0 and a running estimate of outcome volatility.
      Anxiety increases the impact of volatility, shifting weight toward MF when volatility is high.
    - Includes first-stage choice stickiness.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1) per trial (aliens within a planet).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score array; uses stai[0] in [0,1] space.
    model_parameters : list or array-like of float
        [alpha, beta, w0, tau_vol, kappa_stick]
        Bounds:
        - alpha: [0,1] learning rate for value updates at both stages.
        - beta: [0,10] inverse temperature for both stages.
        - w0: [0,1] baseline weight on model-based values at the first stage.
        - tau_vol: [0,1] learning rate for a running volatility estimate of the outcome process.
        - kappa_stick: [0,1] strength of first-stage choice stickiness.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, w0, tau_vol, kappa_stick = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    # Initialize value functions
    q1_mf = np.zeros(2)         # model-free first-stage values
    q2 = np.zeros((2, 2))       # second-stage values per state and action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Running volatility estimate of outcome-generating process
    vol = 0.0

    # Stickiness memory
    prev_a1 = None

    for t in range(n_trials):
        # Model-based first-stage values from current second-stage values
        max_q2 = np.max(q2, axis=1)             # best value in each second-stage state
        q1_mb = transition_matrix @ max_q2      # expected value for A/U via transitions

        # Anxiety-weighted arbitration: higher anxiety + higher volatility reduces MB weight
        w = w0 * (1.0 - np.clip(stai * vol, 0.0, 1.0))
        w = np.clip(w, 0.0, 1.0)

        # First-stage policy with stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        stick = kappa_stick * (0.5 + 0.5 * stai)  # higher anxiety -> stronger stickiness
        q1_mix = w * q1_mb + (1.0 - w) * q1_mf
        logits1 = beta * (q1_mix - np.max(q1_mix)) + stick * bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        q2_s = q2[s].copy()
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]

        # Update second-stage value (model-free)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update first-stage MF value towards realized second-stage value (TD(1))
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update volatility estimate (unsigned PE of second stage)
        vol = (1.0 - tau_vol) * vol + tau_vol * np.abs(pe2)

        # Update stickiness memory
        prev_a1 = a1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'w0', 'tau_vol', 'kappa_stick']"
iter4_run0_participant32.json,cognitive_model2,341.25283444885827,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Risk-sensitive second-stage learning and transition-surprise bonus at first stage, anxiety-modulated.

    Idea:
    - Second-stage utility is mean-variance: u = r - z_risk_eff * Var_est, where Var_est â q*(1-q) for Bernoulli rewards.
      Anxiety increases effective risk sensitivity, penalizing uncertain options.
    - First-stage model-free values receive a transient surprise bonus based on transition improbability (rare > common).
      Anxiety amplifies the influence of surprise on subsequent first-stage values/policy.
    - Includes first-stage choice persistence.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices per trial (0/1).
    reward : array-like of float
        Reward outcome per trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha, beta, z_risk, xi_surp, persev]
        Bounds:
        - alpha: [0,1] learning rate for both stages.
        - beta: [0,10] inverse temperature for both stages.
        - z_risk: [0,1] baseline risk aversion weight for variance penalty at second stage.
        - xi_surp: [0,1] baseline transition-surprise bonus strength at first stage.
        - persev: [0,1] first-stage choice persistence strength.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, z_risk, xi_surp, persev = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition probabilities
    trans = np.array([[0.7, 0.3],
                      [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Compute first-stage policy with persistence and surprise bias (from previous transition)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        stick = persev * (0.5 + 0.5 * stai)

        # Surprise bonus computed on the fly for the action actually taken this trial after observing transition
        # For the choice policy, we use the pre-transition expectation of surprise from last trial (zero at t=0).
        # Implement as an action-dependent additive bias derived from immediate transition likelihood.
        # We'll compute it after seeing the state, then apply to MF update (not to current softmax since state not yet known).
        surprise_bias = np.zeros(2)  # zero for the policy; surprise affects learning below

        logits1 = beta * (q1_mf - np.max(q1_mf)) + stick * bias1 + surprise_bias
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Second-stage policy (risk sensitivity only influences learning, not policy temperature)
        q2_s = q2[s].copy()
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second-stage update with risk-sensitive utility (penalize high-variance options)
        # Bernoulli variance estimate for chosen option: v â q*(1-q)
        q_sa = q2[s, a2]
        var_est = q_sa * (1.0 - q_sa)
        z_eff = z_risk * (0.5 + 0.5 * stai)  # anxiety increases risk penalty
        u = r - z_eff * var_est
        pe2 = u - q_sa
        q2[s, a2] += alpha * pe2

        # First-stage MF update with transition surprise bonus
        # Surprise = -log P(s | a1). Higher surprise -> larger bonus magnitude
        p_s_given_a = trans[a1, s]
        surpr = -np.log(max(p_s_given_a, 1e-8))
        xi_eff = xi_surp * (0.5 + 0.5 * stai)
        bonus = xi_eff * surpr  # nonnegative

        target1 = q2[s, a2] + bonus
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'z_risk', 'xi_surp', 'persev']"
iter4_run0_participant32.json,cognitive_model3,381.3500946193102,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    MB win-stay/lose-switch heuristic bias with anxiety-scaled lapses and eligibility trace.

    Idea:
    - Core model-free learner (both stages) with eligibility trace from second stage to first stage.
    - Adds a model-based heuristic bias on first-stage choices: after a rewarded common transition, tend to repeat;
      after a rewarded rare transition, tend to switch; patterns invert after losses. Anxiety amplifies this bias.
    - Includes an epsilon-lapse component that increases with anxiety, blending uniform choice with softmax.
    - Separate softmax is used at both stages; epsilon applies at both stages.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage actions per trial (0/1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Trait anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha, beta, eps0, tau_comm, eta_trace]
        Bounds:
        - alpha: [0,1] learning rate for value updates.
        - beta: [0,10] inverse temperature for both stages.
        - eps0: [0,1] baseline lapse probability; effective epsilon increases with anxiety.
        - tau_comm: [0,1] strength of MB win-stay/lose-switch heuristic bias at first stage.
        - eta_trace: [0,1] eligibility trace strength from second-stage PE to first-stage value update.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, eps0, tau_comm, eta_trace = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure for determining common vs. rare
    # Common: A->X (0->0) and U->Y (1->1) with prob 0.7
    common_map = {(0, 0): True, (0, 1): False, (1, 0): False, (1, 1): True}

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous trial info for heuristic bias
    prev_a1 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        # Compute heuristic bias for first-stage action based on previous trial outcome and transition type
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            was_common = common_map[(prev_a1, prev_s)]
            # Heuristic: after rewarded common -> repeat; rewarded rare -> switch
            # after non-rewarded common -> switch; non-rewarded rare -> repeat
            tendency_repeat = (prev_r > 0 and was_common) or (prev_r == 0 and not was_common)
            a_repeat = prev_a1
            a_switch = 1 - prev_a1
            tau_eff = tau_comm * (0.5 + 0.5 * stai)  # anxiety amplifies heuristic
            if tendency_repeat:
                bias1[a_repeat] += tau_eff
            else:
                bias1[a_switch] += tau_eff

        # First-stage softmax with lapse
        logits1 = beta * (q1_mf - np.max(q1_mf)) + bias1
        probs1_soft = np.exp(logits1 - np.max(logits1))
        probs1_soft = probs1_soft / np.sum(probs1_soft)
        eps = np.clip(eps0 * (0.5 + 0.5 * stai), 0.0, 1.0)  # anxiety increases lapse
        probs1 = (1.0 - eps) * probs1_soft + eps * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with lapse
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2_soft = np.exp(logits2 - np.max(logits2))
        probs2_soft = probs2_soft / np.sum(probs2_soft)
        probs2 = (1.0 - eps) * probs2_soft + eps * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Second-stage TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First-stage TD update with eligibility trace from second-stage PE
        # Update the chosen first-stage action toward the updated second-stage value,
        # and also add an eligibility component proportional to the immediate second-stage PE.
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * (pe1 + eta_trace * pe2)

        # Store for next trial heuristic
        prev_a1 = a1
        prev_s = s
        prev_r = r

    tiny = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + tiny)) + np.sum(np.log(p_choice_2 + tiny)))
    return neg_log_lik","['alpha', 'beta', 'eps0', 'tau_comm', 'eta_trace']"
iter4_run0_participant34.json,cognitive_model1,425.9181149224843,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-suppressed model-based control and eligibility traces.
    
    The agent blends model-free (MF) and model-based (MB) values at the first stage.
    Anxiety reduces the MB weight, shifting control toward MF. A TD(Î»)-style
    eligibility trace propagates second-stage value prediction errors back to the
    first stage. Single learning rate is used for both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1 for the aliens within the state) for each trial.
    reward : array-like of float
        Reward obtained on each trial (e.g., 0 or 1 coins).
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alphaQ, beta, w_mb0, k_anx_w, lambda_elig]
        - alphaQ in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - w_mb0 in [0,1]: baseline weight for MB control at stage 1.
        - k_anx_w in [0,1]: how strongly anxiety reduces MB weight
          (effective w_mb = clip(w_mb0 * (1 - k_anx_w * stai), 0, 1)).
        - lambda_elig in [0,1]: eligibility trace from stage 2 PE to stage 1 MF value.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alphaQ, beta, w_mb0, k_anx_w, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective MB weight reduced by anxiety
    w_mb = w_mb0 * (1.0 - k_anx_w * stai_val)
    if w_mb < 0.0:
        w_mb = 0.0
    if w_mb > 1.0:
        w_mb = 1.0

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    for t in range(n_trials):
        # Model-based first-stage action values via one-step lookahead
        max_q2 = np.max(q2_mf, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        # Hybrid Q for policy
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Softmax for stage 1
        logits1 = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = q2_mf[s] - np.max(q2_mf[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD updates
        # Stage 2
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alphaQ * delta2

        # Stage 1 MF with eligibility trace from stage 2 value
        boot = q2_mf[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alphaQ * lambda_elig * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alphaQ', 'beta', 'w_mb0', 'k_anx_w', 'lambda_elig']"
iter4_run0_participant34.json,cognitive_model2,427.06731134144445,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""MF with anxiety-amplified uncertainty bonus, value decay, and second-stage stickiness.
    
    The agent is model-free but augments first-stage action preferences with an
    exploration bonus proportional to expected uncertainty of the next state.
    Uncertainty is higher when second-stage action values in a state are similar.
    Anxiety scales this uncertainty bonus, encouraging exploration under higher anxiety.
    Values decay toward 0.5 to capture forgetting/drift. Second-stage choices exhibit
    within-state stickiness (perseveration).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien 0 or 1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher means greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, k_unc_anx, tau_forget, zeta_stick2]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - k_unc_anx in [0,1]: scales how strongly anxiety increases the uncertainty bonus.
          Bonus scale = k_unc_anx * stai.
        - tau_forget in [0,1]: per-trial decay of Q-values toward 0.5.
        - zeta_stick2 in [0,1]: weight for second-stage within-state stickiness.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, k_unc_anx, tau_forget, zeta_stick2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure for computing expected uncertainty
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    bonus_scale = k_unc_anx * stai_val

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Track previous second-stage choices per state for stickiness
    prev_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        # Value decay toward 0.5 (forgetting/drift)
        q1 = (1.0 - tau_forget) * q1 + tau_forget * 0.5
        q2 = (1.0 - tau_forget) * q2 + tau_forget * 0.5

        # Compute state uncertainty: high when the two aliens are similar
        # u_s in [0,1], where 1 means maximally uncertain (values equal), 0 means certain (values far apart by 1).
        diff = np.abs(q2[:, 0] - q2[:, 1])
        u_state = 1.0 - diff  # already bounded since q in [0,1] typically

        # Expected uncertainty bonus for each first-stage action
        # b[a] = E[u_state | choose a] using transition matrix
        bonus = T @ u_state  # shape (2,)

        # First-stage policy: MF Q plus anxiety-weighted uncertainty bonus
        logits1 = q1 + bonus_scale * bonus
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with within-state stickiness
        s = state[t]
        stick_vec = np.zeros(2)
        if prev_a2[s] != -1:
            stick_vec[prev_a2[s]] = 1.0

        logits2 = q2[s] + zeta_stick2 * stick_vec
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Bootstrap to stage 1
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        q1[a1] += alpha * delta1

        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'k_unc_anx', 'tau_forget', 'zeta_stick2']"
iter4_run0_participant34.json,cognitive_model3,372.3292480672933,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid with anxiety-biased transition learning and first-stage perseveration.
    
    The agent learns the first-stage transition probabilities online and uses them
    for model-based evaluation. Anxiety biases transition learning: rare transitions
    (relative to current belief) are learned faster when anxiety is high, while
    common transitions are learned more conservatively. Additionally, first-stage
    choices exhibit perseveration that scales with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien 0 or 1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alphaQ, beta, tau_T, k_anx_trans_bias, psi_perseverate]
        - alphaQ in [0,1]: learning rate for MF values (both stages).
        - beta in [0,10]: inverse temperature for both stages.
        - tau_T in [0,1]: base learning rate for transition probabilities.
        - k_anx_trans_bias in [0,1]: scales anxiety-dependent modulation of transition learning.
          Effective tau: increased for rare transitions, decreased for common ones:
          tau_eff = tau_T * (1 + k_anx_trans_bias * stai) if rare else tau_T * (1 - k_anx_trans_bias * stai).
        - psi_perseverate in [0,1]: base perseveration weight at stage 1; actual bias is psi_perseverate * stai.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alphaQ, beta, tau_T, k_anx_trans_bias, psi_perseverate = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize transition model; start uninformative (0.5/0.5) to allow learning
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    prev_a1 = None

    for t in range(n_trials):
        # Model-based first-stage values from current transition estimate
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Perseveration at stage 1 scales with anxiety
        stick_vec = np.zeros(2)
        if prev_a1 is not None:
            stick_vec[prev_a1] = 1.0
        perseveration_bias = psi_perseverate * stai_val * stick_vec

        # Combine MB and MF equally (no extra parameter to keep total <= 5)
        q1_comb = 0.5 * q1_mb + 0.5 * q1_mf

        # Stage 1 policy
        logits1 = q1_comb + perseveration_bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model for the chosen first-stage action a1
        # Determine if observed transition was ""rare"" under current belief
        p_obs = T[a1, s]
        is_rare = p_obs < 0.5
        if is_rare:
            tau_eff = tau_T * (1.0 + k_anx_trans_bias * stai_val)
        else:
            tau_eff = tau_T * (1.0 - k_anx_trans_bias * stai_val)
        # Clamp tau_eff into [0,1] to be safe
        if tau_eff < 0.0:
            tau_eff = 0.0
        if tau_eff > 1.0:
            tau_eff = 1.0

        # Row-wise update toward the observed state s
        # Increase T[a1, s], decrease the other entry to keep row stochastic
        T[a1, s] += tau_eff * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] = 1.0 - T[a1, s]

        # MF learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alphaQ * delta2

        # MF bootstrap to stage 1
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alphaQ * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alphaQ', 'beta', 'tau_T', 'k_anx_trans_bias', 'psi_perseverate']"
iter4_run0_participant35.json,cognitive_model1,422.16014569355275,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-weighted arbitration with anxiety-amplified stickiness.

    Overview
    - Learns second-stage values model-free and propagates to the first stage (MF).
    - Computes a model-based estimate from the fixed transition model.
    - Arbitrates (blends) MB and MF by an uncertainty signal derived from running second-stage PE^2.
    - Higher anxiety increases reliance on MF under uncertainty and increases choice stickiness.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1).
    - reward: array-like of floats in [0,1], outcomes.
    - stai: array-like length-1 with scalar in [0,1], anxiety score.
    - model_parameters: tuple/list of 5 params
        alpha2: stage-2 learning rate in [0,1]
        beta: inverse temperature for both stages in [0,10]
        phi_arbit: base arbitration weight in [0,1], scales MB influence
        kappa_stick: base choice stickiness in [0,1]
        zeta_unc: scales how much uncertainty reduces MB control with anxiety in [0,1]

    Mechanisms and anxiety usage
    - Uncertainty u_t ~ running average of PE^2 at stage-2: higher u_t â lower MB weight.
    - Effective MB weight w_mb_t = clip( phi_arbit*(1 - 0.4*stai) * (1 - (zeta_unc*stai)*u_t) ).
      Thus anxiety amplifies the uncertainty penalty on MB control.
    - Choice stickiness magnitude = kappa_stick * (0.5 + 0.5*stai).
      Anxiety increases perseveration.

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha2, beta, phi_arbit, kappa_stick, zeta_unc = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: AâX common, UâY common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)        # MF action values at stage 1 for A/U
    q2 = np.zeros((2, 2))      # Second-stage state-action values: states X/Y x aliens 0/1

    # Uncertainty tracker (running mean PE^2 at stage-2)
    unc = 0.0                  # scalar uncertainty summary
    alpha_unc = max(1e-6, min(1.0, 0.25 + 0.5 * alpha2))  # link to learning rate

    # Choice stickiness (keeps previous choice bias)
    prev_a1 = None
    stick_mag = kappa_stick * (0.5 + 0.5 * stai)
    stick_vec = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):

        # MB shortcut: expected best second-stage value per planet
        max_q2 = np.max(q2, axis=1)            # [X, Y]
        q1_mb = T @ max_q2                     # project planets to ships

        # Arbitration weight with uncertainty and anxiety
        w_base = max(0.0, min(1.0, phi_arbit * (1.0 - 0.4 * stai)))
        penalty = max(0.0, min(1.0, (zeta_unc * stai) * unc))
        w_mb_t = max(0.0, min(1.0, w_base * (1.0 - penalty)))

        # Stage-1 decision values + stickiness
        if prev_a1 is not None:
            stick_vec[:] = 0.0
            stick_vec[prev_a1] = stick_mag
        else:
            stick_vec[:] = 0.0

        q1_dec = w_mb_t * q1_mb + (1.0 - w_mb_t) * q1_mf + stick_vec

        # Softmax stage 1
        q1c = q1_dec - np.max(q1_dec)
        probs1 = np.exp(beta * q1c)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = max(eps, probs1[a1])

        # Stage-2 policy
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = max(eps, probs2[a2])

        # Outcomes and learning
        r = reward[t]

        # Stage-2 PE and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Update uncertainty (running PE^2)
        unc = (1.0 - alpha_unc) * unc + alpha_unc * (pe2 * pe2)
        # Clip to [0,1] (rewards in [0,1] -> PE in [-1,1] -> PE^2 <= 1)
        unc = max(0.0, min(1.0, unc))

        # Stage-1 MF bootstrapping from realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1  # uses same alpha2 to limit params

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return nll

","['alpha2', 'beta', 'phi_arbit', 'kappa_stick', 'zeta_unc']"
iter4_run0_participant35.json,cognitive_model2,448.08885168912946,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning model with anxiety-sensitive rare-transition aversion and forgetting.

    Overview
    - Learns its own transition matrix T_est via delta rule.
    - Computes model-based Q1 from learned transitions and blends with MF Q1 using a confidence term.
    - Confidence = 1 - entropy(row), per chosen ship; higher entropy -> less MB control.
    - Anxiety increases aversion to rare transitions and increases value forgetting (decay).
    - Second stage learned model-free and bootstrapped to stage 1.

    Parameters
    - action_1: array-like of ints {0,1}, first-stage actions.
    - state: array-like of ints {0,1}, reached state (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage actions.
    - reward: array-like of floats in [0,1].
    - stai: array-like length-1, scalar in [0,1].
    - model_parameters: 5 parameters
        alpha2: stage-2 learning rate in [0,1]
        beta: inverse temperature for both stages in [0,10]
        alpha_T: transition learning rate in [0,1]
        omega_rare: base penalty for rare transitions in [0,1]
        psi_decay: base value decay rate in [0,1]

    Mechanisms and anxiety usage
    - Transition learning: T_est[a, :] updated toward observed state with alpha_T.
    - Confidence-weighted arbitration: w_mb_t = conf(a1)*(1 - 0.6*stai), conf = 1 - H(T_est[a1,:])/H_max.
    - Rare-transition aversion: add bias against choosing the ship that just produced a rare transition,
      magnitude = omega_rare * (0.5 + 0.5*stai).
    - Forgetting: q2 and q1_mf decay toward 0 each trial by psi_eff = psi_decay*(0.3 + 0.7*stai).

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha2, beta, alpha_T, omega_rare, psi_decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transitions near canonical structure
    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Action values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Rare transition tracker for biasing next choice
    last_rare = np.zeros(2)  # indicator per action (A/U) if last time it led to rare transition
    rare_penalty_base = omega_rare * (0.5 + 0.5 * stai)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    H_max = np.log(2.0)

    for t in range(n_trials):

        # Apply forgetting
        psi_eff = psi_decay * (0.3 + 0.7 * stai)
        q2 *= (1.0 - psi_eff)
        q1_mf *= (1.0 - psi_eff)

        # Compute MB plan from learned transitions
        max_q2 = np.max(q2, axis=1)      # [X, Y]
        q1_mb = T_est @ max_q2

        # Confidence of current action's transition (we approximate using each ship's entropy)
        ent_A = -np.sum(T_est[0] * np.log(T_est[0] + eps))
        ent_U = -np.sum(T_est[1] * np.log(T_est[1] + eps))
        conf = np.array([1.0 - ent_A / H_max, 1.0 - ent_U / H_max])  # in [0,1]

        # Anxiety reduces MB weight
        w_mb_vec = conf * (1.0 - 0.6 * stai)  # per action weights used as linear blending proxies

        # Rare-transition aversion bias against ships recently rare
        bias_rare = -rare_penalty_base * last_rare

        # Combine MB and MF with action-specific weights (approximate by averaging)
        w_mb = np.mean(w_mb_vec)
        w_mb = max(0.0, min(1.0, w_mb))
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias_rare

        # Stage-1 softmax
        q1c = q1_dec - np.max(q1_dec)
        probs1 = np.exp(beta * q1c)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = max(eps, probs1[a1])

        # Stage-2 policy softmax
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = max(eps, probs2[a2])

        r = reward[t]

        # Determine if transition was rare relative to T_est
        # Rare if prob of observed state under chosen action < 0.5 of the larger prob in that row
        row = T_est[a1]
        common_prob = np.max(row)
        rare_now = 1 if row[s] < (0.5 * common_prob) else 0
        last_rare = np.zeros(2)
        last_rare[a1] = rare_now

        # Update transitions by delta rule toward one-hot of observed state
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T_est[a1] = (1.0 - alpha_T) * T_est[a1] + alpha_T * target_row
        # Ensure normalization (should already sum to 1)
        T_est[a1] /= np.sum(T_est[a1])

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage-1 MF bootstrapping
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

    nll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return nll

","['alpha2', 'beta', 'alpha_T', 'omega_rare', 'psi_decay']"
iter4_run0_participant35.json,cognitive_model3,414.0484576885359,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk- and safety-sensitive learning with anxiety-modulated eligibility and planet safety bias.

    Overview
    - Uses an eligibility-trace-like update: stage-2 PE also adjusts stage-1 MF via lambda.
    - Tracks per (state, action) reward variance and applies risk-averse shaping to decision values.
    - Anxiety increases risk aversion and strengthens a safety-seeking planet bias (toward lower-variance planet).
    - Second-stage softmax uses risk-shaped Q-values; first-stage decision includes MB projection plus safety bias.

    Parameters
    - action_1: array-like of ints {0,1}, first-stage actions.
    - state: array-like of ints {0,1}, reached state (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage actions (0/1).
    - reward: array-like of floats in [0,1].
    - stai: array-like length-1, scalar in [0,1].
    - model_parameters: 5 parameters
        alpha: base learning rate in [0,1] (used for both stages)
        beta: inverse temperature in [0,10]
        lambda_elig: base eligibility strength in [0,1]
        nu_risk: base risk-aversion weight in [0,1] (penalizes reward variance)
        chi_alarm: base safety-bias weight in [0,1] added at stage 1

    Mechanisms and anxiety usage
    - Eligibility: lambda_eff = clip(lambda_elig * (0.5 + 0.5*stai)).
      Higher anxiety increases credit assignment from stage-2 PE to stage-1.
    - Risk shaping at stage 2: Q2_tilde = Q2 - nu_eff * Var2, with
      nu_eff = nu_risk * (0.5 + 0.5*stai). Var2 is an EW variance estimate per (state,action).
    - Safety bias at stage 1: compute planet variances VarX, VarY (mean over actions).
      Bias toward lower-variance planet, magnitude = chi_eff = chi_alarm * (0.5 + 0.5*stai).
      Action bias = T @ [bias_X, bias_Y] where bias for safer planet is +chi_eff, riskier is -chi_eff.
    - MB planning uses fixed transitions.

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha, beta, lambda_elig, nu_risk, chi_alarm = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and variance trackers
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    # Track EW mean and EW variance per (state,action) for rewards
    m2 = np.zeros((2, 2))   # running mean
    v2 = np.zeros((2, 2))   # running variance (EW)
    alpha_var = max(1e-6, min(1.0, 0.25 + 0.5 * alpha))  # smoother variance learning

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    lambda_eff = max(0.0, min(1.0, lambda_elig * (0.5 + 0.5 * stai)))
    nu_eff = max(0.0, min(1.0, nu_risk * (0.5 + 0.5 * stai)))
    chi_eff = max(0.0, min(1.0, chi_alarm * (0.5 + 0.5 * stai)))

    eps = 1e-12

    for t in range(n_trials):

        # Compute safety bias from planet variances
        # Use average across actions as planet variance summary
        VarX = float(np.mean(v2[0]))
        VarY = float(np.mean(v2[1]))

        if VarX < VarY:
            desirability = np.array([+chi_eff, -chi_eff])  # prefer X
        elif VarY < VarX:
            desirability = np.array([-chi_eff, +chi_eff])  # prefer Y
        else:
            desirability = np.array([0.0, 0.0])

        action_bias = T @ desirability  # bias added to A/U

        # MB from current Q2 (max per planet)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MB and MF (simple average; eligibility will couple learning)
        q1_dec = 0.5 * q1_mb + 0.5 * q1_mf + action_bias

        # Stage-1 policy
        q1c = q1_dec - np.max(q1_dec)
        probs1 = np.exp(beta * q1c)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = max(eps, probs1[a1])

        # Stage-2 risk-shaped policy
        # Penalize options with higher estimated reward variance
        s = int(state[t])
        q2_risk_adj = q2[s] - nu_eff * v2[s]
        q2c = q2_risk_adj - np.max(q2_risk_adj)
        probs2 = np.exp(beta * q2c)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = max(eps, probs2[a2])

        # Outcome
        r = reward[t]

        # Update EW mean/variance for (s,a2)
        # EW mean
        m_prev = m2[s, a2]
        m_new = (1.0 - alpha_var) * m_prev + alpha_var * r
        # EW variance update using squared deviation to new mean
        # v_new = (1 - alpha_var)*v_prev + alpha_var*(r - m_prev)^2 adjusted; use standard EW var update
        dev = r - m_prev
        v_new = (1.0 - alpha_var) * v2[s, a2] + alpha_var * (dev * dev)
        m2[s, a2] = m_new
        v2[s, a2] = max(0.0, min(1.0, v_new))

        # Stage-2 PE and update (MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update via eligibility trace using stage-2 PE
        q1_mf[a1] += alpha * lambda_eff * pe2

    nll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return nll","['alpha', 'beta', 'lambda_elig', 'nu_risk', 'chi_alarm']"
iter4_run0_participant39.json,cognitive_model1,366.67405318680903,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Kalman hybrid RL with anxiety-modulated uncertainty and temperature scaling.

    The agent maintains Gaussian beliefs (mean and variance) over second-stage
    values for each alien and updates them using a Kalman filter. Model-based
    values at the first stage are computed from the current second-stage value
    means. Uncertainty reduces second-stage choice precision and anxiety inflates
    perceived process/measurement noise, further lowering effective precision.
    Planning weight is reduced by anxiety; a perseveration bias is present at
    stage 1.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), reached second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; alien choice)
    - reward:   np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai:     np.array with scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters
        beta: [0,10] base inverse temperature for both stages
        proc_base: [0,1] baseline process noise (diffusion) for second-stage values
        mix_mb_base: [0,1] baseline planning weight at stage 1
        stick1: [0,1] perseveration strength at stage 1
        unc_temp_base: [0,1] strength of uncertainty-based softmax temperature reduction at stage 2

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    beta, proc_base, mix_mb_base, stick1, unc_temp_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition structure (A->X, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Second-stage value means and variances (Kalman)
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.25  # initial uncertainty

    # First-stage model-free values (kept minimal; used only if stickiness requires baseline)
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Process noise increases with anxiety (more perceived volatility)
    tau = (0.001 + 0.499 * proc_base) * (1.0 + stai0)
    # Measurement noise inflated by anxiety
    sigma2 = 0.20 + 0.30 * stai0  # within reasonable range for Bernoulli rewards

    # Planning weight reduced by anxiety
    w_mb = np.clip(mix_mb_base * (1.0 - 0.5 * stai0), 0.0, 1.0)
    # Perseveration increased by anxiety
    kappa = stick1 * (1.0 + stai0)

    prev_a1 = None
    eps = 1e-12

    for t in range(n_trials):
        # Diffusion: increase uncertainty for all second-stage actions
        q2_var += tau

        # Model-based Q at stage 1 from current q2 means
        max_q2 = np.max(q2_mean, axis=1)  # size 2
        q1_mb = transition_matrix @ max_q2

        # Combine with stage-1 MF (kept at zero baseline) and stickiness
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        if prev_a1 is not None:
            stick = np.zeros(2); stick[prev_a1] = 1.0
            q1 = q1 + kappa * stick

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with uncertainty-modulated temperature
        s2 = state[t]
        # Use average uncertainty in current state to scale temperature
        u_state = np.mean(q2_var[s2])
        u_norm = u_state / (u_state + sigma2 + eps)  # in [0,1)
        beta2 = beta * max(1e-3, 1.0 - unc_temp_base * u_norm * (1.0 + stai0))

        q2 = q2_mean[s2]
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta2 * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and Kalman update for visited (s2, a2)
        r = reward[t]
        # Prior variance already diffused by tau above
        v_prior = q2_var[s2, a2]
        K = v_prior / (v_prior + sigma2 + eps)
        pe = r - q2_mean[s2, a2]
        q2_mean[s2, a2] = q2_mean[s2, a2] + K * pe
        q2_var[s2, a2] = (1.0 - K) * v_prior

        # Optional minimal MF bootstrapping for stage-1 (helps identifiability)
        q1_target = q2_mean[s2, a2]
        q1_mf[a1] += 0.05 * (q1_target - q1_mf[a1])  # tiny fixed-rate update

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['beta', 'proc_base', 'mix_mb_base', 'stick1', 'unc_temp_base']"
iter4_run0_participant39.json,cognitive_model2,475.58141234973573,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Dual learning rates with eligibility trace and anxiety-modulated sensitivity to rare transitions.

    The agent learns second-stage values with asymmetric learning rates for
    positive vs negative prediction errors. Stage-1 values are updated via an
    eligibility trace that propagates second-stage prediction errors backward.
    First-stage choices are a hybrid of model-based and model-free values; the
    planning weight is modulated by whether the transition was rare vs common,
    and anxiety reduces the rare-transition upweighting.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), reached second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; alien choice)
    - reward:   np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai:     np.array with scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters
        alpha_pos: [0,1] learning rate for positive PEs at stage 2
        alpha_neg: [0,1] learning rate for negative PEs at stage 2
        beta:      [0,10] inverse temperature for both stages
        w_mb_base: [0,1] baseline planning weight (before transition- and anxiety-modulation)
        lambda_elig: [0,1] eligibility trace factor for propagating PE2 to stage 1

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_pos, alpha_neg, beta, w_mb_base, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q at stage 1
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Determine if observed transition was rare
        a1 = action_1[t]
        s2 = state[t]
        is_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        rare = 0 if is_common else 1

        # Planning weight with rare-transition gain that is dampened by anxiety
        # If rare==1, weight shifts by factor proportional to (1 - 2*stai): low anxiety upweights, high anxiety downweights
        rare_gain = 1.0 + 0.6 * (1.0 - 2.0 * stai0) * rare
        w_mb = np.clip(w_mb_base * rare_gain, 0.0, 1.0)

        # Hybrid stage-1 value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2 = q2_mf[s2]
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2_mf[s2, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2_mf[s2, a2] += alpha2 * pe2

        # Eligibility-trace update at stage 1 (propagate stage-2 PE back)
        # Anxiety reduces effective trace (shallower backpropagation)
        lam_eff = lambda_elig * (1.0 - 0.5 * stai0)
        q1_mf[a1] += lam_eff * alpha2 * pe2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'w_mb_base', 'lambda_elig']"
iter4_run0_participant39.json,cognitive_model3,477.5445742094423,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Controllability-belief hybrid RL with anxiety-modulated lapse and strategic common-ship bias.

    The agent learns both reward values and an internal belief about transition
    controllability (how often chosen ships go to their common planets). The
    controllability belief scales the contribution of model-based values at the
    first stage. Anxiety reduces perceived controllability and increases a lapse
    probability (random responding). A strategic bias favors the ship that more
    commonly reaches the currently more valuable planet; anxiety attenuates this bias.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), reached second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; alien choice)
    - reward:   np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai:     np.array with scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters
        alpha: [0,1] learning rate for second-stage values and controllability belief
        beta_base: [0,10] base inverse temperature for both stages
        ctrl_base: [0,1] baseline scaling for model-based control via controllability belief
        lapse_base: [0,1] base lapse probability coefficient
        bias_common_base: [0,1] strength of strategic bias toward the ship leading to the better planet

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta_base, ctrl_base, lapse_base, bias_common_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))  # second-stage MF values
    q1_mf = np.zeros(2)    # optional MF at stage 1 (kept minimal via bootstrap)

    # Controllability belief phi: tracks probability that chosen ship goes to its common planet
    phi = 0.7  # prior consistent with task

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based component from current q2
        max_q2 = np.max(q2, axis=1)  # value of X and Y
        q1_mb = transition_matrix @ max_q2

        # Anxiety reduces perceived controllability; ctrl scales MB contribution
        ctrl_eff = np.clip(ctrl_base * phi * (1.0 - stai0), 0.0, 1.0)

        # Strategic common-ship bias: favor ship that tends to reach the better planet
        # If planet X currently looks better, bias toward ship A; else toward ship U.
        better_X = max_q2[0] >= max_q2[1]
        bias_vec = np.array([1.0, 0.0]) if better_X else np.array([0.0, 1.0])
        bias_strength = bias_common_base * (1.0 - stai0)  # anxiety attenuates strategy use

        # Combine values
        q1 = ctrl_eff * q1_mb + (1.0 - ctrl_eff) * q1_mf + bias_strength * bias_vec

        # Policies with lapse; anxiety increases lapse
        beta = beta_base
        lapse = np.clip(lapse_base * stai0, 0.0, 1.0)

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        soft1 = np.exp(beta * q1c)
        soft1 = soft1 / (np.sum(soft1) + eps)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (with same beta; no lapse at stage 2 to keep identifiability clean)
        s2 = state[t]
        q2c = q2[s2] - np.max(q2[s2])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2
        # Minimal bootstrap to stage-1 MF
        q1_mf[a1] += 0.1 * alpha * (q2[s2, a2] - q1_mf[a1])

        # Update controllability belief phi: did we see a common transition?
        is_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        obs = 1.0 if is_common else 0.0
        phi += alpha * (obs - phi)
        # Keep phi within [0,1]
        phi = min(1.0, max(0.0, phi))

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta_base', 'ctrl_base', 'lapse_base', 'bias_common_base']"
iter4_run0_participant4.json,cognitive_model3,477.8149050486296,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Online transition learning with anxiety-biased uncertainty and stickiness.

    Overview
    - Learns the transition matrix online from observed transitions.
    - Stage-1 uses model-based values computed from learned transitions.
    - Anxiety inflates perceived transition uncertainty by blending learned transitions
      toward uniform, reducing planning precision.
    - Stage-2 values learned via RescorlaâWagner.
    - Includes stickiness at stage 1 to capture perseveration.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha: [0,1]      Learning rate for Q updates at stage 2 (and bootstrapping).
    - beta:  [0,10]     Inverse temperature for softmax.
    - tau_T: [0,1]      Learning rate for updating transition probabilities.
    - zeta_pess: [0,1]  Strength of anxiety-driven pessimistic/uncertain blending of transitions.
    - xi_stick: [0,1]   Stickiness added to the previously chosen stage-1 action.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, tau_T, zeta_pess, xi_stick].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, tau_T, zeta_pess, xi_stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition probabilities close to common/rare
    T_hat = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Stage values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2) + 0.5  # MF backup for stage-1 (from stage-2 PE)

    # Stickiness (only stage 1)
    prev_a1 = None

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Anxiety-biased transition blending toward uniform (more uncertainty with higher stai)
        blend = zeta_pess * stai  # in [0,1]
        T_eff = (1.0 - blend) * T_hat + blend * 0.5  # 0.5 for both outcomes

        # Model-based Q for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_eff @ max_q2

        # Add MF component (simple hybrid 50-50 for robustness)
        q1_h = 0.5 * q1_mb + 0.5 * q1_mf

        # Add stickiness to previously chosen action
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += xi_stick

        logits1 = beta * q1_h + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = probs2[a2]

        # Learning: stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # MF backup at stage 1 from stage-2 PE
        q1_mf[a1] += alpha * pe2

        # Transition learning: update row a1 toward observed state s
        onehot = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_hat[a1] = (1.0 - tau_T) * T_hat[a1] + tau_T * onehot
        # Ensure normalization (numerical stability)
        T_row_sum = np.sum(T_hat[a1])
        if T_row_sum > 0:
            T_hat[a1] /= T_row_sum

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)","['alpha', 'beta', 'tau_T', 'zeta_pess', 'xi_stick']"
iter4_run0_participant40.json,cognitive_model1,536.8045198604475,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-weighted arbitration and stage-2 stickiness.
    
    The agent combines a model-based (MB) estimate using the known transition
    structure with a model-free (MF) estimate learned from experience. Anxiety
    down-weights the MB contribution. Stage-2 choices include a stickiness bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (0/1 alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, chi, kappa2, nu)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - chi in [0,1]: baseline weight on model-based control at stage 1.
        - kappa2 in [0,1]: stage-2 stickiness strength within a planet.
        - nu in [0,1]: anxiety sensitivity that reduces MB weight.
          Effective MB weight: omega = chi * (1 - nu*stai).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha2, beta, chi, kappa2, nu = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed, known transition structure (rows: A/U; cols: X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities of the observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q2 = np.zeros((2, 2))         # stage-2 state-action values
    Q1_MF = np.zeros(2)           # stage-1 model-free values

    # Stage-2 stickiness: separate previous action per planet
    prev_a2 = [None, None]

    # Effective MB weight shaped by anxiety
    omega = chi * (1.0 - nu * stai_val)
    if omega < 0.0:
        omega = 0.0
    if omega > 1.0:
        omega = 1.0

    for t in range(n_trials):

        # Model-based stage-1 values: expectation over next-state max Q2
        max_Q2 = np.max(Q2, axis=1)          # per state
        Q1_MB = T @ max_Q2                   # per action

        # Hybrid value for stage-1
        Q1_hyb = omega * Q1_MB + (1.0 - omega) * Q1_MF

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_hyb
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (with within-state stickiness)
        s2 = int(state[t])
        a2 = int(action_2[t])

        bias2 = np.zeros(2)
        if prev_a2[s2] is not None:
            bias2[prev_a2[s2]] = kappa2

        logits2 = beta * Q2[s2] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Propagate MF credit to stage-1 chosen action (bootstrapped by observed stage-2 value)
        # This assigns credit to the chosen first-stage action in proportion to the realized value at stage 2.
        target_Q1 = Q2[s2, a2]
        Q1_MF[a1] += alpha2 * (target_Q1 - Q1_MF[a1])

        # Update stickiness memory
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'chi', 'kappa2', 'nu']"
iter4_run0_participant5.json,cognitive_model1,440.3608894162873,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-gated transition-surprise arbitration (common vs. rare transition dependent MB weight).

    Idea:
    - Stage 1 uses a hybrid of model-based (MB) and model-free (MF) values.
    - The arbitration weight depends on whether the previous transition was common or rare.
      After common transitions, participants tend to rely more on MB control; after rare
      transitions, they rely more on MF control. Anxiety accentuates this asymmetry by
      boosting MF reliance after rare transitions and dampening MB reliance after rare transitions.
    - Stage 2 is standard MF with learning rate alpha.

    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the planet).
    reward : array-like of float in [0,1]
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score. Higher values amplify the gap between rare- vs common-based arbitration
        by further reducing MB weight after rare transitions.
    model_parameters : array-like of float
        [alpha, beta, wC0, wR0]
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - wC0 in [0,1]: baseline MB weight used after common transitions.
        - wR0 in [0,1]: baseline MB weight used after rare transitions.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, wC0, wR0 = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Fixed transition structure of the task
    T = np.array([[0.7, 0.3],  # action A -> X common, Y rare
                  [0.3, 0.7]]) # action U -> X rare,   Y common

    # Value functions
    q1_mf = np.zeros(2)         # MF values for first-stage actions
    q2 = 0.5 * np.ones((2, 2))  # Second-stage Q-values

    # Anxiety-modulated arbitration weights:
    # Encourage MB more after common transitions (slightly boosted when anxiety is low)
    wC = np.clip(wC0 + (1.0 - st) * 0.2 * (1.0 - wC0), 0.0, 1.0)
    # Discourage MB after rare transitions, especially when anxiety is high
    wR = np.clip(wR0 - st * 0.4 * wR0, 0.0, 1.0)

    eps = 1e-12
    p1 = np.zeros(n)
    p2 = np.zeros(n)

    # Track previous transition type: start neutral (use average of wC/wR)
    prev_common = None

    for t in range(n):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based evaluation for current trial (before observing current transition)
        max_q2 = np.max(q2, axis=1)  # best option at each planet
        q1_mb = T @ max_q2

        # Arbitration weight for this decision based on previous transition type
        if prev_common is None:
            w_t = 0.5 * (wC + wR)
        else:
            w_t = wC if prev_common else wR

        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage 1 choice probability
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 choice probability
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Determine whether the observed transition was common or rare for the chosen action
        # Common if (A->X) or (U->Y)
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

        # Learning
        # Stage 2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward realized second-stage value (SARSA-like bootstrapping)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_common = is_common

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'wC0', 'wR0']"
iter4_run0_participant5.json,cognitive_model2,496.87258842999125,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-attenuated intrinsic exploration with count-based bonus and eligibility.

    Idea:
    - Stage 2 choices include an intrinsic, count-based exploration bonus that diminishes
      with experience (1/sqrt(N)), encouraging trying less-sampled aliens.
    - Anxiety reduces the exploration bonus (more anxious -> less intrinsic exploration).
    - Stage 1 is MF but uses an eligibility trace to propagate stage-2 prediction error
      back to stage-1; the effective trace is dampened by anxiety (weaker credit assignment).
    - Softmax uses a single beta across stages.

    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the planet).
    reward : array-like of float in [0,1]
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce the exploration bonus and the eligibility trace.
    model_parameters : array-like of float
        [alpha, beta, eta0, trace1]
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: inverse temperature for softmax.
        - eta0 in [0,1]: base scale of the intrinsic exploration bonus.
        - trace1 in [0,1]: base eligibility trace to backpropagate PE2 to stage-1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, eta0, trace1 = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Fixed transitions for computing model-based target if needed (not used in policy here)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Counts for intrinsic bonus
    N = np.ones((2, 2))  # start at 1 to avoid division by zero and to moderate early bonus

    # Anxiety effects
    eta_eff = np.clip(eta0 * (1.0 - st), 0.0, 1.0)           # less exploration when anxious
    lam_eff = np.clip(trace1 * (1.0 - 0.5 * st), 0.0, 1.0)   # reduced eligibility when anxious

    eps = 1e-12
    p1 = np.zeros(n)
    p2 = np.zeros(n)

    for t in range(n):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Intrinsic exploration bonus at stage 2 (state-dependent, action-dependent)
        bonus2 = eta_eff / np.sqrt(N[s])
        q2_aug = q2[s] + bonus2

        # Stage 1 policy: MF values (no direct MB in policy, but learning uses eligibility)
        logits1 = beta * (q1_mf - np.max(q1_mf))
        probs1 = np.exp(logits1); probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 policy with intrinsic bonus
        logits2 = beta * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(logits2); probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Update counts for visited second-stage pair (after using it for policy)
        N[s, a2] += 1.0

        # Learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility-style backpropagation of PE2 to stage 1 action value
        q1_mf[a1] += alpha * lam_eff * pe2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'eta0', 'trace1']"
iter4_run0_participant5.json,cognitive_model3,445.8662075524297,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-driven loss aversion and transition-reactive temperature with hybrid control.

    Idea:
    - Subjective utility is asymmetric: 0 outcomes are treated as aversive relative to 1.
      Anxiety increases this loss aversion (penalizes zero-reward outcomes more).
    - Softmax temperature drops after rare transitions, and this effect is stronger with anxiety
      (more randomness or caution after unexpected transitions).
    - Stage 1 policy is a hybrid of MB and MF, with MB weight determined by anxiety alone
      (lower anxiety -> more MB). Stage 2 is MF with the same alpha.

    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the planet).
    reward : array-like of float in [0,1]
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase loss aversion and increase temperature sensitivity
        to rare transitions.
    model_parameters : array-like of float
        [alpha, beta, phi0, kappa0]
        - alpha in [0,1]: learning rate for MF updates.
        - beta in [0,10]: baseline inverse temperature.
        - phi0 in [0,1]: base loss aversion (penalty weight on zero reward).
        - kappa0 in [0,1]: base sensitivity of beta to rare transitions (reduces beta after rare).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, phi0, kappa0 = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Transition matrix (known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Anxiety effects
    # Loss aversion applied to learning signal: u(r) = r - phi*(1-r)
    phi = np.clip(phi0 * (0.5 + 0.5 * st), 0.0, 1.0)
    # MB weight governed by anxiety: less anxious -> more MB
    w_mb = np.clip(0.5 * (1.0 - st) + 0.1, 0.0, 1.0)
    # Beta reduction after rare transitions, increasing with anxiety
    kappa = np.clip(kappa0 * (0.5 + 0.5 * st), 0.0, 1.0)

    eps = 1e-12
    p1 = np.zeros(n)
    p2 = np.zeros(n)

    prev_rare = False  # for temperature adjustment based on previous trial's transition

    for t in range(n):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Subjective utility
        # r in {0,1} -> u(r) = 1 if r=1; u(0) = -phi (aversive)
        u = r - phi * (1.0 - r)

        # Compute MB estimate for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Adjust temperature if the previous transition was rare
        beta_eff = beta * (1.0 - kappa * (1.0 if prev_rare else 0.0))
        beta_eff = np.clip(beta_eff, 0.0, 10.0)

        # Stage 1 choice probability
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1); probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 choice probability
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2); probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Determine whether current transition is rare for chosen action
        is_rare = (a1 == 0 and s == 1) or (a1 == 1 and s == 0)

        # Learning with subjective utility
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward realized stage-2 value (using subjective utility target)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_rare = is_rare

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)","['alpha', 'beta', 'phi0', 'kappa0']"
iter4_run0_participant7.json,cognitive_model2,505.8963635230251,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with directed exploration bonus modulated by anxiety and novelty.

    Overview:
    - Stage-2 values are learned via TD(0).
    - Both stages include a directed exploration bonus that decays with visit count (novelty seeking).
    - Anxiety reduces novelty seeking: higher stai dampens the exploration bonus.
    - Stage-1 decisions are a mixture of MB (from Q2) and MF values.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates (both stages).
    - beta: [0,10] inverse temperature for both stages.
    - w_mb: [0,1] weight on model-based control at stage 1.
    - b0: [0,1] baseline directed exploration strength.
    - k_anx: [0,1] degree to which anxiety reduces exploration; bonus *= (1 - k_anx*stai).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choices.
    - state: array of ints {0,1}, observed second-stage state.
    - action_2: array of ints {0,1}, second-stage choices.
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha, beta, w_mb, b0, k_anx).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w_mb, b0, k_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transitions
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Q-values and visit counters for bonuses
    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)
    N1 = np.ones(2)       # initialize to 1 to avoid huge bonuses on first trial
    N2 = np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective exploration scaling given anxiety
    bonus_scale = b0 * max(0.0, 1.0 - k_anx * stai)

    eps = 1e-12

    for t in range(n_trials):
        # Compute MB values from Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Directed exploration bonuses (decay with visits)
        B1 = bonus_scale / np.sqrt(N1)
        B2 = bonus_scale / np.sqrt(N2)

        # Stage-1 combined values with bonus
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf + B1

        # Stage-1 policy
        q1 = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with bonus
        s = state[t]
        q2v = (Q2[s] + B2[s]) - np.max(Q2[s] + B2[s])
        probs_2 = np.exp(beta * q2v)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Update visit counts post choice (counts drive next-trial bonuses)
        N1[a1] += 1.0
        N2[s, a2] += 1.0

        # Learning
        r = reward[t]
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_mb', 'b0', 'k_anx']"
iter4_run0_participant7.json,cognitive_model3,505.8963635230251,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated replay/generalization across unchosen actions.

    Overview:
    - Stage-2 values are learned via TD(0).
    - Additionally, after outcome, values of unchosen actions are updated via a replay/generalization
      process within the same state, controlled by g_eff.
    - Stage-1 MF values also receive a replay-like update towards the obtained stage-2 value for the
      unchosen first-stage action, scaled by g_eff (simulating counterfactual/replay updating).
    - Anxiety increases replay/generalization strength (more mental simulation when anxious).

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for direct (chosen) value updates.
    - beta: [0,10] inverse temperature for both stages.
    - w_mb: [0,1] weight on model-based control at stage 1.
    - g_base: [0,1] baseline replay/generalization strength.
    - k_g: [0,1] increase of replay with anxiety; g_eff = clip(g_base + k_g*stai, 0, 1).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choices.
    - state: array of ints {0,1}, observed second-stage state.
    - action_2: array of ints {0,1}, second-stage choices.
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha, beta, w_mb, g_base, k_g).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w_mb, g_base, k_g = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transitions (known)
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Q-values
    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    g_eff = max(0.0, min(1.0, g_base + k_g * stai))
    eps = 1e-12

    for t in range(n_trials):
        # Model-based at stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        q1 = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2v = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * q2v)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Standard TD update for chosen stage-2 action
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Replay/generalization: update unchosen stage-2 action within same state towards the same outcome
        a2_other = 1 - a2
        delta2_other = r - Q2[s, a2_other]
        Q2[s, a2_other] += alpha * g_eff * delta2_other

        # Stage-1 MF update for chosen action
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Replay-like update for unchosen first-stage action towards the obtained bootstrapped value
        a1_other = 1 - a1
        delta1_other = boot - Q1_mf[a1_other]
        Q1_mf[a1_other] += alpha * g_eff * delta1_other

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'w_mb', 'g_base', 'k_g']"
iter4_run0_participant8.json,cognitive_model1,415.900831024381,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free controller with anxiety-gated arbitration, 
    eligibility trace from stage 2 to stage 1, and anxiety-weighted confirmation bias.
    
    Summary
    -------
    - Learns second-stage action values (Q2) via a delta rule.
    - Maintains stage-1 model-free values (Q1_MF) updated via an eligibility trace from Q2.
    - Computes a model-based (MB) plan for stage 1 via the fixed transition matrix.
    - Arbitration weight between MB and MF is a logistic function of a baseline (arb0) 
      and the participant's anxiety (stai). Higher anxiety shifts arbitration toward MF.
    - Confirmation bias in learning at stage 2: positive and negative PEs are weighted
      differently as a function of anxiety and a bias strength parameter (conf).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet (0/1).
    reward : array-like of float
        Trial outcomes (0.0 or 1.0 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [lr, beta, arb0, lam, conf]
        - lr: base learning rate for value updates [0,1]
        - beta: inverse temperature for both stages [0,10]
        - arb0: baseline arbitration bias (positive favors MB); combined with stai [0,1]
        - lam: eligibility trace strength from stage 2 to stage 1 [0,1]
        - conf: confirmation bias strength (weights pos vs neg PE) [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    lr, beta, arb0, lam, conf = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (A -> X common, U -> Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize values
    Q2 = np.zeros((2, 2)) + 0.5  # state x action
    Q1_MF = np.zeros(2) + 0.0

    # Anxiety-gated arbitration: logistic on (arb0 - k*stai)
    # Higher anxiety reduces MB weight by shifting the logit negative.
    k_anx = 2.0  # fixed sensitivity to anxiety
    w_mb = 1.0 / (1.0 + np.exp(-(arb0 - k_anx * st)))  # in (0,1)

    for t in range(n_trials):
        # Model-based stage-1 Q as expected max Q2 under transitions
        max_Q2 = np.max(Q2, axis=1)  # length 2 (planet X, Y)
        Q1_MB = T @ max_Q2  # shape (2,)

        # Hybrid Q for stage 1
        Q1 = w_mb * Q1_MB + (1.0 - w_mb) * Q1_MF

        # Stage-1 softmax
        logits1 = Q1 - np.max(Q1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax (within reached state)
        s = state[t]
        logits2 = Q2[s] - np.max(Q2[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Confirmation bias: weight positive vs negative PE as a function of stai and conf
        pe2 = r - Q2[s, a2]
        pos = 1.0 if pe2 >= 0.0 else 0.0
        neg = 1.0 - pos
        # Effective learning rate for this PE:
        # positive PEs amplified by (1 + conf*st), negative PEs attenuated by (1 - conf*st)
        lr_eff = lr * (pos * (1.0 + conf * st) + neg * (1.0 - conf * st))
        lr_eff = min(max(lr_eff, 0.0), 1.0)
        Q2[s, a2] += lr_eff * pe2

        # Eligibility trace from stage 2 to stage 1 (model-free backup)
        td1 = Q2[s, a2] - Q1_MF[a1]
        Q1_MF[a1] += lam * lr * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr', 'beta', 'arb0', 'lam', 'conf']"
iter4_run0_participant8.json,cognitive_model2,525.347753002684,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planner with anxiety-modulated information bonus via uncertainty tracking.
    
    Summary
    -------
    - Learns second-stage Q-values (Q2) via delta rule.
    - Tracks action uncertainty U2 per planet-action via a leaky integrator of absolute PEs.
    - Adds an information-seeking bonus proportional to uncertainty when choosing at stage 2.
      The bonus strength decreases with anxiety (higher anxiety -> less directed exploration).
    - Stage 1 is purely model-based, rolling the bonus-adjusted Q2 through the known transitions.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0.0 or 1.0 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [lr, beta, info0, decay_u]
        - lr: learning rate for Q2 updates [0,1]
        - beta: inverse temperature for both stages [0,10]
        - info0: baseline information-bonus strength [0,1]
        - decay_u: uncertainty tracker update rate [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    lr, beta, info0, decay_u = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize value and uncertainty
    Q2 = np.zeros((2, 2)) + 0.5
    U2 = np.zeros((2, 2)) + 1.0  # start uncertain

    # Anxiety-modulated info bonus: higher anxiety -> smaller bonus
    k_info = info0 * (0.2 + 0.8 * (1.0 - st))  # in [0,1], shrinks with stai

    for t in range(n_trials):
        # Bonus-adjusted Q2 for both states
        Q2_bonus = Q2 + k_info * U2

        # Stage-1 MB planning using bonus-adjusted Q2
        max_Q2b = np.max(Q2_bonus, axis=1)
        Q1_MB = T @ max_Q2b

        # Stage-1 softmax
        logits1 = Q1_MB - np.max(Q1_MB)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with info bonus in the reached state
        s = state[t]
        logits2 = Q2_bonus[s] - np.max(Q2_bonus[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += lr * pe2

        # Uncertainty tracker: leaky integration of absolute PE
        U2[s, a2] = (1.0 - decay_u) * U2[s, a2] + decay_u * abs(pe2)
        # Keep uncertainty bounded to [0,1]
        U2[s, a2] = min(max(U2[s, a2], 0.0), 1.0)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr', 'beta', 'info0', 'decay_u']"
iter4_run0_participant8.json,cognitive_model3,370.269878049549,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Two-step SARSA(Î») with anxiety-modulated eligibility and temperature split,
    plus state-specific win-stay/lose-shift bias at stage 2.
    
    Summary
    -------
    - Purely model-free temporal-difference control across both stages.
    - Uses an eligibility trace (Î») to propagate stage-2 value to stage-1; Î» decreases with anxiety.
    - Separate effective temperature at stage 1 vs stage 2: stage-1 temperature increases with anxiety 
      (noisier choices), while stage-2 uses the base beta.
    - Adds a state-specific WSLS bias at stage 2: tendency to repeat the last action in the same planet
      after a win and to switch after a loss; the lose-shift component strengthens with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0.0 or 1.0 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [lr, beta, lam0, wsls0, temp_split]
        - lr: learning rate for Q updates [0,1]
        - beta: base inverse temperature [0,10]
        - lam0: baseline eligibility trace parameter [0,1]
        - wsls0: baseline WSLS strength at stage 2 [0,1]
        - temp_split: strength of anxiety effect on stage-1 temperature [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    lr, beta, lam0, wsls0, temp_split = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    Q1 = np.zeros(2) + 0.0
    Q2 = np.zeros((2, 2)) + 0.5

    # State-specific previous action and reward (for WSLS)
    prev_a2 = [-1, -1]  # for states 0 and 1
    prev_r = [0.0, 0.0]

    # Anxiety-modulated parameters
    lam = lam0 * (1.0 - 0.5 * st)  # higher anxiety -> shorter credit assignment
    lam = min(max(lam, 0.0), 1.0)

    # Stage-1 inverse temperature decreases with anxiety (more noise)
    beta1 = beta * (0.5 + (1.0 - temp_split * st))  # in [0.5*beta, 1.5*beta] if temp_split<=1
    beta1 = max(beta1, 0.0)

    # WSLS: stronger lose-shift with anxiety; keep win-stay near baseline
    wsls_win = wsls0 * (1.0 - 0.3 * st)
    wsls_lose = wsls0 * (1.0 + 0.7 * st)

    for t in range(n_trials):
        # Stage 1 policy
        logits1 = Q1 - np.max(Q1)
        probs1 = np.exp(beta1 * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with WSLS bias in reached state
        s = state[t]
        base2 = Q2[s].copy()

        # Construct bias towards repeating/swapping previous action in this state
        bias2 = np.zeros(2)
        if prev_a2[s] in (0, 1):
            if prev_r[s] >= 0.5:
                # Win-stay: add bias to previous action
                bias2[prev_a2[s]] += wsls_win
            else:
                # Lose-shift: add bias to the alternative action
                bias2[1 - prev_a2[s]] += wsls_lose

        logits2 = (base2 + bias2) - np.max(base2 + bias2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = reward[t]

        # TD learning at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += lr * pe2

        # Backup to stage 1 via eligibility trace using current Q2 of chosen action
        td1 = Q2[s, a2] - Q1[a1]
        Q1[a1] += lr * lam * td1

        # Update WSLS memory for this state
        prev_a2[s] = a2
        prev_r[s] = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['lr', 'beta', 'lam0', 'wsls0', 'temp_split']"
iter5_run0_participant0.json,cognitive_model1,496.0212532141265,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated associability with model-based planning, second-stage uncertainty bonus, and first-stage perseveration.

    Idea
    - Second-stage learning uses a dynamic associability (PearceâHall style): the learning rate adapts to unsigned prediction error.
    - Anxiety scales associability upward, producing faster, more volatile learning when stai is high.
    - First-stage choices are model-based using the fixed 0.7/0.3 transition structure.
    - An uncertainty bonus at stage 2 (based on current associability) promotes exploration; its impact is attenuated or amplified by stai.
    - First-stage perseveration bias favors repeating the most recent first-stage choice.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices on the planet (0 or 1).
    - reward: array-like (n_trials,), obtained reward (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 6 parameters, all used:
        alpha0   : initial associability/learning-rate seed for second stage.
        beta     : inverse temperature for both stages [0,10].
        kappa    : associability update inertia (higher -> slower changes).
        xi_ucb   : weight on the second-stage uncertainty bonus (directed exploration).
        pers1    : first-stage perseveration bias to repeat last first-stage action.
        phi_stai : multiplicative scaling of associability/uncertainty by anxiety.

    Returns
    - Negative log-likelihood over the observed first- and second-stage choices.
    """"""
    alpha0, beta, kappa, xi_ucb, pers1, phi_stai = model_parameters

    # Clip parameters to bounds
    alpha0 = min(1.0, max(0.0, alpha0))
    beta = min(10.0, max(1e-6, beta))
    kappa = min(1.0, max(0.0, kappa))
    xi_ucb = min(1.0, max(0.0, xi_ucb))
    pers1 = min(1.0, max(0.0, pers1))
    phi_stai = min(1.0, max(0.0, phi_stai))

    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (A->X common; U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions and tracking
    q2 = np.zeros((2, 2))         # second-stage Q
    assoc = np.ones((2, 2)) * alpha0  # associability per state-action
    last_a1 = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage MB values from second-stage Q
        max_q2 = np.max(q2, axis=1)  # best action per state
        q1_mb = T @ max_q2
        q1 = q1_mb.copy()

        # Add first-stage perseveration (bias towards last chosen first-stage action)
        if last_a1 is not None:
            q1[last_a1] += pers1

        # Softmax for first-stage policy
        exp1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with uncertainty bonus based on associability
        s = state[t]
        # Uncertainty (associability) scaled by anxiety
        ucb_bonus = xi_ucb * assoc[s] * (1.0 + phi_stai * st)
        q2_aug = q2[s] + ucb_bonus
        exp2 = np.exp(beta * (q2_aug - np.max(q2_aug)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update second-stage Q via anxiety-scaled associability
        r = reward[t]
        pe2 = r - q2[s, a2]

        # Update associability: A <- (1 - kappa) * A + kappa * |PE|
        assoc[s, a2] = (1.0 - kappa) * assoc[s, a2] + kappa * abs(pe2)
        # Effective learning rate with anxiety amplification
        alpha_eff = assoc[s, a2] * (1.0 + phi_stai * st)
        alpha_eff = min(1.0, max(0.0, alpha_eff))
        q2[s, a2] += alpha_eff * pe2

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'kappa', 'xi_ucb', 'pers1', 'phi_stai']"
iter5_run0_participant0.json,cognitive_model2,479.62318735086205,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with Dirichlet transition learning and anxiety-shaped structural priors plus second-stage perseveration.

    Idea
    - Transition probabilities are learned with a simple Dirichlet count model per first-stage action.
      The prior strength is increased with anxiety, making high-anxiety agents more conservative in updating transitions.
    - First-stage action values are a convex combination of model-based (using learned transitions) and model-free values.
    - Model-free first-stage value learns from second-stage reward via a simple TD rule.
    - Second-stage perseveration biases repeating the last second-stage choice at the same state.
    - Reward is also transformed by a concavity parameter influenced by anxiety (soft utility sensitivity).

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), obtained reward (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 6 parameters, all used:
        eta_r   : second-stage learning rate for Q2 (and MF backprop to Q1).
        beta    : inverse temperature for both stages [0,10].
        omega0  : baseline arbitration weight on model-based control (0=MF, 1=MB).
        nu0     : base transition prior strength (mapped to total pseudo-counts).
        chi_anx : anxiety gain on prior strength (higher stai -> stronger prior -> slower transition adaptation).
        kappa2  : second-stage perseveration bias to repeat last stage-2 action within the same state.

    Returns
    - Negative log-likelihood over the observed first- and second-stage choices.
    """"""
    eta_r, beta, omega0, nu0, chi_anx, kappa2 = model_parameters

    # Clip parameters
    eta_r = min(1.0, max(0.0, eta_r))
    beta = min(10.0, max(1e-6, beta))
    omega0 = min(1.0, max(0.0, omega0))
    nu0 = min(1.0, max(0.0, nu0))
    chi_anx = min(1.0, max(0.0, chi_anx))
    kappa2 = min(1.0, max(0.0, kappa2))

    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transitions via Dirichlet priors per first-stage action.
    # Prior strength depends on nu0 and anxiety.
    prior_strength = 2.0 + 20.0 * nu0 * (1.0 + chi_anx * st)
    # Prior means reflect common transitions (0.7/0.3)
    prior_A = np.array([0.7, 0.3]) * prior_strength
    prior_U = np.array([0.3, 0.7]) * prior_strength
    counts = np.vstack([prior_A, prior_U])  # shape (2 actions, 2 states)

    # Values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    # Track last second-stage choice per state for perseveration
    last_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Utility sensitivity: transform reward with concavity shaped by anxiety
    # rho_eff in [0,1]; higher anxiety -> stronger concavity (diminished sensitivity)
    rho_eff = max(0.0, min(1.0, 1.0 - 0.5 * st))  # no extra parameter; ties anxiety to reward curvature

    for t in range(n_trials):
        # Expected transitions from current counts
        T_hat = counts / np.sum(counts, axis=1, keepdims=True)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2

        # Combine MB and MF for stage 1
        q1 = omega0 * q1_mb + (1.0 - omega0) * q1_mf

        # First-stage policy
        exp1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with perseveration
        s = state[t]
        q2_aug = q2[s].copy()
        if last_a2[s] != -1:
            q2_aug[last_a2[s]] += kappa2
        exp2 = np.exp(beta * (q2_aug - np.max(q2_aug)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and apply utility curvature modulated by anxiety
        r_raw = reward[t]
        r_util = (r_raw ** rho_eff)  # with 0/1 rewards, this reduces sensitivity to partial rewards if present

        # Update second-stage Q
        pe2 = r_util - q2[s, a2]
        q2[s, a2] += eta_r * pe2

        # Update model-free first-stage Q via TD(1) from realized second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += eta_r * td1

        # Update transition counts (Dirichlet) for the chosen first-stage action
        counts[a1, s] += 1.0

        # Update perseveration memory
        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta_r', 'beta', 'omega0', 'nu0', 'chi_anx', 'kappa2']"
iter5_run0_participant0.json,cognitive_model3,528.4967656023553,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Directed exploration via pseudo-count bonuses, with anxiety-dependent exploration/exploitation and first-stage choice kernel.

    Idea
    - Stage-2: add a directed exploration bonus b2 ~ phi_b2 / sqrt(N_s,a + 1) encouraging trying less-visited actions.
    - Stage-1: model-based values plus a directed exploration bonus based on transition uncertainty (via action visit counts),
      b1 ~ phi_b1 / sqrt(N_a + 1); the bonus is attenuated by anxiety (higher stai -> less directed exploration).
    - Anxiety also reduces effective beta (more random exploration): beta_eff = beta / (1 + xi_beta * stai).
    - First-stage choice kernel (perseveration) encourages repeating the last first-stage action.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), obtained reward (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 6 parameters, all used:
        alpha_r : second-stage learning rate for rewards.
        beta    : base inverse temperature [0,10].
        phi_b1  : magnitude of first-stage directed exploration bonus.
        phi_b2  : magnitude of second-stage directed exploration bonus.
        xi_beta : anxiety gain controlling softmax temperature (higher stai -> lower beta_eff).
        stick1  : first-stage perseveration bias to repeat last chosen ship.

    Returns
    - Negative log-likelihood over the observed first- and second-stage choices.
    """"""
    alpha_r, beta, phi_b1, phi_b2, xi_beta, stick1 = model_parameters

    # Clip parameters
    alpha_r = min(1.0, max(0.0, alpha_r))
    beta = min(10.0, max(1e-6, beta))
    phi_b1 = min(1.0, max(0.0, phi_b1))
    phi_b2 = min(1.0, max(0.0, phi_b2))
    xi_beta = min(1.0, max(0.0, xi_beta))
    stick1 = min(1.0, max(0.0, stick1))

    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and counts
    q2 = np.zeros((2, 2))
    counts_a1 = np.zeros(2)       # how often each first-stage action was chosen
    counts_s2 = np.zeros((2, 2))  # how often each (state, action2) was chosen

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    for t in range(n_trials):
        # Effective beta reduced by anxiety
        beta_eff = beta / (1.0 + xi_beta * st)
        beta_eff = min(10.0, max(1e-6, beta_eff))

        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Directed exploration bonus for stage 1 (less bonus with higher anxiety)
        bonus1 = np.zeros(2)
        for a in (0, 1):
            bonus1[a] = phi_b1 * (1.0 - st) / np.sqrt(counts_a1[a] + 1.0)

        q1 = q1_mb + bonus1

        # First-stage perseveration
        if last_a1 is not None:
            q1[last_a1] += stick1

        # First-stage policy
        exp1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage directed exploration bonus
        s = state[t]
        bonus2 = np.zeros(2)
        for a2_idx in (0, 1):
            bonus2[a2_idx] = phi_b2 * (1.0 - st) / np.sqrt(counts_s2[s, a2_idx] + 1.0)

        q2_aug = q2[s] + bonus2

        exp2 = np.exp(beta_eff * (q2_aug - np.max(q2_aug)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update Q2
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Update counts
        counts_a1[a1] += 1.0
        counts_s2[s, a2] += 1.0

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta', 'phi_b1', 'phi_b2', 'xi_beta', 'stick1']"
iter5_run0_participant1.json,cognitive_model1,530.9995749032132,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-tuned exploration, eligibility trace, and UCB-style bonus.

    This model blends a model-based (MB) first-stage planner with a model-free (MF) first-stage value learned
    from second-stage prediction errors via an eligibility-like trace. Anxiety (stai) softens first-stage choices
    and increases directed exploration at the second stage via an uncertainty bonus.

    Parameters (bounds):
    - alpha: [0,1] learning rate for second-stage Q-values and MF first-stage updates via trace.
    - beta: [0,10] inverse temperature for softmax choices (base).
    - w_hyb: [0,1] weight of MB plan at stage 1; (1 - w_hyb) is MF contribution.
    - nu_anx: [0,1] strength by which anxiety reduces stage-1 beta: beta1_eff = beta * (1 - nu_anx*stai).
    - tau_ucb: [0,1] scale of UCB directed exploration bonus at stage 2; bonus = (tau_ucb*stai)/sqrt(visit_count+1).
    - eta_tr: [0,1] eligibility trace strength to backpropagate second-stage PE to first-stage MF value.

    Args:
        action_1: array-like, shape (T,), first-stage actions (0/1).
        state: array-like, shape (T,), reached second-stage states (0/1).
        action_2: array-like, shape (T,), second-stage actions within reached state (0/1).
        reward: array-like, shape (T,), rewards (0/1 or real in [0,1]).
        stai: array-like with one element in [0,1], participant anxiety score.
        model_parameters: iterable with parameters in the order above.

    Returns:
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w_hyb, nu_anx, tau_ucb, eta_tr = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (known to the participant)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    Q2 = np.zeros((2, 2))      # second-stage Q(s,a)
    Q1_mf = np.zeros(2)        # first-stage model-free value

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Visit counts for UCB at second stage
    N2 = np.zeros((2, 2))  # counts per (state, action)

    eps = 1e-12

    for t in range(n_trials):
        # Effective beta at stage 1 is softened by anxiety
        beta1_eff = beta * (1.0 - nu_anx * stai)
        beta1_eff = max(beta1_eff, 1e-3)
        beta2_eff = max(beta, 1e-3)

        # Model-based plan at stage 1: expected max second-stage value under T
        max_q2 = np.max(Q2, axis=1)               # shape (2,)
        Q1_mb = T @ max_q2                        # shape (2,)

        # Hybrid first-stage value
        Q1 = w_hyb * Q1_mb + (1.0 - w_hyb) * Q1_mf

        # First-stage policy
        logits1 = beta1_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with UCB-style directed exploration modulated by anxiety
        s = state[t]
        bonus = np.zeros(2)
        bonus[:] = (tau_ucb * stai) / np.sqrt(N2[s] + 1.0)
        logits2 = beta2_eff * Q2[s] + bonus
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update values
        r = reward[t]

        # Second-stage update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2
        N2[s, a2] += 1.0

        # First-stage MF update via eligibility-like trace from second-stage PE
        # Also a direct TD update towards the chosen second-stage action value
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * (eta_tr * pe2 + (1.0 - eta_tr) * td1)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w_hyb', 'nu_anx', 'tau_ucb', 'eta_tr']"
iter5_run0_participant1.json,cognitive_model3,561.0420087176643,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk- and valence-asymmetric learning with anxiety-weighted confirmation bias.

    The model is purely model-based at stage 1 (planning via known transitions) but incorporates:
    - Valence-asymmetric learning at stage 2 (alpha_pos for positive PEs, alpha_neg for negative PEs).
    - Risk-sensitivity controlling PE gain nonlinearly.
    - Anxiety-weighted confirmation bias at stage 1: tendency to repeat the previous first-stage action if it
      was rewarded, stronger with higher stai.

    Parameters (bounds):
    - alpha_pos: [0,1] learning rate for positive second-stage PEs.
    - alpha_neg: [0,1] learning rate for negative second-stage PEs.
    - beta: [0,10] inverse temperature for both stages.
    - theta_risk: [0,1] exponent shaping PE gain: effective gain scales with |PE|^theta_eff.
    - chi_conf: [0,1] strength of confirmation bias on first-stage logits when prior choice was rewarded.
    - zeta_anx: [0,1] anxiety weighting of valence asymmetry: increases sensitivity to negative PEs with stai.

    Args:
        action_1: array-like, shape (T,), first-stage actions (0/1).
        state: array-like, shape (T,), reached second-stage states (0/1).
        action_2: array-like, shape (T,), second-stage actions (0/1).
        reward: array-like, shape (T,), rewards in [0,1].
        stai: array-like with one element in [0,1], participant anxiety score.
        model_parameters: iterable with parameters in the order above.

    Returns:
        Negative log-likelihood of observed choices.
    """"""
    alpha_pos, alpha_neg, beta, theta_risk, chi_conf, zeta_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous first-stage action and its reward to implement confirmation bias
    prev_a1 = -1
    prev_r1 = 0.0

    eps = 1e-12
    beta_eff = max(beta, 1e-3)

    for t in range(n_trials):
        # Stage-1 model-based values: expected max over second stage
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Confirmation bias on logits if previous first-stage action was rewarded
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1) and prev_r1 is not None:
            # Additive bias favoring repeating rewarded first-stage action; scaled by anxiety
            bias1[prev_a1] += chi_conf * stai * (2.0 * prev_r1 - 1.0)  # + if rewarded, - if not

        # First-stage policy
        logits1 = beta_eff * Q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (pure softmax on Q2)
        s = state[t]
        logits2 = beta_eff * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]
        pe2 = r - Q2[s, a2]

        # Anxiety-weighted valence asymmetry: increase negative-learning with stai, decrease positive slightly
        alpha_pos_eff = np.clip(alpha_pos * (1.0 - 0.5 * zeta_anx * stai), 0.0, 1.0)
        alpha_neg_eff = np.clip(alpha_neg * (1.0 + 0.5 * zeta_anx * stai), 0.0, 1.0)

        # Risk-sensitive PE gain
        theta_eff = theta_risk * (0.5 + 0.5 * stai)  # stronger nonlinearity with higher anxiety
        gain = (np.abs(pe2) + 1e-12) ** theta_eff

        if pe2 >= 0:
            Q2[s, a2] += alpha_pos_eff * gain * pe2
        else:
            Q2[s, a2] += alpha_neg_eff * gain * pe2

        # Update confirmation memory for next trial
        prev_a1 = a1
        prev_r1 = float(r)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_pos', 'alpha_neg', 'beta', 'theta_risk', 'chi_conf', 'zeta_anx']"
iter5_run0_participant12.json,cognitive_model1,476.0942180068855,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety- and surprise-modulated arbitration.

    This model mixes model-free (MF) and model-based (MB) control at stage 1.
    The arbitration weight for MB starts at a baseline and is modulated by:
      - Anxiety (stai): higher or lower MB weighting depending on xi_stai.
      - Transition surprise (rare transitions): increases/decreases MB weighting via kappa_trans,
        with the direction and strength scaled by anxiety.

    Stage 2 values are learned model-free; stage 1 MF values are learned via TD from stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial on the visited planet (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration and surprise sensitivity.
    model_parameters : list or array
        [alpha, beta, mb_base, xi_stai, kappa_trans]
        Bounds:
          alpha in [0,1]       : learning rate for MF updates
          beta in [0,10]       : inverse temperature
          mb_base in [0,1]     : baseline MB weight at stage 1
          xi_stai in [0,1]     : anxiety influence on MB weight (0 reduces MB with higher STAI, 1 increases it)
          kappa_trans in [0,1] : strength of surprise-driven MB reweighting

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, mb_base, xi_stai, kappa_trans = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure: A(0)->X(0) common; U(1)->Y(1) common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Q-values
    q1_mf = np.zeros(2)            # stage-1 MF
    q2_mf = np.zeros((2, 2))       # stage-2 MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map STAI and xi_stai to signed modulation in [-1,1]
    stai_signed = 2.0 * stai - 1.0           # [-1, 1]
    xi_signed = 2.0 * xi_stai - 1.0          # [-1, 1]

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute MB action values via one-step lookahead over the learned stage-2 MF values
        max_q2 = np.max(q2_mf, axis=1)           # best alien per planet
        q1_mb = transition_matrix @ max_q2       # expected value per spaceship

        # Anxiety-modulated baseline MB weight
        w_base = mb_base + 0.5 * stai_signed * xi_signed
        w_base = np.clip(w_base, 0.0, 1.0)

        # Surprise of transition (1 if rare, 0 if common)
        common_to = a1  # 0->state 0 is common; 1->state 1 is common
        surprise = 1.0 if s != common_to else 0.0

        # Surprise adjustment scaled by anxiety and kappa
        w = w_base + kappa_trans * surprise * stai_signed
        w = np.clip(w, 0.0, 1.0)

        # Hybrid stage-1 Q
        q1 = (1.0 - w) * q1_mf + w * q1_mb

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (pure MF)
        q2s = q2_mf[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c) / np.sum(np.exp(beta * q2c))
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Backup to stage-1 MF (TD toward obtained second-stage value)
        td1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'mb_base', 'xi_stai', 'kappa_trans']"
iter5_run0_participant12.json,cognitive_model2,524.1573428311237,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive MB with anxiety-modulated utility curvature and forgetting.

    This model chooses using a model-based (MB) planner at stage 1 that evaluates spaceships
    via the transition matrix and second-stage utilities. Utilities are risk-transformed:
      u(r) = r^gamma, where gamma depends on a baseline risk parameter and anxiety (stai).
    Second-stage MF values are learned on utilities and subject to forgetting toward 0.5.

    Anxiety increases or decreases risk aversion (via gamma) and also softens choice (lower beta)
    proportionally to stai_gain.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate utility curvature and softmax temperature.
    model_parameters : list or array
        [alpha, beta, risk_base, forget, stai_gain]
        Bounds:
          alpha in [0,1]    : learning rate for utility-based MF updates
          beta in [0,10]    : baseline inverse temperature
          risk_base in [0,1]: base risk sensitivity (maps to gamma in [0.5, 1.5])
          forget in [0,1]   : forgetting rate toward 0.5 for all second-stage Q-values
          stai_gain in [0,1]: strength of STAI effects on risk curvature and temperature

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, risk_base, forget, stai_gain = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Utility-transformed MF Q at stage 2
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map risk_base to curvature gamma in [0.5, 1.5]
    gamma_base = 0.5 + risk_base  # in [0.5, 1.5]
    # Anxiety shifts gamma: stai in [0,1], stai_gain in [0,1] -> delta in [-0.5, +0.5] scaled
    stai_signed = 2.0 * stai - 1.0  # [-1, 1]
    delta_gamma = 0.5 * stai_signed * (2.0 * stai_gain - 1.0)  # in [-0.5, 0.5]
    gamma = np.clip(gamma_base + delta_gamma, 0.25, 2.0)

    # Anxiety also reduces effective beta (more exploration with higher anxiety if stai_gain>0.5)
    beta_eff = beta / (1.0 + stai_gain * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply global forgetting toward 0.5 baseline (uninformative utility)
        q2 = (1.0 - forget) * q2 + forget * 0.5

        # Stage-2 policy on current planet (MF with utility Q)
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta_eff * q2c) / np.sum(np.exp(beta_eff * q2c))
        p_choice_2[t] = probs_2[a2]

        # Utility of observed reward and learning update
        util = r ** gamma
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MB evaluation using expected utilities
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta_eff * q1c) / np.sum(np.exp(beta_eff * q1c))
        p_choice_1[t] = probs_1[a1]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'risk_base', 'forget', 'stai_gain']"
iter5_run0_participant12.json,cognitive_model3,489.265349594801,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-bonus exploration with anxiety-modulated bonus and MF/MB integration.

    This model learns second-stage MF values and their uncertainties (running variance).
    An exploration bonus proportional to uncertainty is added to second-stage values when choosing,
    scaled by a parameter and modulated by anxiety. The model computes stage-1 MB values from the
    exploration-augmented second-stage values. It also maintains a model-free stage-1 value via
    an eligibility trace and mixes MB and MF at stage 1 with a STAI-dependent arbitration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Modulates exploration bonus and arbitration.
    model_parameters : list or array
        [alpha, beta, k_unc, stai_explore, elig]
        Bounds:
          alpha in [0,1]        : learning rate for MF and variance tracking
          beta in [0,10]        : inverse temperature
          k_unc in [0,1]        : scale of uncertainty exploration bonus
          stai_explore in [0,1] : strength and direction of STAI modulation on exploration/arbitration
          elig in [0,1]         : eligibility trace backing up stage-2 PE to stage-1 MF

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, k_unc, stai_explore, elig = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Stage-2 MF values and running variance (per state-action)
    q2 = np.zeros((2, 2))
    var2 = np.ones((2, 2)) * 0.25  # start with moderate uncertainty

    # Stage-1 MF values
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # STAI mappings
    stai_signed = 2.0 * stai - 1.0                # [-1, 1]
    explore_signed = 2.0 * stai_explore - 1.0     # [-1, 1]

    # STAI-modulated arbitration weight between MB and MF at stage 1
    w_mb = np.clip(0.5 + 0.5 * stai_signed * explore_signed, 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Uncertainty bonus for the current planet: UCB-like (std dev)
        std_s = np.sqrt(np.maximum(var2[s], 1e-12))
        bonus_scale = k_unc * (1.0 + stai_signed * explore_signed)  # increases with anxiety if explore_signed>0
        bonus_scale = np.clip(bonus_scale, 0.0, 2.0 * k_unc)
        q2_aug = q2[s] + bonus_scale * std_s

        # Stage-2 policy on augmented values
        q2c = q2_aug - np.max(q2_aug)
        probs_2 = np.exp(beta * q2c) / np.sum(np.exp(beta * q2c))
        p_choice_2[t] = probs_2[a2]

        # Learning: update mean and variance for chosen state-action using exponential estimates
        pe = r - q2[s, a2]
        q2[s, a2] += alpha * pe
        # Update variance with exponential moving average of squared residuals
        var2[s, a2] = (1.0 - alpha) * var2[s, a2] + alpha * (pe ** 2)

        # Stage-1 MB values from augmented second-stage values (use max over aliens with bonus per planet)
        # Construct augmented per-planet max values
        std_all = np.sqrt(np.maximum(var2, 1e-12))
        q2_aug_all = q2 + bonus_scale * std_all  # use same scale within trial
        max_q2_aug = np.max(q2_aug_all, axis=1)
        q1_mb = transition_matrix @ max_q2_aug

        # Stage-1 hybrid values
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-1 MF update via eligibility trace using the immediate stage-2 PE
        q1_mf[a1] += elig * alpha * pe

        # Optional additional TD correction toward observed second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'k_unc', 'stai_explore', 'elig']"
iter5_run0_participant14.json,cognitive_model1,521.1852875576304,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated planning weight and forgetting at stage 2.
    
    This model combines model-based (MB) and model-free (MF) values at stage 1.
    The MB component uses the known common transition structure (0.7/0.3) and
    the current best second-stage values. The MF component learns a direct value
    for the first-stage spaceships from second-stage outcomes. Stage 2 uses
    standard Q-learning, with a small forgetting applied to unvisited aliens.
    
    Anxiety (stai) modulates the relative MB weight: higher anxiety increases
    or decreases planning weight depending on parameters, allowing data to decide.
    
    Parameters (model_parameters):
    - alpha_r: learning rate for Q updates (used for both stages), in [0,1]
    - k_forget: forgetting rate for unvisited second-stage Q-values, in [0,1]
    - beta: inverse temperature for both stages' softmax, in [0,10]
    - omega0: baseline MB weight at stage 1, in [0,1]
    - anx_slope: gain mapping anxiety to MB weight, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (float)
    - stai: array-like with a single float in [0,1] (trait anxiety)
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_r, k_forget, beta, omega0, anx_slope = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known asymmetric transition structure
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])  # rows: [A,U], cols: [X,Y]

    # Action value storage
    q1_mf = np.zeros(2)        # MF values for stage 1: [A, U]
    q2 = np.zeros((2, 2))      # stage-2 values per planet [X,Y] x alien [0,1]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight
    omega = omega0 + anx_slope * (stai - 0.51)
    omega = np.clip(omega, 0.0, 1.0)

    for t in range(n_trials):
        # Compute MB stage-1 values from current stage-2 values
        max_q2 = np.max(q2, axis=1)  # best alien per planet: shape (2,)
        q1_mb = T_fixed @ max_q2     # expected value of spaceships

        # Hybrid values
        q1_hyb = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage 1 policy
        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes and learning
        r = reward[t]

        # Stage 2 Q-learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Forget unvisited second-stage actions/planets slightly
        q2[s, 1 - a2] *= (1.0 - k_forget)
        other_s = 1 - s
        q2[other_s, 0] *= (1.0 - k_forget)
        q2[other_s, 1] *= (1.0 - k_forget)

        # Stage 1 MF credit assignment from stage-2 value of chosen alien
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'k_forget', 'beta', 'omega0', 'anx_slope']"
iter5_run0_participant14.json,cognitive_model2,553.6075952270476,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned transitions with anxiety-sharpened expectations (confidence-weighted MB).
    
    This model learns the first-stage transition matrix from experience. Anxiety
    increases a sharpening of the learned transitions, effectively over-trusting
    common transitions and down-weighting rare transitions in planning. Stage 2
    uses MF Q-learning.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for stage-2 Q, in [0,1]
    - alpha_m: learning rate for transition matrix rows (row-wise EWMA), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - phi0: baseline sharpening weight (0=no sharpening, 1=strong), in [0,1]
    - g_anx: anxiety gain controlling increase in sharpening with stai, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (float)
    - stai: array-like with a single float in [0,1] (trait anxiety)
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_r, alpha_m, beta, phi0, g_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transitions to uninformative
    T = np.ones((2, 2)) * 0.5  # rows: [A,U], cols: [X,Y]
    q2 = np.zeros((2, 2))      # planet x alien values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated sharpening parameter
    phi = np.clip(phi0 + g_anx * (stai - 0.31), 0.0, 1.0)
    # Convert to exponent in [1,5] to sharpen distributions
    exp_pow = 1.0 + 4.0 * phi

    for t in range(n_trials):
        # Compute MB values using sharpened transitions
        max_q2 = np.max(q2, axis=1)  # best alien per planet
        # Sharpen each row of T by elementwise exponent then renormalize
        T_eff = np.empty_like(T)
        for a in range(2):
            row = T[a] ** exp_pow
            row = np.clip(row, 1e-12, None)
            T_eff[a] = row / (np.sum(row) + 1e-12)

        q1_mb = T_eff @ max_q2

        # Stage 1 softmax
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage 2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Transition learning (row-wise update toward observed planet)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += alpha_m * (target - T[a1])

        # Keep rows normalized and bounded
        T[a1] = np.clip(T[a1], 1e-9, 1.0)
        T[a1] /= (np.sum(T[a1]) + 1e-12)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'alpha_m', 'beta', 'phi0', 'g_anx']"
iter5_run0_participant14.json,cognitive_model3,581.0093311210964,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Regret-biased model-based choice with anxiety-amplified regret sensitivity.
    
    Stage 1 uses a model-based value computed from the known common transitions
    (0.7/0.3) and current second-stage values. In addition, a dynamic bias
    pushes choices toward the spaceship that commonly reaches the planet with
    currently better potential value (regret for not going there). Regret is the
    difference between the best attainable planet value and the value actually
    obtained on the current trial. This bias decays over trials (trace) and is
    amplified by anxiety.
    
    Stage 2 uses MF Q-learning.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - regret0: baseline sensitivity to regret for bias updates, in [0,1]
    - g_anx: anxiety gain that scales regret sensitivity, in [0,1]
    - trace: persistence of bias across trials (0=no carryover, 1=full), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (float)
    - stai: array-like with a single float in [0,1] (trait anxiety)
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_r, beta, regret0, g_anx, trace = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure for MB evaluation
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])  # rows: [A,U], cols: [X,Y]

    q2 = np.zeros((2, 2))      # planet x alien values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Directed bias toward A vs U; applied as [+b, -b] to logits
    b = 0.0
    gain = regret0 * np.clip(1.0 + g_anx * (stai - 0.31), 0.0, 2.0)

    for t in range(n_trials):
        # Base MB values from second-stage estimates
        max_q2 = np.max(q2, axis=1)  # [val_X, val_Y]
        q1_mb = T_fixed @ max_q2

        # Apply regret-driven directional bias to logits: prefer A if planet X better, U if Y better
        logits1 = beta * q1_mb + np.array([b, -b])
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage 2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update regret bias after observing outcome
        # Regret compares the best attainable planet value vs the obtained value
        best_state = 0 if max_q2[0] >= max_q2[1] else 1
        obtained_val = q2[s, a2]
        target_val = max_q2[best_state]
        regret = max(0.0, target_val - obtained_val)

        # Direction: +1 favors A (planet X), -1 favors U (planet Y)
        direction = 1.0 if best_state == 0 else -1.0

        # Decay previous bias and add new regret-driven increment
        b = (1.0 - trace) * b + trace * gain * regret * direction
        # Keep bias bounded to maintain numerical stability
        b = float(np.clip(b, -1.0, 1.0))

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta', 'regret0', 'g_anx', 'trace']"
iter5_run0_participant15.json,cognitive_model2,inf,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning MB control with prospect-style utility, eligibility trace, and anxiety modulation.
    
    Idea:
    - Learn the transition model T via a simple delta-rule (alpha_t).
    - Use utility-transformed rewards r^gamma instead of raw r; anxiety makes utility more concave (risk-averse).
    - TD learning at Stage-2 with learning rate alpha_r.
    - Propagate Stage-2 prediction errors to Stage-1 model-free values via an eligibility trace lambda.
    
    Parameters (total=5):
    - alpha_r: [0,1] Learning rate for reward values (q2).
    - alpha_t: [0,1] Learning rate for the transition model T.
    - beta: [0,10] Inverse temperature for both stages.
    - lam: [0,1] Eligibility trace weight for backpropagating Stage-2 PE to Stage-1.
    - risk: [0,1] Base risk parameter; smaller => more concave utility. Anxiety skews toward concavity.
    
    Anxiety use:
    - Utility curvature: gamma_eff = risk*(1 - stai) + 0.5*stai (interpolates toward 0.5 under high anxiety).
    - Eligibility strength: lambda_eff = lam * (1 - 0.5*stai) (higher anxiety reduces credit assignment to Stage-1).
    - Temperature scaling: beta_eff = beta * (1 - 0.3*stai) (moderate exploration increase with anxiety).
    
    Inputs:
    - action_1: array of ints in {0,1}.
    - state: array of ints in {0,1}.
    - action_2: array of ints in {0,1}.
    - reward: array of floats in [0,1].
    - stai: array-like with a single float in [0,1].
    - model_parameters: iterable (alpha_r, alpha_t, beta, lam, risk).
    
    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha_r, alpha_t, beta, lam, risk = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Learned transition model; initialize near-common but not deterministic
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Values
    q2 = 0.5 * np.ones((2, 2))
    q1_mf = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    beta_eff = beta * (1.0 - 0.3 * stai)
    beta_eff = max(1e-6, beta_eff)
    gamma_eff = risk * (1.0 - stai) + 0.5 * stai
    gamma_eff = float(np.clip(gamma_eff, 0.01, 1.0))
    lambda_eff = lam * (1.0 - 0.5 * stai)
    lambda_eff = float(np.clip(lambda_eff, 0.0, 1.0))

    for t in range(n_trials):
        s = state[t]

        # Model-based Q1 using learned transitions
        mb_q1 = T @ np.max(q2, axis=1)

        # Combine MB plan with MF cached q1 via simple average that is implicitly regulated by transition learning
        q1 = 0.5 * (mb_q1 + q1_mf)

        # Stage-1 policy
        z1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p2[t] = probs2[a2]

        # Observe utility-transformed reward
        r = reward[t]
        u = (r ** gamma_eff)

        # TD at Stage-2
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Eligibility-trace update to Stage-1 model-free values
        q1_mf[a1] += alpha_r * lambda_eff * pe2

        # Learn transitions: move predicted distribution for chosen a1 toward observed state s (one-hot)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1 - alpha_t) * T[a1, :] + alpha_t * target
        # Re-normalize to guard against drift
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll

","['alpha_r', 'alpha_t', 'beta', 'lam', 'risk']"
iter5_run0_participant15.json,cognitive_model3,572.1273410936572,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-gated novelty exploration with lapse and planning.
    
    Idea:
    - Hybrid policy with planning over fixed transitions and MF caching of Stage-1 values.
    - Add a novelty bonus (count-based) to both stages, but anxiety gates the bonus:
      more novelty at Stage-1 when anxious, less at Stage-2 (cautious exploitation when outcomes are close).
    - Include a lapse probability that grows with anxiety, mixing softmax with uniform.
    
    Parameters (total=5):
    - alpha: [0,1] Learning rate for q2 and MF q1 updates.
    - beta: [0,10] Base inverse temperature.
    - nu: [0,1] Base novelty bonus strength.
    - kappa_a: [0,1] Temperature anxiety gain: beta_eff = beta * exp(kappa_a*(0.5 - stai)).
    - lapse: [0,1] Baseline lapse that is amplified by anxiety.
    
    Anxiety use:
    - Temperature: beta_eff as above; higher anxiety (stai>0.5) lowers beta, increasing exploration.
    - Novelty bonuses: nu1 = nu * (1 + stai) at Stage-1; nu2 = nu * (1 - stai) at Stage-2.
    - Lapse: lapse_eff = lapse * (0.5 + 0.5*stai) applied at both stages.
    
    Inputs:
    - action_1: array of ints in {0,1}.
    - state: array of ints in {0,1}.
    - action_2: array of ints in {0,1}.
    - reward: array of floats in [0,1].
    - stai: array-like with a single float in [0,1].
    - model_parameters: iterable (alpha, beta, nu, kappa_a, lapse).
    
    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha, beta, nu, kappa_a, lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = 0.5 * np.ones((2, 2))
    q1_mf = np.zeros(2)

    # Visit counts for novelty
    c1 = np.ones(2)        # counts for Stage-1 actions (start at 1 to avoid div by zero)
    c2 = np.ones((2, 2))   # counts for state-action at Stage-2

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-modulated components
    beta_eff = beta * np.exp(kappa_a * (0.5 - stai))
    beta_eff = float(max(1e-6, beta_eff))
    nu1 = nu * (1.0 + stai)
    nu2 = nu * (1.0 - stai)
    lapse_eff = lapse * (0.5 + 0.5 * stai)
    lapse_eff = float(np.clip(lapse_eff, 0.0, 1.0))

    for t in range(n_trials):
        s = state[t]

        # Novelty bonuses
        bonus1 = nu1 * (1.0 / np.sqrt(c1))
        bonus2_s = nu2 * (1.0 / np.sqrt(c2[s]))

        # Hybrid Stage-1 values with MB plan plus MF cache, then add novelty bonus
        mb_q1 = T @ np.max(q2, axis=1)
        q1 = 0.5 * (mb_q1 + q1_mf) + bonus1

        # Stage-1 policy with lapse
        z1 = beta_eff * (q1 - np.max(q1))
        soft1 = np.exp(z1)
        soft1 /= np.sum(soft1)
        probs1 = (1.0 - lapse_eff) * soft1 + lapse_eff * 0.5
        a1 = action_1[t]
        p1[t] = probs1[a1]

        # Stage-2 policy with novelty and lapse
        q2_aug = q2[s] + bonus2_s
        z2 = beta_eff * (q2_aug - np.max(q2_aug))
        soft2 = np.exp(z2)
        soft2 /= np.sum(soft2)
        probs2 = (1.0 - lapse_eff) * soft2 + lapse_eff * 0.5
        a2 = action_2[t]
        p2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Simple MF update at Stage-1 toward current state's best q2
        target1 = q2[s, a2]
        q1_mf[a1] += alpha * (target1 - q1_mf[a1])

        # Update counts for novelty
        c1[a1] += 1.0
        c2[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll","['alpha', 'beta', 'nu', 'kappa_a', 'lapse']"
iter5_run0_participant16.json,cognitive_model1,505.32552701293866,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated arbitration with transition learning and eligibility traces.
    
    Mechanism
    - Learns second-stage action values (Q2) with a TD rule.
    - Learns first-stage action values (Q1_MF) via an eligibility trace from stage 2.
    - Learns the transition model T(a->s) and plans model-based values (Q1_MB = T @ max_a Q2).
    - Arbitration between model-based and model-free at stage 1 depends on transition uncertainty
      and anxiety: higher anxiety down-weights model-based control when transitions are uncertain.
    - Anxiety also reduces stage-2 choice determinism and reduces learning of transitions.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices per trial (0=spaceship A, 1=spaceship U).
    state : array-like of int in {0,1}
        Reached planet per trial (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage (alien) choices per trial.
    reward : array-like of float in [0,1]
        Received coins per trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values reduce planning and transition learning, increase MF trace.
    model_parameters : iterable of 5 floats
        - alpha_v in [0,1]: learning rate for Q values (both stages).
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2 (scaled down by anxiety).
        - w0_base in [0,1]: baseline model-based arbitration weight.
        - alphaT_base in [0,1]: baseline learning rate for transition probabilities.
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices.
    """"""
    alpha_v, beta1, beta2, w0_base, alphaT_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize values
    Q2 = np.zeros((2, 2))           # state x action at stage 2
    Q1_MF = np.zeros(2)             # model-free values at stage 1
    T_est = np.array([[0.7, 0.3],   # learned transition model initialized to task structure
                      [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Stage-2 determinism decreases with anxiety
    beta2_eff = max(0.0, beta2 * (1.0 - 0.35 * stai))
    # Transition learning reduced by anxiety
    alpha_T = alphaT_base * (1.0 - 0.5 * stai)
    # MF eligibility trace increases with anxiety (more credit to recent MF)
    lam = np.clip(0.3 + 0.6 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based value: expected max Q2 over learned transitions
        max_Q2 = np.max(Q2, axis=1)  # size 2 (per state)
        Q1_MB = T_est @ max_Q2       # size 2 (per action)

        # Arbitration weight per action depends on transition uncertainty (entropy) and anxiety
        # Compute row-wise normalized entropy for each action's transition distribution
        H = np.zeros(2)
        for a in range(2):
            p = np.clip(T_est[a], 1e-8, 1.0)
            p = p / np.sum(p)
            H[a] = -np.sum(p * np.log(p))
        H_max = np.log(2.0)
        H_norm = H / (H_max + 1e-12)

        # Base weight is w0_base; reduce weight under anxiety and under uncertainty
        w = np.zeros(2)
        for a in range(2):
            base = w0_base * (1.0 - 0.5 * stai)
            # When entropy is low (predictable), increase MB weight; anxiety dampens this benefit
            incr = (1.0 - H_norm[a]) * (1.0 - stai)
            w[a] = np.clip(base + incr, 0.0, 1.0)

        # Blend MF and MB at stage 1
        Q1 = w * Q1_MB + (1.0 - w) * Q1_MF

        # Stage-1 policy
        logits1 = beta1 * Q1
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (anxiety-softened)
        s = state[t]
        logits2 = beta2_eff * Q2[s]
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Transition learning for chosen first-stage action from observed state
        # Move T_est[a1] toward one-hot(state)
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1] = (1.0 - alpha_T) * T_est[a1] + alpha_T * oh
        # Renormalize row
        row_sum = np.sum(T_est[a1])
        if row_sum > 0:
            T_est[a1] = T_est[a1] / row_sum

        # Stage-2 TD update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_v * delta2

        # Stage-1 MF eligibility-trace update: push toward realized Q2 of reached state/action
        target1 = Q2[s, a2]
        delta1 = target1 - Q1_MF[a1]
        Q1_MF[a1] += alpha_v * lam * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_v', 'beta1', 'beta2', 'w0_base', 'alphaT_base']"
iter5_run0_participant16.json,cognitive_model2,494.91154413111735,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Valence-asymmetric MF-MB hybrid with anxiety-modulated lapse and perseveration.
    
    Mechanism
    - Second-stage values (Q2) are learned model-free with asymmetric learning rates for
      positive vs. negative outcomes. Asymmetry diminishes with higher anxiety.
    - First-stage values use a fixed model-based planner over the known transition matrix (0.7/0.3)
      combined with a model-free cached Q1 via a stai-dependent weight (more anxiety -> more MF).
    - Choice behavior includes an anxiety-inflated lapse rate at both stages and stage-2
      perseveration (stickiness) that increases with anxiety.
    - Shared beta_base is mapped to distinct stage-1/2 temperatures; anxiety reduces stage-2 determinism more.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices per trial.
    reward : array-like of float in [0,1]
        Rewards per trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values increase lapses and perseveration, reduce planning/temperature.
    model_parameters : iterable of 5 floats
        - alpha_base in [0,1]: base learning rate (scaled for valence).
        - beta_base in [0,10]: base inverse temperature for both stages.
        - lapse_base in [0,1]: base lapse probability (uniform random choice).
        - persev_base in [0,1]: base stickiness at stage 2.
        - z_base in [0,1]: valence asymmetry factor (positive > negative); damped by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_base, beta_base, lapse_base, persev_base, z_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition matrix (task structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q2 = np.zeros((2, 2))
    Q1_MF = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map parameters with anxiety
    # Valence asymmetry decreases with anxiety
    asym = z_base * (1.0 - stai)
    alpha_pos = np.clip(alpha_base * (1.0 + asym), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 - asym), 0.0, 1.0)

    # Stage temperatures: reduce more at stage 2
    beta1 = max(0.0, beta_base * (1.0 - 0.2 * stai))
    beta2 = max(0.0, beta_base * (1.0 - 0.5 * stai))

    # Lapse increases with anxiety
    lapse = np.clip(lapse_base * (0.5 + 0.5 * stai), 0.0, 0.49)

    # Stage-2 perseveration increases with anxiety
    kappa2 = persev_base * (1.0 + 0.5 * stai)

    prev_a2 = None

    for t in range(n_trials):
        # Model-based Q1 via known transitions and current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Anxiety-weighted MF vs MB mixture (no extra param)
        w_mb = np.clip(0.7 * (1.0 - stai), 0.0, 1.0)
        Q1 = w_mb * Q1_MB + (1.0 - w_mb) * Q1_MF

        # Stage-1 softmax with lapse
        logits1 = beta1 * Q1
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with perseveration and lapse
        s = state[t]
        logits2 = beta2 * Q2[s].copy()
        if prev_a2 is not None:
            logits2[prev_a2] += kappa2
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - lapse) * probs2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 asymmetric TD
        pe2 = r - Q2[s, a2]
        if pe2 >= 0:
            Q2[s, a2] += alpha_pos * pe2
        else:
            Q2[s, a2] += alpha_neg * pe2

        # Stage-1 MF bootstrapping toward realized second-stage value
        target1 = Q2[s, a2]
        Q1_MF[a1] += alpha_base * (target1 - Q1_MF[a1])

        prev_a2 = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_base', 'beta_base', 'lapse_base', 'persev_base', 'z_base']"
iter5_run0_participant16.json,cognitive_model3,544.0437750214172,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-like planning with anxiety-modulated predictive horizon and UCB exploration.
    
    Mechanism
    - Second-stage Q-values learned model-free; exploration bonus (UCB) encourages sampling
      uncertain options. Anxiety increases information-seeking (larger bonus).
    - First-stage planning uses a successor-like transition weighting: the immediate common
      transition is up-weighted by a lambda parameter; higher anxiety shortens predictive horizon
      (larger lambda toward the common state, less averaging over full transition).
    - Learns the transition model T_est from experience; transition learning is reduced by anxiety.
    - Stage-2 choice determinism is reduced by anxiety; stage-1 uses its own temperature.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices per trial.
    reward : array-like of float in [0,1]
        Rewards per trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values increase exploration bonus and shorten predictive horizon.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for Q2.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2 (reduced by anxiety).
        - sr_lambda_base in [0,1]: base weight toward the common transition (predictive myopia).
        - xi_base in [0,1]: base UCB exploration bonus scale at stage 2.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta1, beta2, sr_lambda_base, xi_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize
    Q2 = np.zeros((2, 2))
    counts2 = np.ones((2, 2))  # for uncertainty bonus
    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Stronger exploration with anxiety (information-seeking)
    xi = xi_base * (0.5 + 0.5 * stai)
    # Stage-2 temperature reduced by anxiety
    beta2_eff = max(0.0, beta2 * (1.0 - 0.3 * stai))
    # Predictive myopia increases with anxiety (more weight on common transition shortcut)
    sr_lambda = np.clip(sr_lambda_base * (0.5 + 0.5 * stai), 0.0, 1.0)
    # Transition learning reduced by anxiety; derive from alpha
    alpha_T = np.clip(0.5 * alpha * (1.0 - 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # UCB-like bonus for stage-2
        bonus = xi / np.sqrt(counts2 + 1.0)  # state-action specific
        Q2_aug = Q2 + bonus

        # Stage-1 successor-like planning:
        # For action 0 (A), common state is 0; for action 1 (U), common state is 1.
        max_Q2_aug = np.max(Q2_aug, axis=1)
        Q1 = np.zeros(2)
        for a in range(2):
            common_state = a  # mapping: A->X(0), U->Y(1)
            one_hot = np.array([1.0 if i == common_state else 0.0 for i in range(2)])
            p_eff = (1.0 - sr_lambda) * (T_est[a] / np.sum(T_est[a])) + sr_lambda * one_hot
            p_eff = p_eff / (np.sum(p_eff) + 1e-12)
            Q1[a] = np.dot(p_eff, max_Q2_aug)

        # Stage-1 policy
        logits1 = beta1 * Q1
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with exploration-augmented values
        s = state[t]
        logits2 = beta2_eff * Q2_aug[s]
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and learn
        r = reward[t]
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2
        counts2[s, a2] += 1.0

        # Update transition model from chosen action and observed state
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1] = (1.0 - alpha_T) * T_est[a1] + alpha_T * oh
        T_est[a1] = T_est[a1] / (np.sum(T_est[a1]) + 1e-12)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta1', 'beta2', 'sr_lambda_base', 'xi_base']"
iter5_run0_participant18.json,cognitive_model1,484.2753027570178,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with learned transitions and anxiety-modulated exploration.

    Description:
    - Stage-1 values are a weighted hybrid of model-based (using a learned transition matrix) and model-free.
    - The MB weight is shaped by anxiety via a tunable tilt parameter rho_anx: when rho_anx > 0.5,
      lower anxiety increases MB control and higher anxiety reduces it (and vice versa if < 0.5).
    - The transition model is learned with a dedicated transition learning rate tau_trans.
    - Stage-2 policy includes a directed exploration bonus based on outcome uncertainty q*(1-q),
      whose strength is dampened by anxiety: effective bonus = chi_dir * (1 - stai).
    - MF updates use standard TD(0) at stage-2, and eligibility propagation to stage-1
      with an anxiety-dependent trace parameter lambda = (1 - stai).

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - tau_trans: transition learning rate in [0,1]
    - chi_dir: directed exploration bonus weight in [0,1]
    - rho_anx: anxiety-tilt for MB arbitration in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, tau_trans, chi_dir, rho_anx)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, tau_trans, chi_dir, rho_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T_hat (rows: actions A,U; cols: states X,Y)
    T_hat = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Value functions
    Q1_mf = np.zeros(2)       # Stage-1 MF values for actions A/U
    Q2 = np.zeros((2, 2))     # Stage-2 MF values per state and action

    # Track outcome uncertainty at stage-2 via Bernoulli variance proxy q*(1-q)
    # (computed on-the-fly from Q2 which estimates mean reward)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration: MB weight w_mb
    # rho_anx in [0,1] sets how strongly anxiety tilts arbitration.
    # When rho_anx > 0.5: low anxiety => more MB; high anxiety => less MB.
    w_mb = np.clip(0.5 + (0.5 - st) * (2.0 * rho_anx - 1.0), 0.0, 1.0)

    # Directed exploration effective strength is reduced by anxiety
    dir_bonus_strength = chi_dir * (1.0 - st)

    # Eligibility trace for propagating stage-2 prediction error to stage-1
    lam = 1.0 - st  # higher anxiety -> shorter credit assignment window

    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])

        # Model-based value for stage-1 via learned transitions
        max_Q2 = np.max(Q2, axis=1)       # best value at each state
        Q1_mb = T_hat @ max_Q2

        # Hybrid stage-1 action values
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        pref1 = Q1_hyb
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus based on uncertainty
        # Uncertainty proxy u = q*(1-q) from current Q2 estimates (bounded in [0,0.25])
        u_vec = Q2[s] * (1.0 - Q2[s])
        pref2 = Q2[s] + dir_bonus_strength * u_vec
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # MF learning: stage-2 TD(0)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # MF learning: stage-1 via bootstrapping on reached state/action and eligibility
        delta1_boot = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * lam * delta1_boot
        # Additionally propagate a fraction of the immediate outcome PE to stage-1
        Q1_mf[a1] += alpha * lam * delta2

        # Learn transitions for chosen action toward observed state
        # One-hot target vector for observed state s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T_hat[a1] = (1.0 - tau_trans) * T_hat[a1] + tau_trans * target

        # Ensure each row remains a proper probability distribution
        row_sum = np.sum(T_hat[a1])
        if row_sum > 0:
            T_hat[a1] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'tau_trans', 'chi_dir', 'rho_anx']"
iter5_run0_participant18.json,cognitive_model2,359.1926832503142,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""MB/MF with anxiety-driven lapses and decaying perseveration traces.

    Description:
    - Stage-1 uses a fixed model-based lookahead combined with MF values (50-50 baseline),
      then adds a decaying perseveration trace that biases repeating past actions.
    - A lapse parameter injects uniform-random responding; the effective lapse increases with anxiety:
      epsilon_eff = epsilon_base * stai.
    - Perseveration is implemented as action-specific traces at both stages that decay each trial
      with rate 'decay'; its strength is reduced by anxiety: stick_eff = k_stick * (1 - stai).
    - Stage-2 is purely MF in value but includes the same perseveration bias in the policy.
    - MF updates use TD(0) at stage-2 and an eligibility trace to stage-1 with lambda = decay.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - epsilon_base: baseline lapse probability in [0,1]
    - k_stick: perseveration strength in [0,1]
    - decay: trace decay and eligibility parameter in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, epsilon_base, k_stick, decay)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, epsilon_base, k_stick, decay = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition matrix (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Perseveration traces (action-preference boosts), decaying each trial
    trace1 = np.zeros(2)
    trace2 = np.zeros((2, 2))

    # Effective parameters shaped by anxiety
    epsilon_eff = np.clip(epsilon_base * st, 0.0, 1.0)       # more anxiety -> more lapses
    stick_eff = k_stick * (1.0 - st)                         # more anxiety -> weaker perseveration
    lam = decay                                              # share decay as eligibility parameter

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])

        # Model-based plan: expected best value after each spaceship
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid base: equal MB/MF weighting (kept simple to isolate lapse and stick effects)
        Q1_base = 0.5 * Q1_mb + 0.5 * Q1_mf

        # Add perseveration traces to preferences
        pref1 = Q1_base + stick_eff * trace1

        # Softmax with lapse at stage-1
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        soft1 = exp1 / (np.sum(exp1) + eps)
        probs1 = (1.0 - epsilon_eff) * soft1 + epsilon_eff * 0.5  # uniform over 2 actions
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration trace
        pref2 = Q2[s] + stick_eff * trace2[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        soft2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - epsilon_eff) * soft2 + epsilon_eff * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF update via bootstrapping and eligibility on immediate outcome
        delta1_boot = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * lam * delta1_boot
        Q1_mf[a1] += alpha * lam * delta2

        # Update perseveration traces: decay and reinforce chosen actions
        trace1 *= decay
        trace2 *= decay
        trace1[a1] += 1.0
        trace2[s, a2] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'epsilon_base', 'k_stick', 'decay']"
iter5_run0_participant18.json,cognitive_model3,500.36323780407497,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive hybrid with anxiety-shaped utility and surprise-avoidance bias.

    Description:
    - Rewards undergo a concave utility transform u(r) = r^(gamma_eff) to capture risk attitudes;
      anxiety increases concavity by reducing gamma_eff: gamma_eff = gamma_u * (1 - stai).
    - Stage-1 uses a hybrid of MB and MF values with MB weight decreasing as anxiety increases:
      w_mb = omega0 * (1 - stai) + (1 - omega0) * stai.
    - A surprise-avoidance bias on stage-1 preferences penalizes repeating actions that produced
      surprising (rare) transitions on the previous trial. The bias strength scales with both
      nu_surp and stai and decays geometrically with factor (1 - nu_surp).
    - Stage-2 is MF with softmax.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - gamma_u: base utility curvature in [0,1] (smaller => more concave)
    - omega0: base MB weight anchor in [0,1]
    - nu_surp: surprise-bias strength/decay in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, gamma_u, omega0, nu_surp)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, gamma_u, omega0, nu_surp = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Surprise-bias accumulator over stage-1 actions, decays each trial
    bias1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    # Anxiety-shaped parameters
    gamma_eff = max(1e-6, gamma_u * (1.0 - st))  # higher anxiety => more concave utility
    w_mb = np.clip(omega0 * (1.0 - st) + (1.0 - omega0) * st, 0.0, 1.0)

    prev_a1 = None
    prev_s = None

    for t in range(n_trials):
        s = int(state[t])

        # MB plan from fixed transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid values
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Apply decaying surprise-avoidance bias
        bias1 *= (1.0 - nu_surp)
        pref1 = Q1_hyb + bias1

        # Stage-1 policy
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (MF)
        pref2 = Q2[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward
        r_raw = float(reward[t])
        r = r_raw ** gamma_eff

        # Learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        delta1_boot = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1_boot

        # Update surprise-avoidance bias based on the just-experienced transition (for next trial)
        # Compute the probability of the observed transition under the chosen action
        p_obs = T[a1, s]
        surprise = 1.0 - p_obs  # 0.3 for common, 0.7 for rare
        # Penalize repeating the chosen action proportionally to surprise and anxiety
        # This makes anxious participants more avoidance-biased after surprising outcomes.
        bias1[a1] -= st * nu_surp * surprise
        # Optionally, reward the alternative slightly to keep center-of-mass stable
        bias1[1 - a1] += 0.5 * st * nu_surp * surprise

        prev_a1 = a1
        prev_s = s

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'gamma_u', 'omega0', 'nu_surp']"
iter5_run0_participant21.json,cognitive_model2,505.3195336187638,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned-transition model with anxiety-modulated inverse temperature and eligibility-trace credit assignment.

    Overview:
    - Learns both the transition probabilities (from spaceships to planets) and stage-2 values.
    - Stage-1 values are model-based expectations using the learned transition matrix plus a model-free component updated via an eligibility trace.
    - Anxiety scales decision determinism (inverse temperature): higher anxiety can increase or decrease beta depending on kappa.
    
    Parameters (bounds):
    - model_parameters[0] = lr in [0,1]: learning rate for rewards (stage-2 Q) and for MF stage-1 via eligibility
    - model_parameters[1] = beta0 in [0,10]: baseline inverse temperature
    - model_parameters[2] = kappa_anx_temp in [0,1]: strength of anxiety modulation of beta; beta_eff = clip(beta0 * (1 + kappa_anx_temp * (2*stai - 1)), 1e-3, 10)
    - model_parameters[3] = tau_T in [0,1]: learning rate for transition probabilities
    - model_parameters[4] = lam in [0,1]: eligibility-trace weight for propagating reward to stage-1 MF
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0/1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    lr, beta0, kappa_anx_temp, tau_T, lam = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix (rows: a1 in {0,1}; cols: states {0,1}); start near-agnostic
    T = np.full((2, 2), 0.5)

    # Value tables
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated inverse temperature (same for both stages)
    beta_eff = beta0 * (1.0 + kappa_anx_temp * (2.0 * stai_val - 1.0))
    beta_eff = float(np.clip(beta_eff, 1e-3, 10.0))

    for t in range(n_trials):
        # Stage-1 model-based values from learned transitions
        max_q2 = np.max(q2, axis=1)      # shape (2,)
        q1_mb = T @ max_q2               # shape (2,)

        # Combine MB and MF by simple summation (scales are compatible since both track expected value)
        q1_net = q1_mb + q1_mf
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]

        # Stage-2 policy
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learn transitions via simple delta rule toward observed next state
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = (1.0 - tau_T) * T[a1] + tau_T * one_hot_s
        # Ensure row-normalization (numerical safety)
        T[a1] = T[a1] / np.sum(T[a1])

        # Stage-2 value update
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr * delta2

        # Eligibility-trace update to stage-1 MF values (propagate outcome back)
        q1_mf[a1] += lr * lam * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr', 'beta0', 'kappa_anx_temp', 'tau_T', 'lam']"
iter5_run0_participant21.json,cognitive_model3,515.8953323438329,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Loss-aversion utility with anxiety modulation and lapse-biased softmax; model-based planning at stage-1.

    Overview:
    - Stage-2 updates use a utility-transformed outcome: rewards are positive, nonrewards are treated as aversive with anxiety-modulated magnitude.
    - Stage-1 uses model-based planning from a fixed transition structure to the learned utility-based stage-2 values.
    - Action selection includes a lapse component that increases with anxiety.

    Parameters (bounds):
    - model_parameters[0] = lr in [0,1]: learning rate for stage-2 values
    - model_parameters[1] = beta in [0,10]: inverse temperature for softmax at both stages
    - model_parameters[2] = phi0 in [0,1]: baseline loss-aversion for nonreward (utility of 0 outcome is -phi_eff)
    - model_parameters[3] = phi_anx in [0,1]: anxiety modulation of loss-aversion; phi_eff = clip(phi0 + phi_anx*stai, 0,1)
    - model_parameters[4] = eps0 in [0,1]: baseline lapse rate; effective lapse = eps0 * stai

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0/1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    lr, beta, phi0, phi_anx, eps0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure for planning
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Stage-2 values (in utility space)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss aversion and lapse
    phi_eff = float(np.clip(phi0 + phi_anx * stai_val, 0.0, 1.0))
    eps_lapse = float(np.clip(eps0 * stai_val, 0.0, 1.0))

    for t in range(n_trials):
        # Stage-1 model-based values via forward planning over utility-based q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Lapse-biased softmax at stage 1
        q1c = q1_mb - np.max(q1_mb)
        soft_1 = np.exp(beta * q1c)
        soft_1 = soft_1 / np.sum(soft_1)
        probs_1 = (1.0 - eps_lapse) * soft_1 + eps_lapse * 0.5  # uniform over 2 actions
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]

        # Stage-2 policy with lapse
        q2c = q2[s] - np.max(q2[s])
        soft_2 = np.exp(beta * q2c)
        soft_2 = soft_2 / np.sum(soft_2)
        probs_2 = (1.0 - eps_lapse) * soft_2 + eps_lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Utility-transformed outcome
        r = reward[t]
        u = r if r > 0 else -phi_eff

        # Stage-2 TD update in utility space
        delta2 = u - q2[s, a2]
        q2[s, a2] += lr * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['lr', 'beta', 'phi0', 'phi_anx', 'eps0']"
iter5_run0_participant22.json,cognitive_model1,329.72538785572215,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety- and uncertainty-modulated arbitration plus MF decay.

    This model blends model-based (MB) and model-free (MF) values at stage 1.
    The arbitration weight w_t is shaped by:
    - baseline w0,
    - state of second-stage uncertainty (smaller alien value differences => higher MB weight),
    - anxiety (higher anxiety reduces MB reliance).
    Stage-2 uses MF Q-learning. MF values decay for unchosen actions to capture forgetting.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for MF updates at stage 2 and bootstrapped update to stage 1.
    - beta: [0,10] inverse temperature for both stages.
    - w0: [0,1] baseline MB weight at stage 1.
    - k_unc: [0,1] strength of uncertainty-driven increase in MB weight.
    - k_decay: [0,1] decay/forgetting of unupdated MF action values per trial.

    Inputs:
    - action_1: int array in {0,1}, chosen spaceship per trial.
    - state: int array in {0,1}, reached planet per trial.
    - action_2: int array in {0,1}, chosen alien per trial.
    - reward: float array, coins received per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha, beta, w0, k_unc, k_decay].

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, w0, k_unc, k_decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # action 0 (A): P(X)=0.7, P(Y)=0.3
                  [0.3, 0.7]]) # action 1 (U): P(X)=0.3, P(Y)=0.7

    # MF Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))  # state x action

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Estimate second-stage uncertainty: smaller abs diff => more uncertainty
        diff_x = abs(q2[0, 0] - q2[0, 1])
        diff_y = abs(q2[1, 0] - q2[1, 1])
        # Average uncertainty across planets
        unc = 1.0 - 0.5 * (np.tanh(diff_x) + np.tanh(diff_y))  # in (0,1), saturating

        # Anxiety reduces MB reliance; uncertainty increases MB reliance
        w = w0 * (1.0 - 0.6 * stai) + k_unc * unc
        w = min(1.0, max(0.0, w))

        # Model-based Q for first stage: expected max of second-stage values under T
        max_q2 = np.max(q2, axis=1)  # [X_best, Y_best]
        q1_mb = T @ max_q2

        # Hybrid Q for stage 1
        q1_hybrid = (1.0 - w) * q1_mf + w * q1_mb

        # Stage-1 policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p2[t] = probs2[a2]

        # Learning updates
        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF bootstrapped update toward current second-stage value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Decay unchosen MF values (both levels) to capture forgetting
        other_a1 = 1 - a1
        q1_mf[other_a1] *= (1.0 - k_decay)
        # For second stage, decay actions not chosen in each state
        other_a2 = 1 - a2
        q2[s, other_a2] *= (1.0 - k_decay)
        # Also decay the other state's actions slightly to reflect global forgetting
        other_s = 1 - s
        q2[other_s, :] *= (1.0 - 0.5 * k_decay)

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'k_unc', 'k_decay']"
iter5_run0_participant24.json,cognitive_model1,464.5141720314919,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-gated model-based control.
    
    This model mixes model-free (MF) and model-based (MB) action values at the first stage.
    The MB weight is gated by anxiety: higher anxiety increases/decreases MB control depending
    on parameters. Second-stage values are learned with MF updates. First-stage MF values
    are backed up from the realized second-stage action value, with an anxiety-scaled learning gain.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1) within the visited state.
    reward : array-like of float
        Received reward each trial (e.g., 0/1 or probabilistic).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [lr, beta, theta_mb0, chi_anx]
        - lr in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax.
        - theta_mb0 in [0,1]: baseline weight on MB control at stage 1.
        - chi_anx in [0,1]: how strongly anxiety shifts the MB weight.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    lr, beta, theta_mb0, chi_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)      # MF values at stage 1
    q2 = np.zeros((2, 2))    # MF values at stage 2 (state, action)

    # Storage for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-gated MB weight, clipped to [0,1]
    omega = theta_mb0 + chi_anx * s
    if omega < 0.0:
        omega = 0.0
    elif omega > 1.0:
        omega = 1.0

    for t in range(n_trials):
        st = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based action values at stage 1 from second-stage values
        max_q2 = np.max(q2, axis=1)     # best attainable value in each second-stage state
        q1_mb = T @ max_q2              # expectation under transition model

        # Hybrid action values and softmax policy for stage 1
        q1_hybrid = (1.0 - omega) * q1_mf + omega * q1_mb
        prefs1 = beta * (q1_hybrid - np.max(q1_hybrid))
        exp1 = np.exp(prefs1)
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        prefs2 = beta * (q2[st] - np.max(q2[st]))
        exp2 = np.exp(prefs2)
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 MF update
        pe2 = r - q2[st, a2]
        q2[st, a2] += lr * pe2

        # Stage-1 MF update with anxiety-scaled gain (higher anxiety -> larger gain)
        gain1 = lr * (0.5 + 0.5 * s)  # in [0.5*lr, lr]
        backup = q2[st, a2]
        pe1 = backup - q1_mf[a1]
        q1_mf[a1] += gain1 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr', 'beta', 'theta_mb0', 'chi_anx']"
iter5_run0_participant24.json,cognitive_model2,450.53230208180844,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive MF learning with anxiety-modulated risk and stickiness.
    
    Rewards are transformed by a power utility u(r) = r^(1 - xi) before learning.
    Anxiety increases or decreases risk aversion via xi(s) = xi0 + xi_stai * s.
    Policies at both stages include a choice stickiness bias that scales with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1).
    reward : array-like of float
        Reward per trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [alpha, beta, xi0, xi_stai, tau_stick]
        - alpha in [0,1]: MF learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - xi0 in [0,1]: baseline risk-aversion parameter in utility transform.
        - xi_stai in [0,1]: how much anxiety changes risk-aversion.
        - tau_stick in [0,1]: base stickiness magnitude; effective stickiness is tau_stick * stai.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, xi0, xi_stai, tau_stick = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Effective parameters
    xi = xi0 + xi_stai * s
    if xi < 0.0:
        xi = 0.0
    elif xi > 1.0:
        xi = 1.0
    kappa = tau_stick * s  # anxiety-scaled stickiness

    # Values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness memory
    last_a1 = None
    last_a2 = {0: None, 1: None}

    for t in range(n_trials):
        st = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stickiness biases
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        bias2 = np.zeros(2)
        if last_a2[st] is not None:
            bias2[last_a2[st]] += kappa

        # Policies
        prefs1 = q1 + bias1
        prefs1 = beta * (prefs1 - np.max(prefs1))
        exp1 = np.exp(prefs1)
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        prefs2 = q2[st] + bias2
        prefs2 = beta * (prefs2 - np.max(prefs2))
        exp2 = np.exp(prefs2)
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward (risk sensitivity)
        # Ensure r in [0,1]; power transform is well-defined.
        u = (r ** (1.0 - xi)) if r >= 0 else -((-r) ** (1.0 - xi))

        # MF updates
        pe2 = u - q2[st, a2]
        q2[st, a2] += alpha * pe2

        pe1 = q2[st, a2] - q1[a1]
        q1[a1] += alpha * pe1

        # Update stickiness memory
        last_a1 = a1
        last_a2[st] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'xi0', 'xi_stai', 'tau_stick']"
iter5_run0_participant24.json,cognitive_model3,462.97601274599174,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid with learned transitions and anxiety-modulated mixing.
    
    The agent learns both second-stage MF values and first-stage transition probabilities.
    First-stage decisions use a mixture of MF and model-based values computed from the learned
    transition model. Anxiety shifts the MF/MB mixing weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1).
    reward : array-like of float
        Reward per trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [lr, beta, eta_T, w_mix0, delta_anx]
        - lr in [0,1]: MF learning rate for rewards.
        - beta in [0,10]: inverse temperature for softmax.
        - eta_T in [0,1]: learning rate for transition probabilities.
        - w_mix0 in [0,1]: baseline weight on MB control at stage 1.
        - delta_anx in [0,1]: how strongly anxiety shifts the MB weight.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    lr, beta, eta_T, w_mix0, delta_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize learned transition model close to uniform
    T = np.full((2, 2), 0.5)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        st = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB values from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Anxiety-modulated mixing weight
        w = w_mix0 + delta_anx * s
        if w < 0.0:
            w = 0.0
        elif w > 1.0:
            w = 1.0

        q1_hybrid = (1.0 - w) * q1_mf + w * q1_mb

        # Policies
        prefs1 = beta * (q1_hybrid - np.max(q1_hybrid))
        exp1 = np.exp(prefs1)
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        prefs2 = beta * (q2[st] - np.max(q2[st]))
        exp2 = np.exp(prefs2)
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Outcome learning: stage-2 then stage-1 MF
        pe2 = r - q2[st, a2]
        q2[st, a2] += lr * pe2

        pe1 = q2[st, a2] - q1_mf[a1]
        q1_mf[a1] += lr * pe1

        # Transition learning for chosen first-stage action
        # Update T[a1] toward the observed state (one-hot), keep row normalized
        target = np.array([1.0, 0.0]) if st == 0 else np.array([0.0, 1.0])
        T[a1] = (1.0 - eta_T) * T[a1] + eta_T * target
        # Ensure numerical normalization
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['lr', 'beta', 'eta_T', 'w_mix0', 'delta_anx']"
iter5_run0_participant29.json,cognitive_model2,301.60786741655676,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free learner with anxiety-gated credit assignment and transition-surprise amplification.
    Rare transitions increase credit assignment to the first-stage action,
    and anxiety amplifies this effect. Adds an anxiety-modulated choice inertia.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for MF updates at both stages
        beta: [0,10] â inverse temperature
        kappa_transition: [0,1] â strength of transition-surprise gating for stage-1 learning
        choice_inertia: [0,1] â base bias to repeat the last action
        anx_credit: [0,1] â scales how anxiety increases surprise gating and inertia

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.

    Notes
    - Surprise is computed relative to the fixed common transitions: P(common)=0.7
      surprise = 1 for rare transitions and 0 for common transitions.
    - Effective stage-1 learning rate: alpha1_eff = alpha * (0.5 + 0.5*kappa_transition*surprise*(1+anx_credit*stai))
    - Choice inertia is increased by anxiety for both stages.
    """"""
    alpha, beta, kappa_transition, choice_inertia, anx_credit = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])
    eps = 1e-12

    # Fixed transition structure (used only to compute surprise)
    # A->X common, U->Y common
    common_prob = 0.7

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_a2 = [None, None]

    # Anxiety-modulated inertia used as additive bias on logits
    inertia_strength1 = choice_inertia * (1.0 + anx_credit * stai_val)
    inertia_strength2 = 0.5 * choice_inertia * (1.0 + anx_credit * stai_val)

    for t in range(n_trials):
        # Stage 1 policy with inertia
        logits1 = q1.copy()
        if last_a1 is not None:
            logits1[last_a1] += inertia_strength1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with inertia, in reached state
        s = state[t]
        logits2 = q2[s, :].copy()
        if last_a2[s] is not None:
            logits2[last_a2[s]] += inertia_strength2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Compute transition surprise (1 for rare, 0 for common)
        # Determine the ""common"" next state for chosen a1
        common_state = 0 if a1 == 0 else 1
        prob_common = common_prob
        is_common = 1 if s == common_state else 0
        surprise = 1.0 - is_common  # 1 if rare, 0 if common

        # Anxiety-gated amplification of credit assignment at stage 1
        gate = 0.5 + 0.5 * kappa_transition * surprise * (1.0 + anx_credit * stai_val)
        gate = np.clip(gate, 0.0, 1.0)

        # Stage 2 update (MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 update (MF toward realized second-stage value), gated by surprise/anxiety
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += (alpha * gate) * pe1

        # Update inertia memory
        last_a1 = a1
        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'kappa_transition', 'choice_inertia', 'anx_credit']"
iter5_run0_participant29.json,cognitive_model3,511.84125503920177,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Kalman-like uncertainty tracking with anxiety-amplified exploration bonus and adaptive MB weighting.
    The agent tracks both expected reward and uncertainty for each alien using a simple
    scalar Kalman filter. Uncertainty yields an exploration bonus at stage 2,
    which is amplified by anxiety, and propagated to stage 1 via model-based planning.
    Stage 1 also has a model-free component; MB/MF mix is adaptively set by anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â additional step-size blending with the Kalman update (helps track drift)
        beta: [0,10] â inverse temperature
        volatility: [0,1] â process noise scale for uncertainty dynamics
        unc_bonus: [0,1] â base weight of uncertainty bonus in valuation
        anx_unc_gain: [0,1] â how strongly anxiety amplifies uncertainty bonus

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.

    Notes
    - Measurement noise is fixed; process noise scales with 'volatility'.
    - MB/MF mix at stage 1 is set as: mb_weight = 0.5 + 0.5*(1 - stai), i.e., higher anxiety => less MB.
    """"""
    alpha, beta, volatility, unc_bonus, anx_unc_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])
    eps = 1e-12

    # Fixed transition matrix (task structure known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Reward mean and uncertainty (variance) for each alien (state, action)
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.1  # initial uncertainty

    # Stage-1 model-free values
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Kalman parameters
    # Process noise increases with volatility
    process_q = 0.01 + 0.19 * volatility  # in [0.01, 0.20]
    meas_r = 0.25  # fixed observation noise for Bernoulli-like rewards

    for t in range(n_trials):
        # Exploration bonus from uncertainty
        bonus_scale = unc_bonus * (1.0 + anx_unc_gain * stai_val)

        # Stage-2 policy in observed state
        s = state[t]
        bonus2 = bonus_scale * np.sqrt(np.maximum(q2_var[s, :], 1e-8))
        q2_aug = q2_mean[s, :] + bonus2

        logits2 = q2_aug - np.max(q2_aug)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB values via expected max value per state (including bonus)
        bonus2_all = bonus_scale * np.sqrt(np.maximum(q2_var, 1e-8))
        q2_aug_all = q2_mean + bonus2_all
        max_q2_per_state = np.max(q2_aug_all, axis=1)
        q1_mb = T @ max_q2_per_state

        # MB/MF mixture weight depends on anxiety (higher anxiety => lower MB)
        mb_weight = 0.5 + 0.5 * (1.0 - stai_val)
        q1_combined = mb_weight * q1_mb + (1.0 - mb_weight) * q1_mf

        # Stage-1 policy
        logits1 = q1_combined - np.max(q1_combined)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Observe reward and update Kalman filter for the chosen alien
        r = reward[t]
        # Time update (increase uncertainty due to process noise)
        q2_var[s, a2] = q2_var[s, a2] + process_q
        # Measurement update
        S = q2_var[s, a2] + meas_r
        K = q2_var[s, a2] / (S + eps)  # Kalman gain in [0,1)
        pred = q2_mean[s, a2]
        pe = r - pred
        # Blend a small additional step-size alpha to track non-stationarity robustly
        effective_update = K * pe
        q2_mean[s, a2] = pred + (alpha * pe + (1.0 - alpha) * effective_update)
        q2_var[s, a2] = (1.0 - K) * q2_var[s, a2]

        # Stage-1 MF update toward realized stage-2 (mean) value without bonus
        target1 = q2_mean[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha', 'beta', 'volatility', 'unc_bonus', 'anx_unc_gain']"
iter5_run0_participant31.json,cognitive_model1,439.0956596466573,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-modulated hybrid control with anxiety-sensitive learning-rate gating.

    Overview
    --------
    This model blends model-based (MB) and model-free (MF) control at the first stage.
    Second-stage values are learned via TD. A dynamically tracked uncertainty signal
    (from unsigned RPE) gates both:
      - the effective learning rate (higher uncertainty => faster learning),
      - the MB/MF arbitration (higher uncertainty and higher anxiety => shift toward MF).

    Anxiety use
    -----------
    The participant's anxiety (stai) increases the gain on the learning rate when
    uncertainty is high and reduces the MB weight under uncertainty.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Rewards received (e.g., 0/1).
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1].
    model_parameters : array-like of floats, length 5
        [alpha0, beta, tau_h0, upsilon, k_anx_eta]
        - alpha0 in [0,1]: base learning rate for value updates.
        - beta in [0,10]: inverse temperature for softmax.
        - tau_h0 in [0,1]: baseline MB weight at stage 1.
        - upsilon in [0,1]: update rate for uncertainty (unsigned RPE) tracker.
        - k_anx_eta in [0,1]: anxiety gain on learning rate under uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha0, beta, tau_h0, upsilon, k_anx_eta = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # rows: actions A/U; cols: states X/Y

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q1_mf = np.zeros(2)        # MF first-stage Q
    q2 = np.zeros((2, 2))      # second-stage Q per state

    # Uncertainty tracker (unsigned RPE EMA)
    uncert = 0.0

    for t in range(n_trials):
        # Model-based Q from transition model and current second-stage values
        max_q2 = np.max(q2, axis=1)                 # length 2 over states
        q1_mb = transition_matrix @ max_q2          # length 2 over first-stage actions

        # Anxiety-uncertainty arbitration:
        # - learning rate scales with uncertainty and anxiety
        # - MB weight is reduced under uncertainty more for anxious individuals
        # Learning-rate gain: base scaled by uncertainty (0.5+uncert) and anxiety gain around 0.5
        eta = alpha0 * (0.5 + uncert) * (1.0 + k_anx_eta * (stai - 0.5) * 2.0)
        # Clamp to [0,1]
        if eta < 0.0:
            eta = 0.0
        if eta > 1.0:
            eta = 1.0

        # Hybrid weight: baseline minus anxiety-weighted uncertainty
        tau_h = tau_h0 - (stai * uncert * 0.5)
        if tau_h < 0.0:
            tau_h = 0.0
        if tau_h > 1.0:
            tau_h = 1.0

        q1_h = tau_h * q1_mb + (1.0 - tau_h) * q1_mf

        # First-stage policy
        logits1 = beta * q1_h
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1_sum = np.sum(p1) + 1e-12
        p1 /= p1_sum
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2_sum = np.sum(p2) + 1e-12
        p2 /= p2_sum
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Outcomes
        r = reward[t]

        # TD updates
        # Stage-2 TD error
        delta2 = r - q2[s, a2]
        q2[s, a2] += eta * delta2

        # Stage-1 MF bootstrapping towards the chosen second-stage action value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += eta * delta1

        # Update uncertainty (unsigned RPE EMA from stage-2)
        uncert = (1.0 - upsilon) * uncert + upsilon * abs(delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'tau_h0', 'upsilon', 'k_anx_eta']"
iter5_run0_participant31.json,cognitive_model2,414.5568315614387,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with anxiety-driven aversion to surprising transitions and negative-outcome emphasis.

    Overview
    --------
    A purely model-free learner updates first- and second-stage Q values via SARSA(0).
    Two biases shape policy/value construction:
      1) Surprise bias at stage 1: common transitions receive a bonus; rare transitions a penalty.
         This bias is stronger when anxiety is high.
      2) Negative-outcome emphasis: zero reward is treated as a negative utility whose magnitude
         increases with anxiety (captures anxious sensitivity to non-reward).

    Anxiety use
    -----------
    - Reduces inverse temperature (more exploration).
    - Increases surprise bias magnitude.
    - Increases negative weighting of zero outcomes.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state.
    reward : array-like of float
        Obtained rewards (e.g., 0/1).
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1].
    model_parameters : array-like of floats, length 5
        [alpha, beta, kappa0, zeta_tr, psi_anx_exp]
        - alpha in [0,1]: TD learning rate.
        - beta in [0,10]: base inverse temperature.
        - kappa0 in [0,1]: baseline disutility magnitude for zero reward.
        - zeta_tr in [0,1]: magnitude of transition surprise bias in first-stage logits.
        - psi_anx_exp in [0,1]: anxiety effect on exploration and surprise weighting.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, kappa0, zeta_tr, psi_anx_exp = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective temperature: higher anxiety => more exploration
    beta_eff = beta * (1.0 - 0.7 * psi_anx_exp * stai)
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Surprise indicator: common=+1, rare=-1 based on fixed task structure
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        surprise_bias = 1.0 if is_common else -1.0

        # Anxiety-weighted transition bias applied to first-stage logits
        tr_bias_mag = zeta_tr * (1.0 + stai * psi_anx_exp)
        bias_vec = np.zeros(2)
        # Apply bias in favor of the action that would make the observed transition common
        # For action A (0), common -> X (0); for U (1), common -> Y (1).
        # We nudge the chosen action's logit according to whether current transition was common/rare.
        bias_vec[a1] = tr_bias_mag * surprise_bias

        # Stage-1 policy
        logits1 = beta_eff * q1 + bias_vec
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= (np.sum(p1) + 1e-12)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= (np.sum(p2) + 1e-12)
        p_choice_2[t] = p2[a2]

        # Subjective outcome: treat zero as negative utility with anxiety-dependent magnitude
        kappa = kappa0 * (0.5 + stai)
        subj_r = r if r > 0.0 else -kappa

        # TD updates
        delta2 = subj_r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'kappa0', 'zeta_tr', 'psi_anx_exp']"
iter5_run0_participant31.json,cognitive_model3,440.5487413711461,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-driven meta-control: dynamic temperature and arbitration with anxiety coupling.

    Overview
    --------
    This hybrid model adapts both the exploration level (beta) and the arbitration
    between model-based and model-free values (phi) based on an online volatility
    estimate of the reward environment. Volatility is tracked as the EMA of the
    change in the signed RPE across trials. Anxiety increases sensitivity to
    volatility, pushing the agent toward more exploration and more MF control.

    Mechanics
    ---------
    - Volatility v_t = (1 - k_vol) * v_{t-1} + k_vol * |delta2_t - delta2_{t-1}|
    - Effective beta_t = beta0 * (1 + (0.5 - stai) * rho_stai) / (1 + v_t)
    - Hybrid weight phi_t = clip(phi0 - rho_stai * stai * v_t, 0, 1)

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float
        Rewards obtained (e.g., 0/1).
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1].
    model_parameters : array-like of floats, length 5
        [alpha, beta0, phi0, k_vol, rho_stai]
        - alpha in [0,1]: TD learning rate for values.
        - beta0 in [0,10]: baseline inverse temperature.
        - phi0 in [0,1]: baseline MB weight at stage 1.
        - k_vol in [0,1]: update rate for volatility tracker.
        - rho_stai in [0,1]: strength of anxiety coupling to meta-control.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta0, phi0, k_vol, rho_stai = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    v = 0.0
    prev_delta2 = 0.0

    for t in range(n_trials):
        # Compute MB first-stage Q
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Volatility-driven meta-control and exploration
        beta_t = beta0 * (1.0 + (0.5 - stai) * rho_stai)
        beta_t = beta_t / (1.0 + v)
        if beta_t < 0.0:
            beta_t = 0.0
        if beta_t > 10.0:
            beta_t = 10.0

        phi_t = phi0 - rho_stai * stai * v
        if phi_t < 0.0:
            phi_t = 0.0
        if phi_t > 1.0:
            phi_t = 1.0

        q1_h = phi_t * q1_mb + (1.0 - phi_t) * q1_mf

        # First-stage policy
        logits1 = beta_t * q1_h
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta_t * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Outcome and TD updates
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update volatility from RPE change and carry over for next trial
        v = (1.0 - k_vol) * v + k_vol * abs(delta2 - prev_delta2)
        prev_delta2 = delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta0', 'phi0', 'k_vol', 'rho_stai']"
iter5_run0_participant32.json,cognitive_model1,382.30129015300037,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Hybrid MB/MF with learned transitions and anxiety-modulated planning weight.

    Core idea:
    - Second-stage (aliens) values are learned model-free.
    - First-stage values are a hybrid of model-based planning (using a learned transition model)
      and model-free bootstrapping from second-stage values.
    - The model-based weighting is modulated by anxiety: higher stai can shift reliance between MB and MF.
    - Transition probabilities from each spaceship to planets are learned via Rescorla-Wagner updating.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices per trial (0/1: two aliens within the reached state).
    reward : array-like of float
        Reward obtained each trial (e.g., 0/1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1].
    model_parameters : list or array-like of float
        [alpha2, beta, w_mb0, kappa_anx, tau_trans]
        Bounds:
        - alpha2: [0,1] learning rate for MF value updates (both stages).
        - beta:   [0,10] inverse temperature for both stages.
        - w_mb0:  [0,1] baseline weight on model-based value at stage 1.
        - kappa_anx: [0,1] strength with which stai shifts the MB weight (positive pushes toward MB).
        - tau_trans: [0,1] learning rate for updating transition probabilities.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha2, beta, w_mb0, kappa_anx, tau_trans = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Q-values
    q1_mf = np.zeros(2)        # model-free first-stage action values
    q2 = np.zeros((2, 2))      # second-stage values: state x action

    # Learned transition model: rows = action (A/U), cols = state (X/Y)
    # Initialize to neutral 0.5/0.5
    T = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Anxiety-modulated MB weight: logistic transform to keep in [0,1]
        # Effective logit is shifted by kappa_anx*(stai-0.5) around baseline w_mb0.
        eps_clip = 1e-8
        w0 = np.clip(w_mb0, eps_clip, 1 - eps_clip)
        logit_w0 = np.log(w0 / (1 - w0))
        w_mb = 1.0 / (1.0 + np.exp(-(logit_w0 + (kappa_anx * (stai - 0.5) * 4.0))))
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # Model-based action values at stage 1: expectation over learned transitions of max Q2 at each state
        max_q2_per_state = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2_per_state           # shape (2,)

        # Hybrid value for stage 1
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Policy stage 1
        logits1 = beta * (q1_hybrid - np.max(q1_hybrid))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (within reached state)
        s = state[t]
        q2_s = q2[s].copy()
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage values (standard MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Update first-stage MF value toward the obtained second-stage value (bootstrapping)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Update transitions for the chosen action: RW learning toward observed state
        # T[a1, :] moves toward one-hot(s)
        for st in range(2):
            target = 1.0 if st == s else 0.0
            T[a1, st] += tau_trans * (target - T[a1, st])

        # Re-normalize rows to guard against numerical drift
        T[a1, :] = np.clip(T[a1, :], 1e-6, 1.0)
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha2', 'beta', 'w_mb0', 'kappa_anx', 'tau_trans']"
iter5_run0_participant32.json,cognitive_model2,487.9806982016096,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Risk-sensitive second stage with learned variance and anxiety-modulated uncertainty bonus.

    Core idea:
    - Learn second-stage action values and their outcome variance.
    - Choice at stage 2 uses a mean-variance utility: U = mean - xi_risk * variance.
    - Add an uncertainty bonus to prefer options with higher estimated variance; the bonus is
      amplified with higher anxiety (exploration under anxiety).
    - First-stage values are computed model-based from fixed transitions (A->X common, U->Y common),
      using the max utility in each state.
    - Standard MF backup from stage 2 to stage 1.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Reward (e.g., 0/1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1].
    model_parameters : list or array-like of float
        [alpha, beta, xi_risk, zeta_unc, phi_anx]
        Bounds:
        - alpha: [0,1] learning rate for updating means and variance (variance via squared PE).
        - beta: [0,10] inverse temperature for both stages (applied to utilities).
        - xi_risk: [0,1] weight on variance penalty in utility (0 risk-neutral; 1 risk-averse).
        - zeta_unc: [0,1] base weight of uncertainty (variance) bonus.
        - phi_anx: [0,1] anxiety modulation strength on the uncertainty bonus.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, xi_risk, zeta_unc, phi_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Values and variance estimates for second stage
    q2_mean = np.zeros((2, 2))
    q2_var = np.zeros((2, 2))   # running estimate of outcome variance

    # First-stage MF values for bootstrapping
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Fixed transition structure (common: 0.7, rare: 0.3)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    for t in range(n_trials):
        # Anxiety-modulated uncertainty bonus weight (increases with anxiety)
        unc_weight = zeta_unc * (1.0 + phi_anx * stai)
        # Construct risk-sensitive utilities at second stage for each state
        utilities_state = np.zeros((2, 2))
        for s in range(2):
            # mean-variance utility
            utilities_state[s, :] = q2_mean[s, :] - xi_risk * np.clip(q2_var[s, :], 0.0, 1.0)
            # add exploration bonus based on estimated variance
            utilities_state[s, :] += unc_weight * np.sqrt(np.clip(q2_var[s, :], 0.0, 1.0))

        # Stage 1 model-based values from fixed transitions and utilities
        maxU_per_state = np.max(utilities_state, axis=1)
        q1_mb = T_fixed @ maxU_per_state

        # Hybrid choice at stage 1: we use pure MB for policy but include MF backup in learning.
        q1_policy = q1_mb

        # Stage 1 policy
        logits1 = beta * (q1_policy - np.max(q1_policy))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in reached state using utilities
        s = state[t]
        u2 = utilities_state[s, :].copy()
        logits2 = beta * (u2 - np.max(u2))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates from realized outcome
        r = reward[t]

        # Update second-stage mean and variance (EWMA of squared PE)
        pe2 = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha * pe2
        # Variance update: v <- (1-alpha)*v + alpha*(pe^2)
        q2_var[s, a2] = (1.0 - alpha) * q2_var[s, a2] + alpha * (pe2 ** 2)

        # MF backup to stage 1: move toward the updated mean value (not utility to avoid double-counting bonuses)
        target1 = q2_mean[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha', 'beta', 'xi_risk', 'zeta_unc', 'phi_anx']"
iter5_run0_participant35.json,cognitive_model1,354.5458009863861,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated perseveration and lapse with MF-MB blending.

    This model blends model-free (MF) and model-based (MB) action values at stage 1,
    learns second-stage Q-values with a TD rule, and propagates MF value to stage 1 via
    bootstrapping. Anxiety increases choice perseveration and lapse, and reduces MB reliance.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], obtained reward
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: list/tuple of 5 parameters (bounds in brackets):
        alpha2 [0,1]: learning rate for second-stage MF Q-values
        beta  [0,10]: inverse temperature for softmax at both stages
        w0    [0,1]: baseline weight of MB vs MF at stage 1
        kappa_stay [0,1]: baseline perseveration strength (bias to repeat previous action)
        eps0  [0,1]: baseline lapse probability (uniform random choice component)

    Anxiety usage
    - MB reliance decreases with anxiety: w_mb = clip(w0 * (1 - 0.4*stai))
    - Perseveration increases with anxiety: kappa = kappa_stay * (0.5 + stai)
    - Lapse increases with anxiety: epsilon = clip(0.5, eps0 * (0.5 + 0.8*stai))

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha2, beta, w0, kappa_stay, eps0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: rows = actions [A,U], cols = states [X,Y]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q2 = np.zeros((2, 2))      # stage-2 MF Q(s, a2)
    q1_mf = np.zeros(2)        # stage-1 MF Q(a1)

    # Choice probability storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration traces (for previous actions)
    prev_a1 = None
    prev_a2_by_state = [None, None]

    # Anxiety-modulated control parameters
    w_mb = min(1.0, max(0.0, w0 * (1.0 - 0.4 * stai)))
    kappa = kappa_stay * (0.5 + stai)
    epsilon = min(0.5, max(0.0, eps0 * (0.5 + 0.8 * stai)))

    for t in range(n_trials):
        # Model-based stage-1 values by planning to second-stage max Q
        max_q2 = np.max(q2, axis=1)       # [X, Y]
        q1_mb = T @ max_q2                # [A, U]

        # Combine MB and MF, add perseveration bias on previous action
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        if prev_a1 is not None:
            bias = np.array([0.0, 0.0])
            bias[prev_a1] += kappa
            q1 = q1 + bias

        # Softmax with lapse: pi = (1-eps)*softmax(beta*q) + eps*uniform
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (per-state softmax with perseveration on last a2 in that state)
        s = int(state[t])
        q2_state = q2[s].copy()
        if prev_a2_by_state[s] is not None:
            q2_state[prev_a2_by_state[s]] += kappa
        q2_centered = q2_state - np.max(q2_state)
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # Stage-2 TD learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage-1 MF bootstrapping from experienced second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use same learning rate as stage-2 for simplicity and parsimony
        q1_mf[a1] += alpha2 * pe1

        # Update perseveration traces
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'w0', 'kappa_stay', 'eps0']"
iter5_run0_participant35.json,cognitive_model2,529.3775202374998,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-weighted uncertainty bonus via Beta-Bernoulli with leaky counts.

    This model maintains Beta posteriors over each alien's payout probability and uses
    the posterior mean plus an exploration bonus proportional to posterior uncertainty.
    The bonus magnitude scales up with anxiety. Stage-1 values are model-based projections
    of second-stage action values, blended with a simple MF stage-1 value learned via a
    leaky bootstrap. No free learning-rate for second-stage is needed because we update
    exact Beta counts.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1) within the state
    - reward: array-like of floats in [0,1], reward obtained
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: list/tuple of 4 parameters (bounds in brackets):
        beta        [0,10]: inverse temperature for softmax policies
        kappa_unc   [0,1]: base coefficient for the uncertainty (std) exploration bonus
        w_mb        [0,1]: weight on model-based values at stage 1 (vs MF)
        gamma_leak  [0,1]: leak rate toward priors for Beta counts and MF stage-1 value

    Anxiety usage
    - Uncertainty-driven exploration increases with anxiety:
        bonus_scale = kappa_unc * (0.5 + stai)
      so higher anxiety yields larger directed exploration toward uncertain aliens.

    Returns
    - Negative log-likelihood of the observed choices.
    """"""
    beta, kappa_unc, w_mb, gamma_leak = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition matrix (fixed, known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Beta-Bernoulli parameters for each alien: alpha_succ, beta_fail
    # Initialize to (1,1) uniform prior
    succ = np.ones((2, 2), dtype=float)
    fail = np.ones((2, 2), dtype=float)

    # Stage-1 MF value (leaky bootstrap)
    q1_mf = np.zeros(2, dtype=float)

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    bonus_scale = kappa_unc * (0.5 + stai)

    for t in range(n_trials):
        # Posterior means and stds for each alien
        mean = succ / (succ + fail)
        # Posterior variance for Beta(a,b): ab / [(a+b)^2 (a+b+1)]
        a = succ
        b = fail
        var = (a * b) / ((a + b) ** 2 * (a + b + 1.0))
        std = np.sqrt(var)

        # Stage-2 target values with directed exploration bonus
        q2 = mean + bonus_scale * std

        # Model-based stage-1 value: expected best alien per planet under T
        max_q2 = np.max(q2, axis=1)   # best alien value on X and Y
        q1_mb = T @ max_q2

        # Stage-1 decision values: blend MB and MF
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = int(state[t])
        q2_state = q2[s]
        q2_centered = q2_state - np.max(q2_state)
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward and update Beta counts with leak toward prior (1,1)
        r = float(reward[t])
        # Leak toward prior: counts <- (1 - gamma)*counts + gamma*1
        succ = (1.0 - gamma_leak) * succ + gamma_leak * 1.0
        fail = (1.0 - gamma_leak) * fail + gamma_leak * 1.0
        # Add fractional Bernoulli evidence from outcome
        succ[s, a2] += r
        fail[s, a2] += (1.0 - r)

        # Update stage-1 MF value by leaky bootstrap from the realized second-stage chosen value (posterior mean)
        target1 = mean[s, a2]
        q1_mf = (1.0 - gamma_leak) * q1_mf
        q1_mf[a1] += gamma_leak * target1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['beta', 'kappa_unc', 'w_mb', 'gamma_leak']"
iter5_run0_participant39.json,cognitive_model1,491.54412066762313,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid RL with learned transitions and anxiety-modulated information bonus.

    The agent learns:
      - Second-stage values model-free from reward.
      - First-stage transition probabilities for each spaceship.
      - First-stage action values as a hybrid of model-based (via learned transitions)
        and model-free values. An information bonus encourages or discourages choosing
        uncertain transitions, with anxiety modulating the direction/strength.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state: np.array (n_trials,), observed second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; alien choice)
    - reward: np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai: np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        lr_r:   [0,1] learning rate for second-stage Q-values
        beta:   [0,10] inverse temperature for both stages
        lr_T0:  [0,1] baseline learning rate for transition probabilities
        info0:  [0,1] baseline weight on information bonus at stage 1
        mix0:   [0,1] baseline weight of model-based over model-free at stage 1

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """"""
    lr_r, beta, lr_T0, info0, mix0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize values
    q1_mf = np.zeros(2)        # model-free values for spaceships
    q2 = np.zeros((2, 2))      # second-stage Q-values: state x action
    # Learned transition probabilities p(s'|a1). Start uninformative 0.5.
    p_trans = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Effective transition learning rate: higher anxiety -> faster updating (hypervigilance)
    lr_T = np.clip(lr_T0 * (0.5 + 0.5 * stai0), 0.0, 1.0)
    # Effective info weight: high anxiety turns bonus into uncertainty aversion
    # (sign flip around zero when stai>0.5).
    info_w = info0 * (1.0 - 2.0 * stai0)
    # Effective MB mixing weight: high anxiety reduces planning reliance
    w_mb = np.clip(mix0 * (1.0 - 0.5 * stai0), 0.0, 1.0)

    eps = 1e-10
    for t in range(n_trials):
        # Model-based Q for stage 1 using learned transitions and current q2
        max_q2 = np.max(q2, axis=1)  # size 2 for states X/Y
        q1_mb = np.array([
            p_trans[0, 0] * max_q2[0] + p_trans[0, 1] * max_q2[1],
            p_trans[1, 0] * max_q2[0] + p_trans[1, 1] * max_q2[1]
        ])

        # Uncertainty per action (Bernoulli variance)
        uncert = p_trans[:, 0] * (1.0 - p_trans[:, 0])  # same as for state 1
        q1_aug = (w_mb * q1_mb + (1.0 - w_mb) * q1_mf) + info_w * uncert

        # Stage 1 policy
        q1s = q1_aug - np.max(q1_aug)
        probs1 = np.exp(beta * q1s)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s2 = state[t]
        q2s = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta * q2s)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn transitions for chosen a1 toward observed state s2
        # For binary state, we update p(a1->s2) toward 1 and the other toward 0
        p_old = p_trans[a1, s2]
        p_trans[a1, s2] = p_old + lr_T * (1.0 - p_old)
        other = 1 - s2
        p_old_other = p_trans[a1, other]
        p_trans[a1, other] = p_old_other + lr_T * (0.0 - p_old_other)

        # Stage 2 value update
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += lr_r * pe2

        # Stage 1 model-free update toward realized second-stage action value
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += lr_r * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr_r', 'beta', 'lr_T0', 'info0', 'mix0']"
iter5_run0_participant39.json,cognitive_model2,442.2376560251216,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric win/loss learning with anxiety-modulated transition-based perseveration.

    The agent learns second-stage values with separate learning rates for wins vs losses.
    First-stage choices include a perseveration bias that depends on whether the last
    transition was common or rare. Anxiety strengthens rare-transition-driven biases.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state: np.array (n_trials,), observed second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward: np.array (n_trials,), outcomes (0/1)
    - stai: np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        lr_win0: [0,1] baseline learning rate when reward=1
        lr_loss0: [0,1] baseline learning rate when reward=0
        beta: [0,10] inverse temperature for both stages
        stick0: [0,1] baseline perseveration after common transitions
        rare0: [0,1] baseline perseveration after rare transitions

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """"""
    lr_win0, lr_loss0, beta, stick0, rare0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition structure for common/rare check (A->X, U->Y commonly)
    common_prob = 0.7

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_common = None

    # Anxiety modulation:
    # - High anxiety lowers win learning and increases loss learning (negativity bias).
    lr_win = np.clip(lr_win0 * (1.0 - 0.5 * stai0), 0.0, 1.0)
    lr_loss = np.clip(lr_loss0 * (0.5 + 0.5 * stai0), 0.0, 1.0)
    # - Perseveration: high anxiety reduces common-stick but increases rare-driven bias
    stick_common = stick0 * (1.0 - stai0)
    stick_rare = rare0 * (0.5 + 0.5 * stai0)

    eps = 1e-10
    for t in range(n_trials):
        # First-stage softmax with transition-dependent perseveration
        bias = np.zeros(2)
        if prev_a1 is not None:
            if prev_common:
                bias[prev_a1] += stick_common
            else:
                # After rare transitions, anxiety-weighted bias to repeat previous a1
                bias[prev_a1] += stick_rare
        q1_eff = q1_mf + bias
        q1s = q1_eff - np.max(q1_eff)
        probs1 = np.exp(beta * q1s)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second stage policy
        s2 = state[t]
        q2s = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta * q2s)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage Q with asymmetric learning rates
        alpha2 = lr_win if r > 0.0 else lr_loss
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha2 * pe2

        # Stage-1 MF update toward the realized second-stage action value
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        # Use an averaged alpha from win/loss to reflect affective bias at stage 1
        alpha1 = 0.5 * (lr_win + lr_loss)
        q1_mf[a1] += alpha1 * pe1

        # Determine if the current transition was common vs rare for bias next trial
        if a1 == 0:
            # spaceship A commonly goes to X (0)
            was_common = (s2 == 0)
        else:
            # spaceship U commonly goes to Y (1)
            was_common = (s2 == 1)

        prev_a1 = a1
        prev_common = was_common

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr_win0', 'lr_loss0', 'beta', 'stick0', 'rare0']"
iter5_run0_participant39.json,cognitive_model3,534.426781303293,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pure model-based planning with dual temperatures, reward sensitivity, and anxiety-driven decay.

    The agent:
      - Learns second-stage values model-free with reward sensitivity (utility scaling).
      - Plans at stage 1 using the fixed transition model (common=0.7, rare=0.3).
      - Uses separate inverse temperatures for stages 1 and 2.
      - Applies anxiety-modulated decay (forgetting) of second-stage values toward 0.5.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state: np.array (n_trials,), observed second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward: np.array (n_trials,), outcomes (0/1)
    - stai: np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha: [0,1] learning rate for second-stage Q-values
        beta1_base: [0,10] baseline inverse temperature at stage 1
        beta2_base: [0,10] baseline inverse temperature at stage 2
        decay0: [0,1] baseline decay rate toward 0.5 at second stage
        rho0: [0,1] baseline reward sensitivity (scales reward before learning)

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta1_base, beta2_base, decay0, rho0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Transition matrix known
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation:
    # - Higher anxiety -> more noise at stage 1, sharper focus at stage 2 (vigilant local policy)
    beta1 = max(0.0, beta1_base * (1.0 - 0.7 * stai0))
    beta2 = max(0.0, beta2_base * (0.6 + 0.4 * stai0))
    # - Higher anxiety -> stronger decay (forgetting/volatility tracking)
    decay = np.clip(decay0 * (0.5 + 0.5 * stai0), 0.0, 1.0)
    # - Reward sensitivity: high anxiety blunts utility impact
    rho = rho0 * (1.0 - 0.5 * stai0)

    eps = 1e-10
    for t in range(n_trials):
        # Model-based Q for stage 1 from current second-stage max values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage 1 policy
        q1s = q1_mb - np.max(q1_mb)
        probs1 = np.exp(beta1 * q1s)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s2 = state[t]
        q2s = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta2 * q2s)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Apply decay toward 0.5 baseline before updating (anxiety -> more decay)
        q2 = (1.0 - decay) * q2 + decay * 0.5

        # Reward-sensitive update at second stage
        r = reward[t]
        util = rho * r + (1.0 - rho) * 0.5  # interpolates raw reward with neutral 0.5
        pe2 = util - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # No explicit stage-1 MF term; first-stage learning is purely model-based

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta1_base', 'beta2_base', 'decay0', 'rho0']"
iter5_run0_participant4.json,cognitive_model1,445.0509411862366,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-gated arbitration and action bias.

    Overview
    - Stage-2 values (Q2) learned via RescorlaâWagner (alpha).
    - Stage-1 choice values are a weighted mixture of model-based (MB) and model-free (MF) values.
    - The arbitration weight w_t shifts toward MB when transitions are expected and away when surprise occurs;
      anxiety amplifies the impact of transition surprise on reducing MB control.
    - A static first-stage action bias term depends on anxiety (approach/avoid tendency).

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:     [0,1]   Learning rate for Q updates (both stages).
    - beta:      [0,10]  Inverse temperature for softmax at both stages.
    - w0:        [0,1]   Initial model-based weight at stage 1.
    - alpha_w:   [0,1]   Update rate for arbitration weight based on transition surprise.
    - kappa_bias:[0,1]   Magnitude of first-stage static bias; sign set by (stai - 0.5).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, w0, alpha_w, kappa_bias].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, w0, alpha_w, kappa_bias = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Task transition structure (fixed, known)
    # Rows: actions (A=0, U=1). Cols: states (X=0, Y=1).
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    q1_mf = np.zeros(2) + 0.5         # stage-1 model-free action values
    q2 = np.zeros((2, 2)) + 0.5       # stage-2 values per state-action

    # Arbitration weight
    w = float(w0)

    # Anxiety-dependent static bias at stage 1 (favor A vs U depending on sign)
    # Bias vector applied as additive logits shift.
    bias_mag = kappa_bias * (stai - 0.5)  # in [-0.5*kappa_bias, 0.5*kappa_bias]
    bias1 = np.array([+bias_mag, -bias_mag])

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q at stage 1 from current Q2
        v2 = np.max(q2, axis=1)                 # value of each state (best alien)
        q1_mb = T @ v2

        # Mixture of MB and MF, plus anxiety-dependent static bias
        q1_mix = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1_mix + bias1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Model-free update at stage 1 toward the realized second-stage value
        delta1 = (q2[s, a2] - q1_mf[a1])
        q1_mf[a1] += alpha * delta1

        # Arbitration weight update: reduce MB weight when transition is surprising,
        # with anxiety scaling this effect.
        p_obs = T[a1, s]                 # probability of observed transition given chosen action
        trans_surprise = 1.0 - p_obs     # 0 for common (0.7), 0.7 for rare (0.3)
        target_w = 1.0 - stai * trans_surprise
        target_w = min(1.0, max(0.0, target_w))
        w = (1.0 - alpha_w) * w + alpha_w * target_w

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['alpha', 'beta', 'w0', 'alpha_w', 'kappa_bias']"
iter5_run0_participant4.json,cognitive_model2,501.04849862948674,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Kalman-Rescorla hybrid with anxiety-inflated observation noise and stage-2 choice kernel.

    Overview
    - Stage-2 values are learned with an uncertainty-adaptive learning rate (Kalman-like):
      alpha_t = Var / (Var + sigma_obs_eff), updated per state-action.
    - Anxiety increases effective observation noise, reducing learning from outcomes.
    - A stage-2 choice kernel biases perseveration, strengthened by anxiety.
    - Stage-1 is purely model-based using the known transition matrix and current Q2.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - beta:        [0,10]  Inverse temperature for softmax (both stages).
    - sigma_obs:   [0,1]   Base observation noise for outcome (Bernoulli-like).
    - kappa_proc:  [0,1]   Process noise; increases uncertainty over time when options are sampled.
    - rho_anx:     [0,1]   Scales how much anxiety inflates observation noise: sigma_eff = sigma_obs*(1 + rho_anx*stai).
    - tau_ck2:     [0,1]   Learning rate/strength for stage-2 choice kernel (perseveration).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [beta, sigma_obs, kappa_proc, rho_anx, tau_ck2].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    beta, sigma_obs, kappa_proc, rho_anx, tau_ck2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Stage-2 value means and variances
    q2 = np.zeros((2, 2)) + 0.5
    v2 = np.zeros((2, 2)) + 0.25  # initial uncertainty

    # Stage-2 choice kernel (per-state)
    ck2 = np.zeros((2, 2))

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-inflated observation noise
    sigma_eff = sigma_obs * (1.0 + rho_anx * stai)
    sigma_eff = max(1e-6, min(1.0, sigma_eff))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1: pure MB from current Q2
        v_state = np.max(q2, axis=1)
        q1_mb = T @ v_state

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Stage-2 policy with choice kernel; kernel gain increases with anxiety
        kernel_gain = 1.0 + stai
        logits2 = beta * q2[s] + ck2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Kalman-like update at stage 2 for chosen (s, a2)
        # Adaptive learning rate
        alpha_t = v2[s, a2] / (v2[s, a2] + sigma_eff)
        alpha_t = max(0.0, min(1.0, float(alpha_t)))

        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * pe2

        # Update uncertainty: posterior variance and process noise injection
        v2[s, a2] = (1.0 - alpha_t) * v2[s, a2] + kappa_proc
        v2[s, a2] = max(1e-6, min(1.0, v2[s, a2]))

        # Update choice kernel (perseveration) with anxiety-scaled increment
        ck2[s] = (1.0 - tau_ck2) * ck2[s]
        ck2[s, a2] += tau_ck2 * kernel_gain

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['beta', 'sigma_obs', 'kappa_proc', 'rho_anx', 'tau_ck2']"
iter5_run0_participant4.json,cognitive_model3,451.9136930087336,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning MB with anxiety-weighted risk sensitivity and stage-1 stickiness.

    Overview
    - Learns the state transition probabilities online (per action) via a simple delta rule.
    - Stage-1 model-based values incorporate a risk penalty: V_state = E[Q2] - zeta*stai*Var,
      where Var is approximated by q*(1-q) for the best action in each state.
    - Stage-2 values learned via RescorlaâWagner (alpha).
    - Stage-1 stickiness (choice kernel) biases repeating the previous stage-1 choice,
      with magnitude scaled by anxiety.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:        [0,1]   Learning rate for Q2 updates.
    - beta:         [0,10]  Inverse temperature for softmax at both stages.
    - zeta_risk:    [0,1]   Strength of risk penalty at stage 1, scaled by anxiety.
    - phi_trans:    [0,1]   Learning rate for updating transition probabilities T_hat.
    - kappa_stick:  [0,1]   Stage-1 stickiness (choice kernel) learning rate/strength.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, zeta_risk, phi_trans, kappa_stick].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, zeta_risk, phi_trans, kappa_stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T_hat to be agnostic (0.5/0.5 per action)
    T_hat = np.ones((2, 2)) * 0.5

    # Stage-2 values
    q2 = np.zeros((2, 2)) + 0.5

    # Stage-1 choice kernel (stickiness)
    ck1 = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Risk-adjusted state values: V_state = max_a q2[s,a] - zeta*stai*Var
        q2_max = np.max(q2, axis=1)
        q2_argmax = np.argmax(q2, axis=1)
        # Approximate Bernoulli variance for the best action in each state
        best_probs = q2[np.arange(2), q2_argmax]
        var_state = best_probs * (1.0 - best_probs)
        risk_penalty = zeta_risk * stai * var_state
        v_state = q2_max - risk_penalty

        # Stage-1 model-based Q using learned transitions
        q1_mb = T_hat @ v_state

        # Add stage-1 stickiness (anxiety scales its effective strength)
        stick_gain = 0.5 + 0.5 * stai
        logits1 = beta * q1_mb + ck1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update stickiness kernel
        ck1 = (1.0 - kappa_stick) * ck1
        ck1[a1] += kappa_stick * stick_gain

        # Transition learning for chosen action: move probability toward observed state
        # Ensure row sums to 1 by updating both columns.
        T_hat[a1, s] += phi_trans * (1.0 - T_hat[a1, s])
        other = 1 - s
        T_hat[a1, other] = 1.0 - T_hat[a1, s]

        # Keep probabilities in [0,1]
        T_hat[a1, s] = min(1.0, max(0.0, T_hat[a1, s]))
        T_hat[a1, other] = 1.0 - T_hat[a1, s]

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)","['alpha', 'beta', 'zeta_risk', 'phi_trans', 'kappa_stick']"
iter5_run0_participant40.json,cognitive_model1,510.915048855613,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MBâMF with anxiety-gated arbitration, decay-forgetting, and choice perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, omega_base, kappa_rep, mu_forget)
        - alpha2 in [0,1]: learning rate for second-stage MF values and stage-1 MF bootstrapping.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - omega_base in [0,1]: baseline MB arbitration weight at stage 1.
        - kappa_rep in [0,1]: choice perseveration strength (applied to both stages).
        - mu_forget in [0,1]: global forgetting factor applied to all second-stage Q-values.
          Effective forgetting scales with anxiety: decay = mu_forget * stai.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes
    -----
    - Transition structure is known: AâX and UâY with prob 0.7 (rare=0.3).
    - Stage-1 policy is hybrid: Q1 = (1 - omega_eff)*Q1_MF + omega_eff*(T @ max(Q2)).
      Anxiety reduces reliance on model-based control: omega_eff = omega_base * (1 - stai).
    - Second-stage forgetting is stronger with higher anxiety: Q2 â (1 - mu_forget*stai) * Q2 each trial.
    - Perseveration biases logits toward repeating the last chosen action; bias magnitude increases
      mildly with anxiety: bias = kappa_rep * (0.5 + 0.5*stai) on the previously chosen action.
    """"""
    alpha2, beta, omega_base, kappa_rep, mu_forget = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition matrix: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities of observed actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    Q2 = np.zeros((2, 2))      # second-stage MF values: Q2[state, action]
    Q1_MF = np.zeros(2)        # first-stage MF values

    prev_a1 = None
    prev_a2 = None

    # Anxiety-gated arbitration and forgetting
    omega_eff = omega_base * (1.0 - stai_val)
    decay = mu_forget * stai_val

    for t in range(n_trials):
        # Model-based contribution to stage-1 values
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Hybrid Q at stage 1
        Q1 = (1.0 - omega_eff) * Q1_MF + omega_eff * Q1_MB

        # Perseveration biases (increase with anxiety)
        bias_mag = kappa_rep * (0.5 + 0.5 * stai_val)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += bias_mag

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += bias_mag
        logits2 = beta * Q2[s2] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Learning: stage-2 TD
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Stage-1 MF bootstrapping from realized second-stage chosen value
        delta1 = Q2[s2, a2] - Q1_MF[a1]
        Q1_MF[a1] += alpha2 * delta1

        # Anxiety-scaled forgetting across all Q2 values
        if decay > 0.0:
            Q2 *= (1.0 - decay)

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'omega_base', 'kappa_rep', 'mu_forget']"
iter5_run0_participant5.json,cognitive_model1,457.657484470399,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-gated MB/MF arbitration with learned transitions and lapse.

    Core ideas
    - Learn the transition matrix P(planet | spaceship) online (alpha_tr).
    - First-stage policy mixes model-based (MB) and model-free (MF) values with
      an arbitration weight that decreases with transition uncertainty and with anxiety.
    - Second-stage values learned via TD (alpha_mf).
    - A lapse component increases with anxiety, adding choice noise.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher anxiety reduces MB arbitration weight and increases lapse.
    model_parameters : array-like of float
        [alpha_mf, beta, k_mb0, alpha_tr, lapse0]
        - alpha_mf in [0,1]: TD learning rate for MF Q updates.
        - beta in [0,10]: inverse temperature for both stages.
        - k_mb0 in [0,1]: baseline MB arbitration weight (scaled by uncertainty and anxiety).
        - alpha_tr in [0,1]: learning rate for transition probabilities.
        - lapse0 in [0,1]: baseline lapse rate blended with softmax; increases with anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha_mf, beta, k_mb0, alpha_tr, lapse0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix; start near canonical structure but uncertain
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # MF action values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    # Lapse increases with anxiety
    lapse_eff = np.clip(lapse0 * (0.5 + 0.5 * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute transition uncertainty (entropy averaged across actions)
        row_ent = []
        for a in range(2):
            p_row = np.clip(T[a], eps, 1.0)
            h = -np.sum(p_row * np.log(p_row))
            row_ent.append(h)
        unc = 0.5 * (row_ent[0] + row_ent[1])  # higher = more uncertain
        # Normalize uncertainty to [0, ln2]; convert to [0,1]
        unc_norm = np.clip(unc / np.log(2.0), 0.0, 1.0)

        # MB estimate for first stage
        max_q2 = np.max(q2, axis=1)  # best available at each planet
        q1_mb = T @ max_q2

        # Arbitration weight: lower when transitions are uncertain or anxiety is high
        w_mb = np.clip(k_mb0 * (1.0 - 0.6 * unc_norm) * (1.0 - 0.5 * st), 0.0, 1.0)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage policy: softmax with lapse
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        probs1 = (1.0 - lapse_eff) * probs1 + lapse_eff * 0.5
        p1[t] = probs1[a1]

        # Second-stage policy: softmax with the same lapse
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        probs2 = (1.0 - lapse_eff) * probs2 + lapse_eff * 0.5
        p2[t] = probs2[a2]

        # Update learned transitions using observed outcome of first-stage action
        # Move the chosen row toward the observed state
        T[a1] *= (1.0 - alpha_tr)
        T[a1, s] += alpha_tr
        # Ensure rows sum to 1, stay within bounds
        T[a1] = np.clip(T[a1], 0.0, 1.0)
        T[a1] /= (np.sum(T[a1]) + eps)

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_mf * pe2

        # Stage-1 MF update bootstrapped from obtained second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_mf * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha_mf', 'beta', 'k_mb0', 'alpha_tr', 'lapse0']"
iter5_run0_participant5.json,cognitive_model2,421.64186216810504,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Asymmetric learning with anxiety amplification and perseveration bias.

    Core ideas
    - Stage-2 learning uses different learning rates for wins and losses, and anxiety
      amplifies loss learning and dampens win learning.
    - First-stage value is a hybrid: MB weight is a simple function of anxiety (no parameter),
      MF learned via TD from stage-2.
    - Perseveration bias (same-choice tendency) is present at both stages and grows with anxiety.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase perseveration and loss-focused learning.
    model_parameters : array-like of float
        [alpha_win, alpha_lose, beta, kappa_pers]
        - alpha_win in [0,1]: learning rate when reward prediction error is positive.
        - alpha_lose in [0,1]: learning rate when reward prediction error is non-positive.
        - beta in [0,10]: inverse temperature for both stages.
        - kappa_pers in [0,1]: baseline perseveration strength.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha_win, alpha_lose, beta, kappa_pers = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure according to task description
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # MF values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Perseveration kernels (one-hot of last choice)
    last_a1 = None
    last_a2 = [None, None]  # per state

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-gated components
    # MB weight increases when anxiety is low
    w_mb = np.clip(0.7 * (1.0 - st), 0.0, 1.0)
    # Effective perseveration grows with anxiety
    kappa_eff = np.clip(kappa_pers * (0.5 + 0.5 * st), 0.0, 5.0)  # cap bias magnitude

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Anxiety-attenuated reward impact (reduced utility under high anxiety)
        r_eff = (0.8 - 0.3 * st) * r

        # MB first-stage values from current second-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add perseveration bias to first stage
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa_eff

        logits1 = beta * (q1 - np.max(q1)) + bias1 - np.max(bias1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Second-stage policy with state-dependent perseveration
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += kappa_eff

        logits2 = beta * (q2[s] - np.max(q2[s])) + bias2 - np.max(bias2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Stage-2 asymmetric learning with anxiety amplification
        pe2 = r_eff - q2[s, a2]
        if pe2 > 0:
            alpha2 = alpha_win * (1.0 - 0.5 * st)
        else:
            alpha2 = alpha_lose * (0.5 + 0.5 * st)
        q2[s, a2] += alpha2 * pe2

        # Stage-1 MF update from obtained second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use a moderate learning rate derived from the two, scaled by anxiety
        alpha1 = (0.5 * (alpha_win + alpha_lose)) * (0.8 + 0.2 * (1.0 - st))
        q1_mf[a1] += alpha1 * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha_win', 'alpha_lose', 'beta', 'kappa_pers']"
iter5_run0_participant5.json,cognitive_model3,414.58037886263327,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Kalman-like reward tracking with anxiety-driven volatility and uncertainty bonus.

    Core ideas
    - Track each second-stage action's mean reward and uncertainty (variance).
    - Volatility/process noise increases with anxiety, yielding higher Kalman gains.
    - Stage-2 policy includes a directed exploration bonus proportional to uncertainty,
      attenuated by anxiety.
    - Stage-1 uses a hybrid of MB (via expected values from the transition model)
      and MF with small forgetting.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase volatility and reduce directed exploration.
    model_parameters : array-like of float
        [beta, vol0, omega_mix, theta_decay]
        - beta in [0,10]: inverse temperature for both stages.
        - vol0 in [0,1]: baseline process noise for reward dynamics (higher -> more volatile).
        - omega_mix in [0,1]: baseline MB weight at stage 1.
        - theta_decay in [0,1]: MF stage-1 forgetting toward 0 (per trial).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    beta, vol0, omega_mix, theta_decay = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure as per task
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 estimates: mean and variance for each planet-alien pair
    m2 = 0.5 * np.ones((2, 2))
    v2 = 0.25 * np.ones((2, 2))  # initial uncertainty
    v_min, v_max = 1e-6, 0.25

    # Stage-1 MF values
    q1_mf = np.zeros(2)

    # Anxiety effects
    # Volatility increases with anxiety
    q_proc = np.clip(vol0 * (0.5 + 0.5 * st), 0.0, 1.0)
    # Directed exploration bonus scale decreases with anxiety
    bonus_scale = np.clip(0.5 * (1.0 - st), 0.0, 1.0)
    # MB weight decreases with anxiety
    w_mb = np.clip(omega_mix * (1.0 - 0.5 * st), 0.0, 1.0)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Build uncertainty-bonus-augmented values at stage 2
        bonus = bonus_scale * np.sqrt(np.clip(v2, v_min, v_max))
        q2_aug = m2 + bonus

        # First-stage MB values: expectation over next-state max augmented value
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_q2_aug

        # Combine MB and MF for stage 1
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Policies
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        logits2 = beta * (q2_aug[s] - np.max(q2_aug[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Kalman-like update for the chosen second-stage action
        # Predict step: increase variance by process noise
        v_pred = np.clip(v2[s, a2] + q_proc, v_min, v_max)
        m_pred = m2[s, a2]

        # Observation noise for Bernoulli reward approximated by p(1-p)
        rvar = np.clip(m_pred * (1.0 - m_pred), 1e-4, 0.25)

        K = v_pred / (v_pred + rvar)  # Kalman gain in [0,1]
        pe = r - m_pred
        m2[s, a2] = m_pred + K * pe
        v2[s, a2] = (1.0 - K) * v_pred
        v2[s, a2] = np.clip(v2[s, a2], v_min, v_max)

        # Stage-1 MF: decay then TD update toward observed second-stage mean value
        q1_mf = (1.0 - theta_decay) * q1_mf
        target1 = m2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Effective learning rate mirrors the current Kalman gain (confidence in update)
        q1_mf[a1] += K * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)","['beta', 'vol0', 'omega_mix', 'theta_decay']"
iter5_run0_participant7.json,cognitive_model1,505.896363523025,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive hybrid MB/MF with anxiety-modulated reward curvature.

    Overview:
    - Stage 2 values learned via TD(0).
    - Stage 1 choice values are a mixture of model-based (using known transitions) and model-free values.
    - Rewards are transformed by a concave/convex power utility u(r) = sign(r)*|r|^gamma_eff.
    - Trait anxiety (stai) modulates curvature: gamma_eff = clip(gamma0 + k_gamma*stai, [0,1]).
      Lower gamma => more risk/loss aversion; higher gamma => more risk seeking in magnitude.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates (both stages).
    - beta: [0,10] inverse temperature for both stages.
    - w: [0,1] weight on model-based control at stage 1.
    - gamma0: [0,1] base utility curvature.
    - k_gamma: [0,1] how strongly anxiety shifts curvature (gamma_eff = gamma0 + k_gamma*stai).

    Inputs:
    - action_1: array-like of ints {0,1}, first-stage choices (0=A, 1=U).
    - state: array-like of ints {0,1}, second-stage states (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage choices (aliens indices).
    - reward: array-like of floats, coins received (can be negative/zero/positive).
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha, beta, w, gamma0, k_gamma).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w, gamma0, k_gamma = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Initialize values
    Q2 = np.zeros((2, 2))   # stage-2 Q(s, a2)
    Q1_mf = np.zeros(2)     # stage-1 MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated utility curvature
    gamma_eff = gamma0 + k_gamma * stai
    if gamma_eff < 0.0:
        gamma_eff = 0.0
    if gamma_eff > 1.0:
        gamma_eff = 1.0

    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q at stage 1 from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Mixture MB/MF
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # Stage 1 choice probability
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 choice probability
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Utility transform of reward with anxiety-modulated curvature
        r = reward[t]
        if r >= 0:
            u = (abs(r)) ** gamma_eff
        else:
            u = - (abs(r)) ** gamma_eff

        # TD update stage 2
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage 1 MF bootstrapping from updated stage-2 value
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w', 'gamma0', 'k_gamma']"
iter5_run0_participant7.json,cognitive_model2,489.3714930467389,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated surprise-seeking intrinsic motivation.

    Overview:
    - Stage 2: standard TD(0) learning on rewards.
    - Stage 1: mixture of model-based and model-free values.
    - Additionally, an intrinsic ""surprise"" signal is maintained per first-stage action:
        surprise_t = -log P(observed_state | chosen_ship) under the known transition model.
      A running trace of surprise per action S[a] is updated and added as a bonus to stage-1 values.
    - Trait anxiety scales the strength of the surprise bonus:
        z_eff = clip(z0 + k_z * stai, [0,1]).
      Higher z_eff increases preference for actions that recently produced surprising transitions.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for Q-value updates and surprise trace updates.
    - beta: [0,10] inverse temperature for both stages.
    - w: [0,1] weight on model-based control at stage 1.
    - z0: [0,1] base weight of surprise bonus.
    - k_z: [0,1] how strongly anxiety increases surprise bonus (z_eff = z0 + k_z*stai).

    Inputs:
    - action_1: array-like of ints {0,1}, first-stage choices (0=A, 1=U).
    - state: array-like of ints {0,1}, second-stage states (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage choices.
    - reward: array-like of floats.
    - stai: array-like with a single float in [0,1].
    - model_parameters: tuple/list of 5 params (alpha, beta, w, z0, k_z).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w, z0, k_z = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transitions for surprise computation
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    Q2 = np.zeros((2, 2))   # stage-2
    Q1_mf = np.zeros(2)     # stage-1 MF
    S = np.zeros(2)         # surprise trace per first-stage action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    z_eff = z0 + k_z * stai
    if z_eff < 0.0:
        z_eff = 0.0
    if z_eff > 1.0:
        z_eff = 1.0

    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q at stage 1 from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Add intrinsic surprise bonus to stage-1 values
        # The bonus is added additively to the MB/MF mixture.
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf + z_eff * S

        # Stage 1 choice probability
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 choice probability
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward learning at stage 2
        r = reward[t]
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage 1 MF bootstrapping
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Update surprise trace for the chosen first-stage action using observed transition
        # surprise = -log P(s | a1) under known T
        p_s_given_a = T[a1, s]
        # Prevent log(0)
        if p_s_given_a < 1e-8:
            p_s_given_a = 1e-8
        surprise = -np.log(p_s_given_a)
        # Running exponential average with learning rate alpha
        S[a1] += alpha * (surprise - S[a1])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w', 'z0', 'k_z']"
iter5_run0_participant8.json,cognitive_model1,520.7292947424411,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""SRâMB arbitration with anxiety-modulated model-based control.
    
    Summary
    -------
    This model blends a model-based (MB) planner with a learned successor-like
    representation (SR) of first-stage actions predicting second-stage states.
    Anxiety reduces reliance on the MB planner in favor of the SR cache.
    
    Mechanics
    ---------
    - Stage 2: Learn Q-values for aliens via delta-rule.
    - SR: Learn a mapping M[a1, s2] via a delta-rule toward the reached state.
    - Stage 1 values:
        MB: E[max_a2 Q2 | transition_matrix]
        SR: SR-predicted occupancy dot max_a2 Q2
      Q1 = w_mb * MB + (1 - w_mb) * SR, where w_mb decreases with anxiety.
    - Policies: softmax with inverse temperature beta at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1]. Higher anxiety reduces MB weight.
    model_parameters : iterable of floats
        [alpha_r, beta, w0, alpha_sr, kappa_anx]
        - alpha_r: learning rate for stage-2 Q-values [0,1]
        - beta: inverse temperature for both stages [0,10]
        - w0: baseline MB weight in arbitration [0,1]
        - alpha_sr: learning rate for SR mapping [0,1]
        - kappa_anx: strength by which anxiety lowers MB reliance [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_r, beta, w0, alpha_sr, kappa_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure (common transitions)
    T_mb = np.array([[0.7, 0.3],  # From A to [X, Y]
                     [0.3, 0.7]], # From U to [X, Y]
                    dtype=float)

    # Initialize tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values (planets x aliens), initialized to 0.5
    q2 = np.zeros((2, 2)) + 0.5

    # Successor-like mapping: M[a1, s2] ~ P(s2 | a1) learned from experience
    M = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Effective MB weight, reduced by anxiety
    w_mb = w0 * (1.0 - kappa_anx * st)
    w_mb = np.clip(w_mb, 0.0, 1.0)

    for t in range(n_trials):
        # Compute MB and SR action values for stage 1
        vmax2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T_mb @ vmax2        # model-based
        q1_sr = M @ vmax2           # SR-based
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_sr

        # Stage 1 choice probability
        logits1 = q1 - np.max(q1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy within reached planet
        s2 = state[t]
        logits2 = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        # Update stage-2 Q
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_r * pe2

        # Update SR mapping from chosen a1 toward the reached state
        target = np.array([0.0, 0.0])
        target[s2] = 1.0
        M[a1] = (1.0 - alpha_sr) * M[a1] + alpha_sr * target
        # Normalize row to stay a distribution
        row_sum = M[a1, 0] + M[a1, 1]
        if row_sum > 0:
            M[a1] = M[a1] / row_sum

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'w0', 'alpha_sr', 'kappa_anx']"
iter5_run0_participant8.json,cognitive_model2,324.084119203652,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Directed exploration via leaky Bayesian counts, scaled by anxiety.
    
    Summary
    -------
    This model encourages sampling uncertain aliens by adding an uncertainty bonus
    to second-stage values, with leaky accumulation of successes and counts to track
    slowly drifting probabilities. Anxiety increases the directed exploration bonus and
    slightly reduces perseveration bias.
    
    Mechanics
    ---------
    - Maintain leaky counts N[s,a] and successes S[s,a].
    - Estimated mean: m = S / (N + tiny); uncertainty: u = 1 / (N + 1).
    - Stage 2 value: Q2 = m + phi(stai) * u, where phi increases with anxiety.
    - Stage 1 value: MB projection of max Q2 via known transitions.
    - Perseveration at stage 1 reduced by anxiety.
    - Policies: softmax with inverse temperature beta at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1]. Higher anxiety increases exploration bonus.
    model_parameters : iterable of floats
        [rho_decay, beta, phi0, pi0]
        - rho_decay: leaky decay factor applied each trial to counts and successes [0,1]
                     (larger means slower forgetting; 0 = full reset, 1 = no decay)
        - beta: inverse temperature for both stages [0,10]
        - phi0: baseline directed exploration bonus weight [0,1]
        - pi0: baseline perseveration bias magnitude at stage 1 [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    rho_decay, beta, phi0, pi0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Leaky Bayesian counts for each alien
    N = np.zeros((2, 2)) + 1.0  # start with weak prior count to avoid zero
    S = np.zeros((2, 2)) + 0.5  # weak prior successes for ~0.5 mean

    prev_a1 = None

    # Anxiety-modulated exploration bonus and perseveration
    phi = phi0 * (0.5 + st)           # higher anxiety -> larger exploration bonus
    persev = pi0 * (1.0 - 0.5 * st)   # higher anxiety -> less perseveration

    for t in range(n_trials):
        # Leak counts each trial
        N *= rho_decay
        S *= rho_decay

        # Compute posterior means and uncertainty
        m = S / (N + 1e-8)
        u = 1.0 / (N + 1.0)

        # Stage 2 action values with uncertainty bonus
        q2 = m + phi * u

        # Stage 1 MB projection
        vmax2 = np.max(q2, axis=1)
        q1 = T @ vmax2

        # Add perseveration bias at stage 1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += persev

        logits1 = q1 + bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in reached planet
        s2 = state[t]
        logits2 = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Update counts with leaky accumulation using observed outcome
        r = reward[t]
        N[s2, a2] += 1.0
        S[s2, a2] += r

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['rho_decay', 'beta', 'phi0', 'pi0']"
iter6_run0_participant0.json,cognitive_model1,515.0375425820882,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-tilted arbitration and transition stickiness.

    Bounds for parameters:
    - alpha: [0,1] learning rate for second-stage values
    - beta: [0,10] inverse temperature for both stages
    - w0: [0,1] baseline model-based weight at stage 1
    - kappa_trans: [0,1] transition-stickiness bias toward the ship commonly leading to last reached planet
    - lambda_elig: [0,1] eligibility trace strength from stage 2 PE to stage 1 MF value
    - anx_gain: [0,1] scales how anxiety (stai) shifts arbitration weight toward MB

    Inputs
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U)
    - state: array-like (n_trials,), reached planet (0=X, 1=Y)
    - action_2: array-like (n_trials,), second-stage choices (0 or 1)
    - reward: array-like (n_trials,), reward per trial (e.g., 0/1)
    - stai: array-like (1,), anxiety score in [0,1]
    - model_parameters: [alpha, beta, w0, kappa_trans, lambda_elig, anx_gain]

    Model summary
    - Stage-2 values learned with a simple delta rule.
    - Stage-1 action values are a convex combination of model-based (MB) and model-free (MF) values.
      The MB/MF arbitration weight is shifted by anxiety: w_mb = clip(w0 + anx_gain*(stai-0.5), 0,1).
    - Transition stickiness: after visiting a planet, the agent is biased toward the ship that commonly goes to that planet
      (i.e., if last state was X, bias ship A; if Y, bias ship U) by kappa_trans.
    - Eligibility trace: stage-2 prediction error propagates to the MF value of the chosen first-stage action scaled by lambda_elig.

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, w0, kappa_trans, lambda_elig, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clamp parameters to their bounds
    alpha = min(1.0, max(0.0, alpha))
    beta = min(10.0, max(1e-6, beta))
    w0 = min(1.0, max(0.0, w0))
    kappa_trans = min(1.0, max(0.0, kappa_trans))
    lambda_elig = min(1.0, max(0.0, lambda_elig))
    anx_gain = min(1.0, max(0.0, anx_gain))

    # Fixed transition structure (common 0.7, rare 0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value structures
    q2 = np.zeros((2, 2), dtype=float)     # stage-2 Q(s,a2)
    q1_mf = np.zeros(2, dtype=float)       # stage-1 MF Q(a1)

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Last visited planet to implement transition stickiness
    last_state = None

    # Anxiety-tilted arbitration
    w_mb = w0 + anx_gain * (stai - 0.5)
    w_mb = min(1.0, max(0.0, w_mb))

    for t in range(n_trials):

        # Model-based stage-1 values via expected max stage-2 values
        v2_max = np.max(q2, axis=1)                 # value per planet
        q1_mb = T @ v2_max                          # expected by each ship

        # Transition stickiness bias
        bias = np.zeros(2, dtype=float)
        if last_state is not None:
            # ship 0 commonly -> state 0; ship 1 commonly -> state 1
            bias[last_state] += kappa_trans  # adds to ship matching last visited planet

        # Combine MB and MF
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb + bias

        # Stage-1 choice policy
        q1_c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_c)
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice policy
        s = state[t]
        q2_s = q2[s]
        q2s_c = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2s_c)
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observed reward
        r = reward[t]

        # Stage-2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility trace to stage-1 MF
        q1_mf[a1] += alpha * lambda_elig * pe2

        # Update last visited state for next trial stickiness
        last_state = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'w0', 'kappa_trans', 'lambda_elig', 'anx_gain']"
iter6_run0_participant0.json,cognitive_model2,498.3467371688894,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Second-stage uncertainty bonus via adaptive variance, with anxiety-amplified exploration and first-stage perseveration.

    Bounds for parameters:
    - eta_base: [0,1] base scaling of adaptive learning (influences gain via variance)
    - beta: [0,10] inverse temperature for both stages
    - phi_ucb: [0,1] base UCB bonus weight on uncertainty (variance)
    - anx_ucb: [0,1] scales UCB bonus with anxiety
    - chi_decay: [0,1] decay/smoothing of variance estimate after PE
    - stick1: [0,1] perseveration bias to repeat last first-stage action

    Inputs
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U)
    - state: array-like (n_trials,), reached planet (0=X, 1=Y)
    - action_2: array-like (n_trials,), second-stage choices (0 or 1)
    - reward: array-like (n_trials,), reward per trial
    - stai: array-like (1,), anxiety score in [0,1]
    - model_parameters: [eta_base, beta, phi_ucb, anx_ucb, chi_decay, stick1]

    Model summary
    - Maintains stage-2 Q-values and an uncertainty (variance-like) tracker per state-action.
    - Trial-wise learning gain k_t is proportional to current uncertainty: k_t = clip(eta_base * var, 0, 1).
    - UCB-like bonus added to stage-2 decision values: bonus = (phi_ucb * (1 + anx_ucb*stai)) * sqrt(var).
      Anxiety increases the exploration bonus linearly.
    - First-stage values are purely model-based, using expected max of (Q2 + bonus).
    - Perseveration at stage 1 biases repeating the previous ship choice.

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    eta_base, beta, phi_ucb, anx_ucb, chi_decay, stick1 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clamp parameters
    eta_base = min(1.0, max(0.0, eta_base))
    beta = min(10.0, max(1e-6, beta))
    phi_ucb = min(1.0, max(0.0, phi_ucb))
    anx_ucb = min(1.0, max(0.0, anx_ucb))
    chi_decay = min(1.0, max(0.0, chi_decay))
    stick1 = min(1.0, max(0.0, stick1))

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 value and variance trackers
    q2 = np.zeros((2, 2), dtype=float)
    var2 = np.ones((2, 2), dtype=float) * 0.5  # start with moderate uncertainty

    # Likelihood containers
    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Perseveration memory
    last_a1 = None

    # Anxiety-scaled UCB gain
    ucb_gain = phi_ucb * (1.0 + anx_ucb * stai)

    for t in range(n_trials):

        # Compute UCB bonuses for both states
        bonus = ucb_gain * np.sqrt(np.maximum(var2, 1e-8))

        # Stage-1 MB values: expectation over max (Q2 + bonus) at each planet
        v2_aug = np.max(q2 + bonus, axis=1)  # per planet
        q1_mb = T @ v2_aug

        # Add perseveration bias to last chosen action
        q1 = q1_mb.copy()
        if last_a1 is not None:
            q1[last_a1] += stick1

        # Stage-1 choice policy
        q1_c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_c)
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice policy using augmented values at visited state
        s = state[t]
        q2_aug_s = q2[s] + bonus[s]
        q2s_c = q2_aug_s - np.max(q2_aug_s)
        exp_q2 = np.exp(beta * q2s_c)
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Prediction error on raw Q (no bonus in learning)
        pe = r - q2[s, a2]

        # Learning gain proportional to uncertainty, scaled by eta_base
        k_t = eta_base * var2[s, a2]
        k_t = min(1.0, max(0.0, k_t))

        # Update value
        q2[s, a2] += k_t * pe

        # Update variance (higher with surprise, decays otherwise)
        var2[s, a2] = (1.0 - chi_decay) * var2[s, a2] + chi_decay * abs(pe)
        # Keep variance within [0,1]
        var2[s, a2] = min(1.0, max(0.0, var2[s, a2]))

        # Perseveration memory
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta_base', 'beta', 'phi_ucb', 'anx_ucb', 'chi_decay', 'stick1']"
iter6_run0_participant0.json,cognitive_model3,523.8222731901786,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pearce-Hall associability with anxiety-weighted loss sensitivity, eligibility trace, and lapse.

    Bounds for parameters:
    - alpha0: [0,1] base learning rate scaling (modulated by associability)
    - beta: [0,10] inverse temperature for both stages
    - epsilon: [0,1] lapse probability mixed with uniform choice
    - lambda_elig: [0,1] eligibility trace strength to stage-1 MF
    - phi_ph: [0,1] Pearce-Hall associability update rate
    - rho_loss: [0,1] loss sensitivity factor; anxiety increases effective loss aversion
    - w_base: [0,1] baseline model-based weight in stage-1 arbitration (modulated by associability and anxiety)

    Inputs
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U)
    - state: array-like (n_trials,), reached planet (0=X, 1=Y)
    - action_2: array-like (n_trials,), second-stage choices (0 or 1)
    - reward: array-like (n_trials,), reward per trial (e.g., 0/1)
    - stai: array-like (1,), anxiety score in [0,1]
    - model_parameters: [alpha0, beta, epsilon, lambda_elig, phi_ph, rho_loss, w_base]

    Model summary
    - Stage-2 learning uses a Pearce-Hall associability per state-action that tracks absolute PE.
      Learning rate_t = alpha0 * associability_t * (1 + stai), allowing higher anxiety to amplify reactivity.
    - Outcomes are transformed with anxiety-weighted loss sensitivity:
        u(r) = 1 for r=1; u(r) = -rho_loss*(1+stai) for r=0.
    - Stage-1 values combine MF and MB with a dynamic weight:
        w_t = clip(w_base + (assoc_mean - 0.5) * (1 - 2*stai) * 0.5, 0, 1),
      meaning higher surprise shifts arbitration toward MB when anxiety is low; with higher anxiety, this shift is dampened or reversed.
    - Eligibility trace propagates stage-2 PE to the MF value of the chosen first-stage action.
    - Choice policies include an epsilon lapse that mixes softmax with uniform (0.5 at both stages).

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha0, beta, epsilon, lambda_elig, phi_ph, rho_loss, w_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clamp parameters
    alpha0 = min(1.0, max(0.0, alpha0))
    beta = min(10.0, max(1e-6, beta))
    epsilon = min(1.0, max(0.0, epsilon))
    lambda_elig = min(1.0, max(0.0, lambda_elig))
    phi_ph = min(1.0, max(0.0, phi_ph))
    rho_loss = min(1.0, max(0.0, rho_loss))
    w_base = min(1.0, max(0.0, w_base))

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and associabilities
    q2 = np.zeros((2, 2), dtype=float)
    assoc = np.ones((2, 2), dtype=float) * 0.5  # start at moderate associability
    q1_mf = np.zeros(2, dtype=float)

    # Likelihood holders
    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):

        # MB stage-1 values via expected max stage-2 values
        v2_max = np.max(q2, axis=1)
        q1_mb = T @ v2_max

        # Dynamic arbitration weight from associability and anxiety
        assoc_mean = np.mean(assoc)
        w_t = w_base + (assoc_mean - 0.5) * (1.0 - 2.0 * stai) * 0.5
        w_t = min(1.0, max(0.0, w_t))

        # Final stage-1 values
        q1 = (1.0 - w_t) * q1_mf + w_t * q1_mb

        # Stage-1 policy with epsilon lapse
        q1_c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_c)
        soft1 = exp_q1 / np.sum(exp_q1)
        probs1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with epsilon lapse
        s = state[t]
        q2_s = q2[s]
        q2s_c = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2s_c)
        soft2 = exp_q2 / np.sum(exp_q2)
        probs2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Anxiety-weighted loss-sensitive utility
        r = reward[t]
        u = 1.0 if r > 0.0 else -rho_loss * (1.0 + stai)

        # Stage-2 PE and associability-based learning rate
        pe2 = u - q2[s, a2]
        lr2 = alpha0 * assoc[s, a2] * (1.0 + stai)
        lr2 = min(1.0, max(0.0, lr2))
        q2[s, a2] += lr2 * pe2

        # Update associability (Pearce-Hall): move toward |PE|
        assoc[s, a2] = (1.0 - phi_ph) * assoc[s, a2] + phi_ph * abs(pe2)
        assoc[s, a2] = min(1.0, max(0.0, assoc[s, a2]))

        # Eligibility trace to stage-1 MF
        q1_mf[a1] += lr2 * lambda_elig * pe2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha0', 'beta', 'epsilon', 'lambda_elig', 'phi_ph', 'rho_loss', 'w_base']"
iter6_run0_participant1.json,cognitive_model1,529.9333595115036,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with uncertainty-effort arbitration, Dirichlet transition confidence, and MF eligibility.
    
    This model blends model-based (MB) and model-free (MF) control at the first stage via an arbitration
    signal that increases with transition-confidence (low entropy of learned transition rows) and decreases
    with cognitive effort costs that rise with anxiety. Second-stage values are learned with MF TD and
    an eligibility trace propagates reward back to first-stage MF values.

    Parameters (all in [0,1] except beta in [0,10]):
    - alphaQ: MF learning rate for Q-values at both stages (0..1).
    - beta: inverse temperature for both stages (0..10).
    - kappa1: first-stage perseveration bias (adds to logit for repeating previous spaceship; 0..1).
    - c_effort: baseline effort cost of MB planning (0..1).
    - zeta_eff_anx: anxiety scaling of effort cost; effective effort = c_effort * (1 + zeta_eff_anx*stai) (0..1).
    - lam: eligibility trace for propagating second-stage PE to first-stage MF Q (0..1).

    Args:
        action_1: 1D array of first-stage actions (0=A, 1=U).
        state: 1D array of encountered second-stage states (0=X, 1=Y).
        action_2: 1D array of second-stage actions within the encountered state (0/1).
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element in [0,1]: participant's anxiety score.
        model_parameters: [alphaQ, beta, kappa1, c_effort, zeta_eff_anx, lam]
    
    Returns:
        Negative log-likelihood of observed choices across both stages.
    """"""
    alphaQ, beta, kappa1, c_effort, zeta_eff_anx, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition counts with informative priors (reflecting common transitions)
    # Dirichlet pseudocounts: A->X common, U->Y common
    counts = np.array([[7.0, 3.0],   # for action 0 (A): counts over states [X, Y]
                       [3.0, 7.0]])  # for action 1 (U): counts over states [X, Y]
    # MF values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    prev_a1 = -1  # for perseveration
    prev_a2 = np.array([-1, -1], dtype=int)  # per state

    for t in range(n_trials):
        # Transition estimate and its confidence (low entropy -> high confidence)
        T = counts / (np.sum(counts, axis=1, keepdims=True) + eps)  # shape (2,2)
        # entropy per action row in bits
        H = -np.sum(T * (np.log(T + eps) / np.log(2.0)), axis=1)
        H_norm = H / 1.0  # max entropy in 2-outcome is 1 bit
        confidence_MB = 1.0 - np.mean(H_norm)  # in [0,1]

        # Anxiety-adjusted effort cost
        effort = c_effort * (1.0 + zeta_eff_anx * stai)
        # Arbitration weight favoring MB: increase with confidence, decrease with effort
        w = confidence_MB - effort
        # clip to [0,1]
        if w < 0.0:
            w = 0.0
        elif w > 1.0:
            w = 1.0

        # Model-based first-stage values via transition model and current second-stage values
        max_q2 = np.max(Q2, axis=1)  # value of each second-stage state
        Q1_mb = T @ max_q2  # shape (2,)

        # Combine MB and MF for first-stage action values
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # Add first-stage perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        # First-stage policy
        logits1 = beta * Q1 + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with simple perseveration within state
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa1 * 0.5  # reuse kappa1 at half-strength for stage 2 to keep params â¤7

        logits2 = beta * Q2[s] + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Second-stage TD
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaQ * pe2

        # First-stage MF with eligibility trace driven by second-stage PE
        Q1_mf[a1] += alphaQ * lam * pe2

        # Update transition counts with the observed transition
        counts[a1, s] += 1.0

        # Update perseverations
        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alphaQ', 'beta', 'kappa1', 'c_effort', 'zeta_eff_anx', 'lam']"
iter6_run0_participant1.json,cognitive_model2,530.9802701092412,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive MB at stage 1 with anxiety-amplified loss aversion and volatility-adaptive learning.
    
    First-stage choices are model-based using the fixed transition structure (common 0.7/0.3).
    Second-stage learning uses a dynamic learning rate that increases with unsigned prediction error
    (PearceâHall style). Anxiety amplifies both volatility sensitivity and loss aversion for omitted rewards.
    Perseveration at stage 2 captures habitual repetition within a state.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha0: base learning rate for second-stage Q (0..1).
    - beta: inverse temperature for both stages (0..10).
    - phi_loss: baseline loss-aversion parameter; anxiety scales the penalty for r=0 (0..1).
    - kappa_vol: volatility sensitivity; alpha_t = clip(alpha0 + kappa_vol*stai*|PE|) (0..1).
    - kappa2: second-stage perseveration bias (0..1).
    - epsilon: lapse rate applied to both stages (0..1).

    Args:
        action_1: 1D array of first-stage actions (0=A, 1=U).
        state: 1D array of encountered second-stage states (0=X, 1=Y).
        action_2: 1D array of second-stage actions within the encountered state (0/1).
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element in [0,1]: participant's anxiety score.
        model_parameters: [alpha0, beta, phi_loss, kappa_vol, kappa2, epsilon]
    
    Returns:
        Negative log-likelihood of observed choices across both stages.
    """"""
    alpha0, beta, phi_loss, kappa_vol, kappa2, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    prev_a1 = -1
    prev_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        # Stage 1: model-based evaluation from current Q2
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Optional small perseveration at stage 1 reusing kappa2 minimally
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += 0.5 * kappa2

        logits1 = beta * Q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        # Lapse
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2: within encountered state, include perseveration bias
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa2

        logits2 = beta * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome utility with anxiety-amplified loss aversion for r=0
        r = reward[t]
        # r in {0,1}. Penalize omissions: utility u = r - (phi_loss*stai)*(1-r)
        u = r - (phi_loss * stai) * (1.0 - r)

        # TD update with volatility-adaptive learning rate
        pe2 = u - Q2[s, a2]
        alpha_t = alpha0 + kappa_vol * stai * abs(pe2)
        # clip alpha_t
        if alpha_t < 0.0:
            alpha_t = 0.0
        elif alpha_t > 1.0:
            alpha_t = 1.0
        Q2[s, a2] += alpha_t * pe2

        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'phi_loss', 'kappa_vol', 'kappa2', 'epsilon']"
iter6_run0_participant12.json,cognitive_model1,504.1648808843041,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and learned transitions.
    
    This model blends a model-based (MB) and model-free (MF) controller at the first stage.
    The MB component uses a learned transition model T(s1->s2), updated trial-by-trial.
    The MF component backs up second-stage rewards to first-stage values.
    Anxiety (stai) shifts the arbitration weight toward or away from MB control.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=spaceship A, 1=spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration toward MB control.
    model_parameters : list or array
        [alpha, beta, omega_base, stai_alpha, alpha_trans]
        Bounds:
          alpha in [0,1]        : reward learning rate
          beta in [0,10]        : inverse temperature
          omega_base in [0,1]   : baseline MB weight at stage 1
          stai_alpha in [0,1]   : sensitivity of MB weight to STAI
          alpha_trans in [0,1]  : transition-learning rate for T
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, omega_base, stai_alpha, alpha_trans = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize value functions
    q_stage1_mf = np.zeros(2)        # MF values at stage 1 (A vs U)
    q_stage2 = np.zeros((2, 2))      # Second-stage Q-values: states X/Y x aliens (0/1)

    # Initialize transition model (row = first-stage action, col = second-stage state)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight toward MB control
    omega = np.clip(omega_base + stai_alpha * (stai - 0.5), 0.0, 1.0)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q at stage 1 using learned transitions
        max_q2 = np.max(q_stage2, axis=1)         # value of best alien in each state
        q1_mb = T @ max_q2                        # expected value of each first-stage action
        q1 = (1.0 - omega) * q_stage1_mf + omega * q1_mb

        # Stage 1 policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy for the visited state
        q2_s = q_stage2[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning: second stage TD
        pe2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        # MF backup to stage 1
        q_stage1_mf[a1] += alpha * pe2

        # Learn transitions from observed (a1 -> s)
        # Move the transition probabilities of row a1 toward the observed state s
        T[a1, :] = (1.0 - alpha_trans) * T[a1, :]
        T[a1, s] += alpha_trans
        # Numerical safety: renormalize the row
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'omega_base', 'stai_alpha', 'alpha_trans']"
iter6_run0_participant12.json,cognitive_model2,521.4060212508506,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive value transformation with anxiety-modulated curvature and lapse.
    
    This model is primarily model-based at stage 1 using fixed common-rare transitions.
    Decision values at both stages are transformed by a power-law utility curvature applied
    to learned expected values, capturing risk sensitivity. Anxiety (stai) modulates both the
    curvature and a small lapse (stimulus-independent) choice noise at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=spaceship A, 1=spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Modulates risk curvature and lapse probability.
    model_parameters : list or array
        [alpha, beta, curv_base, stai_gain, eps_base]
        Bounds:
          alpha in [0,1]      : reward learning rate
          beta in [0,10]      : inverse temperature
          curv_base in [0,1]  : baseline curvature exponent on Q (lower => more risk-averse)
          stai_gain in [0,1]  : how strongly STAI shifts curvature and lapse
          eps_base in [0,1]   : baseline lapse rate mixed with uniform choice
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, curv_base, stai_gain, eps_base = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Learned second-stage Q-values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated curvature and lapse
    curv = np.clip(curv_base + stai_gain * (stai - 0.5), 0.0, 1.0)
    lapse = np.clip(eps_base + 0.5 * stai_gain * (stai - 0.5), 0.0, 1.0)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q at stage 1 by propagating best alien values
        max_q2 = np.max(q2, axis=1)        # expected values per second-stage state
        q1_mb = T @ max_q2

        # Apply curvature to decision values (risk sensitivity on expected values)
        # Use small epsilon to avoid 0**0 ambiguity
        epsv = 1e-8
        q1_val = np.power(np.clip(q1_mb, 0.0, 1.0) + epsv, curv)
        q2_val = np.power(np.clip(q2[s], 0.0, 1.0) + epsv, curv)

        # Softmax with lapse at stage 1
        q1_centered = q1_val - np.max(q1_val)
        p1_soft = np.exp(beta * q1_centered)
        p1_soft = p1_soft / np.sum(p1_soft)
        p1 = (1.0 - lapse) * p1_soft + lapse * 0.5
        p_choice_1[t] = p1[a1]

        # Softmax with lapse at stage 2
        q2_centered = q2_val - np.max(q2_val)
        p2_soft = np.exp(beta * q2_centered)
        p2_soft = p2_soft / np.sum(p2_soft)
        p2 = (1.0 - lapse) * p2_soft + lapse * 0.5
        p_choice_2[t] = p2[a2]

        # Learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'curv_base', 'stai_gain', 'eps_base']"
iter6_run0_participant12.json,cognitive_model3,475.06927669948584,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""MF with transition-outcome interaction bias modulated by anxiety and an eligibility trace.
    
    This model is primarily model-free but adds a first-stage bias term that captures the
    canonical transition x outcome interaction (repeat after common+reward or rare+no-reward;
    switch after common+no-reward or rare+reward). Anxiety modulates the strength of this bias.
    A stage-2 to stage-1 eligibility trace propagates reward to the chosen first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=spaceship A, 1=spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Modulates strength of the interaction bias.
    model_parameters : list or array
        [alpha, beta, zeta_base, delta_stai, omega_trace]
        Bounds:
          alpha in [0,1]        : reward learning rate
          beta in [0,10]        : inverse temperature
          zeta_base in [0,1]    : baseline strength of transition-outcome interaction bias
          delta_stai in [0,1]   : how strongly STAI shifts the bias
          omega_trace in [0,1]  : eligibility trace from stage 2 to stage 1
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, zeta_base, delta_stai, omega_trace = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated bias strength
    zeta = np.clip(zeta_base + delta_stai * (stai - 0.5), 0.0, 1.0)

    prev_a1 = None
    prev_state = None
    prev_reward = None

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Build transition-outcome interaction bias on first-stage values
        bias_vec = np.zeros(2)
        if prev_a1 is not None:
            # common if A->X or U->Y; rare otherwise
            was_common = int((prev_a1 == 0 and prev_state == 0) or (prev_a1 == 1 and prev_state == 1))
            was_rewarded = 1 if prev_reward > 0 else 0

            # Interaction signal: +1 for repeat-favoring (common&reward or rare&no-reward), -1 otherwise
            interact = 1 if (was_common == was_rewarded) else -1

            # Apply symmetric bias to the previous action vs the alternative
            bias_vec[prev_a1] += zeta * interact
            bias_vec[1 - prev_a1] -= zeta * interact

        # Stage 1 policy (MF values plus bias)
        q1 = q1_mf + bias_vec
        q1_centered = q1 - np.max(q1)
        p1_soft = np.exp(beta * q1_centered)
        p1_soft = p1_soft / np.sum(p1_soft)
        p_choice_1[t] = p1_soft[a1]

        # Stage 2 policy
        q2_s = q2[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        p2_soft = np.exp(beta * q2_centered)
        p2_soft = p2_soft / np.sum(p2_soft)
        p_choice_2[t] = p2_soft[a2]

        # Learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility trace: propagate stage-2 PE to chosen first-stage action
        q1_mf[a1] += alpha * omega_trace * pe2

        # Optional MF bootstrapping toward current second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Store for next-trial bias
        prev_a1 = a1
        prev_state = s
        prev_reward = r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'zeta_base', 'delta_stai', 'omega_trace']"
iter6_run0_participant14.json,cognitive_model1,538.2546971290006,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and lapse exploration.

    Core ideas:
    - Stage-2 values (Q2) learned via model-free TD(0).
    - Stage-1 uses a hybrid of model-based (MB) and model-free (MF) values.
      MB values come from known transitions (common=0.7, rare=0.3) and max Q2 per planet.
      MF values are learned via bootstrapping from Q2.
    - Anxiety (stai) increases: 
        (a) the arbitration weight on MB planning (w), and 
        (b) a lapse-like random exploration rate (epsilon).
      This reflects higher anxiety shifting decisions toward planning yet noisier exploration.

    Parameters (model_parameters):
    - lr: learning rate for Q-value updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - w_base: base MB arbitration weight (before anxiety), in [0,1]
    - k_anx_w: anxiety gain on MB weight (positive pushes toward MB), in [0,1]
    - k_explore: anxiety gain on lapse rate (random choice probability), in [0,1]

    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on that planet (0/1)
    - reward: array of rewards per trial, typically 0/1
    - stai: array-like with single float [0-1] anxiety score
    - model_parameters: array-like of parameters as above

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    lr, beta, w_base, k_anx_w, k_explore = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known (instructed) transition structure: rows=spaceships [A,U], cols=planets [X,Y]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value tables
    q2 = np.zeros((2, 2))   # per planet s in {X,Y} and alien action a2 in {0,1}
    q1_mf = np.zeros(2)     # MF values for spaceships {A,U}

    # Prob tracking for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight (sigmoid mapping to [0,1])
    # Reference point at medium-high boundary (.51)
    w_raw = w_base + k_anx_w * (st - 0.51)
    w_mb = 1.0 / (1.0 + np.exp(-10.0 * (w_raw - 0.5)))  # sharpened sigmoid to keep within [0,1]

    # Anxiety-modulated lapse (epsilon-greedy-like mixture with uniform)
    eps1 = np.clip(k_explore * st, 0.0, 1.0)
    eps2 = np.clip(0.5 * k_explore * st, 0.0, 1.0)  # slightly smaller lapse at stage 2

    for t in range(n_trials):
        # Model-based Q at stage 1 from current Q2
        max_q2 = np.max(q2, axis=1)    # best alien per planet
        q1_mb = T @ max_q2             # expectation over planets given spaceships

        # Hybrid stage-1 value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy with softmax + lapse mixture
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        soft1 = exp1 / (np.sum(exp1) + 1e-12)
        probs1 = (1.0 - eps1) * soft1 + eps1 * 0.5  # uniform over 2 actions

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (planet-specific)
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        soft2 = exp2 / (np.sum(exp2) + 1e-12)
        probs2 = (1.0 - eps2) * soft2 + eps2 * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes and learning
        r = reward[t]

        # Stage-2 TD(0)
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2

        # Stage-1 MF bootstrapping toward the attained stage-2 value
        boot = q2[s, a2]
        pe1 = boot - q1_mf[a1]
        q1_mf[a1] += lr * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr', 'beta', 'w_base', 'k_anx_w', 'k_explore']"
iter6_run0_participant14.json,cognitive_model2,563.1553993615021,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-scaled associability (PearceâHall) with MB planning and lapse.

    Core ideas:
    - Stage 1 uses model-based evaluation from instructed transitions (0.7/0.3).
    - Stage 2 uses model-free TD learning with a dynamic, PE-driven associability
      that sets the learning rate per trial and per state-action.
    - Associability increases with unsigned prediction error (|PE|) and is
      further amplified by anxiety (stai), enabling faster adaptation under
      higher anxiety when outcomes are surprising.
    - A global lapse rate increases with anxiety and mixes softmax with uniform.

    Parameters (model_parameters):
    - alpha0: base learning-rate floor, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - phi_anx: anxiety gain on PE-driven associability, in [0,1]
    - lapse0: base lapse probability, in [0,1]
    - g_lapse: anxiety gain on lapse, in [0,1]

    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on that planet (0/1)
    - reward: array of rewards per trial, typically 0/1
    - stai: array-like with single float [0-1] anxiety score
    - model_parameters: array-like of parameters as above

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha0, beta, phi_anx, lapse0, g_lapse = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Instructed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and associability per (state, action2)
    q2 = np.zeros((2, 2))
    assoc = np.ones((2, 2)) * alpha0  # initialize associability near base rate

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated lapse used at both stages
    lapse = np.clip(lapse0 + g_lapse * st, 0.0, 1.0)

    for t in range(n_trials):
        # Stage-1 MB evaluation from current Q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Policy stage 1 (softmax + lapse)
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        soft1 = exp1 / (np.sum(exp1) + 1e-12)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy on reached planet
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        soft2 = exp2 / (np.sum(exp2) + 1e-12)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning with anxiety-scaled associability
        r = reward[t]
        pe2 = r - q2[s, a2]

        # Trial- and option-specific learning rate from associability
        # alpha_t = clip(alpha0 + phi_anx * stai * |PE|, 0, 1)
        alpha_t = np.clip(alpha0 + phi_anx * st * abs(pe2), 0.0, 1.0)

        # Update Q2 with this dynamic rate
        q2[s, a2] += alpha_t * pe2

        # Update associability itself toward |PE| (PearceâHall style)
        # assoc <- (1 - k) * assoc + k * |PE|, with k tied to anxiety for reactivity
        k = np.clip(phi_anx * st, 0.0, 1.0)
        assoc[s, a2] = (1.0 - k) * assoc[s, a2] + k * abs(pe2)
        assoc[s, a2] = np.clip(assoc[s, a2], 0.0, 1.0)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'phi_anx', 'lapse0', 'g_lapse']"
iter6_run0_participant15.json,cognitive_model1,565.9460145130412,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""MB-MF mixture with anxiety-shifted planning weight and uncertainty bonus.
    
    Idea:
    - Stage-2 values learned with TD(0).
    - Stage-1 uses a mixture of model-based (MB) values and model-free (MF) values.
    - An uncertainty-driven exploration bonus (UCB-like) is added to Stage-2 values when planning,
      scaled upward with anxiety.
    - Anxiety also shifts the balance between MB and MF control.

    Parameters (all used; total=5):
    - lr0: [0,1] Base learning rate for Stage-2 TD updates.
    - beta: [0,10] Inverse temperature applied at both stages.
    - omega0: [0,1] Baseline MB weight at Stage-1 (mixing with MF).
    - chi: [0,1] Strength of anxiety-driven shift of MB weight: omega = clip(omega0 + chi*(stai - 0.5)).
    - xi_e: [0,1] Base weight of uncertainty bonus; effective weight scales with anxiety (xi_eff = xi_e * stai).

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (spaceships).
    - state: array of ints in {0,1}, second-stage states (planets).
    - action_2: array of ints in {0,1}, second-stage choices (aliens).
    - reward: array of floats in [0,1], obtained coins.
    - stai: array-like of length 1 with participant's anxiety score in [0,1].
    - model_parameters: iterable [lr0, beta, omega0, chi, xi_e].

    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    lr0, beta, omega0, chi, xi_e = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common=0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value tables
    q2 = 0.5 * np.ones((2, 2))     # Stage-2 MF values
    q1_mf = np.zeros(2)            # Stage-1 MF values (initialized at 0)

    # Anxiety-modulated weights
    omega = omega0 + chi * (stai - 0.5)  # shift MB weight towards higher MB if stai>0.5 (if chi>0)
    omega = min(1.0, max(0.0, omega))
    xi_eff = xi_e * stai                 # more uncertainty-driven exploration under anxiety

    prev_q2_for_update = None  # optional helper (not strictly needed but clarifies ordering)

    for t in range(n_trials):
        s = state[t]

        # Compute uncertainty bonus per second-stage state-action as: u = 1 - 2*|q - 0.5|
        # Larger when values are closer to 0.5 (uncertain); in [0,1].
        u_bonus = 1.0 - 2.0 * np.abs(q2 - 0.5)
        u_bonus = np.maximum(0.0, u_bonus)  # guard numerical

        # For planning, add bonus to q2
        q2_plan = q2 + xi_eff * u_bonus

        # Model-based Stage-1 action values: expected max over second stage
        mb_q1 = transition_matrix @ np.max(q2_plan, axis=1)

        # Mixture with Stage-1 model-free values
        q1 = omega * mb_q1 + (1.0 - omega) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited state
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes
        r = reward[t]

        # TD update Stage-2
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr0 * pe2

        # Stage-1 MF update toward the realized stage-2 value (semi-gradient)
        # Use the chosen second-stage action's current value (after learning) as target
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += lr0 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr0', 'beta', 'omega0', 'chi', 'xi_e']"
iter6_run0_participant15.json,cognitive_model2,571.2434608334141,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Structure-learning MB planner with anxiety-modulated transition learning and stage-specific temperature.
    
    Idea:
    - Learns the transition structure online from data (Dirichlet-like exponential recency via eta_t).
    - Plans at Stage-1 using the learned transition matrix and current Stage-2 values.
    - Stage-2 values are learned via TD(0).
    - Anxiety reduces Stage-1 decisiveness (lower beta1) and increases Stage-2 decisiveness (higher beta2),
      and slows transition learning.

    Parameters (all used; total=5):
    - eta_r: [0,1] Learning rate for Stage-2 TD updates.
    - beta1: [0,10] Base inverse temperature at Stage-1.
    - beta2: [0,10] Base inverse temperature at Stage-2.
    - eta_t: [0,1] Base learning rate for updating the transition matrix from experienced transitions.
    - kappa_s: [0,1] Strength of anxiety modulation of temperatures and transition learning:
        beta1_eff = beta1 * (1 - kappa_s*stai)
        beta2_eff = beta2 * (1 + kappa_s*stai), clipped to [0,10]
        eta_t_eff = eta_t * (1 - kappa_s*stai)

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (spaceships).
    - state: array of ints in {0,1}, second-stage states (planets).
    - action_2: array of ints in {0,1}, second-stage choices (aliens).
    - reward: array of floats in [0,1], obtained coins.
    - stai: array-like of length 1 with participant's anxiety score in [0,1].
    - model_parameters: iterable [eta_r, beta1, beta2, eta_t, kappa_s].

    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    eta_r, beta1, beta2, eta_t, kappa_s = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix with weak prior toward common transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))

    # Anxiety modulations
    beta1_eff = beta1 * (1.0 - kappa_s * stai)
    beta2_eff = beta2 * (1.0 + kappa_s * stai)
    beta1_eff = min(10.0, max(0.0, beta1_eff))
    beta2_eff = min(10.0, max(0.0, beta2_eff))
    eta_t_eff = eta_t * (1.0 - kappa_s * stai)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 policy via learned transitions
        mb_q1 = T @ np.max(q2, axis=1)
        z1 = beta1_eff * (mb_q1 - np.max(mb_q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in realized state
        z2 = beta2_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 TD learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta_r * pe2

        # Transition learning: Update the row of T corresponding to chosen a1 toward observed state s
        # One-hot target for observed transition from a1 to s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1.0 - eta_t_eff) * T[a1, :] + eta_t_eff * target
        # Ensure row normalization and numerical safety
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta_r', 'beta1', 'beta2', 'eta_t', 'kappa_s']"
iter6_run0_participant15.json,cognitive_model3,557.6625459771788,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive MB-MF planner with anxiety-amplified risk aversion and perseveration.
    
    Idea:
    - Stage-2 values learned via TD(0) with learning rate mu.
    - Stage-1 values combine MB planning and MF (cached) values.
    - Risk sensitivity implemented by penalizing high-variance outcomes: q' = q - rho_eff * q*(1-q).
      Since rewards are Bernoulli-like, q*(1-q) approximates outcome variance.
    - Anxiety increases risk sensitivity and perseveration bias.

    Parameters (all used; total=5):
    - mu: [0,1] Learning rate for Stage-2 TD and Stage-1 MF updates.
    - beta: [0,10] Inverse temperature for both stages.
    - rho_risk: [0,1] Baseline risk penalty weight; effective rho_eff = rho_risk * stai.
    - phi_pers: [0,1] Baseline perseveration strength; effective stick = phi_pers * stai.
    - w_plan: [0,1] Baseline MB weight in Stage-1 mixture; effective weight shifts with anxiety:
        w_eff = clip( w_plan*(1 - stai) + (1 - w_plan)*stai ), biasing away from MB under low anxiety if w_plan<0.5.

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices.
    - state: array of ints in {0,1}, second-stage states.
    - action_2: array of ints in {0,1}, second-stage choices.
    - reward: array of floats in [0,1].
    - stai: array-like of length 1 with participant's anxiety score in [0,1].
    - model_parameters: iterable [mu, beta, rho_risk, phi_pers, w_plan].

    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    mu, beta, rho_risk, phi_pers, w_plan = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))  # Stage-2 MF values
    q1_mf = np.zeros(2)         # Stage-1 MF values

    # Effective parameters with anxiety
    rho_eff = rho_risk * stai
    stick_eff = phi_pers * stai
    w_eff = w_plan * (1.0 - stai) + (1.0 - w_plan) * stai
    w_eff = min(1.0, max(0.0, w_eff))

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]

        # Risk-sensitive transform of second-stage values for planning: penalize variance q*(1 - q)
        variance_proxy = q2 * (1.0 - q2)
        q2_risk_adj = q2 - rho_eff * variance_proxy

        # MB planning uses adjusted values
        mb_q1 = transition_matrix @ np.max(q2_risk_adj, axis=1)

        # Mixture with Stage-1 MF values
        base_q1 = w_eff * mb_q1 + (1.0 - w_eff) * q1_mf

        # Add perseveration bias toward previous Stage-1 choice
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        z1 = beta * (base_q1 + bias - np.max(base_q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (no explicit risk penalty at choice; penalty is expressed via planning only)
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += mu * pe2

        # Stage-1 MF update toward the realized Stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += mu * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['mu', 'beta', 'rho_risk', 'phi_pers', 'w_plan']"
iter6_run0_participant16.json,cognitive_model1,475.9524316554572,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated eligibility, perseveration, and transition-outcome bias.

    This model combines:
      - Model-free updates at stage 2 with TD learning.
      - An eligibility trace from stage 2 to stage 1 MF values, scaled up by anxiety.
      - A model-based (MB) planner using the known transition matrix.
      - Perseveration (stickiness) at stage 1 that increases with anxiety.
      - A transition-outcome bias that promotes stay after common+reward or rare+no-reward,
        and switch after common+no-reward or rare+reward; its strength increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1 within the reached state).
    reward : array-like of float in [0,1]
        Received rewards.
    stai : array-like with a single float in [0,1]
        Anxiety score. Higher values increase eligibility trace, perseveration, and transition-outcome bias,
        and reduce reliance on model-based planning.
    model_parameters : iterable of 5 floats
        - eta in [0,1]: learning rate for MF action values (stage 2) and eligibility at stage 1.
        - b1 in [0,10]: inverse temperature at stage 1.
        - b2 in [0,10]: inverse temperature at stage 2.
        - pi_base in [0,1]: baseline perseveration strength at stage 1.
        - chi_base in [0,1]: baseline weight for transition-outcome interaction (stay/switch bias).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    eta, b1, b2, pi_base, chi_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],  # A -> (X,Y)
                  [0.3, 0.7]]) # U -> (X,Y)

    # Action values
    q2 = np.zeros((2, 2))       # stage-2 MF values per state and action
    q1_mf = np.zeros(2)         # stage-1 MF cached values

    # Anxiety-dependent elements
    lam = np.clip(0.4 + 0.5 * stai, 0.0, 1.0)                 # eligibility trace
    pi = np.clip(pi_base * (1.0 + 0.8 * stai), 0.0, 5.0)      # perseveration bias added to previous action logit
    chi = np.clip(chi_base * (0.5 + 0.5 * stai), 0.0, 5.0)    # transition-outcome interaction weight
    w_mb = np.clip(0.7 - 0.4 * stai, 0.0, 1.0)                # anxiety reduces MB reliance

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_sigma = 0.0  # transition-outcome signal from previous trial

    for t in range(n_trials):
        # Compute MB values from current q2
        max_q2 = np.max(q2, axis=1)         # value per state
        q1_mb = T @ max_q2                  # plan with transition structure

        # Hybrid Q at stage 1
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 logits with biases
        logits1 = b1 * q1
        # Perseveration
        if prev_a1 is not None:
            logits1[prev_a1] += pi
            # Transition-outcome interaction: bias to repeat vs switch
            # sigma from previous trial: +1 promotes repetition, -1 promotes switching
            logits1[prev_a1] += chi * prev_sigma

        # Softmax for stage 1
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        logits2 = b2 * q2[s]
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # TD update at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += eta * delta2

        # Eligibility trace to stage 1 MF
        q1_mf[a1] += eta * lam * delta2

        # Update previous signals for next trial biases
        # Compute transition commonality for current trial
        common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Transition-outcome interaction signal:
        # +1 for (common & reward) or (rare & no reward)
        # -1 for (common & no reward) or (rare & reward)
        sigma = 1 if (common and r > 0.5) or ((1 - common) and r <= 0.5) else -1
        prev_sigma = float(sigma)
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['eta', 'b1', 'b2', 'pi_base', 'chi_base']"
iter6_run0_participant18.json,cognitive_model1,414.9148197676101,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Confidence-weighted MB/MF arbitration with anxiety-driven forgetting and lapses.

    Core idea:
    - Stage-1 action values are a trial-by-trial mixture of model-based (MB) and model-free (MF) estimates.
      The mixing weight w_t is computed from the relative confidence of the MB and MF systems.
    - Confidence is estimated online as the inverse of exponentially averaged squared prediction errors.
    - Higher anxiety increases both lapse probability and forgetting of learned values, reducing stability.
    - Stage-2 uses MF values only.
    - MF updates use TD(0) at stage-2 and an eligibility trace to stage-1.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature for softmax in [0,10]
    - k_conf: confidence-to-weight sensitivity in [0,1]; scales how strongly relative confidence shifts w_t
    - eta_forget: global forgetting rate in [0,1]; stronger forgetting with higher anxiety
    - z_lapse: lapse scaling in [0,1]; effective lapse epsilon = z_lapse * stai

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet (0 or 1)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; participant's anxiety score
    - model_parameters: tuple/list (alpha, beta, k_conf, eta_forget, z_lapse)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, k_conf, eta_forget, z_lapse = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (spaceship A=0 commonly -> X=0; U=1 commonly -> Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    Q1_mf = np.zeros(2)          # stage-1 MF
    Q2 = np.zeros((2, 2))        # stage-2 MF per planet/state

    # Confidence trackers: exponentially-weighted squared prediction error (lower is higher confidence)
    v_mf = 1e-2                  # initial MF PE^2 average (scalar, for simplicity)
    v_mb = 1e-2                  # initial MB PE^2 average (based on reward surprise downstream)

    # Effective parameters modulated by anxiety
    eps_lapse = np.clip(z_lapse * st, 0.0, 1.0)
    # Anxiety increases forgetting: higher st -> larger effective forgetting
    forget_eff = np.clip(eta_forget * (0.5 + 0.5 * st), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])

        # Model-based stage-1 values from current stage-2 MF values
        max_Q2 = np.max(Q2, axis=1)        # best alien per planet
        Q1_mb = T @ max_Q2                 # expected value via transitions

        # Compute arbitration weight from relative confidence
        # Confidence ~ 1 / (running PE^2); map difference to [0,1] via sigmoid-like transform
        conf_mf = 1.0 / (v_mf + 1e-6)
        conf_mb = 1.0 / (v_mb + 1e-6)
        rel = conf_mb - conf_mf
        # Sensitivity scaled by k_conf and reduced by anxiety (more anxiety -> flatter arbitration)
        sens = 5.0 * k_conf * (1.0 - 0.5 * st)  # factor 5 sharpens but stays within [0,1] scaling intent
        w_mb = 1.0 / (1.0 + np.exp(-sens * rel))  # in [0,1]

        # Stage-1 policy
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        a1 = int(action_1[t])
        pref1 = beta * (Q1 - np.max(Q1))
        soft1 = np.exp(pref1) / (np.sum(np.exp(pref1)) + eps)
        probs1 = (1.0 - eps_lapse) * soft1 + eps_lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (MF only)
        a2 = int(action_2[t])
        pref2 = beta * (Q2[s] - np.max(Q2[s]))
        soft2 = np.exp(pref2) / (np.sum(np.exp(pref2)) + eps)
        probs2 = (1.0 - eps_lapse) * soft2 + eps_lapse * 0.5
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD updates
        # Stage-2 TD error and update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF bootstrapping via eligibility-like update
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Update confidence trackers with squared TD magnitudes
        # MF confidence from stage-1 MF TD error
        v_mf = (1.0 - eta_forget) * v_mf + eta_forget * (delta1 ** 2 + delta2 ** 2) * 0.5
        # MB confidence from reward surprise at stage-2 (what MB ultimately predicts via max_Q2)
        pred_mb = max_Q2[s]
        v_mb = (1.0 - eta_forget) * v_mb + eta_forget * ((r - pred_mb) ** 2)

        # Anxiety-driven forgetting applied after learning
        Q1_mf *= (1.0 - forget_eff)
        Q2 *= (1.0 - forget_eff)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'k_conf', 'eta_forget', 'z_lapse']"
iter6_run0_participant18.json,cognitive_model2,526.8777100959271,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk/uncertainty-averse planning with anxiety-weighted stay/switch bias and eligibility.

    Core idea:
    - Stage-2 maintains MF estimates of each alien's mean reward AND an uncertainty (std) estimate.
    - Utility for choice trades off mean vs. uncertainty: U = mean - rho * std, where rho increases with anxiety.
    - Stage-1 is model-based over utilities (not raw means), plus a stay/switch bias:
      after common transitions, bias to stay; after rare transitions, bias to switch, and this bias grows with anxiety.
    - Stage-1 also carries a small MF component via an eligibility-like update.

    Parameters (model_parameters):
    - alpha: learning rate for both mean and uncertainty trackers in [0,1]
    - beta: inverse temperature in [0,10]
    - rho_base: baseline risk/uncertainty aversion in [0,1]; higher => more penalty on std
    - k_bias: strength of stay/switch bias in [0,1]
    - z_elig: eligibility strength for propagating stage-2 learning to stage-1 MF in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet (0 or 1)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; participant's anxiety score
    - model_parameters: tuple/list (alpha, beta, rho_base, k_bias, z_elig)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, rho_base, k_bias, z_elig = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 mean rewards and uncertainty (std) per planet x alien
    Q2_mean = np.zeros((2, 2))
    Q2_m2 = np.zeros((2, 2))  # second moment for variance tracking
    # Stage-1 MF value
    Q1_mf = np.zeros(2)

    # Effective risk aversion increases with anxiety
    rho_eff = np.clip(rho_base + (1.0 - rho_base) * st, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # For stay/switch bias dependent on transition commonness
    last_a1 = None
    last_common = None

    for t in range(n_trials):
        s = int(state[t])

        # Compute std from mean and second moment (ensure non-negativity)
        var = np.maximum(Q2_m2 - Q2_mean ** 2, 0.0)
        std = np.sqrt(var)

        # Utilities penalize uncertainty
        U2 = Q2_mean - rho_eff * std

        # Model-based stage-1 values from utilities
        maxU2 = np.max(U2, axis=1)
        Q1_mb = T @ maxU2

        # Stay/switch bias term on logits at stage-1
        bias = np.zeros(2)
        if last_a1 is not None and last_common is not None:
            # Bias magnitude grows with anxiety; direction: +stay after common, -stay (i.e., switch) after rare
            mag = k_bias * (0.5 + 0.5 * st)
            if last_common:
                bias[last_a1] += mag
            else:
                bias[last_a1] -= mag  # encourages switch

        # Combine MB utility with a small MF baseline via eligibility later; here policy uses MB + bias
        a1 = int(action_1[t])
        pref1 = Q1_mb + bias + 0.1 * Q1_mf  # small MF contribution to stabilize
        pref1 = beta * (pref1 - np.max(pref1))
        probs1 = np.exp(pref1) / (np.sum(np.exp(pref1)) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy uses utilities directly
        a2 = int(action_2[t])
        pref2 = beta * (U2[s] - np.max(U2[s]))
        probs2 = np.exp(pref2) / (np.sum(np.exp(pref2)) + eps)
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Update mean and second moment (EWMA) for chosen alien only
        m_old = Q2_mean[s, a2]
        Q2_mean[s, a2] = (1.0 - alpha) * Q2_mean[s, a2] + alpha * r
        Q2_m2[s, a2] = (1.0 - alpha) * Q2_m2[s, a2] + alpha * (r ** 2)

        # Stage-1 MF update via eligibility using change in expected utility
        # Use the change in U2 for the visited state-action as the teaching signal
        # Compute updated utility component for the visited pair
        var_sa = max(Q2_m2[s, a2] - Q2_mean[s, a2] ** 2, 0.0)
        std_sa = np.sqrt(var_sa)
        u_new = Q2_mean[s, a2] - rho_eff * std_sa
        u_old = m_old - rho_eff * np.sqrt(max((Q2_m2[s, a2] - (m_old ** 2)), 0.0))
        deltaU = u_new - u_old
        Q1_mf[a1] += z_elig * deltaU

        # Track whether this trial's transition was common or rare for use next trial
        # Common if T[a1, s] >= 0.5
        last_common = (T[a1, s] >= 0.5)
        last_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'rho_base', 'k_bias', 'z_elig']"
iter6_run0_participant18.json,cognitive_model3,552.910403617116,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned transition model with anxiety-dependent transition learning and reduced exploration.

    Core idea:
    - The agent learns the transition matrix T online via a simple delta rule.
      Transition learning rate increases with anxiety, reflecting heightened sensitivity to surprising moves.
    - Stage-2 values are MF, but include an exploration bonus proportional to state-action uncertainty;
      anxiety reduces the bonus (more anxious => less exploration).
    - Stage-1 uses an MB/MF mixture with a baseline weight w_init, then adapts weight toward MB
      when observed transitions are predictable; this adaptation is attenuated by anxiety.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - lrT: base transition learning rate in [0,1]
    - k_bonus: exploration-bonus coefficient in [0,1]
    - w_init: initial MB weight in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet (0 or 1)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; participant's anxiety score
    - model_parameters: tuple/list (alpha, beta, lrT, k_bonus, w_init)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, lrT, k_bonus, w_init = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix near the known structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 MF values and uncertainty tracker (counts-based)
    Q2 = np.zeros((2, 2))
    N2 = np.zeros((2, 2))  # visit counts to modulate uncertainty

    # Stage-1 MF values
    Q1_mf = np.zeros(2)

    # Effective transition learning increases with anxiety
    lrT_eff = np.clip(lrT * (0.5 + 0.5 * st), 0.0, 1.0)
    # Effective exploration bonus decreases with anxiety
    bonus_eff = k_bonus * (1.0 - st)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # Adaptive MB weight
    w_mb = np.clip(w_init, 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])

        # Compute exploration bonus for current state's aliens based on counts
        # Use 1/sqrt(N) as an uncertainty proxy (0 if never visited -> high bonus)
        unc = np.zeros(2)
        for a in range(2):
            unc[a] = 1.0 / np.sqrt(max(N2[s, a], 1.0))

        # Stage-2 policy values include bonus
        V2_pref = Q2[s] + bonus_eff * unc

        # Stage-1 MB values from current learned transitions and stage-2 preferences (max over aliens)
        max_V2 = np.max(Q2 + bonus_eff * np.where(N2 > 0, 1.0 / np.sqrt(np.maximum(N2, 1.0)), 1.0), axis=1)
        Q1_mb = T @ max_V2

        # Combine MB and MF for stage-1
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 choice probability
        a1 = int(action_1[t])
        pref1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(pref1) / (np.sum(np.exp(pref1)) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probability
        a2 = int(action_2[t])
        pref2 = beta * (V2_pref - np.max(V2_pref))
        probs2 = np.exp(pref2) / (np.sum(np.exp(pref2)) + eps)
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Reward learning at stage-2 (MF)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2
        N2[s, a2] += 1.0

        # Back up to stage-1 MF (eligibility via observed value)
        target1 = Q2[s, a2]
        Q1_mf[a1] += alpha * (target1 - Q1_mf[a1])

        # Learn transition model T[a1] toward observed state s
        # Update row a1 toward one-hot of observed state with lrT_eff
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] = (1.0 - lrT_eff) * T[a1, sp] + lrT_eff * target
        # Ensure row remains normalized
        T[a1] /= np.sum(T[a1]) + eps

        # Adapt MB weight based on predictability of this transition; anxiety dampens adaptation
        # If the observed state had high predicted probability, increase w_mb; else decrease.
        pred_p = T[a1, s]
        shift = (pred_p - 0.5) * (1.0 - st) * 0.5  # capped small change, attenuated by anxiety
        w_mb = np.clip(w_mb + shift, 0.0, 1.0)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'lrT', 'k_bonus', 'w_init']"
iter6_run0_participant21.json,cognitive_model1,470.64785732599273,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Rare-transition-gated hybrid control with valence-asymmetric learning modulated by anxiety.

    Overview:
    - Stage-2 Q-values are learned with separate learning rates for positive vs. negative prediction errors.
    - Stage-1 choice uses a hybrid of model-based and model-free values.
    - The model-based weight is dynamically gated by whether the previous trial produced a rare vs. common transition,
      and this gating is amplified or dampened by the participant's anxiety.
    - Intuition: more anxious participants rely less on rare-transition-driven re-planning (reduced gating effect).

    Parameters (bounds):
    - model_parameters[0] = alpha_pos in [0,1]: learning rate for positive stage-2 prediction errors (r - Q > 0)
    - model_parameters[1] = alpha_neg in [0,1]: learning rate for negative stage-2 prediction errors (r - Q < 0)
    - model_parameters[2] = beta in [0,10]: inverse temperature for softmax at both stages
    - model_parameters[3] = gate0 in [0,1]: baseline model-based weight at stage 1
    - model_parameters[4] = anx_gate in [0,1]: anxiety sensitivity of rare-transition gating

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0 or 1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of the 5 parameters defined above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha_pos, alpha_neg, beta, gate0, anx_gate = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known task transition structure: rows are actions (0=A,1=U), columns are states (0=X,1=Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Model-free and model-based components
    q1_mf = np.zeros(2)        # stage-1 model-free Q-values
    q2 = np.zeros((2, 2))      # stage-2 Q-values for each state-action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track whether previous transition was rare relative to the chosen action's common destination
    prev_is_rare = 0.0  # initialize as common for the first trial

    for t in range(n_trials):
        # Compute model-based stage-1 values from current stage-2 values
        max_q2 = np.max(q2, axis=1)            # best available at each state
        q1_mb = transition_matrix @ max_q2     # expected value of each first-stage action

        # Anxiety-gated hybridization: when previous trial was rare, the gating term activates.
        # Effective MB weight for this trial:
        # gate_effect = (prev_is_rare - 0.5) ranges in {-0.5, 0.5}; amplify rare, attenuate common.
        # Anxiety reduces amplification: larger stai -> smaller adjustment around gate0.
        gate_adjust = (prev_is_rare - 0.5) * anx_gate * (1.0 - stai_val)
        omega_t = np.clip(gate0 + gate_adjust, 0.0, 1.0)

        q1_net = omega_t * q1_mb + (1.0 - omega_t) * q1_mf

        # Stage-1 policy
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (no stickiness here; purely value-based)
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 learning with valence-asymmetric learning rates
        delta2 = r - q2[s, a2]
        lr2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * delta2

        # Stage-1 model-free update by bootstrapping from updated stage-2 value
        # Use the same valence-dependent rate but based on the sign of the effective TD at stage-1
        delta1 = q2[s, a2] - q1_mf[a1]
        lr1 = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1_mf[a1] += lr1 * delta1

        # Determine whether the current transition was rare to gate the next trial
        # A transition is ""common"" if s equals argmax transition prob for chosen a1
        common_state = 0 if transition_matrix[a1, 0] >= transition_matrix[a1, 1] else 1
        prev_is_rare = 1.0 if s != common_state else 0.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'gate0', 'anx_gate']"
iter6_run0_participant21.json,cognitive_model2,532.5827598984926,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-adaptive learning with anxiety-amplified volatility and softmax choice.

    Overview:
    - Maintains a latent volatility signal v_t that tracks recent unsigned prediction errors.
    - Learning rate adapts to volatility: alpha_t = clip(alpha0 + vol_gain * (v_t + anx_vol * stai), 0, 1).
      Thus, higher anxiety increases perceived volatility and speeds learning.
    - Both stages use the same adaptive learning rate at each trial for simplicity.
    - Stage-1 choice is model-based via transition structure combined with learned stage-2 values only
      (no explicit model-free first-stage cache).

    Parameters (bounds):
    - model_parameters[0] = alpha0 in [0,1]: baseline learning rate
    - model_parameters[1] = beta in [0,10]: inverse temperature for softmax at both stages
    - model_parameters[2] = vol_gain in [0,1]: scaling from volatility to learning rate
    - model_parameters[3] = anx_vol in [0,1]: anxiety contribution to perceived volatility
    - model_parameters[4] = kappa in [0,1]: volatility update rate (how fast v_t tracks unsigned errors)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0 or 1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of the 5 parameters defined above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha0, beta, vol_gain, anx_vol, kappa = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # stage-2 values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    v = 0.0  # latent volatility proxy

    for t in range(n_trials):
        # Effective trial-wise learning rate
        alpha_t = alpha0 + vol_gain * (v + anx_vol * stai_val)
        alpha_t = 0.0 if alpha_t < 0.0 else (1.0 if alpha_t > 1.0 else alpha_t)

        # Stage-1 model-based values from current Q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 learning with adaptive learning rate
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * delta2

        # Update volatility proxy v_t from unsigned prediction error, with leak (1 - kappa)
        v = (1.0 - kappa) * v + kappa * abs(delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'vol_gain', 'anx_vol', 'kappa']"
iter6_run0_participant21.json,cognitive_model3,515.8953323438329,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive utility and novelty-seeking bonus with anxiety modulation.

    Overview:
    - Stage-2 values learn from a transformed utility rather than raw reward:
      u(1) = 1, u(0) = -lambda_eff * 0.5, capturing stronger aversion to omissions when lambda_eff > 1.
    - lambda_eff is driven by anxiety: lambda_eff = 0.5 + 1.5 * clip(lambda0 + anx_risk * stai, 0, 1),
      so higher anxiety increases loss sensitivity.
    - An intrinsic novelty bonus encourages exploration at stage 2 and propagates to stage 1 via planning.
      Bonus decays with visit count and is reduced by anxiety.
    - Policies at both stages use softmax over augmented values.

    Parameters (bounds):
    - model_parameters[0] = alpha in [0,1]: learning rate for stage-2 values
    - model_parameters[1] = beta in [0,10]: inverse temperature for softmax at both stages
    - model_parameters[2] = lambda0 in [0,1]: baseline (bounded) loss-sensitivity seed
    - model_parameters[3] = anx_risk in [0,1]: anxiety weight on loss sensitivity
    - model_parameters[4] = bonus_scale in [0,1]: scale of the novelty bonus

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0 or 1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of the 5 parameters defined above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    alpha, beta, lambda0, anx_risk, bonus_scale = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # learned values (expected utility)
    visits = np.zeros((2, 2))  # visitation counts for novelty bonus

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss sensitivity mapped to [0.5, 2.0]
    lam_eff = np.clip(lambda0 + anx_risk * stai_val, 0.0, 1.0)
    lam_eff = 0.5 + 1.5 * lam_eff

    # Anxiety reduces novelty drive
    bonus_gain = bonus_scale * (1.0 - stai_val)

    for t in range(n_trials):
        # Compute novelty bonuses for current trial
        # b[s,a] = bonus_gain / sqrt(1 + visits[s,a])
        b = np.zeros((2, 2))
        for s_idx in (0, 1):
            for a_idx in (0, 1):
                b[s_idx, a_idx] = bonus_gain / np.sqrt(1.0 + visits[s_idx, a_idx])

        # Stage-1 model-based planning with bonuses propagated via max over stage-2
        max_q2_b = np.max(q2 + b, axis=1)
        q1_mb = transition_matrix @ max_q2_b

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with novelty bonus
        s = state[t]
        q2_net = q2[s] + b[s]
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Risk-sensitive utility transform
        u = 1.0 if r >= 1.0 else (-lam_eff * 0.5)

        # Stage-2 learning on utility
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update visitation counts after experiencing the action
        visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'lambda0', 'anx_risk', 'bonus_scale']"
iter6_run0_participant29.json,cognitive_model2,299.86351579814715,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with uncertainty aversion and an anxiety-modulated choice kernel.
    The agent dislikes uncertain second-stage options: both action selection and learning
    penalize options with higher reward uncertainty (estimated from current Q2 via p*(1-p)).
    Anxiety increases uncertainty aversion. Additionally, a simple choice kernel captures
    repetition tendencies at both stages, with strength growing slightly with anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within the reached state.
    - reward: array-like of floats in [0,1]. Coins received on each trial.
    - stai: array-like with one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for MF values
        beta: [0,10] â inverse temperature for both stages
        kernel_lr: [0,1] â learning/decay rate of the choice kernels
        risk_penalty: [0,1] â base penalty weight for uncertainty
        anx_risk_gain: [0,1] â how strongly anxiety amplifies uncertainty aversion and kernel strength

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    alpha, beta, kernel_lr, risk_penalty, anx_risk_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels (stage 1, and per-state stage 2)
    k1 = np.zeros(2)
    k2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective strengths
    lam = risk_penalty * (1.0 + anx_risk_gain * stai_val)
    kernel_strength = kernel_lr * (1.0 + 0.5 * anx_risk_gain * stai_val)

    eps = 1e-12

    for t in range(n_trials):
        # Uncertainty estimate from current Q2: var ~ p*(1-p), use sqrt for scaling
        var2 = q2 * (1.0 - q2)
        unc2 = np.sqrt(np.clip(var2, 0.0, 0.25))  # in [0, 0.5]

        # Stage 1 logits: MF Q1 plus choice kernel bias
        # Kernel contribution: map kernel in [0,1] to [-1,1] via (2k - 1)
        logits1 = q1 + kernel_strength * (2.0 * k1 - 1.0)
        logits1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 logits: MF Q2 penalized by uncertainty + choice kernel
        s = state[t]
        logits2 = (q2[s] - lam * unc2[s]) + kernel_strength * (2.0 * k2[s] - 1.0)
        logits2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Stage 2: subjective reward penalizes uncertainty of the chosen option
        var_chosen = q2[s, a2] * (1.0 - q2[s, a2])
        unc_chosen = np.sqrt(max(0.0, min(0.25, var_chosen)))
        r_subj = r - lam * unc_chosen
        pe2 = r_subj - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1: bootstrap toward updated second-stage value (no explicit MB)
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Update choice kernels: decay and reinforce chosen action
        k1 = (1.0 - kernel_lr) * k1
        k1[a1] += kernel_lr

        k2[s] = (1.0 - kernel_lr) * k2[s]
        k2[s, a2] += kernel_lr

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'kernel_lr', 'risk_penalty', 'anx_risk_gain']"
iter6_run0_participant29.json,cognitive_model3,491.7678854465648,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-boosted transition updating and Q2 decay.
    The agent learns its own first-stage transition model T(a->state) and uses it for model-based
    planning at stage 1 (MB), while also maintaining model-free values (MF). Stage 1 choice uses a
    dynamic hybrid: Q1 = 0.5*(MF + MB). Second-stage values slowly decay toward 0.5 to capture
    non-stationarity, while anxiety accelerates transition learning.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within the reached state.
    - reward: array-like of floats in [0,1]. Coins received on each trial.
    - stai: array-like with one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] â learning rate for MF values
        beta: [0,10] â inverse temperature for both stages
        decay2: [0,1] â per-trial decay of Q2 toward 0.5 (captures drifting rewards)
        trans_learn: [0,1] â base learning rate for transition probabilities
        anx_trans_gain: [0,1] â how strongly anxiety increases transition learning

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """"""
    alpha, beta, decay2, trans_learn, anx_trans_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Initialize learned transition model T[a, s'], start neutral
    T = np.full((2, 2), 0.5)  # rows sum to 1 with updates

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Compute MB Q at stage 1 from learned transitions and current stage-2 values
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2

        # Hybrid: simple average to keep params <=5
        q1 = 0.5 * (q1_mf + q1_mb)

        # Stage 1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Transition learning: update only the row for the chosen action
        tlr = trans_learn * (1.0 + anx_trans_gain * stai_val)
        # Move probability mass toward the observed state
        T[a1] = (1.0 - tlr) * T[a1]
        T[a1, s] += tlr
        # Normalize defensively (should remain normalized)
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

        # Stage 2 decay toward 0.5 to model drifting payoffs
        q2 = (1.0 - decay2) * q2 + decay2 * 0.5

        # Stage 2 MF update with reward
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward observed second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha', 'beta', 'decay2', 'trans_learn', 'anx_trans_gain']"
iter6_run0_participant3.json,cognitive_model1,386.4062966386524,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-gated model-based control with anxiety-amplified arbitration and stickiness.

    Mechanism overview:
    - Stage-2 values Q2(s2, a2) learned with a single learning rate.
    - Stage-1 hybrid action values combine a model-based (MB) projection using a learned
      transition model T(a1->s2) and a model-free (MF) backup from the last reached Q2.
    - A Dirichlet transition posterior is tracked from observed transitions; the entropy of T
      indexes transition uncertainty. Higher uncertainty increases MB reliance, and this
      effect is amplified by anxiety (stai).
    - A single perseveration parameter k1 acts at both stages; its effect is attenuated
      at stage-2 by anxiety (higher anxiety reduces stage-2 stickiness).

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices (A=0, U=1)
    - state:    int array (n_trials,) in {0,1}; reached second-stage planet (X=0, Y=1)
    - action_2: int array (n_trials,) in {0,1}; second-stage alien choice
    - reward:   float array (n_trials,) in [0,1]; coins received
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        rho_v   in [0,1]: value learning rate for Q2 and MF backup to Q1
        beta    in [0,10]: inverse temperature for softmax at both stages
        k1      in [0,1]: perseveration strength (shared); anxiety scales its stage-2 effect
        omega0  in [0,1]: baseline weight on model-based control at stage-1
        xi_unc  in [0,1]: strength of uncertainty-driven boost to MB weight

    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """"""
    rho_v, beta, k1, omega0, xi_unc = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialization
    q2 = np.zeros((2, 2), dtype=float)   # Q2[s2, a2]
    q1_mf = np.zeros(2, dtype=float)     # model-free Q at stage-1
    # Dirichlet posterior over transitions per action: counts[a1, s2]
    trans_counts = np.ones((2, 2), dtype=float)  # symmetric prior -> starts at 0.5/0.5
    eps = 1e-12

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        # Current transition probs from Dirichlet posterior
        T = trans_counts / (np.sum(trans_counts, axis=1, keepdims=True) + eps)

        # Transition uncertainty via mean entropy across actions
        # H(p) = -sum p log p, normalized by log(2) to be in [0,1]
        ent = -np.sum(T * (np.log(T + eps)), axis=1) / np.log(2 + eps)
        unc = 0.5 * (ent[0] + ent[1])  # scalar in [0,1]

        # MB projection
        max_q2 = np.max(q2, axis=1)        # value of each second-stage state
        q1_mb = T @ max_q2                 # MB value for each first-stage action

        # Anxiety-amplified arbitration: more uncertainty -> more MB, amplified by anxiety
        omega_eff = omega0 + xi_unc * unc * (0.5 + 0.5 * s_anx)
        omega_eff = float(np.clip(omega_eff, 0.0, 1.0))

        # Hybrid Q at stage-1
        q1 = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # Stickiness vectors
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Stage-1 policy
        logits1 = beta * q1 + k1 * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (anxiety reduces stickiness at stage-2)
        s2 = int(state[t])
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        k2_eff = k1 * (1.0 - 0.7 * s_anx)  # stronger anxiety -> less stage-2 perseveration
        logits2 = beta * q2[s2] + k2_eff * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Transition posterior update (Dirichlet counts)
        trans_counts[a1, s2] += 1.0  # simple Bayesian counting update

        # Value learning at stage-2
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += rho_v * delta2

        # MF backup to stage-1: learn from reached Q2 and obtained reward
        target1 = q2[s2, a2]  # could include immediate r via delta2 already embedded
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += rho_v * delta1
        # Optional additional TD(1) style boost with the stage-2 TD error
        q1_mf[a1] += rho_v * delta2

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['rho_v', 'beta', 'k1', 'omega0', 'xi_unc']"
iter6_run0_participant3.json,cognitive_model2,503.7897465930695,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-driven arousal model: anxiety scales how transition surprise lowers beta.

    Mechanism overview:
    - Q2 learned with a single learning rate; Q1 uses a hybrid MB+MF value.
    - Transition model T(a1->s2) is learned incrementally.
    - Trial-by-trial transition surprise (absolute deviation between observed s2 and predicted T)
      modulates the next-trial softmax temperature: higher surprise reduces beta (more randomness).
      This arousal-like effect is multiplicatively scaled by anxiety (stai).
    - A stage-2 perseveration term is included.

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}
    - state:    int array (n_trials,) in {0,1}
    - action_2: int array (n_trials,) in {0,1}
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]
    - model_parameters: tuple/list with five params:
        alpha   in [0,1]: learning rate for Q-values
        beta    in [0,10]: baseline inverse temperature (when no surprise)
        mu_T    in [0,1]: learning rate for the transition model T
        phi_s   in [0,1]: strength of surprise-induced beta reduction
        kappa2  in [0,1]: perseveration at stage-2; stage-1 has no stickiness here

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha, beta, mu_T, phi_s, kappa2 = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])
    eps = 1e-12

    # Initialize values and transitions
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # start from nominal task structure

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a2 = -1
    # Surprise from the previous transition; initialize to zero (no arousal initially)
    prev_surprise = 0.0

    for t in range(n_trials):
        # Surprise-adjusted inverse temperature (lower beta -> more random)
        beta_t = beta / (1.0 + phi_s * s_anx * prev_surprise)

        # Model-based projection
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid Q1: 50-50 MB/MF blend (beta_t already captures arousal/surprise)
        q1 = 0.5 * q1_mb + 0.5 * q1_mf

        # Stage-1 policy (no stickiness term here)
        logits1 = beta_t * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration
        s2 = int(state[t])
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta_t * q2[s2] + kappa2 * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Transition learning with delta rule
        onehot_s = np.array([1.0 if i == s2 else 0.0 for i in range(2)])
        T[a1] = (1.0 - mu_T) * T[a1] + mu_T * onehot_s
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Compute surprise for next trial arousal modulation
        # Surprise = 1 - predicted probability of observed state
        prev_surprise = 1.0 - float(T[a1, s2])

        # Q-learning at stage-2
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # MF backup to Q1 using reached second-stage value
        target1 = q2[s2, a2]
        q1_mf[a1] += alpha * (target1 - q1_mf[a1])
        # Optional extra with TD error
        q1_mf[a1] += alpha * delta2

        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'mu_T', 'phi_s', 'kappa2']"
iter6_run0_participant3.json,cognitive_model3,514.8906874751511,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive utility with eligibility trace and anxiety-modulated forgetting.

    Mechanism overview:
    - Subjective utility u(r) = r^gamma with gamma <= 1 capturing risk aversion for probabilities.
      Anxiety increases risk aversion by reducing gamma (stronger concavity).
    - TD learning on Q2 uses utility u(r).
    - An eligibility trace at stage-1 propagates the stage-2 TD error back to Q1 (Î»-like backup).
      Anxiety increases the effective trace length.
    - Global forgetting pulls unchosen Q-values toward a neutral baseline (0.5) each trial;
      anxiety reduces forgetting (more rigid values).

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}
    - state:    int array (n_trials,) in {0,1}
    - action_2: int array (n_trials,) in {0,1}
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]
    - model_parameters: tuple/list with five params:
        eta        in [0,1]: base learning rate for value updates
        beta       in [0,10]: inverse temperature for both stages
        lam_elig   in [0,1]: eligibility-trace strength (backups to Q1)
        psi_risk   in [0,1]: risk-sensitivity strength (larger -> more concavity)
        nu_forget  in [0,1]: forgetting strength toward 0.5 baseline for unchosen actions

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    eta, beta, lam_elig, psi_risk, nu_forget = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])
    eps = 1e-12

    # Initialize values
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    # Fixed transition structure (task-known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective parameters modulated by anxiety
    # Risk exponent gamma in (0,1]; stronger anxiety -> smaller gamma (more concave)
    gamma = 1.0 - 0.7 * psi_risk * s_anx
    gamma = float(np.clip(gamma, 0.05, 1.0))
    # Eligibility increased by anxiety
    lam_eff = float(np.clip(lam_elig * (1.0 + 0.5 * s_anx), 0.0, 1.0))
    # Forgetting reduced by anxiety
    forget_eff = nu_forget * (1.0 - 0.7 * s_anx)

    for t in range(n_trials):
        # Stage-1 MB projection from current Q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MB and MF equally (focus of this model is utility/trace/forgetting)
        q1 = 0.5 * q1_mb + 0.5 * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Subjective utility
        r = float(reward[t])
        u = r ** gamma

        # TD at stage-2 (with utility)
        delta2 = u - q2[s2, a2]
        q2[s2, a2] += eta * delta2

        # Eligibility trace backup to stage-1 MF value
        # Two components: (i) bootstrapped from reached Q2, (ii) direct TD error propagation
        target1 = q2[s2, a2]
        q1_mf[a1] += eta * (target1 - q1_mf[a1])
        q1_mf[a1] += eta * lam_eff * delta2

        # Forgetting toward neutral baseline (0.5) for unchosen actions
        # Stage-2: decay all non-visited state-action pairs this trial
        for s_idx in (0, 1):
            for a_idx in (0, 1):
                if not (s_idx == s2 and a_idx == a2):
                    q2[s_idx, a_idx] += forget_eff * (0.5 - q2[s_idx, a_idx])

        # Stage-1: decay the unchosen action
        unchosen_a1 = 1 - a1
        q1_mf[unchosen_a1] += forget_eff * (0.5 - q1_mf[unchosen_a1])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['eta', 'beta', 'lam_elig', 'psi_risk', 'nu_forget']"
iter6_run0_participant31.json,cognitive_model1,430.533479086948,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-modulated learning rate and second-stage lapse.

    Idea:
    - First-stage policy blends a model-free Q1 with a model-based plan computed from the
      known transition structure (70/30). The mixture weight is xi_mb.
    - Anxiety increases or decreases the learning rate (alpha) via k_anx_alpha.
    - Second-stage choice includes an epsilon-like lapse (lapse2): with probability lapse2,
      choice is random; otherwise it follows softmax.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered state (0/1).
    reward : array-like of float
        Rewards obtained on each trial.
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1]. Higher scores modulate learning rate.
    model_parameters : array-like of floats, length 5
        [alpha, beta, xi_mb, k_anx_alpha, lapse2]
        - alpha in [0,1]: base learning rate for Q updates.
        - beta in [0,10]: inverse temperature for softmax.
        - xi_mb in [0,1]: weight of model-based Q in first-stage decision values.
        - k_anx_alpha in [0,1]: how strongly anxiety modulates learning rate.
                                alpha_eff = alpha * (1 + (stai - 0.5) * 2 * k_anx_alpha).
        - lapse2 in [0,1]: second-stage lapse probability (random choice fraction).

    Returns
    -------
    float
        Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, xi_mb, k_anx_alpha, lapse2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated learning rate
    alpha_eff = alpha * (1.0 + (stai_val - 0.5) * 2.0 * k_anx_alpha)
    if alpha_eff < 0.0:
        alpha_eff = 0.0
    if alpha_eff > 1.0:
        alpha_eff = 1.0

    # Fixed transitions: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])  # rows: A/U; cols: X/Y

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)        # A/U
    q2_mf = np.zeros((2, 2))   # state X/Y x action (0/1)

    for t in range(n_trials):
        # Model-based first-stage values: plan via max over second-stage Q
        max_q2 = np.max(q2_mf, axis=1)  # values of planets X and Y
        q1_mb = transition_matrix @ max_q2  # expected value of choosing A or U

        # Hybrid value
        q1_hybrid = (1.0 - xi_mb) * q1_mf + xi_mb * q1_mb

        # First-stage policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Second-stage policy with lapse
        s = int(state[t])
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        p2_soft = np.exp(logits2)
        p2_soft = p2_soft / (np.sum(p2_soft) + 1e-12)
        p2 = (1.0 - lapse2) * p2_soft + lapse2 * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha_eff * delta2

        # Stage-1 TD update toward second-stage action value (SARSA-like bootstrap)
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_eff * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'xi_mb', 'k_anx_alpha', 'lapse2']"
iter6_run0_participant31.json,cognitive_model2,445.03451986520577,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with risk-sensitive second-stage choice and anxiety-amplified risk aversion, plus second-stage perseveration.

    Idea:
    - Maintain MF Q-values at both stages with a single learning rate.
    - Maintain an exponential estimate of outcome variance at the second stage (per state-action).
      The policy penalizes actions with higher estimated variance (risk).
    - Anxiety increases effective risk-aversion.
    - Include second-stage perseveration (stickiness) that biases repeating the last second-stage choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered state (0/1).
    reward : array-like of float
        Rewards obtained on each trial.
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1]. Higher anxiety => stronger risk penalty.
    model_parameters : array-like of floats, length 5
        [alpha, beta, k_risk0, k_anx_loss, pers2]
        - alpha in [0,1]: learning rate for Q and variance estimates.
        - beta in [0,10]: inverse temperature for softmax.
        - k_risk0 in [0,1]: baseline weight for risk penalty on second-stage values.
        - k_anx_loss in [0,1]: scales how anxiety amplifies risk penalty:
                               k_risk_eff = k_risk0 * (1 + k_anx_loss * stai).
        - pers2 in [0,1]: second-stage perseveration strength added to last chosen action's logit.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, k_risk0, k_anx_loss, pers2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    k_risk_eff = k_risk0 * (1.0 + k_anx_loss * stai_val)
    if k_risk_eff < 0.0:
        k_risk_eff = 0.0
    if k_risk_eff > 1.0:
        k_risk_eff = 1.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)              # MF Q at stage 1
    q2 = np.zeros((2, 2))         # MF Q at stage 2
    v2 = np.zeros((2, 2))         # running variance estimate per state-action

    prev_a2 = [None, None]        # track last action within each second-stage state

    for t in range(n_trials):
        # First-stage softmax (MF only)
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Second-stage policy with risk penalty and perseveration
        s = int(state[t])

        # Perseveration bias for this state
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] = pers2

        # Risk-adjusted logits
        risk_penalty = k_risk_eff * v2[s]  # subtract variance-weighted penalty
        logits2 = beta * (q2[s] - risk_penalty) + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update variance estimate with squared prediction error
        v2[s, a2] += alpha * (delta2 * delta2 - v2[s, a2])

        # Stage-1 TD update towards q2 value (SARSA-like)
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'k_risk0', 'k_anx_loss', 'pers2']"
iter6_run0_participant31.json,cognitive_model3,367.0417902975837,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with anxiety-modulated forgetting and model-based heuristic bias toward common transitions.

    Idea:
    - Use MF Q-learning at both stages with per-trial forgetting/decay of all Q-values.
    - Anxiety increases the effective decay (forgetting), capturing higher volatility beliefs.
    - First-stage decision includes a heuristic bias that favors the spaceship whose common
      destination currently has higher estimated value (based on max second-stage Q for each planet).
      This uses the known transition structure without full planning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered state (0/1).
    reward : array-like of float
        Rewards obtained on each trial.
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1]. Higher anxiety => stronger forgetting.
    model_parameters : array-like of floats, length 5
        [alpha, beta, decay, k_anx_decay, tr_bias]
        - alpha in [0,1]: learning rate.
        - beta in [0,10]: inverse temperature.
        - decay in [0,1]: baseline forgetting rate applied each trial to all Q-values.
        - k_anx_decay in [0,1]: scales how anxiety increases effective decay:
                                decay_eff = min(1, decay + k_anx_decay * stai).
        - tr_bias in [0,1]: strength of heuristic transition bias added to first-stage logits.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, decay, k_anx_decay, tr_bias = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated decay
    decay_eff = decay + k_anx_decay * stai_val
    if decay_eff < 0.0:
        decay_eff = 0.0
    if decay_eff > 1.0:
        decay_eff = 1.0
    keep = 1.0 - decay_eff

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Heuristic transition bias: compare planet values
        v_planet = np.max(q2, axis=1)  # [vX, vY]
        # Map to spaceship bias: A commonly -> X; U commonly -> Y
        # Add +tr_bias * (vX - vY) to A, and - that to U
        bias1 = np.array([tr_bias * (v_planet[0] - v_planet[1]),
                          -tr_bias * (v_planet[0] - v_planet[1])])

        # First-stage policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        s = int(state[t])
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Learning with forgetting
        r = reward[t]

        # Apply decay to all Q-values before update
        q1 *= keep
        q2 *= keep

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 TD update toward second-stage chosen value
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'decay', 'k_anx_decay', 'tr_bias']"
iter6_run0_participant32.json,cognitive_model3,462.7850751468654,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Uncertainty-bonus exploration with anxiety-dampened novelty and common-transition reward bias.

    Core idea:
    - Second-stage values learned with standard TD.
    - Add an Upper-Confidence-Bound (UCB) exploration bonus at stage 2 that decreases with experience.
    - Anxiety diminishes the exploration bonus (more anxious => less novelty seeking).
    - Stage 1 uses model-based values from fixed transitions applied to augmented (value + bonus) second-stage options.
    - Add a first-stage bias to repeat the previous spaceship if the previous rewarded transition was common,
      scaled up by anxiety.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (alien) per trial (0/1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1].
    model_parameters : list or array-like of float
        [alpha, beta, c_ucb, zeta_anx, rho_comm]
        Bounds:
        - alpha: [0,1] learning rate for second-stage Q-values.
        - beta: [0,10] inverse temperature (both stages).
        - c_ucb: [0,1] base strength of the exploration bonus.
        - zeta_anx: [0,1] scaling of how much anxiety suppresses exploration bonus.
        - rho_comm: [0,1] base bias to repeat previous first-stage choice after common rewarded transitions.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, c_ucb, zeta_anx, rho_comm = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # P(X|A), P(Y|A)
                  [0.3, 0.7]]) # P(X|U), P(Y|U)

    # Second-stage Q-values and visit counts
    q2 = np.zeros((2, 2))
    N = np.zeros((2, 2))  # visit counts per state-action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous trial info for common-transition rewarded bias
    prev_a1 = None
    prev_trans_common = False
    prev_reward_pos = False

    # Anxiety-dampened exploration
    c_ucb_eff = c_ucb * (1.0 - zeta_anx * stai)
    c_ucb_eff = max(0.0, c_ucb_eff)

    for t in range(n_trials):

        # Compute UCB bonuses
        bonus = c_ucb_eff * np.sqrt(1.0 / (1.0 + N))

        # Augmented second-stage action values
        aug2 = q2 + bonus

        # Model-based first-stage values using augmented second-stage values
        max_aug2 = np.max(aug2, axis=1)
        q1_mb = T @ max_aug2

        # Add common-rewarded repetition bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_trans_common and prev_reward_pos:
            # Favor repeating the previous first-stage choice; stronger with anxiety
            rho_eff = rho_comm * (0.5 + 0.5 * stai)
            bias1[prev_a1] += rho_eff

        # Stage 1 policy
        vals1 = q1_mb + bias1
        v1 = vals1 - np.max(vals1)
        probs1 = np.exp(beta * v1 - np.max(beta * v1))
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy at reached state
        s = int(state[t])
        vals2 = aug2[s]
        v2 = vals2 - np.max(vals2)
        probs2 = np.exp(beta * v2 - np.max(beta * v2))
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward and update second-stage values and counts
        r = reward[t]
        N[s, a2] += 1.0
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update prev-trial markers for the next trial
        # Determine whether the observed transition was common
        # A->X (s=0) and U->Y (s=1) are common; the other pairings are rare.
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        prev_trans_common = is_common
        prev_reward_pos = r > 0.0
        prev_a1 = a1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik","['alpha', 'beta', 'c_ucb', 'zeta_anx', 'rho_comm']"
iter6_run0_participant35.json,cognitive_model1,412.90974692562963,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Kalman-TD with anxiety-modulated volatility, uncertainty-weighted learning, and eligibility propagation.

    Summary
    - Second-stage values are learned via a Kalman filter per state-action (uncertainty-adaptive learning).
    - Anxiety increases assumed environmental volatility and observation noise, raising Kalman gain when outcomes vary.
    - First-stage values are updated via an eligibility-like propagation of second-stage PEs, with a small model-based blend.
    - Model-based influence is reduced with anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1) on the reached planet
    - reward: array-like of floats in [0,1], coin outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters (all in [0,1] except beta in [0,10]):
        beta          (float in [0,10]): inverse temperature for both stages
        w_plan_base   (float in [0,1]) : baseline model-based weight at stage 1
        eta_elig      (float in [0,1]) : eligibility strength to propagate PE2 to stage-1 MF value
        sigma0        (float in [0,1]) : baseline observation noise scale for Kalman update
        phi_vol       (float in [0,1]) : baseline process noise (volatility) for Kalman update

    Anxiety usage
    - Effective process noise q_eff increases with anxiety: q_eff = q0 + kq * phi_vol * (0.5 + 0.5*stai)
    - Effective observation noise r_eff increases with anxiety: r_eff = r0 + kr * sigma0 * (0.5 + 0.5*stai)
    - Model-based weight decreases with anxiety: w_mb = clip(w_plan_base * (1 - 0.4*stai), 0, 1)

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    beta, w_plan_base, eta_elig, sigma0, phi_vol = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-1 model-free values and stage-2 Kalman means/variances
    q1_mf = np.zeros(2)
    q2_mean = np.zeros((2, 2))
    q2_var = np.full((2, 2), 0.2)  # initial uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated noise terms for Kalman updates
    # Keep within reasonable [~0.01, ~0.5] range to avoid degenerate gains
    q0, kq = 0.01, 0.49
    r0, kr = 0.01, 0.49
    anxiety_scale = 0.5 + 0.5 * stai
    q_eff = q0 + kq * phi_vol * anxiety_scale
    r_eff = r0 + kr * sigma0 * anxiety_scale

    # MB weight reduced by anxiety
    w_mb = w_plan_base * (1.0 - 0.4 * stai)
    w_mb = max(0.0, min(1.0, w_mb))

    for t in range(n_trials):
        # Model-based estimate from stage-2 values
        max_q2 = np.max(q2_mean, axis=1)
        q1_mb = T @ max_q2

        # Decision values for stage 1
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        v1 = q1_dec - np.max(q1_dec)
        pi1 = np.exp(beta * v1)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy at the reached state
        s = int(state[t])
        v2 = q2_mean[s] - np.max(q2_mean[s])
        pi2 = np.exp(beta * v2)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        # Kalman update at stage 2
        r = reward[t]
        pe2 = r - q2_mean[s, a2]
        prior_var = q2_var[s, a2] + q_eff
        K = prior_var / (prior_var + r_eff)  # Kalman gain in (0,1)
        q2_mean[s, a2] += K * pe2
        q2_var[s, a2] = (1.0 - K) * prior_var

        # Eligibility-like propagation to stage-1 MF value
        # Use the same PE2 as a teaching signal for chosen first-stage action
        q1_mf[a1] += eta_elig * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['beta', 'w_plan_base', 'eta_elig', 'sigma0', 'phi_vol']"
iter6_run0_participant35.json,cognitive_model2,445.3344470208739,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning MB/MF blend with anxiety-sensitive rare-transition switching bias.

    Summary
    - Learns second-stage Q-values with a standard learning rate.
    - Learns transition probabilities online (action-specific rows of T) via a simple delta rule.
    - Stage-1 decisions blend MB and MF values using learned transitions.
    - Anxiety reduces MB reliance and increases an adaptive switch-bias after rare transitions.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters (all in [0,1] except beta in [0,10]):
        alpha_s   (float in [0,1]) : learning rate for second-stage values
        beta      (float in [0,10]): inverse temperature for both stages
        eta_T     (float in [0,1]) : learning rate for transitions
        w_mb0     (float in [0,1]) : baseline model-based weight for stage 1
        zeta_rare (float in [0,1]) : magnitude of rare-transition-induced switch bias

    Anxiety usage
    - MB weight decreases with anxiety: w_mb = clip(w_mb0 * (1 - 0.5*stai), 0, 1)
    - Rare-transition switch bias is amplified by anxiety:
        if previous trial was rare, add +b to the action that switches and -b to the action that stays,
        where b = zeta_rare * stai.

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha_s, beta, eta_T, w_mb0, zeta_rare = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix (rows sum to 1; start near canonical)
    T = np.array([[0.65, 0.35],
                  [0.35, 0.65]], dtype=float)

    # Stage-1 model-free values and stage-2 values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    w_mb = w_mb0 * (1.0 - 0.5 * stai)
    w_mb = max(0.0, min(1.0, w_mb))

    prev_a1 = None
    prev_rare = False

    for t in range(n_trials):
        # Model-based from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Rare-transition switch bias based on previous trial
        bias = np.zeros(2)
        if prev_a1 is not None and prev_rare:
            b = zeta_rare * stai
            # Encourage switching away from previous action
            bias[1 - prev_a1] += b
            bias[prev_a1]     -= b

        # Stage-1 decision values
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias
        v1 = q1_dec - np.max(q1_dec)
        pi1 = np.exp(beta * v1)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy
        s = int(state[t])
        v2 = q2[s] - np.max(q2[s])
        pi2 = np.exp(beta * v2)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        r = reward[t]

        # Update second-stage values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_s * pe2

        # Update stage-1 MF by bootstrapping from realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_s * pe1

        # Learn transitions via simple delta rule toward observed state
        # For chosen action, move probability mass toward observed state s
        # Keep row normalized by complementary update
        row = T[a1].copy()
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            row[sp] += eta_T * (target - row[sp])
        # Normalize row to sum to 1 and keep probabilities within [0.01, 0.99] to avoid degeneracy
        row = np.clip(row, 1e-3, 1.0)
        row /= np.sum(row)
        T[a1] = row

        # Determine if current transition was rare using the canonical structure for rarity detection
        # (A->Y or U->X = rare). This is used only for the bias mechanism.
        rare_now = (a1 == 0 and s == 1) or (a1 == 1 and s == 0)
        prev_a1 = a1
        prev_rare = rare_now

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_s', 'beta', 'eta_T', 'w_mb0', 'zeta_rare']"
iter6_run0_participant39.json,cognitive_model1,486.27838665354216,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning hybrid with anxiety-shifted MF reliance and second-stage stickiness.
    
    Mechanism:
    - Learns second-stage action values model-free.
    - Learns first-stage transition probabilities online; uses them to compute
      model-based first-stage values as T @ max(Q2).
    - First-stage policy is a hybrid of model-based values and model-free Q1,
      but anxiety (stai) increases reliance on model-free control.
    - Second-stage policy includes an action stickiness term that is stronger with anxiety.
    - Anxiety also slightly reduces choice precision (lower beta) to capture exploratory tendency.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; e.g., W/S in X or P/H in Y)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha_r:       [0,1] learning rate for second-stage rewards (Q2)
        beta_base:     [0,10] inverse temperature base for both stages
        tau_T_base:    [0,1] transition learning rate (for updating T)
        w_mf_base:     [0,1] baseline weight on model-free values at stage 1
        kappa2_base:   [0,1] stickiness strength at stage 2
    Returns
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha_r, beta_base, tau_T_base, w_mf_base, kappa2_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transition matrix (rows: actions A/U, cols: states X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value tables
    q1_mf = np.zeros(2)           # model-free values for first-stage actions
    q2 = np.zeros((2, 2))         # second-stage action values per state

    # Likelihood accumulators
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective parameters modulated by anxiety
    # Anxiety increases MF reliance at stage 1
    w_mf_eff = np.clip(w_mf_base + stai0 * (1.0 - w_mf_base), 0.0, 1.0)
    # Anxiety reduces precision slightly (more exploration)
    beta = beta_base * (1.0 - 0.25 * stai0)
    # Anxiety increases second-stage stickiness
    kappa2 = kappa2_base * (1.0 + stai0)
    # Anxiety increases vigilance to transitions
    tau_T = np.clip(tau_T_base * (1.0 + 0.5 * stai0), 0.0, 1.0)

    # Second-stage stickiness: last action chosen within each state
    prev_a2 = [None, None]

    eps = 1e-10
    for t in range(n_trials):
        # Model-based Q1 from current transition estimates and max Q2 per state
        max_q2 = np.max(q2, axis=1)              # shape (2,)
        q1_mb = T @ max_q2                       # shape (2,)

        # Hybrid first-stage action values (anxiety shifts weight toward MF)
        q1_hyb = w_mf_eff * q1_mf + (1.0 - w_mf_eff) * q1_mb

        # First-stage choice probabilities
        q1_centered = q1_hyb - np.max(q1_hyb)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice probabilities with stickiness
        s2 = state[t]
        q2_s = q2[s2].copy()
        if prev_a2[s2] is not None:
            stick = np.zeros(2)
            stick[prev_a2[s2]] = 1.0
            q2_s = q2_s + kappa2 * stick

        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update values
        r = reward[t]

        # Update second-stage Q-values (model-free)
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_r * pe2

        # Update first-stage MF toward realized second-stage value
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1  # reuse alpha_r for simplicity; bounded in [0,1]

        # Update transition matrix for the chosen first-stage action based on observed state
        # Ensure row sums to 1
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s2 else 0.0
            T[a1, s_idx] += tau_T * (target - T[a1, s_idx])
        # Normalize for numerical stability
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

        # Update stickiness memory
        prev_a2[s2] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta_base', 'tau_T_base', 'w_mf_base', 'kappa2_base']"
iter6_run0_participant39.json,cognitive_model3,386.82698484545097,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-representation hybrid with directed exploration bonus and anxiety effects.
    
    Mechanism:
    - Second-stage values learned model-free; track uncertainty (running variance of PE)
      per state-action and add an exploration bonus proportional to uncertainty.
    - First-stage values are a hybrid: SR-based model-based values (using fixed transitions)
      combined with MF first-stage values. Anxiety reduces SR weighting and directed exploration.
    - Includes first-stage perseveration (choice kernel).

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        beta:           [0,10] inverse temperature for both stages
        decay_u_base:   [0,1] PE-variance decay for uncertainty tracking at stage 2
        xi_bonus_base:  [0,1] exploration bonus weight on sqrt(uncertainty)
        kappa1_base:    [0,1] perseveration strength at stage 1
        omega_sr_base:  [0,1] baseline weight on SR (model-based) control at stage 1
    Returns
    - Negative log-likelihood of observed choices.
    """"""
    beta, decay_u_base, xi_bonus_base, kappa1_base, omega_sr_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed SR for first-stage actions: expected visitation of second-stage states given choice
    # Equivalent to the known common transition structure
    SR = np.array([[0.7, 0.3],
                   [0.3, 0.7]], dtype=float)

    # Value and uncertainty tables
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    var2 = np.zeros((2, 2))  # running PE variance proxy

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Anxiety reduces SR reliance and exploration bonus; increases perseveration slightly
    omega_sr = np.clip(omega_sr_base * (1.0 - stai0), 0.0, 1.0)
    xi_bonus = xi_bonus_base * (1.0 - stai0)
    kappa1 = kappa1_base * (1.0 + 0.5 * stai0)
    decay_u = np.clip(decay_u_base, 0.0, 1.0)
    beta_eff = beta * (1.0 - 0.2 * stai0)

    prev_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        # SR-based evaluation uses max Q2 per state
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_sr = SR @ max_q2          # shape (2,)
        q1_hyb = omega_sr * q1_sr + (1.0 - omega_sr) * q1_mf

        # Add perseveration bias at stage 1
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] = 1.0
            q1_hyb = q1_hyb + kappa1 * bias

        # First-stage softmax
        q1_c = q1_hyb - np.max(q1_hyb)
        probs_1 = np.exp(beta_eff * q1_c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage softmax with directed exploration bonus
        s2 = state[t]
        bonus = xi_bonus * np.sqrt(np.maximum(var2[s2], 0.0))
        q2_bonus = q2[s2] + bonus
        q2_c = q2_bonus - np.max(q2_bonus)
        probs_2 = np.exp(beta_eff * q2_c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s2, a2]
        # Update Q2
        q2[s2, a2] += decay_u * pe2  # reuse decay_u as a bounded learning rate
        # Update uncertainty proxy (running variance of PEs)
        var2[s2, a2] = (1.0 - decay_u) * var2[s2, a2] + decay_u * (pe2 ** 2)

        # Update Q1 MF toward realized second-stage value (SARSA(0)-like bootstrap)
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += decay_u * pe1  # same bounded step size

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['beta', 'decay_u_base', 'xi_bonus_base', 'kappa1_base', 'omega_sr_base']"
iter6_run0_participant4.json,cognitive_model1,552.2013412186209,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-weighted model-based arbitration with asymmetric learning and transition-sensitive credit.

    Overview
    - Stage-2 values learned via asymmetric RescorlaâWagner (separate alpha for positive/negative PE).
    - Stage-1 choice uses a dynamic arbitration between model-based (MB) and model-free (MF) values.
      The MB weight increases when anxiety is low and decreases when anxiety is high.
    - Transition-sensitive credit assignment to stage-1 MF values:
      learning from rare transitions is down-weighted, especially under higher anxiety.
      This uses the same anxiety sensitivity parameter that controls MB arbitration.
    - No explicit transition input is needed; common vs. rare is inferred from (action_1, state).

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha_pos: [0,1]       Learning rate for positive PE at stage-2.
    - alpha_neg: [0,1]       Learning rate for negative PE at stage-2.
    - beta:      [0,10]      Inverse temperature for softmax choices (both stages).
    - w_base:    [0,1]       Baseline model-based weight for stage-1 arbitration.
    - psi_anx:   [0,1]       Anxiety sensitivity: higher values amplify the effect of anxiety on
                             reducing MB weight and reducing learning from rare transitions.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha_pos, alpha_neg, beta, w_base, psi_anx].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha_pos, alpha_neg, beta, w_base, psi_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    # We'll use it for MB evaluation and to infer common/rare after the fact.
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    # Value functions
    q2 = np.zeros((2, 2)) + 0.5    # stage-2 Q-values per state and action (aliens)
    q1_mf = np.zeros(2) + 0.0      # stage-1 MF cache

    # Likelihood accumulators
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight for MB (higher anxiety => lower MB)
    # w_mb_t = clip(w_base * (1 - psi_anx * stai), 0, 1)
    w_mb = max(0.0, min(1.0, w_base * (1.0 - psi_anx * stai)))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 policy: MB evaluation via transition matrix times max stage-2 Q
        max_q2 = np.max(q2, axis=1)                  # [X_max, Y_max]
        q1_mb = transition_matrix @ max_q2           # value of [A, U] from MB
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb     # arbitration

        # Softmax for stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1]

        # Stage-2 policy for reached state
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = probs2[a2]

        # Stage-2 learning with asymmetric rates
        pe2 = r - q2[s, a2]
        alpha = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha * pe2

        # Transition-sensitive credit to stage-1 MF:
        # common if A->X or U->Y; rare otherwise.
        common = 1 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0
        # Anxiety reduces credit on rare transitions using psi_anx
        rare_weight = 1.0 - psi_anx * stai
        credit = 1.0 if common == 1 else max(0.0, rare_weight)
        # MF update uses stage-2 PE scaled by credit
        q1_mf[a1] += alpha * credit * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['alpha_pos', 'alpha_neg', 'beta', 'w_base', 'psi_anx']"
iter6_run0_participant4.json,cognitive_model2,411.9833191216216,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Control-cost regularization with anxiety-weighted uncertainty and stay bias.

    Overview
    - Stage-2 values learned via standard RescorlaâWagner.
    - A control-cost term increases exploration when uncertainty is high; anxiety amplifies this effect.
      We implement this by reducing effective inverse temperature based on a running uncertainty estimate.
    - Uncertainty is tracked per state as a running average of absolute prediction errors (uses alpha).
    - A simple stay bias (perseveration) at both stages encourages repeating previous choices.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:      [0,1]   Learning rate for Q updates and uncertainty tracking.
    - beta:       [0,10]  Base inverse temperature for softmax.
    - omega_cost: [0,1]   Strength by which uncertainty reduces effective beta (control cost).
    - rho_stay:   [0,1]   Strength of stay bias on both stages (applied as additive logits term).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, omega_cost, rho_stay].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, omega_cost, rho_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values
    q1 = np.zeros(2) + 0.0
    q2 = np.zeros((2, 2)) + 0.5

    # Uncertainty per state (running mean absolute PE), initialized small
    u_state = np.zeros(2) + 0.05

    # Stay biases (logit boosts for repeating)
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)  # per state

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Effective beta reduced by control cost of exerting precise control under uncertainty
        # beta_eff = beta / (1 + omega_cost * stai * u) ; for stage-1 use average across states
        u_bar = 0.5 * (u_state[0] + u_state[1])
        beta1 = beta / (1.0 + omega_cost * stai * u_bar)
        beta2 = beta / (1.0 + omega_cost * stai * u_state[s])
        beta1 = max(1e-3, beta1)
        beta2 = max(1e-3, beta2)

        # Stay bias logits
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_stay

        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += rho_stay

        # Stage-1 policy (pure MF here)
        logits1 = beta1 * (q1 - np.max(q1)) + bias1
        # subtract max again for numerical stability after adding bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1]

        # Stage-2 policy
        q2_s = q2[s]
        logits2 = beta2 * (q2_s - np.max(q2_s)) + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = probs2[a2]

        # Stage-2 learning and uncertainty update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2
        u_state[s] = (1.0 - alpha) * u_state[s] + alpha * abs(pe2)

        # Stage-1 MF update via eligibility trace from stage-2 PE
        q1[a1] += alpha * pe2

        # Update stay memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)

","['alpha', 'beta', 'omega_cost', 'rho_stay']"
iter6_run0_participant4.json,cognitive_model3,580.86337539593,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty bonus (UCB-like) with anxiety-modulated optimism and exploration.

    Overview
    - Stage-2 values learned via RescorlaâWagner.
    - An uncertainty bonus encourages exploration (Upper Confidence Bound style) using a running
      estimate of uncertainty per state-action from absolute PEs.
    - Anxiety modulates both the optimism toward uncertain options (bonus strength) and the
      effective exploration temperature (reducing inverse temperature when anxious and uncertain).
    - Stage-1 MF cache is backed up from stage-2 PE; stage-1 decision uses an MB look-ahead
      (via known transitions) of uncertainty-augmented stage-2 values.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:      [0,1]   Learning rate for Q updates and uncertainty tracking.
    - beta:       [0,10]  Base inverse temperature for softmax.
    - phi_u:      [0,1]   Smoothing for uncertainty estimate from absolute PEs (higher -> faster).
    - zeta_bonus: [0,1]   Base strength of uncertainty bonus added to Q.
    - xi_anx:     [0,1]   Anxiety sensitivity: scales how much anxiety boosts exploration bonus
                          and reduces effective beta under uncertainty.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, phi_u, zeta_bonus, xi_anx].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, phi_u, zeta_bonus, xi_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure for MB look-ahead
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and uncertainties
    q2 = np.zeros((2, 2)) + 0.5
    u2 = np.zeros((2, 2)) + 0.05  # running uncertainty per state-action
    q1_mf = np.zeros(2) + 0.0

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Anxiety-adjusted uncertainty bonus strength and effective betas
        # More anxious -> larger bonus and lower beta when uncertainty is high
        bonus_strength = zeta_bonus * (1.0 + xi_anx * stai)

        # Stage-2 augmented values with bonus
        q2_aug = q2 + bonus_strength * u2

        # Effective beta reduced by average uncertainty weighted by anxiety
        u_bar = float(np.mean(u2))
        beta_eff1 = beta / (1.0 + xi_anx * stai * u_bar)
        beta_eff2 = beta / (1.0 + xi_anx * stai * float(np.mean(u2[s])))
        beta_eff1 = max(1e-3, beta_eff1)
        beta_eff2 = max(1e-3, beta_eff2)

        # Stage-1 MB look-ahead using augmented stage-2 values
        max_q2_aug = np.max(q2_aug, axis=1)      # best augmented value per planet
        q1_mb = T @ max_q2_aug                   # MB value for [A, U]
        q1 = 0.5 * q1_mf + 0.5 * q1_mb           # fixed 50/50 arbitration to keep params bounded

        # Stage-1 softmax
        logits1 = beta_eff1 * (q1 - np.max(q1))
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1]

        # Stage-2 softmax at reached state using augmented values
        logits2 = beta_eff2 * (q2_aug[s] - np.max(q2_aug[s]))
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = probs2[a2]

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update uncertainty (running abs PE)
        u2[s, a2] = (1.0 - phi_u) * u2[s, a2] + phi_u * abs(pe2)

        # Stage-1 MF update via eligibility from stage-2 PE
        q1_mf[a1] += alpha * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)","['alpha', 'beta', 'phi_u', 'zeta_bonus', 'xi_anx']"
iter6_run0_participant40.json,cognitive_model1,509.82669230973374,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free SARSA(Î») with anxiety-asymmetric learning, lapse, and repetition bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on the visited planet).
    reward : array-like of float
        Outcome on each trial (gold coins; can be negative).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha, beta, elig, stick, lapse)
        - alpha in [0,1]: base learning rate.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - elig in [0,1]: eligibility trace transferring credit to stage-1 action.
        - stick in [0,1]: repetition bias strength added to logits for repeating last action at each stage.
        - lapse in [0,1]: choice lapse; with prob lapse, choices are random at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes
    -----
    - Anxiety modulates learning asymmetry: effective alpha is larger for negative outcomes
      and smaller for positive outcomes as stai increases:
          alpha_pos = alpha * (1 - 0.5 * stai)
          alpha_neg = alpha * (1 + 0.5 * stai)
      Updates use alpha_pos if TD error >= 0, else alpha_neg.
    - Purely model-free SARSA(Î»): stage-1 value updated toward stage-2 action value and reward,
      with eligibility trace parameter 'elig'.
    - Repetition bias is applied at both stages (separate previous-action memories).
    - Lapse produces mixture of softmax choice and uniform random choice.
    """"""
    alpha, beta, elig, stick, lapse = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values: stage 1 has 2 actions; stage 2 has 2 states x 2 actions
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Previous actions for repetition bias
    prev_a1 = None
    prev_a2 = [None, None]  # one per state

    # Anxiety-modulated learning rates
    alpha_pos = alpha * (1.0 - 0.5 * stai_val)
    alpha_neg = alpha * (1.0 + 0.5 * stai_val)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 policy: softmax over Q1 with repetition bias
        logits1 = beta * Q1.copy()
        if prev_a1 is not None:
            logits1[prev_a1] += stick
        # Softmax
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        # Lapse mixture with uniform
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: softmax over Q2[s2] with repetition bias
        logits2 = beta * Q2[s2].copy()
        if prev_a2[s2] is not None:
            logits2[prev_a2[s2]] += stick
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        probs2 = (1.0 - lapse) * probs2 + lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # TD errors
        # SARSA target at stage 2 is immediate reward (terminal after second choice)
        delta2 = r - Q2[s2, a2]
        # Anxiety-asymmetric learning rate
        a2_lr = alpha_pos if delta2 >= 0 else alpha_neg
        Q2[s2, a2] += a2_lr * delta2

        # Stage-1 TD error uses bootstrapped value of chosen stage-2 action (post-update)
        delta1 = Q2[s2, a2] - Q1[a1]
        a1_lr = alpha_pos if delta1 >= 0 else alpha_neg
        # Eligibility trace moves a fraction of the stage-2 update to stage-1
        Q1[a1] += a1_lr * (elig * delta2 + (1.0 - elig) * delta1)

        # Update previous choices
        prev_a1 = a1
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'elig', 'stick', 'lapse']"
iter6_run0_participant40.json,cognitive_model3,517.8868716355205,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-Representation stage-1 control with anxiety-modulated planning depth and forgetting.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on the visited planet).
    reward : array-like of float
        Outcome on each trial (gold coins).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha2, beta, delta, fade, biasc)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax.
        - delta in [0,1]: SR discount controlling planning depth from stage 1 to stage 2.
        - fade in [0,1]: forgetting rate applied to Q2 each trial (toward zero baseline).
        - biasc in [0,1]: constant bias favoring spaceship A at stage 1; scaled down by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes
    -----
    - Successor representation for stage 1: M = I + delta_eff * T, where T is the known transition
      matrix from actions to states; Q1_SR = M_row_action Â· V where V = max_a Q2[state, a].
    - Anxiety reduces effective planning depth: delta_eff = delta * (1 - stai).
    - Forgetting: each trial Q2 <- (1 - fade_eff) * Q2, where fade_eff = fade * (0.5 + 0.5*stai),
      making higher anxiety accelerate forgetting of second-stage values.
    - A small constant bias toward spaceship A is added to logits, attenuated by anxiety:
      bias1 = [biasc * (1 - stai), 0].
    """"""
    alpha2, beta, delta, fade, biasc = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition matrix: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize storage for choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values for stage 2
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    delta_eff = delta * (1.0 - stai_val)
    fade_eff = fade * (0.5 + 0.5 * stai_val)

    for t in range(n_trials):
        # Apply forgetting to Q2 before computing policy
        if fade_eff > 0.0:
            Q2 *= (1.0 - fade_eff)

        # Compute SR-based Q1
        # M = I + delta_eff * T for a one-step augmented occupancy
        I2 = np.eye(2)
        M = I2 + delta_eff * T
        V2 = np.max(Q2, axis=1)  # value of each state given best alien
        Q1_SR = M @ V2  # 2 actions

        # Stage-1 policy with bias toward spaceship A (attenuated by anxiety)
        bias_vec = np.array([biasc * (1.0 - stai_val), 0.0])
        logits1 = beta * Q1_SR + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Update Q2 with outcome
        r = reward[t]
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha2', 'beta', 'delta', 'fade', 'biasc']"
iter6_run0_participant5.json,cognitive_model1,460.51264273262774,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Reliability-adaptive model-based arbitration with learned transitions.

    Core idea:
    - Learn second-stage action values (Q2) via TD learning.
    - Learn the transition model T from experience.
    - Compute a model-based (MB) plan using the learned T and a model-free (MF) Q1.
    - Arbitration weight for MB depends on (i) how reliable the learned transitions are,
      and (ii) anxiety: higher anxiety down-weights MB planning.
    
    Parameters (all in [0,1] except beta in [0,10]):
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received (0/1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce reliance on MB planning in proportion to learned transition reliability.
    model_parameters : array-like of float
        [learn_r, beta, learn_T, conf_gain]
        - learn_r in [0,1]: learning rate for Q2 and Q1 MF updates.
        - beta in [0,10]: inverse temperature for both stages.
        - learn_T in [0,1]: learning rate for the transition matrix T.
        - conf_gain in [0,1]: step size for updating a running transition-reliability signal.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    learn_r, beta, learn_T, conf_gain = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize
    # Transition model T[action, state]; start uncertain (0.5/0.5)
    T = 0.5 * np.ones((2, 2))
    # Running ""confidence"" per action: how deterministic the learned transitions are
    conf = np.zeros(2)
    # Stage-2 action values (aliens in each planet)
    Q2 = 0.5 * np.ones((2, 2))
    # Stage-1 model-free values for spaceships
    Q1_mf = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based evaluation from learned transitions
        max_Q2 = np.max(Q2, axis=1)           # value of best alien on each planet
        Q1_mb = T @ max_Q2                    # expected value for each spaceship

        # Anxiety- and reliability-gated arbitration weight
        # Determinism per action: |T[a,0]-0.5| scaled to [0,1]
        det = np.abs(T[:, 0] - 0.5) * 2.0
        # Update confidence only for chosen action (EMA)
        conf[a1] = (1.0 - conf_gain) * conf[a1] + conf_gain * det[a1]
        conf_mean = 0.5 * (conf[0] + conf[1])

        # Weight MB more when transitions are reliable and anxiety is low
        w_mb = np.clip(conf_mean * (1.0 - st), 0.0, 1.0)

        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        logits1 = beta * (Q1 - np.max(Q1))
        prob1 = np.exp(logits1)
        prob1 /= (np.sum(prob1) + eps)
        p1[t] = prob1[a1]

        # Stage-2 policy
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        prob2 = np.exp(logits2)
        prob2 /= (np.sum(prob2) + eps)
        p2[t] = prob2[a2]

        # Update transitions T for the chosen action toward observed state
        # Row a1 moves toward one-hot of the observed state
        T[a1, s] += learn_T * (1.0 - T[a1, s])
        T[a1, 1 - s] += learn_T * (0.0 - T[a1, 1 - s])

        # TD learning at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += learn_r * pe2

        # Backpropagate value to stage 1 (MF)
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += learn_r * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['learn_r', 'beta', 'learn_T', 'conf_gain']"
iter6_run0_participant5.json,cognitive_model2,449.20495861983505,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxious surprise-weighted outcomes and action bias at stage 1.

    Core idea:
    - Stage 2 values learned from an outcome augmented by transition ""surprise"":
      rare transitions boost the effective outcome more when anxiety is high.
      Common transitions mildly dampen outcomes when anxiety is low.
    - Stage 1 uses a hybrid MB/MF policy with fixed known transition structure (0.7 common).
    - A bias toward spaceship A at stage 1 increases with anxiety (action bias modulation).

    Parameters (all in [0,1] except beta in [0,10]):
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher anxiety amplifies the positive impact of rare transitions on learning
        and strengthens a bias toward spaceship A.
    model_parameters : array-like of float
        [alpha, beta, biasA0, omega_surprise]
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: inverse temperature for both stages.
        - biasA0 in [0,1]: baseline additive bias for choosing spaceship A at stage 1,
          scaled upward by anxiety.
        - omega_surprise in [0,1]: weight on rare-transition surprise added to the observed outcome.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, biasA0, omega_surprise = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed known transition structure for MB planning (A->X common, U->Y common)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    Q2 = 0.5 * np.ones((2, 2))
    Q1_mf = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    # MB weight increases when anxiety is low (not a free parameter)
    w_mb = np.clip(0.3 + 0.4 * (1.0 - st), 0.0, 1.0)
    # Anxiety-modulated bias toward spaceship A (action 0)
    biasA = biasA0 * (0.5 + 0.5 * st)  # more bias with higher anxiety

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Determine rarity of observed transition under the fixed model
        # Rare if A->Y or U->X
        rare = 1 if ((a1 == 0 and s == 1) or (a1 == 1 and s == 0)) else 0

        # Surprise-weighted effective outcome for learning at stage 2
        # Rare transitions add a positive boost (stronger with anxiety).
        # Common transitions get a small dampening when anxiety is low.
        boost_rare = omega_surprise * (0.5 + 0.5 * st) * rare
        damp_common = omega_surprise * (0.3 * (1.0 - st)) * (1 - rare)
        r_eff = r + boost_rare - damp_common

        # Model-based evaluation using fixed transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_fixed @ max_Q2

        # Combine MB and MF; add anxiety-modulated action bias for A
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        Q1_biased = np.array([Q1[0] + biasA, Q1[1]])

        # Stage 1 policy
        logits1 = beta * (Q1_biased - np.max(Q1_biased))
        prob1 = np.exp(logits1)
        prob1 /= (np.sum(prob1) + eps)
        p1[t] = prob1[a1]

        # Stage 2 policy
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        prob2 = np.exp(logits2)
        prob2 /= (np.sum(prob2) + eps)
        p2[t] = prob2[a2]

        # Stage 2 learning with surprise-weighted outcome
        pe2 = r_eff - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward realized stage-2 value (using r_eff)
        pe1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'biasA0', 'omega_surprise']"
iter6_run0_participant5.json,cognitive_model3,443.0933578721059,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-gated exploration at stage 2 and credit assignment to stage 1.

    Core idea:
    - Stage 2 exploration increases with anxiety (lower beta at stage 2 only).
    - Credit assignment from stage 2 to stage 1 is reduced by anxiety
      via an eligibility-like credit parameter.
    - Stage 1 uses a hybrid MB/MF policy where MB weight increases when anxiety is low.

    Parameters (all in [0,1] except beta_base in [0,10]):
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received (0/1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce stage-2 beta (more exploration)
        and diminish credit assignment to stage 1.
    model_parameters : array-like of float
        [alpha, beta_base, credit0, beta_boost, mix_base]
        - alpha in [0,1]: learning rate for Q updates.
        - beta_base in [0,10]: baseline inverse temperature.
        - credit0 in [0,1]: base credit from stage 2 to stage 1 (eligibility-like).
        - beta_boost in [0,1]: scales the reduction of stage-2 beta with anxiety.
        - mix_base in [0,1]: baseline MB weight at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta_base, credit0, beta_boost, mix_base = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure for MB evaluation
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q2 = 0.5 * np.ones((2, 2))
    Q1_mf = np.zeros(2)

    # Anxiety-gated parameters
    w_mb = np.clip(mix_base * (0.5 + 0.5 * (1.0 - st)), 0.0, 1.0)
    beta1 = np.clip(beta_base, 0.0, 10.0)
    # Reduce beta2 with anxiety to increase exploration
    beta2 = np.clip(beta_base * (1.0 - 0.5 * beta_boost * st), 0.0, 10.0)
    credit = np.clip(credit0 * (1.0 - st), 0.0, 1.0)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # MB evaluation for stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Policies
        logits1 = beta1 * (Q1 - np.max(Q1))
        prob1 = np.exp(logits1)
        prob1 /= (np.sum(prob1) + eps)
        p1[t] = prob1[a1]

        logits2 = beta2 * (Q2[s] - np.max(Q2[s]))
        prob2 = np.exp(logits2)
        prob2 /= (np.sum(prob2) + eps)
        p2[t] = prob2[a2]

        # Stage 2 TD update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Credit assignment from stage 2 to stage 1 MF value
        pe1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * credit * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)","['alpha', 'beta_base', 'credit0', 'beta_boost', 'mix_base']"
iter7_run0_participant0.json,cognitive_model2,528.4975333233983,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planner with anxiety-amplified exploration bonus at stage 2 and rare-transition-sensitive perseveration at stage 1.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial (0/1 or continuous in [0,1]).
    - stai: array-like (1,), anxiety score in [0,1]; scales exploration.
    - model_parameters: 6 parameters, all used:
        alpha_r        [0,1]  - learning rate for second-stage reward expectations.
        beta           [0,10] - inverse temperature at both stages.
        ucb_base       [0,1]  - baseline scale for optimistic exploration bonus (UCB-like).
        anx_ucb_slope  [0,1]  - slope by which anxiety increases exploration bonus.
        pers1          [0,1]  - first-stage perseveration to repeat the last chosen ship.
        zeta_rt        [0,1]  - attenuation of perseveration on rare transitions (0=no attenuation, 1=full attenuation).
    
    Model summary
    - Stage-2 Q-values are learned with alpha_r.
    - An exploration bonus b(s,a) = scale / sqrt(visit_count(s,a) + 1) is added to stage-2 values,
      where scale = ucb_base * (1 + anx_ucb_slope * stai). Higher anxiety -> larger bonus -> more exploration.
    - Stage-1 values are model-based by projecting the max (Q2 + bonus) through the fixed transitions (0.7/0.3).
    - First-stage perseveration is applied to the last chosen action; it is attenuated after rare transitions by factor (1 - zeta_rt).
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_r, beta, ucb_base, anx_ucb_slope, pers1, zeta_rt = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2), dtype=float)
    n_visits = np.zeros((2, 2), dtype=float)

    last_a1 = None
    last_rare = 0

    # exploration bonus scale
    bonus_scale = ucb_base * (1.0 + anx_ucb_slope * stai)

    for t in range(n_trials):

        # Exploration bonuses for each planet-action
        bonus = bonus_scale / np.sqrt(n_visits + 1.0)

        # MB first-stage values based on max over (Q2 + bonus)
        max_stage2 = np.max(q2 + bonus, axis=1)
        q1 = T @ max_stage2

        # Perseveration on first-stage repeating, attenuated if last transition was rare
        if last_a1 is not None:
            persev = pers1 * (1.0 - zeta_rt * last_rare)
            q1[last_a1] += persev

        # First-stage policy
        a1 = action_1[t]
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy includes exploration bonus
        s = state[t]
        a2 = action_2[t]
        q2_eff = q2[s] + bonus[s]
        q2c = q2_eff - np.max(q2_eff)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        n_visits[s, a2] += 1.0
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update rare/common flag for next trial
        common_dest = a1  # 0->X(0), 1->Y(1)
        last_rare = int(s != common_dest)
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'ucb_base', 'anx_ucb_slope', 'pers1', 'zeta_rt']"
iter7_run0_participant0.json,cognitive_model3,525.1508318041363,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-sensitive learning with anxiety amplification, hybrid MB/MF arbitration, and state-specific second-stage perseveration.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial.
    - stai: array-like (1,), anxiety score in [0,1]; scales volatility impact and arbitration.
    - model_parameters: 7 parameters, all used:
        alpha0        [0,1]  - base learning rate for second-stage values.
        beta          [0,10] - inverse temperature at both stages.
        kappa_vol     [0,1]  - gain by which local volatility inflates the learning rate.
        tau_leak      [0,1]  - leak for volatility estimate (higher -> faster adaptation).
        stick2        [0,1]  - state-specific perseveration on repeating the same second-stage action.
        w0            [0,1]  - baseline weight on model-based control at the first stage.
        rho_anx_mb    [0,1]  - anxiety slope increasing MB weight and volatility sensitivity.
    
    Model summary
    - Each planet maintains a volatility estimate z(s) tracking absolute prediction error; it is updated by a leaky integrator.
    - Effective learning rate at the visited planet s is:
        alpha_eff(s) = clip(alpha0 + kappa_vol * z(s) * (1 + rho_anx_mb * stai), 0, 1).
      Higher volatility and anxiety increase learning speed.
    - First-stage values are a MB/MF mixture: Q1 = w * MB + (1-w) * MF, with
        w = clip(w0 + rho_anx_mb * stai, 0, 1).
      MB uses fixed transition probabilities (0.7/0.3). MF credit uses the stage-2 PE.
    - Second-stage policy includes state-specific perseveration: at each planet, repeating the last chosen alien gets +stick2.
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha0, beta, kappa_vol, tau_leak, stick2, w0, rho_anx_mb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    # Volatility estimates per planet
    z = np.zeros(2, dtype=float)

    # Track last second-stage action per state for perseveration
    last_a2 = np.array([-1, -1], dtype=int)

    # Anxiety-modulated MB weight
    w = w0 + rho_anx_mb * stai
    w = max(0.0, min(1.0, w))

    for t in range(n_trials):

        # MB component via current q2 and fixed transitions
        mb_component = T @ np.max(q2, axis=1)
        q1 = w * mb_component + (1.0 - w) * q1_mf

        # First-stage policy
        a1 = action_1[t]
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with state-specific perseveration
        s = state[t]
        q2_eff = q2[s].copy()
        if last_a2[s] != -1:
            q2_eff[last_a2[s]] += stick2

        q2c = q2_eff - np.max(q2_eff)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        delta2 = r - q2[s, a2]

        # Update volatility estimate (leaky integration of absolute PE)
        z[s] = (1.0 - tau_leak) * z[s] + tau_leak * abs(delta2)

        # Anxiety- and volatility-modulated learning rate
        alpha_eff = alpha0 + kappa_vol * z[s] * (1.0 + rho_anx_mb * stai)
        alpha_eff = max(0.0, min(1.0, alpha_eff))

        # Update second-stage values
        q2[s, a2] += alpha_eff * delta2

        # MF credit to first stage from stage-2 PE
        q1_mf[a1] += delta2

        # Update last chosen second-stage action for perseveration
        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha0', 'beta', 'kappa_vol', 'tau_leak', 'stick2', 'w0', 'rho_anx_mb']"
iter7_run0_participant12.json,cognitive_model1,465.8930729373293,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety- and volatility-gated arbitration and eligibility trace.
    
    Idea
    ----
    First-stage action values are a convex combination of model-free (MF) and model-based (MB)
    controllers. The arbitration weight dynamically depends on:
      - Anxiety (stai): higher anxiety reduces reliance on MB planning.
      - Recent reward volatility (via an entropy tracker): higher entropy reduces MB reliance.
    Second-stage values are purely model-free and are backed up to stage 1 with an eligibility trace.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the planet (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Rewards received per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to gate arbitration.
    model_parameters : list or array
        [alpha_q, beta, mix_base, ent_sens, lambda_e]
        Bounds:
          alpha_q   in [0,1]   : learning rate for Q-updates at stage 2
          beta      in [0,10]  : inverse temperature for both stages
          mix_base  in [0,1]   : baseline MB weight (at minimal anxiety/volatility)
          ent_sens  in [0,1]   : sensitivity to reward entropy (volatility gate)
          lambda_e  in [0,1]   : eligibility trace from stage 2 to stage 1
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_q, beta, mix_base, ent_sens, lambda_e = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure (common A->X and U->Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # rows: action (A,U), cols: state (X,Y)

    # Initialize values
    q_stage1_mf = np.zeros(2)          # MF values at stage 1
    q_stage2_mf = np.zeros((2, 2))     # MF values per planet and alien

    # For likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track recent reward entropy (exponential moving estimate of Bernoulli parameter)
    p_r = 0.5
    # Decay rate for the moving estimate reuses ent_sens (more sensitivity -> faster tracking)
    alpha_h = max(1e-6, ent_sens)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based stage-1 values: expect max Q on each planet
        max_q2 = np.max(q_stage2_mf, axis=1)  # per state
        q1_mb = transition_matrix @ max_q2

        # Model-free stage-1 values already in q_stage1_mf
        # Arbitration weight: baseline reduced by anxiety and by entropy
        # - Anxiety gate: g_stai in [0,1], where higher stai reduces MB weight
        g_stai = 1.0 - stai  # low anxiety -> closer to 1, high anxiety -> closer to 0
        # - Entropy gate: compute current entropy of reward distribution estimate
        eps = 1e-12
        p_r = (1 - alpha_h) * p_r + alpha_h * reward[t]  # update with current outcome
        H = -(p_r * np.log(p_r + eps) + (1 - p_r) * np.log(1 - p_r + eps)) / np.log(2.0)  # normalized to [0,1]
        g_ent = 1.0 - H                                 # low entropy => more MB

        omega = mix_base * g_stai * (ent_sens * g_ent + (1 - ent_sens) * 1.0)
        omega = np.clip(omega, 0.0, 1.0)

        # Hybrid Q for stage 1
        q1_hybrid = (1.0 - omega) * q_stage1_mf + omega * q1_mb

        # Stage-1 policy
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c); probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (purely MF)
        q2 = q_stage2_mf[s].copy()
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c); probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2
        pe2 = reward[t] - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_q * pe2

        # Eligibility-backed update to stage 1
        q_stage1_mf[a1] += lambda_e * alpha_q * pe2

        # Optional direct TD correction towards realized state-action value
        td1 = np.max(q_stage2_mf[s]) - q_stage1_mf[a1]
        q_stage1_mf[a1] += (1.0 - lambda_e) * alpha_q * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_q', 'beta', 'mix_base', 'ent_sens', 'lambda_e']"
iter7_run0_participant12.json,cognitive_model2,420.1444108776419,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned-transition MB with anxiety-slowed transition learning and stickiness.
    
    Idea
    ----
    The agent learns the transition function P(planet | spaceship) online. Anxiety reduces the
    effective transition learning rate, slowing adaptation to rare transitions. Action stickiness
    at stage 1 is scaled up by anxiety, capturing perseveration under stress. Values at stage 2
    are model-free and inform the MB planner.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to slow transition learning and increase stickiness.
    model_parameters : list or array
        [alpha_q, beta, alpha_T0, anx_slope, stick_base]
        Bounds:
          alpha_q   in [0,1]   : learning rate for second-stage values
          beta      in [0,10]  : inverse temperature
          alpha_T0  in [0,1]   : baseline transition learning rate
          anx_slope in [0,1]   : strength of anxiety effects on alpha_T and stickiness
          stick_base in [0,1]  : baseline first-stage stickiness
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_q, beta, alpha_T0, anx_slope, stick_base = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition matrix P(s | a); start near the instructed structure
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Values
    q2 = np.zeros((2, 2))  # second-stage MF values
    q1_mf = np.zeros(2)    # optional MF cache from eligibility

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation
    alpha_T = alpha_T0 * (1.0 - anx_slope * stai)           # higher stai -> smaller alpha_T
    alpha_T = np.clip(alpha_T, 1e-6, 1.0)
    stick_eff = stick_base * (1.0 + anx_slope * stai)       # higher stai -> stronger stickiness

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based first-stage values from learned T and current q2
        mb_q1 = T @ np.max(q2, axis=1)

        # Add stickiness bias to last chosen a1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        # Combine MB with small MF cache (0.5 weight for stability)
        q1 = mb_q1 * 0.8 + q1_mf * 0.2 + bias

        # Stage 1 policy
        q1c = q1 - np.max(q1)
        prob1 = np.exp(beta * q1c); prob1 = prob1 / np.sum(prob1)
        p_choice_1[t] = prob1[a1]

        # Stage 2 policy
        q2c = q2[s] - np.max(q2[s])
        prob2 = np.exp(beta * q2c); prob2 = prob2 / np.sum(prob2)
        p_choice_2[t] = prob2[a2]

        # Update second-stage MF values
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Small eligibility backup to stage 1 MF cache
        q1_mf[a1] += 0.5 * alpha_q * pe2

        # Learn transition function T for executed a1 toward observed s
        # Move row a1 toward one-hot of observed state s
        target = np.array([0.0, 0.0]); target[s] = 1.0
        T[a1] = (1 - alpha_T) * T[a1] + alpha_T * target
        # Renormalize to guard against drift
        T[a1] = T[a1] / np.sum(T[a1])

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_q', 'beta', 'alpha_T0', 'anx_slope', 'stick_base']"
iter7_run0_participant12.json,cognitive_model3,527.5359333280031,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-bonus planner with anxiety-modulated exploration and lapse.
    
    Idea
    ----
    The agent augments second-stage Q-values with an uncertainty bonus proportional to the
    estimated reward variance m*(1-m), where m is the learned mean reward for each alien.
    Anxiety modulates the strength and sign of this bonus: higher anxiety reduces (or can
    reverse) directed exploration. A small lapse mixes policies with uniform choice noise.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Rewards received per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to modulate uncertainty-driven exploration and lapse.
    model_parameters : list or array
        [alpha_q, beta, uncert_base, lapse_base, anx_gain]
        Bounds:
          alpha_q     in [0,1]   : learning rate for mean reward estimates
          beta        in [0,10]  : inverse temperature
          uncert_base in [0,1]   : baseline uncertainty-bonus strength
          lapse_base  in [0,1]   : baseline lapse probability
          anx_gain    in [0,1]   : how strongly STAI modulates uncertainty bonus and lapse
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_q, beta, uncert_base, lapse_base, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition matrix for planning
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Reward mean estimates (Q-like) and induced uncertainty (from mean)
    m = np.full((2, 2), 0.5)  # initial mean reward per alien
    # Stage-1 MF cache to allow quick backup from stage 2 (helps fit stay/switch effects)
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated uncertainty bonus and lapse
    # Map stai in [0,1] to a signed modulation in [-1,1] via centered scaling
    mod = (2.0 * stai - 1.0) * anx_gain
    # Effective uncertainty coefficient: positive for directed exploration, negative for ambiguity aversion
    uncert_coef = uncert_base * (1.0 - 2.0 * stai * anx_gain) + mod * 0.0  # keep in [-uncert_base, uncert_base]
    uncert_coef = np.clip(uncert_coef, -uncert_base, uncert_base)

    lapse = np.clip(lapse_base * (1.0 + 0.5 * mod), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute uncertainty bonus per alien using variance proxy var = m*(1-m)
        var = m * (1.0 - m)
        bonus = uncert_coef * np.sqrt(var + 1e-6)

        # Augmented second-stage values
        q2_aug = m + bonus

        # Model-based evaluation of first-stage actions using augmented second-stage values
        mb_q1 = T @ np.max(q2_aug, axis=1)

        # Combine with MF cache (small weight)
        q1 = 0.85 * mb_q1 + 0.15 * q1_mf

        # Stage-1 softmax
        q1c = q1 - np.max(q1)
        pi1 = np.exp(beta * q1c); pi1 = pi1 / np.sum(pi1)
        # Apply lapse (mixture with uniform over 2 actions)
        pi1 = (1.0 - lapse) * pi1 + lapse * 0.5
        p_choice_1[t] = pi1[a1]

        # Stage-2 softmax using augmented values in the visited state
        q2s = q2_aug[s]
        q2c = q2s - np.max(q2s)
        pi2 = np.exp(beta * q2c); pi2 = pi2 / np.sum(pi2)
        pi2 = (1.0 - lapse) * pi2 + lapse * 0.5
        p_choice_2[t] = pi2[a2]

        # Learning: update mean reward for the chosen alien
        pe = reward[t] - m[s, a2]
        m[s, a2] += alpha_q * pe

        # Eligibility-like backup to stage-1 MF cache
        q1_mf[a1] += 0.5 * alpha_q * pe

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_q', 'beta', 'uncert_base', 'lapse_base', 'anx_gain']"
iter7_run0_participant14.json,cognitive_model1,518.9818633138276,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive-volatility hybrid MB/MF with anxiety-scaled learning and eligibility.
    
    Idea:
    - Second-stage Q-values are updated with an adaptive learning rate that increases
      with surprise |delta|. Anxiety scales the volatility sensitivity, so higher STAI
      increases the adjustment of learning rate by surprise.
    - First-stage choice values are a hybrid: w_mb * model-based + (1 - w_mb) * model-free.
      The model-free stage-1 values are updated via an eligibility trace from the stage-2 TD error.
      The trace strength is tied to anxiety (more anxious -> stronger credit assignment to the
      chosen spaceship).
    - Transitions are assumed known and stable (common = 0.7, rare = 0.3).
    
    Parameters (model_parameters):
    - lr0: base learning rate for Q2 and MF Q1 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - vol0: base volatility sensitivity scaling surprise into learning rate, in [0,1]
    - anx_vol: anxiety gain on volatility sensitivity, in [0,1]
    - w_mb: weight on model-based value at stage 1, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0.0/1.0)
    - stai: array-like with single float in [0,1]
    - model_parameters: array-like [lr0, beta, vol0, anx_vol, w_mb]
    
    Returns:
    - Negative log-likelihood of both stage choices under the model.
    """"""
    lr0, beta, vol0, anx_vol, w_mb = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure (rows: [A,U], cols: [X,Y])
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    q2 = np.zeros((2, 2))      # planet x alien values
    q1_mf = np.zeros(2)        # model-free spaceship values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated learning-rate adaptivity
    vol_eff = np.clip(vol0 + anx_vol * stai, 0.0, 1.0)
    # Eligibility trace strength increases with anxiety (credit assignment to a1 from a2 outcome)
    lam = np.clip(0.1 + 0.8 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based Q at stage 1 from current q2
        max_q2 = np.max(q2, axis=1)     # best alien per planet
        q1_mb = T @ max_q2

        # Hybrid value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited planet
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        delta2 = r - q2[s, a2]

        # Adaptive learning rate from surprise, scaled by anxiety
        alpha_adapt = np.clip(lr0 * (1.0 + vol_eff * abs(delta2)), 0.0, 1.0)

        # Update second-stage value
        q2[s, a2] += alpha_adapt * delta2

        # Update model-free stage-1 via eligibility from stage-2 TD error
        q1_mf[a1] += lr0 * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr0', 'beta', 'vol0', 'anx_vol', 'w_mb']"
iter7_run0_participant14.json,cognitive_model2,572.081570750733,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive utility with anxiety and directed exploration at stage 2.
    
    Idea:
    - Rewards are transformed by a concave/convex utility u(r) = r^eta, where
      eta = risk0 + anx_risk * stai. Higher anxiety can bias toward risk aversion
      (eta<1) or risk seeking (eta>1), depending on fitted signs within [0,1].
    - Stage-2 choice incorporates a directed exploration bonus proportional to an
      uncertainty trace u_trace[s,a] (running absolute TD errors). Anxiety speeds up
      the updating of this trace, increasing exploration under high uncertainty.
    - Stage-1 uses model-based values from fixed transitions and the risk-transformed
      expected Q2, no MF component here to keep the mechanism focused on risk/explore.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - risk0: base risk exponent for utility, in [0,1] (eta base)
    - anx_risk: anxiety gain on risk exponent, in [0,1] (eta = risk0 + anx_risk*stai, clipped to [0,1])
    - k_ucb: weight of uncertainty bonus at stage 2, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0.0/1.0)
    - stai: array-like with single float in [0,1]
    - model_parameters: array-like [alpha_r, beta, risk0, anx_risk, k_ucb]
    
    Returns:
    - Negative log-likelihood of both stage choices.
    """"""
    alpha_r, beta, risk0, anx_risk, k_ucb = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Risk exponent from anxiety
    eta = np.clip(risk0 + anx_risk * stai, 0.0, 1.0)

    q2 = np.zeros((2, 2))
    u_trace = np.zeros((2, 2))  # uncertainty trace per planet-alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety increases how quickly uncertainty traces adapt to new errors
    kappa_u = np.clip(0.3 + 0.7 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Stage-1: MB values computed from risk-transformed q2
        # Use the max over aliens per planet
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2: include uncertainty bonus
        s = state[t]
        bonus = k_ucb * u_trace[s]
        logits2 = beta * (q2[s] + bonus)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome with risk-sensitive utility
        r_raw = reward[t]
        r_util = r_raw ** eta

        # TD update
        delta2 = r_util - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update uncertainty trace from absolute TD errors (directed exploration)
        u_trace[s, a2] = (1.0 - kappa_u) * u_trace[s, a2] + kappa_u * abs(delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'risk0', 'anx_risk', 'k_ucb']"
iter7_run0_participant14.json,cognitive_model3,511.87275331204944,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned transitions with anxiety-driven confusion (uniform pull) and MF credit.
    
    Idea:
    - The agent learns the transition matrix from experience (row-wise updates).
    - Anxiety induces a ""transition confusion"" bias: the effective transition used
      for planning is a convex combination of the learned T and a uniform matrix,
      T_eff = (1 - tau_eff) * T + tau_eff * 0.5, where tau_eff increases with STAI.
      This captures difficulty trusting the learned structure under higher anxiety.
    - Stage-2 uses MF Q-learning. Stage-1 combines MB planning (via T_eff) and MF
      credit assignment from the stage-2 TD error through an eligibility trace
      whose strength grows with anxiety.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2, in [0,1]
    - alpha_t: transition learning rate, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - tau0: base confusion strength toward uniform transitions, in [0,1]
    - g_tau: anxiety gain on confusion, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0.0/1.0)
    - stai: array-like with single float in [0,1]
    - model_parameters: array-like [alpha_r, alpha_t, beta, tau0, g_tau]
    
    Returns:
    - Negative log-likelihood of both stage choices.
    """"""
    alpha_r, alpha_t, beta, tau0, g_tau = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transitions to uniform and Q2 to zeros
    T = np.ones((2, 2)) * 0.5  # rows sum to 1
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-dependent confusion toward uniform
    tau_eff = np.clip(tau0 + g_tau * stai, 0.0, 1.0)
    # Eligibility trace strength increases with anxiety
    lam = np.clip(0.2 + 0.7 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Effective transition matrix used for planning
        T_eff = (1.0 - tau_eff) * T + tau_eff * 0.5

        # MB values at stage 1 from T_eff and q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_eff @ max_q2

        # Combine MB and MF (simple sum; MF will be updated via eligibility)
        q1 = q1_mb + q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited state
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Reward and updates
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update learned transitions row for chosen a1 toward observed state s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += alpha_t * (target - T[a1])

        # Normalize to keep a valid row-stochastic matrix
        T[a1] = np.clip(T[a1], 1e-8, 1.0)
        T[a1] /= np.sum(T[a1])

        # Model-free credit assignment to stage 1 via eligibility
        q1_mf[a1] += alpha_r * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'alpha_t', 'beta', 'tau0', 'g_tau']"
iter7_run0_participant15.json,cognitive_model1,573.6056482369256,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-arbitrated MB/MF learner with anxiety-gated arbitration.
    
    Idea:
    - Stage-2 (aliens) values learned with TD(0) and a moving estimate of uncertainty (mean squared PE).
    - Arbitration between Model-Based (MB) and Model-Free (MF) control at Stage-1 depends on current uncertainty
      about Stage-2 values and the participant's anxiety.
        w_t = clip( a0*(1 - stai) + kappa * mean_uncertainty )
      So MB control increases with uncertainty (exploration/planning demand), but baseline MB is lower
      when anxiety is higher (since 1 - stai scales a0).
    - Stage-1 MF values are updated via eligibility (backing up Stage-2 value).
    
    Parameters (all used; total = 5):
    - lr_mf: [0,1] Learning rate for MF updates (Stage-2 TD and Stage-1 MF eligibility backup).
    - lr_mb: [0,1] Learning rate for the uncertainty estimator (moving average of squared PEs).
    - beta: [0,10] Inverse temperature for softmax at both stages.
    - a0: [0,1] Baseline arbitration term favoring MB when stai is low.
    - kappa: [0,1] Weight of online uncertainty driving MB arbitration.
    
    Inputs:
    - action_1: int array in {0,1}; first-stage spaceship choices (0=A, 1=U).
    - state: int array in {0,1}; second-stage planet (0=X, 1=Y).
    - action_2: int array in {0,1}; second-stage alien choices.
    - reward: float array in [0,1]; coins received.
    - stai: array-like with one float in [0,1]; anxiety score.
    - model_parameters: iterable [lr_mf, lr_mb, beta, a0, kappa].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    lr_mf, lr_mb, beta, a0, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value and uncertainty estimates
    q2 = 0.5 * np.ones((2, 2))   # Stage-2 MF values per planet x alien
    u2 = 0.0 * np.ones((2, 2))   # Moving estimate of squared PE (uncertainty proxy)
    q1_mf = np.zeros(2)          # Stage-1 MF values for spaceships

    for t in range(n_trials):
        s = state[t]

        # Risk-neutral MB evaluation based on current Stage-2 values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Uncertainty-driven arbitration (with anxiety gate)
        mean_unc = np.mean(u2)  # [0, ~1]
        w_t = a0 * (1.0 - stai) + kappa * mean_unc
        # clip to [0,1]
        if w_t < 0.0:
            w_t = 0.0
        elif w_t > 1.0:
            w_t = 1.0

        q1_combined = w_t * mb_q1 + (1.0 - w_t) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1_combined - np.max(q1_combined))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes
        r = reward[t]

        # Stage-2 update: TD(0)
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr_mf * pe2

        # Update uncertainty proxy with moving average of squared PEs
        u2[s, a2] = (1.0 - lr_mb) * u2[s, a2] + lr_mb * (pe2 * pe2)

        # Stage-1 MF eligibility backup toward observed Stage-2 value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += lr_mf * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr_mf', 'lr_mb', 'beta', 'a0', 'kappa']"
iter7_run0_participant15.json,cognitive_model2,555.5744696679691,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-sensitive credit misassignment with anxiety scaling and perseveration.
    
    Idea:
    - Pure MF action values at Stage-1; Stage-2 values learned by TD(0).
    - After rare transitions, some credit is shifted from the chosen spaceship to the unchosen one
      (misassignment), scaled by anxiety. This captures model-free behavior that is sensitive to the
      transition structure but in a maladaptive way that increases with anxiety.
    - Perseveration bias also scales with anxiety.
    
    Parameters (all used; total = 5):
    - eta2: [0,1] Learning rate for Stage-2 TD and Stage-1 MF updates.
    - beta: [0,10] Inverse temperature for both stages.
    - lam_ca: [0,1] Strength of credit passed to the unchosen action after rare transitions.
    - xi_trans: [0,1] Scales transition-based misassignment; effective xi_eff = xi_trans * stai.
    - stick0: [0,1] Baseline perseveration; effective stick = stick0 * stai.
    
    Inputs:
    - action_1: int array in {0,1}; first-stage choices (0=A, 1=U).
    - state: int array in {0,1}; second-stage planet (0=X, 1=Y).
    - action_2: int array in {0,1}; second-stage choices.
    - reward: float array in [0,1]; coins received.
    - stai: array-like with one float in [0,1]; anxiety score.
    - model_parameters: iterable [eta2, beta, lam_ca, xi_trans, stick0].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    eta2, beta, lam_ca, xi_trans, stick0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))  # Stage-2 MF values
    q1_mf = np.zeros(2)         # Stage-1 MF values

    prev_a1 = None
    stick_eff = stick0 * stai
    xi_eff = xi_trans * stai
    lam_eff = lam_ca * (1.0 - 0.5 * stai)  # anxiety slightly reduces how much is shifted

    for t in range(n_trials):
        s = state[t]

        # Perseveration bias at Stage-1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        # Stage-1 policy (pure MF with bias)
        q1b = q1_mf + bias
        z1 = beta * (q1b - np.max(q1b))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at Stage-2
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta2 * pe2

        # Determine whether the transition was common or rare for chosen spaceship
        # common if (A->X) or (U->Y)
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Stage-1 MF credit assignment
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]

        if is_common:
            # Normal credit to chosen after common transition
            q1_mf[a1] += eta2 * pe1
        else:
            # Rare transition: shift some credit to unchosen, scaled by anxiety
            q1_mf[a1] += eta2 * (1.0 - xi_eff) * pe1
            q1_mf[1 - a1] += eta2 * lam_eff * xi_eff * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta2', 'beta', 'lam_ca', 'xi_trans', 'stick0']"
iter7_run0_participant15.json,cognitive_model3,565.9460145130414,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Entropy- and surprise-modulated MB/MF mixture with anxiety shaping exploration and switching.
    
    Idea:
    - Stage-2 values learned via TD(0) with learning rate lr.
    - Stage-1 values combine MB planning and MF cached values with a weight that decreases with anxiety.
      w_eff = w_mix * (1 - stai).
    - Exploration/temperature: an entropy-like exploration factor lowers effective beta when anxiety is low:
      beta_eff = beta * (1 - zeta_ent * (1 - stai)).
    - Surprise-driven switching: after a surprising transition on the previous trial, low-anxiety agents
      tend to switch at Stage-1, high-anxiety agents tend to stick. Implemented as a bias on current Stage-1 policy
      toward the alternative action proportional to surprise_prev * kappa_surp * (0.5 - stai).
    
    Parameters (all used; total = 5):
    - lr: [0,1] Learning rate for TD updates at Stage-2 and Stage-1 MF eligibility.
    - beta: [0,10] Base inverse temperature for both stages.
    - zeta_ent: [0,1] Strength of entropy-based exploration; reduces beta when anxiety is low.
    - kappa_surp: [0,1] Weight on surprise-driven switching bias across trials.
    - w_mix: [0,1] Baseline MB mixture weight; effective weight decreases with anxiety.
    
    Inputs:
    - action_1: int array in {0,1}; first-stage choices.
    - state: int array in {0,1}; second-stage states.
    - action_2: int array in {0,1}; second-stage choices.
    - reward: float array in [0,1].
    - stai: array-like with one float in [0,1].
    - model_parameters: iterable [lr, beta, zeta_ent, kappa_surp, w_mix].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    lr, beta, zeta_ent, kappa_surp, w_mix = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))  # Stage-2 values
    q1_mf = np.zeros(2)         # Stage-1 MF values

    # Effective parameters shaped by anxiety
    beta1_eff = beta * (1.0 - zeta_ent * (1.0 - stai))
    beta2_eff = beta * (1.0 - zeta_ent * (1.0 - stai))
    if beta1_eff < 1e-6:
        beta1_eff = 1e-6
    if beta2_eff < 1e-6:
        beta2_eff = 1e-6
    w_eff = w_mix * (1.0 - stai)

    prev_a1 = None
    prev_is_common = None

    for t in range(n_trials):
        s = state[t]

        # Model-based Stage-1 values from current Stage-2 values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)
        q1_base = w_eff * mb_q1 + (1.0 - w_eff) * q1_mf

        # Surprise-driven switching bias based on previous trial's transition
        bias = np.zeros(2)
        if prev_a1 is not None and prev_is_common is not None:
            p_trans = 0.7 if prev_is_common else 0.3
            surprise_prev = -np.log(max(p_trans, 1e-8))  # higher for rare
            # Positive for low anxiety (switch), negative for high anxiety (stick)
            bias_to_alt = kappa_surp * (0.5 - stai) * surprise_prev
            bias[1 - prev_a1] += bias_to_alt

        # Stage-1 policy
        z1 = beta1_eff * (q1_base + bias - np.max(q1_base + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (entropy-modulated)
        z2 = beta2_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += lr * pe1

        # Store info for next-trial surprise bias
        prev_a1 = a1
        prev_is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['lr', 'beta', 'zeta_ent', 'kappa_surp', 'w_mix']"
iter7_run0_participant18.json,cognitive_model2,501.0445036772453,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk/variance-sensitive MF learning with anxiety-driven loss aversion and lapses.

    Core idea:
    - The agent is risk-sensitive at stage-2: subjective utility discounts reward variance.
      Utility_t = r_t - k_var_eff * Var_t(s,a), where k_var_eff increases with anxiety.
    - Learning uses MF TD with asymmetric sensitivity to negative prediction errors
      (loss aversion) that grows with anxiety.
    - Stage-1 choices are model-based (fixed transition model) over MF Q2 values
      computed from utilities; a small lapse probability increases with anxiety.

    Parameters (model_parameters):
    - alpha: [0,1] base learning rate for means and moments at stage-2 (and for stage-1 MF bootstrapping)
    - beta:  [0,10] inverse temperature for both stages
    - k_var: [0,1] base weight on variance penalty in utility; scaled up by anxiety
    - lambda_loss: [0,1] base extra sensitivity to negative prediction errors; scaled by anxiety
    - zeta_lapse: [0,1] baseline lapse probability; amplified by anxiety

    Inputs:
    - action_1: array of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state:    array of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array of ints in {0,1}; chosen alien on the observed planet
    - reward:   array of floats in [0,1]; received coins
    - stai:     array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, k_var, lambda_loss, zeta_lapse)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, k_var, lambda_loss, zeta_lapse = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed (true) transition structure for MB propagation at stage-1
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # MF values and risk-estimation moments at stage-2
    Q2 = np.zeros((2, 2))
    m1 = np.zeros((2, 2))   # running mean of rewards per (state, action)
    m2 = np.zeros((2, 2))   # running mean of squared rewards per (state, action)

    Q1_mf = np.zeros(2)     # MF cache for stage-1 (bootstrapped)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    # Anxiety effects
    k_var_eff = k_var * (0.5 + 0.5 * st)          # more anxiety -> stronger variance penalty
    lambda_loss_eff = lambda_loss * (0.5 + 0.5 * st)  # more anxiety -> stronger loss aversion
    lapse = np.clip(zeta_lapse * (0.5 + 0.5 * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute utility-based Q2 preference (no change to storage yet)
        var_sa = np.maximum(m2[s, a2] - m1[s, a2] ** 2, 0.0)
        util_sa = r - k_var_eff * var_sa  # current trial utility realization

        # Stage-1 MB value from current Q2 (using expected utilities via Q2)
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        # Combine with stage-1 MF cache (simple 50-50 to keep params bounded)
        Q1 = 0.5 * Q1_mb + 0.5 * Q1_mf

        # Policies with lapse at both stages
        # Stage-1
        logits1 = beta * (Q1 - np.max(Q1))
        exp1 = np.exp(logits1)
        soft1 = exp1 / (np.sum(exp1) + eps)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        exp2 = np.exp(logits2)
        soft2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Update risk moments (EWMA with alpha)
        m1[s, a2] = (1.0 - alpha) * m1[s, a2] + alpha * r
        m2[s, a2] = (1.0 - alpha) * m2[s, a2] + alpha * (r * r)

        # MF stage-2 update using utility-based TD; amplify negative PEs by loss aversion
        # Define utility expectation as Q2 value; current utility realization:
        u_t = util_sa
        pe2 = u_t - Q2[s, a2]
        adj_alpha2 = alpha * (1.0 + lambda_loss_eff) if pe2 < 0.0 else alpha
        Q2[s, a2] += adj_alpha2 * pe2

        # Stage-1 MF bootstrapping
        bootstrap = Q2[s, a2]
        pe1 = bootstrap - Q1_mf[a1]
        adj_alpha1 = alpha * (1.0 + 0.5 * lambda_loss_eff) if pe1 < 0.0 else alpha
        Q1_mf[a1] += adj_alpha1 * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'k_var', 'lambda_loss', 'zeta_lapse']"
iter7_run0_participant21.json,cognitive_model1,474.67004269200106,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid planner with learned transitions and anxiety-reduced trust in planning.

    Overview:
    - Stage-2 (alien) values are learned via a delta-rule.
    - Transition probabilities from spaceships to planets are learned online.
    - Stage-1 action values blend model-based (using learned transitions) and model-free values.
    - Anxiety reduces the trust/weight placed on model-based planning.

    Parameters (bounds):
    - model_parameters[0] = eta_r (0 to 1): learning rate for stage-2 values and for bootstrapping stage-1 MF values
    - model_parameters[1] = beta (0 to 10): inverse temperature used for both stages
    - model_parameters[2] = tau_tr (0 to 1): learning rate for transition probabilities (per chosen spaceship)
    - model_parameters[3] = trust0 (0 to 1): baseline weight on model-based value at stage 1
    - model_parameters[4] = anx_trust (0 to 1): how strongly anxiety reduces the model-based weight
        Effective MB weight: w_MB = clip(trust0 * (1 - anx_trust * stai), 0, 1)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, chosen alien per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score for this participant
    - model_parameters: list/array of five parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """"""
    eta_r, beta, tau_tr, trust0, anx_trust = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize with the task's common/rare structure as a prior; learn from experience
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: a1 in {0,1}, cols: states in {0,1}

    q1_mf = np.zeros(2)           # model-free stage-1 values
    q2 = np.zeros((2, 2))         # stage-2 values: states x actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-reduced trust in planning
    w_MB = np.clip(trust0 * (1.0 - anx_trust * stai_val), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based stage-1 values from learned transitions
        max_q2_by_state = np.max(q2, axis=1)          # shape (2,)
        q1_mb = T @ max_q2_by_state                   # shape (2,)

        # Hybrid value for stage 1
        q1 = w_MB * q1_mb + (1.0 - w_MB) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at the reached state
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update stage-2 value
        delta2 = r - q2[s, a2]
        q2[s, a2] += eta_r * delta2

        # Bootstrap stage-1 model-free value from observed stage-2 value (SARSA-style)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += eta_r * delta1

        # Update learned transition probabilities for the chosen spaceship
        # Move the row T[a1] toward a one-hot on the observed state s
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T[a1] = (1.0 - tau_tr) * T[a1] + tau_tr * target_row
        # Renormalize to avoid numerical drift
        T[a1] = T[a1] / np.sum(T[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta_r', 'beta', 'tau_tr', 'trust0', 'anx_trust']"
iter7_run0_participant21.json,cognitive_model2,436.6115758691726,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Valence-asymmetric learning with anxiety-tilted learning rates and bi-stage stickiness.

    Overview:
    - Stage-2 values are learned with separate learning rates for positive vs. negative prediction errors.
    - Anxiety tilts the balance: higher anxiety increases sensitivity to negative outcomes and reduces sensitivity to positive ones.
    - Stage-1 uses pure model-based planning from the fixed transition structure.
    - Perseveration (choice stickiness) applies at both stages and weakens with higher anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha_pos0 (0 to 1): baseline learning rate for positive PE at stage 2
    - model_parameters[1] = alpha_neg0 (0 to 1): baseline learning rate for negative PE at stage 2
    - model_parameters[2] = beta (0 to 10): inverse temperature for both stages
    - model_parameters[3] = anx_shift (0 to 1): strength of anxiety-induced tilt of learning rates
        Effective rates: alpha_pos = clip(alpha_pos0 * (1 - anx_shift * stai), 0, 1)
                         alpha_neg = clip(alpha_neg0 * (1 + anx_shift * stai), 0, 1)
    - model_parameters[4] = stick0 (0 to 1): baseline perseveration magnitude (reduced by anxiety)
        Effective stickiness: stick = stick0 * (1 - stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, chosen alien per trial
    - reward: array-like of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of five parameters as specified above

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """"""
    alpha_pos0, alpha_neg0, beta, anx_shift, stick0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (task known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))  # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_by_state = [None, None]

    # Anxiety-modulated parameters
    alpha_pos = np.clip(alpha_pos0 * (1.0 - anx_shift * stai_val), 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg0 * (1.0 + anx_shift * stai_val), 0.0, 1.0)
    stick = stick0 * (1.0 - stai_val)

    for t in range(n_trials):
        # Model-based stage-1 values from fixed transitions
        max_q2_by_state = np.max(q2, axis=1)          # shape (2,)
        q1_mb = T @ max_q2_by_state                   # shape (2,)

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        q1_net = q1_mb + bias1

        # Stage-1 policy
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with perseveration at the reached state
        s = int(state[t])
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick

        q2_net = q2[s] + bias2
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 update with valence asymmetry
        pe = r - q2[s, a2]
        lr = alpha_pos if pe >= 0.0 else alpha_neg
        q2[s, a2] += lr * pe

        # Update perseveration trackers
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos0', 'alpha_neg0', 'beta', 'anx_shift', 'stick0']"
iter7_run0_participant21.json,cognitive_model3,532.6207929611534,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Count-based exploration bonus with anxiety-curbed novelty seeking.

    Overview:
    - Stage-2 values are learned via a delta-rule.
    - An exploration bonus encourages choosing less-visited aliens (uncertainty bonus).
    - Anxiety curbs novelty seeking: higher anxiety reduces the exploration bonus.
    - Stage-1 is model-based using the task transition structure while propagating the exploration bonus.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 values
    - model_parameters[1] = beta (0 to 10): inverse temperature for both stages
    - model_parameters[2] = bonus0 (0 to 1): baseline magnitude of the exploration bonus
    - model_parameters[3] = anx_curb (0 to 1): strength by which anxiety reduces the bonus
        Effective bonus scale: b_eff = bonus0 * (1 - anx_curb * stai)
    - model_parameters[4] = decay_u (0 to 1): controls how fast uncertainty decays with visits

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, chosen alien per trial
    - reward: array-like of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of five parameters as specified above

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """"""
    alpha, beta, bonus0, anx_curb, decay_u = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))          # stage-2 values
    visits = np.zeros((2, 2))      # visit counts for state-action pairs

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-curbed exploration bonus scale
    b_eff = bonus0 * (1.0 - anx_curb * stai_val)
    b_eff = max(0.0, b_eff)

    for t in range(n_trials):
        # Compute per-state uncertainty bonuses from visit counts
        # u = 1 / sqrt(1 + decay_u * N) ensures [0,1] and decreasing with visits and decay_u
        u_state = 1.0 / np.sqrt(1.0 + decay_u * visits)  # shape (2,2)
        bonus = b_eff * u_state

        # Stage-1 model-based values propagate the bonus-augmented stage-2 values
        max_q2_bonus = np.max(q2 + bonus, axis=1)        # shape (2,)
        q1_mb = T @ max_q2_bonus                         # shape (2,)

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at reached state with exploration bonus
        s = int(state[t])
        q2_net = q2[s] + bonus[s]
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update stage-2 value
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update visit counts after observing the choice
        visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'bonus0', 'anx_curb', 'decay_u']"
iter7_run0_participant24.json,cognitive_model1,462.97601274599174,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MBâMF with anxiety-modulated arbitration and eligibility trace from stage 2 to stage 1.
    
    This model learns model-free values at both stages with an eligibility trace that
    propagates second-stage value back to the first-stage action. The first-stage
    policy mixes a model-based (transition-structured) value with the MF value.
    The MB/MF mixing weight is modulated by the participant's anxiety (stai).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score in [0,1]; we use stai[0].
    model_parameters : array-like
        [alpha2, beta, mix0, anx_mix, lam]
        - alpha2 in [0,1]: learning rate for stage-2 MF values and the stage-1 backprop update.
        - beta in [0,10]: softmax inverse temperature at both stages.
        - mix0 in [0,1]: baseline MB weight at stage 1 (when stai=0.5).
        - anx_mix in [0,1]: strength with which anxiety shifts MB weight (positive -> higher stai increases MB reliance).
        - lam in [0,1]: eligibility trace strength from stage 2 to stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha2, beta, mix0, anx_mix, lam = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure (common=0.7; A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)       # MF values for first-stage actions
    q2 = np.zeros((2, 2))     # MF values for second-stage actions by state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB/MF mixing weight (kept in [0,1])
    # Center stai at 0.5 so mix0 is the midpoint baseline.
    omega = mix0 + anx_mix * (s - 0.5)
    omega = min(1.0, max(0.0, omega))

    for t in range(n_trials):
        st = state[t]

        # Model-based Q at stage 1 from transition expectations and current stage-2 values
        v2 = np.max(q2, axis=1)         # V(s) = max_a Q2(s,a)
        q1_mb = T @ v2                  # MB action values

        # Mixed policy at stage 1
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf
        pref1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(pref1) / np.sum(np.exp(pref1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        q2_s = q2[st]
        pref2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(pref2) / np.sum(np.exp(pref2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha2 * pe2

        # Backpropagate with eligibility trace to stage 1 (MF)
        # Using the realized stage-2 action-value as the bootstrap target.
        target1 = q2[st, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += (alpha2 * lam) * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'mix0', 'anx_mix', 'lam']"
iter7_run0_participant24.json,cognitive_model2,536.5538763183188,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-like first-stage evaluation with anxiety-modulated planning horizon.
    
    This model learns:
    - A first-stage state-occupancy predictor (a light-weight successor representation) M[a, s2]
      equal to the probability of landing in each second-stage state given first-stage action a.
      It is updated from observed transitions.
    - Second-stage MF action values Q2(s2, a2) from rewards.
    
    The first-stage action values are computed as a discounted expectation of second-stage
    state values under M. Anxiety modulates the effective planning horizon via a discount
    factor gamma_eff: higher gamma puts more weight on future value; anxiety can increase
    or decrease gamma depending on parameters.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (X:0=W,1=S; Y:0=P,1=H).
    reward : array-like of float
        Rewards per trial (0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [alpha_sr, beta, gamma0, gamma_anx]
        - alpha_sr in [0,1]: learning rate for both M and Q2.
        - beta in [0,10]: softmax inverse temperature for both stages.
        - gamma0 in [0,1]: baseline planning discount applied to first-stage values.
        - gamma_anx in [0,1]: how much anxiety shifts the discount (positive -> higher stai increases discount).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_sr, beta, gamma0, gamma_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize M with the known common/rare structure as a prior
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated planning discount
    gamma_eff = gamma0 + gamma_anx * s
    gamma_eff = min(1.0, max(0.0, gamma_eff))

    for t in range(n_trials):
        st = state[t]

        # Second-stage policy
        q2_s = Q2[st]
        pref2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(pref2) / np.sum(np.exp(pref2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # First-stage policy: discounted expected second-stage value under learned M
        V2 = np.max(Q2, axis=1)       # state values
        Q1 = gamma_eff * (M @ V2)     # discounted expectation
        pref1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(pref1) / np.sum(np.exp(pref1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning updates
        r = reward[t]

        # Update M row for chosen action toward the observed next state (one-hot)
        oh = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        M[a1] += alpha_sr * (oh - M[a1])

        # Update Q2 at the visited state/action
        pe2 = r - Q2[st, a2]
        Q2[st, a2] += alpha_sr * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_sr', 'beta', 'gamma0', 'gamma_anx']"
iter7_run0_participant24.json,cognitive_model3,457.6827079212672,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Arbitrated MBâMF with learned transitions and anxiety-modulated surprise gating.
    
    This model learns:
    - Second-stage MF action values Q2(s2, a2) from reward.
    - First-stage MF values Q1_mf(a1) from bootstrapping onto realized second-stage values.
    - Action-specific transition models P(s2 | a1) from observed transitions.
    
    The first-stage decision value is a mixture of MB and MF components. The MB weight
    is dynamically reduced by transition surprise and by anxiety: on trials where the
    transition is surprising (relative to the learned P), the model relies more on MF.
    Higher anxiety further down-weights MB control.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float
        Received reward per trial (0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [alpha_q, beta, kappa_surprise0, anx_gain]
        - alpha_q in [0,1]: learning rate for both Q2 and the transition model.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa_surprise0 in [0,1]: baseline trust in MB on low-surprise trials (higher -> more MB).
        - anx_gain in [0,1]: how much anxiety down-weights MB control (higher -> more MF with higher stai).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_q, beta, kappa_surprise0, anx_gain = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize transition model with the known common/rare prior
    P = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        st = state[t]

        # MB action values from current transition model and second-stage values
        V2 = np.max(Q2, axis=1)
        Q1_mb = P @ V2

        # Surprise w.r.t. the chosen action's predicted next state probabilities
        a1 = action_1[t]
        # Surprise as 1 - predicted probability of the observed state
        surpr = 1.0 - P[a1, st]

        # Anxiety- and surprise-modulated MB weight
        # Start from kappa_surprise0 and reduce with surprise and anxiety
        w_mb = kappa_surprise0 * (1.0 - surpr)
        w_mb *= (1.0 - anx_gain * s)
        w_mb = min(1.0, max(0.0, w_mb))

        # First-stage policy from arbitrated value
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        pref1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(pref1) / np.sum(np.exp(pref1))
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        a2 = action_2[t]
        q2_s = Q2[st]
        pref2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(pref2) / np.sum(np.exp(pref2))
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Update transition model for chosen action toward the observed state
        oh = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        P[a1] += alpha_q * (oh - P[a1])

        # Update Q2
        pe2 = r - Q2[st, a2]
        Q2[st, a2] += alpha_q * pe2

        # Update Q1_mf by bootstrapping to realized second-stage value
        target1 = Q2[st, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_q * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_q', 'beta', 'kappa_surprise0', 'anx_gain']"
iter7_run0_participant29.json,cognitive_model1,461.0869809933252,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MF/MB with an uncertainty-seeking exploration bonus attenuated by anxiety.
    
    Idea
    - Stage-2 values (Q2) are learned model-freely.
    - A running, leaky visit-count per state-action estimates uncertainty; fewer recent visits => higher uncertainty.
    - An intrinsic information bonus (added to Q2 for choice and propagated to stage-1 via planning) promotes exploring uncertain aliens.
      This bonus is attenuated by anxiety: higher STAI reduces the weight on uncertainty-driven exploration.
    - Stage-1 choices are a convex combination of MF Q1 and MB plan (via known transition structure), mixed by omega.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}. Second-stage choices (0=left alien on that planet, 1=right alien).
    - reward: array of floats in [0,1]. Coins received.
    - stai: array-like with single float in [0,1]. Anxiety score (higher = more anxious).
    - model_parameters: iterable of 5 floats
        alpha        â [0,1] learning rate for MF values
        beta         â [0,10] inverse temperature for both stages
        omega        â [0,1] mixing weight of model-based value at stage 1
        leak         â [0,1] leak for visit-counts (higher => faster forgetting, more sustained uncertainty)
        info_bonus   â [0,1] base weight of the uncertainty bonus (attenuated by anxiety)
    
    Returns
    - Negative log-likelihood of observed action_1 and action_2.
    """"""
    alpha, beta, omega, leak, info_bonus = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # shape (a1, state)

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Leaky visit counts for uncertainty
    counts = np.zeros((2, 2))  # per (state, action)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-attenuated information bonus weight
    # Higher anxiety -> weaker bonus
    bonus_w = info_bonus * (1.0 - stai_val)

    for t in range(n_trials):
        # Uncertainty from leaky counts: unc = 1 / sqrt(1 + counts)
        unc2 = 1.0 / np.sqrt(1.0 + counts)
        # Information bonus bounded roughly in (0,1]
        bonus2 = bonus_w * unc2

        # Model-based plan at stage 1 uses future best (Q2 + bonus)
        max_q2_bonus = np.max(q2 + bonus2, axis=1)  # per state
        q1_mb = T @ max_q2_bonus  # shape (2,)

        # Hybrid action values for stage 1
        q1 = (1.0 - omega) * q1_mf + omega * q1_mb

        # Softmax for stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in reached state with bonus
        s = state[t]
        q2_aug = q2[s] + bonus2[s]
        logits2 = beta * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update leaky visit counts (uncertainty estimator)
        counts = (1.0 - leak) * counts
        counts[s, a2] += 1.0

        # MF learning at stage 1 toward obtained second-stage value (without bonus in learning)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'omega', 'leak', 'info_bonus']"
iter7_run0_participant29.json,cognitive_model2,402.0684501357661,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-gated model-based arbitration with anxiety-modulated sensitivity.
    
    Idea
    - The agent learns second-stage MF values (Q2) and a simple estimate of the transition matrix from experience.
    - On each trial, the degree of model-based control (w_mb) increases with the surprise of the observed transition
      (1 - current predicted probability to the reached state). Anxiety modulates sensitivity to surprise:
      higher anxiety down-weights surprise-based arbitration.
    - First-stage values are a hybrid of MF Q1 and MB plan using learned transitions and current Q2.
    - An eligibility trace propagates second-stage prediction errors back to Q1.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array of ints in {0,1}. First-stage choices.
    - state: array of ints in {0,1}. Reached second-stage state.
    - action_2: array of ints in {0,1}. Second-stage choices.
    - reward: array of floats in [0,1].
    - stai: array-like with single float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha        â [0,1] learning rate for MF values (both stages)
        beta         â [0,10] inverse temperature
        phi_surprise â [0,1] base sensitivity of arbitration to surprise (also used as transition-learning rate scale)
        lambda_elig  â [0,1] eligibility for backing up stage-2 PE to stage 1
        anx_gate     â [0,1] strength by which anxiety reduces surprise sensitivity (effective sensitivity multiplied by (1 - anx_gate*stai))
    
    Returns
    - Negative log-likelihood of observed action_1 and action_2.
    """"""
    alpha, beta, phi_surprise, lambda_elig, anx_gate = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Learnable transition model T_hat[a1, s]
    T_hat = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Effective surprise sensitivity after anxiety modulation
    sens = phi_surprise * (1.0 - anx_gate * stai_val)

    for t in range(n_trials):
        # MB plan from learned transitions
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T_hat @ max_q2

        # Compute surprise for current choice and reached state (before updating T_hat)
        a1 = action_1[t]
        s = state[t]
        pred_p = T_hat[a1, s]
        surprise = 1.0 - pred_p  # larger when transition was unlikely under current belief

        # Surprise-gated arbitration weight
        w_mb = np.clip(sens * surprise, 0.0, 1.0)

        # Hybrid action values
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Softmax stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF learning stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility-based update to Q1 MF using stage-2 PE
        q1_mf[a1] += lambda_elig * alpha * pe2

        # Update transition belief for chosen action using a scaled delta rule
        # T_hat[a1] moves toward a one-hot vector for reached state
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T_hat[a1, s_idx] += phi_surprise * (target - T_hat[a1, s_idx])

        # Renormalize row to stay within simplex (guarding numerical drift)
        row_sum = T_hat[a1, 0] + T_hat[a1, 1]
        if row_sum > 0:
            T_hat[a1, :] /= row_sum

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'phi_surprise', 'lambda_elig', 'anx_gate']"
iter7_run0_participant29.json,cognitive_model3,433.83746104182734,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Valence-asymmetric learning with an anxiety-modulated Pavlovian approach/avoid bias at stage 2.
    
    Idea
    - Stage-2 MF learning uses separate learning rates for positive vs negative prediction errors.
    - A Pavlovian approach bias favors a default alien within each state (index 0 in each planet), and is
      attenuated by anxiety (more anxious -> weaker approach, more neutral choices).
    - Stage-1 MF values are updated toward the obtained stage-2 value with the same valence-asymmetric rates.
    
    Parameters (bounds: alpha_* and biases in [0,1], beta in [0,10])
    - action_1: array of ints in {0,1}.
    - state: array of ints in {0,1}.
    - action_2: array of ints in {0,1}.
    - reward: array of floats in [0,1].
    - stai: array-like with single float in [0,1].
    - model_parameters: iterable of 5 floats
        alpha_pos   â [0,1] learning rate when PE >= 0
        alpha_neg   â [0,1] learning rate when PE < 0
        beta        â [0,10] inverse temperature
        pav_bias    â [0,1] base magnitude of approach bias toward default alien (index 0) at stage 2
        anx_valence â [0,1] how strongly anxiety attenuates the Pavlovian bias and enhances loss learning
    
    Returns
    - Negative log-likelihood of observed action_1 and action_2.
    """"""
    alpha_pos, alpha_neg, beta, pav_bias, anx_valence = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety modulation:
    # - attenuate approach bias: higher anxiety -> smaller effective Pavlovian bias
    pav_eff = pav_bias * (1.0 - anx_valence * stai_val)
    # - increase sensitivity to negative outcomes in learning by scaling alpha_neg
    alpha_neg_eff = np.clip(alpha_neg * (1.0 + anx_valence * stai_val), 0.0, 1.0)

    for t in range(n_trials):
        # Stage 1 policy (pure MF)
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with Pavlovian bias toward alien index 0
        s = state[t]
        pav_vec = np.array([+pav_eff, -pav_eff])  # approach 0, avoid 1 (state-independent sign)
        logits2 = beta * ((q2[s] + pav_vec) - np.max(q2[s] + pav_vec))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage 2 learning with valence asymmetry
        pe2 = r - q2[s, a2]
        if pe2 >= 0.0:
            q2[s, a2] += alpha_pos * pe2
            alpha1 = alpha_pos
        else:
            q2[s, a2] += alpha_neg_eff * pe2
            alpha1 = alpha_neg_eff

        # Stage 1 learning: move toward obtained stage-2 value with the same valence rate
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha1 * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha_pos', 'alpha_neg', 'beta', 'pav_bias', 'anx_valence']"
iter7_run0_participant3.json,cognitive_model1,513.8951646801222,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Dual-rate valence learning with anxiety-tuned model-based weighting and value decay.

    Mechanism:
    - Stage-2 Q-values (Q2[s2,a2]) are updated with separate learning rates for positive vs.
      negative prediction errors (alpha_plus, alpha_minus).
    - Stage-1 action values are a hybrid of model-based (MB) and model-free (MF) values.
      The MB component uses a fixed transition structure (common=0.7) to project the best
      second-stage value from each action. The MF component is learned by backing up the
      reached Q2 value.
    - Anxiety (stai) increases reliance on MB control and accelerates value decay, modeling
      anxious discounting/forgetting of old outcomes.
    - A small value decay/forgetting pulls Q2 toward a neutral prior (0.5).

    Parameters and bounds:
    - model_parameters = (alpha_plus, alpha_minus, beta, w0, phi_decay)
        alpha_plus in [0,1]: learning rate for positive Q2 prediction errors
        alpha_minus in [0,1]: learning rate for negative Q2 prediction errors
        beta in [0,10]: inverse temperature for softmax policies (both stages)
        w0 in [0,1]: baseline MB weight at stage-1
        phi_decay in [0,1]: base decay strength; anxiety scales it upward

    Inputs:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices (A=0, U=1)
    - state:    int array (n_trials,) in {0,1}; second-stage planet reached (X=0, Y=1)
    - action_2: int array (n_trials,) in {0,1}; second-stage alien choice
    - reward:   float array (n_trials,) in [0,1]; coins received
    - stai:     float array with single element in [0,1]; anxiety score
    - Returns negative log-likelihood of the observed choices.
    """"""
    alpha_plus, alpha_minus, beta, w0, phi_decay = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed known transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float) + 0.5  # initialize neutral expectation
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12
    # Anxiety-tuned parameters
    # MB weight increases with anxiety; clip to [0,1]
    w_mb = float(np.clip(w0 + (s_anx - 0.5) * (0.8), 0.0, 1.0))
    # Value decay increases with anxiety
    decay = float(np.clip(phi_decay * (0.5 + 0.5 * s_anx), 0.0, 1.0))

    for t in range(n_trials):
        # Stage-1 policy: hybrid of MB and MF
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: softmax over Q2 in reached state
        s2 = int(state[t])
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Stage-2 learning with valence asymmetry
        pe2 = r - q2[s2, a2]
        lr2 = alpha_plus if pe2 >= 0.0 else alpha_minus
        q2[s2, a2] += lr2 * pe2

        # Value decay toward 0.5 (neutral) for all Q2 entries
        q2 = (1.0 - decay) * q2 + decay * 0.5

        # Stage-1 MF update by backing up the reached Q2 value (SARSA-style)
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        # Use mean of valence rates to update MF for stability; anxiety scales learning slightly
        lr1 = 0.5 * (alpha_plus + alpha_minus) * (0.75 + 0.25 * s_anx)
        q1_mf[a1] += lr1 * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_plus', 'alpha_minus', 'beta', 'w0', 'phi_decay']"
iter7_run0_participant3.json,cognitive_model2,396.50940473140747,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned transitions with confidence-weighted arbitration and anxiety-shaped exploration and stickiness.

    Mechanism:
    - The transition model T(a1->s2) is learned via a simple delta rule (eta_T).
    - Stage-2 values Q2(s2, a2) are learned with a single learning rate (eta_V = eta_T).
    - Stage-1 action values hybridize MB and MF values. The MB weight increases with
      transition confidence (1 - entropy(T)); confidence scaling starts at omega0.
    - Separate perseveration (stickiness) for stage-1 and stage-2 (k1, k2).
    - Anxiety reduces effective beta (more exploration) and attenuates stage-2 stickiness.

    Parameters and bounds:
    - model_parameters = (eta_T, beta, k1, k2, omega0)
        eta_T in [0,1]: learning rate for transitions and Q2 values
        beta in [0,10]: inverse temperature
        k1 in [0,1]: stage-1 perseveration strength
        k2 in [0,1]: stage-2 perseveration strength
        omega0 in [0,1]: base weight scaling confidence into MB arbitration

    Inputs:
    - action_1: int array (n_trials,) in {0,1}
    - state:    int array (n_trials,) in {0,1}
    - action_2: int array (n_trials,) in {0,1}
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]
    - Returns negative log-likelihood of observed choices.
    """"""
    eta_T, beta, k1, k2, omega0 = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize transition model to uniform uncertainty (0.5/0.5)
    T = np.ones((2, 2), dtype=float) * 0.5
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = -1
    prev_a2 = -1

    eps = 1e-12

    for t in range(n_trials):
        # Transition confidence via normalized entropy
        ent = -np.sum(T * (np.log(T + eps)), axis=1) / np.log(2 + eps)  # in [0,1]
        conf = 1.0 - 0.5 * (ent[0] + ent[1])  # average confidence in [0,1]

        # Arbitration weight increases with confidence; anxiety amplifies this effect
        omega = omega0 * (1.0 + 0.5 * s_anx * conf)
        omega = float(np.clip(omega, 0.0, 1.0))

        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stickiness vectors
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Anxiety reduces effective beta (more exploration)
        beta_eff1 = beta * (1.0 - 0.4 * s_anx)
        logits1 = beta_eff1 * q1 + k1 * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s2 = int(state[t])

        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0

        # Anxiety attenuates stage-2 stickiness
        k2_eff = k2 * (1.0 - 0.6 * s_anx)
        beta_eff2 = beta * (1.0 - 0.3 * s_anx)
        logits2 = beta_eff2 * q2[s2] + k2_eff * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Transition learning: delta rule toward observed outcome
        # Move probability mass for chosen action toward the observed state
        for s_alt in (0, 1):
            target = 1.0 if s_alt == s2 else 0.0
            T[a1, s_alt] += eta_T * (target - T[a1, s_alt])
        # Renormalize to protect against drift
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Value learning at stage-2
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += eta_T * pe2

        # Model-free backup to stage-1
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta_T * pe1

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['eta_T', 'beta', 'k1', 'k2', 'omega0']"
iter7_run0_participant3.json,cognitive_model3,inf,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Utility-transformed rewards with anxiety-shaped sensitivity and surprise-seeking bias.

    Mechanism:
    - Stage-2 learning uses a utility-transformed reward u(r) that flattens or sharpens the
      impact of outcomes depending on risk/sensitivity parameter and anxiety.
    - Stage-1 values combine model-based projections with a surprise-seeking bonus assigned
      to actions that recently produced rare transitions (relative to a fixed transition).
    - A static choice bias toward spaceship A (bias_side) operates at stage-1 and extends
      to the lower stage (alien 0) to capture idiosyncratic preferences; anxiety modulates
      the expression of this bias.

    Parameters and bounds:
    - model_parameters = (alpha, beta, theta_sense, psi_surprise, bias_side)
        alpha in [0,1]: learning rate for Q2
        beta in [0,10]: inverse temperature
        theta_sense in [0,1]: baseline reward sensitivity shaping; higher -> flatter utility
        psi_surprise in [0,1]: weight of surprise-seeking bonus at stage-1
        bias_side in [0,1]: baseline bias toward action index 0 at both stages

    Inputs:
    - action_1: int array (n_trials,) in {0,1}
    - state:    int array (n_trials,) in {0,1}
    - action_2: int array (n_trials,) in {0,1}
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]
    - Returns negative log-likelihood of the observed choices.
    """"""
    alpha, beta, theta_sense, psi_surprise, bias_side = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure to evaluate surprise
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    # Keep track of whether last observed transition was rare, by action
    last_surprise = np.zeros(2, dtype=float)  # action-indexed bonus memory

    for t in range(n_trials):
        # Compute MB projection using fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Surprise-seeking bonus at stage-1, decays implicitly by overwrite
        # Anxiety amplifies the weight of surprise seeking
        surprise_weight = psi_surprise * (0.5 + 0.5 * s_anx)
        bonus1 = surprise_weight * last_surprise.copy()

        # Static side bias toward action 0, scaled by anxiety expression
        side_bias1 = np.array([1.0, 0.0]) * (2.0 * bias_side - 1.0) * (0.5 + 0.5 * s_anx)

        logits1 = beta * (q1_mb + bonus1) + side_bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s2 = int(state[t])

        # Stage-2 policy with analogous side bias toward alien 0
        side_bias2 = np.array([1.0, 0.0]) * (2.0 * bias_side - 1.0) * (0.4 + 0.6 * s_anx)
        logits2 = beta * q2[s2] + side_bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Utility transformation of reward: compress with theta_sense and anxiety
        # Higher anxiety + higher theta_sense -> more compression (diminished sensitivity)
        gamma = 1.0 - 0.6 * theta_sense * (0.5 + 0.5 * s_anx)  # in (0.4,1]
        u = r**gamma  # preserves ordering; compresses when gamma<1

        # Stage-2 learning on utility
        pe2 = u - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # Update surprise memory: 1 if rare transition occurred, else 0
        # Rare if observed state is the low-prob branch for chosen action
        prob_common = T_fixed[a1, s2]
        is_rare = 1.0 if prob_common < 0.5 else 0.0
        last_surprise = np.zeros(2, dtype=float)
        last_surprise[a1] = is_rare

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'theta_sense', 'psi_surprise', 'bias_side']"
iter7_run0_participant32.json,cognitive_model1,381.34997503057195,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Hybrid MF-MB learner with anxiety-modulated transition learning and eligibility trace.

    Core idea:
    - Model-free (MF) values are learned at both stages.
    - Model-based (MB) values use learned transition probabilities that are updated from experience.
    - Anxiety increases transition learning (volatility sensitivity) and reduces reliance on MB planning.
    - An eligibility trace propagates reward to the first-stage MF values.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state reached per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1 for the two aliens on the reached planet).
    reward : array-like of float
        Reward (coins) received on each trial (e.g., 0 or 1).
    stai : array-like of float
        Participant anxiety score array (use stai[0]).
    model_parameters : list or array-like of float
        [alpha_mf, beta, omega_mb, nu_vol, lambda_et]
        Bounds:
        - alpha_mf: [0,1] learning rate for MF values.
        - beta: [0,10] inverse temperature for both stages (softmax).
        - omega_mb: [0,1] baseline weight of MB plan in stage-1 action values.
        - nu_vol: [0,1] transition volatility sensitivity; scales transition learning with anxiety.
        - lambda_et: [0,1] eligibility trace strength to propagate reward to stage-1 MF.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_mf, beta, omega_mb, nu_vol, lambda_et = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize values
    q1_mf = np.zeros(2)           # MF values for first-stage actions A/U
    q2 = np.zeros((2, 2))         # Second-stage state x action values

    # Initialize transition model T[a] = [P(X|a), P(Y|a)]
    T = np.array([[0.7, 0.3],     # A -> X common
                  [0.3, 0.7]],    # U -> Y common
                 dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated components:
    # - Effective MB weighting: anxiety reduces planning reliance
    omega_eff = np.clip(omega_mb * (1.0 - 0.5 * stai_val), 0.0, 1.0)
    # - Transition learning rate (volatility sensitivity)
    alpha_T = np.clip(alpha_mf * nu_vol * (0.5 + 0.5 * stai_val), 0.0, 1.0)

    for t in range(n_trials):
        # Compute MB action values at stage 1 from current T and q2
        max_q2_by_state = np.max(q2, axis=1)              # [V(X), V(Y)]
        q1_mb = T @ max_q2_by_state                       # expected value of each spaceship
        q1 = (1.0 - omega_eff) * q1_mf + omega_eff * q1_mb

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Update second-stage MF value
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_mf * pe2

        # Update first-stage MF value:
        # 1) Bootstrapped TD from second-stage chosen value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_mf * pe1
        # 2) Eligibility trace: propagate reward further
        pe1_r = r - q1_mf[a1]
        q1_mf[a1] += lambda_et * alpha_mf * pe1_r

        # Update transition model for the chosen first-stage action
        # Move T[a1] toward the observed next state s
        onehot_s = np.array([1.0 if i == s else 0.0 for i in range(2)], dtype=float)
        T[a1, :] = (1.0 - alpha_T) * T[a1, :] + alpha_T * onehot_s

        # Keep rows normalized (numerical safety)
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_mf', 'beta', 'omega_mb', 'nu_vol', 'lambda_et']"
iter7_run0_participant32.json,cognitive_model2,381.1194597146275,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Directed exploration via UCB with anxiety scaling and value forgetting.

    Core idea:
    - Second-stage choices trade off exploitation (Q) and directed exploration (uncertainty bonus).
    - Uncertainty is captured via visit counts; bonus ~ kappa * sqrt(uncertainty).
    - Anxiety amplifies the exploration bonus and forgetting of learned values.
    - First-stage choices incorporate both MF backup and an exploration bonus based on
      the expected uncertainty of successor states.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices per trial (0/1).
    reward : array-like of float
        Reward per trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha, beta, kappa_ucb, chi_anxExp, z_forget]
        Bounds:
        - alpha: [0,1] learning rate for Q-values.
        - beta: [0,10] inverse temperature for softmax.
        - kappa_ucb: [0,1] baseline UCB exploration bonus weight.
        - chi_anxExp: [0,1] scales how strongly anxiety increases exploration bonus.
        - z_forget: [0,1] forgetting strength toward 0.5 after each trial, scaled by anxiety.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, kappa_ucb, chi_anxExp, z_forget = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Values
    q1 = np.zeros(2)            # first-stage MF values
    q2 = np.zeros((2, 2))       # second-stage values
    # Visit counts for UCB uncertainty
    n2 = np.zeros((2, 2))       # counts per state-action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Fixed transition structure for first-stage exploration bonus
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Anxiety-modulated UCB weight and forgetting
    kappa_eff = kappa_ucb * (0.5 + stai_val * chi_anxExp)
    forget = np.clip(z_forget * (0.5 + 0.5 * stai_val), 0.0, 1.0)
    prior = 0.5

    for t in range(n_trials):
        # Compute uncertainty at second stage as sqrt(1/(n+1))
        u2 = np.sqrt(1.0 / (n2 + 1.0))

        # Stage-2 policy: softmax over Q + UCB bonus
        s = state[t]
        bonus2 = kappa_eff * u2[s]
        logits2 = beta * ((q2[s] + bonus2) - np.max(q2[s] + bonus2))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 policy:
        # MF backup (bootstrapped from reached state later)
        # Directed exploration bonus at stage 1 from expected successor-state uncertainty
        # For each first-stage action a, expected uncertainty bonus is T[a] dot max(u2[state])
        max_u2_by_state = np.max(u2, axis=1)  # [u_X, u_Y]
        bonus1 = kappa_eff * (T @ max_u2_by_state)
        q1_policy = q1 + bonus1
        logits1 = beta * (q1_policy - np.max(q1_policy))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Outcome and updates
        r = reward[t]

        # Update second-stage Q and counts
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2
        n2[s, a2] += 1.0

        # Update first-stage MF value from second-stage chosen value (bootstrapped TD)
        pe1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * pe1

        # Forgetting toward prior
        q2 = (1.0 - forget) * q2 + forget * prior
        q1 = (1.0 - forget) * q1 + forget * prior

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'kappa_ucb', 'chi_anxExp', 'z_forget']"
iter7_run0_participant32.json,cognitive_model3,382.18729026058804,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Common-rare sensitivity with anxiety-biased planning and lapse.

    Core idea:
    - Hybrid MB/MF decision at stage 1.
    - The MB planner uses a transition matrix that is sharpened toward common transitions
      proportionally to anxiety (capturing common-rare sensitivity).
    - Credit assignment to the first stage is asymmetric: common transitions get higher learning
      rate than rare transitions, scaled by anxiety.
    - A lapse parameter mixes softmax policy with uniform noise and increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choice per trial (0/1).
    reward : array-like of float
        Reward per trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha, beta, omega_hyb, delta_cmbias, tau_lapse]
        Bounds:
        - alpha: [0,1] learning rate for MF values.
        - beta: [0,10] inverse temperature for softmax.
        - omega_hyb: [0,1] baseline weight on MB plan at stage 1.
        - delta_cmbias: [0,1] strength of common-rare bias in both planning and learning, scaled by anxiety.
        - tau_lapse: [0,1] baseline lapse rate mixed with uniform choice; increases with anxiety.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha, beta, omega_hyb, delta_cmbias, tau_lapse = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Base transition structure
    base_p_common = 0.7

    # Anxiety-modulated components
    # Sharpen commonness in MB transitions
    p_common_biased = 0.5 + (base_p_common - 0.5) * (1.0 + delta_cmbias * stai_val)
    p_common_biased = float(np.clip(p_common_biased, 0.5, 1.0))  # ensure valid range
    T_bias = np.array([[p_common_biased, 1.0 - p_common_biased],   # A
                       [1.0 - p_common_biased, p_common_biased]],  # U
                      dtype=float)

    # Anxiety-modulated MB weight (slight reduction with anxiety)
    omega_eff = np.clip(omega_hyb + 0.25 * (0.5 - stai_val), 0.0, 1.0)

    # Anxiety-modulated lapse
    tau_eff = np.clip(tau_lapse * (0.5 + 0.5 * stai_val), 0.0, 1.0)

    for t in range(n_trials):
        # Stage-1 MB value from biased transitions
        max_q2_by_state = np.max(q2, axis=1)
        q1_mb = T_bias @ max_q2_by_state
        q1 = (1.0 - omega_eff) * q1_mf + omega_eff * q1_mb

        # Stage-1 policy with lapse
        logits1 = beta * (q1 - np.max(q1))
        probs1_sm = np.exp(logits1)
        probs1_sm /= np.sum(probs1_sm)
        probs1 = (1.0 - tau_eff) * probs1_sm + tau_eff * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with standard softmax (no lapse)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Second-stage MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First-stage MF update with common-rare asymmetric credit
        # Determine if the observed transition was common given chosen a1
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        # Boost or damp learning rate depending on commonness, scaled by anxiety
        boost = (1.0 + delta_cmbias * stai_val) if is_common else (1.0 - delta_cmbias * stai_val)
        lr1 = np.clip(alpha * boost, 0.0, 1.0)
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += lr1 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'omega_hyb', 'delta_cmbias', 'tau_lapse']"
iter7_run0_participant35.json,cognitive_model1,329.2173342774262,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated pessimistic planning with repetition bias.

    Overview
    - Second-stage values are learned model-free.
    - First-stage uses a mixture of model-based (MB) and model-free (MF) values.
    - Anxiety increases pessimism in planning by applying a concave utility transform
      to second-stage values when computing MB values.
    - Anxiety also scales a repetition bias that favors repeating the previous first-stage action.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U), length n_trials
    - state: array-like of ints in {0,1}, second-stage state reached (0=X, 1=Y), length n_trials
    - action_2: array-like of ints in {0,1}, second-stage actions, length n_trials
    - reward: array-like of floats in [0,1], obtained reward, length n_trials
    - stai: array-like length-1 with scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha2: second-stage learning rate in [0,1]
        beta: inverse temperature for both stages in [0,10]
        phi_risk: anxiety coupling for pessimistic (concave) utility in [0,1]
        w_mb0: baseline model-based weight at stage 1 in [0,1]
        bias_repeat: baseline repetition bias magnitude in [0,1]

    Bounds
    - alpha2, phi_risk, w_mb0, bias_repeat in [0,1]
    - beta in [0,10]

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha2, beta, phi_risk, w_mb0, bias_repeat = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2)        # model-free first-stage action values
    q2 = np.zeros((2, 2))      # second-stage action values for states X=0, Y=1

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated pessimistic utility exponent for MB planning
    # Higher stai -> smaller gamma -> more concave utility -> pessimism
    gamma = 1.0 - 0.5 * phi_risk * stai
    gamma = max(0.2, min(1.0, gamma))  # keep a sensible range

    # Anxiety reduces reliance on MB planning
    w_mb = w_mb0 * (1.0 - 0.4 * stai)
    w_mb = min(1.0, max(0.0, w_mb))

    # Repetition bias scaled by anxiety (stronger with higher stai)
    rep_scale = (0.5 + 0.5 * stai)  # in [0.5, 1]
    last_a1 = None

    for t in range(n_trials):
        # Compute MB stage-1 values using pessimistic utility on q2
        max_q2 = np.max(q2, axis=1)
        # Map values to a ""utility"" space for planning (concave transform)
        max_q2_u = np.power(np.clip(max_q2, 0.0, 1.0), gamma)
        q1_mb = T @ max_q2_u

        # Combine MB and MF for decision values at stage 1
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add repetition bias (to previously chosen first-stage action)
        if last_a1 is not None:
            bias_vec = np.zeros(2)
            bias_vec[last_a1] = bias_repeat * rep_scale
            q1_dec = q1_dec + bias_vec

        # Softmax for stage 1
        q1c = q1_dec - np.max(q1_dec)
        probs1 = np.exp(beta * q1c)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # MF bootstrapping for stage 1 toward realized second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1  # same alpha for simplicity and parsimony

        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'phi_risk', 'w_mb0', 'bias_repeat']"
iter7_run0_participant35.json,cognitive_model3,339.9012212429144,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Confidence-trace arbitration with anxiety-modulated volatility and lapses.

    Overview
    - Second-stage values are learned model-free; first-stage blends MB and MF values.
    - Arbitration weight for MB at stage 1 is a leaky running estimate of second-stage
      choice confidence from the previous trial(s).
    - Anxiety increases the leak rate (more volatile arbitration, tracking recent confidence).
    - Anxiety also increases lapse probability (random choice mixture at both stages).

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U), length n_trials
    - state: array-like of ints in {0,1}, second-stage state (0=X, 1=Y), length n_trials
    - action_2: array-like of ints in {0,1}, second-stage actions, length n_trials
    - reward: array-like of floats in [0,1], obtained reward, length n_trials
    - stai: array-like length-1 with scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha2: second-stage learning rate in [0,1]
        beta: inverse temperature for both stages in [0,10]
        theta_conf0: baseline arbitration strength in [0,1] (scales the confidence trace)
        gamma_leak: anxiety coupling for confidence leak rate in [0,1]
        epsilon: baseline lapse rate in [0,1] (anxiety scales it upward)

    Bounds
    - alpha2, theta_conf0, gamma_leak, epsilon in [0,1]
    - beta in [0,10]

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha2, beta, theta_conf0, gamma_leak, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition matrix for MB planning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Confidence trace and anxiety-modulated leak
    # Higher stai -> higher effective leak -> more weight on recent confidence
    leak_eff = min(1.0, max(0.0, 0.2 + 0.6 * gamma_leak * stai))
    C = 0.5  # initial confidence trace (uninformative mid-point)

    # Anxiety-modulated lapse
    eps_lapse = min(0.5, max(0.0, epsilon * stai))

    for t in range(n_trials):
        # Compute MB first-stage values (using last known q2)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Arbitration weight from confidence trace
        w_mb = min(1.0, max(0.0, theta_conf0 * C))

        # Decision values and softmax for stage 1, with lapse
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        q1c = q1_dec - np.max(q1_dec)
        probs1_soft = np.exp(beta * q1c)
        probs1_soft /= np.sum(probs1_soft)
        probs1 = (1.0 - eps_lapse) * probs1_soft + eps_lapse * 0.5

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (softmax with lapse)
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2_soft = np.exp(beta * q2c)
        probs2_soft /= np.sum(probs2_soft)
        probs2 = (1.0 - eps_lapse) * probs2_soft + eps_lapse * 0.5

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # MF bootstrapping at stage 1
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Update confidence trace based on observed stage-2 choice certainty this trial
        # Confidence measure: c_t in [0,1], higher when one action dominates
        c_t = 2.0 * np.max(probs2_soft) - 1.0  # in [0,1]
        C = (1.0 - leak_eff) * C + leak_eff * c_t

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha2', 'beta', 'theta_conf0', 'gamma_leak', 'epsilon']"
iter7_run0_participant39.json,cognitive_model1,345.4997273347768,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-gated hybrid control with anxiety-modulated stickiness at both stages.

    The agent learns second-stage values model-free and uses a fixed transition
    model to compute model-based first-stage values. The weight on model-based
    control is dynamically increased by transition surprise (rare transitions),
    with a gain parameter scaled by anxiety. Perseveration (choice stickiness)
    is present at both stages and is modulated by anxiety in opposite directions:
    anxiety increases first-stage perseveration and decreases second-stage
    perseveration.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety trait score in [0,1]
    - model_parameters: iterable of 5 parameters, all used
        alpha: [0,1] learning rate for stage-2 MF values and stage-1 MF bootstrapping
        beta:  [0,10] inverse temperature for softmax at both stages
        rho_surp0: [0,1] gain for surprise-gated increase in planning weight
        kappa_rep0: [0,1] strength of first-stage perseveration (repeat last a1)
        zeta_pers2: [0,1] strength of second-stage perseveration (repeat last a2 in a state)

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha, beta, rho_surp0, kappa_rep0, zeta_pers2 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed transition structure: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)         # model-free first-stage values
    q2_mf = np.zeros((2, 2))    # second-stage MF values (state x action)

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Dynamic planning weight; baseline from anxiety (lower with higher anxiety)
    # and trial-wise modulation by surprise.
    # Initialize with baseline before any surprise is observed.
    w_mb = max(0.0, min(1.0, 1.0 - stai0))

    # Stickiness trackers
    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    eps = 1e-12
    for t in range(n_trials):

        # Compute MB first-stage values from current second-stage values
        max_q2 = np.max(q2_mf, axis=1)           # best alien per planet
        q1_mb = T @ max_q2                       # expected value per spaceship

        # Combine MF and MB with current weight
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage stickiness (anxiety increases perseveration)
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            kappa_eff = kappa_rep0 * (1.0 + stai0)
            q1_hybrid = q1_hybrid + kappa_eff * stick

        # First-stage policy
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with stickiness in the reached state
        s2 = state[t]
        q2 = q2_mf[s2].copy()
        prev_a2 = prev_a2_by_state[s2]
        if prev_a2 is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2] = 1.0
            # Anxiety reduces second-stage perseveration (enhanced flexibility under pressure)
            zeta_eff = zeta_pers2 * (1.0 - 0.5 * stai0)
            q2 = q2 + zeta_eff * stick2

        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning at stage-2 (MF)
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # Learning at stage-1 (MF bootstrapping toward the obtained second-stage value)
        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update dynamic planning weight by transition surprise
        # Surprise = 1 - P(observed state | chosen action)
        p_trans = T[a1, s2]
        surprise = 1.0 - p_trans
        # Anxiety increases sensitivity to surprise (greater shift to MB after rare transitions)
        w_mb = w_mb + rho_surp0 * (1.0 + stai0) * (surprise - (w_mb - (1.0 - stai0)))
        w_mb = max(0.0, min(1.0, w_mb))

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_by_state[s2] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'rho_surp0', 'kappa_rep0', 'zeta_pers2']"
iter7_run0_participant39.json,cognitive_model2,507.8640122732754,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-learning model with anxiety-modulated information bonus and lapse.

    The agent learns both second-stage reward values and first-stage transition
    probabilities online. First-stage choice is model-based using the learned
    transition model, augmented by an information-seeking bonus that favors
    visiting states with more uncertain reward options. Anxiety increases the
    information bonus and lapse (choice noise).

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), anxiety trait score in [0,1]
    - model_parameters: iterable of 5 parameters, all used
        lr_val:   [0,1] learning rate for second-stage reward values
        lr_tran:  [0,1] learning rate for transitions P(state | action1)
        beta:     [0,10] inverse temperature for both stages
        inv_u0:   [0,1] baseline weight for information (uncertainty) bonus at stage 1
        lapse0:   [0,1] baseline lapse probability for both stages

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    lr_val, lr_tran, beta, inv_u0, lapse0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transitions T[a, s], start from weak prior (0.5,0.5)
    T = np.ones((2, 2)) * 0.5

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    for t in range(n_trials):
        # Information bonus per first-stage action:
        # We approximate state uncertainty using reward-value dispersion in each state.
        # Compute per-state uncertainty as variance of q2 within state around 0.5
        # (larger when aliens are similar/unknown).
        state_unc = np.zeros(2)
        for s in range(2):
            m = 0.5 * (q2[s, 0] + q2[s, 1])
            v = 0.5 * ((q2[s, 0] - m) ** 2 + (q2[s, 1] - m) ** 2)
            state_unc[s] = v

        # MB value from learned transitions
        max_q2 = np.max(q2, axis=1)  # best alien per state
        q1_mb = T @ max_q2

        # Info bonus: expected state uncertainty under each action
        info_bonus = T @ state_unc

        # Anxiety increases info-seeking and lapse
        inv_u_eff = inv_u0 * (1.0 + stai0)
        lapse_eff = min(1.0, lapse0 * (1.0 + 0.5 * stai0))

        q1_total = q1_mb + inv_u_eff * info_bonus

        # First-stage policy with softmax-lapse mixture
        q1c = q1_total - np.max(q1_total)
        sm = np.exp(beta * q1c)
        sm = sm / (np.sum(sm) + eps)
        probs_1 = (1.0 - lapse_eff) * sm + lapse_eff * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (no info bonus), same lapse mechanism
        s2 = state[t]
        q2c = q2[s2] - np.max(q2[s2])
        sm2 = np.exp(beta * q2c)
        sm2 = sm2 / (np.sum(sm2) + eps)
        probs_2 = (1.0 - lapse_eff) * sm2 + lapse_eff * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learn second-stage rewards
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += lr_val * pe2

        # Learn transitions toward the observed state for the chosen action
        # One-hot target for observed state
        for s in range(2):
            target = 1.0 if s == s2 else 0.0
            T[a1, s] += lr_tran * (target - T[a1, s])

        # Renormalize to keep rows stochastic (for numerical stability)
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, 0] /= row_sum
            T[a1, 1] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['lr_val', 'lr_tran', 'beta', 'inv_u0', 'lapse0']"
iter7_run0_participant39.json,cognitive_model3,484.49735447121645,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-gated learning with forgetting, rare-transition switch bias, and hybrid control.

    Second-stage values are learned with a PE-dependent learning rate that
    increases under anxiety (trait-sensitive volatility gain). Unchosen second-stage
    actions decay toward a neutral value. First-stage choice uses a hybrid of
    model-based (fixed transition model) and model-free values. If the previous
    transition was rare, a switch bias is applied at the first stage; this bias
    is amplified by anxiety.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), anxiety trait score in [0,1]
    - model_parameters: iterable of 5 parameters, all used
        eta_base:  [0,1] base learning rate for second-stage values
        beta:      [0,10] inverse temperature for both stages
        chi_decay: [0,1] decay rate of unchosen second-stage actions toward 0.5
        psi_rare:  [0,1] bias magnitude to switch after rare transition at stage 1
        xi_mb:     [0,1] baseline weight on model-based control in first-stage choice

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    eta_base, beta, chi_decay, psi_rare, xi_mb = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous trial info for rare-transition-induced switching bias
    prev_a1 = None
    prev_rare = False

    eps = 1e-12
    for t in range(n_trials):
        # Compute MB first-stage values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid weight modulated by anxiety (anxiety slightly reduces MB influence)
        w_mb = np.clip(xi_mb * (1.0 - 0.5 * stai0), 0.0, 1.0)
        q1_hyb = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Rare-transition switch bias: if previous transition was rare, bias away from last a1
        if prev_a1 is not None and prev_rare:
            bias = np.zeros(2)
            # Encourage switching by penalizing repeating prev_a1 and favoring the other option
            bias[prev_a1] -= 1.0
            bias[1 - prev_a1] += 1.0
            psi_eff = psi_rare * (1.0 + stai0)  # anxiety amplifies the rare-switch tendency
            q1_hyb = q1_hyb + psi_eff * bias

        # First-stage policy
        q1c = q1_hyb - np.max(q1_hyb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        q2c = q2[s2] - np.max(q2[s2])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # PE-dependent learning rate at stage 2, scaled by anxiety
        pe2 = r - q2[s2, a2]
        eta = np.clip(eta_base * (1.0 + stai0 * abs(pe2)), 0.0, 1.0)
        q2[s2, a2] += eta * pe2

        # Forgetting (decay) for the unchosen action in the visited state toward 0.5
        unchosen_a2 = 1 - a2
        q2[s2, unchosen_a2] += chi_decay * (1.0 + stai0) * (0.5 - q2[s2, unchosen_a2])
        # Optional mild cross-state decay to stabilize values (no anxiety mod here)
        other_state = 1 - s2
        q2[other_state, :] += chi_decay * 0.0 * (0.5 - q2[other_state, :])  # zeroed but uses chi_decay param meaningfully above

        # Update first-stage MF toward obtained second-stage value (bootstrap)
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += eta_base * pe1  # use base rate for MF stage-1 update

        # Determine if current transition was rare to set next-trial bias
        p_trans = T[a1, s2]
        prev_rare = (p_trans < 0.5)
        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['eta_base', 'beta', 'chi_decay', 'psi_rare', 'xi_mb']"
iter7_run0_participant4.json,cognitive_model1,406.85140768891256,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-biased transition learning with model-based planning and stay bias.

    Overview
    - Learns state-2 action values via RescorlaâWagner (model-free).
    - Learns state-transition probabilities P(state | action) with learning rate alphaT.
    - First-stage action values are model-based: Q1_MB = P_effective @ max_a2 Q2(state,a2).
    - Anxiety biases transition use toward a prior that favors the common transitions,
      blending the learned transition matrix with the fixed prior as a function of stai and zeta_anx.
    - Includes a stay/perseveration bias at both stages.

    Parameters (all used)
    - alphaQ:     [0,1]   Learning rate for Q updates at stage 2 and for credit assignment to stage 1.
    - beta:       [0,10]  Inverse temperature for softmax at both stages.
    - alphaT:     [0,1]   Learning rate for transition probability updates.
    - zeta_anx:   [0,1]   Weight of anxiety-driven pull toward the prior transition matrix.
    - rho_stay:   [0,1]   Additive stay bias applied to logits for repeating the last action.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alphaQ, beta, alphaT, zeta_anx, rho_stay].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alphaQ, beta, alphaT, zeta_anx, rho_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed prior transitions reflecting task structure (common 0.7)
    prior_T = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Initialize learned transitions to uniform to allow learning
    T = np.ones((2, 2), dtype=float) / 2.0

    # Stage-2 Q-values and stage-1 MF trace (credit assigned via PE2)
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2)

    # For stay bias
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Anxiety-biased effective transitions: blend learned T with prior
        w_prior = np.clip(zeta_anx * stai, 0.0, 1.0)
        T_eff = (1.0 - w_prior) * T + w_prior * prior_T

        # Model-based first-stage values using current Q2
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T_eff @ max_q2

        # Combine MB and MF at stage 1 by simple sum (both on same scale)
        q1 = q1_mb + q1_mf

        # Stay bias logits
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_stay
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += rho_stay

        # Stage-1 choice probability
        logits1 = beta * (q1 - np.max(q1)) + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probability
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s)) + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Learning: Stage-2 RW
        pe2 = r - q2[s, a2]
        q2[s, a2] += alphaQ * pe2

        # Credit assignment to stage-1 MF via stage-2 PE
        q1_mf[a1] += alphaQ * pe2

        # Transition learning: update row for chosen action toward observed state
        # One-hot target for observed transition
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * target

        # Renormalize to ensure valid probabilities
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alphaQ', 'beta', 'alphaT', 'zeta_anx', 'rho_stay']"
iter7_run0_participant4.json,cognitive_model2,417.3802498041463,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive eligibility-trace model with anxiety-modulated loss aversion and stay bias.

    Overview
    - Stage-2 values via RW but with risk-sensitive utility: outcomes are down-weighted when local
      outcome variance is high; anxiety amplifies this aversion.
    - Tracks per-state running variance proxy of rewards to compute utility scaling.
    - Eligibility-trace style credit assignment from stage-2 PE to stage-1 MF value.
    - Stay bias at both stages.

    Parameters (all used)
    - alpha:      [0,1]   Learning rate for Q updates and variance tracking.
    - beta:       [0,10]  Inverse temperature.
    - eta_trace:  [0,1]   Strength of eligibility credit from stage 2 back to stage 1.
    - nu_risk:    [0,1]   Baseline risk sensitivity; amplified by anxiety.
    - rho_rep:    [0,1]   Perseveration strength (stay bias) at both stages.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, eta_trace, nu_risk, rho_rep].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, eta_trace, nu_risk, rho_rep = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2)

    # Running statistics per state for risk estimation: mean and mean absolute deviation proxy
    m_state = np.zeros(2) + 0.5
    d_state = np.zeros(2) + 0.1  # deviation proxy (|r - m| averaged)

    # For stay bias
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute risk-adjusted utility based on local deviation and anxiety
        # Effective risk weight increases with anxiety
        risk_w = nu_risk * (1.0 + stai)
        # Utility: down-weight reward toward its local mean depending on deviation magnitude
        # u = r - risk_w * d_state[s] * (r - m_state[s])
        # This pulls outcomes toward mean; for high variance and high anxiety, utility is more conservative.
        u = r - risk_w * d_state[s] * (r - m_state[s])

        # Stay biases
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_rep
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += rho_rep

        # Stage-1 softmax over MF q1
        q1 = q1_mf.copy()
        logits1 = beta * (q1 - np.max(q1)) + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s)) + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-2 PE using utility
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility-trace style credit to stage-1
        q1_mf[a1] += eta_trace * pe2

        # Update running mean and deviation for risk estimation in this state
        m_state[s] = (1.0 - alpha) * m_state[s] + alpha * r
        d_state[s] = (1.0 - alpha) * d_state[s] + alpha * abs(r - m_state[s])

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'eta_trace', 'nu_risk', 'rho_rep']"
iter7_run0_participant4.json,cognitive_model3,537.5072788309548,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Information-bonus model-based planner with anxiety-dependent lapse and volatility tracking.

    Overview
    - Stage-2 values via RW.
    - Tracks per-state outcome volatility (running abs prediction error of rewards).
    - Adds a directed information bonus at stage-2 state values; stage-1 model-based values
      plan through the fixed transition matrix to prefer actions leading to volatile states.
    - Lapse probability at both stages increases with anxiety (softens choice independently of values).

    Parameters (all used)
    - alpha:        [0,1]   Learning rate for Q updates and volatility tracking.
    - beta:         [0,10]  Inverse temperature for softmax.
    - kappa_info:   [0,1]   Weight of information bonus added to state values.
    - alpha_vol:    [0,1]   Learning rate for volatility (surprise) tracking.
    - xi_lapse_anx: [0,1]   Coefficient controlling anxiety-dependent lapse rate.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, kappa_info, alpha_vol, xi_lapse_anx].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, kappa_info, alpha_vol, xi_lapse_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition matrix (common 0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and volatility trackers
    q2 = np.zeros((2, 2)) + 0.5
    vol = np.zeros(2) + 0.05  # per-state volatility proxy

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Information bonus added to state values (state-level, not action-level)
        v_state = np.max(q2, axis=1) + kappa_info * (1.0 + stai) * vol

        # Stage-1 model-based values by planning through T
        q1 = T @ v_state

        # Anxiety-dependent lapse probabilities
        p_lapse = np.clip(xi_lapse_anx * stai, 0.0, 0.5)  # cap lapse to keep informative choices
        uniform1 = np.array([0.5, 0.5])
        uniform2 = np.array([0.5, 0.5])

        # Stage-1 choice probabilities
        logits1 = beta * (q1 - np.max(q1))
        logits1 = logits1 - np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - p_lapse) * soft1 + p_lapse * uniform1
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probabilities
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        logits2 = logits2 - np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - p_lapse) * soft2 + p_lapse * uniform2
        p_choice_2[t] = probs2[a2]

        # Learning at stage-2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Volatility update from absolute PE (separate rate alpha_vol)
        vol[s] = (1.0 - alpha_vol) * vol[s] + alpha_vol * abs(pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'kappa_info', 'alpha_vol', 'xi_lapse_anx']"
iter7_run0_participant40.json,cognitive_model1,564.6714000391739,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive model-free SARSA(Î») with anxiety-weighted loss aversion and lapse.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on visited planet).
    reward : array-like of float
        Obtained reward on each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, lambda_e, rho_risk, epsilon)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - lambda_e in [0,1]: eligibility trace from stage-2 TD error onto stage-1 action value.
        - rho_risk in [0,1]: risk sensitivity coefficient; higher increases loss aversion.
        - epsilon in [0,1]: base lapse rate (random choice), amplified by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Pure model-free control. Stage-1 values are learned by bootstrapping from stage-2 via an eligibility trace.
    - Anxiety increases loss aversion for negative outcomes and increases lapse.
    - Utility transform: for r >= 0, u = r; for r < 0, u = (1 + rho_risk * stai) * r (amplified losses).
    - Lapse effective rate: eps_eff = min(0.5, epsilon * (0.5 + 0.5 * stai)).
    """"""
    alpha2, beta, lambda_e, rho_risk, epsilon = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Probabilities recorded for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    Q1 = np.zeros(2)        # first-stage actions
    Q2 = np.zeros((2, 2))   # second-stage (state, action)

    # Anxiety-modulated lapse (capped to avoid degenerate likelihoods)
    eps_eff = min(0.5, max(0.0, epsilon * (0.5 + 0.5 * stai_val)))

    for t in range(n_trials):

        # Stage-1 policy (softmax over model-free Q1)
        a1 = int(action_1[t])
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        # Apply lapse mixture
        probs1 = (1.0 - eps_eff) * probs1 + eps_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax over state-conditional Q2)
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)

        # Apply lapse at stage-2
        probs2 = (1.0 - eps_eff) * probs2 + eps_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Risk-sensitive utility shaped by anxiety (amplify losses)
        r = float(reward[t])
        if r >= 0.0:
            u = r
        else:
            u = (1.0 + rho_risk * stai_val) * r

        # TD updates
        # Stage-2 update
        delta2 = u - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Stage-1 update with eligibility trace from stage-2 TD error
        Q1[a1] += alpha2 * lambda_e * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'lambda_e', 'rho_risk', 'epsilon']"
iter7_run0_participant40.json,cognitive_model2,534.3960754208516,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with learned transitions and anxiety-damped directed exploration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, tau_T, bonus0, omega0)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax.
        - tau_T in [0,1]: learning rate for updating the transition matrix and uncertainty.
        - bonus0 in [0,1]: scale of directed exploration bonus at stage-2.
        - omega0 in [0,1]: baseline arbitration weight for model-based control at stage-1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Learns transition matrix T online; stage-1 values are a mixture of MB and MF:
        Q1 = w_eff * (T @ max_a Q2[s,a]) + (1 - w_eff) * Q1_MF
      where w_eff = omega0 * (1 - stai).
    - Directed exploration at stage-2: add a bonus proportional to recency-weighted uncertainty,
      dampened by anxiety: bonus = bonus0 * (1 - stai) * sqrt(V[s,a]).
    - Uncertainty V is tracked via a delta-squared update: V <- (1 - tau_T)*V + tau_T*delta2^2.
    """"""
    alpha2, beta, tau_T, bonus0, omega0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Probabilities recorded for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize learned transition matrix T (rows = actions A/U, cols = states X/Y)
    T = np.full((2, 2), 0.5)

    # Value functions
    Q1_MF = np.zeros(2)      # model-free cached value at stage-1
    Q2 = np.zeros((2, 2))    # second-stage values per state and action

    # Uncertainty tracker for directed exploration at stage-2
    V2 = np.zeros((2, 2))

    # Effective arbitration weight reduced by anxiety
    w_eff_base = omega0 * (1.0 - stai_val)
    w_eff_base = max(0.0, min(1.0, w_eff_base))

    for t in range(n_trials):

        # Model-based evaluation of stage-1
        max_Q2 = np.max(Q2, axis=1)                # per state
        Q1_MB = T @ max_Q2                         # action -> expected value

        # Combine MB and MF with anxiety-damped weight
        Q1 = w_eff_base * Q1_MB + (1.0 - w_eff_base) * Q1_MF

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus
        s2 = int(state[t])
        a2 = int(action_2[t])

        # Anxiety-damped directed exploration
        bonus_scale = bonus0 * (1.0 - stai_val)
        bonus_vec = bonus_scale * np.sqrt(np.maximum(V2[s2], 0.0))
        logits2 = beta * (Q2[s2] + bonus_vec)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = float(reward[t])

        # Update transitions for the chosen first-stage action
        if tau_T > 0.0:
            # Move the chosen row toward a one-hot on the observed state
            T[a1, :] = (1.0 - tau_T) * T[a1, :]
            T[a1, s2] += tau_T
            # Renormalize row
            row_sum = np.sum(T[a1, :])
            if row_sum > 0.0:
                T[a1, :] /= row_sum

        # Stage-2 value update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Update uncertainty estimate for directed exploration
        V2[s2, a2] = (1.0 - tau_T) * V2[s2, a2] + tau_T * (delta2 ** 2)

        # Model-free stage-1 update via bootstrapping from stage-2
        # Use the current greedy value as a bootstrap target
        Q1_MF[a1] += alpha2 * (Q2[s2, a2] - Q1_MF[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'tau_T', 'bonus0', 'omega0']"
iter7_run0_participant40.json,cognitive_model3,469.42610958881943,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Effort-avoidant planner: anxiety- and surprise-gated inverse temperature, with forgetting.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta0, phi, rep, decay)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta0 in [0,10]: baseline inverse temperature for softmax (both stages).
        - phi in [0,1]: sensitivity of choice stochasticity to surprise Ã anxiety.
        - rep in [0,1]: repetition bias for stage-1 (tendency to repeat previous spaceship).
        - decay in [0,1]: forgetting rate applied to all second-stage Q-values each trial.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Stage-1 policy is fully model-based with known transitions (AâX, UâY are common).
    - Surprise is high on rare transitions; beta_eff = beta0 * (1 - phi * surprise * stai).
      Thus, anxiety increases the impact of surprise on reducing choice precision (effort avoidance).
    - A small repetition bias is added to the previously chosen stage-1 action, attenuated by anxiety.
    - Second-stage values undergo anxious forgetting: Q2 â (1 - decay_eff) * Q2,
      where decay_eff = decay * (0.5 + 0.5 * stai).
    """"""
    alpha2, beta0, phi, rep, decay = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (common=0.7, rare=0.3)
    T_known = np.array([[0.7, 0.3],  # action 0 (A): P(X), P(Y)
                        [0.3, 0.7]]) # action 1 (U): P(X), P(Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))  # stage-2 action values

    prev_a1 = None

    # Anxiety-weighted forgetting rate at stage-2
    decay_eff = decay * (0.5 + 0.5 * stai_val)

    for t in range(n_trials):

        # Apply forgetting before decisions
        if decay_eff > 0.0:
            Q2 *= (1.0 - decay_eff)

        # Stage-1 model-based values: expected max Q2 under known transitions
        max_Q2 = np.max(Q2, axis=1)  # per state
        Q1_MB = T_known @ max_Q2

        # Surprise based on whether the observed transition is common or rare
        a1 = int(action_1[t])
        s2 = int(state[t])
        prob_trans = T_known[a1, s2]
        surprise = 1.0 if prob_trans < 0.5 else 0.0

        # Anxiety- and surprise-gated inverse temperature
        beta_eff = beta0 * (1.0 - phi * surprise * stai_val)
        beta_eff = max(0.0, beta_eff)

        # Repetition bias attenuated by anxiety
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias_strength = rep * (1.0 - stai_val)
            bias1[prev_a1] = bias_strength

        # Stage-1 policy
        logits1 = beta_eff * Q1_MB + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy uses the same beta_eff (choice precision spillover)
        a2 = int(action_2[t])
        logits2 = beta_eff * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Reward and learning
        r = float(reward[t])
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha2', 'beta0', 'phi', 'rep', 'decay']"
iter7_run0_participant5.json,cognitive_model1,507.85239806222194,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Surprise-gated model-based arbitration with anxiety-modulated lapse and learning.

    Core ideas
    - Stage 1 uses a hybrid of model-based (MB) and model-free (MF) values.
      The MB weight increases on surprising (rare) transitions, and this
      surprise sensitivity is amplified by higher anxiety.
    - Stage 2 values are updated with a single learning rate; Stage 1 MF values
      are updated toward the experienced Stage 2 value.
    - Anxiety increases a lapse component in both stages and slightly reduces
      exploitation (effective beta).

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state reached (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1 = alien within planet).
    reward : array-like of float
        Coins received (e.g., 0 or 1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase surprise-gated MB arbitration and lapse,
        and slightly reduce effective inverse temperature.
    model_parameters : array-like of float
        [alpha_s2, beta, arb0, surpr0, lapse0]
        - alpha_s2: learning rate for stage-2 Q updates and scaled stage-1 MF updates.
        - beta: inverse temperature base.
        - arb0: baseline MB weight at stage 1 (0=MF,1=MB) before surprise.
        - surpr0: strength with which transition surprise boosts MB weight.
        - lapse0: baseline lapse rate mixed into softmax; grows with anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha_s2, beta_base, arb0, surpr0, lapse0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-gated parameters
    # Effective beta slightly reduced by anxiety (more randomness)
    beta_eff = np.clip(beta_base * (0.75 + 0.25 * (1.0 - st)), 0.0, 10.0)
    # Lapse increases with anxiety
    lapse_eff = np.clip(lapse0 * (0.5 + 0.5 * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Transition surprise based on realized transition probability
        p_common = 0.7 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0.3
        surprise = 1.0 - p_common  # 0.3 for common, 0.7 for rare

        # MB weight boosted by surprise; high anxiety amplifies this boost
        w_mb = arb0 + surpr0 * surprise * (0.5 + 0.5 * st)
        w_mb = float(np.clip(w_mb, 0.0, 1.0))

        # Hybrid stage-1 values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 choice probability with lapse
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        probs1 = (1.0 - lapse_eff) * probs1 + lapse_eff * 0.5
        p1[t] = probs1[a1]

        # Stage 2 choice probability with same beta, lapse
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        probs2 = (1.0 - lapse_eff) * probs2 + lapse_eff * 0.5
        p2[t] = probs2[a2]

        # Learning
        # Stage 2 PE and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_s2 * pe2

        # Stage 1 MF update toward the obtained second-stage value
        # Anxiety slightly increases stage-1 learning when low anxiety (better credit assignment)
        alpha1 = alpha_s2 * (0.6 + 0.4 * (1.0 - st))
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1 * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha_s2', 'beta_base', 'arb0', 'surpr0', 'lapse0']"
iter7_run0_participant5.json,cognitive_model2,492.2348475751695,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Learned-transition hybrid with anxiety-sensitive exploration and arbitration.

    Core ideas
    - The agent learns the transition matrix T (A->X, U->Y) via a delta rule.
    - Stage 1 uses a hybrid of MB and MF. MB weight decreases when transition
      entropy is high (uncertain), and this reduction is amplified by anxiety.
    - Stage 2 includes an uncertainty-directed exploration bonus that is reduced
      by anxiety (high anxiety â less directed exploration).

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1 = alien within planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce directed exploration and MB reliance
        when transition structure is uncertain.
    model_parameters : array-like of float
        [alpha, beta, tau_T, u_bonus, omega_anx]
        - alpha: learning rate for value updates (stage 2) and stage-1 MF bootstrapping.
        - beta: inverse temperature base.
        - tau_T: learning rate for transition probabilities.
        - u_bonus: base directed-exploration bonus at stage 2.
        - omega_anx: strength with which anxiety suppresses MB control under transition uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, tau_T, u_bonus, omega_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T[a, s]
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety reduces directed exploration bonus
    bonus_eff = np.clip(u_bonus * (0.6 + 0.4 * (1.0 - st)), 0.0, 1.0)
    beta_eff = float(np.clip(beta, 0.0, 10.0))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # MB values from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Transition uncertainty (entropy) for chosen action
        p = np.clip(T[a1, 0], 1e-6, 1 - 1e-6)
        ent = -(p * np.log(p) + (1 - p) * np.log(1 - p)) / np.log(2.0)  # in [0,1]
        # MB weight decreases with entropy; anxiety amplifies this reduction
        w_mb = 1.0 - ent * (0.5 + omega_anx * st)
        w_mb = float(np.clip(w_mb, 0.0, 1.0))

        # Hybrid stage-1 values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 choice
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 with directed exploration bonus reduced by anxiety
        # Uncertainty bonus encourages choosing less-known alien: use proximity to 0.5
        uncert = 1.0 - np.abs(q2[s] - 0.5) * 2.0  # 1 at 0.5, 0 at 0 or 1
        bonus = bonus_eff * uncert
        q2_aug = q2[s] + bonus

        logits2 = beta_eff * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Transition learning: move T[a1] toward the observed state s
        target = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1] = (1.0 - tau_T) * T[a1] + tau_T * target
        # Keep rows normalized (should already be)
        T[a1] /= (np.sum(T[a1]) + eps)

        # Value learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF bootstrapping toward obtained stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Slightly reduce alpha at stage 1 when anxiety high (poorer credit assignment)
        alpha1 = alpha * (0.7 + 0.3 * (1.0 - st))
        q1_mf[a1] += alpha1 * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'tau_T', 'u_bonus', 'omega_anx']"
iter7_run0_participant5.json,cognitive_model3,452.66129522213873,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Volatility-sensitive learning with anxiety-gated optimism and perseveration.

    Core ideas
    - Tracks planet-specific volatility from absolute prediction errors; higher
      volatility increases learning rate (adaptive learning).
    - Initial optimism (prior over rewards) is reduced by anxiety.
    - Perseveration (choice stickiness) at both stages increases with anxiety.
    - Stage 1 uses a hybrid of MB (via fixed transitions) and MF; MB weight is
      higher when anxiety is low and when estimated volatility is low.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1 = alien within planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase perseveration, increase volatility
        sensitivity, and reduce optimistic priors and MB reliance.
    model_parameters : array-like of float
        [alpha, beta, k_vol, pers0, prior0]
        - alpha: base learning rate.
        - beta: inverse temperature base.
        - k_vol: volatility gain (how strongly volatility boosts learning).
        - pers0: base perseveration strength added to logits for repeating the last action.
        - prior0: optimistic prior magnitude for initial Q2; reduced by anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, k_vol, pers0, prior0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initial optimistic prior reduced by anxiety
    optimism = prior0 * (0.5 + 0.5 * (1.0 - st))
    q2 = 0.5 + optimism * (np.ones((2, 2)) - 0.5)

    q1_mf = np.zeros(2)

    # Planet-specific volatility trackers
    v = np.zeros(2)  # volatility per state

    # Perseveration increases with anxiety
    stick = pers0 * (0.5 + 0.5 * st)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    prev_a1 = None
    prev_a2 = [None, None]  # track per state

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # MB weight decreases with anxiety and with mean volatility
        mean_v = float(np.mean(v))
        w_mb = np.clip(0.7 * (1.0 - st) * (1.0 - 0.5 * mean_v), 0.0, 1.0)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 stickiness logits
        logits1 = beta * (q1 - np.max(q1))
        if prev_a1 is not None:
            logits1[prev_a1] += stick

        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 stickiness logits (state-dependent)
        logits2 = beta * (q2[s] - np.max(q2[s]))
        if prev_a2[s] is not None:
            logits2[prev_a2[s]] += stick

        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Learning with volatility adaptation
        pe2 = r - q2[s, a2]

        # Update volatility for this state using a simple PE-driven filter
        gamma = 0.3 + 0.7 * np.clip(k_vol * st, 0.0, 1.0)  # stronger update when anxious
        v[s] = (1.0 - gamma) * v[s] + gamma * abs(pe2)
        # Adaptive learning rate boosted by volatility
        alpha_eff = np.clip(alpha * (0.5 + 0.5 * (1.0 - st)) + k_vol * v[s], 0.0, 1.0)

        q2[s, a2] += alpha_eff * pe2

        # Stage-1 MF bootstrapping
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_eff * pe1

        # Update perseveration trackers
        prev_a1 = a1
        prev_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)","['alpha', 'beta', 'k_vol', 'pers0', 'prior0']"
iter8_run0_participant0.json,cognitive_model1,502.2144292059152,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB+MF with anxiety-modulated credit assignment after rare transitions and conflict-based arbitration.
    
    Idea
    - Second-stage values are learned model-free.
    - First-stage choice uses a weighted hybrid of model-based (MB) and model-free (MF) values.
    - Arbitration weight increases under action-value conflict; credit assignment to the first-stage MF system is
      attenuated after rare transitions in proportion to anxiety (higher stai -> less credit on rare trials).
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 7 parameters (all used)
        alpha_mf: learning rate for second-stage Q-values (and first-stage MF trace).
        beta: inverse temperature for softmax at both stages [0,10].
        omega_mb: base weight for MB contribution at stage-1 decisions.
        chi_conflict: arbitration boost under conflict (higher when MB action values are close).
        psi_anx_ca: anxiety modulation of credit assignment after rare transitions (attenuates MF credit).
        pers1: perseveration bias to repeat the last first-stage action.
        lambda_elig: eligibility trace from stage-2 PE to first-stage MF values.
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_mf, beta, omega_mb, chi_conflict, psi_anx_ca, pers1, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clip parameters to valid bounds
    alpha_mf = min(1.0, max(0.0, alpha_mf))
    beta = min(10.0, max(1e-6, beta))
    omega_mb = min(1.0, max(0.0, omega_mb))
    chi_conflict = min(1.0, max(0.0, chi_conflict))
    psi_anx_ca = min(1.0, max(0.0, psi_anx_ca))
    pers1 = min(1.0, max(0.0, pers1))
    lambda_elig = min(1.0, max(0.0, lambda_elig))

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # A goes to X with 0.7, Y with 0.3
                  [0.3, 0.7]])  # U goes to X with 0.3, Y with 0.7

    # Value structures
    q2 = np.zeros((2, 2))         # second-stage values: state x action
    q1_mf = np.zeros(2)           # first-stage model-free values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    for t in range(n_trials):

        # Model-based first-stage values are expectations of best second-stage option
        max_q2 = np.max(q2, axis=1)            # value of best alien on each planet
        q1_mb = T @ max_q2                     # MB value for each spaceship

        # Conflict-based arbitration: more MB weight when MB action values are close (high conflict)
        # conflict in [0,1]: 1 when values equal, ~0 when far apart
        diff = abs(q1_mb[0] - q1_mb[1])
        conflict = 1.0 - np.tanh(diff)         # smooth, in (0,1]
        w = omega_mb + chi_conflict * conflict
        w = min(1.0, max(0.0, w))

        # Combine MB and MF for stage-1 action values
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Perseveration to repeat previous first-stage action
        if last_a1 is not None:
            q1[last_a1] += pers1

        # First-stage choice probability
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability (softmax)
        s = state[t]
        q2_centered = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Second-stage MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_mf * pe2

        # Stage-1 MF credit assignment via eligibility trace
        # Anxiety-modulated attenuation after rare transitions
        common_dest = a1  # common planet index for chosen ship
        is_rare = 1.0 if (s != common_dest) else 0.0
        ca_scale = 1.0 - psi_anx_ca * stai * is_rare
        ca_scale = min(1.0, max(0.0, ca_scale))

        q1_mf[a1] += lambda_elig * ca_scale * pe2

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_mf', 'beta', 'omega_mb', 'chi_conflict', 'psi_anx_ca', 'pers1', 'lambda_elig']"
iter8_run0_participant0.json,cognitive_model3,496.6543959279888,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Dirichlet transition learning with uncertainty-sensitive arbitration modulated by anxiety.
    
    Idea
    - The agent learns transition probabilities per first-stage action via simple Dirichlet counts.
    - Stage-1 decisions arbitrate between model-based (MB) and model-free (MF) values based on transition
      uncertainty. Higher anxiety increases aversion to uncertain transition models, reducing MB weight when
      the current transition estimate has high entropy.
    - Second-stage includes forgetting of unchosen action values.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 7 parameters (all used)
        alpha_r: reward learning rate for second-stage values.
        beta: inverse temperature for both stages [0,10].
        w0_mb: baseline MB weight at stage-1 (when uncertainty is low).
        kappa_unc: strength of uncertainty aversion in arbitration.
        mu_forget: decay on the unchosen second-stage action at the visited state.
        rep1: perseveration bias to repeat last first-stage choice.
        xi_prior: scales Dirichlet prior strength with anxiety (higher stai -> stronger prior).
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha_r, beta, w0_mb, kappa_unc, mu_forget, rep1, xi_prior = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clip parameters
    alpha_r = min(1.0, max(0.0, alpha_r))
    beta = min(10.0, max(1e-6, beta))
    w0_mb = min(1.0, max(0.0, w0_mb))
    kappa_unc = min(1.0, max(0.0, kappa_unc))
    mu_forget = min(1.0, max(0.0, mu_forget))
    rep1 = min(1.0, max(0.0, rep1))
    xi_prior = min(1.0, max(0.0, xi_prior))

    # Dirichlet prior strength increases with anxiety
    prior_strength = 1.0 + 9.0 * xi_prior * stai  # in [1,10]
    # Initialize Dirichlet counts: for each action, counts over states (X,Y)
    dir_counts = np.array([[prior_strength, prior_strength],
                           [prior_strength, prior_strength]], dtype=float)

    # Values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    for t in range(n_trials):

        # Current transition estimates from Dirichlet counts
        T_hat = dir_counts / np.sum(dir_counts, axis=1, keepdims=True)  # rows sum to 1

        # MB values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2

        # Uncertainty (entropy) per action; use entropy of row for last chosen action if available,
        # else average uncertainty across actions for arbitration. Here we combine across both rows
        # by taking the mean entropy.
        ent_rows = []
        for a in (0, 1):
            p = T_hat[a]
            # numerical stability
            p = np.maximum(p, 1e-12)
            ent = -np.sum(p * np.log(p)) / np.log(2.0)  # normalized by log(2) to be in [0,1]
            ent_rows.append(ent)
        mean_entropy = 0.5 * (ent_rows[0] + ent_rows[1])

        # Anxiety-weighted uncertainty aversion: reduce MB weight when entropy is high
        w_eff = w0_mb * (1.0 - kappa_unc * mean_entropy * (1.0 + stai))
        w_eff = min(1.0, max(0.0, w_eff))

        # Combine MB and MF for stage-1 values
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Perseveration
        if last_a1 is not None:
            q1[last_a1] += rep1

        # First-stage policy
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        q2_centered = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and second-stage learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Forgetting of the unchosen action at visited state
        other = 1 - a2
        q2[s, other] *= (1.0 - mu_forget)

        # First-stage MF bootstrap (eligibility via second-stage PE without extra parameter)
        q1_mf[a1] += alpha_r * pe2

        # Update Dirichlet counts with observed transition
        dir_counts[a1, s] += 1.0

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta', 'w0_mb', 'kappa_unc', 'mu_forget', 'rep1', 'xi_prior']"
iter8_run0_participant12.json,cognitive_model1,475.0962064314839,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and learned transitions.
    
    This model blends a model-based (MB) and a model-free (MF) controller at stage 1.
    The MB controller uses a learned transition model that is updated from experience.
    Anxiety modulates both the arbitration weight favoring MB control and the decision
    temperature. Stage-2 reward values are learned via a delta-rule.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to modulate arbitration and temperature.
    model_parameters : list or array
        [alpha_q, beta, mb_bias, anx_mod, trans_lr]
        Bounds:
          alpha_q in [0,1]   : learning rate for Q-values
          beta in [0,10]     : base inverse temperature
          mb_bias in [0,1]   : baseline tendency to favor MB over MF (mapped internally)
          anx_mod in [0,1]   : strength of anxiety influence on arbitration and temperature
          trans_lr in [0,1]  : transition learning rate for MB transition matrix

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha_q, beta, mb_bias, anx_mod, trans_lr = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model T[a, s]
    # Start neutral at 0.5 and learn toward observed transitions
    T = np.full((2, 2), 0.5)

    # Model-free action values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map mb_bias in [0,1] to an internal logit bias around 0 via atanh-like stretching
    # Simple linear center: mb_bias_c in [-1,1]
    mb_bias_c = 2.0 * mb_bias - 1.0
    # Anxiety effect centered around 0, scaled by anx_mod
    anx_centered = (2.0 * stai - 1.0) * (2.0 * anx_mod - 1.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])

        # Model-based stage-1 Q from learned transition model
        max_q2 = np.max(q2, axis=1)  # best option per second-stage state
        q1_mb = T @ max_q2  # expected value under learned transitions

        # Arbitration weight w in [0,1]: sigmoid of (bias + anxiety)
        # Higher anxiety reduces MB weight if anx_centered > 0 (can also flip depending on sign).
        # Use a smooth mapping via logistic function.
        arb_signal = 2.0 * (mb_bias_c - anx_centered)  # combine bias and anxiety
        w = 1.0 / (1.0 + np.exp(-arb_signal))
        w = np.clip(w, 0.0, 1.0)

        # Combine MB and MF for decision at stage 1
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Anxiety-modulated temperature (bounded to be non-negative implicitly by beta bounds)
        beta_eff = beta * (1.0 + 0.5 * (-anx_centered))

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta_eff * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        exp_q2s = np.exp(beta_eff * q2c)
        probs_2 = exp_q2s / np.sum(exp_q2s)
        p_choice_2[t] = probs_2[a2]

        # Learning: Stage-2 Q-learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Learning: Stage-1 MF bootstrapping toward realized second-stage value
        # Targets the current realized second-stage value (after the update).
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_q * td1

        # Update learned transition model row for chosen action using recency-weighted counts
        # Move probability toward observed state; enforce row normalization via two-value update
        T[a1, s] += trans_lr * (1.0 - T[a1, s])
        T[a1, 1 - s] += trans_lr * (0.0 - T[a1, 1 - s])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_q', 'beta', 'mb_bias', 'anx_mod', 'trans_lr']"
iter8_run0_participant12.json,cognitive_model2,452.19979448672814,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with anxiety-modulated loss aversion and rare-transition credit dampening.
    
    Stage-2 values are learned model-free and backed up to stage 1 via an eligibility trace.
    Outcomes are passed through an anxiety-modulated loss-aversion utility. Additionally,
    eligibility backup to stage 1 is dampened on rare transitions, with a strength that
    increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to modulate utility and credit assignment.
    model_parameters : list or array
        [alpha, beta, loss_av_base, anx_impact, trace]
        Bounds:
          alpha in [0,1]         : learning rate for Q-values
          beta in [0,10]         : inverse temperature
          loss_av_base in [0,1]  : baseline loss aversion (penalty for zero reward)
          anx_impact in [0,1]    : sensitivity of loss aversion and credit dampening to anxiety
          trace in [0,1]         : eligibility trace for backing up stage-2 to stage-1

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta, loss_av_base, anx_impact, trace = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss aversion: penalize zero reward
    loss_av = np.clip(loss_av_base * (1.0 + anx_impact * (2.0 * stai - 1.0)), 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])

        # Determine whether transition was common for chosen a1
        # Common if (A->X) or (U->Y)
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        is_rare = 0.0 if is_common else 1.0

        # Policies
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        exp_q2s = np.exp(beta * q2c)
        probs_2 = exp_q2s / np.sum(exp_q2s)
        p_choice_2[t] = probs_2[a2]

        # Utility transformation with loss aversion for zero reward (reward in {0,1})
        r = reward[t]
        util = r - (1.0 - r) * loss_av

        # Stage-2 MF update
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility backup to stage 1, dampened on rare transitions with anxiety
        damp = 1.0 - anx_impact * stai * is_rare
        lamb_eff = np.clip(trace * damp, 0.0, 1.0)
        q1[a1] += alpha * lamb_eff * pe2

        # Optional direct TD correction of stage-1 toward updated second-stage value
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * 0.5 * td1  # mild stabilization

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'loss_av_base', 'anx_impact', 'trace']"
iter8_run0_participant12.json,cognitive_model3,520.6316413071479,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based controller with anxiety- and surprise-adaptive temperature and lapse.
    
    A purely model-based (MB) planner uses the known transition structure (common=0.7).
    Stage-2 values are learned via a delta-rule. The softmax temperature and a lapse
    probability adapt to anxiety and to recent transition surprise (rare vs. common).
    Surprise raises or lowers exploitation depending on kappa_surprise, while anxiety
    reduces temperature and increases lapse.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1; modulates temperature and lapse.
    model_parameters : list or array
        [alpha, beta_base, lapse_base, anx_temp, kappa_surprise]
        Bounds:
          alpha in [0,1]           : learning rate for second-stage Q-values
          beta_base in [0,10]      : baseline inverse temperature
          lapse_base in [0,1]      : baseline lapse probability (uniform mixing)
          anx_temp in [0,1]        : anxiety sensitivity of temperature and lapse
          kappa_surprise in [0,1]  : weight of surprise on temperature and learning

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """"""
    alpha, beta_base, lapse_base, anx_temp, kappa_surprise = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed known transition structure
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    q2 = np.zeros((2, 2))  # second-stage Q-values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_surprise = 0.0  # previous-trial surprise (1=rare, 0=common)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])

        # MB stage-1 Qs via Bellman expectation with fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Adaptive temperature and lapse
        beta_eff = beta_base * (1.0 - anx_temp * stai) + kappa_surprise * prev_surprise * beta_base
        beta_eff = max(1e-8, beta_eff)  # keep positive

        lapse_eff = np.clip(lapse_base * (1.0 + anx_temp * stai), 0.0, 1.0)

        # Stage-1 policy with lapse
        q1c = q1_mb - np.max(q1_mb)
        exp_q1 = np.exp(beta_eff * q1c)
        softmax_1 = exp_q1 / np.sum(exp_q1)
        probs_1 = (1.0 - lapse_eff) * softmax_1 + lapse_eff * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with same adaptive parameters
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        exp_q2s = np.exp(beta_eff * q2c)
        softmax_2 = exp_q2s / np.sum(exp_q2s)
        probs_2 = (1.0 - lapse_eff) * softmax_2 + lapse_eff * 0.5
        p_choice_2[t] = probs_2[a2]

        # Learning with optional surprise modulation of learning rate on rare transitions
        r = reward[t]
        # Compute whether current transition is rare
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        is_rare = 0.0 if is_common else 1.0

        alpha_eff = np.clip(alpha * (1.0 + kappa_surprise * (0.5 - stai) * is_rare), 0.0, 1.0)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * pe2

        # Update surprise for next trial's temperature adaptation
        prev_surprise = is_rare

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta_base', 'lapse_base', 'anx_temp', 'kappa_surprise']"
iter8_run0_participant14.json,cognitive_model1,454.7856427997662,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 1: MB planning with a learned choice-kernel bias modulated by anxiety.
    
    Summary:
    - Stage 1: model-based planning using known transitions (0.7/0.3).
      Adds a choice-kernel bias (recency-driven tendency) learned from previous
      choices independently of reward, with strength amplified by anxiety.
    - Stage 2: model-free Q-learning with a single reward learning rate.
    
    Anxiety use:
    - Increases the effective strength of the choice-kernel bias at stage 1,
      reflecting stronger action inertia under higher anxiety.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for stage-2 Q-values, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - k_ck: choice-kernel learning rate (how fast the bias updates), in [0,1]
    - g_anx: anxiety gain scaling for the choice-kernel magnitude, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of reached planets (0=X, 1=Y)
    - action_2: array of second-stage choices (0/1) per reached planet
    - reward: array of rewards per trial (float)
    - stai: array-like with single float [0,1] anxiety score
    - model_parameters: array-like of params as above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_r, beta, k_ck, g_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: rows: spaceship [A(0), U(1)], cols: planet [X(0), Y(1)]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Q-values for second-stage per planet-alien
    q2 = np.zeros((2, 2))

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Choice kernel over first-stage actions (tendency to repeat independent of reward)
    ck = np.zeros(2)
    # Anxiety-modulated kernel gain (centered around medium anxiety ~0.41; slope by g_anx)
    # Higher anxiety -> stronger effect; clip to keep reasonable range
    ck_gain = 1.0 + g_anx * (stai - 0.41)
    ck_gain = np.clip(ck_gain, 0.0, 2.0)

    for t in range(n_trials):
        # Stage 1 MB values = T @ max_a2 Q2(s,a2)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add kernel bias to logits (scaled by anxiety)
        logits1 = beta * q1_mb + ck_gain * ck
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: softmax over Q2 at reached state
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Reward and updates
        r = reward[t]
        # Stage 2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update choice kernel toward the chosen action (recency inertia)
        # One-hot of chosen a1
        oh = np.array([0.0, 0.0])
        oh[a1] = 1.0
        ck += k_ck * (oh - ck)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'k_ck', 'g_anx']"
iter8_run0_participant14.json,cognitive_model2,559.9135543887138,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model 2: PearceâHall surprise-adaptive learning with anxiety-amplified surprise sensitivity.
    
    Summary:
    - Stage 1: model-based planning using known transitions (0.7/0.3).
    - Stage 2: model-free Q-learning with a surprise-adaptive learning rate:
      alpha_eff = alpha0 + k_ph_eff * |RPE|, where k_ph_eff scales with anxiety.
    - Surprise also slightly boosts exploration by adding a small, RPE-dependent
      entropy drive via the same gain (implemented by adjusting logits).
    
    Anxiety use:
    - Increases sensitivity to surprise (|RPE|), which increases the effective
      learning rate and a mild exploration push (akin to attention/arousal).
    
    Parameters (model_parameters):
    - alpha0: base learning rate for Q2 in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - k_ph: base PearceâHall gain (how much surprise modulates learning) in [0,1]
    - g_anx: anxiety gain scaling the PH effect in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of reached planets (0=X, 1=Y)
    - action_2: array of second-stage choices (0/1) per reached planet
    - reward: array of rewards per trial (float)
    - stai: array-like with single float [0,1] anxiety score
    - model_parameters: array-like of params as above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha0, beta, k_ph, g_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-scaled PH gain; higher anxiety -> stronger reliance on surprise
    k_ph_eff_scale = 1.0 + g_anx * (stai - 0.31)
    k_ph_eff_scale = np.clip(k_ph_eff_scale, 0.0, 2.0)

    for t in range(n_trials):
        # Stage 1 MB values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # For stage 2, compute PH-modulated exploration bump from last trial RPE at this state
        # We compute RPE online using the current trial's reward and then use alpha_eff and
        # a small exploration scaling on the logits before logging the probability.
        # First compute softmax probabilities given current q2:
        base_logits2 = beta * q2[s]
        base_logits2 -= np.max(base_logits2)

        # Sample action probability for logging before updating:
        base_probs2 = np.exp(base_logits2)
        base_probs2 /= (np.sum(base_probs2) + 1e-12)

        a2 = action_2[t]
        r = reward[t]
        # Compute RPE on chosen action
        rpe = r - q2[s, a2]
        surprise = abs(rpe)

        # Effective learning rate (capped at 1)
        alpha_eff = alpha0 + (k_ph * k_ph_eff_scale) * surprise
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Mild exploration bump proportional to surprise (entropy drive).
        # Implement as additive zero-mean perturbation to logits:
        # push non-chosen slightly up to encourage exploration when surprise is high.
        # Construct a centered vector v with +0.5 to non-chosen, -0.5 to chosen.
        v = np.array([0.5, 0.5])
        v[a2] = -0.5
        expl_strength = 0.2 * (k_ph * k_ph_eff_scale) * surprise  # bounded by parameters and surprise
        logits2 = base_logits2 + expl_strength * v

        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        p_choice_2[t] = probs2[a2]

        # Q2 update with surprise-adaptive learning rate
        q2[s, a2] += alpha_eff * rpe

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha0', 'beta', 'k_ph', 'g_anx']"
iter8_run0_participant15.json,cognitive_model1,574.228838487294,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated planning, leaky transition trust, and TD(Î») credit assignment.
    
    Core ideas:
    - Stage-2 values are learned model-free (TD(0)).
    - Stage-1 has both MF values and MB plan values; weight shifts with anxiety:
        w_mb = 1 - stai (higher anxiety => less planning).
    - Anxiety also reduces decisiveness: beta_eff = beta / (1 + c_anx * stai).
    - Imperfect belief in the transition model: a 'leak' blends the true transition with uniform,
      increasing with anxiety (leak_eff = mb_leak * stai).
    - TD(Î»): Stage-1 MF receives both the standard bootstrapped update from Q2 and an extra Î»-weighted
      update from the immediate Stage-2 prediction error.
    
    Parameters (all used; total=5):
    - eta: [0,1] Learning rate for both Stage-1 and Stage-2 value updates.
    - beta: [0,10] Inverse temperature for both stages (modulated by anxiety).
    - lam_e: [0,1] Eligibility trace strength to propagate Stage-2 PE into Stage-1 MF.
    - c_anx: [0,1] Anxiety scaling for temperature softening: beta_eff = beta / (1 + c_anx*stai).
    - mb_leak: [0,1] Degree to leak the transition model toward uniform; effective leak = mb_leak*stai.
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1} (0=X, 1=Y).
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [eta, beta, lam_e, c_anx, mb_leak].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    eta, beta, lam_e, c_anx, mb_leak = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # True transition structure (A->X common, U->Y common)
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]], dtype=float)

    # Anxiety-modulated effective transition belief (leak toward uniform)
    leak_eff = mb_leak * stai
    T_eff = (1.0 - leak_eff) * T_true + leak_eff * 0.5  # row-wise; 0.5 uniform to each state

    # Storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q2 = 0.5 * np.ones((2, 2))  # Stage-2 MF values per planet/state and action
    q1_mf = np.zeros(2)         # Stage-1 MF values (for A and U)

    # Anxiety-modulated arbitration and temperature
    w_mb = max(0.0, min(1.0, 1.0 - stai))  # more anxious => less MB
    beta_eff = beta / (1.0 + c_anx * stai + 1e-12)

    for t in range(n_trials):
        s = state[t]

        # Model-based plan at Stage-1: expected max Q2 under T_eff
        max_q2 = np.max(q2, axis=1)           # value of each planet
        q1_mb = T_eff @ max_q2                # expected value per spaceship

        # Hybrid Stage-1 value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        z1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learn from reward
        r = reward[t]

        # Stage-2 TD(0)
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta * pe2

        # Stage-1 MF update with TD(Î»): bootstrapped target + Î» * stage-2 PE
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        lam_eff = lam_e  # can be seen as baseline trace; anxiety already impacts via w_mb and beta
        q1_mf[a1] += eta * (pe1 + lam_eff * pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta', 'beta', 'lam_e', 'c_anx', 'mb_leak']"
iter8_run0_participant15.json,cognitive_model2,578.211679847747,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Utility-transformed learning with uncertainty bonus and anxiety-dampened exploration.
    
    Core ideas:
    - Stage-2 rewards are transformed through a concave utility u(r) = r^(rho_eff),
      where rho_eff increases with anxiety (more anxiety => more concavity => risk-averse).
    - Stage-2 exploration bonus based on value uncertainty: bonus ~ q*(1-q), scaled down by anxiety.
    - Stage-1 uses MB planning from the transformed Q2 values (expected max utility),
      with no explicit MF component to keep parameters parsimonious.
    - Anxiety reduces exploration bonus and slightly softens choice via beta_eff.
    
    Parameters (all used; total=5):
    - eta: [0,1] Learning rate for Stage-2 value learning.
    - beta: [0,10] Inverse temperature for both stages (modulated by anxiety).
    - rho0: [0,1] Baseline utility curvature; higher => more concave utility.
    - zeta_ent: [0,1] Baseline weight of uncertainty bonus at Stage-2.
    - tau_anx: [0,1] Strength by which anxiety increases risk aversion (utility curvature).
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [eta, beta, rho0, zeta_ent, tau_anx].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    eta, beta, rho0, zeta_ent, tau_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize Stage-2 values
    q2 = 0.5 * np.ones((2, 2))

    # Anxiety effects
    rho_eff = max(0.0, min(1.0, rho0 + tau_anx * stai))      # utility curvature
    zeta_eff = zeta_ent * (1.0 - stai)                       # anxious => less exploration bonus
    beta_eff = beta / (1.0 + 0.5 * stai)                     # modest softening with anxiety

    for t in range(n_trials):
        s = state[t]

        # Stage-2 uncertainty bonus (normalized: max uncertainty for Bernoulli is 0.25 at q=0.5)
        unc = q2[s] * (1.0 - q2[s])           # elementwise; max 0.25
        bonus = zeta_eff * (unc / 0.25)       # in [0, zeta_eff]
        q2_aug = q2[s] + bonus

        # Stage-2 policy
        z2 = beta_eff * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB: plan over max utility values at each planet
        max_q2 = np.max(q2 + (zeta_eff * ((q2 * (1 - q2)) / 0.25))[:, None] * 0.0, axis=1)
        # Note: Stage-1 uses the base q2 expectations (without adding a Stage-1 bonus),
        # focusing on expected best action utility per planet.
        q1_mb = T @ np.max(q2, axis=1)

        # Stage-1 policy
        z1 = beta_eff * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning with utility-transformed reward
        r = reward[t]
        u = (r ** rho_eff) if r >= 0.0 else -((-r) ** rho_eff)  # robust power utility; here r in [0,1]
        pe2 = u - q2[s, a2]
        q2[s, a2] += eta * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta', 'beta', 'rho0', 'zeta_ent', 'tau_anx']"
iter8_run0_participant15.json,cognitive_model3,553.5797523975847,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learning the transition model with anxiety-weighted 'safety' bias toward predictable spaceships.
    
    Core ideas:
    - The agent learns transition probabilities online (alpha_t) with forgetting (f_forget).
    - Stage-1 uses a hybrid of learned MB values and Stage-1 MF values, with MB weight higher when
      anxiety is low: w_mb = 0.5 + 0.5*(1 - stai).
    - An anxiety-weighted safety bias penalizes actions with higher transition entropy (uncertainty),
      capturing preference for predictable outcomes under anxiety.
    - Stage-2 values are learned model-free.
    
    Parameters (all used; total=5):
    - eta: [0,1] Learning rate for Stage-2 MF values and Stage-1 MF bootstrapping.
    - beta: [0,10] Inverse temperature for both stages.
    - alpha_t: [0,1] Learning rate for updating the transition matrix rows upon observing transitions.
    - f_forget: [0,1] Per-trial forgetting toward uniform for transition probabilities.
    - omega_safe: [0,1] Strength of safety bias; effective bias scales with stai.
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [eta, beta, alpha_t, f_forget, omega_safe].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    eta, beta, alpha_t, f_forget, omega_safe = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix; start near known structure but not exact
    T_learn = np.array([[0.6, 0.4],
                        [0.4, 0.6]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q2 = 0.5 * np.ones((2, 2))
    q1_mf = np.zeros(2)

    # Anxiety-modulated MB weight and safety bias
    w_mb = 0.5 + 0.5 * (1.0 - stai)          # in [0.5,1.0]; more anxiety => closer to 0.5
    safety_scale = omega_safe * stai          # stronger bias with higher anxiety

    for t in range(n_trials):
        s = state[t]

        # Stage-1 MB plan from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_learn @ max_q2

        # Safety bias: negative entropy of transitions per action (higher entropy => more penalty)
        # Entropy for binary distribution [p, 1-p]: H = -sum p log p (natural log).
        # Normalize by max entropy (ln 2) so bias in [0,1].
        ent = np.zeros(2)
        for a in range(2):
            p = np.clip(T_learn[a, 0], 1e-8, 1.0 - 1e-8)
            q = 1.0 - p
            H = -(p * np.log(p) + q * np.log(q))
            ent[a] = H / np.log(2.0)
        bias = -safety_scale * ent  # penalize uncertainty

        # Hybrid Stage-1 value + safety bias
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias

        # Stage-1 policy
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Reward learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * pe1

        # Transition model forgetting toward uniform
        T_learn = (1.0 - f_forget) * T_learn + f_forget * 0.5

        # Update chosen row of transition matrix toward observed state
        # Move probability mass toward the observed state s
        T_learn[a1, s] += alpha_t * (1.0 - T_learn[a1, s])
        other = 1 - s
        T_learn[a1, other] += alpha_t * (0.0 - T_learn[a1, other])

        # Renormalize rows to sum to 1 and keep probabilities in [eps, 1-eps]
        for a in range(2):
            row = T_learn[a]
            row = np.clip(row, 1e-8, 1.0)
            T_learn[a] = row / np.sum(row)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['eta', 'beta', 'alpha_t', 'f_forget', 'omega_safe']"
iter8_run0_participant18.json,cognitive_model1,468.9045471436043,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-gated arbitration, eligibility, and lapses.

    Description:
    - Stage-1 action values combine model-based (MB) and model-free (MF) estimates.
      The arbitration weight increases with lower anxiety; higher anxiety shifts weight away from MB.
    - Stage-2 policy is purely MF.
    - Reward prediction errors at stage-2 are assigned back to stage-1 via an eligibility parameter
      that is reduced by anxiety (less credit assignment under higher anxiety).
    - A small lapse probability increases with anxiety at both stages.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature for softmax in [0,10]
    - theta_mb: baseline MB weight in [0,1]
    - lambda0: baseline eligibility/credit assignment to stage-1 in [0,1]
    - epsilon: baseline lapse probability in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet (0/1)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, theta_mb, lambda0, epsilon)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, theta_mb, lambda0, epsilon = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (A->X, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q1_mf = np.zeros(2)        # MF stage-1 action values
    Q2 = np.zeros((2, 2))      # Stage-2 state-action values

    # Anxiety-modulated parameters
    w_eff = np.clip((1.0 - st) * theta_mb + st * 0.2, 0.0, 1.0)    # higher anxiety -> less MB
    lam_eff = np.clip(lambda0 * (1.0 - 0.7 * st), 0.0, 1.0)        # higher anxiety -> weaker credit assignment
    eps_lapse = np.clip(epsilon * st, 0.0, 1.0)                    # higher anxiety -> more lapses

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 MB estimate: expected max value at next state
        max_Q2 = np.max(Q2, axis=1)       # max over aliens for each planet
        Q1_mb = T @ max_Q2                # expected value for A and U

        # Arbitration
        Q1 = (1.0 - w_eff) * Q1_mf + w_eff * Q1_mb

        # Stage-1 policy with lapses
        pref1 = Q1
        pref1 -= np.max(pref1)  # stabilize
        exp1 = np.exp(beta * pref1)
        soft1 = exp1 / (np.sum(exp1) + eps)
        probs1 = (1.0 - eps_lapse) * soft1 + eps_lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapses (pure MF)
        pref2 = Q2[s]
        pref2 -= np.max(pref2)
        exp2 = np.exp(beta * pref2)
        soft2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - eps_lapse) * soft2 + eps_lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 TD(0)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 credit assignment: combine bootstrapping and direct outcome PE
        # Bootstrapped error from second-stage value
        delta1_boot = Q2[s, a2] - Q1_mf[a1]
        # Update MF value at stage-1 with eligibility scaled by anxiety
        Q1_mf[a1] += alpha * lam_eff * (delta1_boot + delta2)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'theta_mb', 'lambda0', 'epsilon']"
iter8_run0_participant18.json,cognitive_model2,310.31862818580964,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive temperature with anxiety, stage-1 perseveration, and Q-forgetting.

    Description:
    - Anxiety reduces choice precision via an adaptive inverse temperature:
      beta_eff = beta * (1 - k_temp * stai), bounded below by a small floor.
    - Stage-1 includes a perseveration bias that decays over trials; the bias strength is
      reduced by anxiety.
    - Stage-2 is MF; both stage-2 values and perseveration traces are subject to mild forgetting
      toward zero (recency), controlled by tau_forget.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: base inverse temperature in [0,10]
    - k_temp: anxiety sensitivity of temperature in [0,1] (higher -> more reduction in beta)
    - tau_forget: forgetting/decay parameter in [0,1] applied to Q2 and perseveration traces
    - pers: baseline perseveration strength in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, k_temp, tau_forget, pers)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, k_temp, tau_forget, pers = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Perseveration trace for stage-1 actions
    trace1 = np.zeros(2)

    # Anxiety-modulated parameters
    beta_eff = max(1e-3, beta * (1.0 - k_temp * st))  # higher anxiety -> lower precision
    stick_strength = pers * (1.0 - st)                # higher anxiety -> weaker perseveration
    decay = np.clip(tau_forget, 0.0, 1.0)             # used for Q2 and trace forgetting

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1: MB lookahead based on Q2, combined with MF and perseveration bias
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        pref1 = 0.5 * Q1_mf + 0.5 * Q1_mb + stick_strength * trace1

        centered1 = pref1 - np.max(pref1)
        exp1 = np.exp(beta_eff * centered1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2: MF softmax with adaptive temperature
        pref2 = Q2[s]
        centered2 = pref2 - np.max(pref2)
        exp2 = np.exp(beta_eff * centered2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning
        # Apply forgetting (toward zero) before updating to emphasize recency
        Q2 *= (1.0 - decay)

        # Stage-2 TD(0)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF update with bootstrapping from Q2
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Perseveration trace update with decay
        trace1 *= (1.0 - decay)
        trace1[a1] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'k_temp', 'tau_forget', 'pers']"
iter8_run0_participant21.json,cognitive_model1,471.93171859855124,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-surprise-gated hybrid control with anxiety-modulated planning and credit assignment.

    Idea:
    - Stage-2 (aliens) values are learned via delta-rule.
    - Stage-1 (spaceships) uses a hybrid of model-based (MB) and model-free (MF) values.
    - Anxiety reduces reliance on planning (MB) and increases MF credit assignment following rare transitions.
    - A bootstrapping parameter controls how strongly stage-2 values propagate to stage-1 MF values.

    Parameters (bounds):
    - model_parameters[0] = a2 (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = w_pln0 (0 to 1): baseline weight on model-based values at stage 1
    - model_parameters[3] = anx_trn (0 to 1): anxiety sensitivity; higher stai reduces MB weight and boosts MF credit after rare transitions
    - model_parameters[4] = chi (0 to 1): bootstrapping strength to update stage-1 MF from stage-2

    Inputs:
    - action_1: array-like ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like floats, received coins per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    a2, beta, w_pln0, anx_trn, chi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: rows are spaceships (A=0, U=1); columns are planets (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))   # stage-2 values for each planet and alien
    q1_mf = np.zeros(2)     # model-free stage-1 values

    # Choice likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated planning weight
    w_eff = np.clip(w_pln0 - anx_trn * stai_val, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based stage-1 values from current stage-2 values
        max_q2 = np.max(q2, axis=1)            # best alien per planet
        q1_mb = transition_matrix @ max_q2     # MB spaceship values

        # Hybrid Q for stage 1
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (on reached planet)
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2_t = action_2[t]
        p_choice_2[t] = probs_2[a2_t]

        # Outcome
        r = reward[t]

        # Update stage-2 value
        delta2 = r - q2[s, a2_t]
        q2[s, a2_t] += a2 * delta2

        # Transition surprise for the experienced transition on this trial
        # P_common given chosen spaceship a1 and planet s
        p_trans = transition_matrix[a1, s]
        # surprise is higher for rare transitions (0.7 for rare, 0.3 for common)
        surprise = 1.0 - p_trans
        # Scale surprise into [0,1] with common->0 and rare->1
        surprise_scaled = (surprise - 0.3) / (0.7 - 0.3)

        # Anxiety boosts MF credit assignment after rare transitions
        chi_eff = chi * (1.0 + anx_trn * surprise_scaled)
        chi_eff = np.clip(chi_eff, 0.0, 1.0)

        # Stage-1 MF bootstrapping
        delta1 = (q2[s, a2_t]) - q1_mf[a1]
        q1_mf[a1] += chi_eff * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['a2', 'beta', 'w_pln0', 'anx_trn', 'chi']"
iter8_run0_participant21.json,cognitive_model2,532.6207929611533,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Directed uncertainty exploration at stage-2 and anxiety-weighted pessimism at stage-1.

    Idea:
    - Stage-2 values learned with a delta-rule. A directed exploration bonus encourages sampling uncertain aliens.
    - The exploration bonus shrinks with higher anxiety.
    - Stage-1 model-based values are computed from stage-2, but are pessimistically attenuated in proportion to anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = u_base (0 to 1): baseline magnitude of uncertainty bonus at stage-2
    - model_parameters[3] = anx_unc (0 to 1): anxiety sensitivity for exploration bonus (higher stai reduces bonus)
    - model_parameters[4] = rho_pess (0 to 1): maximum pessimism weight applied to stage-1 values, scaled by stai

    Inputs:
    - action_1: array-like ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like floats, received coins per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, u_base, anx_unc, rho_pess = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))          # stage-2 values
    n_sa = np.zeros((2, 2))        # visit counts for uncertainty estimates

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration magnitude and pessimism
    u_eff = u_base * (1.0 - anx_unc * stai_val)         # less directed exploration with higher anxiety
    u_eff = max(0.0, u_eff)
    rho_eff = np.clip(rho_pess * stai_val, 0.0, 1.0)    # more pessimism with higher anxiety

    for t in range(n_trials):
        # Stage-1 values from MB planning over pessimistically transformed stage-2
        max_q2 = np.max(q2, axis=1)           # best alien per planet
        # Pessimism shrinks values toward 0 (minimum attainable reward)
        v_planet = (1.0 - rho_eff) * max_q2   # min reward assumed 0
        q1_mb = transition_matrix @ v_planet

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with directed uncertainty bonus
        s = state[t]
        # Uncertainty proxy per alien in current state
        unc = 1.0 / np.sqrt(n_sa[s] + 1.0)    # higher for less-sampled actions
        q2_net = q2[s] + u_eff * unc
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2_t = action_2[t]
        p_choice_2[t] = probs_2[a2_t]

        # Outcome and learning
        r = reward[t]
        delta2 = r - q2[s, a2_t]
        q2[s, a2_t] += alpha * delta2
        n_sa[s, a2_t] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'u_base', 'anx_unc', 'rho_pess']"
iter8_run0_participant21.json,cognitive_model3,532.6207929611534,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based control with anxiety- and surprise-modulated lapse rate.

    Idea:
    - Stage-2 values learned via delta-rule.
    - Both stages use model-based action values, but choices are corrupted by a lapse process.
    - Lapse probability increases with anxiety and with recent surprise (unsigned reward prediction error magnitude).

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax component at both stages
    - model_parameters[2] = z_base (0 to 1): baseline lapse probability
    - model_parameters[3] = anx_gain (0 to 1): sensitivity of lapse to anxiety (multiplies stai)
    - model_parameters[4] = kappa_pe (0 to 1): sensitivity of lapse to unsigned reward PE from previous trial

    Inputs:
    - action_1: array-like ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like floats, received coins per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta, z_base, anx_gain, kappa_pe = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous unsigned PE to modulate lapse
    prev_abs_pe = 0.0

    for t in range(n_trials):
        # Model-based stage-1 values from current stage-2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Compute dynamic lapse probability
        eps_t = z_base + anx_gain * stai_val + kappa_pe * prev_abs_pe
        eps_t = float(np.clip(eps_t, 0.0, 1.0))

        # Stage-1 policy: mixture of softmax and uniform due to lapse
        q1c = q1_mb - np.max(q1_mb)
        sm1 = np.exp(beta * q1c)
        sm1 = sm1 / np.sum(sm1)
        probs_1 = (1.0 - eps_t) * sm1 + eps_t * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy: mixture of softmax and uniform due to lapse
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        sm2 = np.exp(beta * q2c)
        sm2 = sm2 / np.sum(sm2)
        probs_2 = (1.0 - eps_t) * sm2 + eps_t * 0.5
        a2_t = action_2[t]
        p_choice_2[t] = probs_2[a2_t]

        # Outcome and learning at stage-2
        r = reward[t]
        delta2 = r - q2[s, a2_t]
        q2[s, a2_t] += alpha * delta2

        # Update surprise signal for next trial's lapse modulation
        prev_abs_pe = min(1.0, abs(delta2))  # rewards are in [0,1], keep within [0,1]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'z_base', 'anx_gain', 'kappa_pe']"
iter8_run0_participant24.json,cognitive_model1,462.9760127459916,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with anxiety-reduced model-based control and MF eligibility.
    
    Summary
    - Stage-1 choice values are a mixture of model-based and model-free values.
    - Second-stage values are learned model-free from rewards.
    - An eligibility-like parameter propagates second-stage value to stage-1 MF values.
    - Anxiety reduces the model-based mixture weight linearly.
    
    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (in visited state; X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received rewards per trial (e.g., 0/1).
    stai : array-like of float in [0,1]
        Trait anxiety score; uses stai[0].
    model_parameters : array-like
        [alpha, beta, pi_mb, xi_anx, trace]
        - alpha in [0,1]: learning rate for MF value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - pi_mb in [0,1]: baseline model-based mixture weight.
        - xi_anx in [0,1]: degree to which anxiety reduces model-based control.
        - trace in [0,1]: eligibility-like weight for propagating second-stage value to stage-1 MF.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha, beta, pi_mb, xi_anx, trace = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure: A->X common (0.7), U->Y common (0.7)
    T = np.array([[0.7, 0.3],  # from A to (X,Y)
                  [0.3, 0.7]]) # from U to (X,Y)

    # Initialize values
    q1_mf = np.zeros(2)        # stage-1 MF action values
    q2 = np.zeros((2, 2))      # stage-2 MF action values by state
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-reduced model-based weight
    w_mb = pi_mb * (1.0 - xi_anx * s)
    w_mb = min(1.0, max(0.0, w_mb))

    for t in range(n_trials):
        # Model-based Q at stage 1: transition-expected max Q at stage 2
        max_q2 = np.max(q2, axis=1)  # max over actions per state
        q1_mb = T @ max_q2

        # Mixture policy at stage 1
        q1_mix = (1.0 - w_mb) * q1_mf + w_mb * q1_mb
        prefs1 = q1_mix
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in visited state
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Update stage-2 MF values
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha * pe2

        # Update stage-1 MF with an eligibility-like backup from the realized second-stage value
        backed_value = q2[st, a2]
        pe1 = backed_value - q1_mf[a1]
        q1_mf[a1] += alpha * trace * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'pi_mb', 'xi_anx', 'trace']"
iter8_run0_participant24.json,cognitive_model2,462.29141178660666,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive-MF with anxiety-dampened transition-based heuristic bias and volatility-gated learning.
    
    Summary
    - Pure model-free SARSA(0) values at both stages.
    - Learning rate adapts to an online estimate of outcome volatility (EWMA of |PE|).
    - First-stage policy includes a heuristic transition-based bias (stay after reward+common;
      switch after reward+rare), whose strength is reduced by anxiety.
    
    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (in visited state).
    reward : array-like of float
        Received rewards per trial (e.g., 0/1).
    stai : array-like of float in [0,1]
        Trait anxiety score; uses stai[0].
    model_parameters : array-like
        [mu0, beta, phi_vol, tau_v, zeta_tr]
        - mu0 in [0,1]: baseline learning rate for MF updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - phi_vol in [0,1]: gain on volatility to increase learning rate.
        - tau_v in [0,1]: EWMA smoothing for volatility (higher -> faster tracking).
        - zeta_tr in [0,1]: magnitude of transition-based heuristic bias at stage 1.
          Effective bias is zeta_tr*(1 - stai), so anxiety dampens the heuristic.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    mu0, beta, phi_vol, tau_v, zeta_tr = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Volatility tracker (EWMA of absolute PE at stage 2)
    vol = 0.0

    # For heuristic bias we need previous trial info
    prev_a1 = None
    prev_st = None
    prev_r = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage heuristic transition-based bias
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            # Common if action matches state index under the canonical mapping
            was_common = (prev_a1 == prev_st)  # A->X (0->0), U->Y (1->1) is common
            b = zeta_tr * (1.0 - s)
            if prev_r is not None and prev_r >= 0.5:
                if was_common:
                    # Reward + common: bias to repeat previous action
                    bias1[prev_a1] += b
                else:
                    # Reward + rare: bias to switch (favor the other action)
                    bias1[1 - prev_a1] += b
            else:
                # If not rewarded, apply the opposite (mild) heuristic: avoid previous mapping
                # Here we attenuate by 0.5 to keep it subtle
                b2 = 0.5 * b
                if was_common:
                    bias1[1 - prev_a1] += b2
                else:
                    bias1[prev_a1] += b2

        # Stage 1 policy
        prefs1 = q1 + bias1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in visited state
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning with adaptive learning rate based on volatility
        r = reward[t]
        pe2 = r - q2[st, a2]
        vol = (1.0 - tau_v) * vol + tau_v * abs(pe2)
        eff_alpha = mu0 + phi_vol * vol * (0.5 + 0.5 * s)  # anxiety amplifies volatility gating
        eff_alpha = max(0.0, min(1.0, eff_alpha))

        # Update stage 2
        q2[st, a2] += eff_alpha * pe2

        # Update stage 1 via SARSA-style bootstrap from realized second-stage value
        pe1 = q2[st, a2] - q1[a1]
        q1[a1] += eff_alpha * pe1

        # Carry previous info
        prev_a1 = a1
        prev_st = st
        prev_r = r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['mu0', 'beta', 'phi_vol', 'tau_v', 'zeta_tr']"
iter8_run0_participant24.json,cognitive_model3,462.97601274599185,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-representation (SR) enhanced planning with anxiety-shortened horizon and MF critic.
    
    Summary
    - Learns second-stage MF values from reward.
    - Learns a first-stage successor map over second-stage states (SR over states given action).
    - Converts SR into action values by multiplying with the current state values (max over actions).
    - Mixes SR-based values with MF first-stage values.
    - Anxiety shortens the effective planning horizon by reducing the discount factor in SR.
    
    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (in visited state).
    reward : array-like of float
        Received rewards per trial (e.g., 0/1).
    stai : array-like of float in [0,1]
        Trait anxiety score; uses stai[0].
    model_parameters : array-like
        [alpha_v, beta, disc0, chi_stai, kappa_mix]
        - alpha_v in [0,1]: learning rate for both MF critic and SR map.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - disc0 in [0,1]: baseline discount factor used in SR updates.
        - chi_stai in [0,1]: how much anxiety reduces the discount (shorter horizon).
          Effective discount is disc = disc0 * (1 - chi_stai * stai).
        - kappa_mix in [0,1]: mixture weight on SR-derived Q at stage 1
          (1-kappa_mix) is the weight on MF stage-1 Q.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_v, beta, disc0, chi_stai, kappa_mix = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Values
    q1_mf = np.zeros(2)    # MF first-stage values
    q2 = np.zeros((2, 2))  # MF second-stage values

    # SR: mapping from first-stage actions -> discounted occupancy of second-stage states
    # M[a] is a length-2 vector over states [X, Y]
    M = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective discount reduced by anxiety
    disc = disc0 * (1.0 - chi_stai * s)
    disc = min(1.0, max(0.0, disc))

    for t in range(n_trials):
        # Compute SR-based Q for stage 1 from current critic of states
        V_states = np.max(q2, axis=1)  # value of each second-stage state as max over its actions
        q1_sr = M @ V_states

        # Mix SR-based with MF values for stage 1
        q1_mix = (1.0 - kappa_mix) * q1_mf + kappa_mix * q1_sr

        # Stage 1 policy
        prefs1 = q1_mix
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in visited state
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 (MF critic)
        r = reward[t]
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_v * pe2

        # Update SR for the chosen stage-1 action toward the observed next-state occupancy
        # Here the SR target is: one-hot(next_state) + disc * 0 (terminal), so effectively learning
        # the (discounted) transition probabilities with discount shaping.
        onehot_st = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        target = onehot_st  # terminal after second stage
        pe_sr = target - M[a1]
        M[a1] += alpha_v * pe_sr  # TD(0) on SR given terminal at second stage

        # Update MF first-stage value toward realized second-stage action value
        backed = q2[st, a2]
        pe1 = backed - q1_mf[a1]
        q1_mf[a1] += alpha_v * pe1

        # Optional: very small discount leakage (if disc<1, encourage sharper mapping)
        # Use discount implicitly by shrinking off-diagonal mass each trial
        if disc < 1.0:
            M[a1] = disc * M[a1] + (1.0 - disc) * target

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_v', 'beta', 'disc0', 'chi_stai', 'kappa_mix']"
iter8_run0_participant29.json,cognitive_model1,507.82935955553415,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Counterfactual second-stage learning with anxiety-damped transition confidence.

    Idea
    - First-stage decisions are model-based using a fixed canonical transition (0.7 common),
      but the agent's confidence in this model is imperfect and is modulated by anxiety.
      Lower confidence blends the transition toward an uninformative 0.5/0.5 mapping.
    - Second-stage values are learned model-free with both factual and counterfactual
      (fictive) updates. The counterfactual update strength increases with anxiety.

    Parameters (all floats)
    - alpha: [0,1] learning rate for MF values (both stages)
    - beta:  [0,10] inverse temperature for both stages
    - trans_conf: [0,1] baseline confidence in the canonical transition structure
                  (1.0 = fully trust 0.7/0.3; 0.0 = assume 0.5/0.5)
    - cf_rate: [0,1] baseline rate of counterfactual update to the unchosen second-stage action
    - anx_cf_gain: [0,1] how much anxiety amplifies counterfactual learning and reduces
                   transition confidence

    Inputs
    - action_1: array-like of ints {0,1} (0=A, 1=U)
    - state:    array-like of ints {0,1} (0=X, 1=Y)
    - action_2: array-like of ints {0,1} within reached state
    - reward:   array-like of floats in [0,1]
    - stai:     array-like with one float in [0,1] (anxiety score)
    - model_parameters: iterable of 5 floats as above

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, trans_conf, cf_rate, anx_cf_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Canonical transition: A->X common, U->Y common
    base_T = np.array([[0.7, 0.3],  # A to [X, Y]
                       [0.3, 0.7]])  # U to [X, Y]

    # Anxiety reduces confidence in the canonical transition; blend toward 0.5/0.5
    conf_eff = max(0.0, min(1.0, trans_conf * (1.0 - 0.6 * anx_cf_gain * stai_val)))
    T = 0.5 + conf_eff * (base_T - 0.5)

    # Anxiety amplifies counterfactual learning
    cf_eff = max(0.0, min(1.0, cf_rate * (1.0 + anx_cf_gain * stai_val)))

    # Action values
    q1 = np.zeros(2)         # stage-1 MF cache (bootstrapped from Q2)
    q2 = np.zeros((2, 2))    # stage-2 MF values: state in {X=0,Y=1}, action in {0,1}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Model-based evaluation for stage 1 using current Q2 and (anxiety-damped) transition
        max_q2 = np.max(q2, axis=1)         # shape (2,)
        q1_mb = T @ max_q2                  # shape (2,)

        # Combine MB evaluation with MF cache (simple average that is implicitly learned through q1)
        # Use purely MB for policy but include MF q1 in learning target below.
        logits1 = beta * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        # Stage-2 policy
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 learning: factual
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-2 counterfactual learning: unchosen action in same state moves toward 1 - r
        a2_alt = 1 - a2
        pe2_cf = (1.0 - r) - q2[s, a2_alt]
        q2[s, a2_alt] += cf_eff * pe2_cf

        # Stage-1 MF bootstrapping from current second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'trans_conf', 'cf_rate', 'anx_cf_gain']"
iter8_run0_participant29.json,cognitive_model2,312.5973645223257,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free first-stage with reward-contingent recency bias gated by anxiety.

    Idea
    - Stage-2 values are learned model-free.
    - Stage-1 choice is primarily model-free (bootstrapped from Q2) but is influenced by a
      reward-contingent recency bias that depends on whether the last transition was common
      or rare. Anxiety strengthens the recency bias and speeds its accumulation.
      This produces stronger ""win-stay/lose-switch"" after common transitions (and the
      opposite after rare), modulated by anxiety.

    Parameters (all floats)
    - alpha: [0,1] learning rate for MF values (both stages)
    - beta:  [0,10] inverse temperature for both stages
    - recency_rate: [0,1] update/decay rate for the first-stage recency bias
    - common_gain:  [0,1] scales how much a rewarded common transition increases bias
                    relative to rare transitions (signed)
    - anx_recency_gain: [0,1] how strongly anxiety amplifies the recency learning rate
                        and its impact on choice

    Inputs
    - action_1: array-like of ints {0,1} (0=A, 1=U)
    - state:    array-like of ints {0,1} (0=X, 1=Y)
    - action_2: array-like of ints {0,1} within reached state
    - reward:   array-like of floats in [0,1]
    - stai:     array-like with one float in [0,1] (anxiety score)
    - model_parameters: iterable of 5 floats as above

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, recency_rate, common_gain, anx_recency_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Stage values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # First-stage recency bias per action (dynamic)
    b1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Effective recency learning rate and bias strength grow with anxiety
    rec_lr = max(0.0, min(1.0, recency_rate * (1.0 + anx_recency_gain * stai_val)))
    bias_strength = (0.5 + 0.5 * anx_recency_gain * stai_val)  # in [0.5,1]

    for t in range(n_trials):
        # Stage-1 policy: MF q1 plus recency bias
        logits1 = q1 + bias_strength * b1
        logits1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Stage-2 policy: MF q2
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 MF learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF bootstrapping from the obtained second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Update recency bias: decay then add signed increment based on reward and transition type
        # Determine if the observed transition was common (A->X or U->Y)
        is_common = 1.0 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0.0
        sign = (2.0 * is_common - 1.0)  # +1 for common, -1 for rare
        # Reward-contingent increment: rewarded common -> positive toward chosen; rewarded rare -> negative
        inc = r * sign * (2.0 * common_gain - 1.0)

        # Decay and update bias
        b1 = (1.0 - rec_lr) * b1
        b1[a1] += rec_lr * inc
        # Also apply a small symmetric opposite update to the unchosen action to keep biases centered
        b1[1 - a1] -= rec_lr * inc * 0.5

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha', 'beta', 'recency_rate', 'common_gain', 'anx_recency_gain']"
iter8_run0_participant29.json,cognitive_model3,489.85508201187514,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Transition-uncertainty-driven exploration at stage 1 with anxiety-suppressed optimism.

    Idea
    - The agent learns second-stage reward values model-free.
    - Simultaneously, it keeps simple Dirichlet-like counts of the transition mapping for
      each first-stage action to estimate p(X|A) and p(X|U).
    - Stage-1 evaluation is model-based using these learned transition probabilities.
      Additionally, an exploration bonus encourages sampling actions whose transition
      probabilities are more uncertain (close to 0.5). Anxiety suppresses this optimism.
    - Stage-2 choice includes a mild choice-trace kernel with its own learning/decay rate.

    Parameters (all floats)
    - alpha: [0,1] learning rate for MF values (both stages)
    - beta:  [0,10] inverse temperature for both stages
    - optimism: [0,1] base strength of the transition-uncertainty exploration bonus at stage 1
    - trace_lr: [0,1] learning/decay rate for the stage-2 choice trace kernel
    - anx_explore: [0,1] how strongly anxiety suppresses the optimism bonus and weakens the trace

    Inputs
    - action_1: array-like of ints {0,1} (0=A, 1=U)
    - state:    array-like of ints {0,1} (0=X, 1=Y)
    - action_2: array-like of ints {0,1} within reached state
    - reward:   array-like of floats in [0,1]
    - stai:     array-like with one float in [0,1] (anxiety score)
    - model_parameters: iterable of 5 floats as above

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, optimism, trace_lr, anx_explore = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Second-stage MF values and choice trace
    q2 = np.zeros((2, 2))
    k2 = np.zeros((2, 2))  # choice trace at stage 2

    # First-stage transition counts: for each action a in {0,1}, counts of reaching X and Y
    # Start with symmetric pseudocounts (1,1) for stability
    trans_counts = np.ones((2, 2))  # rows: action (A,U), cols: state (X,Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety suppresses optimism and weakens choice trace usage
    opt_eff = max(0.0, optimism * (1.0 - 0.8 * anx_explore * stai_val))
    trace_eff = max(0.0, min(1.0, trace_lr * (1.0 - 0.5 * anx_explore * stai_val)))

    for t in range(n_trials):

        # Estimate transition probabilities from counts
        p_X_given_A = trans_counts[0, 0] / np.sum(trans_counts[0])
        p_X_given_U = trans_counts[1, 0] / np.sum(trans_counts[1])
        pX = np.array([p_X_given_A, p_X_given_U])  # shape (2,)
        pY = 1.0 - pX

        # Model-based Q for stage 1 using learned transitions and current Q2
        max_q2 = np.max(q2, axis=1)  # [Q(X,*).max, Q(Y,*).max]
        q1_mb = pX * max_q2[0] + pY * max_q2[1]

        # Exploration bonus based on transition uncertainty (max at p=0.5)
        # bonus = 1 - |p - 0.5|, in [0.5,1]; center to [0,1] by subtracting 0.5 and scaling
        trans_unc = 1.0 - np.abs(pX - 0.5)               # in [0.5,1]
        bonus = opt_eff * (2.0 * (trans_unc - 0.5))      # in [0, opt_eff]

        logits1 = beta * (q1_mb + bonus - np.max(q1_mb + bonus))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Stage-2 policy includes a small choice-trace kernel
        logits2 = q2[s] + trace_eff * (2.0 * k2[s] - 1.0)
        logits2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update stage-2 choice trace: decay within state, reinforce chosen action
        k2[s] = (1.0 - trace_eff) * k2[s]
        k2[s, a2] += trace_eff

        # Update transition counts based on observed transition for the chosen first-stage action
        # Increment the count of the reached state for the chosen action
        trans_counts[a1, s] += 1.0

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['alpha', 'beta', 'optimism', 'trace_lr', 'anx_explore']"
iter8_run0_participant3.json,cognitive_model1,387.24589971299474,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated surprise-driven switching with learned transitions (pure model-based at stage-1).

    Mechanism:
    - Stage-2 values Q2(s2, a2) learned with a single learning rate lr_q.
    - Transition model T(a1 -> s2) is learned with learning rate lr_T (row-wise exponential recency-weighting).
    - Stage-1 uses model-based values: Q1_MB(a1) = sum_s2 T[a1, s2] * max_a2 Q2[s2, a2].
    - Perseveration (stay bias) k_stay at both stages; after a rare transition on the previous trial,
      the effective stage-1 stay bias is reduced to encourage switching. This reduction is amplified by anxiety.
    - Stage-2 perseveration is attenuated by anxiety (higher anxiety -> less stickiness).

    Parameters and bounds:
    - model_parameters = (lr_q, beta, k_stay, b_switch, lr_T)
        lr_q    in [0,1]: learning rate for Q2
        beta    in [0,10]: inverse temperature for softmax at both stages
        k_stay  in [0,1]: baseline perseveration (stay) strength
        b_switch in [0,1]: strength by which a previous rare transition reduces stay at stage-1; scaled by anxiety
        lr_T    in [0,1]: learning rate for transition model T

    Inputs:
    - action_1: int array of shape (n_trials,), choices at stage-1 (0=A, 1=U)
    - state:    int array of shape (n_trials,), reached second-stage planet (0=X, 1=Y)
    - action_2: int array of shape (n_trials,), choices at stage-2 (0 or 1)
    - reward:   float array of shape (n_trials,), obtained reward in [0,1]
    - stai:     float array with one element in [0,1], anxiety score
    - model_parameters: tuple/list as defined above

    Returns:
    - Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """"""
    lr_q, beta, k_stay, b_switch, lr_T = model_parameters
    n_trials = len(action_1)
    anx = float(stai[0])

    # Initialize values and transition model
    q2 = np.zeros((2, 2), dtype=float)        # Q2[s2, a2]
    T = np.array([[0.7, 0.3],                 # start from instructed/common structure
                  [0.3, 0.7]] , dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    prev_a1 = -1
    prev_a2 = -1
    prev_rare = 0.0  # indicator for whether previous transition was rare under prior T

    for t in range(n_trials):
        # Compute model-based Q at stage-1
        max_q2 = np.max(q2, axis=1)         # value of each second-stage state
        q1_mb = T @ max_q2                  # shape (2,)

        # Stage-1 perseveration with anxiety-modulated surprise-driven switching
        stick1 = np.zeros(2, dtype=float)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # If last transition was rare, reduce stay bias; anxiety amplifies this reduction
        switch_amp = b_switch * (0.5 + 0.5 * anx)   # in [0,1]
        k1_eff = k_stay * (1.0 - switch_amp * prev_rare)

        logits1 = beta * q1_mb + k1_eff * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with anxiety-attenuated perseveration
        s2 = int(state[t])
        stick2 = np.zeros(2, dtype=float)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        k2_eff = k_stay * (1.0 - 0.7 * anx)  # higher anxiety -> less stickiness at stage-2
        logits2 = beta * q2[s2] + k2_eff * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = float(reward[t])

        # Determine whether current transition was rare under the pre-update T
        # Rare if probability of observed state given chosen action is less than 0.5
        prev_rare = 1.0 if T[a1, s2] < 0.5 else 0.0

        # Update transition model T for the chosen action row towards the observed state
        onehot_s = np.array([1.0 if i == s2 else 0.0 for i in range(2)], dtype=float)
        T[a1] = (1.0 - lr_T) * T[a1] + lr_T * onehot_s
        # Renormalize row to guard against numerical drift
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Update stage-2 Q
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += lr_q * delta2

        # Persist choices for perseveration
        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['lr_q', 'beta', 'k_stay', 'b_switch', 'lr_T']"
iter8_run0_participant3.json,cognitive_model2,inf,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-inflated epsilon-exploration and risk-sensitive utility.

    Mechanism:
    - Stage-2 utility uses a concave power transform u(r) = r^gamma, with gamma decreasing as anxiety rises.
    - Stage-2 Q2(s2, a2) is learned via TD on utility u(r).
    - Stage-1 combines model-based (MB) values from a fixed transition structure (0.7/0.3) and
      model-free (MF) values Q1_MF updated by bootstrapping from the reached second-stage value.
    - Mixture weight omega is reduced by anxiety (higher anxiety -> less MB).
    - Policies at both stages use softmax with inverse temperature beta, blended with epsilon-greedy;
      epsilon increases with anxiety, promoting more random exploration.

    Parameters and bounds:
    - model_parameters = (lr_q, beta, omega_base, nu_u, eps0)
        lr_q       in [0,1]: learning rate for Q2 and Q1_MF
        beta       in [0,10]: inverse temperature
        omega_base in [0,1]: baseline MB weight at stage-1
        nu_u       in [0,1]: controls utility concavity; gamma = 0.5 + 0.5*nu_u*(1 - stai)
        eps0       in [0,1]: baseline epsilon; epsilon_eff = eps0 * (0.5 + 0.5*stai)

    Inputs:
    - action_1, state, action_2, reward: arrays per trial (see task)
    - stai: array with one element in [0,1]
    - model_parameters: tuple/list as defined above

    Returns:
    - Negative log-likelihood of the observed choices.
    """"""
    lr_q, beta, omega_base, nu_u, eps0 = model_parameters
    n_trials = len(action_1)
    anx = float(stai[0])

    # Fixed transition structure as instructed/common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float)  # Q2[s2, a2]
    q1_mf = np.zeros(2, dtype=float)    # model-free Q at stage-1

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    # Anxiety-modulated parameters
    omega = float(np.clip(omega_base * (1.0 - 0.5 * anx), 0.0, 1.0))
    epsilon_eff = float(np.clip(eps0 * (0.5 + 0.5 * anx), 0.0, 1.0))
    gamma_util = 0.5 + 0.5 * nu_u * (1.0 - anx)   # in [0.5, 1.0]

    for t in range(n_trials):
        # MB component at stage-1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy: epsilon-softmax
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        probs1 = (1.0 - epsilon_eff) * soft1 + 0.5 * epsilon_eff
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: epsilon-softmax on Q2
        s2 = int(state[t])
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        probs2 = (1.0 - epsilon_eff) * soft2 + 0.5 * epsilon_eff
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome with risk-sensitive utility
        r = float(reward[t])
        u = (r + eps) ** gamma_util

        # Update Q2
        delta2 = u - q2[s2, a2]
        q2[s2, a2] += lr_q * delta2

        # Update Q1_MF toward the reached state's value (bootstrapping on max Q2)
        target1 = np.max(q2[s2])
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += lr_q * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['lr_q', 'beta', 'omega_base', 'nu_u', 'eps0']"
iter8_run0_participant3.json,cognitive_model3,388.2131403609412,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-like first-stage representation with anxiety-modulated discount and forgetting.

    Mechanism:
    - Maintain an action-conditioned state-occupancy map M[a1, s2] (successor-like kernel).
      For this two-step task, M approximates the transition structure but is allowed to decay/forget.
    - Stage-1 model-based values use M to project the best available second-stage values: Q1(a1) = sum_s M[a1,s]*V[s],
      where V[s] = max_a Q2[s,a].
    - Stage-2 Q2(s2, a2) learned by TD with a single learning rate.
    - Anxiety shortens the effective planning horizon via a lower discount (gamma) and increases forgetting.
    - Perseveration (stickiness) at both stages; stage-2 stickiness is reduced by anxiety.

    Parameters and bounds:
    - model_parameters = (lr_q, beta, gamma0, f0, k_stick)
        lr_q    in [0,1]: learning rate for Q2 and for updating M toward observed occupancy
        beta    in [0,10]: inverse temperature
        gamma0  in [0,1]: baseline discount controlling how sharply M focuses on immediate outcomes
        f0      in [0,1]: baseline forgetting; higher means faster decay of M each trial
        k_stick in [0,1]: perseveration strength (applied to both stages; attenuated at stage-2 by anxiety)

    Inputs:
    - action_1, state, action_2, reward: arrays per trial (see task)
    - stai: array with one element in [0,1]
    - model_parameters: tuple/list as defined above

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    lr_q, beta, gamma0, f0, k_stick = model_parameters
    n_trials = len(action_1)
    anx = float(stai[0])

    # Initialize successor-like kernel M and Q2
    # Start from instructed/common structure
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)
    q2 = np.zeros((2, 2), dtype=float)

    # Anxiety-modulated planning discount and forgetting
    gamma_eff = float(np.clip(gamma0 * (1.0 - 0.5 * anx), 0.0, 1.0))
    forget_eff = float(np.clip(f0 * (0.5 + 0.5 * anx), 0.0, 1.0))

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        # Value projection using M
        V = np.max(q2, axis=1)         # V[s2]
        q1 = M @ V                     # stage-1 values

        # Stage-1 perseveration
        stick1 = np.zeros(2, dtype=float)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0
        logits1 = beta * q1 + k_stick * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with anxiety-attenuated perseveration
        s2 = int(state[t])
        stick2 = np.zeros(2, dtype=float)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        k2_eff = k_stick * (1.0 - 0.7 * anx)
        logits2 = beta * q2[s2] + k2_eff * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = float(reward[t])

        # Update Q2
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += lr_q * delta2

        # Update successor-like kernel M for the chosen action:
        # 1) Global forgetting toward zero (blunting old associations), stronger with anxiety
        M *= (1.0 - forget_eff)

        # 2) Move the chosen action's row toward the observed state occupancy,
        #    with an immediacy emphasis set by (1 - gamma_eff).
        onehot_s = np.array([1.0 if i == s2 else 0.0 for i in range(2)], dtype=float)
        target_row = (1.0 - gamma_eff) * onehot_s + gamma_eff * M[a1]  # retain some of prior structure
        M[a1] = (1.0 - lr_q) * M[a1] + lr_q * target_row

        # Renormalize rows to keep a proper weighting over states (avoid collapse)
        row_sums = np.sum(M, axis=1, keepdims=True) + eps
        M = M / row_sums

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['lr_q', 'beta', 'gamma0', 'f0', 'k_stick']"
iter8_run0_participant32.json,cognitive_model1,380.99055714018004,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Hybrid MBâMF with anxiety-modulated transition uncertainty and eligibility-trace credit assignment.

    Core mechanisms
    - Second-stage model-free Q-learning with learning rate eta_q.
    - First-stage hybrid policy: convex combination of learned model-based values (from a learned transition model)
      and model-free Q1 values. Anxiety reduces MB reliance and increases transition uncertainty.
    - Transition model is learned online and simultaneously ""blurred"" toward 0.5 by anxiety (uncertainty inflation).
    - Eligibility-trace credit assignment from stage 2 to stage 1 controlled by lambda_elig.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int {0,1}
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1 for alien) per trial.
    reward : array-like of float
        Reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1]-like range (as provided).
    model_parameters : list or array-like of float
        [eta_q, beta, zeta_mb, xi_anxTrans, lambda_elig]
        Bounds:
        - eta_q: [0,1] learning rate for Q-updates.
        - beta: [0,10] inverse temperature for both stages.
        - zeta_mb: [0,1] baseline weight on model-based valuation at stage 1.
        - xi_anxTrans: [0,1] anxiety-sensitivity for transition uncertainty/learning.
        - lambda_elig: [0,1] eligibility-trace strength for credit assignment to stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    eta_q, beta, zeta_mb, xi_anxTrans, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize value functions
    q1_mf = np.zeros(2)          # model-free first-stage Q
    q2 = np.zeros((2, 2))        # second-stage Q: state x action

    # Initialize transition model T(a, s') with nominal common transitions (A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Effective MB weight decreases with anxiety (less reliance on planning under higher anxiety)
    w_mb_eff = np.clip(zeta_mb * (1.0 - 0.6 * stai), 0.0, 1.0)
    # Anxiety increases uncertainty inflation of the learned transition model toward 0.5
    blur_strength = np.clip(xi_anxTrans * (0.3 + 0.7 * stai), 0.0, 1.0)
    # Transition learning rate also scales with anxiety (learn faster from observed transitions)
    tau_T = np.clip(0.2 + 0.6 * xi_anxTrans * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Stage 1 policy (hybrid MB + MF)
        max_q2 = np.max(q2, axis=1)           # value of each second-stage state
        q1_mb = T @ max_q2                    # model-based action values
        q1_hyb = (1.0 - w_mb_eff) * q1_mf + w_mb_eff * q1_mb

        # Softmax for stage 1
        logits1 = beta * (q1_hyb - np.max(q1_hyb))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        q2_s = q2[s].copy()
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Second-stage TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta_q * pe2

        # First-stage credit assignment with eligibility trace:
        # Combine direct backpropagation of stage-2 PE and alignment toward the best value in the reached state.
        target1 = (lambda_elig * r) + ((1.0 - lambda_elig) * np.max(q2[s]))
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta_q * pe1

        # Transition model update toward observed state
        # Move chosen action's transition row toward one-hot on observed state
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T[a1, s_idx] = (1.0 - tau_T) * T[a1, s_idx] + tau_T * target

        # Anxiety-driven uncertainty inflation (blurring rows toward 0.5)
        T[a1, :] = (1.0 - blur_strength) * T[a1, :] + blur_strength * 0.5
        # Renormalize to guard against drift
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['eta_q', 'beta', 'zeta_mb', 'xi_anxTrans', 'lambda_elig']"
iter8_run0_participant32.json,cognitive_model2,382.0466964010218,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Successor-representation (SR) stage-1 valuation with anxiety-modulated SR reliance and lapse.

    Core mechanisms
    - Learn a one-step SR row for each first-stage action (effectively the transition distribution),
      and value actions by SR dot second-stage state values (max Q2).
    - Combine SR-based values with model-free Q1 via a weight that decreases with anxiety.
    - Anxiety increases a small choice lapse (stimulus-independent noise).
    - Second-stage choices are standard softmax on Q2.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [mu, beta, theta_sr, z_anxShift, eps_lapse]
        Bounds:
        - mu: [0,1] learning rate for Q and SR updates.
        - beta: [0,10] inverse temperature for both stages.
        - theta_sr: [0,1] baseline weight on SR-based valuation at stage 1.
        - z_anxShift: [0,1] scales how much anxiety reduces SR reliance.
        - eps_lapse: [0,1] baseline lapse; scaled up by anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    mu, beta, theta_sr, z_anxShift, eps_lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # SR over next-state occupancy for each action (rows sum to 1 initially)
    # Initialize with nominal common transitions
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    w_sr = np.clip(theta_sr * (1.0 - z_anxShift * (0.5 + 0.5 * stai)), 0.0, 1.0)
    lapse = np.clip(eps_lapse * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Stage 1 SR-based value
        v_state = np.max(q2, axis=1)     # value of each second-stage state
        q1_sr = M @ v_state
        q1_hyb = (1.0 - w_sr) * q1_mf + w_sr * q1_sr

        # Softmax with lapse at stage 1
        logits1 = beta * (q1_hyb - np.max(q1_hyb))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (with the same lapse for parsimony)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - lapse) * probs2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Second-stage learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += mu * pe2

        # First-stage MF learning toward the realized second-stage action value
        target1 = q2[s, a2]
        q1_mf[a1] += mu * (target1 - q1_mf[a1])

        # SR update for the chosen action towards the observed state one-hot
        e = np.array([1.0 if i == s else 0.0 for i in range(2)], dtype=float)
        M[a1, :] = (1.0 - mu) * M[a1, :] + mu * e
        # Renormalize for numerical stability
        M[a1, :] = M[a1, :] / np.sum(M[a1, :])

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['mu', 'beta', 'theta_sr', 'z_anxShift', 'eps_lapse']"
iter8_run0_participant32.json,cognitive_model3,397.0575682435215,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Volatility-adaptive learning with anxiety-modulated exploration and MB mixing.

    Core mechanisms
    - Learning rates adapt online to surprise: unsigned second-stage PE and transition surprise increase alpha_t.
    - Anxiety reduces decisiveness (temperature scaling) and interacts with surprise sensitivity.
    - First-stage policy is a hybrid of MF Q1 and MB values using a fixed mixing parameter.
    - Transition model is learned and contributes to transition surprise.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Reward per trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha0, beta, k_vol, w_mb, psi_tempAnx]
        Bounds:
        - alpha0: [0,1] baseline learning rate.
        - beta: [0,10] base inverse temperature.
        - k_vol: [0,1] sensitivity of learning rate to surprise/volatility.
        - w_mb: [0,1] weight of model-based valuation at stage 1.
        - psi_tempAnx: [0,1] how strongly anxiety reduces inverse temperature.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha0, beta, k_vol, w_mb, psi_tempAnx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values and transitions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-reduced decisiveness
    beta_eff = beta * np.clip(1.0 - psi_tempAnx * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based action values
        v_state = np.max(q2, axis=1)
        q1_mb = T @ v_state
        q1_hyb = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage 1 choice
        logits1 = beta_eff * (q1_hyb - np.max(q1_hyb))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Surprise computations
        # Transition surprise: 1 - predicted probability of observed state
        trans_surprise = 1.0 - T[a1, s]
        # Reward PE (unsigned)
        pe2_unsigned = abs(r - q2[s, a2])

        # Volatility-adaptive learning rates (per trial)
        alpha2_t = np.clip(alpha0 + k_vol * (0.5 * pe2_unsigned + 0.5 * trans_surprise) * (0.5 + 0.5 * stai), 0.0, 1.0)
        alpha1_t = np.clip(0.5 * alpha0 + k_vol * abs(np.max(q2[s]) - q1_mf[a1]) * (0.5 + 0.5 * stai), 0.0, 1.0)

        # Update Q2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2_t * pe2

        # Update Q1 toward the reached-state value
        target1 = q2[s, a2]
        q1_mf[a1] += alpha1_t * (target1 - q1_mf[a1])

        # Update transition model for chosen action
        tau_T = np.clip(0.1 + 0.8 * k_vol * (0.5 + 0.5 * stai), 0.0, 1.0)
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T[a1, s_idx] = (1.0 - tau_T) * T[a1, s_idx] + tau_T * target
        # Normalize
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik","['alpha0', 'beta', 'k_vol', 'w_mb', 'psi_tempAnx']"
iter8_run0_participant35.json,cognitive_model1,338.8504663419586,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""MF-MB hybrid with anxiety-damped directed exploration via value-uncertainty (UCB-like).

    This model learns mean and uncertainty (variance proxy) for each second-stage action and
    uses an uncertainty bonus to drive exploration at both stages. Anxiety reduces the directed
    exploration bonus, encouraging exploitation. First-stage choice values mix model-based
    and model-free components. Second-stage values are mean reward plus an exploration bonus.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_r: reward learning rate for Q2 in [0,1]
        beta: inverse temperature for both stages in [0,10]
        theta_mb: base MB weight for stage-1 mixing in [0,1]
        nu_var: process noise for uncertainty dynamics in [0,1] (bigger -> more uncertainty)
        xi_dir: base exploration bonus coefficient in [0,1]

    Bounds
    - alpha_r, theta_mb, nu_var, xi_dir in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Directed exploration bonus is reduced by anxiety:
        xi_eff = xi_dir * (1 - stai)
      Higher anxiety lowers uncertainty-directed exploration.
    - Model-based weight is modestly reduced by anxiety:
        w_mb = clip(theta_mb * (1 - 0.3*stai), 0, 1)

    Returns
    - Negative log-likelihood of observed choices under the model.
    """"""
    alpha_r, beta, theta_mb, nu_var, xi_dir = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed environment transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: actions A/U; cols: states X/Y

    # Value and uncertainty trackers
    q1_mf = np.zeros(2)           # MF first-stage cached values for A/U
    q2_mean = np.zeros((2, 2))    # mean value for each state-action
    q2_var = np.ones((2, 2)) * 0.25  # initialize moderate uncertainty

    # Likelihood containers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated coefficients
    xi_eff = max(0.0, min(1.0, xi_dir * (1.0 - stai)))
    w_mb = max(0.0, min(1.0, theta_mb * (1.0 - 0.3 * stai)))

    for t in range(n_trials):
        # Construct exploration bonus at second stage
        bonus2 = xi_eff * np.sqrt(np.maximum(q2_var, 1e-12))  # state-action specific

        # Model-based forward value uses max over Q2 + bonus
        max_q2_bonus = np.max(q2_mean + bonus2, axis=1)  # size 2 for states X/Y
        q1_mb = T @ max_q2_bonus

        # Stage-1 decision values as MB/MF mix
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 softmax
        c_q1 = q1 - np.max(q1)
        probs1 = np.exp(beta * c_q1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with exploration bonus
        s = int(state[t])
        q2_dec = q2_mean[s] + bonus2[s]
        c_q2 = q2_dec - np.max(q2_dec)
        probs2 = np.exp(beta * c_q2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Update second-stage mean with simple delta rule
        pe2 = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha_r * pe2

        # Update uncertainty: shrink when sampled, inflate globally with process noise
        # Simple variance proxy update: sampled arm uncertainty contracts, all arms diffuse
        # Contract sampled: (1 - alpha_r) factor; Diffuse: + nu_var * 0.05
        q2_var *= (1.0 + 0.0 * 0)  # no-op to emphasize array op
        q2_var[s, a2] = (1.0 - alpha_r) * q2_var[s, a2] + nu_var * 0.05
        # Unchosen action at the same state diffuses slightly
        other = 1 - a2
        q2_var[s, other] = q2_var[s, other] + nu_var * 0.02
        # Keep bounds
        q2_var = np.clip(q2_var, 1e-6, 1.0)

        # MF backup to stage-1 (bootstrapping from realized state-action value)
        target1 = q2_mean[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'theta_mb', 'nu_var', 'xi_dir']"
iter8_run0_participant35.json,cognitive_model2,340.99461561699695,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learning transitions with anxiety-amplified rare-event sensitivity and MB/MF arbitration.

    This model learns the transition matrix online and uses it to compute model-based (MB)
    first-stage values, which are mixed with model-free (MF) values. When a rare transition
    is observed, anxiety increases the learning rate for that transition, making high-anxiety
    participants update their beliefs more strongly on surprising events.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_q: learning rate for Q-values in [0,1]
        beta: inverse temperature for both stages in [0,10]
        tau_T: base learning rate for transition probabilities in [0,1]
        delta_rare: extra learning-rate gain on rare transitions in [0,1]
        w_mix: base MB mixture weight in [0,1]

    Bounds
    - alpha_q, tau_T, delta_rare, w_mix in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Rare transition update boost:
        lr_T = tau_T * (1 + delta_rare * stai) if rare else tau_T
      Rarity defined relative to the canonical structure (A->X common, U->Y common).
    - MB weight reduced with anxiety: w_eff = clip(w_mix * (1 - 0.4*stai), 0, 1)

    Returns
    - Negative log-likelihood of observed choices under the model.
    """"""
    alpha_q, beta, tau_T, delta_rare, w_mix = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Canonical environment (for rarity detection only)
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]], dtype=float)

    # Learned transition matrix: initialize neutral (0.5/0.5)
    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    w_eff = max(0.0, min(1.0, w_mix * (1.0 - 0.4 * stai)))

    for t in range(n_trials):
        # MB stage-1 values via learned transitions and current q2
        max_q2 = np.max(q2, axis=1)         # best alien per planet
        q1_mb = T_hat @ max_q2              # propagate through learned transitions

        # Mix MB and MF for stage-1 choice
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        c_q1 = q1 - np.max(q1)
        probs1 = np.exp(beta * c_q1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax over q2
        s = int(state[t])
        c_q2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * c_q2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Reward update at second stage
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # MF backup to stage-1 from realized state-action value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # Transition learning update for chosen action row of T_hat
        # Determine if observed transition was rare under canonical structure
        is_rare = ((a1 == 0 and s == 1) or (a1 == 1 and s == 0))
        lr_T = tau_T * (1.0 + (delta_rare * stai if is_rare else 0.0))
        lr_T = max(0.0, min(1.0, lr_T))

        # For two states, keep row normalized by updating prob-to-X and setting prob-to-Y=1-p
        p_to_X = T_hat[a1, 0]
        target = 1.0 if s == 0 else 0.0
        p_to_X = (1.0 - lr_T) * p_to_X + lr_T * target
        T_hat[a1, 0] = p_to_X
        T_hat[a1, 1] = 1.0 - p_to_X

        # Slight inertia for the unchosen action row (no update)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_q', 'beta', 'tau_T', 'delta_rare', 'w_mix']"
iter8_run0_participant35.json,cognitive_model3,308.4487917908324,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive utility with anxiety-enhanced WSLS at stage 2 and value leakage.

    This model uses:
    - Risk-sensitive utility at stage 2 with loss aversion that increases with anxiety.
    - A mixture of softmax and win-stay/lose-shift (WSLS) at stage 2; anxiety increases
      the WSLS weight, promoting heuristic repetition/switching.
    - A leak/forgetting term on Q-values to capture drift and reduced confidence.
    - Stage-1 values are MF bootstrapped from Q2; softmax inverse temperature decreases
      with anxiety (noisier choices).

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_q: base learning rate for Q updates in [0,1]
        beta_base: base inverse temperature in [0,10]
        phi_leak: value leak/forgetting rate in [0,1] (applied each trial)
        zeta_wsls: base WSLS mixture weight in [0,1]
        nu_loss: base loss aversion coefficient in [0,1] (utility for negative outcomes)

    Bounds
    - alpha_q, phi_leak, zeta_wsls, nu_loss in [0,1]
    - beta_base in [0,10]

    Anxiety usage
    - Loss aversion increases with anxiety: nu_eff = nu_loss * (1 + stai)
    - WSLS weight increases with anxiety: w_wsls = clip(zeta_wsls * (0.5 + 0.5*stai), 0, 1)
    - Inverse temperature decreases with anxiety: beta = beta_base * (1 - 0.5*stai)

    Returns
    - Negative log-likelihood of observed choices under the model.
    """"""
    alpha_q, beta_base, phi_leak, zeta_wsls, nu_loss = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective parameters after anxiety modulation
    beta = max(0.0, min(10.0, beta_base * (1.0 - 0.5 * stai)))
    w_wsls = max(0.0, min(1.0, zeta_wsls * (0.5 + 0.5 * stai)))
    nu_eff = max(0.0, min(2.0, nu_loss * (1.0 + stai)))  # cap at 2 to keep utilities reasonable
    leak = max(0.0, min(1.0, phi_leak))

    # Value functions
    q1 = np.zeros(2)         # MF first-stage
    q2 = np.zeros((2, 2))    # second-stage

    # For WSLS at stage 2, track previous action and reward sign per state
    prev_a2 = np.zeros(2, dtype=int)  # last chosen action for each state
    prev_sign = np.zeros(2)           # last reward sign (+1 / -1) for each state
    has_prev = np.zeros(2, dtype=bool)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage-1 softmax over MF q1
        c_q1 = q1 - np.max(q1)
        probs1 = np.exp(beta * c_q1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: mixture softmax(Q2) and WSLS heuristic within observed state
        s = int(state[t])
        c_q2 = q2[s] - np.max(q2[s])
        probs2_soft = np.exp(beta * c_q2)
        probs2_soft /= np.sum(probs2_soft)

        # WSLS policy vector for current state
        if has_prev[s]:
            if prev_sign[s] >= 0.0:
                # Win-stay: pick previous action
                wsls_probs = np.array([0.0, 0.0])
                wsls_probs[prev_a2[s]] = 1.0
            else:
                # Lose-shift: choose the other action
                wsls_probs = np.array([0.0, 0.0])
                wsls_probs[1 - prev_a2[s]] = 1.0
        else:
            wsls_probs = np.array([0.5, 0.5])

        probs2 = (1.0 - w_wsls) * probs2_soft + w_wsls * wsls_probs
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and risk-sensitive utility
        r = float(reward[t])
        util = r if r >= 0.0 else -nu_eff * (-r)

        # Apply leak (forgetting) before update
        q2 *= (1.0 - leak)
        q1 *= (1.0 - leak)

        # Update Q2 with utility-based PE
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Backup to Q1 from realized state-action (MF TD(1)-like)
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha_q * pe1

        # Update WSLS memory
        prev_a2[s] = a2
        prev_sign[s] = 1.0 if r >= 0.0 else -1.0
        has_prev[s] = True

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_q', 'beta_base', 'phi_leak', 'zeta_wsls', 'nu_loss']"
iter8_run0_participant39.json,cognitive_model1,463.40876192995313,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric-valence learning with anxiety-modulated eligibility and learned transitions (MB confidence-weighted hybrid).

    The agent:
    - Learns second-stage (planet-alien) values model-free with asymmetric learning rates for reward vs. no-reward.
    - Learns the first-stage transition model T online with its own learning rate.
    - Computes model-based first-stage values from the learned T and max second-stage values.
    - Maintains a model-free first-stage value updated via an eligibility trace from stage-2 prediction errors.
    - Blends MB and MF first-stage values per action by a confidence weight derived from each row of learned T,
      scaled down by anxiety (higher stai -> less planning).
    - Anxiety (stai) also reduces the eligibility trace and increases learning from negative outcomes.

    Parameter bounds:
    - alpha_pos: [0,1] learning rate for positive outcomes at stage-2
    - alpha_neg: [0,1] learning rate for negative outcomes at stage-2
    - beta: [0,10] inverse temperature used at both stages
    - lambda_et: [0,1] eligibility trace strength for stage-1 MF update
    - tau_T: [0,1] learning rate for transition model T

    Inputs:
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens within the visited planet)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array with one scalar in [0,1], trait anxiety score
    - model_parameters: iterable of 5 parameters [alpha_pos, alpha_neg, beta, lambda_et, tau_T]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_pos, alpha_neg, beta, lambda_et, tau_T = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transition model; start neutral (0.5/0.5 per action) to express initial uncertainty
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Model-free values
    q1_mf = np.zeros(2, dtype=float)         # first-stage MF values for spaceships
    q2_mf = np.zeros((2, 2), dtype=float)    # second-stage MF values per planet x alien

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        # Compute max second-stage values per planet (for MB planning)
        max_q2 = np.max(q2_mf, axis=1)                 # shape (2,)

        # Model-based first-stage values from learned transitions
        q1_mb = T @ max_q2                              # shape (2,)

        # Per-action MB confidence weight: higher when a row of T is far from uniform
        # conf_a in [0,1], then anxiety reduces reliance on MB
        conf = np.abs(T - 0.5)
        conf_per_row = np.sum(conf, axis=1) / np.sum(np.abs(np.array([0.5, 0.5]) - 0.5))  # denominator = 0 -> guard below
        # The denominator above is zero; so do a safe computation:
        # For a 2-prob row, sum |p-0.5| ranges from 0 (0.5,0.5) to 1 (1,0). Normalize by 1.
        conf_per_row = np.sum(np.abs(T - 0.5), axis=1)  # already in [0,1]
        w_mb_vec = conf_per_row * (1.0 - stai0)         # element-wise per action weight in [0,1]

        # Blend MF and MB action values per action
        q1_hybrid = w_mb_vec * q1_mb + (1.0 - w_mb_vec) * q1_mf

        # First-stage policy
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (pure MF values)
        s2 = state[t]
        q2 = q2_mf[s2].copy()
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Stage-2 update with asymmetric learning; anxiety attenuates positive learning and enhances negative learning
        if r > q2_mf[s2, a2]:
            alpha2 = alpha_pos * (1.0 - 0.3 * stai0)
        else:
            alpha2 = alpha_neg * (1.0 + 0.3 * stai0)
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha2 * pe2

        # Stage-1 MF update via eligibility trace from stage-2 PE
        lambda_eff = lambda_et * (1.0 - 0.5 * stai0)
        q1_mf[a1] += alpha2 * lambda_eff * pe2

        # Update learned transition model T row for the chosen action toward observed state
        # Anxiety reduces confidence in transition learning slightly (slower updates at high stai)
        tau_eff = tau_T * (1.0 - 0.3 * stai0)
        target_row = np.array([0.0, 0.0], dtype=float)
        target_row[s2] = 1.0
        T[a1, :] = (1.0 - tau_eff) * T[a1, :] + tau_eff * target_row

        # Keep rows normalized
        row_sum = np.sum(T[a1, :]) + eps
        T[a1, :] = T[a1, :] / row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_pos', 'alpha_neg', 'beta', 'lambda_et', 'tau_T']"
iter8_run0_participant39.json,cognitive_model2,422.9932280106285,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Information-bonus exploration with anxiety-modulated temperature, lapse, and first-stage bias.

    The agent:
    - Uses model-based evaluation at stage 1 with the fixed transition matrix (A->X, U->Y common).
    - Learns second-stage values model-free.
    - Adds an uncertainty-driven information bonus at stage 2 inversely proportional to visit counts.
    - Applies a soft lapse that blends the softmax policy with a uniform policy; lapse increases with anxiety.
    - Adds a first-stage bias toward commonly transitioning ships, strengthened by anxiety.

    Parameter bounds:
    - alpha: [0,1] learning rate for stage-2 MF and stage-1 MF bootstrapping
    - beta_base: [0,10] base inverse temperature (reduced by anxiety)
    - lapse_base: [0,1] base lapse rate (increased by anxiety)
    - omega_info: [0,1] information bonus strength at stage 2
    - kappa_bias: [0,1] strength of first-stage bias toward common transitions

    Inputs:
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1) within state
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array with one scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters [alpha, beta_base, lapse_base, omega_info, kappa_bias]

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha, beta_base, lapse_base, omega_info, kappa_bias = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed transition model
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and counts
    q1_mf = np.zeros(2, dtype=float)
    q2_mf = np.zeros((2, 2), dtype=float)
    visit_counts = np.ones((2, 2), dtype=float)  # start at 1 to avoid div-by-zero; state x alien

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    # Anxiety-modulated parameters
    beta = beta_base * (1.0 - 0.5 * stai0)              # higher anxiety -> lower temperature (more noise)
    lapse = min(1.0, lapse_base * (0.5 + stai0))        # higher anxiety -> more lapses
    omega_eff = omega_info * (1.0 - stai0)              # higher anxiety -> less info seeking
    bias_mag = kappa_bias * (0.5 + stai0)               # higher anxiety -> stronger bias

    for t in range(n_trials):
        # Compute MB first-stage values
        max_q2 = np.max(q2_mf, axis=1)                 # best alien per planet
        q1_mb = T @ max_q2

        # Add small MF leak scaled by anxiety (higher anxiety relies slightly more on MF shortcut)
        q1_val = q1_mb + stai0 * q1_mf

        # Add first-stage bias toward commonly transitioning ships
        # Common: A->X, U->Y; implement as [+1, +1] toward their respective common outcomes.
        # With two actions, assign bias +bias_mag to both in proportion to their common transition prob minus 0.5.
        common_bias = np.array([T[0, 0] - 0.5, T[1, 1] - 0.5]) * (2.0 * bias_mag)  # scales to ~[0, bias_mag]
        q1_val = q1_val + common_bias

        # Stage-1 policy with lapse
        q1c = q1_val - np.max(q1_val)
        pi1_soft = np.exp(beta * q1c)
        pi1_soft = pi1_soft / (np.sum(pi1_soft) + eps)
        pi1 = (1.0 - lapse) * pi1_soft + lapse * 0.5  # uniform over two actions
        a1 = action_1[t]
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy with info bonus
        s2 = state[t]
        q2 = q2_mf[s2].copy()
        # Uncertainty bonus: inverse sqrt of visits
        bonus = omega_eff / np.sqrt(visit_counts[s2] + 0.0)
        q2_bonus = q2 + bonus
        q2c = q2_bonus - np.max(q2_bonus)
        pi2_soft = np.exp(beta * q2c)
        pi2_soft = pi2_soft / (np.sum(pi2_soft) + eps)
        pi2 = (1.0 - lapse) * pi2_soft + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = pi2[a2]

        # Outcome
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # Update counts
        visit_counts[s2, a2] += 1.0

        # Stage-1 MF bootstrapping from realized second-stage value
        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta_base', 'lapse_base', 'omega_info', 'kappa_bias']"
iter8_run0_participant39.json,cognitive_model3,348.09201556926575,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor representation (SR) with anxiety-modulated discount, forgetting, and stickiness.

    The agent:
    - Learns second-stage MF values.
    - Learns a first-stage successor representation M (actions -> expected discounted occupancy of planets).
      Since the environment has a single transition step, M approximates the transition mapping scaled by a discount.
    - Computes first-stage values as q1 = M @ max(Q2), i.e., feature expectation times learned state features.
    - Applies forgetting to M to allow adaptation to nonstationarities; both discount and forgetting depend on anxiety.
    - Includes choice stickiness at both stages, reduced by anxiety at stage 2 but increased at stage 1.

    Parameter bounds:
    - alpha:   [0,1] learning rate for value and SR updates
    - beta:    [0,10] inverse temperature at both stages
    - gamma0:  [0,1] base discount used in SR updates
    - forget0: [0,1] base forgetting rate for SR
    - stick0:  [0,1] base stickiness strength

    Inputs:
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1) within state
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array with one scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters [alpha, beta, gamma0, forget0, stick0]

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha, beta, gamma0, forget0, stick0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Successor representation from actions to planets: M[a, s]
    M = np.zeros((2, 2), dtype=float)

    # MF values for second-stage
    q2_mf = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    # Anxiety-modulated SR parameters
    gamma_eff = gamma0 * (1.0 - 0.5 * stai0)          # higher anxiety -> shorter planning horizon
    forget_eff = forget0 * (0.5 + stai0)              # higher anxiety -> more forgetting
    kappa1 = stick0 * (0.5 + stai0)                   # anxiety increases first-stage stickiness
    kappa2 = stick0 * (1.0 - 0.5 * stai0)             # anxiety decreases second-stage stickiness

    for t in range(n_trials):
        # First-stage value via SR features
        features = np.max(q2_mf, axis=1)             # state features from MF values
        q1_sr = M @ features

        # Add first-stage stickiness
        if prev_a1 is not None:
            stick_vec = np.zeros(2, dtype=float)
            stick_vec[prev_a1] = 1.0
            q1_sr = q1_sr + kappa1 * stick_vec

        # Stage-1 policy
        q1c = q1_sr - np.max(q1_sr)
        pi1 = np.exp(beta * q1c)
        pi1 = pi1 / (np.sum(pi1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy with stickiness within state
        s2 = state[t]
        q2 = q2_mf[s2].copy()
        prev_a2 = prev_a2_by_state[s2]
        if prev_a2 is not None:
            stick2 = np.zeros(2, dtype=float)
            stick2[prev_a2] = 1.0
            q2 = q2 + kappa2 * stick2

        q2c = q2 - np.max(q2)
        pi2 = np.exp(beta * q2c)
        pi2 = pi2 / (np.sum(pi2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = pi2[a2]

        # Outcome and learning
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # SR update for the chosen action toward observed state occupancy
        # One-step environment: target successor for chosen action is gamma * onehot(s2)
        target = gamma_eff * (np.array([0.0, 0.0], dtype=float))
        target[s2] = gamma_eff
        # Forgetting (leaky integration) then TD-like update
        M[a1, :] = (1.0 - forget_eff) * M[a1, :] + alpha * (target - M[a1, :])

        # Bookkeeping for stickiness
        prev_a1 = a1
        prev_a2_by_state[s2] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'gamma0', 'forget0', 'stick0']"
iter8_run0_participant4.json,cognitive_model1,536.3124135685375,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-amplified directed exploration via count-based bonuses.

    Overview
    - Stage 2 learns model-free Q-values (Rescorla-Wagner).
    - First-stage action values are a hybrid of model-based planning and model-free value.
      MB uses the known transition structure (common 0.7 / rare 0.3).
    - Adds directed exploration bonuses derived from action-visit counts:
        b2[s,a] = k_eff / sqrt(count[s,a] + 1), where k_eff increases with anxiety (stai).
      First-stage bonuses equal the expected downstream bonus under the transition matrix.
    - Anxiety (stai) increases the exploration bonus scale: k_eff = kappa_ent * (1 + xi_anx * stai).

    Parameters (all used)
    - alpha:      [0,1]   Learning rate for Q2 and MF credit to Q1.
    - beta:       [0,10]  Inverse temperature for softmax at both stages.
    - kappa_ent:  [0,1]   Baseline directed exploration bonus weight.
    - xi_anx:     [0,1]   Scales exploration bonus by anxiety (multiplicative).
    - omega_mf:   [0,1]   Weight of model-free Q1 in the hybrid (0=MB only, 1=MF only).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, kappa_ent, xi_anx, omega_mf].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, kappa_ent, xi_anx, omega_mf = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Value functions
    q2 = np.zeros((2, 2), dtype=float) + 0.5  # stage-2 Q-values
    q1_mf = np.zeros(2, dtype=float)          # stage-1 MF values

    # Count-based exploration (for directed exploration bonuses)
    counts2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Anxiety-amplified exploration scale
        k_eff = kappa_ent * (1.0 + xi_anx * stai)

        # Compute per-action directed exploration bonus at stage 2
        b2 = k_eff / np.sqrt(counts2 + 1.0)  # shape (2,2)

        # Stage-1 MB values: expect max Q2 + bonus under known transitions
        max_q2_plus_bonus = np.max(q2 + b2, axis=1)  # shape (2,)
        q1_mb = T_known @ max_q2_plus_bonus          # shape (2,)

        # Hybrid Q1
        q1 = (1.0 - omega_mf) * q1_mb + omega_mf * q1_mf

        # Policy for stage 1
        logits1 = beta * (q1 - np.max(q1))
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Policy for stage 2 in reached state s, with exploration bonus
        q2_s_eff = q2[s] + b2[s]
        logits2 = beta * (q2_s_eff - np.max(q2_s_eff))
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Model-free credit to stage-1 value for chosen a1
        # Use the same PE2 as a credit signal (eligibility style)
        q1_mf[a1] += alpha * pe2

        # Update counts for directed exploration
        counts2[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'kappa_ent', 'xi_anx', 'omega_mf']"
iter8_run0_participant4.json,cognitive_model2,389.1938029748802,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planner with anxiety-inflated noise after rare transitions and perseveration.

    Overview
    - Stage 2 learns Q-values via Rescorla-Wagner (model-free).
    - Stage 1 uses purely model-based planning using the known transition matrix.
    - Anxious surprise effect: effective inverse temperature decreases after rare transitions,
      scaled by stai and eta_surp. This increases stochasticity when a rare transition occurs.
      Applied to both stages on that trial.
    - Includes a perseveration (stay) bias applied to both stages, parameterized by kappa_pers.

    Parameters (all used)
    - alpha:       [0,1]   Learning rate for stage-2 Q-values.
    - beta_base:   [0,10]  Baseline inverse temperature.
    - eta_surp:    [0,1]   Magnitude by which surprise (rare transition) reduces beta.
    - kappa_pers:  [0,1]   Additive bias to repeat previous action at each stage.
                           Implemented as an additive term in the logits.
    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta_base, eta_surp, kappa_pers].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta_base, eta_surp, kappa_pers = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Values
    q2 = np.zeros((2, 2), dtype=float) + 0.5

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Determine if observed transition was rare
        # A (0) commonly -> X (0); U (1) commonly -> Y (1)
        is_rare = (a1 == 0 and s == 1) or (a1 == 1 and s == 0)

        # Anxiety-inflated surprise reduces beta on rare transitions
        beta_eff = beta_base * (1.0 - eta_surp * stai * (1.0 if is_rare else 0.0))
        beta_eff = max(1e-6, beta_eff)

        # Stage-1 MB values: expected max Q2 under known transitions
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T_known @ max_q2

        # Perseveration biases
        bias1 = np.zeros(2, dtype=float)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_pers
        bias2 = np.zeros(2, dtype=float)
        if prev_a2[s] is not None:
            bias2[int(prev_a2[s])] += kappa_pers

        # Policy for stage 1
        logits1 = beta_eff * (q1_mb - np.max(q1_mb)) + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Policy for stage 2
        q2_s = q2[s]
        logits2 = beta_eff * (q2_s - np.max(q2_s)) + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Learning stage-2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update previous choices
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta_base', 'eta_surp', 'kappa_pers']"
iter8_run0_participant4.json,cognitive_model3,490.4660640195128,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""MF with anxiety-gated eligibility credit, forgetting of unchosen options, and prior bias for ship A.

    Overview
    - Stage 2 learns Q-values via Rescorla-Wagner.
    - Stage 1 uses purely model-free Q1 updated from the stage-2 prediction error via an
      anxiety-gated eligibility credit. High anxiety increases the credit assigned from
      second-stage outcomes back to the first-stage choice.
    - Adds forgetting on the unchosen second-stage action within the visited state.
    - Adds a prior choice bias favoring spaceship A (status-quo bias) that scales with anxiety.

    Parameters (all used)
    - alpha_base:  [0,1]   Baseline learning rate for Q2 and Q1 credit.
    - beta:        [0,10]  Inverse temperature for softmax at both stages.
    - phi_forget:  [0,1]   Forgetting factor applied to the unchosen second-stage action.
                           q2[s, 1-a2] <- (1 - phi_forget) * q2[s, 1-a2].
    - theta_anx:   [0,1]   Scales eligibility credit to Q1 as theta_eff = theta_anx * stai.
    - omega_prior: [0,1]   Prior bias toward choosing ship A at stage 1, scaled by stai
                           (logit bonus added to A only).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha_base, beta, phi_forget, theta_anx, omega_prior].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha_base, beta, phi_forget, theta_anx, omega_prior = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values
    q2 = np.zeros((2, 2), dtype=float) + 0.5
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Anxiety-gated eligibility credit strength
    theta_eff = theta_anx * stai

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 policy (pure MF) with prior bias toward A scaled by anxiety
        bias1 = np.array([omega_prior * stai, 0.0], dtype=float)  # add to logits
        logits1 = beta * (q1_mf - np.max(q1_mf)) + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_base * pe2

        # Forgetting of the unchosen action in the visited state
        unchosen = 1 - a2
        q2[s, unchosen] = (1.0 - phi_forget) * q2[s, unchosen]

        # Anxiety-gated eligibility credit from stage 2 back to Q1 for chosen a1
        q1_mf[a1] += alpha_base * theta_eff * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha_base', 'beta', 'phi_forget', 'theta_anx', 'omega_prior']"
iter8_run0_participant40.json,cognitive_model1,549.4093983030316,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""MF-MB arbitration via confidence; anxiety shifts arbitration toward MF.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha, beta, arb0, xi)
        - alpha in [0,1]: learning rate for stage-2 TD and MF credit to stage-1.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - arb0 in [0,1]: baseline arbitration bias toward MB (transformed to [-2,2] internally).
        - xi in [0,1]: confidence gain; how strongly choice confidence pushes toward MB.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Stage-1 action values are a convex combination of MB and MF values:
        Q1 = w * Q1_MB + (1 - w) * Q1_MF
      where w is determined by a logistic transform of (baseline + confidence - anxiety).
    - Confidence (from previous trial) is 1 - normalized entropy of the stage-1 policy,
      with 0 meaning uncertain and 1 meaning certain.
    - Anxiety reduces the MB weight directly and reduces eligibility credit to stage-1:
      lambda = 1 - stai.
    - Transitions are assumed known and stationary (A->X 0.7, U->Y 0.7).
    """"""
    alpha, beta, arb0, xi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q2 = np.zeros((2, 2))   # stage-2 values
    Q1_MF = np.zeros(2)     # stage-1 model-free values

    # Previous-trial policy information for confidence
    prev_logits1 = None
    log2 = np.log(2.0)

    # Transform parameters to useful ranges
    # arb0 in [0,1] -> baseline in [-2, 2] to allow strong prior toward MB
    baseline = 4.0 * (arb0 - 0.5)
    xi_eff = xi  # already [0,1]
    lam = 1.0 - stai_val  # eligibility trace to stage-1 dampened by anxiety

    for t in range(n_trials):
        # Compute MB values for stage-1 from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Confidence from previous trial's stage-1 policy
        if prev_logits1 is None:
            conf = 0.0  # no confidence on first trial
        else:
            logits1_prev = prev_logits1 - np.max(prev_logits1)
            probs_prev = np.exp(logits1_prev)
            probs_prev /= np.sum(probs_prev)
            # entropy normalized to [0,1]
            H = -np.sum(probs_prev * (np.log(probs_prev + 1e-12)))
            conf = 1.0 - (H / log2)

        # Arbitration weight toward MB
        z = baseline + xi_eff * conf - stai_val
        w = 1.0 / (1.0 + np.exp(-z))  # in (0,1)

        # Combined stage-1 values
        Q1 = w * Q1_MB + (1.0 - w) * Q1_MF

        # Stage-1 policy and likelihood
        a1 = int(action_1[t])
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy and likelihood
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha * delta2

        # Model-free credit to chosen first-stage action via eligibility
        Q1_MF[a1] += alpha * lam * delta2

        # Store for next-trial confidence
        prev_logits1 = beta * Q1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'arb0', 'xi']"
iter8_run0_participant40.json,cognitive_model2,540.6104847314886,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based control with learned transitions and anxiety-modulated utility curvature.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Observed reward each trial.
    stai : array-like of float
        Trait anxiety in [0,1]; stai[0] used.
    model_parameters : tuple/list
        (alpha2, beta, eta_T, rho0)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - eta_T in [0,1]: transition learning rate for updating P(state|action).
        - rho0 in [0,1]: baseline utility curvature; anxiety increases concavity.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - The agent learns the transition matrix T (rows = actions, cols = states) online.
    - Utility transformation: u(r) = sign(r) * |r|^rho_eff,
      where rho_eff = 0.5 + 0.5 * (1 - stai) * rho0, making outcomes more concave
      (risk-averse) as anxiety increases.
    - Transition learning is mildly dampened by anxiety: eta_eff = eta_T * (1 - 0.5*stai).
    - Stage-1 policy is fully model-based using learned T and current stage-2 values.
    """"""
    alpha2, beta, eta_T, rho0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transitions to agnostic 0.5
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    eta_eff = eta_T * (1.0 - 0.5 * stai_val)
    rho_eff = 0.5 + 0.5 * (1.0 - stai_val) * rho0  # in [0.5, 1]

    for t in range(n_trials):
        # Model-based stage-1 values from current T and Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Observed reward -> utility
        r = reward[t]
        # utility transformation with curvature
        abs_r = np.abs(r)
        u = np.sign(r) * (abs_r ** rho_eff)

        # Update transitions for the chosen first-stage action
        if eta_eff > 0.0:
            a_row = a1
            # Decay row and add mass to observed state, then renormalize
            T[a_row, :] = (1.0 - eta_eff) * T[a_row, :]
            T[a_row, s2] += eta_eff
            row_sum = np.sum(T[a_row, :])
            if row_sum > 0:
                T[a_row, :] /= row_sum

        # TD update at stage-2 on utility
        delta2 = u - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'eta_T', 'rho0']"
iter8_run0_participant40.json,cognitive_model3,463.59370785253697,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Surprise-weighted credit assignment with anxiety-amplified misassignment and perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Observed rewards.
    stai : array-like of float
        Trait anxiety in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, surpr, pi_rep)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - surpr in [0,1]: strength of surprise-driven misassignment after rare transitions.
        - pi_rep in [0,1]: perseveration strength added to repeating previous first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    
    Notes
    -----
    - Stage-1 action values are a hybrid: MB values plus MF values carried via eligibility.
    - After rare transitions (given fixed transition structure 0.7/0.3), a portion of
      credit is misassigned to the unchosen first-stage action. This misassignment
      scales with anxiety (higher anxiety -> stronger misassignment).
    - Perseveration bias is added to logits for repeating the previous first-stage choice.
    """"""
    alpha2, beta, surpr, pi_rep = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure to define ""rare"" vs ""common""
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))
    Q1_MF = np.zeros(2)

    prev_a1 = None

    # Eligibility to chosen action is reduced by anxiety; misassignment enhanced by anxiety
    lam_chosen = 1.0 - stai_val
    lam_unchosen_base = surpr  # base scale
    lam_unchosen = lam_unchosen_base * stai_val  # only active proportionally to anxiety

    for t in range(n_trials):
        # Model-based Q1 from fixed transitions and current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T_fixed @ max_Q2

        # Combine MB and MF for stage-1 values
        Q1 = 0.5 * Q1_MB + 0.5 * Q1_MF

        # Add perseveration bias to logits
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = pi_rep

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning at stage-2
        r = reward[t]
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Determine whether transition was ""common"" or ""rare""
        prob_seen = T_fixed[a1, s2]
        is_rare = 1 if prob_seen < 0.5 else 0  # here rare = 0.3

        # Credit assignment to stage-1 MF
        # Always credit chosen action
        Q1_MF[a1] += alpha2 * lam_chosen * delta2

        # If rare transition, misassign some credit to the unchosen action; anxiety scales it
        if is_rare:
            a1_other = 1 - a1
            Q1_MF[a1_other] += alpha2 * lam_unchosen * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha2', 'beta', 'surpr', 'pi_rep']"
iter8_run0_participant5.json,cognitive_model1,441.70630904982204,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Surprise-gated model-based control with learned transitions and stage-specific learning.

    Core ideas:
    - Learn the transition matrix from experience (rather than assuming it).
    - Stage-1 action values are a hybrid of model-based (via learned transitions)
      and model-free; the MB weight is increased when recent transition surprise is high,
      especially for participants with lower anxiety.
    - Separate learning rates for stage 1 and stage 2 allow differential updating.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within planet (0/1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values dampen the surprise-to-MB gating.
    model_parameters : array-like of float
        [alpha_s1, alpha_s2, beta, wmb_base, t_alpha]
        - alpha_s1 in [0,1]: learning rate for stage-1 MF values.
        - alpha_s2 in [0,1]: learning rate for stage-2 values.
        - beta in [0,10]: inverse temperature for both stages.
        - wmb_base in [0,1]: baseline weight on MB at stage 1.
        - t_alpha in [0,1]: learning rate for transition probabilities.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha_s1, alpha_s2, beta, wmb_base, t_alpha = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T[a, s]; start near-common but not deterministic
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Stage-2 values per state-action
    q2 = 0.5 * np.ones((2, 2), dtype=float)
    # Stage-1 model-free values
    q1_mf = np.zeros(2, dtype=float)

    p1 = np.zeros(n_trials, dtype=float)
    p2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    # Running estimate of transition surprise (absolute prediction error on transitions)
    surpr = 0.0

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q at stage 1 using learned transitions
        max_q2 = np.max(q2, axis=1)  # best value at each planet
        q1_mb = T @ max_q2

        # Surprise-gated MB weight: more MB when surprise is high and anxiety is low
        w_mb = np.clip(wmb_base + (1.0 - st) * surpr, 0.0, 1.0)

        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Policy stage 1
        logits1 = beta * (q1 - np.max(q1))
        prob1 = np.exp(logits1)
        prob1 /= (np.sum(prob1) + eps)
        p1[t] = prob1[a1]

        # Policy stage 2
        logits2 = beta * (q2[s] - np.max(q2[s]))
        prob2 = np.exp(logits2)
        prob2 /= (np.sum(prob2) + eps)
        p2[t] = prob2[a2]

        # Update transition matrix from observed transition a1 -> s
        # Prediction error on transition for chosen action
        pe_T = 1.0 - T[a1, s]
        # Update chosen state's probability toward 1 and the other toward 0
        T[a1, s] += t_alpha * pe_T
        T[a1, 1 - s] += t_alpha * (-T[a1, 1 - s])
        # Keep rows normalized (numerical safety)
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum
        # Update running surprise (EWMA of absolute transition PE)
        surpr = 0.8 * surpr + 0.2 * abs(pe_T)

        # Stage-2 value update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_s2 * pe2

        # Stage-1 MF update towards the realized second-stage value (SARSA-style backup)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_s1 * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha_s1', 'alpha_s2', 'beta', 'wmb_base', 't_alpha']"
iter8_run0_participant5.json,cognitive_model2,424.64211660248753,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Anxiety-gated exploration with perseveration and hybrid MB/MF valuation.

    Core ideas:
    - Stage 1 combines model-based and model-free values using a fixed blend.
    - Anxiety decreases effective beta (more exploration) and increases both lapse
      probability and perseveration strength (tendency to repeat).
    - Perseveration biases both stages toward repeating previous actions.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within planet (0/1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce beta and increase lapse/perseveration.
    model_parameters : array-like of float
        [alpha, beta_base, pers0, lapse_base]
        - alpha in [0,1]: learning rate for both stages.
        - beta_base in [0,10]: baseline inverse temperature.
        - pers0 in [0,1]: baseline perseveration strength.
        - lapse_base in [0,1]: baseline lapse probability (choice noise).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha, beta_base, pers0, lapse_base = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition matrix (task structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2, dtype=float)
    q2 = 0.5 * np.ones((2, 2), dtype=float)

    # Anxiety-gated parameters
    beta_eff = np.clip(beta_base * (0.6 + 0.4 * (1.0 - st)), 0.0, 10.0)
    lapse = np.clip(lapse_base * (0.5 + 0.5 * st), 0.0, 1.0)
    pers = np.clip(pers0 * (0.5 + 0.5 * st), 0.0, 1.0)

    # MB/MF blend fixed
    w_mb = 0.5

    # Perseveration traces (last actions), initialized to 0 (no bias)
    last_a1 = None
    last_a2 = [None, None]  # separate per second-stage state

    p1 = np.zeros(n_trials, dtype=float)
    p2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Perseveration biases as additive logits
        bias1 = np.zeros(2, dtype=float)
        if last_a1 is not None:
            bias1[last_a1] += pers

        bias2 = np.zeros(2, dtype=float)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += pers

        # Stage 1 choice with lapse
        logits1 = beta_eff * (q1 - np.max(q1)) + bias1
        soft1 = np.exp(logits1)
        soft1 /= (np.sum(soft1) + eps)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p1[t] = probs1[a1]

        # Stage 2 choice with lapse
        logits2 = beta_eff * (q2[s] - np.max(q2[s])) + bias2
        soft2 = np.exp(logits2)
        soft2 /= (np.sum(soft2) + eps)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p2[t] = probs2[a2]

        # Update values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha', 'beta_base', 'pers0', 'lapse_base']"
iter8_run0_participant5.json,cognitive_model3,428.90343382005676,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Risk-asymmetric learning with anxiety-tuned UCB exploration and common-transition bias.

    Core ideas:
    - Stage-2 learning uses separate learning rates for positive vs. negative prediction errors.
    - Exploration at stage 2 uses an uncertainty bonus (UCB) whose strength increases with anxiety,
      modeling anxiety-driven information seeking about uncertain aliens.
    - Stage-1 policy is model-based from the known transition structure, plus a bias favoring the
      more common transition option; this bias increases with anxiety.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within planet (0/1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase UCB bonus and the common-transition bias,
        and mildly reduce beta.
    model_parameters : array-like of float
        [alpha_pos, alpha_neg, beta, ucb_base, common_bias0]
        - alpha_pos in [0,1]: learning rate when PE > 0.
        - alpha_neg in [0,1]: learning rate when PE < 0.
        - beta in [0,10]: inverse temperature (both stages).
        - ucb_base in [0,1]: baseline uncertainty bonus weight at stage 2.
        - common_bias0 in [0,1]: baseline bias toward the more common spaceship at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha_pos, alpha_neg, beta, ucb_base, common_bias0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition matrix (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 values and counts to quantify uncertainty (1/sqrt(N))
    q2 = 0.5 * np.ones((2, 2), dtype=float)
    n2 = np.ones((2, 2), dtype=float)  # start at 1 to avoid div-by-zero

    # Stage-1 value is purely model-based here; no MF component to keep parameter budget
    p1 = np.zeros(n_trials, dtype=float)
    p2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    # Anxiety effects
    beta_eff = np.clip(beta * (0.7 + 0.3 * (1.0 - st)), 0.0, 10.0)
    ucb_w = np.clip(ucb_base * (0.5 + 0.5 * st), 0.0, 1.0)
    bias_common = np.clip(common_bias0 * (0.5 + 0.5 * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 model-based values: expected max Q2 under transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add common-transition bias: favor A over U by +b/-b (A is more common to X)
        # This captures anxious reliance on habitual/common routes.
        bias_vec = np.array([bias_common, -bias_common], dtype=float)
        logits1 = beta_eff * (q1_mb - np.max(q1_mb)) + bias_vec
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage-2 policy with UCB exploration bonus based on visit uncertainty
        # Uncertainty proxy: 1/sqrt(N)
        ucb_bonus = ucb_w * (1.0 / np.sqrt(n2[s] + eps))
        q2_aug = q2[s] + ucb_bonus
        logits2 = beta_eff * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Update counts
        n2[s, a2] += 1.0

        # Stage-2 asymmetric learning
        pe2 = r - q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * pe2

        # No separate MF at stage 1; values for stage 1 are recomputed from updated q2 via MB

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)","['alpha_pos', 'alpha_neg', 'beta', 'ucb_base', 'common_bias0']"
iter9_run0_participant0.json,cognitive_model1,495.7475159352964,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Volatility-scaled reward learning, anxiety-weighted rare-transition correction, and second-stage choice-kernel.
    
    Rationale
    - Second-stage reward learning rate increases with unsigned prediction error and anxiety (volatility-scaled).
    - After rare transitions, an anxiety-weighted bias shifts first-stage values toward the ship whose common destination matches the last visited planet (reward-independent).
    - A second-stage, state-specific choice-kernel captures short-term repetition at the alien level.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet index (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial.
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: length-6
        alpha_r: base learning rate for second-stage Q-values.
        beta: inverse temperature used at both stages [0,10].
        nu_vol: volatility gain on learning rate via |PE| and anxiety.
        kappa_choice2: kernel learning rate for second-stage choice kernel (strength of updating toward chosen).
        decay_k2: decay of second-stage choice kernel toward 0 each trial.
        anx_common: magnitude of anxiety-weighted bias after rare transitions at stage 1.

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_r, beta, nu_vol, kappa_choice2, decay_k2, anx_common = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clip parameters to bounds
    beta = max(1e-6, min(10.0, beta))
    alpha_r = min(1.0, max(0.0, alpha_r))
    nu_vol = min(1.0, max(0.0, nu_vol))
    kappa_choice2 = min(1.0, max(0.0, kappa_choice2))
    decay_k2 = min(1.0, max(0.0, decay_k2))
    anx_common = min(1.0, max(0.0, anx_common))

    # Fixed transition structure (common A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value tables
    q2 = np.zeros((2, 2))       # second-stage Q-values for planets X(0), Y(1), aliens (0/1)
    K2 = np.zeros((2, 2))       # second-stage choice kernel (state-dependent)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_state = None
    last_transition_rare = 0  # for biasing next first-stage choice

    for t in range(n_trials):
        # Model-based first-stage values from current second-stage values
        v2 = np.max(q2, axis=1)  # best alien value on each planet
        q1_mb = T @ v2
        q1 = q1_mb.copy()

        # Anxiety-weighted rare-transition correction (reward-independent)
        # If the previous transition was rare, bias toward the ship whose common destination equals last_state.
        if last_state is not None and last_transition_rare == 1:
            # ship 0 commonly -> state 0; ship 1 commonly -> state 1
            target_ship = last_state  # 0 if last_state==X, 1 if Y
            bias = anx_common * stai
            q1[target_ship] += bias
            q1[1 - target_ship] -= bias

        # Softmax for first-stage choice
        a1 = action_1[t]
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Second stage policy with choice-kernel bias
        s = state[t]
        a2 = action_2[t]
        q2_biased = q2[s] + K2[s]
        exp_q2 = np.exp(beta * (q2_biased - np.max(q2_biased)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Second-stage learning with volatility scaling driven by |PE| and anxiety
        pe2 = r - q2[s, a2]
        alpha_eff = alpha_r * (1.0 + nu_vol * stai * abs(pe2))
        alpha_eff = min(1.0, max(0.0, alpha_eff))
        q2[s, a2] += alpha_eff * pe2

        # Update second-stage choice kernel: decay then strengthen chosen
        K2[s] *= (1.0 - decay_k2)
        # move chosen action kernel toward +1 and the other toward -1 with rate kappa_choice2
        K2[s, a2] += kappa_choice2 * (1.0 - K2[s, a2])
        K2[s, 1 - a2] += kappa_choice2 * (-1.0 - K2[s, 1 - a2])

        # Determine whether transition was rare on this trial for next-trial bias
        common_dest = a1  # 0->X, 1->Y
        last_transition_rare = int(s != common_dest)
        last_state = s
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'nu_vol', 'kappa_choice2', 'decay_k2', 'anx_common']"
iter9_run0_participant0.json,cognitive_model2,510.863985949108,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Arbitrated MB/MF stage-1 control via surprise and anxiety, with global first-stage choice trace and anxiety-scaled temperature.
    
    Rationale
    - Stage-1 action values combine model-based (MB) and model-free (MF) components.
    - Arbitration weight increases with transition surprise (rare) and decreases with anxiety.
    - A decaying first-stage choice trace captures ship-level perseveration.
    - Anxiety also reduces effective inverse temperature (more stochastic) at both stages.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: (n_trials,) first-stage choices (0=A, 1=U).
    - state: (n_trials,) reached planet (0=X, 1=Y).
    - action_2: (n_trials,) second-stage choices (0 or 1).
    - reward: (n_trials,) reward per trial.
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: length-6
        alpha_r: learning rate for second-stage Q-values and MF first-stage backup.
        beta: base inverse temperature [0,10].
        psi_mix: baseline logit for MB weight at stage 1 (higher -> more MB).
        chi_surprise: sensitivity of MB weight to rare-transition surprise (rare -> more MB).
        rep1_decay: strength of first-stage choice trace update/decay.
        anx_beta_scale: scales how much anxiety reduces beta (exploration under anxiety).

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha_r, beta, psi_mix, chi_surprise, rep1_decay, anx_beta_scale = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clip parameters
    beta = max(1e-6, min(10.0, beta))
    alpha_r = min(1.0, max(0.0, alpha_r))
    psi_mix = min(1.0, max(0.0, psi_mix))     # keep in [0,1], then map to logit below
    chi_surprise = min(1.0, max(0.0, chi_surprise))
    rep1_decay = min(1.0, max(0.0, rep1_decay))
    anx_beta_scale = min(1.0, max(0.0, anx_beta_scale))

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q2 = np.zeros((2, 2))      # second-stage
    q1_mf = np.zeros(2)        # stage-1 model-free values
    choice_trace1 = np.zeros(2)  # first-stage choice trace

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map psi_mix in [0,1] to a logit center around 0 for flexibility
    # logit(x) with stabilization:
    x = max(1e-6, min(1 - 1e-6, psi_mix))
    psi_logit = np.log(x) - np.log(1 - x)

    last_a1 = None

    for t in range(n_trials):
        # Model-based Q1
        v2 = np.max(q2, axis=1)
        q1_mb = T @ v2

        # Surprise (1 if rare transition on previous trial, else 0)
        if t == 0:
            surprise_prev = 0.0
        else:
            a1_prev = action_1[t - 1]
            s_prev = state[t - 1]
            common_prev = a1_prev
            surprise_prev = 1.0 if (s_prev != common_prev) else 0.0

        # Arbitration weight w_mb via logistic mapping
        # Base logit + surprise gain - anxiety penalty
        logit_w = psi_logit + chi_surprise * surprise_prev - (stai * psi_logit)  # anxiety reduces MB weight proportional to baseline
        w_mb = 1.0 / (1.0 + np.exp(-logit_w))
        w_mb = min(1.0, max(0.0, w_mb))

        # Combine MB and MF for stage-1 Q
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + choice_trace1

        # Anxiety-scaled temperature (higher anxiety -> lower beta_eff)
        beta_eff = beta * (1.0 - anx_beta_scale * stai)
        beta_eff = max(1e-6, min(10.0, beta_eff))

        # First-stage policy
        a1 = action_1[t]
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        a2 = action_2[t]
        exp_q2 = np.exp(beta_eff * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Second-stage update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # One-step MF backup to stage-1 for chosen a1 using current second-stage state value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

        # Update first-stage choice trace: decay overall, then increment chosen
        choice_trace1 *= (1.0 - rep1_decay)
        choice_trace1[a1] += rep1_decay

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'psi_mix', 'chi_surprise', 'rep1_decay', 'anx_beta_scale']"
iter9_run0_participant0.json,cognitive_model3,536.8520755703436,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric PE learning modulated by anxiety, transition relearning with surprise-driven reset, and planet preference bias.
    
    Rationale
    - Second-stage learning uses an anxiety-scaled asymmetry: negative PEs learn faster when anxiety is high.
    - Transition matrix is learned online; after rare transitions it partially resets toward uncertainty, scaled by anxiety.
    - First-stage values include an anxiety-modulated planet preference projected through the learned transition.
    - A global second-stage stickiness (across planets) biases repeating the last alien choice.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: (n_trials,) first-stage choices (0=A, 1=U).
    - state: (n_trials,) reached planet (0=X, 1=Y).
    - action_2: (n_trials,) second-stage choices (0 or 1).
    - reward: (n_trials,) reward per trial.
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: length-7
        alpha0: base learning rate for second-stage Q-values.
        beta: inverse temperature used at both stages [0,10].
        kappa_loss_av: scales anxiety-driven boost for negative PE learning (loss aversion in learning).
        alpha_t: base learning rate for transition matrix rows for the chosen ship.
        reset_t: surprise-driven reset toward uniform transitions after rare transitions (scaled by anxiety).
        phi_planet: baseline preference for planet X over Y; expressed via first-stage expected reach (MB term).
        stick2g: global second-stage stickiness to repeat last alien (across states).

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha0, beta, kappa_loss_av, alpha_t, reset_t, phi_planet, stick2g = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Bound parameters
    beta = max(1e-6, min(10.0, beta))
    alpha0 = min(1.0, max(0.0, alpha0))
    kappa_loss_av = min(1.0, max(0.0, kappa_loss_av))
    alpha_t = min(1.0, max(0.0, alpha_t))
    reset_t = min(1.0, max(0.0, reset_t))
    phi_planet = min(1.0, max(0.0, phi_planet))
    stick2g = min(1.0, max(0.0, stick2g))

    # Initialize learned transitions near common structure but learnable
    T_hat = np.array([[0.65, 0.35],
                      [0.35, 0.65]], dtype=float)

    q2 = np.zeros((2, 2))  # second-stage Q-values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a2 = None  # for global second-stage stickiness

    for t in range(n_trials):
        # Planet preference projected to actions via current T_hat:
        # preference vector over planets: pref[X]=+b, pref[Y]=-b, with anxiety sign/scale
        b = phi_planet * (2.0 * stai - 1.0)  # high anxiety -> closer to +phi or -phi depending on stai>0.5
        pref_planet = np.array([b, -b])
        pref_action = T_hat @ pref_planet

        # Model-based stage-1 values from learned transitions + planet preference
        v2 = np.max(q2, axis=1)
        q1_mb = T_hat @ v2
        q1 = q1_mb + pref_action

        # First-stage policy
        a1 = action_1[t]
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with global stickiness
        s = state[t]
        a2 = action_2[t]
        bias2 = np.zeros(2)
        if last_a2 is not None:
            bias2[last_a2] += stick2g
            bias2[1 - last_a2] -= stick2g
        exp_q2 = np.exp(beta * ((q2[s] + bias2) - np.max(q2[s] + bias2)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Asymmetric, anxiety-modulated second-stage learning
        pe2 = r - q2[s, a2]
        neg = 1.0 if pe2 < 0.0 else 0.0
        pos = 1.0 - neg
        # negative PE gets boosted learning rate with anxiety; positive PE slightly discounted to keep overall in [0,1]
        alpha_eff = alpha0 * (pos * (1.0 - 0.5 * kappa_loss_av * stai) + neg * (1.0 + kappa_loss_av * stai))
        alpha_eff = min(1.0, max(0.0, alpha_eff))
        q2[s, a2] += alpha_eff * pe2

        # Update learned transition row for chosen a1 toward observed state s
        row = T_hat[a1].copy()
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            row[ss] = row[ss] + alpha_t * (target - row[ss])

        # Normalize and optionally reset toward uncertainty after rare transition
        # Rare if s != common destination (a1)
        common_dest = a1
        is_rare = int(s != common_dest)
        if is_rare == 1:
            # mix with uniform according to reset scaled by anxiety
            lam = reset_t * stai
            row = (1.0 - lam) * row + lam * np.array([0.5, 0.5])

        row_sum = np.sum(row)
        if row_sum <= 0.0:
            row = np.array([0.5, 0.5])
        else:
            row = row / row_sum
        T_hat[a1] = row

        last_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha0', 'beta', 'kappa_loss_av', 'alpha_t', 'reset_t', 'phi_planet', 'stick2g']"
iter9_run0_participant12.json,cognitive_model1,475.8541205317017,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated arbitration and learned transitions.
    
    This model blends a model-free (MF) and model-based (MB) controller at stage 1.
    The transition matrix T(s|a) is learned online and row-normalized via a soft update.
    Anxiety (stai) modulates:
      - the effective MB/MF arbitration weight (w_eff)
      - the transition learning rate (alpha_T)
    An eligibility trace backs up second-stage prediction errors to the chosen first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien on that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration and transition learning.
    model_parameters : list or array
        [alpha_v, beta, w_hyb, anx_trans, elig]
        Bounds:
          alpha_v   in [0,1]  : MF learning rate for second-stage values
          beta      in [0,10] : inverse temperature for both stages
          w_hyb     in [0,1]  : baseline MB weight at stage 1 (before anxiety modulation)
          anx_trans in [0,1]  : strength of anxiety's effect on arbitration and transition learning
          elig      in [0,1]  : eligibility trace for backing up stage-2 PE to stage-1 MF value
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_v, beta, w_hyb, anx_trans, elig = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition model T(a, s)
    T = np.ones((2, 2)) * 0.5  # start uninformative, rows sum to 1

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation term in [-1, 1]
    mod = 2.0 * (stai - 0.5)

    # Effective MB weight: move w_hyb toward MB (increase) when mod>0 and anx_trans large
    w_eff_base_shift = anx_trans * mod
    w_eff = np.clip(w_hyb + w_eff_base_shift * (1.0 - w_hyb), 0.0, 1.0)

    # Transition learning rate increases with anxiety if mod>0
    alpha_T = np.clip(0.5 + 0.5 * anx_trans * mod, 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based Q at stage 1 from current transition beliefs and MF q2
        max_q2 = np.max(q2, axis=1)         # best alien on each planet
        q1_mb = T @ max_q2                  # expected value per spaceship via learned transitions

        # Hybrid stage-1 value
        q1 = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Softmax for stage 1
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy: softmax over planet-conditional q2
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # MF learning at stage 2
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha_v * pe2

        # Back up to stage 1 MF with eligibility
        q1_mf[a1] += elig * alpha_v * pe2

        # Additional temporal-difference consistency update at stage 1 MF
        td1 = (q2[s, a2] - q1_mf[a1])
        q1_mf[a1] += (1.0 - elig) * alpha_v * td1

        # Update transition beliefs for the chosen spaceship: soft row update keeps row summing to 1
        # Move probability mass toward the observed state s with rate alpha_T
        T[a1, :] = (1.0 - alpha_T) * T[a1, :]
        T[a1, s] += alpha_T

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_v', 'beta', 'w_hyb', 'anx_trans', 'elig']"
iter9_run0_participant12.json,cognitive_model2,512.0958416734384,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pure model-based with anxiety-modulated utility curvature and forgetting.
    
    The agent plans at stage 1 using the known common-rare transition structure (0.7/0.3).
    At stage 2, it learns expected utility values, where instantaneous utility is u(r) = r**gamma_eff.
    Anxiety increases curvature (risk aversion) and forgetting of unchosen actions.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien on that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Modulates utility curvature and forgetting.
    model_parameters : list or array
        [alpha_u, beta, phi_curv, anx_gain, kappa_forget]
        Bounds:
          alpha_u      in [0,1]  : learning rate for expected utility at stage 2
          beta         in [0,10] : inverse temperature for both stages
          phi_curv     in [0,1]  : baseline utility curvature (gamma baseline)
          anx_gain     in [0,1]  : strength of anxiety's influence on curvature
          kappa_forget in [0,1]  : baseline forgetting rate for unchosen second-stage actions
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_u, beta, phi_curv, anx_gain, kappa_forget = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Stage-2 expected utility values
    q2u = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated curvature: gamma_eff in (0,1], more concave with higher stai
    # Move phi_curv toward stronger concavity as stai and anx_gain increase.
    gamma_eff = np.clip(phi_curv * (1.0 - anx_gain * stai) + (1.0 - anx_gain * stai) * 0.0 + anx_gain * (1.0 - stai) * 0.0 + 1e-9, 1e-6, 1.0)
    # Simplify: gamma_eff = phi_curv * (1 - anx_gain*stai), clipped to (0,1]

    # Anxiety-modulated forgetting: stronger with higher stai
    forget_eff = np.clip(kappa_forget * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage 1: MB evaluation using expected utility values
        max_q2 = np.max(q2u, axis=1)
        q1_mb = T_known @ max_q2

        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2: softmax over expected utility on visited planet
        q2s = q2u[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Utility of observed reward
        u = reward[t] ** gamma_eff

        # Update chosen action toward utility
        q2u[s, a2] += alpha_u * (u - q2u[s, a2])

        # Forget unchosen action on the visited planet
        other = 1 - a2
        q2u[s, other] = (1.0 - forget_eff) * q2u[s, other]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_u', 'beta', 'phi_curv', 'anx_gain', 'kappa_forget']"
iter9_run0_participant12.json,cognitive_model3,521.4060212508506,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-bonus exploration with anxiety-scaled bonus and temperature collapse.
    
    Second-stage action values are learned model-free. An exploration bonus inversely
    proportional to visit count is added to second-stage values; anxiety scales this bonus.
    Stage-1 is model-based using the known transition structure and bonus-augmented second-stage values.
    Inverse temperature collapses with anxiety (more noise for higher STAI).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien on that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Modulates exploration bonus and decision temperature.
    model_parameters : list or array
        [alpha_r, beta0, xi_bonus, anx_explore, tau_decay]
        Bounds:
          alpha_r     in [0,1]  : learning rate for second-stage rewards
          beta0       in [0,10] : baseline inverse temperature
          xi_bonus    in [0,1]  : base weight of uncertainty bonus
          anx_explore in [0,1]  : how strongly STAI scales the bonus
          tau_decay   in [0,1]  : fraction by which STAI reduces temperature (higher = more collapse)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """"""
    alpha_r, beta0, xi_bonus, anx_explore, tau_decay = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # MF values at stage 2 and visit counts for uncertainty bonus
    q2 = np.zeros((2, 2))
    visits = np.ones((2, 2))  # start at 1 to avoid division by zero

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-scaled temperature: higher STAI -> lower beta_eff
    beta_eff = beta0 * (1.0 - tau_decay * stai)
    beta_eff = max(beta_eff, 1e-6)

    # Anxiety-scaled exploration bonus factor
    bonus_scale = xi_bonus * (1.0 + anx_explore * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute uncertainty bonus for both planets
        bonus = bonus_scale / np.sqrt(visits + 1e-9)
        # Bonus-augmented second-stage values
        q2_plus = q2 + bonus

        # Stage 1 MB using bonus-augmented values
        max_q2_plus = np.max(q2_plus, axis=1)
        q1_mb = T_known @ max_q2_plus

        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy on visited planet with bonus
        q2s_plus = q2_plus[s].copy()
        q2c = q2s_plus - np.max(q2s_plus)
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # MF learning at stage 2
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Update visit counts
        visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_r', 'beta0', 'xi_bonus', 'anx_explore', 'tau_decay']"
iter9_run0_participant14.json,cognitive_model1,538.0324051834536,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety- and surprise-adaptive arbitration.
    
    Mechanism:
    - Stage 2 values are learned via MF Q-learning.
    - Stage 1 action values combine model-based (MB) planning with a model-free
      (MF) cache. The arbitration weight toward MB is dynamically modulated by:
        (a) participant anxiety (stai), and
        (b) the magnitude of the previous trial's reward prediction error
            (""surprise""), pushing toward MF when surprises are large.
    - Transitions are assumed known: A->X common, U->Y common (0.7/0.3).
    
    Parameters (model_parameters):
    - alpha_b: base learning rate for rewards (Q2) and MF cache updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - mix0: base MB mixing weight at stage 1, in [0,1]
    - anx_mix: anxiety modulation of MB weight (positive increases MB reliance
               with higher anxiety; negative decreases), in [0,1] effectively
               scaled by centered stai
    - k_pe: arbitration sensitivity to previous unsigned prediction error
            (larger shifts weight toward MF), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (e.g., 0/1)
    - stai: array-like with a single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_b, beta, mix0, anx_mix, k_pe = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure (rows: spaceships A/U, cols: planets X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q2 = np.zeros((2, 2))   # per-planet, per-alien
    q1_mf = np.zeros(2)     # MF cache for stage 1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_abs_pe = 0.0

    # Anxiety-centered scaling around the medium/high boundary (0.51)
    anx_centered = stai - 0.51

    for t in range(n_trials):

        # Model-based projection of stage-1 values
        max_q2 = np.max(q2, axis=1)      # best alien per planet
        q1_mb = T @ max_q2               # MB value for spaceships

        # Arbitration weight toward MB considering anxiety and surprise
        mix = mix0 + anx_mix * anx_centered - k_pe * prev_abs_pe
        mix = np.clip(mix, 0.0, 1.0)

        q1 = mix * q1_mb + (1.0 - mix) * q1_mf

        # Stage 1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage 2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_b * pe2

        # Stage 1 MF cache update toward realized second-stage value
        # (bootstrapping MF cache from the experienced branch)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_b * pe1

        # Update previous unsigned prediction error for arbitration
        prev_abs_pe = min(1.0, abs(pe2))

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_b', 'beta', 'mix0', 'anx_mix', 'k_pe']"
iter9_run0_participant14.json,cognitive_model2,537.9932700667008,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid planning with anxiety-reduced planning weight and stage-2 choice kernel.
    
    Mechanism:
    - Stage 1 uses a hybrid of model-based (MB) planning and model-free (MF)
      cache, with the planning weight reduced by higher anxiety.
    - Stage 2 uses MF Q-learning combined with a per-planet choice kernel that
      captures perseveration/exploitative inertia; kernel strength increases
      with anxiety.
    - Transitions are assumed known: A->X, U->Y with 0.7 common probability.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate (Q2 and MF cache), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - plan0: baseline weight for MB at stage 1, in [0,1]
    - anx_shift: how much anxiety shifts MB weight (reduces plan with higher stai)
                 and increases the stage-2 kernel strength, in [0,1]
    - k2: choice-kernel learning/decay rate at stage 2, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (e.g., 0/1)
    - stai: array-like with a single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha_r, beta, plan0, anx_shift, k2 = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # second-stage MF Q-values
    q1_mf = np.zeros(2)     # stage-1 MF cache
    kernel2 = np.zeros((2, 2))  # per-planet choice kernel for stage 2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    anx_centered = stai - 0.51
    # Higher anxiety reduces planning and increases perseveration kernel
    plan_eff = np.clip(plan0 - anx_shift * anx_centered, 0.0, 1.0)
    theta2 = k2 * (1.0 + np.clip(stai, 0.0, 1.0) * anx_shift)  # kernel strength

    for t in range(n_trials):

        # Stage 1 hybrid values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = plan_eff * q1_mb + (1.0 - plan_eff) * q1_mf

        logits1 = beta * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with choice kernel
        s = state[t]
        logits2 = beta * q2[s] + theta2 * kernel2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Q2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Stage-1 MF cache update toward realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

        # Update stage-2 choice kernel: decay + reinforce chosen
        kernel2[s] *= (1.0 - k2)
        kernel2[s, a2] += k2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'plan0', 'anx_shift', 'k2']"
iter9_run0_participant14.json,cognitive_model3,554.2727453925696,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Learned transitions with anxiety-modulated valence asymmetry and rare-transition bias.
    
    Mechanism:
    - Learns the transition matrix from experience.
    - Stage 1 uses purely model-based planning from the learned transitions,
      but includes a bias term based on whether the previous transition was
      common vs rare: rare transitions bias switching; common transitions bias
      repeating. The magnitude of this bias grows with anxiety.
    - Stage 2 MF Q-learning uses an anxiety-modulated valence asymmetry:
      higher anxiety increases learning from negative outcomes (and/or reduces
      learning from positive), controlled by 'valence'.
    
    Parameters (model_parameters):
    - alpha: base learning rate for rewards (Q2), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - valence: strength of anxiety-driven valence asymmetry in Q2 learning, in [0,1]
    - trans_sens: sensitivity to previous trial's transition type in stage-1 bias, in [0,1]
    - eta_t: transition learning rate (for learned transition matrix), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (e.g., 0/1)
    - stai: array-like with a single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta, valence, trans_sens, eta_t = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition matrix with weak prior toward common mapping
    # Rows: A(0), U(1); Cols: X(0), Y(1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))   # second-stage MF Q-values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_s = None

    for t in range(n_trials):

        # Model-based Q at stage 1 from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Bias from previous transition type:
        # - If previous transition was rare, bias to switch.
        # - If previous transition was common, bias to repeat.
        bias = np.zeros(2)
        if prev_a1 is not None and prev_s is not None:
            was_common = (prev_s == prev_a1)  # A->X and U->Y are ""common""
            # Anxiety scales the magnitude of the bias
            anx_gain = (0.5 + 0.5 * np.clip(stai, 0.0, 1.0))
            mag = trans_sens * anx_gain
            if was_common:
                bias[prev_a1] += mag  # repeat after common
            else:
                bias[1 - prev_a1] += mag  # switch after rare

        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (pure MF at second stage)
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Valence- and anxiety-modulated learning rate
        # Positive outcomes (r>0.5): amplify learning less with higher anxiety if valence<0,
        # or more if valence>0. Negative outcomes (r<=0.5) amplified with higher anxiety when valence>0.
        if r > 0.5:
            alpha_eff = alpha * (1.0 + valence * (stai - 0.5))
        else:
            alpha_eff = alpha * (1.0 - valence * (stai - 0.5))
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Q2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * pe2

        # Learn the transition matrix row for chosen spaceship toward observed planet
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += eta_t * (target - T[a1])
        # Normalize to keep valid probabilities
        T[a1] = np.clip(T[a1], 1e-8, 1.0)
        T[a1] /= np.sum(T[a1])

        prev_a1 = a1
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'valence', 'trans_sens', 'eta_t']"
iter9_run0_participant15.json,cognitive_model1,573.6407397445566,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-modulated eligibility trace with choice perseveration (pure model-free).
    
    Core ideas:
    - Two-stage model-free Q-learning with an eligibility trace Î» that increases when anxiety is low.
    - Perseveration (stickiness) bias to repeat previous actions at both stages; bias grows with anxiety.
    - One inverse temperature beta for both stages.
    
    Parameters (all used; total=5):
    - eta: [0,1] Learning rate for Q-value updates.
    - beta: [0,10] Inverse temperature for softmax at both stages.
    - lam0: [0,1] Baseline eligibility trace parameter.
    - kappa_p: [0,1] Base magnitude of perseveration bias.
    - anx_gain: [0,1] Modulates Î» with anxiety: Î»_eff = lam0 + anx_gain*(1 - stai).
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [eta, beta, lam0, kappa_p, anx_gain].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    eta, beta, lam0, kappa_p, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective eligibility and perseveration magnitudes
    lam = max(0.0, min(1.0, lam0 + anx_gain * (1.0 - stai)))  # higher when anxiety is lower
    stick_mag = kappa_p * (0.5 + 0.5 * stai)                  # stronger stickiness with higher anxiety

    # Q-values
    q1 = np.zeros(2)           # stage-1 MF values over ships
    q2 = 0.5 * np.ones((2, 2)) # stage-2 MF values over aliens per planet

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration traces (last choices), initialized to 0 (no bias on first trial)
    last_a1 = None
    last_a2 = [None, None]  # separate memory per planet

    for t in range(n_trials):
        s = int(state[t])

        # Perseveration features
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stick_mag

        bias2 = np.zeros(2)
        la2 = last_a2[s]
        if la2 is not None:
            bias2[la2] += stick_mag

        # Stage-1 policy (softmax over q1 + bias)
        prefs1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(prefs1 - np.max(prefs1))
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax over q2[s] + bias)
        prefs2 = beta * (q2[s] - np.max(q2[s])) + bias2
        probs2 = np.exp(prefs2 - np.max(prefs2))
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = float(reward[t])

        # Save q2 value before updating for lambda bootstrapping
        q2_old_sa = q2[s, a2]

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta * pe2

        # Stage-1 TD(Î») update: mix of bootstrapping via q2_old and outcome r
        # pe1_lambda = (1 - Î») * (q2_old - q1[a1]) + Î» * (r - q1[a1])
        pe1 = (1.0 - lam) * (q2_old_sa - q1[a1]) + lam * (r - q1[a1])
        q1[a1] += eta * pe1

        # Update perseveration memories
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['eta', 'beta', 'lam0', 'kappa_p', 'anx_gain']"
iter9_run0_participant15.json,cognitive_model2,573.887670416665,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Count-based exploration at stage-2 with anxiety-dampened novelty and transition-aware credit at stage-1.
    
    Core ideas:
    - Stage-2 uses a count-based bonus (novelty seeking), scaled down by anxiety.
    - Stage-1 Q is model-free but its credit assignment depends on transition commonality,
      with stronger preference to credit common transitions (and to discount rare) as anxiety increases.
    - Hybrid action values at stage-1: blend of MF Q1 and a simple model-based estimate via known transitions.
    
    Parameters (all used; total=5):
    - alpha_r: [0,1] Learning rate for reward prediction errors (applied to both stages).
    - beta: [0,10] Inverse temperature for softmax at both stages.
    - xi0: [0,1] Base magnitude of count-based exploration bonus at stage-2.
    - kappa_anx: [0,1] Scales how much anxiety suppresses exploration: xi_eff = xi0*(1 - kappa_anx*stai).
    - omega_trans: [0,1] Strength of common-vs-rare credit asymmetry at stage-1 (stronger with higher anxiety).
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [alpha_r, beta, xi0, kappa_anx, omega_trans].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """"""
    alpha_r, beta, xi0, kappa_anx, omega_trans = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Effective exploration bonus magnitude decreases with anxiety
    xi_eff = xi0 * max(0.0, 1.0 - kappa_anx * stai)

    # Q-values
    q1_mf = np.zeros(2)           # stage-1 MF values
    q2 = 0.5 * np.ones((2, 2))    # stage-2 values per planet/alien

    # Count table for exploration bonus
    N = np.ones((2, 2), dtype=float)  # start at 1 to avoid division by zero

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MB-MF arbitration weight depends on anxiety (less anxious -> more MB)
    w_mb = 0.5 + 0.5 * (1.0 - stai)

    for t in range(n_trials):
        s = int(state[t])

        # Stage-2 policy with count-based exploration bonus
        bonus = xi_eff / np.sqrt(N[s] + 1e-8)
        q2_bonus = q2[s] + bonus

        z2 = beta * (q2_bonus - np.max(q2_bonus))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Compute model-based proxy for stage-1: expected best value at next state
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Observe reward and update
        r = float(reward[t])

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2
        N[s, a2] += 1.0

        # Transition commonality
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Weight common credits more, rare credits less; effect larger with higher anxiety and omega_trans
        base = 0.5
        ampl = 0.5 * omega_trans * (0.5 + 0.5 * stai)  # in [0,0.5]
        w_common = base + ampl
        w_rare = base - ampl
        w_credit = w_common if is_common else w_rare

        # Stage-1 MF credit toward the realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += (alpha_r * w_credit) * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_r', 'beta', 'xi0', 'kappa_anx', 'omega_trans']"
iter9_run0_participant21.json,cognitive_model1,524.9837514335527,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Optimistic-uncertainty model with anxiety-modulated optimism and adaptive subjective transitions.

    Core ideas:
    - Stage-2 Q-values are learned via incremental prediction errors.
    - An uncertainty bonus encourages exploration early; its magnitude is modulated by anxiety.
      Higher anxiety reduces optimism/exploration.
    - Stage-1 planning uses a learned subjective transition matrix that adapts to experienced transitions.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = opt0 (0 to 1): baseline optimism/uncertainty bonus strength
    - model_parameters[3] = anx_opt (0 to 1): strength of anxiety modulation of optimism
        optimism_eff = clip(opt0 + (0.5 - stai) * anx_opt, 0, 1); higher stai lowers optimism if anx_opt>0
    - model_parameters[4] = kappaT (0 to 1): learning rate for updating subjective transition matrix

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial on the reached planet
    - reward: array of floats in [0,1], coins received per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified

    Returns:
    - Negative log-likelihood of the observed sequence of choices (stage 1 and stage 2).
    """"""
    alpha, beta, opt0, anx_opt, kappaT = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize canonical transition structure (A->X common, U->Y common)
    T0 = np.array([[0.7, 0.3],
                   [0.3, 0.7]], dtype=float)
    # Subjective transition model starts at canonical but adapts with kappaT
    T_hat = T0.copy()

    # Stage-2 values and simple visitation counts for uncertainty
    q2 = np.zeros((2, 2), dtype=float)
    n_sa = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Anxiety-modulated optimism/uncertainty bonus
    optimism_eff = np.clip(opt0 + (0.5 - stai_val) * anx_opt, 0.0, 1.0)

    for t in range(n_trials):
        # Compute uncertainty bonus for each state-action
        bonus = optimism_eff / np.sqrt(n_sa + 1.0)

        # Stage-1: model-based planning using subjective transitions and optimistic values
        max_q2_bonus = np.max(q2 + bonus, axis=1)   # shape (2,)
        q1_mb = T_hat @ max_q2_bonus               # shape (2,)
        q1c = q1_mb - np.max(q1_mb)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2: softmax on optimistic values in the actually reached state
        s = int(state[t])
        q2_net = q2[s] + bonus[s]
        q2c = q2_net - np.max(q2_net)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Learning at stage 2
        n_sa[s, a2] += 1.0
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update subjective transition model for the chosen stage-1 action toward observed state
        # Move the chosen row toward a one-hot vector indicating the observed state
        # This preserves row stochasticity.
        oh = np.array([0.0, 0.0])
        oh[s] = 1.0
        T_hat[a1] = (1.0 - kappaT) * T_hat[a1] + kappaT * oh
        # Ensure numerical issues don't break stochasticity
        T_hat[a1] = T_hat[a1] / np.sum(T_hat[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'opt0', 'anx_opt', 'kappaT']"
iter9_run0_participant21.json,cognitive_model2,510.1531460558224,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Asymmetric model-free learning with eligibility trace and anxiety-driven lapse.

    Core ideas:
    - Stage-2 and Stage-1 model-free values are updated using asymmetric learning rates
      for positive vs negative prediction errors.
    - An eligibility trace propagates Stage-2 outcomes to Stage-1 values on the chosen action.
    - Choice policy includes an anxiety-driven lapse component: higher anxiety -> more random choice.

    Parameters (bounds):
    - model_parameters[0] = a_pos (0 to 1): learning rate for positive prediction errors
    - model_parameters[1] = a_neg (0 to 1): learning rate for negative prediction errors
    - model_parameters[2] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[3] = lapse0 (0 to 1): baseline lapse mixing weight
        Effective lapse = lapse0 * stai (higher anxiety increases random responding)
    - model_parameters[4] = lam (0 to 1): eligibility trace strength from stage 2 to stage 1

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial on the reached planet
    - reward: array of floats in [0,1], coins received per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified

    Returns:
    - Negative log-likelihood of the observed sequence of choices (stage 1 and stage 2).
    """"""
    a_pos, a_neg, beta, lapse0, lam = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective lapse increases with anxiety
    lapse = np.clip(lapse0 * stai_val, 0.0, 1.0)

    # Model-free values
    q1 = np.zeros(2, dtype=float)        # stage-1 action values (MF)
    q2 = np.zeros((2, 2), dtype=float)   # stage-2 values for each state

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        # Stage-1 policy (with lapse)
        q1c = q1 - np.max(q1)
        soft1 = np.exp(beta * q1c)
        soft1 = soft1 / np.sum(soft1)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (with lapse)
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        soft2 = np.exp(beta * q2c)
        soft2 = soft2 / np.sum(soft2)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Stage-2 learning with asymmetric rates
        delta2 = r - q2[s, a2]
        alpha2 = a_pos if delta2 >= 0.0 else a_neg
        q2[s, a2] += alpha2 * delta2

        # Stage-1 MF update via eligibility trace from Stage-2 PE (asymmetric)
        alpha1 = a_pos if delta2 >= 0.0 else a_neg
        q1[a1] += (alpha1 * lam) * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['a_pos', 'a_neg', 'beta', 'lapse0', 'lam']"
iter9_run0_participant21.json,cognitive_model3,534.5921847769213,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Risk-sensitive utility with volatility-adaptive learning and anxiety-modulated risk aversion.

    Core ideas:
    - Rewards are transformed by a power utility u(r) = r^(rho_eff),
      where rho_eff decreases with anxiety (more risk-averse with higher anxiety).
    - The learning rate adapts to recent outcome volatility estimated from absolute prediction errors.
    - Stage-1 uses model-based planning via fixed known transitions.

    Parameters (bounds):
    - model_parameters[0] = lr0 (0 to 1): base learning rate
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = rho0 (0 to 1): baseline risk sensitivity exponent
    - model_parameters[3] = anx_rho (0 to 1): anxiety modulation of risk sensitivity
        rho_eff = clip(rho0 - anx_rho * stai, 0, 1); higher stai -> smaller rho -> more concave utility
    - model_parameters[4] = tau_v (0 to 1): volatility smoothing rate for absolute PEs

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial on the reached planet
    - reward: array of floats in [0,1], coins received per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified

    Returns:
    - Negative log-likelihood of the observed sequence of choices (stage 1 and stage 2).
    """"""
    lr0, beta, rho0, anx_rho, tau_v = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (known to the participant)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Anxiety-modulated risk sensitivity
    rho_eff = np.clip(rho0 - anx_rho * stai_val, 0.0, 1.0)

    # Values and volatility tracker
    q2 = np.zeros((2, 2), dtype=float)
    vol = 0.0  # running estimate of absolute PE magnitude in [0,1]

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        # Stage-1 model-based planning using current stage-2 values
        max_q2 = np.max(q2, axis=1)         # best action per state
        q1_mb = T @ max_q2
        q1c = q1_mb - np.max(q1_mb)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Risk-sensitive utility of reward
        r = float(reward[t])
        # Avoid 0^0 by adding tiny epsilon inside power when needed
        u = (r + 1e-12) ** rho_eff

        # Prediction error and volatility update
        delta2 = u - q2[s, a2]
        vol = (1.0 - tau_v) * vol + tau_v * abs(delta2)
        # Volatility-adaptive learning rate: larger vol -> larger effective lr, bounded in [0, lr0]
        alpha_eff = lr0 * (0.5 + 0.5 * np.clip(vol, 0.0, 1.0))

        # Update stage-2 value
        q2[s, a2] += alpha_eff * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['lr0', 'beta', 'rho0', 'anx_rho', 'tau_v']"
iter9_run0_participant24.json,cognitive_model1,485.43919353640433,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated model-based reliance and exploration bonus.

    This model mixes a model-free (MF) learner with a model-based (MB) planner. It adds an
    uncertainty-driven exploration bonus at the second stage that is stronger with higher anxiety.
    Anxiety also shifts the MB/MF mixing weight toward more model-based control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, psi_mb0, chi_bonus0, chi_anx]
        - alpha in [0,1]: learning rate for MF values at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - psi_mb0 in [0,1]: baseline weight on MB control (before anxiety modulation).
        - chi_bonus0 in [0,1]: baseline strength of second-stage exploration bonus.
        - chi_anx in [0,1]: how much anxiety increases the exploration bonus.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, psi_mb0, chi_bonus0, chi_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T_fixed = np.array([[0.7, 0.3],  # A to [X, Y]
                        [0.3, 0.7]])  # U to [X, Y]

    # Values
    q1_mf = np.zeros(2)        # MF at stage 1
    q2_mf = np.zeros((2, 2))   # MF at stage 2 (per state, per alien)

    # Visit counts for exploration bonus
    n_visits = np.ones((2, 2))  # start at 1 to keep bonus finite

    # Anxiety-modulated MB weight and bonus strength
    # Higher anxiety pushes weight toward 1 (more MB).
    omega = psi_mb0 * (1 - s) + (1 - psi_mb0) * s
    # Bonus strength increases with anxiety
    bonus_gain = chi_bonus0 + chi_anx * s

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute MB Q-values at stage 1 using fixed transitions and current stage-2 MF values + bonus
        bonus_per_state = bonus_gain / np.sqrt(1.0 + np.sum(n_visits, axis=1))  # smaller bonus as state gets visited
        # Add per-action bonus at stage 2
        bonus_actions = bonus_gain / np.sqrt(n_visits)  # shape (2,2)
        max_q2_with_bonus = np.max(q2_mf + bonus_actions, axis=1)  # shape (2,)
        q1_mb = T_fixed @ max_q2_with_bonus

        # Mixed stage-1 values
        q1_mix = omega * q1_mb + (1 - omega) * q1_mf

        # Stage-1 policy
        prefs1 = q1_mix
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in the visited state, with exploration bonus
        st = state[t]
        prefs2 = q2_mf[st] + bonus_actions[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates (MF)
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2_mf[st, a2]
        q2_mf[st, a2] += alpha * pe2

        # Stage-1 MF update bootstrapping on stage-2 value (post-update)
        pe1 = q2_mf[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update visit counts
        n_visits[st, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'psi_mb0', 'chi_bonus0', 'chi_anx']"
iter9_run0_participant24.json,cognitive_model2,539.197357680262,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Pure model-based with learned transitions, anxiety-modulated surprise weighting, and lapse.

    This model learns:
      - the transition model T(a->s) online,
      - second-stage values for aliens,
    and chooses using a model-based plan. Anxiety increases sensitivity to surprising (rare) transitions
    when updating T, and increases a lapse rate that mixes choices with uniform randomness.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [nu_q, beta, k_T, eps0, eps_anx]
        - nu_q in [0,1]: learning rate for second-stage alien values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - k_T in [0,1]: baseline learning rate for transition probabilities.
        - eps0 in [0,1]: baseline lapse rate mixing with uniform choice.
        - eps_anx in [0,1]: how much anxiety increases the lapse rate.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    nu_q, beta, k_T, eps0, eps_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize transition model close to symmetric (uninformative)
    T = np.ones((2, 2)) * 0.5  # rows sum to 1
    q2 = np.zeros((2, 2))      # second-stage values

    # Common transitions given task structure (for defining ""surprise"")
    T_common = np.array([[0.7, 0.3],
                         [0.3, 0.7]])

    # Anxiety-modulated lapse rate
    eps = min(1.0, eps0 + eps_anx * s)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB action values at stage 1: expected value across next states
        max_q2 = np.max(q2, axis=1)      # value of best alien per state
        q1_mb = T @ max_q2               # shape (2,)

        # Stage-1 softmax with lapse
        prefs1 = q1_mb
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        soft1 = exp1 / np.sum(exp1)
        probs1 = (1 - eps) * soft1 + eps * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with lapse (within visited state)
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        soft2 = exp2 / np.sum(exp2)
        probs2 = (1 - eps) * soft2 + eps * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Update second-stage values (MB learns expected rewards at aliens)
        pe2 = r - q2[st, a2]
        q2[st, a2] += nu_q * pe2

        # Update transition model T based on surprise (rare transitions get larger updates with higher anxiety)
        # Identify whether the observed transition was ""common"" or ""rare"" under the known structure.
        observed_common_prob = T_common[a1, st]
        was_rare = 1.0 if observed_common_prob < 0.5 else 0.0

        # Surprise-scaled learning rate: increase for rare transitions proportionally to anxiety
        k_eff = k_T * (1.0 + s * 0.75 * was_rare)
        k_eff = min(1.0, max(0.0, k_eff))

        # Move T[a1] toward the one-hot of the observed state
        target = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        T[a1] = (1 - k_eff) * T[a1] + k_eff * target
        # Ensure row normalization
        T[a1] = T[a1] / np.sum(T[a1])

    eps_num = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps_num)) + np.sum(np.log(p_choice_2 + eps_num)))
    return nll

","['nu_q', 'beta', 'k_T', 'eps0', 'eps_anx']"
iter9_run0_participant24.json,cognitive_model3,461.3852280582463,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-free with anxiety-driven uncertainty aversion and forgetting.

    This model is purely model-free but includes two anxiety-dependent mechanisms:
      - Uncertainty aversion at the second stage: preference subtracts a penalty proportional
        to the current reward uncertainty of each alien; anxiety increases this penalty.
      - Value forgetting: Q-values decay toward 0.5 each trial; forgetting increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, kappa0, kappa_anx, phi0]
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa0 in [0,1]: baseline penalty weight for uncertainty at stage 2.
        - kappa_anx in [0,1]: how much anxiety increases uncertainty aversion.
        - phi0 in [0,1]: baseline forgetting rate toward 0.5 per trial (scaled by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """"""
    alpha, beta, kappa0, kappa_anx, phi0 = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Track counts and success counts per alien to estimate uncertainty
    # Beta-Bernoulli posterior with Beta(1,1) prior
    n_counts = np.zeros((2, 2))
    n_success = np.zeros((2, 2))

    # Anxiety-modulated parameters
    kappa = kappa0 + kappa_anx * s                 # uncertainty penalty weight
    phi = min(1.0, phi0 * (0.5 + s))               # forgetting toward 0.5, larger with anxiety

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute uncertainty estimates per alien: posterior variance of Beta(a,b)
        a_post = 1.0 + n_success
        b_post = 1.0 + n_counts - n_success
        p_post = a_post / (a_post + b_post)
        var_post = (a_post * b_post) / (((a_post + b_post) ** 2) * (a_post + b_post + 1.0))

        # Stage-1 policy (MF)
        prefs1 = q1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in visited state: subtract uncertainty penalty
        st = state[t]
        prefs2 = q2[st] - kappa * var_post[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = reward[t]

        # Update counts for uncertainty estimates
        n_counts[st, a2] += 1.0
        n_success[st, a2] += r

        # MF learning updates
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha * pe2

        # Bootstrapped update to stage-1 MF value using the chosen alien's value (post-update)
        pe1 = q2[st, a2] - q1[a1]
        q1[a1] += alpha * pe1

        # Forgetting toward 0.5 baseline, scaled by anxiety
        q2 = (1 - phi) * q2 + phi * 0.5
        q1 = (1 - phi) * q1 + phi * 0.5

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'kappa0', 'kappa_anx', 'phi0']"
iter9_run0_participant29.json,cognitive_model1,461.0869809933252,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with anxiety-modulated transition trust.
    The agent learns second-stage MF values and a simple transition model (per action),
    and combines MF and MB estimates for first-stage choice. Anxiety reduces trust in
    the learned transition model, effectively blending it toward an agnostic (0.5/0.5)
    transition belief, which shifts control toward model-free.

    Parameters
    - action_1: array-like (ints in {0,1}). First-stage choices (0=A, 1=U).
    - state: array-like (ints in {0,1}). Second-stage state reached (0=X, 1=Y).
    - action_2: array-like (ints in {0,1}). Second-stage choices within the reached state.
    - reward: array-like (floats in [0,1]). Outcome on each trial.
    - stai: array-like with one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha_q: [0,1] â learning rate for Q-values (both stages)
        beta: [0,10] â inverse temperature for both stages
        trust0: [0,1] â step size for updating transition beliefs and baseline trust
        mb_bias: [0,1] â baseline weight toward model-based control at stage 1
        anx_trust_gain: [0,1] â how strongly anxiety reduces transition trust (and thus MB control)

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_q, beta, trust0, mb_bias, anx_trust_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Model-free values
    q1_mf = np.zeros(2)          # A vs U
    q2 = np.zeros((2, 2))        # per state (X,Y) and alien

    # Transition beliefs per first-stage action: P(state=0|action) and P(state=1|action)
    # Initialize around the canonical mapping (A->X, U->Y) but allow learning to move it.
    T_hat = np.array([[0.7, 0.3],   # for action A: P(X), P(Y)
                      [0.3, 0.7]])  # for action U: P(X), P(Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Anxiety-reduced trust in transitions -> blend towards uniform (0.5/0.5)
        trust_c = np.clip(trust0 * (1.0 - anx_trust_gain * stai_val), 0.0, 1.0)
        T_eff = trust_c * T_hat + (1.0 - trust_c) * 0.5  # applied elementwise vs scalar -> broadcast to 0.5

        # Model-based stage-1 action values from current Q2 via expected max over second stage
        max_q2 = np.max(q2, axis=1)  # [X, Y]
        q1_mb = np.array([
            T_eff[0, 0] * max_q2[0] + T_eff[0, 1] * max_q2[1],
            T_eff[1, 0] * max_q2[0] + T_eff[1, 1] * max_q2[1]
        ])

        # MB/MF mixture weight: baseline mb_bias, diminished when trust is low (higher anxiety â lower trust)
        w_mb = np.clip(mb_bias * trust_c + (1.0 - trust_c) * 0.0, 0.0, 1.0)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage policy
        a1 = action_1[t]
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy in reached state
        s = state[t]
        a2 = action_2[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage MF value
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Update first-stage MF value by bootstrapping from second stage
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # Update transition beliefs for the chosen action using trust0 as a step size
        # Move probability mass toward the observed state for the chosen first-stage action.
        # Represent as a simple exponential moving average on P(state|action).
        if s == 0:
            T_hat[a1, 0] = (1.0 - trust0) * T_hat[a1, 0] + trust0 * 1.0
            T_hat[a1, 1] = 1.0 - T_hat[a1, 0]
        else:
            T_hat[a1, 1] = (1.0 - trust0) * T_hat[a1, 1] + trust0 * 1.0
            T_hat[a1, 0] = 1.0 - T_hat[a1, 1]

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha_q', 'beta', 'trust0', 'mb_bias', 'anx_trust_gain']"
iter9_run0_participant29.json,cognitive_model2,454.3554147123431,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""MF with anxiety-modulated valence learning and eligibility credit.
    Learning rate at stage 2 depends on outcome valence and anxiety, making positive
    outcomes learned faster under higher anxiety. Credit assignment from stage 2 to
    stage 1 uses an eligibility trace that grows with anxiety.

    Parameters
    - action_1: array-like (ints in {0,1}). First-stage choices (0=A, 1=U).
    - state: array-like (ints in {0,1}). Second-stage state reached (0=X, 1=Y).
    - action_2: array-like (ints in {0,1}). Second-stage choices within state.
    - reward: array-like (floats in [0,1]). Outcome.
    - stai: array-like with one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha_base: [0,1] â baseline learning rate
        beta: [0,10] â inverse temperature for both stages
        trace_decay: [0,1] â scales eligibility (credit from stage 2 to stage 1)
        valence_sensitivity: [0,1] â how strongly outcome valence modulates alpha
        anx_valence_gain: [0,1] â how strongly anxiety amplifies valence sensitivity

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    alpha_base, beta, trace_decay, valence_sensitivity, anx_valence_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety increases trace-based credit assignment modestly
    lambda_eff_const = np.clip(trace_decay * (1.0 + 0.5 * stai_val), 0.0, 1.0)

    eps = 1e-12

    for t in range(n_trials):
        # First-stage choice
        a1 = action_1[t]
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Second-stage choice
        s = state[t]
        a2 = action_2[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Outcome-valence- and anxiety-modulated learning rate at stage 2
        # Positive outcomes (r>0.5) increase alpha, negative decrease it; anxiety amplifies this.
        valence_term = valence_sensitivity * (r - 0.5)
        alpha2 = np.clip(alpha_base + (1.0 + anx_valence_gain * stai_val) * valence_term, 0.0, 1.0)

        # Update Q2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Eligibility-based credit assignment to stage 1
        # Move Q1[a1] toward the updated second-stage value with anxiety-scaled lambda.
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += np.clip(alpha_base * lambda_eff_const, 0.0, 1.0) * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)

","['alpha_base', 'beta', 'trace_decay', 'valence_sensitivity', 'anx_valence_gain']"
iter9_run0_participant29.json,cognitive_model3,461.0869809933252,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Count-based exploration bonus with anxiety-shifted meta-control split.
    The agent receives an uncertainty (novelty) bonus inversely proportional to the
    square-root of action visit counts. Anxiety reallocates this bonus: more toward
    first-stage exploration and away from second-stage exploration.

    Parameters
    - action_1: array-like (ints in {0,1}). First-stage choices (0=A, 1=U).
    - state: array-like (ints in {0,1}). Second-stage state reached (0=X, 1=Y).
    - action_2: array-like (ints in {0,1}). Second-stage choices within state.
    - reward: array-like (floats in [0,1]). Outcome.
    - stai: array-like with one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        lr: [0,1] â learning rate for MF values
        beta: [0,10] â inverse temperature for both stages
        bonus_scale: [0,1] â scale of the count-based exploration bonus
        split_control: [0,1] â baseline fraction of bonus allocated to stage 1
        anx_bias: [0,1] â how strongly anxiety shifts bonus from stage 2 to stage 1

    Returns
    - Negative log-likelihood of observed choices.
    """"""
    lr, beta, bonus_scale, split_control, anx_bias = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Visit counts for bonuses
    n1 = np.zeros(2)         # first-stage actions
    n2 = np.zeros((2, 2))    # second-stage actions per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Anxiety shifts bonus allocation: more to stage 1, less to stage 2
        w1 = np.clip(split_control + anx_bias * stai_val, 0.0, 1.0)
        w2 = np.clip(1.0 - split_control - anx_bias * stai_val, 0.0, 1.0)

        # First-stage policy with count-based bonus
        b1 = bonus_scale * w1 / np.sqrt(n1 + 1.0)  # per action bonus
        logits1 = beta * ((q1 + b1) - np.max(q1 + b1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with count-based bonus
        s = state[t]
        b2 = bonus_scale * w2 / np.sqrt(n2[s] + 1.0)  # per action in state s
        logits2 = beta * ((q2[s] + b2) - np.max(q2[s] + b2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates (pure MF with slight anxiety-damped rate for stability under high anxiety)
        lr_eff = np.clip(lr * (1.0 - 0.2 * stai_val), 0.0, 1.0)

        # Update Q2
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr_eff * pe2

        # Update Q1 toward the obtained second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += lr_eff * pe1

        # Update counts
        n1[a1] += 1.0
        n2[s, a2] += 1.0

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)","['lr', 'beta', 'bonus_scale', 'split_control', 'anx_bias']"
iter9_run0_participant3.json,cognitive_model1,516.4844679870232,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF model with anxiety-tuned reward curvature and transition mistrust.

    Mechanism overview:
    - Stage-2 values Q2(s2, a2) are learned with a single learning rate.
    - Stage-1 action values are a hybrid of model-based planning (through an effective
      transition matrix) and model-free cached values.
    - High anxiety reduces trust in the known common transitions by blending them toward
      an uncertain (uniform) transition matrix; it also reduces choice precision.
    - Rewards are distorted by a curvature parameter that is amplified by anxiety.

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices (A=0, U=1)
    - state:    int array (n_trials,) in {0,1}; reached second-stage planet (X=0, Y=1)
    - action_2: int array (n_trials,) in {0,1}; second-stage alien choice (W/S on X; P/H on Y)
    - reward:   float array (n_trials,) in [0,1]; coins received
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        alpha_q   in [0,1]: learning rate for Q2 and MF backup to Q1
        beta      in [0,10]: inverse temperature for softmax at both stages
        omega_mix in [0,1]: weight on model-based component at stage-1
        phi_shape in [0,1]: reward curvature control (anxiety-amplified)
        zeta_trans in [0,1]: anxiety-amplified mistrust of transitions (blend toward uniform)

    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """"""
    alpha_q, beta, omega_mix, phi_shape, zeta_trans = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Known common transitions (A->X, U->Y with prob 0.7)
    T_nom = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Value tables
    Q2 = np.zeros((2, 2), dtype=float)   # Q2[s2, a2]
    Q1_mf = np.zeros(2, dtype=float)     # cached model-free values at stage-1

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # Anxiety-tuned effective transition: blend nominal with uniform as anxiety increases
        k_mistrust = zeta_trans * s_anx
        T_eff = (1.0 - k_mistrust) * T_nom + k_mistrust * 0.5

        # Planning with current Q2
        max_q2 = np.max(Q2, axis=1)              # value of second-stage states
        Q1_mb = T_eff @ max_q2                   # model-based action values

        # Anxiety-tuned softmax precision (higher anxiety -> more noise)
        beta_eff = beta * (1.2 - 0.4 * s_anx)

        # Stage-1 policy
        Q1 = omega_mix * Q1_mb + (1.0 - omega_mix) * Q1_mf
        logits1 = beta_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        logits2 = beta_eff * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Anxiety-amplified reward curvature (phi_eff > 0)
        # phi_shape in [0,1] -> center at 1, modulated by anxiety
        phi_eff = 1.0 + (phi_shape - 0.5) * (0.5 + s_anx)
        phi_eff = float(max(0.1, phi_eff))
        r = float(reward[t])
        r_tilde = r ** phi_eff

        # Updates
        # Stage-2 value learning
        delta2 = r_tilde - Q2[s2, a2]
        Q2[s2, a2] += alpha_q * delta2

        # Stage-1 model-free update: bootstrap toward current Q2 and propagate immediate delta
        target1 = Q2[s2, a2]
        delta1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_q * delta1
        # additional eligibility-like propagation of reward prediction error
        Q1_mf[a1] += alpha_q * delta2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_q', 'beta', 'omega_mix', 'phi_shape', 'zeta_trans']"
iter9_run0_participant3.json,cognitive_model2,415.46665900350956,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""MB Stage-1 with anxiety-shaped habit and uncertainty-sensitive exploration at Stage-2.

    Mechanism overview:
    - Learns both transition probabilities and second-stage values.
    - Stage-1 choices are model-based, augmented by a habit term (perseveration) whose
      strength scales with anxiety (higher anxiety -> stronger habit).
    - Stage-2 choices use an uncertainty-sensitive bonus (UCB-style). Anxiety flips the
      sign of the bonus: low anxiety seeks uncertainty; high anxiety avoids it.

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices
    - state:    int array (n_trials,) in {0,1}; reached second-stage state
    - action_2: int array (n_trials,) in {0,1}; second-stage choice
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        alpha_v  in [0,1]: learning rate for Q2
        beta     in [0,10]: inverse temperature at both stages
        c_u      in [0,1]: magnitude of uncertainty bonus at stage-2
        omega_h  in [0,1]: habit strength at stage-1 (anxiety-scaled)
        tau_T    in [0,1]: learning rate for transition probabilities

    Returns:
    - Negative log-likelihood of observed choices (stage-1 and stage-2).
    """"""
    alpha_v, beta, c_u, omega_h, tau_T = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize learned transitions (start agnostic)
    T = np.ones((2, 2), dtype=float) * 0.5

    # Second-stage values and reward-counts for uncertainty estimate
    Q2 = np.zeros((2, 2), dtype=float)
    # Beta-Binomial conjugate counts for each (state, action): prior a=b=1
    a_succ = np.ones((2, 2), dtype=float)
    b_fail = np.ones((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = -1
    eps = 1e-12

    for t in range(n_trials):
        # Model-based value for each first-stage action
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Habit term scaled by anxiety
        habit = np.zeros(2, dtype=float)
        if prev_a1 in (0, 1):
            habit[prev_a1] = 1.0
        habit_weight = omega_h * (0.5 + 0.5 * s_anx)

        # Stage-1 softmax
        logits1 = beta * Q1_mb + habit_weight * habit
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 uncertainty-sensitive softmax (UCB-style bonus)
        s2 = int(state[t])

        # Posterior mean/variance for reward probability
        # var = (a*b)/((a+b)^2 * (a+b+1))
        a_post = a_succ[s2]
        b_post = b_fail[s2]
        var = (a_post * b_post) / (((a_post + b_post) ** 2) * (a_post + b_post + 1.0) + eps)

        # Anxiety flips exploration direction: low anxiety -> seek (positive bonus),
        # high anxiety -> avoid (negative bonus).
        sign = 1.0 - 2.0 * s_anx  # +1 at stai=0, 0 at .5, -1 at 1
        bonus = sign * c_u * np.sqrt(var)

        q2_eff = Q2[s2] + bonus
        logits2 = beta * q2_eff
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = float(reward[t])

        # Transition learning: move chosen row toward observed state
        T[a1, :] = (1.0 - tau_T) * T[a1, :]
        T[a1, s2] += tau_T
        # ensure normalization (small numerical guard)
        T[a1, :] = T[a1, :] / (np.sum(T[a1, :]) + eps)

        # Update Q2
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha_v * delta2

        # Update Beta-Binomial counts for uncertainty tracking
        a_succ[s2, a2] += r
        b_fail[s2, a2] += (1.0 - r)

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha_v', 'beta', 'c_u', 'omega_h', 'tau_T']"
iter9_run0_participant3.json,cognitive_model3,510.1377957801018,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive associability (Pearce-Hall) with anxiety-weighted uncertainty avoidance and stickiness.

    Mechanism overview:
    - Stage-2 values learn with an adaptive, option-specific learning rate (associability)
      that increases with recent surprise. Anxiety amplifies effective learning.
    - Stage-1 action values combine model-based planning with a model-free cache.
    - Anxiety drives avoidance of uncertain states: the MB evaluation subtracts an
      expected uncertainty penalty proportional to the associability of target states.
    - Stage-2 includes action stickiness that grows with anxiety.

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices
    - state:    int array (n_trials,) in {0,1}; reached second-stage state
    - action_2: int array (n_trials,) in {0,1}; second-stage choice
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        kappa0     in [0,1]: associability update rate (toward recent |PE|)
        beta       in [0,10]: inverse temperature at both stages
        omega_plan in [0,1]: weight on MB vs MF at stage-1
        psi_anx    in [0,1]: strength of anxiety effects (on associability and penalty)
        stick2     in [0,1]: stage-2 perseveration strength (anxiety-scaled)

    Returns:
    - Negative log-likelihood of observed choices (stage-1 and stage-2).
    """"""
    kappa0, beta, omega_plan, psi_anx, stick2 = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed nominal transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and associabilities
    Q2 = np.zeros((2, 2), dtype=float)
    A2 = np.ones((2, 2), dtype=float) * 0.5  # associability per (state, action) in [0,1]
    Q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a2 = -1
    eps = 1e-12

    for t in range(n_trials):
        # Expected uncertainty per state as mean associability
        unc_state = np.mean(A2, axis=1)  # shape (2,)

        # Model-based evaluation with anxiety-weighted uncertainty penalty
        max_q2 = np.max(Q2, axis=1)
        # Expected uncertainty of successor states for each a1
        exp_unc = T @ unc_state
        penalty = s_anx * psi_anx * exp_unc
        Q1_mb = (T @ max_q2) - penalty

        # Hybrid with model-free cache
        Q1 = omega_plan * Q1_mb + (1.0 - omega_plan) * Q1_mf

        # Stage-1 policy
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 stickiness with anxiety scaling
        s2 = int(state[t])
        stick_vec = np.zeros(2, dtype=float)
        if prev_a2 in (0, 1):
            stick_vec[prev_a2] = 1.0
        stick2_eff = stick2 * (0.5 + 0.5 * s_anx)

        logits2 = beta * Q2[s2] + stick2_eff * stick_vec
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = float(reward[t])

        # Adaptive learning at stage-2 (Pearce-Hall associability)
        pe2 = r - Q2[s2, a2]
        # Update associability toward |PE|
        A2[s2, a2] = (1.0 - kappa0) * A2[s2, a2] + kappa0 * abs(pe2)
        # Effective learning rate amplified by anxiety
        lr_eff = A2[s2, a2] * (0.5 + 0.5 * psi_anx * s_anx)
        lr_eff = float(np.clip(lr_eff, 0.0, 1.0))
        Q2[s2, a2] += lr_eff * pe2

        # Stage-1 model-free cache: bootstrap toward current Q2 and propagate pe2
        target1 = Q2[s2, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += lr_eff * pe1
        Q1_mf[a1] += lr_eff * pe2

        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['kappa0', 'beta', 'omega_plan', 'psi_anx', 'stick2']"
iter9_run0_participant32.json,cognitive_model1,388.2491627940058,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Kalman-TD with anxiety-modulated observation noise and eligibility trace.
    
    Core idea:
    - Second-stage (aliens) values are learned with a Kalman-like adaptive learning rate that depends on estimated uncertainty.
    - Observation noise increases with anxiety, reducing the Kalman gain (i.e., slower learning under higher anxiety).
    - First-stage values are updated via a TD backup with an eligibility-like gate that scales with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state reached per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage action (alien) per trial.
    reward : array-like of float
        Reward outcome per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1+].
    model_parameters : list or array-like of float
        [v0_init, beta, eta_obs, lambda_trace, chi_anxGain]
        Bounds:
        - v0_init: [0,1] initial uncertainty scale for second-stage Q-values.
        - beta: [0,10] inverse temperature shared across stages.
        - eta_obs: [0,1] baseline observation noise (higher -> smaller learning rates).
        - lambda_trace: [0,1] baseline trace gating for first-stage TD update.
        - chi_anxGain: [0,1] scales how much anxiety inflates observation noise.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    v0_init, beta, eta_obs, lambda_trace, chi_anxGain = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Action values
    q1 = np.zeros(2)           # first-stage actions A/U
    q2 = np.zeros((2, 2))      # second-stage states X/Y by aliens

    # Uncertainty (variance) for second-stage Q-values (Kalman filter variance per state-action)
    V2 = np.ones((2, 2)) * (1e-4 + 0.99 * v0_init)  # small positive to start

    # Process noise (how quickly contingencies drift) tied to initial uncertainty
    process_noise = 0.05 * v0_init

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # POLICY first stage: purely model-free (from q1)
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # POLICY second stage: from q2 of reached state
        s = state[t]
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # OUTCOME
        r = reward[t]

        # Kalman-like update at second stage
        # Observation noise increases with anxiety (reducing gain)
        obs_noise = eta_obs * (1.0 + chi_anxGain * stai) + 1e-8
        # Prior variance increases due to process noise
        V_prior = V2[s, a2] + process_noise
        # Kalman gain
        K = V_prior / (V_prior + obs_noise)
        pe2 = r - q2[s, a2]
        q2[s, a2] += K * pe2
        # Posterior variance
        V2[s, a2] = (1.0 - K) * V_prior

        # First-stage TD update with eligibility-like gate scaled by anxiety
        pe1 = q2[s, a2] - q1[a1]
        gate = lambda_trace * (0.5 + 0.5 * stai)  # stronger trace with higher anxiety
        lr1 = np.clip(gate * K, 0.0, 1.0)        # learning rate inherits adaptivity from K
        q1[a1] += lr1 * pe1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['v0_init', 'beta', 'eta_obs', 'lambda_trace', 'chi_anxGain']"
iter9_run0_participant32.json,cognitive_model2,382.2151940226405,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Learnable transitions with anxiety-modulated model-based reliance under surprise.
    
    Core idea:
    - The agent learns second-stage rewards (model-free) and first-stage transition probabilities.
    - First-stage policy blends model-based and model-free values.
    - Anxiety reduces reliance on the model-based plan when a surprising transition occurs.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Trial-wise reward.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha_r, beta, omega_plan, nu_trans, kappa_anxMB]
        Bounds:
        - alpha_r: [0,1] learning rate for reward values (Q2) and first-stage TD values (Q1-MF).
        - beta: [0,10] inverse temperature for both stages.
        - omega_plan: [0,1] baseline weight on model-based values at the first stage.
        - nu_trans: [0,1] learning rate for the transition model.
        - kappa_anxMB: [0,1] scales how strongly anxiety reduces model-based weight under surprising transitions.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_r, beta, omega_plan, nu_trans, kappa_anxMB = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition model T[a, s]
    # Start with weak prior centered at 0.5 (uncertain)
    T = np.ones((2, 2)) * 0.5  # rows: actions A/U, cols: states X/Y

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute model-based first-stage values from learned transitions
        max_q2 = np.max(q2, axis=1)  # best alien in each state
        q1_mb = T @ max_q2

        # Surprise based on the probability of the observed state given chosen action
        a1 = action_1[t]
        s = state[t]
        p_obs = T[a1, s]
        surprise = 1.0 - p_obs  # higher if transition was unlikely under current model

        # Anxiety reduces MB reliance under surprise
        w = omega_plan - kappa_anxMB * stai * surprise
        w = np.clip(w, 0.0, 1.0)

        # First-stage policy: mixture of MB and MF
        q1_mix = w * q1_mb + (1.0 - w) * q1_mf
        logits1 = beta * (q1_mix - np.max(q1_mix))
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy: softmax over q2 in reached state
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Update transition model T for chosen action
        # Move the row toward a one-hot on the observed state
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T[a1, s_idx] = (1.0 - nu_trans) * T[a1, s_idx] + nu_trans * target
        # Renormalize row to ensure valid probabilities
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        # Update second-stage value (model-free)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Update first-stage model-free value with TD from the realized second-stage value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik

","['alpha_r', 'beta', 'omega_plan', 'nu_trans', 'kappa_anxMB']"
iter9_run0_participant32.json,cognitive_model3,453.894938332413,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Risk-sensitive exploitation with anxiety-amplified uncertainty and exploration bonus.
    
    Core idea:
    - Maintain running estimates of both mean and variance of each alien's payouts.
    - Second-stage choice uses an effective value: mean - risk_penalty*variance + exploration_bonus*sqrt(variance).
      Anxiety increases risk penalty and also boosts the uncertainty bonus (restless exploration).
    - First-stage policy is model-based from a fixed transition structure combined with a TD model-free update.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage actions per trial.
    reward : array-like of float
        Rewards per trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha_rw, beta, tau_risk, z_bon, psi_anxU]
        Bounds:
        - alpha_rw: [0,1] learning rate for mean and variance estimates; also used for first-stage TD.
        - beta: [0,10] inverse temperature.
        - tau_risk: [0,1] baseline weight on variance penalty (risk aversion).
        - z_bon: [0,1] baseline exploration bonus weight on sqrt(variance).
        - psi_anxU: [0,1] scales how anxiety amplifies both risk penalty and uncertainty bonus.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """"""
    alpha_rw, beta, tau_risk, z_bon, psi_anxU = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure (common vs. rare)
    # A -> X common, U -> Y common
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Track mean and variance for each state-action
    m = np.zeros((2, 2))      # running mean of rewards
    m2 = np.zeros((2, 2))     # running second moment for variance estimation

    # First-stage model-free values
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute variance from moments
        var = np.maximum(m2 - m * m, 0.0)

        # Anxiety-modulated weights
        risk_w = tau_risk * (0.5 + 0.5 * stai)
        bon_w = z_bon * (1.0 + psi_anxU * stai)

        # Second-stage effective Q for policy
        q2_eff = m - risk_w * var + bon_w * np.sqrt(var + 1e-12)

        # First-stage model-based value from fixed transitions using effective Q
        max_q2_eff = np.max(q2_eff, axis=1)
        q1_mb = T_fixed @ max_q2_eff

        # Blend MB with MF first-stage values equally weighted by anxiety-dependent confidence
        # Higher anxiety slightly down-weights MF in favor of MB planning here
        w_mb = np.clip(0.5 + 0.3 * stai, 0.0, 1.0)
        q1_mix = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage policy
        logits1 = beta * (q1_mix - np.max(q1_mix))
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy at reached state using q2_eff
        s = state[t]
        q2_s_eff = q2_eff[s]
        logits2 = beta * (q2_s_eff - np.max(q2_s_eff))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning of moments
        r = reward[t]
        # Update moments for the selected state-action
        m_sa = m[s, a2]
        m[s, a2] = (1.0 - alpha_rw) * m[s, a2] + alpha_rw * r
        m2[s, a2] = (1.0 - alpha_rw) * m2[s, a2] + alpha_rw * (r * r)

        # First-stage TD update from realized effective value (using current q2_eff)
        target = q2_eff[s, a2]
        pe1 = target - q1_mf[a1]
        q1_mf[a1] += alpha_rw * pe1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik","['alpha_rw', 'beta', 'tau_risk', 'z_bon', 'psi_anxU']"
iter9_run0_participant35.json,cognitive_model1,331.51311139814544,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid model-based/model-free with eligibility traces and anxiety-weighted arbitration.

    Core ideas
    - Stage-1 action values are a convex combination of model-free (bootstrapped) and
      model-based (transition-structured) values.
    - Stage-2 values are updated via TD learning.
    - An eligibility trace propagates the stage-2 prediction error back to the chosen
      stage-1 action.
    - Anxiety reduces planning reliance (model-based weight) and increases trace strength,
      and also reduces choice precision.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U).
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1).
    - reward: array-like of floats in [0,1], reward outcome per trial.
    - stai: array-like length-1, scalar anxiety score in [0,1].
    - model_parameters: tuple/list with 4 parameters:
        alpha_mf: base learning rate for Q updates in [0,1]
        beta: base inverse temperature for softmax in [0,10]
        w0_hybrid: base weight on model-based control in [0,1]
        lambda_elig: base eligibility trace strength (backprop to stage-1) in [0,1]

    Bounds
    - alpha_mf, w0_hybrid, lambda_elig in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Effective model-based weight: w_mb = clip(w0_hybrid * (1 - 0.7*stai), 0, 1)
      Higher anxiety reduces reliance on planning.
    - Effective eligibility trace: lam = clip(lambda_elig * (0.5 + 0.5*stai), 0, 1)
      Higher anxiety increases credit assignment to the chosen stage-1 action.
    - Effective inverse temperature: beta_eff = clip(beta * (1 - 0.3*stai), 0, 10)
      Higher anxiety yields noisier choices.

    Returns
    - Negative log-likelihood of observed choices under the model.
    """"""
    alpha_mf, beta_base, w0_hybrid, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # rows: action A/U, cols: state X/Y

    # Anxiety-modulated parameters
    beta_eff = max(0.0, min(10.0, beta_base * (1.0 - 0.3 * stai)))
    w_mb = max(0.0, min(1.0, w0_hybrid * (1.0 - 0.7 * stai)))
    lam = max(0.0, min(1.0, lambda_elig * (0.5 + 0.5 * stai)))
    alpha = max(0.0, min(1.0, alpha_mf))

    # Initialize values
    q1_mf = np.zeros(2)         # model-free stage-1 Q
    q2 = np.zeros((2, 2))       # stage-2 Q for states X,Y and actions 0,1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute model-based stage-1 values from current q2
        max_q2 = np.max(q2, axis=1)        # value of each second-stage state
        q1_mb = T @ max_q2                 # expected value under known transitions

        # Hybrid action values for stage 1
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        pi1 = np.exp(beta_eff * q1c)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy for the reached state
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        pi2 = np.exp(beta_eff * q2c)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        # Outcomes and updates
        r = float(reward[t])

        # Stage-2 TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility trace back to chosen stage-1 action (bootstrapped by reached Q2)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += lam * alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_mf', 'beta_base', 'w0_hybrid', 'lambda_elig']"
iter9_run0_participant35.json,cognitive_model2,334.13689488811514,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Adaptive learning rate from surprise with anxiety and perseveration bias at stage 1.

    Core ideas
    - Learning rates adapt on each trial as a function of recent surprise (|prediction error|).
    - Anxiety amplifies the sensitivity to surprise, increasing learning rate when outcomes are
      unexpected, and reduces choice precision.
    - A perseveration (stickiness) bias at stage 1 is strengthened by anxiety.
    - Stage-1 values are model-based (computed via the transition matrix over current stage-2 Q).

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions.
    - state: array-like of ints in {0,1}, reached second-stage state.
    - action_2: array-like of ints in {0,1}, second-stage actions.
    - reward: array-like of floats in [0,1], reward outcome per trial.
    - stai: array-like length-1, scalar anxiety score in [0,1].
    - model_parameters: tuple/list with 4 parameters:
        alpha_base: baseline learning rate in [0,1]
        beta: base inverse temperature in [0,10]
        k_vol: surprise-to-learning multiplier in [0,1]
        psi_stay: perseveration weight added to previous first-stage action in [0,1]

    Bounds
    - alpha_base, k_vol, psi_stay in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Surprise sensitivity: k_eff = k_vol * (1 + 0.8*stai)
    - Inverse temperature: beta_eff = beta * (1 - 0.35*stai), clipped to [0,10]
    - Perseveration: psi_eff = psi_stay * (0.5 + 0.5*stai)

    Returns
    - Negative log-likelihood of observed choices under the model.
    """"""
    alpha_base, beta_base, k_vol, psi_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    beta_eff = max(0.0, min(10.0, beta_base * (1.0 - 0.35 * stai)))
    k_eff = max(0.0, min(1.0, k_vol * (1.0 + 0.8 * stai)))
    psi_eff = max(0.0, min(1.0, psi_stay * (0.5 + 0.5 * stai)))
    alpha0 = max(0.0, min(1.0, alpha_base))

    q2 = np.zeros((2, 2))  # stage-2 values
    q1_prev_action = None  # for perseveration

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track last PE magnitude to adjust next-step learning rate
    last_abs_pe2 = 0.0

    for t in range(n_trials):
        # Model-based stage-1 values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Perseveration bias on stage-1 logits
        bias = np.zeros(2)
        if q1_prev_action is not None:
            bias[int(q1_prev_action)] = psi_eff

        # Stage-1 policy
        logits1 = q1_mb + bias
        logits1 -= np.max(logits1)
        pi1 = np.exp(beta_eff * logits1)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        pi2 = np.exp(beta_eff * q2c)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        # Outcome and adaptive learning rate
        r = float(reward[t])
        alpha_t = alpha0 + k_eff * last_abs_pe2
        alpha_t = max(0.0, min(1.0, alpha_t))

        # Update Q2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * pe2

        # No separate q1 table; stage-1 is purely model-based here

        # Book-keeping
        last_abs_pe2 = abs(pe2)
        q1_prev_action = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha_base', 'beta_base', 'k_vol', 'psi_stay']"
iter9_run0_participant35.json,cognitive_model3,387.3940694373506,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Successor-structured stage-1 values with anxiety-modulated structure reliance,
    information bonus at stage 2, and forgetting.

    Core ideas
    - Stage-1 values are computed from a successor-like expectation over next states:
      Q1[a] = sum_s M[a,s] * max(Q2[s]), where M blends the known transitions with
      a uniform prior; anxiety reduces reliance on structure.
    - Stage-2 choices include an exploration bonus proportional to uncertainty
      (running variance), with anxiety increasing the bonus weight.
    - Both Q2 and uncertainty statistics undergo forgetting to capture nonstationarity.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions.
    - state: array-like of ints in {0,1}, reached second-stage state.
    - action_2: array-like of ints in {0,1}, second-stage actions.
    - reward: array-like of floats in [0,1], reward outcome per trial.
    - stai: array-like length-1, scalar anxiety score in [0,1].
    - model_parameters: tuple/list with 5 parameters:
        alpha_q: stage-2 learning rate in [0,1]
        beta: base inverse temperature in [0,10]
        gamma_sr: base reliance on transition structure in [0,1]
        kappa_bonus: base information bonus weight in [0,1]
        tau_forget: forgetting/decay factor per trial in [0,1]

    Bounds
    - alpha_q, gamma_sr, kappa_bonus, tau_forget in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Structure reliance: gamma_eff = clip(gamma_sr * (1 - 0.5*stai), 0, 1)
      Higher anxiety reduces use of the transition model at stage 1.
    - Exploration bonus: kappa_eff = clip(kappa_bonus * (0.5 + 0.5*stai), 0, 1)
      Higher anxiety increases information seeking/avoidance of uncertainty.
    - Inverse temperature: beta_eff = clip(beta * (1 - 0.3*stai), 0, 10)
    - Forgetting: forget = clip(tau_forget * (0.5 + 0.5*stai), 0, 1)
      Higher anxiety increases value and memory decay.

    Returns
    - Negative log-likelihood of observed choices under the model.
    """"""
    alpha_q, beta_base, gamma_sr, kappa_bonus, tau_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transitions (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    U = np.ones((2, 2)) / 2.0  # uniform prior over next states

    beta_eff = max(0.0, min(10.0, beta_base * (1.0 - 0.3 * stai)))
    gamma_eff = max(0.0, min(1.0, gamma_sr * (1.0 - 0.5 * stai)))
    kappa_eff = max(0.0, min(1.0, kappa_bonus * (0.5 + 0.5 * stai)))
    forget = max(0.0, min(1.0, tau_forget * (0.5 + 0.5 * stai)))
    alpha = max(0.0, min(1.0, alpha_q))

    # Stage-2 value and uncertainty trackers
    q2 = np.zeros((2, 2))
    m = np.zeros((2, 2))    # running mean of rewards per (state, action)
    m2 = np.zeros((2, 2))   # running mean of squared rewards per (state, action)
    n = np.ones((2, 2)) * 1e-6  # pseudo-counts to stabilize early variance

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Successor-like mixing of structure and uniform
        M = gamma_eff * T + (1.0 - gamma_eff) * U

        # Stage-1 values from successor-like expectation over next states
        max_q2 = np.max(q2, axis=1)
        q1 = M @ max_q2

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        pi1 = np.exp(beta_eff * q1c)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy with information bonus
        s = int(state[t])
        # Compute running variance estimate for state s actions
        var_sa = np.maximum(0.0, m2[s] - m[s] ** 2)
        bonus = kappa_eff * np.sqrt(var_sa)
        pref2 = q2[s] + bonus
        pref2c = pref2 - np.max(pref2)
        pi2 = np.exp(beta_eff * pref2c)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        # Outcome
        r = float(reward[t])

        # Forgetting/decay
        q2 *= (1.0 - forget)
        m *= (1.0 - forget)
        m2 *= (1.0 - forget)
        n *= (1.0 - 0.5 * forget)  # keep some inertia in counts

        # Update Q2 with TD error
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update uncertainty statistics (EWMA via pseudo-counts)
        n[s, a2] += 1.0
        w = 1.0 / n[s, a2]
        m[s, a2] = (1 - w) * m[s, a2] + w * r
        m2[s, a2] = (1 - w) * m2[s, a2] + w * (r * r)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha_q', 'beta_base', 'gamma_sr', 'kappa_bonus', 'tau_forget']"
iter9_run0_participant39.json,cognitive_model1,496.73169921596264,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Anxiety-weighted aversion to rare transitions with volatility-adaptive temperature.

    Summary:
    - Model-free learning of second-stage values; stage-1 values are MF bootstraps of stage-2.
    - Aversion to rare transitions at the first stage: the spaceship that just produced a rare
      transition gets a temporary penalty, scaled by anxiety (higher anxiety, stronger avoidance).
    - Softmax temperature adapts to recent volatility (running absolute PE) so that higher
      volatility lowers effective beta (more exploration). Volatility sensitivity increases
      with anxiety.
    - Constant side bias at stage 1 depends on the participantâs anxiety (bias toward A when
      anxiety is lower, toward U when anxiety is higher).

    Parameters (all used; bounds in brackets):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), observed second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens within state)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha       [0,1]: learning rate for stage-2 MF values and stage-1 bootstrapping
        beta0       [0,10]: base inverse temperature for both stages
        chi_rare    [0,1]: penalty magnitude for the spaceship that produced a rare transition
        phi_vol     [0,1]: volatility sensitivity (scales beta reduction with volatility)
        bias0       [0,1]: maximum side bias magnitude at stage 1

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """"""
    alpha, beta0, chi_rare, phi_vol, bias0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],  # P(X|A), P(Y|A)
                  [0.3, 0.7]]) # P(X|U), P(Y|U)

    # Model-free values
    q1_mf = np.zeros(2)          # first-stage MF action values
    q2_mf = np.zeros((2, 2))     # second-stage MF action values

    # Likelihood containers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Rare-transition aversion: penalty to the chosen spaceship if last transition was rare
    rare_penalty = np.zeros(2)   # added to q1 when choosing
    # Volatility tracker (running abs prediction error at stage 2)
    vol = 0.0
    vol_alpha = 0.2 + 0.6 * stai0  # higher anxiety -> track volatility more aggressively

    eps = 1e-12
    for t in range(n_trials):
        # Compute model-based estimate for stage-1 from current q2_mf
        max_q2 = np.max(q2_mf, axis=1)      # per-state best alien value
        q1_mb = T @ max_q2                  # expected value per spaceship

        # Hybrid policy: here we rely on MF for learning but allow MB to shape policy via expectation
        # We combine both sources additively (simple sum) and add penalties/biases.
        # Side bias: bias toward A when stai low, toward U when stai high
        side_bias = np.array([+1.0, -1.0]) * bias0 * (0.5 - stai0)

        # Volatility-adaptive inverse temperature (higher volatility -> lower effective beta)
        beta_eff = beta0 / (1.0 + phi_vol * (1.0 + stai0) * vol)

        # Combine values with rare-transition aversion
        q1_eff = 0.5 * q1_mf + 0.5 * q1_mb + rare_penalty + side_bias
        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second stage policy (pure MF at the encountered state)
        s2 = state[t]
        q2c = q2_mf[s2] - np.max(q2_mf[s2])
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second stage MF values
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # Bootstrap first stage MF from realized second-stage value (SARSA(0)-like)
        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update volatility (running absolute PE magnitude)
        vol = (1.0 - vol_alpha) * vol + vol_alpha * abs(pe2)

        # Update rare-transition aversion for next trial
        # Identify whether the realized transition was common or rare
        p_trans = T[a1, s2]
        is_rare = 1.0 - p_trans  # 0.3 -> rare weight 0.7, 0.7 -> rare weight 0.3
        rare_penalty = np.zeros(2)
        # Apply a penalty to the chosen spaceship if the transition was rare (scaled by anxiety)
        rare_penalty[a1] = -chi_rare * (stai0) * is_rare

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta0', 'chi_rare', 'phi_vol', 'bias0']"
iter9_run0_participant39.json,cognitive_model2,472.84831477090904,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Reliability-based arbitration between model-based and model-free control with anxiety bias.

    Summary:
    - Stage-2 values are learned model-free.
    - A meta-controller arbitrates at stage 1 between model-based (MB) and model-free (MF)
      values using their relative reliabilities. MB reliability decreases with recent
      transition surprise; MF reliability decreases with recent reward PE variance.
    - Anxiety biases arbitration toward MF control (higher anxiety -> more MF).
    - MF values include a small decay to model forgetting.

    Parameters (all used; bounds in brackets):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), observed second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha       [0,1]: learning rate for stage-2 MF values and stage-1 bootstrapping
        beta        [0,10]: inverse temperature for both stages
        theta_rel   [0,1]: sensitivity of the arbitration to reliability differences
        decay_mf    [0,1]: forgetting/decay applied to MF values each trial
        xi_anx      [0,1]: strength of anxiety-driven shift toward MF control

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha, beta, theta_rel, decay_mf, xi_anx = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition model
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Running uncertainty trackers
    # Reward PE variance proxy for MF reliability; transition surprise proxy for MB reliability
    mf_var = 0.0
    mb_var = 0.0
    eta_track = 0.2  # tracker smoothing

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Decay MF values
        q1_mf *= (1.0 - 0.5 * decay_mf)
        q2_mf *= (1.0 - decay_mf)

        # Model-based Q for stage 1 from current MF stage-2 values
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2

        # Compute reliabilities as inverse of uncertainty proxies
        rel_mb = 1.0 / (1.0 + mb_var)
        rel_mf = 1.0 / (1.0 + mf_var)

        # Arbitration weight on MB versus MF
        # anxiety shifts weight toward MF by subtracting from the MB-MF difference
        # w_mb in [0,1] via a smooth mapping
        diff_rel = rel_mb - rel_mf - xi_anx * stai0
        w_mb = 1.0 / (1.0 + np.exp(-10.0 * theta_rel * diff_rel))  # squashing function

        q1_eff = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s2 = state[t]
        q2c = q2_mf[s2] - np.max(q2_mf[s2])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update uncertainty trackers
        # MF uncertainty from reward PE magnitude; MB uncertainty from transition surprise
        mf_var = (1.0 - eta_track) * mf_var + eta_track * (pe2 ** 2)

        p_trans = T[a1, s2]
        surprise = 1.0 - p_trans  # larger for rare transitions
        # Anxiety increases the impact of surprise on MB uncertainty
        mb_var = (1.0 - eta_track) * mb_var + eta_track * ((1.0 + stai0) * surprise) ** 2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha', 'beta', 'theta_rel', 'decay_mf', 'xi_anx']"
iter9_run0_participant39.json,cognitive_model3,485.03232907009203,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""Uncertainty-driven exploration (UCB) with anxiety-dampened bonuses and value decay.

    Summary:
    - Second-stage values are learned via incremental averaging (MF) with a decay term.
    - The policy incorporates an Upper Confidence Bound (UCB) bonus at stage 2 that promotes
      exploration of less-sampled aliens; the bonus is propagated to stage 1 via the known
      transition model.
    - Anxiety reduces the effective exploration bonus (more anxious -> less uncertainty seeking).
    - A separate softmax temperature controls stochasticity.

    Parameters (all used; bounds in brackets):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), observed second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha       [0,1]: learning rate for reward value updates (MF)
        beta        [0,10]: inverse temperature for both stages
        kappa_ucb   [0,1]: base weight of uncertainty bonus
        tau_unc     [0,1]: anxiety sensitivity scaling that suppresses bonus
        decay       [0,1]: value decay applied each trial to encourage continual exploration

    Returns:
    - Negative log-likelihood of observed choices.
    """"""
    alpha, beta, kappa_ucb, tau_unc, decay = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition model
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Model-free values and counts for UCB
    q2 = np.zeros((2, 2))
    n2 = np.zeros((2, 2))  # visit counts per (state, action)
    q1_mf = np.zeros(2)    # MF bootstrap at stage 1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-dampened exploration bonus weight
    kappa_eff = kappa_ucb * (1.0 - tau_unc * stai0)

    for t in range(n_trials):
        # Apply decay
        q2 *= (1.0 - decay)
        q1_mf *= (1.0 - 0.5 * decay)

        # UCB bonus for each state-action
        bonus = np.zeros((2, 2))
        for s in (0, 1):
            for a in (0, 1):
                bonus[s, a] = kappa_eff / np.sqrt(n2[s, a] + 1.0)

        # Stage-2 policy uses Q + bonus at the encountered state
        s2 = state[t]
        q2_bonus = q2[s2] + bonus[s2]
        q2c = q2_bonus - np.max(q2_bonus)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Stage-1 policy uses MB expectation over Q+bonus and combines with MF bootstrap
        max_q2_bonus = np.max(q2 + bonus, axis=1)
        q1_mb = T @ max_q2_bonus
        q1_eff = 0.5 * q1_mb + 0.5 * q1_mf
        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Observe reward and update values/counts
        r = reward[t]
        n2[s2, a2] += 1.0
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # Bootstrap stage-1 MF toward realized second-stage value (without bonus)
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll","['alpha', 'beta', 'kappa_ucb', 'tau_unc', 'decay']"
iter9_run0_participant4.json,cognitive_model2,395.0352248957113,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""Model-based planner with learned transitions biased by anxiety, plus stay bias at both stages.

    Overview
    - Learns transition probabilities T(a1->s) via a simple delta rule.
    - Stage 2 learns Q2(s, a2) via Rescorla-Wagner.
    - Stage 1 uses purely model-based planning with the learned T.
    - Anxiety flattens the learned transition structure by pulling T toward 0.5,
      implemented via an anxiety-weighted shrinkage toward 0.5 after each transition update.
      Higher stai -> more uncertainty about which next state follows.
    - Stay bias (perseveration) applied at both stages.

    Parameters (all used, recommended bounds)
    - alpha:       [0,1]   Learning rate for Q2(s, a2).
    - beta:        [0,10]  Inverse temperature.
    - alpha_trans: [0,1]   Learning rate for transitions.
    - mu_anx_comm: [0,1]   Strength of anxiety-driven shrinkage of T toward 0.5.
    - rho_stay:    [0,1]   Additive perseveration bias for repeating previous action (both stages).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, alpha_trans, mu_anx_comm, rho_stay].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, alpha_trans, mu_anx_comm, rho_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize transition estimates T[a, s], rows sum to 1
    # Start with the canonical common structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Initialize Q2
    q2 = np.zeros((2, 2), dtype=float) + 0.5

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 model-based planning using learned transitions
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2           # shape (2,)

        # Stage-1 policy with stay bias
        bias1 = np.zeros(2, dtype=float)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_stay
        logits1 = beta * (q1_mb - np.max(q1_mb)) + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with state-specific stay bias
        bias2 = np.zeros(2, dtype=float)
        if prev_a2[s] is not None:
            bias2[int(prev_a2[s])] += rho_stay
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s)) + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Learning
        # Q2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Transition learning for the chosen action a1:
        # delta rule toward the actually observed next state s
        for sp in (0, 1):
            targ = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_trans * (targ - T[a1, sp])
        # Normalize to avoid drift
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum <= 0:
            T[a1, :] = np.array([0.5, 0.5])
        else:
            T[a1, :] = T[a1, :] / row_sum

        # Anxiety-driven shrinkage toward 0.5 (uncertainty about transitions)
        shrink = mu_anx_comm * stai
        T[a1, :] = (1.0 - shrink) * T[a1, :] + shrink * np.array([0.5, 0.5])

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)

","['alpha', 'beta', 'alpha_trans', 'mu_anx_comm', 'rho_stay']"
iter9_run0_participant4.json,cognitive_model3,436.4971149408369,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""MB planner with anxiety-driven pruning of rare transitions and lapse, plus MF backup.

    Overview
    - Stage 2: Rescorla-Wagner learning on Q2(s, a2).
    - Stage 1: Combines model-based value that prunes unlikely next states and a small MF backup.
      Pruning reduces the contribution of the less likely next state in the MB backup.
    - Anxiety modulates pruning strength (more anxious -> stronger pruning) and adds a small lapse
      probability (more anxious -> more lapses).
    - MF backup at stage-1 is updated toward realized Q2.

    Parameters (all used, recommended bounds)
    - alpha:       [0,1]   Learning rate for Q2 and stage-1 MF backup.
    - beta:        [0,10]  Inverse temperature baseline.
    - zeta_prune:  [0,1]   Baseline pruning strength (0=no pruning, 1=hard prune).
    - omega_mb:    [0,1]   Weight on MB component in stage-1 values (1=fully MB).
    - kappa_lapse: [0,1]   Baseline lapse rate; effective lapse increases with stai.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, zeta_prune, omega_mb, kappa_lapse].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """"""
    alpha, beta, zeta_prune, omega_mb, kappa_lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Canonical transition probabilities
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q2 = np.zeros((2, 2), dtype=float) + 0.5
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Anxiety-modulated pruning and lapse settings
    prune_strength = np.clip(zeta_prune * (1.0 + stai), 0.0, 1.0)  # cap to [0,1]
    lapse_rate = np.clip(kappa_lapse * stai, 0.0, 0.5)             # keep lapse <= 0.5

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 MB value with pruning:
        # Down-weight the less likely next state by a factor (1 - prune_strength).
        max_q2 = np.max(q2, axis=1)  # [Q*(X), Q*(Y)]
        q1_mb = np.zeros(2, dtype=float)
        for a in (0, 1):
            p_common = T[a, 0] if a == 0 else T[a, 1]  # common prob for A->X, U->Y
            # Identify common and rare next-state values
            q_common = max_q2[0] if a == 0 else max_q2[1]
            q_rare = max_q2[1] if a == 0 else max_q2[0]
            # Effective mixing after pruning (rare contribution reduced)
            w_common = p_common
            w_rare = 1.0 - p_common
            w_rare_eff = (1.0 - prune_strength) * w_rare
            # renormalize
            Z = w_common + w_rare_eff
            if Z <= 1e-12:
                mix = 0.5 * (q_common + q_rare)
            else:
                mix = (w_common * q_common + w_rare_eff * q_rare) / Z
            q1_mb[a] = mix

        # Hybrid stage-1 value: MB with pruning + MF backup
        q1 = omega_mb * q1_mb + (1.0 - omega_mb) * q1_mf

        # Stage-1 policy with lapse: mix softmax with uniform
        logits1 = beta * (q1 - np.max(q1))
        logits1 = logits1 - np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - lapse_rate) * soft1 + lapse_rate * np.array([0.5, 0.5])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with the same lapse
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        logits2 = logits2 - np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - lapse_rate) * soft2 + lapse_rate * np.array([0.5, 0.5])
        p_choice_2[t] = probs2[a2]

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)","['alpha', 'beta', 'zeta_prune', 'omega_mb', 'kappa_lapse']"
iter9_run0_participant40.json,cognitive_model1,547.2229719360333,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""Hybrid MB/MF with volatility-gated arbitration; anxiety reduces reliance on MB and inflates perceived volatility.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used. Higher anxiety reduces MB weighting
        and amplifies volatility gating.
    model_parameters : tuple/list
        (alpha2, beta, mb_weight, vol_lr, kappa2)
        - alpha2 in [0,1]: learning rate for second-stage Q-values and MF backup to stage 1.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - mb_weight in [0,1]: baseline arbitration weight on model-based values.
        - vol_lr in [0,1]: learning rate for estimating transition volatility from surprise.
        - kappa2 in [0,1]: second-stage choice stickiness within each planet.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Model-based (MB) uses a fixed known transition structure: A->X, U->Y common (0.7).
    - A latent volatility v_t is learned from transition surprise (rare=1, common=0).
      The effective MB weight is omega = clip(mb_weight * (1 - stai) * (1 - v_t), 0, 1).
    - Model-free (MF) first-stage values are updated by backing up second-stage values with
      an eligibility trace lambda = 0.5 * (1 - stai).
    - Second-stage policy includes a within-planet stickiness bias kappa2 to repeat the last alien.
    """"""
    alpha2, beta, mb_weight, vol_lr, kappa2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition matrix: rows = actions (A=0, U=1), cols = states (X=0, Y=1)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Prob of each observed choice for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q2 = np.zeros((2, 2))      # second-stage Q-values per planet/state
    Q1_mf = np.zeros(2)        # first-stage model-free Q-values

    # Second-stage stickiness: last chosen alien per state (initialize to -1 meaning none)
    last_a2_by_state = np.array([-1, -1], dtype=int)

    # Volatility estimate (starts neutral)
    v = 0.5

    # Eligibility trace for MF credit assignment (anxiety reduces Î»)
    lam = 0.5 * (1.0 - stai_val)

    for t in range(n_trials):
        a1 = int(action_1[t])

        # Model-based first-stage values from current Q2
        max_Q2 = np.max(Q2, axis=1)   # best alien on each planet
        Q1_mb = T_fixed @ max_Q2

        # Arbitration weight gated by volatility and anxiety
        omega = mb_weight * (1.0 - stai_val) * (1.0 - v)
        if omega < 0.0:
            omega = 0.0
        elif omega > 1.0:
            omega = 1.0

        Q1 = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Stage-1 policy (softmax)
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with within-planet stickiness
        s2 = int(state[t])
        a2 = int(action_2[t])

        bias2 = np.zeros(2)
        if last_a2_by_state[s2] != -1:
            bias2[last_a2_by_state[s2]] = kappa2

        logits2 = beta * Q2[s2] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn Q2 (TD)
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # MF backup to stage-1 with eligibility Î» (uses realized Q2 after update)
        backup_value = Q2[s2, a2]
        delta1 = backup_value - Q1_mf[a1]
        Q1_mf[a1] += alpha2 * lam * delta1

        # Update volatility estimate from transition surprise (rare=1, common=0)
        is_common = int((a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1))
        surprise = 1.0 - float(is_common)  # 0 for common, 1 for rare
        v = (1.0 - vol_lr) * v + vol_lr * surprise

        # Update stickiness memory
        last_a2_by_state[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

","['alpha2', 'beta', 'mb_weight', 'vol_lr', 'kappa2']"
iter9_run0_participant5.json,cognitive_model1,458.3499682602111,"def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Uncertainty-arbitrated MB/MF with anxiety-shaped utility and learned transitions.

    Idea:
    - Learn the transition matrix online.
    - Compute a model-based (MB) value using the learned transitions and current stage-2 values.
    - Compute a model-free (MF) value learned from experienced rewards.
    - Arbitrates MB vs MF using the uncertainty (entropy) of the learned transition model.
      Anxiety increases MF reliance (reduces MB weight).
    - Anxiety also shapes outcome utility by making unrewarded outcomes feel worse.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices on visited planet (0/1=alien).
    reward : array-like of float
        Obtained coins (typically 0 or 1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce MB arbitration weight and increase
        the negativity of unrewarded outcomes.
    model_parameters : array-like of float
        [alpha_q, beta, t_learn, rho_u]
        - alpha_q in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - t_learn in [0,1]: learning rate for updating the transition matrix.
        - rho_u in [0,1]: utility resilience; higher reduces anxiety-driven negativity.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """"""
    alpha_q, beta, t_learn, rho_u = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition model (rows sum to 1)
    T = 0.5 * np.ones((2, 2))  # start agnostic; will learn from experience

    # Stage-1 MF and stage-2 values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    ln2 = np.log(2.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based action values at stage 1 using learned transitions
        max_q2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        # Arbitration weight based on transition uncertainty (entropy)
        # Lower entropy -> higher MB weight; higher anxiety -> lower MB weight.
        H_rows = -np.sum(np.clip(T, eps, 1.0) * np.log(np.clip(T, eps, 1.0)), axis=1)  # entropy per action
        H_norm = np.mean(H_rows) / ln2  # normalize to [0,1]
        w_mb = np.clip((1.0 - H_norm) * (1.0 - 0.6 * st), 0.0, 1.0)

        # Hybrid Q at stage 1
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Choice probabilities stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Choice probabilities stage 2
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Anxiety-shaped utility: unrewarded outcomes feel worse when anxious,
        # mitigated by rho_u. r in {0,1} -> r_eff in [-st*(1-rho_u), 1]
        r_eff = r if r >= 1.0 - eps else -st * (1.0 - rho_u)

        # Q-learning updates
        # Stage 2
        pe2 = r_eff - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Stage 1 MF bootstraps from realized stage-2 action value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # Learn transition model from observed (a1 -> s)
        # Move T[a1] toward one-hot for observed state
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            T[a1, ss] += t_learn * (target - T[a1, ss])
        # Ensure normalization and numerical stability
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum
        T[a1] = np.clip(T[a1], eps, 1.0)
        T[a1] /= np.sum(T[a1])

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha_q', 'beta', 't_learn', 'rho_u']"
iter9_run0_participant5.json,cognitive_model2,430.2193289060814,"def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Valence-asymmetric MF with anxiety-gated eligibility and perseveration.

    Idea:
    - Pure model-free learner with separate learning rates for positive vs negative
      prediction errors at stage 2.
    - Stage-1 values receive additional credit via an eligibility trace from stage 2.
    - Anxiety increases perseveration (choice stickiness) and increases sensitivity
      to negative outcomes (higher effective alpha for negative PEs).
    - Fixed environment transitions are not used for planning (MF-only policy).

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received (0/1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values:
          - increase perseveration strength,
          - increase reliance on negative learning,
          - reduce eligibility trace (shallower credit assignment).
    model_parameters : array-like of float
        [alpha_pos, alpha_neg, beta, stick0, lam0]
        - alpha_pos in [0,1]: learning rate for positive PEs at stage 2.
        - alpha_neg in [0,1]: learning rate for negative PEs at stage 2.
        - beta in [0,10]: inverse temperature for choices at both stages.
        - stick0 in [0,1]: baseline perseveration strength.
        - lam0 in [0,1]: baseline eligibility trace from stage 2 to stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    alpha_pos, alpha_neg, beta, stick0, lam0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Values
    q1 = np.zeros(2)            # stage-1 MF values
    q2 = 0.5 * np.ones((2, 2))  # stage-2 MF values

    # Perseveration states
    prev_a1 = 0
    prev_a2 = np.zeros(2, dtype=int)  # per state

    # Anxiety-gated components
    kappa = 2.0 * stick0  # scale stickiness
    kappa_eff = kappa * (0.5 + 0.5 * st)  # more stickiness with anxiety
    lam_eff = np.clip(lam0 * (1.0 - 0.6 * st), 0.0, 1.0)

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Softmax with perseveration biases
        bias1 = np.array([0.0, 0.0])
        bias1[prev_a1] += kappa_eff
        logits1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        bias2 = np.array([0.0, 0.0])
        bias2[prev_a2[s]] += kappa_eff
        logits2 = beta * (q2[s] - np.max(q2[s])) + bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Stage 2 update with valence asymmetry and anxiety skew
        pe2 = r - q2[s, a2]
        if pe2 >= 0:
            alpha2 = alpha_pos * (1.0 - 0.3 * st)  # anxious -> slightly reduced positive learning
        else:
            alpha2 = alpha_neg * (0.7 + 0.3 * st)  # anxious -> heightened negative learning
        q2[s, a2] += alpha2 * pe2

        # Stage 1: standard bootstrapping to realized stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        # Base update
        q1[a1] += 0.5 * (alpha_pos + alpha_neg) * pe1
        # Eligibility trace credit from stage-2 PE
        q1[a1] += lam_eff * ((alpha_pos + alpha_neg) * 0.5) * pe2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)

","['alpha_pos', 'alpha_neg', 'beta', 'stick0', 'lam0']"
iter9_run0_participant5.json,cognitive_model3,443.26734065828055,"def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """"""
    Volatility-sensitive hybrid with anxiety-driven optimism prior and exploration bonus.

    Idea:
    - Stage 2 learns rewards with a constant learning rate, but we track running
      outcome volatility from absolute prediction errors.
    - Anxiety inflates perceived volatility, which lowers exploitation (reduces beta)
      and shifts arbitration toward MF (less reliance on MB).
    - Optimism prior on alien values is stronger when anxiety is low.
    - Add an uncertainty/exploration bonus at stage 2 that diminishes with visit count
      and grows with perceived volatility and optimism.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within planet).
    reward : array-like of float
        Coins obtained.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase perceived volatility (more exploration,
        less MB arbitration) and reduce optimism prior.
    model_parameters : array-like of float
        [eta_r, beta, phi_vol, opt_bias]
        - eta_r in [0,1]: learning rate for stage-2 Q-value updates.
        - beta in [0,10]: baseline inverse temperature for both stages.
        - phi_vol in [0,1]: sensitivity for volatility update and its impact.
        - opt_bias in [0,1]: optimism prior and exploration gain base.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """"""
    eta_r, beta, phi_vol, opt_bias = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (common transitions)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Optimism prior shaped by anxiety: low anxiety -> higher prior mean
    prior_mean = 0.5 + (1.0 - st) * (opt_bias - 0.5)
    q2 = prior_mean * np.ones((2, 2))

    # Stage-1 MF values
    q1_mf = np.zeros(2)

    # Visit counts for exploration bonus
    counts = np.ones((2, 2))  # start at 1 to avoid div-by-zero in bonus

    # Volatility estimate
    vol = 0.0

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Perceived volatility inflated by anxiety
        vol_eff = np.clip(vol + st * phi_vol, 0.0, 1.0)

        # Arbitration and temperature controlled by volatility and anxiety
        w_mb = np.clip((1.0 - vol_eff) * (1.0 - 0.5 * st), 0.0, 1.0)
        beta_eff = np.clip(beta * (1.0 - 0.5 * vol_eff), 0.0, 10.0)

        # Model-based value for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 2 exploration bonus: larger with perceived volatility and optimism;
        # diminishes with visits to the specific (state,action).
        bonus_gain = vol_eff * opt_bias / (counts[s, :] + 0.0)
        q2_biased = q2[s] + bonus_gain

        # Choice probabilities
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        logits2 = beta_eff * (q2_biased - np.max(q2_biased))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta_r * pe2

        # Update MF stage-1 toward realized second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta_r * pe1

        # Volatility update from absolute PE (running exponential average)
        vol = (1.0 - phi_vol) * vol + phi_vol * abs(pe2)
        vol = np.clip(vol, 0.0, 1.0)

        # Update visit counts for exploration bonus
        counts[s, a2] += 1.0

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)","['eta_r', 'beta', 'phi_vol', 'opt_bias']"
