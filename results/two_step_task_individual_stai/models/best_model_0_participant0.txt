def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-dependent MF credit with anxiety-driven spillover and ship-A bias.
    
    Idea:
    - Standard stage-2 MF learning from rewards.
    - Stage-1 MF receives TD backup from obtained second-stage value.
    - Additionally, after rare transitions, a fraction of the TD signal is misassigned
      to the unchosen first-stage action (transition-dependent MF). This fraction grows
      with anxiety, capturing stress-driven miscrediting.
    - Stage-2 "spillover" generalization: the reward also partially updates the same-index
      alien in the unvisited planet; spillover increases with anxiety.
    - Stage-1 policy includes a baseline bias toward spaceship A.
    - Stage-1 choice value is a mixture of MB (known transitions) and MF with weight
      w = 1 - stai (more MF under higher anxiety).
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for all TD updates
    - spill: [0,1] baseline fraction of reward that generalizes to the other state's same action
    - rare_bias: [0,1] baseline fraction of TD signal credited to the unchosen first-stage action after rare transitions
    - beta: [0,10] inverse temperature for both stages
    - biasA: [0,1] additive bias toward choosing spaceship A at stage 1
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, spill, rare_bias, beta, biasA]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, spill, rare_bias, beta, biasA = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    T_known = np.array([[0.7, 0.3], [0.3, 0.7]])

    w = np.clip(1.0 - stai_score, 0.0, 1.0)

    spill_eff = spill * (0.5 + 0.5 * stai_score)
    rare_eff = rare_bias * (0.5 + 0.5 * stai_score)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        logits1 = q1.copy()
        logits1[0] += biasA

        l1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(l1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = q2[s].copy()
        l2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(l2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        other_s = 1 - s
        q2[other_s, a2] += alpha * spill_eff * (r - q2[other_s, a2])

        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        if not is_common:
            a1_other = 1 - a1

            q1_mf[a1_other] += alpha * rare_eff * (q2[s, a2] - q1_mf[a1_other])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll