def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Learned transitions with anxiety-modulated surprise bonus and choice stickiness.
    
    Core idea:
    - First-stage policy is purely model-based using a learned transition matrix.
    - Anxiety increases the impact of transition surprise on first-stage preferences.
    - Also includes first-stage choice stickiness (perseveration), scaled by anxiety.
    - Second-stage policy is model-free Q-learning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; aliens).
    reward : array-like of float (0 or 1)
        Coins received each trial.
    stai : array-like of float in [0,1]
        Anxiety score used to scale surprise bonus and stickiness.
    model_parameters : list or array
        [alpha, beta, alpha_t, kappa_stick, phi_anx]
        - alpha in [0,1]: learning rate for second-stage Q-value updates.
        - beta in [0,10]: inverse temperature at both stages.
        - alpha_t in [0,1]: transition learning rate for updating the transition matrix.
        - kappa_stick in [0,1]: strength of first-stage choice perseveration.
        - phi_anx in [0,1]: scales the surprise-to-bonus mapping as a function of anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, alpha_t, kappa_stick, phi_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: actions (A,U), cols: states (X,Y)

    q2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = None
    stickiness = np.zeros(2, dtype=float)

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T @ max_q2  # shape (2,)


        bonus = np.zeros(2, dtype=float)
        if t > 0:

            a1_prev = action_1[t - 1]
            s2_prev = state[t - 1]
            p_obs = T[a1_prev, s2_prev]
            surprise = 1.0 - p_obs  # higher when transition was rare/unexpected


            a1_now = action_1[t]
            bonus[a1_now] += phi_anx * stai_val * surprise

        if prev_a1 is not None:
            stickiness = np.zeros(2, dtype=float)

            stickiness[prev_a1] = kappa_stick * (1.0 + stai_val)

        q1_eff = q1_mb + bonus + stickiness
        q1_eff -= np.max(q1_eff)  # softmax stability
        exp_q1 = np.exp(beta * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s2 = state[t]
        q2_s = q2[s2].copy()
        q2_s -= np.max(q2_s)
        exp_q2 = np.exp(beta * q2_s)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2



        a1_row = T[a1]
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s2 else 0.0
            a1_row[s_idx] += alpha_t * (target - a1_row[s_idx])

        T[a1] = a1_row / np.sum(a1_row)

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)