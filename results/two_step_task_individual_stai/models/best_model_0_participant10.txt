def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid RL with learned transitions and anxiety-weighted arbitration and forgetting.
    
    This model learns the first-stage transition structure online and arbitrates
    between model-based (MB) and model-free (MF) control at stage 1. The arbitration
    weight increases with transition certainty but is down-weighted by anxiety.
    Both stage-1 and stage-2 Q-values undergo anxiety-scaled forgetting.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial on the planet (0/1 for the two aliens).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0] in [0,1]. Higher indicates more anxious.
    model_parameters : array-like of float
        [alpha2, beta, alpha_T, w0, decay]
        - alpha2 in [0,1]: stage-2 MF learning rate.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - alpha_T in [0,1]: learning rate for the transition matrix.
        - w0 in [0,1]: baseline arbitration weight for MB control at stage 1.
        - decay in [0,1]: base forgetting rate applied each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha2, beta, alpha_T, w0, decay = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    T = np.full((2, 2), 0.5)

    q1_mf = np.zeros(2)        # model-free stage-1
    q2_mf = np.zeros((2, 2))   # model-free stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    decay_eff = np.clip(decay * (0.5 + 0.5 * np.clip(st, 0.0, 1.0)), 0.0, 1.0)

    for t in range(n_trials):

        q1_mf = (1.0 - decay_eff) * q1_mf
        q2_mf = (1.0 - decay_eff) * q2_mf

        max_q2 = np.max(q2_mf, axis=1)      # value of each planet
        q1_mb = T @ max_q2                  # MB value of each spaceship

        c_actions = 2.0 * np.abs(T[:, 0] - 0.5)
        certainty = np.mean(c_actions)  # overall certainty in [0,1]


        w_eff = w0 * (certainty ** np.clip(1.0 - st, 0.0, 1.0))
        w_eff = np.clip(w_eff, 0.0, 1.0)

        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s = int(state[t])
        q2 = q2_mf[s]
        q2_centered = q2 - np.max(q2)
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]


        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_T * (target - T[a1, sp])

        T[a1] = T[a1] / np.sum(T[a1])

        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha2 * delta2

        v2 = q2_mf[s, a2]
        delta1 = v2 - q1_mf[a1]
        q1_mf[a1] += alpha2 * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll