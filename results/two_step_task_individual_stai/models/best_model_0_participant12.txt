def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-modulated asymmetric learning rates and perseveration.
    
    This purely model-free controller learns second-stage values and backs them up to first-stage
    values via an eligibility trace. Anxiety (stai) creates asymmetry between positive and negative
    prediction-error learning rates and scales a perseveration (choice stickiness) bias at stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate learning-rate asymmetry and perseveration.
    model_parameters : list or array
        [alpha_base, beta, pi_base, stai_weight, lambd]
        Bounds:
          alpha_base in [0,1]  : base learning rate
          beta in [0,10]       : inverse temperature
          pi_base in [0,1]     : baseline perseveration strength
          stai_weight in [0,1] : sensitivity of parameters to STAI (0=no effect, 1=max effect)
          lambd in [0,1]       : eligibility trace from stage 2 to stage 1
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_base, beta, pi_base, stai_weight, lambd = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None


    mod = (2.0 * stai - 1.0) * (2.0 * stai_weight - 1.0)

    alpha_pos = np.clip(alpha_base * (1.0 + mod), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 - mod), 0.0, 1.0)

    pi_eff = pi_base * (1.0 + mod)  # can vary approximately in [0, 2*pi_base]

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        q1 = q_stage1_mf.copy()
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            q1 = q1 + pi_eff * stick

        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        q2_s = q_stage2_mf[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]


        pe2 = reward[t] - q_stage2_mf[s, a2]
        if pe2 >= 0:
            a2_lr = alpha_pos
        else:
            a2_lr = alpha_neg
        q_stage2_mf[s, a2] += a2_lr * pe2

        q_stage1_mf[a1] += a2_lr * lambd * pe2

        td1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]

        alpha1 = np.clip(alpha_base * (1.0 + 0.5 * mod), 0.0, 1.0)
        q_stage1_mf[a1] += alpha1 * td1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll