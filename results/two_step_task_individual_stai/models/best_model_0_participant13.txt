def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free SARSA(Î») with anxiety-modulated arbitration and perseveration.
    
    This model learns second-stage Q-values (per planet/alien) and a model-free first-stage value.
    First-stage choices are driven by a convex combination of model-based and model-free values,
    where the arbitration weight is modulated by the participant's anxiety (STAI). A perseveration
    bias at the first stage is also scaled by anxiety.

    Parameters (model_parameters):
    - alpha in [0,1]: Learning rate for value updates at both stages.
    - beta in [0,10]: Inverse temperature for softmax choice at both stages.
    - lam in [0,1]: Eligibility trace strength propagating second-stage RPE to first stage.
    - w_mb in [0,1]: Baseline weight for model-based values at first stage (before anxiety modulation).
    - stickiness in [0,1]: Strength of first-stage perseveration bias (scaled by STAI).

    Inputs:
    - action_1: array of length n_trials with first-stage actions (0 = A, 1 = U).
    - state: array of length n_trials with observed second-stage state (0 = X, 1 = Y).
    - action_2: array of length n_trials with second-stage actions (0/1 for the two aliens on that planet).
    - reward: array of length n_trials with received reward (e.g., 0 or 1).
    - stai: array-like with a single float in [0,1], the participant's anxiety score.
    - model_parameters: array-like with the parameters [alpha, beta, lam, w_mb, stickiness].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, lam, w_mb, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q_stage2 = np.zeros((2, 2))      # Q2[state, action2]
    q_stage1_mf = np.zeros(2)        # model-free Q at stage 1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10
    w_mb = np.clip(w_mb, eps, 1 - eps)
    logit = np.log(w_mb) - np.log(1 - w_mb)
    w_eff = 1 / (1 + np.exp(-(logit + 2.0 * (0.5 - stai))))  # increases when stai < 0.5, decreases when stai > 0.5
    w_eff = float(np.clip(w_eff, 0.0, 1.0))

    prev_a1 = -1  # for stickiness

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        max_q2 = np.max(q_stage2, axis=1)                  # best alien per planet
        q1_mb = transition_matrix @ max_q2                 # expected value per spaceship
        q1 = w_eff * q1_mb + (1 - w_eff) * q_stage1_mf     # arbitration

        if prev_a1 >= 0:
            kappa_eff = stickiness * (0.5 + stai)          # stronger bias with higher anxiety
            bias = np.array([0.0, 0.0])
            bias[prev_a1] += kappa_eff
            q1 = q1 + bias

        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]


        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        td_to_stage1 = (q_stage2[s, a2] - q_stage1_mf[a1])  # bootstrapping from stage 2
        q_stage1_mf[a1] += alpha * td_to_stage1 + alpha * lam * delta2

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)