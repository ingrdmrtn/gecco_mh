def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Learned-transition model-based RL with anxiety-modulated perseveration.
    
    This model learns the transition matrix from experience and uses a purely
    model-based (MB) first-stage policy computed from the learned transition and
    second-stage Q-values. Stage 2 uses MF Q-learning.
    
    Anxiety (stai) amplifies a perseveration bias to repeat the previous
    spaceship choice, capturing heightened habit-like inertia under higher
    anxiety while still planning via MB values.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2 updates, in [0,1]
    - alpha_t: transition learning rate, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - stick: base perseveration strength added to stage-1 logits, in [0,1]
    - gamma_anx: anxiety gain on perseveration, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, alpha_t, beta, stick, gamma_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    T = np.ones((2, 2)) * 0.5  # rows: spaceships [A,U], cols: planets [X,Y]
    q2 = np.zeros((2, 2))      # second-stage values per planet-alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    stick_eff = stick * np.clip(1.0 + gamma_anx * (stai - 0.31), 0.0, 2.0)

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2


        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += alpha_t * (target - T[a1])

        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll