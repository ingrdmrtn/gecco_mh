def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Learning the transition model with anxiety-weighted 'safety' bias toward predictable spaceships.
    
    Core ideas:
    - The agent learns transition probabilities online (alpha_t) with forgetting (f_forget).
    - Stage-1 uses a hybrid of learned MB values and Stage-1 MF values, with MB weight higher when
      anxiety is low: w_mb = 0.5 + 0.5*(1 - stai).
    - An anxiety-weighted safety bias penalizes actions with higher transition entropy (uncertainty),
      capturing preference for predictable outcomes under anxiety.
    - Stage-2 values are learned model-free.
    
    Parameters (all used; total=5):
    - eta: [0,1] Learning rate for Stage-2 MF values and Stage-1 MF bootstrapping.
    - beta: [0,10] Inverse temperature for both stages.
    - alpha_t: [0,1] Learning rate for updating the transition matrix rows upon observing transitions.
    - f_forget: [0,1] Per-trial forgetting toward uniform for transition probabilities.
    - omega_safe: [0,1] Strength of safety bias; effective bias scales with stai.
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [eta, beta, alpha_t, f_forget, omega_safe].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    eta, beta, alpha_t, f_forget, omega_safe = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T_learn = np.array([[0.6, 0.4],
                        [0.4, 0.6]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))
    q1_mf = np.zeros(2)

    w_mb = 0.5 + 0.5 * (1.0 - stai)          # in [0.5,1.0]; more anxiety => closer to 0.5
    safety_scale = omega_safe * stai          # stronger bias with higher anxiety

    for t in range(n_trials):
        s = state[t]

        max_q2 = np.max(q2, axis=1)
        q1_mb = T_learn @ max_q2



        ent = np.zeros(2)
        for a in range(2):
            p = np.clip(T_learn[a, 0], 1e-8, 1.0 - 1e-8)
            q = 1.0 - p
            H = -(p * np.log(p) + q * np.log(q))
            ent[a] = H / np.log(2.0)
        bias = -safety_scale * ent  # penalize uncertainty

        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias

        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * pe1

        T_learn = (1.0 - f_forget) * T_learn + f_forget * 0.5


        T_learn[a1, s] += alpha_t * (1.0 - T_learn[a1, s])
        other = 1 - s
        T_learn[a1, other] += alpha_t * (0.0 - T_learn[a1, other])

        for a in range(2):
            row = T_learn[a]
            row = np.clip(row, 1e-8, 1.0)
            T_learn[a] = row / np.sum(row)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll