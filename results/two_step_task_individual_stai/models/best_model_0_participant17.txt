def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model with anxiety-shaped MB weighting, anxiety-scaled forgetfulness, and state-conditional perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically in {0,1}.
    stai : array-like of float in [0,1]
        Anxiety score (single value array). Used to modulate MB weighting, forgetfulness, and perseveration.
    model_parameters : tuple/list
        Parameters (all in [0,1] except beta in [0,10]):
        - alpha: learning rate for value updates in [0,1]
        - beta: inverse temperature for softmax in [0,10]
        - w_slope: base MB mixing weight; anxiety shifts toward (1 - w_slope) in [0,1]
                   Effective w_mb = (1 - stai)*w_slope + stai*(1 - w_slope)
        - rho_forget: baseline forgetting/decay strength in [0,1]
                      Effective decay factor each trial: decay = 1 - rho_forget * stai
        - tau_stay: state-conditional perseveration strength in [0,1]
                    Bias to repeat previous second-stage action in same state scales with stai

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, w_slope, rho_forget, tau_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    w_mb = (1.0 - stai) * w_slope + stai * (1.0 - w_slope)
    w_mb = 0.0 if w_mb < 0.0 else (1.0 if w_mb > 1.0 else w_mb)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)        # stage-1 model-free
    q2 = np.zeros((2, 2))      # stage-2 action values

    prev_a2 = None
    prev_s = None

    decay = 1.0 - rho_forget * stai
    if decay < 0.0: decay = 0.0
    if decay > 1.0: decay = 1.0

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)           # best action per state
        q1_mb = T @ max_q2                    # expected value via transition model

        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        z1 = q1 - np.max(q1)
        exp1 = np.exp(beta * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2 is not None and prev_s == s:
            bias2[prev_a2] += tau_stay * stai  # stronger stickiness under higher anxiety

        z2 = (q2_s + bias2) - np.max(q2_s + bias2)
        exp2 = np.exp(beta * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        q1_mf *= decay
        q2 *= decay

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a2 = a2
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll