def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive temperature with anxiety, stage-1 perseveration, and Q-forgetting.

    Description:
    - Anxiety reduces choice precision via an adaptive inverse temperature:
      beta_eff = beta * (1 - k_temp * stai), bounded below by a small floor.
    - Stage-1 includes a perseveration bias that decays over trials; the bias strength is
      reduced by anxiety.
    - Stage-2 is MF; both stage-2 values and perseveration traces are subject to mild forgetting
      toward zero (recency), controlled by tau_forget.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: base inverse temperature in [0,10]
    - k_temp: anxiety sensitivity of temperature in [0,1] (higher -> more reduction in beta)
    - tau_forget: forgetting/decay parameter in [0,1] applied to Q2 and perseveration traces
    - pers: baseline perseveration strength in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, k_temp, tau_forget, pers)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, k_temp, tau_forget, pers = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    trace1 = np.zeros(2)

    beta_eff = max(1e-3, beta * (1.0 - k_temp * st))  # higher anxiety -> lower precision
    stick_strength = pers * (1.0 - st)                # higher anxiety -> weaker perseveration
    decay = np.clip(tau_forget, 0.0, 1.0)             # used for Q2 and trace forgetting

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        pref1 = 0.5 * Q1_mf + 0.5 * Q1_mb + stick_strength * trace1

        centered1 = pref1 - np.max(pref1)
        exp1 = np.exp(beta_eff * centered1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        pref2 = Q2[s]
        centered2 = pref2 - np.max(pref2)
        exp2 = np.exp(beta_eff * centered2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]


        Q2 *= (1.0 - decay)

        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        trace1 *= (1.0 - decay)
        trace1[a1] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll