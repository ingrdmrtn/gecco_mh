def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Pessimistic lookahead with anxiety-modulated loss aversion and forgetting.
    Parameters (model_parameters):
      - alpha: learning rate for Q-value updates at both stages, in [0,1]
      - beta: inverse temperature for both stages, in [0,10]
      - lambda_loss: base loss-aversion coefficient, in [0,1] (effective loss-aversion increases with anxiety)
      - kappa_anx: strength of anxiety-driven pessimism in model-based lookahead, in [0,1]
      - phi_forget: forgetting rate for unchosen actions (value decay), in [0,1]
    Inputs:
      - action_1: array of first-stage choices (0=A, 1=U)
      - state: array of second-stage states (0=X, 1=Y)
      - action_2: array of second-stage choices (0 or 1; e.g., alien index on the planet)
      - reward: array of scalar rewards (can be negative or positive)
      - stai: array-like with a single anxiety score in [0,1]
      - model_parameters: list/tuple as described above
    Returns:
      - Negative log-likelihood of the observed first- and second-stage choices.
    Model summary:
      - Second-stage learning is model-free with loss-averse utility u(r).
      - First-stage decision uses a convex combination of:
          (i) model-free Q1 values bootstrapped from second-stage MF values via eligibility,
          (ii) a pessimistic model-based lookahead over the transition matrix.
        Anxiety increases loss aversion and pessimism, and also increases forgetting of unchosen actions.
    """
    alpha, beta, lambda_loss, kappa_anx, phi_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q1_mf = np.zeros(2)        # model-free values for first-stage actions
    Q2 = np.zeros((2, 2))      # second-stage state-action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)


    lambda_eff = lambda_loss * (1.0 + stai)

    xi = np.clip(1.0 - kappa_anx * stai, 0.0, 1.0)

    forget = np.clip(phi_forget * (0.5 + stai), 0.0, 1.0)

    eps = 1e-12

    for t in range(n_trials):

        vmax = np.max(Q2, axis=1)   # per state
        vmin = np.min(Q2, axis=1)   # per state
        V_state = xi * vmax + (1.0 - xi) * vmin  # pessimism-weighted value per state

        Q1_mb_pess = T @ V_state  # model-based action values under pessimistic evaluation


        w_eff = np.clip(0.5 + 0.5 * stai * (1.0 - lambda_loss), 0.0, 1.0)

        Q1 = w_eff * Q1_mb_pess + (1.0 - w_eff) * Q1_mf

        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        s = int(state[t])
        q2c = Q2[s, :] - np.max(Q2[s, :])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])
        if r >= 0:
            u = r
        else:
            u = - (1.0 + lambda_eff) * (-r)


        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        other_a2 = 1 - a2
        Q2[s, other_a2] *= (1.0 - forget)

        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1

        other_a1 = 1 - a1
        Q1_mf[other_a1] *= (1.0 - forget)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll