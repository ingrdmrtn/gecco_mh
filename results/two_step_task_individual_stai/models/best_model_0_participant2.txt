def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration and eligibility traces.
    
    This model combines model-free Q-values with model-based estimates from the known transition
    structure. The arbitration weight favoring model-based control is modulated by STAI (anxiety)
    such that lower anxiety increases model-based weighting. Stage-1 Q-values are updated with an
    eligibility trace from the stage-2 prediction error.
    
    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like, shape (n_trials,)
        Second-stage states (0 = planet X, 1 = planet Y) actually reached after action_1.
    action_2 : array-like, shape (n_trials,)
        Second-stage choices (0/1; X: W/S, Y: P/H).
    reward : array-like, shape (n_trials,)
        Obtained reward (e.g., coins; typically 0/1).
    stai : array-like, shape (1,) or scalar-like
        Participant STAI score in [0,1]. Lower values indicate lower anxiety.
    model_parameters : iterable
        Model parameters (total <= 5):
        - alpha in [0,1]: learning rate for value updates
        - beta1 in [0,10]: inverse temperature at stage 1
        - beta2 in [0,10]: inverse temperature at stage 2
        - lam in [0,1]: eligibility trace strength from stage 2 PE to stage 1 MF value
        - w0 in [0,1]: baseline arbitration weight that is modulated by STAI

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta1, beta2, lam, w0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(np.asarray(stai)[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]], dtype=float)

    q1_mf = np.zeros(2)           # model-free stage-1 action values (A,U)
    q2 = np.zeros((2, 2))         # stage-2 state-action values: rows X,Y; cols actions 0/1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)




    eps = 1e-10
    w0 = min(max(w0, 0.0), 1.0)

    w0_logit = np.log((w0 + eps) / (1.0 - (w0 + eps)))
    w_logit = w0_logit + 3.0 * (0.5 - stai_val)  # lower STAI -> larger w
    w = 1.0 / (1.0 + np.exp(-w_logit))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = transition_matrix @ max_q2  # shape (2,)

        q1 = (1.0 - w) * q1_mf + w * q1_mb

        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1_shift)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta2 * q2_shift)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]


        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1
        q1_mf[a1] += alpha * lam * delta2

    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)