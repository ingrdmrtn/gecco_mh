def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free control with anxiety-modulated planning weight and bi-stage perseveration.

    Overview:
    - Stage-2 action values are learned via incremental reward prediction errors.
    - Stage-1 uses a hybrid of model-based (via known transitions) and model-free values.
    - The model-based weight is modulated by anxiety (stai).
    - Perseveration (choice stickiness) acts at both stages and is weaker with higher anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 values and stage-1 bootstrapping
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = omega0 (0 to 1): baseline weight on model-based value at stage 1
    - model_parameters[3] = anx_mod (0 to 1): strength of anxiety modulation on model-based weight
       Effective MB weight: omega_eff = clip(omega0 + anx_mod * (0.5 - stai), 0, 1).
       Higher anxiety (stai>0.5) reduces model-based weight.
    - model_parameters[4] = pi (0 to 1): baseline perseveration magnitude; applied as pi*(1 - stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha, beta, omega0, anx_mod, pi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q1_mf = np.zeros(2)        # model-free stage-1 values
    q2 = np.zeros((2, 2))      # stage-2 values: states x actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_by_state = [None, None]

    omega_eff = np.clip(omega0 + anx_mod * (0.5 - stai_val), 0.0, 1.0)
    stick_strength = pi * (1.0 - stai_val)  # less stickiness with higher anxiety

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)                  # shape (2,)
        q1_mb = transition_matrix @ max_q2           # shape (2,)

        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick_strength

        q1_net = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf + bias1

        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick_strength

        q2_net = q2[s] + bias2
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll