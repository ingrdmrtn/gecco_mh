def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Learned transitions with UCB exploration and anxiety-modulated forgetting.

    Idea
    ----
    The agent learns the stage-1 transition probabilities from experience via
    simple Dirichlet counts. Stage-2 action selection includes an uncertainty
    bonus (UCB-style) based on visit counts, with anxiety reducing exploratory
    bonus. First-stage action values combine learned model-based values and
    model-free values; anxiety reduces the model-based blend. Q-values decay
    toward 0 each trial (forgetting), which is stronger under higher anxiety.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices.
    state : array-like of int in {0,1}
        Second-stage state per trial.
    action_2 : array-like of int in {0,1}
        Second-stage actions.
    reward : array-like of float
        Rewards per trial.
    stai : array-like of float in [0,1]
        Anxiety score; only stai[0] is used.
    model_parameters : list/tuple of floats
        [alpha, beta, zeta0, phi0, forget0]
        - alpha in [0,1]: learning rate for Q-values at both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - zeta0 in [0,1]: baseline exploration bonus scale (reduced by anxiety).
        - phi0 in [0,1]: baseline model-based weight at stage 1 (reduced by anxiety).
        - forget0 in [0,1]: baseline per-trial forgetting rate (increased by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, zeta0, phi0, forget0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    zeta = max(0.0, min(1.0, zeta0 * (1.0 - stai)))            # less exploration with anxiety
    phi = max(0.0, min(1.0, phi0 * (1.0 - 0.5 * stai)))        # less MB weighting with anxiety
    forget = max(0.0, min(1.0, forget0 * (0.5 + 0.5 * stai)))  # more forgetting with anxiety

    trans_counts = np.ones((2, 2))  # rows: a1 in {0,1}, cols: state in {0,1}

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    visit_counts = np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        q1_mf *= (1.0 - forget)
        q2_mf *= (1.0 - forget)

        trans_probs = trans_counts / np.sum(trans_counts, axis=1, keepdims=True)  # shape (2,2)

        max_q2 = np.max(q2_mf, axis=1)           # best per state
        q1_mb = trans_probs @ max_q2

        q1_hybrid = phi * q1_mb + (1.0 - phi) * q1_mf

        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        p_choice_1[t] = probs1[a1]

        bonus = zeta / np.sqrt(visit_counts[s] + 1e-8)
        logits2 = beta * (q2_mf[s] + bonus)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        visit_counts[s, a2] += 1.0

        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        backup = q2_mf[s, a2]
        delta1 = backup - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        trans_counts[a1, s] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)