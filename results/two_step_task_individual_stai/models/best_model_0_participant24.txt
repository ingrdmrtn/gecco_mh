def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-free SARSA(0) with anxiety-modulated learning asymmetry and choice stickiness.
    
    This model learns MF values for both stages. Learning rates differ for positive vs. negative
    prediction errors, and anxiety shifts the positivity bias (higher anxiety reduces the difference).
    Choice policies at both stages include a perseveration (stickiness) bias that grows with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha_pos, alpha_neg, beta, kappa0, kappa_stai]
        - alpha_pos in [0,1]: base learning rate for positive prediction errors.
        - alpha_neg in [0,1]: base learning rate for negative prediction errors.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa0 in [0,1]: baseline perseveration bias added to the last chosen action.
        - kappa_stai in [0,1]: how much anxiety increases perseveration (stickiness).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, kappa0, kappa_stai = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    q1 = np.zeros(2)            # stage-1 MF values
    q2 = np.zeros((2, 2))       # stage-2 MF values

    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    kappa = kappa0 + kappa_stai * s

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        st = state[t]
        bias2 = np.zeros(2)
        if last_a2_by_state[st] is not None:
            bias2[last_a2_by_state[st]] += kappa

        prefs1 = q1 + bias1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        prefs2 = q2[st] + bias2
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]




        eff_alpha_pos = (1 - s) * alpha_pos + s * alpha_neg

        eff_alpha_neg = (1 - s) * alpha_neg + s * alpha_pos

        r = reward[t]
        pe2 = r - q2[st, a2]
        a2_lr = eff_alpha_pos if pe2 >= 0 else eff_alpha_neg
        q2[st, a2] += a2_lr * pe2

        pe1 = q2[st, a2] - q1[a1]
        a1_lr = eff_alpha_pos if pe1 >= 0 else eff_alpha_neg
        q1[a1] += a1_lr * pe1

        last_a1 = a1
        last_a2_by_state[st] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll