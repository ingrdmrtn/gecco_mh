def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-modulated learning asymmetry and perseveration.
    
    Overview:
    - Learns an internal transition model T_est for each first-stage action and plans model-based values through it.
    - Combines MB and MF values at stage 1 with a simple anxiety-based arbitration: weight on MF increases with anxiety.
    - Stage-2 learning uses valence-asymmetric learning rates that scale with anxiety (more anxious -> stronger learning from non-reward).
    - Includes an anxiety-amplified perseveration bias at stage 1.

    Parameters (model_parameters):
    - alpha: base learning rate in [0,1].
    - beta: inverse temperature for softmax in [0,10].
    - kappa0: base learning rate for updating the transition model in [0,1].
    - phi: perseveration strength in [0,1], bias to repeat previous stage-1 action.
    - a_neg: asymmetry coefficient in [0,1] scaling how anxiety increases learning from negative outcomes.

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (0=A, 1=U).
    - state: array of ints in {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}, second-stage choices on the observed planet.
    - reward: array of floats (typically 0 or 1).
    - stai: array-like with a single float in [0,1], anxiety score for the participant.
    - model_parameters: list/array [alpha, beta, kappa0, phi, a_neg].

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    alpha, beta, kappa0, phi, a_neg = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    prev_a1 = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12


    w_mf = st
    w_mb = 1.0 - w_mf

    kappa = kappa0 * (0.25 + 0.75 * st)

    alpha_pos = np.clip(alpha * (1.0 - a_neg * st), 0.0, 1.0)
    alpha_neg = np.clip(alpha * (1.0 + a_neg * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        max_q2 = np.max(q_stage2, axis=1)      # best alien on each planet
        q1_mb = T_est @ max_q2

        persev_bias = np.zeros(2)
        if prev_a1 is not None:
            persev_bias[prev_a1] = phi * (0.5 + 0.5 * st)

        q1_total = w_mb * q1_mb + w_mf * q_stage1_mf + persev_bias

        exp_q1 = np.exp(beta * (q1_total - np.max(q1_total)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        q2_vec = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_vec - np.max(q2_vec)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]


        target = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1] = (1.0 - kappa) * T_est[a1] + kappa * target

        T_est[a1] = T_est[a1] / (np.sum(T_est[a1]) + eps)

        pe2 = r - q_stage2[s, a2]
        lr2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q_stage2[s, a2] += lr2 * pe2

        pe1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr2 * pe1

        prev_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss