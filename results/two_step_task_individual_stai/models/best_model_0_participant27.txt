def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-sensitive exploration, rare-transition down-weighting, and Pavlovian safety bias.

    This model blends model-free and model-based control with a fixed weight, but:
    - Down-weights model-based control specifically after rare transitions, more so with higher anxiety.
    - Uses anxiety to reduce effective beta (more exploration with higher anxiety).
    - Adds a Pavlovian 'safety' bias toward action 0 at stage 2 that grows with anxiety and uncertainty.
    - Includes value forgetting toward 0.5 to capture drift in unchosen/unstimulated values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Rewards each trial.
    stai : array-like of float
        Single anxiety score in [0,1].
    model_parameters : list or array of floats
        [alpha, beta, mb_weight, bias_safe, forget]
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: base inverse temperature.
        - mb_weight in [0,1]: baseline model-based mixing weight at stage 1.
        - bias_safe in [0,1]: strength of Pavlovian bias favoring action 0 under uncertainty.
        - forget in [0,1]: forgetting rate pulling Qs toward 0.5 each trial.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, mb_weight, bias_safe, forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = 0.5 * np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

        mb_w_eff = mb_weight if is_common else mb_weight * (1.0 - 0.7 * stai)
        mb_w_eff = np.clip(mb_w_eff, 0.0, 1.0)

        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_fixed @ max_Q2
        Q1 = mb_w_eff * Q1_mb + (1.0 - mb_w_eff) * Q1_mf

        beta_eff = beta * (1.0 - 0.5 * stai)

        logits1 = beta_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        uncertainty = 1.0 - abs(Q2[s, 0] - Q2[s, 1])  # in [0,1]
        bias_vec = np.array([bias_safe * stai * uncertainty, 0.0])

        logits2 = beta_eff * Q2[s] + bias_vec
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]


        Q2 = (1.0 - forget) * Q2 + forget * 0.5
        Q1_mf = (1.0 - forget) * Q1_mf + forget * 0.0  # MF baseline around 0

        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * (0.5 * delta1 + 0.5 * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll