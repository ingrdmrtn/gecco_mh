def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-curvature utility, anxiety-shaped asymmetry, and forgetting.
    
    A model-free learner updates stage-2 values using a nonlinear utility function
    u(r) = r^curv (concave if curv<1). Learning is asymmetric for positive vs negative
    prediction errors, with asymmetry scaled by anxiety. Values undergo forgetting
    toward a neutral prior (0.5). A single inverse temperature governs both stages,
    and its effective strength is reduced for higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=A, 1=U.
    state : array-like of int (0 or 1)
        Second-stage states: 0=X, 1=Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Reward outcome.
    stai : array-like with one float in [0,1]
        Anxiety score; increases learning asymmetry and softens choice.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, c_asym, forget, curv]
        - alpha: base learning rate [0,1]
        - beta: inverse temperature baseline [0,10]
        - c_asym: scales asymmetry via anxiety: 
                  alpha_plus = alpha * (1 + c_asym*stai), 
                  alpha_minus = alpha * (1 - c_asym*stai) [0,1]
        - forget: forgetting rate toward 0.5 for all Qs each trial [0,1]
        - curv: utility curvature for rewards in [0,1], u = r**curv [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, c_asym, forget, curv = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    beta_eff = beta * (1.0 - 0.5 * (stai_val - 0.5))
    if beta_eff < 1e-6:
        beta_eff = 1e-6

    Q1 = np.zeros(2)        # MF stage-1 values
    Q2 = np.zeros((2, 2))   # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    alpha_plus = alpha * (1.0 + c_asym * stai_val)
    alpha_minus = alpha * (1.0 - c_asym * stai_val)
    if alpha_plus > 1.0:
        alpha_plus = 1.0
    if alpha_minus < 0.0:
        alpha_minus = 0.0

    for t in range(n_trials):

        Q1 = (1.0 - forget) * Q1 + forget * 0.5
        Q2 = (1.0 - forget) * Q2 + forget * 0.5

        q1c = Q1 - np.max(Q1)
        ps1 = np.exp(beta_eff * q1c)
        ps1 = ps1 / (np.sum(ps1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = ps1[a1]

        s = int(state[t])
        q2c = Q2[s] - np.max(Q2[s])
        ps2 = np.exp(beta_eff * q2c)
        ps2 = ps2 / (np.sum(ps2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = ps2[a2]

        r = reward[t]
        u = r ** curv

        pe2 = u - Q2[s, a2]
        a2_lr = alpha_plus if pe2 >= 0.0 else alpha_minus
        Q2[s, a2] += a2_lr * pe2

        pe1 = Q2[s, a2] - Q1[a1]
        a1_lr = alpha_plus if pe1 >= 0.0 else alpha_minus
        Q1[a1] += a1_lr * pe1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + 1e-10)) + np.sum(np.log(p_choice_2 + 1e-10)))
    return neg_log_lik