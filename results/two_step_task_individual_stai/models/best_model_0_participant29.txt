def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with uncertainty aversion and an anxiety-modulated choice kernel.
    The agent dislikes uncertain second-stage options: both action selection and learning
    penalize options with higher reward uncertainty (estimated from current Q2 via p*(1-p)).
    Anxiety increases uncertainty aversion. Additionally, a simple choice kernel captures
    repetition tendencies at both stages, with strength growing slightly with anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within the reached state.
    - reward: array-like of floats in [0,1]. Coins received on each trial.
    - stai: array-like with one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for MF values
        beta: [0,10] — inverse temperature for both stages
        kernel_lr: [0,1] — learning/decay rate of the choice kernels
        risk_penalty: [0,1] — base penalty weight for uncertainty
        anx_risk_gain: [0,1] — how strongly anxiety amplifies uncertainty aversion and kernel strength

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha, beta, kernel_lr, risk_penalty, anx_risk_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    k1 = np.zeros(2)
    k2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    lam = risk_penalty * (1.0 + anx_risk_gain * stai_val)
    kernel_strength = kernel_lr * (1.0 + 0.5 * anx_risk_gain * stai_val)

    eps = 1e-12

    for t in range(n_trials):

        var2 = q2 * (1.0 - q2)
        unc2 = np.sqrt(np.clip(var2, 0.0, 0.25))  # in [0, 0.5]


        logits1 = q1 + kernel_strength * (2.0 * k1 - 1.0)
        logits1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = (q2[s] - lam * unc2[s]) + kernel_strength * (2.0 * k2[s] - 1.0)
        logits2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]


        var_chosen = q2[s, a2] * (1.0 - q2[s, a2])
        unc_chosen = np.sqrt(max(0.0, min(0.25, var_chosen)))
        r_subj = r - lam * unc_chosen
        pe2 = r_subj - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        k1 = (1.0 - kernel_lr) * k1
        k1[a1] += kernel_lr

        k2[s] = (1.0 - kernel_lr) * k2[s]
        k2[s, a2] += kernel_lr

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)