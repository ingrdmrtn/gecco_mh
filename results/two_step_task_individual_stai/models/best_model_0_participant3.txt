def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-gated model-based control with anxiety-amplified arbitration and stickiness.

    Mechanism overview:
    - Stage-2 values Q2(s2, a2) learned with a single learning rate.
    - Stage-1 hybrid action values combine a model-based (MB) projection using a learned
      transition model T(a1->s2) and a model-free (MF) backup from the last reached Q2.
    - A Dirichlet transition posterior is tracked from observed transitions; the entropy of T
      indexes transition uncertainty. Higher uncertainty increases MB reliance, and this
      effect is amplified by anxiety (stai).
    - A single perseveration parameter k1 acts at both stages; its effect is attenuated
      at stage-2 by anxiety (higher anxiety reduces stage-2 stickiness).

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices (A=0, U=1)
    - state:    int array (n_trials,) in {0,1}; reached second-stage planet (X=0, Y=1)
    - action_2: int array (n_trials,) in {0,1}; second-stage alien choice
    - reward:   float array (n_trials,) in [0,1]; coins received
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        rho_v   in [0,1]: value learning rate for Q2 and MF backup to Q1
        beta    in [0,10]: inverse temperature for softmax at both stages
        k1      in [0,1]: perseveration strength (shared); anxiety scales its stage-2 effect
        omega0  in [0,1]: baseline weight on model-based control at stage-1
        xi_unc  in [0,1]: strength of uncertainty-driven boost to MB weight

    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    rho_v, beta, k1, omega0, xi_unc = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    q2 = np.zeros((2, 2), dtype=float)   # Q2[s2, a2]
    q1_mf = np.zeros(2, dtype=float)     # model-free Q at stage-1

    trans_counts = np.ones((2, 2), dtype=float)  # symmetric prior -> starts at 0.5/0.5
    eps = 1e-12

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):

        T = trans_counts / (np.sum(trans_counts, axis=1, keepdims=True) + eps)


        ent = -np.sum(T * (np.log(T + eps)), axis=1) / np.log(2 + eps)
        unc = 0.5 * (ent[0] + ent[1])  # scalar in [0,1]

        max_q2 = np.max(q2, axis=1)        # value of each second-stage state
        q1_mb = T @ max_q2                 # MB value for each first-stage action

        omega_eff = omega0 + xi_unc * unc * (0.5 + 0.5 * s_anx)
        omega_eff = float(np.clip(omega_eff, 0.0, 1.0))

        q1 = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        logits1 = beta * q1 + k1 * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s2 = int(state[t])
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        k2_eff = k1 * (1.0 - 0.7 * s_anx)  # stronger anxiety -> less stage-2 perseveration
        logits2 = beta * q2[s2] + k2_eff * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        trans_counts[a1, s2] += 1.0  # simple Bayesian counting update

        delta2 = r - q2[s2, a2]
        q2[s2, a2] += rho_v * delta2

        target1 = q2[s2, a2]  # could include immediate r via delta2 already embedded
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += rho_v * delta1

        q1_mf[a1] += rho_v * delta2

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)