def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with eligibility trace and perseveration.
    
    Anxiety use: Higher STAI reduces model-based control weight (w), biasing toward model-free control.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (spaceship: 0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (planet: 0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien on the planet; index within the planet).
    reward : array-like of float (0 or 1)
        Received reward per trial.
    stai : array-like of float
        Trait anxiety score; stai[0] used here. Interpreted in [0,1].
    model_parameters : array-like of float
        [alpha, beta, w, lambda_, perseveration]
        - alpha in [0,1]: learning rate for Q-value updates.
        - beta in [0,10]: inverse temperature for softmax choice.
        - w in [0,1]: baseline model-based weight; anxiety reduces this.
        - lambda_ in [0,1]: eligibility trace mixing stage-2 PE into stage-1 update.
        - perseveration in [0,1]: tendency to repeat previous action (applied at both stages).
        
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w, lambda_, perseveration = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)           # MF Q for first-stage actions (A,U)
    q_stage2_mf = np.zeros((2, 2))      # MF Q for second-stage actions at each state

    prev_a1 = None
    prev_a2_by_state = [None, None]

    w_eff = np.clip(w * (1.0 - 0.8 * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])

        max_q_stage2 = np.max(q_stage2_mf, axis=1)      # size 2: best alien at X, Y
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value of A and U

        q1 = (1.0 - w_eff) * q_stage1_mf + w_eff * q_stage1_mb

        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += perseveration

        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        q2 = q_stage2_mf[s].copy()
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += perseveration

        logits2 = beta * q2 + bias2
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        delta2 = r - q_stage2_mf[s, a2]

        q_stage2_mf[s, a2] += alpha * delta2

        q_stage1_mf[a1] += alpha * (delta1 + lambda_ * delta2)

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll