def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-modulated forgetting and model-based heuristic bias toward common transitions.

    Idea:
    - Use MF Q-learning at both stages with per-trial forgetting/decay of all Q-values.
    - Anxiety increases the effective decay (forgetting), capturing higher volatility beliefs.
    - First-stage decision includes a heuristic bias that favors the spaceship whose common
      destination currently has higher estimated value (based on max second-stage Q for each planet).
      This uses the known transition structure without full planning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered state (0/1).
    reward : array-like of float
        Rewards obtained on each trial.
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1]. Higher anxiety => stronger forgetting.
    model_parameters : array-like of floats, length 5
        [alpha, beta, decay, k_anx_decay, tr_bias]
        - alpha in [0,1]: learning rate.
        - beta in [0,10]: inverse temperature.
        - decay in [0,1]: baseline forgetting rate applied each trial to all Q-values.
        - k_anx_decay in [0,1]: scales how anxiety increases effective decay:
                                decay_eff = min(1, decay + k_anx_decay * stai).
        - tr_bias in [0,1]: strength of heuristic transition bias added to first-stage logits.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, decay, k_anx_decay, tr_bias = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    decay_eff = decay + k_anx_decay * stai_val
    if decay_eff < 0.0:
        decay_eff = 0.0
    if decay_eff > 1.0:
        decay_eff = 1.0
    keep = 1.0 - decay_eff

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):

        v_planet = np.max(q2, axis=1)  # [vX, vY]


        bias1 = np.array([tr_bias * (v_planet[0] - v_planet[1]),
                          -tr_bias * (v_planet[0] - v_planet[1])])

        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        s = int(state[t])
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        r = reward[t]

        q1 *= keep
        q2 *= keep

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll