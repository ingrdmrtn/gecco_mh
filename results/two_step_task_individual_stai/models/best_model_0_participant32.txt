def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Asymmetric (win/loss) learning with anxiety-modulated forgetting and choice persistence.
    
    Core idea:
    - Model-free learner with separate learning rates for positive vs. negative second-stage prediction errors.
    - Anxiety increases global forgetting of Q-values (toward a neutral prior), reflecting reduced confidence/maintenance.
    - Persistence (choice stickiness) at both stages, scaled up by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices (alien) per trial.
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [mu_win, mu_loss, beta, z_forget, psi_persist]
        Bounds:
        - mu_win: [0,1] learning rate used when the second-stage prediction error is positive.
        - mu_loss: [0,1] learning rate used when the second-stage prediction error is negative.
        - beta: [0,10] inverse temperature for both stages.
        - z_forget: [0,1] forgetting strength per trial, scaled by anxiety.
        - psi_persist: [0,1] baseline stickiness (persistence) strength, scaled by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    mu_win, mu_loss, beta, z_forget, psi_persist = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    q1_mf = np.zeros(2)        # values for first-stage actions A/U
    q2 = np.zeros((2, 2))      # values for second-stage state x action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_state = {0: None, 1: None}

    prior_q1 = 0.5
    prior_q2 = 0.5

    for t in range(n_trials):

        stick = psi_persist * (0.5 + 0.5 * stai)

        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        logits1 = beta * (q1_mf - np.max(q1_mf)) + stick * bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2_state[s] is not None:
            bias2[prev_a2_state[s]] = 1.0
        logits2 = beta * (q2_s - np.max(q2_s)) + stick * bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]
        pe2 = r - q2[s, a2]
        lr2 = mu_win if pe2 >= 0.0 else mu_loss
        q2[s, a2] += lr2 * pe2

        pe1 = q2[s, a2] - q1_mf[a1]
        lr1 = mu_win if pe1 >= 0.0 else mu_loss
        q1_mf[a1] += lr1 * pe1

        f = np.clip(z_forget * (0.5 + 0.5 * stai), 0.0, 1.0)
        q1_mf = (1.0 - f) * q1_mf + f * prior_q1
        q2 = (1.0 - f) * q2 + f * prior_q2

        prev_a1 = a1
        prev_a2_state[s] = a2

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik