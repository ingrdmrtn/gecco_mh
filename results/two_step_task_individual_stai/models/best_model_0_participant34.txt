def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid with anxiety-biased transition learning and first-stage perseveration.
    
    The agent learns the first-stage transition probabilities online and uses them
    for model-based evaluation. Anxiety biases transition learning: rare transitions
    (relative to current belief) are learned faster when anxiety is high, while
    common transitions are learned more conservatively. Additionally, first-stage
    choices exhibit perseveration that scales with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien 0 or 1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alphaQ, beta, tau_T, k_anx_trans_bias, psi_perseverate]
        - alphaQ in [0,1]: learning rate for MF values (both stages).
        - beta in [0,10]: inverse temperature for both stages.
        - tau_T in [0,1]: base learning rate for transition probabilities.
        - k_anx_trans_bias in [0,1]: scales anxiety-dependent modulation of transition learning.
          Effective tau: increased for rare transitions, decreased for common ones:
          tau_eff = tau_T * (1 + k_anx_trans_bias * stai) if rare else tau_T * (1 - k_anx_trans_bias * stai).
        - psi_perseverate in [0,1]: base perseveration weight at stage 1; actual bias is psi_perseverate * stai.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alphaQ, beta, tau_T, k_anx_trans_bias, psi_perseverate = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    prev_a1 = None

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        stick_vec = np.zeros(2)
        if prev_a1 is not None:
            stick_vec[prev_a1] = 1.0
        perseveration_bias = psi_perseverate * stai_val * stick_vec

        q1_comb = 0.5 * q1_mb + 0.5 * q1_mf

        logits1 = q1_comb + perseveration_bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]


        p_obs = T[a1, s]
        is_rare = p_obs < 0.5
        if is_rare:
            tau_eff = tau_T * (1.0 + k_anx_trans_bias * stai_val)
        else:
            tau_eff = tau_T * (1.0 - k_anx_trans_bias * stai_val)

        if tau_eff < 0.0:
            tau_eff = 0.0
        if tau_eff > 1.0:
            tau_eff = 1.0


        T[a1, s] += tau_eff * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] = 1.0 - T[a1, s]

        delta2 = r - q2[s, a2]
        q2[s, a2] += alphaQ * delta2

        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alphaQ * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)