def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with anxiety-enhanced WSLS at stage 2 and value leakage.

    This model uses:
    - Risk-sensitive utility at stage 2 with loss aversion that increases with anxiety.
    - A mixture of softmax and win-stay/lose-shift (WSLS) at stage 2; anxiety increases
      the WSLS weight, promoting heuristic repetition/switching.
    - A leak/forgetting term on Q-values to capture drift and reduced confidence.
    - Stage-1 values are MF bootstrapped from Q2; softmax inverse temperature decreases
      with anxiety (noisier choices).

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_q: base learning rate for Q updates in [0,1]
        beta_base: base inverse temperature in [0,10]
        phi_leak: value leak/forgetting rate in [0,1] (applied each trial)
        zeta_wsls: base WSLS mixture weight in [0,1]
        nu_loss: base loss aversion coefficient in [0,1] (utility for negative outcomes)

    Bounds
    - alpha_q, phi_leak, zeta_wsls, nu_loss in [0,1]
    - beta_base in [0,10]

    Anxiety usage
    - Loss aversion increases with anxiety: nu_eff = nu_loss * (1 + stai)
    - WSLS weight increases with anxiety: w_wsls = clip(zeta_wsls * (0.5 + 0.5*stai), 0, 1)
    - Inverse temperature decreases with anxiety: beta = beta_base * (1 - 0.5*stai)

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_q, beta_base, phi_leak, zeta_wsls, nu_loss = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    beta = max(0.0, min(10.0, beta_base * (1.0 - 0.5 * stai)))
    w_wsls = max(0.0, min(1.0, zeta_wsls * (0.5 + 0.5 * stai)))
    nu_eff = max(0.0, min(2.0, nu_loss * (1.0 + stai)))  # cap at 2 to keep utilities reasonable
    leak = max(0.0, min(1.0, phi_leak))

    q1 = np.zeros(2)         # MF first-stage
    q2 = np.zeros((2, 2))    # second-stage

    prev_a2 = np.zeros(2, dtype=int)  # last chosen action for each state
    prev_sign = np.zeros(2)           # last reward sign (+1 / -1) for each state
    has_prev = np.zeros(2, dtype=bool)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        c_q1 = q1 - np.max(q1)
        probs1 = np.exp(beta * c_q1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s = int(state[t])
        c_q2 = q2[s] - np.max(q2[s])
        probs2_soft = np.exp(beta * c_q2)
        probs2_soft /= np.sum(probs2_soft)

        if has_prev[s]:
            if prev_sign[s] >= 0.0:

                wsls_probs = np.array([0.0, 0.0])
                wsls_probs[prev_a2[s]] = 1.0
            else:

                wsls_probs = np.array([0.0, 0.0])
                wsls_probs[1 - prev_a2[s]] = 1.0
        else:
            wsls_probs = np.array([0.5, 0.5])

        probs2 = (1.0 - w_wsls) * probs2_soft + w_wsls * wsls_probs
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])
        util = r if r >= 0.0 else -nu_eff * (-r)

        q2 *= (1.0 - leak)
        q1 *= (1.0 - leak)

        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha_q * pe1

        prev_a2[s] = a2
        prev_sign[s] = 1.0 if r >= 0.0 else -1.0
        has_prev[s] = True

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll