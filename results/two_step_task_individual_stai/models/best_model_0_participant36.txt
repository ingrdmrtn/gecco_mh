def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Volatility-adaptive hybrid with anxiety-modulated exploration and perseveration.
    
    Overview
    --------
    The agent learns second-stage action values (Q2) with a standard TD rule and backs up a model-free
    first-stage value (Q1_mf). A fixed transition model (0.7 common, 0.3 rare) supports model-based
    evaluation (Q1_mb). First-stage choice uses a hybrid of MB and MF values.
    
    Novel mechanisms:
      - Volatility-adaptive inverse temperature: beta is down-regulated as estimated reward volatility
        increases, and further reduced by anxiety (stai).
      - Perseveration bias at both stages to capture choice stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int {0,1}
        Second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the observed state.
    reward : array-like of float
        Rewards (typically 0 or 1).
    stai : array-like of float
        Anxiety score; stai[0] in [0,1]. Higher anxiety reduces effective exploration temperature.
    model_parameters : list or array
        [alpha, beta, k_vol, w_MB, stickiness]
        - alpha in [0,1]: learning rate for Q updates (both stages).
        - beta in [0,10]: base inverse temperature for both stages.
        - k_vol in [0,1]: volatility learning rate controlling sensitivity to reward PE variance.
        - w_MB in [0,1]: weight on model-based values at stage 1 (1=fully MB).
        - stickiness in [0,1]: strength of choice perseveration bias at both stages.
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta_base, k_vol, w_MB, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    v = 0.0

    prev_a1 = 0
    prev_a2 = 0

    beta_base_eff = beta_base * (1.0 - 0.5 * stai)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        Q1 = w_MB * Q1_mb + (1.0 - w_MB) * Q1_mf

        beta_t = beta_base_eff / (1.0 + v)

        bias1 = np.array([0.0, 0.0])
        bias1[prev_a1] += stickiness

        logits1 = beta_t * (Q1 - np.max(Q1)) + bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        bias2 = np.array([0.0, 0.0])
        bias2[prev_a2] += stickiness
        logits2 = beta_t * (Q2[s] - np.max(Q2[s])) + bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]


        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        v = (1.0 - k_vol) * v + k_vol * (pe2 * pe2)

        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)