def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric learning with anxiety-weighted negativity and value decay; hybrid control.
    
    Stage-2 learning uses outcome-valence asymmetric learning rates. Anxiety increases
    sensitivity to negative outcomes and reduces reliance on model-based planning.
    Both stage-2 and stage-1 MF values decay over time (forgetting). Stage-1 choice
    uses a hybrid of MB planning (using the known transition structure) and MF values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (two aliens per planet).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values increase negative-learning asymmetry and forgetting,
        and reduce MB arbitration weight.
    model_parameters : iterable of floats
        [alpha, beta, mb_w, asym, decay]
        - alpha: [0,1] base learning rate for TD updates.
        - beta: [0,10] inverse temperature for softmax choices (both stages).
        - mb_w: [0,1] baseline MB weight at stage 1 (reduced by anxiety).
        - asym: [0,1] strength of valence asymmetry in learning rates.
        - decay: [0,1] forgetting rate (toward neutral values) per trial.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, mb_w, asym, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)       # stage-1 MF Q for A/U (initialized neutral at 0)
    q2 = np.full((2, 2), 0.5) # stage-2 Q initialized neutral at 0.5

    w = mb_w * (1.0 - stai)
    w = np.clip(w, 0.0, 1.0)

    decay_eff_2 = np.clip(decay * (0.5 + 0.5 * stai), 0.0, 1.0)
    decay_eff_1 = np.clip(decay * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        exp_q2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        delta2 = r - q2[s, a2]

        alpha_pos = np.clip(alpha * (1.0 + asym * (1.0 - stai)), 0.0, 1.0)
        alpha_neg = np.clip(alpha * (1.0 + asym * stai), 0.0, 1.0)
        lr = alpha_pos if delta2 >= 0 else alpha_neg
        q2[s, a2] += lr * delta2

        target1 = q2[s, a2]
        q1_mf[a1] += alpha * (target1 - q1_mf[a1])


        q2 = (1.0 - decay_eff_2) * q2 + decay_eff_2 * 0.5

        q1_mf = (1.0 - decay_eff_1) * q1_mf

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood