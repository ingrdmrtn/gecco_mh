def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-gated hybrid control with anxiety-modulated stickiness at both stages.

    The agent learns second-stage values model-free and uses a fixed transition
    model to compute model-based first-stage values. The weight on model-based
    control is dynamically increased by transition surprise (rare transitions),
    with a gain parameter scaled by anxiety. Perseveration (choice stickiness)
    is present at both stages and is modulated by anxiety in opposite directions:
    anxiety increases first-stage perseveration and decreases second-stage
    perseveration.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety trait score in [0,1]
    - model_parameters: iterable of 5 parameters, all used
        alpha: [0,1] learning rate for stage-2 MF values and stage-1 MF bootstrapping
        beta:  [0,10] inverse temperature for softmax at both stages
        rho_surp0: [0,1] gain for surprise-gated increase in planning weight
        kappa_rep0: [0,1] strength of first-stage perseveration (repeat last a1)
        zeta_pers2: [0,1] strength of second-stage perseveration (repeat last a2 in a state)

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, rho_surp0, kappa_rep0, zeta_pers2 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q1_mf = np.zeros(2)         # model-free first-stage values
    q2_mf = np.zeros((2, 2))    # second-stage MF values (state x action)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)



    w_mb = max(0.0, min(1.0, 1.0 - stai0))

    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    eps = 1e-12
    for t in range(n_trials):

        max_q2 = np.max(q2_mf, axis=1)           # best alien per planet
        q1_mb = T @ max_q2                       # expected value per spaceship

        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            kappa_eff = kappa_rep0 * (1.0 + stai0)
            q1_hybrid = q1_hybrid + kappa_eff * stick

        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s2 = state[t]
        q2 = q2_mf[s2].copy()
        prev_a2 = prev_a2_by_state[s2]
        if prev_a2 is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2] = 1.0

            zeta_eff = zeta_pers2 * (1.0 - 0.5 * stai0)
            q2 = q2 + zeta_eff * stick2

        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1


        p_trans = T[a1, s2]
        surprise = 1.0 - p_trans

        w_mb = w_mb + rho_surp0 * (1.0 + stai0) * (surprise - (w_mb - (1.0 - stai0)))
        w_mb = max(0.0, min(1.0, w_mb))

        prev_a1 = a1
        prev_a2_by_state[s2] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll