def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planner with anxiety-inflated noise after rare transitions and perseveration.

    Overview
    - Stage 2 learns Q-values via Rescorla-Wagner (model-free).
    - Stage 1 uses purely model-based planning using the known transition matrix.
    - Anxious surprise effect: effective inverse temperature decreases after rare transitions,
      scaled by stai and eta_surp. This increases stochasticity when a rare transition occurs.
      Applied to both stages on that trial.
    - Includes a perseveration (stay) bias applied to both stages, parameterized by kappa_pers.

    Parameters (all used)
    - alpha:       [0,1]   Learning rate for stage-2 Q-values.
    - beta_base:   [0,10]  Baseline inverse temperature.
    - eta_surp:    [0,1]   Magnitude by which surprise (rare transition) reduces beta.
    - kappa_pers:  [0,1]   Additive bias to repeat previous action at each stage.
                           Implemented as an additive term in the logits.
    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta_base, eta_surp, kappa_pers].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta_base, eta_surp, kappa_pers = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float) + 0.5

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])


        is_rare = (a1 == 0 and s == 1) or (a1 == 1 and s == 0)

        beta_eff = beta_base * (1.0 - eta_surp * stai * (1.0 if is_rare else 0.0))
        beta_eff = max(1e-6, beta_eff)

        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T_known @ max_q2

        bias1 = np.zeros(2, dtype=float)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_pers
        bias2 = np.zeros(2, dtype=float)
        if prev_a2[s] is not None:
            bias2[int(prev_a2[s])] += kappa_pers

        logits1 = beta_eff * (q1_mb - np.max(q1_mb)) + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        q2_s = q2[s]
        logits2 = beta_eff * (q2_s - np.max(q2_s)) + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)