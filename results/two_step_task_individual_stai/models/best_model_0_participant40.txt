def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based control with learned transitions; anxiety dampens transition learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, tau, kappa1)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - tau in [0,1]: transition learning rate for updating P(state | action).
        - kappa1 in [0,1]: first-stage stickiness strength (bias to repeat previous spaceship).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - The agent learns the transition matrix T (rows = actions, cols = states) online.
    - Anxiety reduces transition learning: tau_eff = tau * (1 - stai).
    - Stage-1 policy is fully model-based using the learned T and current stage-2 values.
    - Stage-2 values are learned via simple TD with learning rate alpha2.
    """
    alpha2, beta, tau, kappa1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))

    prev_a1 = None

    tau_eff = tau * (1.0 - stai_val)

    for t in range(n_trials):

        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = kappa1

        a1 = int(action_1[t])
        logits1 = beta * Q1_MB + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        if tau_eff > 0.0:

            T[a1, :] = (1.0 - tau_eff) * T[a1, :]

            T[a1, s2] += tau_eff

            row_sum = np.sum(T[a1, :])
            if row_sum > 0:
                T[a1, :] /= row_sum

        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll