def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated arbitration and perseveration.
    
    This model combines model-free (MF) and model-based (MB) values at the first stage,
    with an eligibility trace propagating second-stage outcomes back to first-stage values.
    Anxiety (stai) increases the reliance on perseveration and decreases the MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (per planet; 0 or 1) for each trial.
    reward : array-like of float
        Obtained reward on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Participant's anxiety score in [0,1]; here, single-element array with stai[0].
        Higher stai reduces model-based control and increases perseveration.
    model_parameters : list or array of floats
        [alpha, lambda_, beta, w_base, pers_base]
        - alpha in [0,1]: learning rate for both stages.
        - lambda_ in [0,1]: eligibility trace; propagates second-stage TD error to first stage.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w_base in [0,1]: baseline MB/MF mixing weight; anxiety reduces this weight.
        - pers_base in [0,1]: baseline perseveration magnitude; anxiety increases its effect.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, lambda_, beta, w_base, pers_base = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)       # MF values for A/U
    q_stage2_mf = np.zeros((2, 2))  # MF values for second-stage actions in states X/Y

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1  # for perseveration bias


    w_eff_scale = 1.0 - 0.5 * st

    w_eff_base = w_base * w_eff_scale

    pers_eff = pers_base * (0.5 + 0.5 * st)

    for t in range(n_trials):

        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # shape (2,)
        q_stage1_mb = transition_matrix @ max_q_stage2  # shape (2,)

        w_eff = w_eff_base
        q1_combined = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        bias = np.zeros(2)
        if prev_a1 >= 0:
            bias[prev_a1] = pers_eff
        q1_policy_vals = q1_combined + bias

        exp_q1 = np.exp(beta * (q1_policy_vals - np.max(q1_policy_vals)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2_policy_vals = q_stage2_mf[s, :]
        exp_q2 = np.exp(beta * (q2_policy_vals - np.max(q2_policy_vals)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]


        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2


        delta1_boot = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1_boot

        q_stage1_mf[a1] += alpha * lambda_ * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll