def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric model-free SARSA with anxiety-modulated lapse and endogenous eligibility.

    Purely model-free learning with separate learning rates for gains and losses at the
    second stage. A TD(λ) credit assignment to the first stage uses λ = 1 - stai, so higher
    anxiety shortens the eligibility span. An anxiety-modulated lapse (random choice mixing)
    is applied at both stages.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int
        Reached second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state (0/1).
    reward : array-like of float
        Obtained reward per trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher means higher anxiety.
    model_parameters : sequence of floats
        [alpha_gain, alpha_loss, beta1, beta2, lapse_base]
        Bounds:
        - alpha_gain in [0,1]: learning rate when second-stage PE is positive.
        - alpha_loss in [0,1]: learning rate when second-stage PE is negative or zero.
        - beta1 in [0,10]: inverse temperature for first-stage softmax.
        - beta2 in [0,10]: inverse temperature for second-stage softmax.
        - lapse_base in [0,1]: base lapse mixture; effective lapse increases with anxiety:
          lapse_eff = min(0.5, lapse_base * stai).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_g, alpha_l, beta1, beta2, lapse_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    lam = 1.0 - stai
    lam = min(max(lam, 0.0), 1.0)
    lapse = min(0.5, max(0.0, lapse_base * stai))

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):

        pref1 = beta1 * q1
        z1 = np.max(pref1)
        exp1 = np.exp(pref1 - z1)
        soft1 = exp1 / np.sum(exp1)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        pref2 = beta2 * q2[s]
        z2 = np.max(pref2)
        exp2 = np.exp(pref2 - z2)
        soft2 = exp2 / np.sum(exp2)
        probs_2 = (1.0 - lapse) * soft2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        pe2 = r - q2[s, a2]
        alpha2 = alpha_g if pe2 > 0.0 else alpha_l
        q2[s, a2] += alpha2 * pe2

        pe1 = q2[s, a2] - q1[a1]

        q1[a1] += alpha2 * (pe1 + lam * pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll