def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-sensitive model-free learner with anxiety-modulated risk aversion and value decay.

    Idea:
    - Stage-2 maintains exponentially weighted estimates of both mean and second moment for each alien.
      Utility is mean - rho * std, where std is derived from the EW second moment.
    - Stage-1 is purely model-free, bootstrapping toward the realized stage-2 utility.
    - STAI modulates risk aversion rho:
        rho = clip(rho_base + k_anx_rho * stai, 0, 1)
      Higher STAI can increase (or decrease) risk aversion depending on k_anx_rho.
    - A decay parameter shrinks unchosen values toward zero, capturing forgetting/instability.

    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha (0..1), EW update rate for mean and second moment
    - model_parameters[1]: beta (0..10), inverse temperature at both stages
    - model_parameters[2]: rho_base (0..1), baseline risk aversion
    - model_parameters[3]: k_anx_rho (0..1), slope of STAI effect on risk aversion
    - model_parameters[4]: decay (0..1), per-trial decay applied to unchosen values

    Inputs:
    - action_1: array of ints in {0,1}
    - state: array of ints in {0,1}
    - action_2: array of ints in {0,1}
    - reward: array of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of 5 parameters as above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, rho_base, k_anx_rho, decay = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    rho = rho_base + k_anx_rho * stai_val
    if rho < 0.0:
        rho = 0.0
    if rho > 1.0:
        rho = 1.0

    m = np.zeros((2, 2))     # EW mean reward
    m2 = np.zeros((2, 2))    # EW mean of squared reward

    q1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):

        var = np.maximum(m2 - m**2, 0.0)
        std = np.sqrt(var + 1e-12)
        u = m - rho * std  # utility

        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        probs1 = soft1 / (np.sum(soft1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s = int(state[t])
        logits2 = beta * u[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        probs2 = soft2 / (np.sum(soft2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]


        other_a2 = 1 - a2
        m[s, other_a2] = (1.0 - decay) * m[s, other_a2]
        m2[s, other_a2] = (1.0 - decay) * m2[s, other_a2]
        other_state = 1 - s
        m[other_state, 0] = (1.0 - decay) * m[other_state, 0]
        m[other_state, 1] = (1.0 - decay) * m[other_state, 1]
        m2[other_state, 0] = (1.0 - decay) * m2[other_state, 0]
        m2[other_state, 1] = (1.0 - decay) * m2[other_state, 1]

        m[s, a2] = m[s, a2] + alpha * (r - m[s, a2])
        m2[s, a2] = m2[s, a2] + alpha * (r*r - m2[s, a2])

        var_sa = max(m2[s, a2] - m[s, a2]**2, 0.0)
        u_sa = m[s, a2] - rho * np.sqrt(var_sa + 1e-12)


        q1[1 - a1] = (1.0 - decay) * q1[1 - a1]
        delta1 = u_sa - q1[a1]
        q1[a1] = q1[a1] + alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll