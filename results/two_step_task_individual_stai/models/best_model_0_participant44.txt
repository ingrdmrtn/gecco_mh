def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated transition learning, MF/MB arbitration, forgetting, and temperature shift.

    Idea:
    - The agent learns the transition matrix online and uses it for model-based (MB) evaluation.
    - Anxiety increases transition learning rate (hypervigilance) and increases forgetting of Q values.
    - Anxiety decreases inverse temperature (more exploration).
    - First-stage policy arbitrates between MB and model-free (MF) values based on current transition uncertainty and anxiety.
    - MF credit assignment from stage 2 to stage 1 uses an eligibility trace equal to the participant's anxiety (no extra parameter).

    Parameters (with bounds):
    - alpha in [0,1]: learning rate for second-stage Q updates; also used for first-stage MF TD.
    - beta in [0,10]: base inverse temperature.
    - phi_trans in [0,1]: base transition learning rate; anxiety scales it upward.
    - eta_forget in [0,1]: base forgetting rate; anxiety scales it upward (more forgetting under anxiety).
    - xi_anxTemp in [0,1]: strength of anxiety-driven temperature reduction.

    Inputs:
    - action_1: array-like ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like ints in {0,1} for reached planet (0=X, 1=Y).
    - action_2: array-like ints in {0,1} for second-stage choices (0/1 for the two aliens on that planet).
    - reward: array-like floats (typically 0/1).
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, phi_trans, eta_forget, xi_anxTemp].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, phi_trans, eta_forget, xi_anxTemp = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    beta_eff = max(1e-3, beta * (1.0 - xi_anxTemp * stai))  # higher anxiety -> lower beta
    phi_eff = np.clip(phi_trans * (1.0 + 0.5 * stai), 0.0, 1.0)  # higher anxiety -> faster transition learning
    forget_eff = np.clip(eta_forget * stai, 0.0, 1.0)  # higher anxiety -> more forgetting
    lambda_anx = np.clip(stai, 0.0, 1.0)  # eligibility trace equals anxiety

    q1_mf = np.zeros(2)          # MF Q for A/U
    q2 = np.zeros((2, 2))        # Q for aliens on each planet

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):

        max_q2 = np.max(q2, axis=1)  # best value on each planet
        q1_mb = T @ max_q2


        row_ent = []
        for r in range(2):
            p = T[r]

            h = 0.0
            for x in p:
                if x > 0:
                    h -= x * (np.log(x) / np.log(2))
            row_ent.append(h)  # in [0,1] for binary
        ent_mean = 0.5 * (row_ent[0] + row_ent[1])  # 0=certainty, 1=max uncertainty
        w_mb = np.clip((1.0 - stai) * (1.0 - ent_mean), 0.0, 1.0)

        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        logits2 = beta_eff * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]


        q1_mf = (1.0 - forget_eff) * q1_mf
        q2 = (1.0 - forget_eff) * q2

        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        td2 = r - q2[s, a2]
        q2[s, a2] += alpha * td2

        q1_mf[a1] += alpha * lambda_anx * td2


        obs = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = (1.0 - phi_eff) * T[a1, :] + phi_eff * obs

        T[a1, :] = np.clip(T[a1, :], 1e-6, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik