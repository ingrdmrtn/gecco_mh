def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Meta-control via anxiety-gated temperature, MB weight, and reward forgetting.

    Idea:
    - Stage 1 uses a hybrid of MB and MF, with MB weight higher when anxiety is low.
    - Softmax temperature increases when anxiety is low (more exploitation).
    - Stage 2 values decay (forget) over trials; decay grows with anxiety,
      modeling difficulty maintaining stable reward beliefs under high anxiety.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce beta (more randomness), reduce MB weight,
        and increase forgetting of stage-2 values.
    model_parameters : array-like of float
        [alpha, beta_base, forget, mbw0]
        - alpha in [0,1]: learning rate for Q updates.
        - beta_base in [0,10]: baseline inverse temperature.
        - forget in [0,1]: base forgetting factor for stage-2 values.
        - mbw0 in [0,1]: baseline MB weight at stage 1 (boosted when anxiety is low).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta_base, forget, mbw0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))


    w_mb = np.clip(mbw0 + (1.0 - st) * (1.0 - mbw0) * 0.5, 0.0, 1.0)
    beta_eff = np.clip(beta_base * (0.5 + 0.5 * (1.0 - st)), 0.0, 10.0)

    forget_eff = np.clip(forget * (0.5 + 0.5 * st), 0.0, 1.0)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        q2 = (1.0 - forget_eff) * q2 + forget_eff * 0.5

        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]

        alpha1 = alpha * (0.8 + 0.2 * (1.0 - st))
        q1_mf[a1] += alpha1 * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)