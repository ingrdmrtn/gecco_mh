def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-like planning with MF integration, anxiety-modulated arbitration, and memory decay.

    Core idea:
    - Stage 1 values blend a successor-like (transition-based) planner and a model-free cache.
    - Low anxiety increases planning weight and a structure-based stay/shift bias tied to prior common/rare transitions.
    - Soft forgetting (decay) prevents overcommitment to stale values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, omega, zeta, bias_prev]
        - alpha in [0,1]: learning rate for value updates (both stages).
        - beta in [0,10]: inverse temperature for both stages.
        - omega in [0,1]: base weight on transition-based planning at stage 1.
        - zeta in [0,1]: forgetting rate toward zero for unrefreshed Q-values.
        - bias_prev in [0,1]: magnitude of structure-based stay/shift bias at stage 1:
            after a common rewarded trial, bias to repeat; after a rare punished trial, bias to switch.
            Anxiety reduces the impact of this bias.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega, zeta, bias_prev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q1_mf = np.zeros(2)        # model-free cache for stage 1 actions
    q2 = np.zeros((2, 2))      # stage 2 action values per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    omega_eff = np.clip(omega * (1.0 - 0.4 * stai), 0.0, 1.0)
    bias_prev_eff_base = np.clip(bias_prev * (1.0 - stai), 0.0, 1.0)  # lower with anxiety

    a1_prev = None
    s_prev = None
    r_prev = None

    for t in range(n_trials):

        max_q2_by_state = np.max(q2, axis=1)     # best attainable value on each planet
        q1_plan = T @ max_q2_by_state           # expected value of each spaceship

        pref = np.zeros(2)
        if a1_prev is not None:
            was_common = 1 if ((a1_prev == 0 and s_prev == 0) or (a1_prev == 1 and s_prev == 1)) else 0
            sign = 1.0 if r_prev > 0 else (-1.0 if r_prev < 0 else 0.0)

            bias = bias_prev_eff_base * sign * (1.0 if was_common else -1.0)

            pref[a1_prev] += bias
            pref[1 - a1_prev] -= bias

        q1 = omega_eff * q1_plan + (1.0 - omega_eff) * q1_mf + pref

        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        s = int(state[t])
        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        q2 *= (1.0 - zeta)
        q1_mf *= (1.0 - zeta)

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        a1_prev, s_prev, r_prev = a1, s, r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)