def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated surprise-seeking intrinsic motivation.

    Overview:
    - Stage 2: standard TD(0) learning on rewards.
    - Stage 1: mixture of model-based and model-free values.
    - Additionally, an intrinsic "surprise" signal is maintained per first-stage action:
        surprise_t = -log P(observed_state | chosen_ship) under the known transition model.
      A running trace of surprise per action S[a] is updated and added as a bonus to stage-1 values.
    - Trait anxiety scales the strength of the surprise bonus:
        z_eff = clip(z0 + k_z * stai, [0,1]).
      Higher z_eff increases preference for actions that recently produced surprising transitions.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for Q-value updates and surprise trace updates.
    - beta: [0,10] inverse temperature for both stages.
    - w: [0,1] weight on model-based control at stage 1.
    - z0: [0,1] base weight of surprise bonus.
    - k_z: [0,1] how strongly anxiety increases surprise bonus (z_eff = z0 + k_z*stai).

    Inputs:
    - action_1: array-like of ints {0,1}, first-stage choices (0=A, 1=U).
    - state: array-like of ints {0,1}, second-stage states (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage choices.
    - reward: array-like of floats.
    - stai: array-like with a single float in [0,1].
    - model_parameters: tuple/list of 5 params (alpha, beta, w, z0, k_z).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, z0, k_z = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    Q2 = np.zeros((2, 2))   # stage-2
    Q1_mf = np.zeros(2)     # stage-1 MF
    S = np.zeros(2)         # surprise trace per first-stage action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    z_eff = z0 + k_z * stai
    if z_eff < 0.0:
        z_eff = 0.0
    if z_eff > 1.0:
        z_eff = 1.0

    eps = 1e-12

    for t in range(n_trials):

        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2


        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf + z_eff * S

        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1


        p_s_given_a = T[a1, s]

        if p_s_given_a < 1e-8:
            p_s_given_a = 1e-8
        surprise = -np.log(p_s_given_a)

        S[a1] += alpha * (surprise - S[a1])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll