def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Directed exploration via leaky Bayesian counts, scaled by anxiety.
    
    Summary
    -------
    This model encourages sampling uncertain aliens by adding an uncertainty bonus
    to second-stage values, with leaky accumulation of successes and counts to track
    slowly drifting probabilities. Anxiety increases the directed exploration bonus and
    slightly reduces perseveration bias.
    
    Mechanics
    ---------
    - Maintain leaky counts N[s,a] and successes S[s,a].
    - Estimated mean: m = S / (N + tiny); uncertainty: u = 1 / (N + 1).
    - Stage 2 value: Q2 = m + phi(stai) * u, where phi increases with anxiety.
    - Stage 1 value: MB projection of max Q2 via known transitions.
    - Perseveration at stage 1 reduced by anxiety.
    - Policies: softmax with inverse temperature beta at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1]. Higher anxiety increases exploration bonus.
    model_parameters : iterable of floats
        [rho_decay, beta, phi0, pi0]
        - rho_decay: leaky decay factor applied each trial to counts and successes [0,1]
                     (larger means slower forgetting; 0 = full reset, 1 = no decay)
        - beta: inverse temperature for both stages [0,10]
        - phi0: baseline directed exploration bonus weight [0,1]
        - pi0: baseline perseveration bias magnitude at stage 1 [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    rho_decay, beta, phi0, pi0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    N = np.zeros((2, 2)) + 1.0  # start with weak prior count to avoid zero
    S = np.zeros((2, 2)) + 0.5  # weak prior successes for ~0.5 mean

    prev_a1 = None

    phi = phi0 * (0.5 + st)           # higher anxiety -> larger exploration bonus
    persev = pi0 * (1.0 - 0.5 * st)   # higher anxiety -> less perseveration

    for t in range(n_trials):

        N *= rho_decay
        S *= rho_decay

        m = S / (N + 1e-8)
        u = 1.0 / (N + 1.0)

        q2 = m + phi * u

        vmax2 = np.max(q2, axis=1)
        q1 = T @ vmax2

        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += persev

        logits1 = q1 + bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s2 = state[t]
        logits2 = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]
        N[s2, a2] += 1.0
        S[s2, a2] += r

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll