def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free controller with anxiety-modulated forgetting and WSLS bias that depends on reward and transition type.

    Core idea:
    - Pure MF Q-learning at both stages with trial-by-trial forgetting (decay).
    - A first-stage win-stay/lose-shift (WSLS) bias influences action selection:
        - After rewarded-common or unrewarded-rare transitions, bias to repeat is stronger when anxiety is low.
        - After unrewarded-common or rewarded-rare transitions, bias to switch is stronger when anxiety is high.
    - Anxiety also increases forgetting (higher decay when stai is high).

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices: 0=A, 1=U.
    state : array-like of int {0,1}
        Reached second-stage state: 0=X, 1=Y.
    action_2 : array-like of int {0,1}
        Second-stage choices.
    reward : array-like of float
        Rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; higher -> more forgetting and stronger lose-shift after certain outcomes.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, decay, wsls_gain, xi]
        - alpha: [0,1] learning rate for both stages
        - beta: [0,10] inverse temperature (both stages)
        - decay: [0,1] baseline forgetting rate applied each trial to all Q-values
        - wsls_gain: [0,1] magnitude of WSLS bias on first-stage logits
        - xi: [0,1] mixes reward vs transition contributions to the WSLS signal
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, decay, wsls_gain, xi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    q1 = np.zeros(2)        # first-stage MF values
    q2 = np.zeros((2, 2))   # second-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_reward = 0.0
    last_is_common = 0  # 1 common, 0 rare

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        decay_eff = decay * (0.5 + 0.5 * stai_val)
        q1 *= (1.0 - decay_eff)
        q2 *= (1.0 - decay_eff)

        bias1 = np.zeros(2)
        if last_a1 is not None:


            reward_term = (2.0 * np.clip(last_reward, 0.0, 1.0)) - 1.0

            trans_term = 1.0 if last_is_common == 1 else -1.0




            base_signal = xi * reward_term + (1.0 - xi) * trans_term
            anxiety_gain = (0.5 + 0.5 * stai_val)  # scales switching on adverse signals
            signed_signal = base_signal * (1.0 - stai_val) - reward_term * (anxiety_gain - (1.0 - stai_val))

            bias_strength = wsls_gain * signed_signal

            bias1[last_a1] += bias_strength

        logits1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        last_a1 = a1
        last_reward = r
        last_is_common = is_common

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))