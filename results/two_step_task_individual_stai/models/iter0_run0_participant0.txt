def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    
    This model blends model-based (MB) and model-free (MF) control at stage 1 with
    an arbitration weight that decreases with anxiety (higher STAI => more MF).
    Stage-2 values are learned via TD; Stage-1 MF values use a two-step TD with
    an eligibility trace. One softmax (beta) is used at both stages.
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for both stages
    - lambda_e: [0,1] eligibility trace weighting for propagating stage-2 RPE to stage-1 MF
    - w_base: [0,1] baseline MB weight (before anxiety)
    - kappa: [0,1] strength of anxiety effect on MB weight (w_eff = w_base * (1 - kappa*stai))
    - beta: [0,10] inverse temperature for softmax at both stages
    
    Inputs:
    - action_1: array of length T with observed first-stage actions (0 or 1)
    - state: array of length T with observed second-stage states (0: X, 1: Y)
    - action_2: array of length T with observed second-stage actions (0 or 1)
    - reward: array of length T with received rewards (e.g., 0/1 or [0,1])
    - stai: array-like with one element in [0,1]; higher => higher anxiety
    - model_parameters: array-like [alpha, lambda_e, w_base, kappa, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, lambda_e, w_base, kappa, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Transition matrix: rows = action at stage 1 (A=0, U=1), cols = state (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated arbitration weight (more anxiety => lower MB weight)
    w_eff = np.clip(w_base * (1.0 - kappa * stai_score), 0.0, 1.0)

    # Action value containers
    q_stage1_mf = np.zeros(2)          # MF values for stage-1 actions
    q_stage2 = np.zeros((2, 2))        # Q[state, action] for stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based value for stage-1: expected max over states given transitions
        max_q_stage2 = np.max(q_stage2, axis=1)            # max over actions, for each state
        q_stage1_mb = transition_matrix @ max_q_stage2     # expected value per stage-1 action

        # Hybrid stage-1 value
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Policy stage-1
        logits1 = beta * (q1 - np.max(q1))
        exp1 = np.exp(logits1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Policy stage-2 (conditioned on observed state)
        s = state[t]
        q2_s = q_stage2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        exp2 = np.exp(logits2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # TD updates
        # Stage-2 TD error
        r = reward[t]
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 MF update: bootstrap to stage-2 chosen value + eligibility trace for delta2
        bootstrap = q_stage2[s, a2]
        q_stage1_mf[a1] += alpha * (bootstrap - q_stage1_mf[a1]) + alpha * lambda_e * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid with anxiety-modulated perseveration and transition belief distortion.
    
    This model combines MB and MF control (fixed 50/50) at stage 1, adds a
    perseveration (choice stickiness) bonus on stage-1 choices that is
    strengthened or weakened by anxiety, and distorts the perceived transition
    structure as a function of anxiety (altering MB planning).
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for both stages
    - beta: [0,10] inverse temperature for both stages
    - b0: [0,1] baseline perseveration strength added to last chosen stage-1 action
    - eta: [0,1] strength of anxiety effect on perseveration (b_eff = b0*(1 + eta*(2*stai-1)))
    - xi: [0,1] strength of anxiety-driven distortion of transition certainty
           (effective exponent gamma = 1 + xi*(2*stai-1))
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, beta, b0, eta, xi]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, beta, b0, eta, xi = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Initialize values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Anxiety-modulated perseveration and transition distortion
    b_eff = b0 * (1.0 + eta * (2.0 * stai_score - 1.0))     # can be < b0 if stai < 0.5
    gamma = 1.0 + xi * (2.0 * stai_score - 1.0)             # >1 => sharpen common, <1 => flatten
    # Distorted perceived transition probabilities
    p_common = (0.7 ** gamma) / ((0.7 ** gamma) + (0.3 ** gamma))
    transition_matrix = np.array([[p_common, 1 - p_common],
                                  [1 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None  # for perseveration

    for t in range(n_trials):
        # Model-based plan from distorted transitions
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF equally (fixed hybrid without extra parameter)
        q1 = 0.5 * q_stage1_mb + 0.5 * q_stage1_mf

        # Add perseveration bonus to the last chosen stage-1 action
        if last_a1 is not None:
            bonus = np.zeros(2)
            bonus[last_a1] = b_eff
            q1 = q1 + bonus

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax over current state's Q)
        s = state[t]
        logits2 = beta * (q_stage2[s] - np.max(q_stage2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # Stage-2 TD
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2
        # Stage-1 MF bootstrap toward the new stage-2 value
        q_stage1_mf[a1] += alpha * (q_stage2[s, a2] - q_stage1_mf[a1])

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric learning and anxiety-shifted arbitration with stage-2 stickiness.
    
    This model uses separate learning rates for positive vs. negative outcomes at
    stage 2, propagates value to stage 1 (MF), and arbitrates at stage 1 using a
    weight that decreases with anxiety (higher STAI => more MF). It also adds a
    stage-2 action stickiness that increases with anxiety, capturing habit-like
    tendencies under stress.
    
    Parameters (bounds):
    - alpha_pos: [0,1] learning rate when outcome > current Q (positive PE)
    - alpha_neg: [0,1] learning rate when outcome < current Q (negative PE)
    - w_base: [0,1] baseline MB arbitration weight at stage 1
    - stick2: [0,1] baseline stage-2 stickiness bonus for repeating last action in a state
    - beta: [0,10] inverse temperature for both stages
    
    Anxiety usage:
    - Arbitration: w_eff = clip(w_base * (1 - stai), 0, 1)
    - Stage-2 stickiness: stick_eff = stick2 * (1 + 0.5*(2*stai-1))  # stronger with higher anxiety
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha_pos, alpha_neg, w_base, stick2, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha_pos, alpha_neg, w_base, stick2, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Transition matrix (true, used for MB plan)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated arbitration and stage-2 stickiness
    w_eff = np.clip(w_base * (1.0 - stai_score), 0.0, 1.0)
    stick_eff = stick2 * (1.0 + 0.5 * (2.0 * stai_score - 1.0))  # ranges ~[0.5*stick2, 1.5*stick2]

    # Value tables
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # For stage-2 stickiness: last action taken in each state
    last_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB plan for stage-1
        max_q_stage2 = np.max(q_stage2, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid arbitration
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness bonus within the current state
        s = state[t]
        q2_s = q_stage2[s].copy()
        if last_a2[s] != -1:
            q2_s[last_a2[s]] += stick_eff

        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates with asymmetric rates
        r = reward[t]
        pe2 = r - q_stage2[s, a2]
        alpha = alpha_pos if pe2 >= 0 else alpha_neg
        q_stage2[s, a2] += alpha * pe2

        # Propagate to stage-1 MF (simple bootstrap)
        q_stage1_mf[a1] += alpha * (q_stage2[s, a2] - q_stage1_mf[a1])

        # Update stickiness memory for this state
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll