def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration and eligibility trace.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., alien indices W/S on X, P/H on Y).
    reward : array-like of float (0 or 1)
        Received coins on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; used to modulate arbitration toward model-free control.
    model_parameters : list or array
        [alpha, beta, w0, lambda_e, gamma_anx]
        - alpha in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w0 in [0,1]: baseline weight on model-based control at stage 1.
        - lambda_e in [0,1]: eligibility trace for propagating second-stage PE to first-stage MF.
        - gamma_anx in [0,1]: strength of anxiety-driven shift toward model-free control.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, w0, lambda_e, gamma_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Action value tables
    q_stage1_mf = np.zeros(2)               # model-free Q at stage 1
    q_stage2_mf = np.zeros((2, 2))          # model-free Q at stage 2 (state x action)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration: higher anxiety -> more MF (lower w)
    w = w0 * (1.0 - gamma_anx * stai_val)
    w = max(0.0, min(1.0, w))

    for t in range(n_trials):
        # Model-based Q at stage 1: Q_MB = T * max_a Q2(s', a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid Q at stage 1
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Softmax policy at stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy for realized state
        s2 = state[t]
        q2 = q_stage2_mf[s2]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD errors
        delta2 = r - q_stage2_mf[s2, a2]
        delta1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]  # bootstrap via observed state-action value

        # Updates
        q_stage2_mf[s2, a2] += alpha * delta2
        # Eligibility trace to propagate second-stage PE to first-stage MF
        q_stage1_mf[a1] += alpha * (delta1 + lambda_e * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive learning with anxiety-amplified volatility sensitivity and transition-based choice kernel.
    
    Core ideas:
    - Learning rate adapts with unsigned prediction error; anxiety amplifies this adaptation.
    - A transition-based choice kernel biases first-stage choice depending on whether the last transition was common or rare,
      with strength increasing with anxiety (capturing heightened sensitivity to surprising/rare events).
    - Perseveration at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float (0 or 1)
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; modulates volatility sensitivity and transition kernel.
    model_parameters : list or array
        [alpha0, beta, k_vol, kappa, rho]
        - alpha0 in [0,1]: base learning rate (both stages).
        - beta in [0,10]: inverse temperature (both stages).
        - k_vol in [0,1]: scale of PE-dependent learning rate adaptation (amplified by stai).
        - kappa in [0,1]: perseveration strength added to the chosen action's logit on next trial (both stages).
        - rho in [0,1]: transition-based choice kernel strength at stage 1, amplified by stai.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha0, beta, k_vol, kappa, rho = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Transition structure for common/rare detection
    # common if (A->X) or (U->Y)
    def is_common_transition(a, s):
        return (a == 0 and s == 0) or (a == 1 and s == 1)

    q1 = np.zeros(2)            # stage-1 MF Q
    q2 = np.zeros((2, 2))       # stage-2 MF Q

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Choice kernels for perseveration
    prev_a1 = None
    prev_a2 = [None, None]  # per-state perseveration for stage 2
    # Transition-based kernel memory
    last_common = 0  # +1 for common, -1 for rare, 0 for undefined first trial
    last_a1 = None

    for t in range(n_trials):
        # Construct logits for stage 1 with perseveration and transition kernel
        logits1 = q1.copy()

        # Perseveration bias: add kappa to the previously chosen action's logit
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] = kappa
            logits1 = logits1 + bias

        # Transition-based kernel: if last transition was rare, bias away from last chosen action;
        # if common, bias toward it. Strength scaled by anxiety and rho.
        if last_a1 is not None and last_common != 0:
            tbias = np.zeros(2)
            direction = 1.0 if last_common > 0 else -1.0
            tbias[last_a1] = rho * stai_val * direction
            logits1 = logits1 + tbias

        # Softmax stage 1
        exp1 = np.exp(beta * (logits1 - np.max(logits1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with per-state perseveration
        s2 = state[t]
        logits2 = q2[s2].copy()
        if prev_a2[s2] is not None:
            pbias = np.zeros(2)
            pbias[prev_a2[s2]] = kappa
            logits2 = logits2 + pbias

        exp2 = np.exp(beta * (logits2 - np.max(logits2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning rate adaptation via unsigned PE, amplified by anxiety
        delta2 = r - q2[s2, a2]
        alpha_t2 = alpha0 + k_vol * stai_val * abs(delta2)
        alpha_t2 = max(0.0, min(1.0, alpha_t2))
        q2[s2, a2] += alpha_t2 * delta2

        # Bootstrap update for stage-1 Q toward the value realized at stage 2
        v2 = np.max(q2[s2])  # value of reached state
        delta1 = v2 - q1[a1]
        alpha_t1 = alpha0 + 0.5 * k_vol * stai_val * abs(delta1)
        alpha_t1 = max(0.0, min(1.0, alpha_t1))
        q1[a1] += alpha_t1 * delta1

        # Update kernels memory
        common_now = 1 if is_common_transition(a1, s2) else -1
        last_common = common_now
        last_a1 = a1
        prev_a1 = a1
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric learning with anxiety-modulated asymmetry and value decay toward uncertainty.
    
    Core ideas:
    - Separate learning rates for positive vs. negative prediction errors emerge from a base alpha and an
      anxiety-scaled asymmetry parameter: higher anxiety -> stronger learning from negative outcomes.
    - Q-values decay toward 0.5 each trial (forgetting/uncertainty), controlling sensitivity to slow drifts.
    - Perseveration bias at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float (0 or 1)
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; modulates valence asymmetry of learning and effective exploration.
    model_parameters : list or array
        [alpha, beta, asym, phi, kappa]
        - alpha in [0,1]: base learning rate.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - asym in [0,1]: strength of valence asymmetry (scaled by stai).
                         Positive PEs use alpha_pos = alpha*(1 - asym*stai),
                         Negative PEs use alpha_neg = alpha*(1 + asym*stai), both clipped to [0,1].
        - phi in [0,1]: decay rate toward 0.5 after each update (both stages).
        - kappa in [0,1]: perseveration strength added to chosen action's logit on next decision (both stages).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, asym, phi, kappa = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Transition matrix for MB estimate at stage 1
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Q-tables
    q1_mf = np.zeros(2)           # stage-1 MF Q
    q2_mf = np.zeros((2, 2))      # stage-2 MF Q

    # Perseveration memory
    prev_a1 = None
    prev_a2 = [None, None]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute a simple MB estimate to inform stage-1 policy (via blending with MF)
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2

        # Blend MB and MF with an anxiety-implied exploration shift via decay (handled in updates),
        # here we simply average MB and MF before softmax, then include perseveration.
        q1_blend = 0.5 * (q1_mb + q1_mf)

        logits1 = q1_blend.copy()
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] = kappa
            logits1 = logits1 + bias

        exp1 = np.exp(beta * (logits1 - np.max(logits1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s2 = state[t]
        logits2 = q2_mf[s2].copy()
        if prev_a2[s2] is not None:
            pbias = np.zeros(2)
            pbias[prev_a2[s2]] = kappa
            logits2 = logits2 + pbias

        exp2 = np.exp(beta * (logits2 - np.max(logits2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Valence-asymmetric learning rates modulated by anxiety
        def lr_from_delta(base_alpha, asym_, stai_, delta):
            alpha_pos = base_alpha * (1.0 - asym_ * stai_)
            alpha_neg = base_alpha * (1.0 + asym_ * stai_)
            alpha_pos = max(0.0, min(1.0, alpha_pos))
            alpha_neg = max(0.0, min(1.0, alpha_neg))
            return alpha_pos if delta >= 0.0 else alpha_neg

        # Stage-2 update
        delta2 = r - q2_mf[s2, a2]
        alpha2 = lr_from_delta(alpha, asym, stai_val, delta2)
        q2_mf[s2, a2] += alpha2 * delta2
        # Decay all second-stage Qs toward 0.5 (uncertainty) after each trial
        q2_mf = (1.0 - phi) * q2_mf + phi * 0.5

        # Stage-1 bootstrap and update
        v2 = np.max(q2_mf[s2])
        delta1 = v2 - q1_mf[a1]
        alpha1 = lr_from_delta(alpha, asym, stai_val, delta1)
        q1_mf[a1] += alpha1 * delta1
        # Decay stage-1 Qs toward 0.5
        q1_mf = (1.0 - phi) * q1_mf + phi * 0.5

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)