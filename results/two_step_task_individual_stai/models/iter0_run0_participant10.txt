def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration and eligibility.
    
    This model blends model-based (MB) and model-free (MF) action values at stage 1.
    The MB weight is a function of the participant's anxiety (stai), allowing
    more anxious participants to shift arbitration. An eligibility trace from
    stage 2 reward to stage 1 MF values is also scaled by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1 for the planet-specific aliens).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0]. Higher means more anxious.
    model_parameters : array-like of float
        [alpha, beta, w_base, w_sens]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - w_base in [0,1]: baseline MB weight at stai=0.
        - w_sens in [0,1]: sensitivity of MB weight to anxiety (signed effect via (0.5 - stai)).
          Effective MB weight = clip(w_base + w_sens * (0.5 - stai), 0, 1).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha, beta, w_base, w_sens = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Anxiety-modulated arbitration and eligibility
    w_mb = w_base + w_sens * (0.5 - st)  # higher st reduces MB if w_sens>0
    w_mb = np.clip(w_mb, 0.0, 1.0)
    # Eligibility increases with anxiety: more anxious -> stronger credit assignment to stage 1
    lam = 0.3 + 0.7 * np.clip(st, 0.0, 1.0)  # in [0.3, 1.0]

    for t in range(n_trials):
        # Compute MB Q at stage 1 from current MF stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # value of each planet
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value of each spaceship

        # Hybrid Q at stage 1
        q1 = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # Policy and likelihood at stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Policy and likelihood at stage 2, conditional on observed state
        s = int(state[t])
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility: towards both the stage-2 value and final reward
        # First bootstrapped update toward Q2
        delta1_boot = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1_boot
        # Then eligibility update from final reward
        q_stage1_mf[a1] += alpha * lam * (r - q_stage1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based policy with reward-asymmetric learning and anxiety-modulated perseveration.
    
    This model uses a model-based first-stage policy (planning via the transition matrix),
    while second-stage values are learned model-free with separate learning rates for
    rewarded vs. unrewarded outcomes. Anxiety increases the tendency to perseverate
    (repeat previous actions) at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = A, 1 = U).
    state : array-like of int
        Second-stage state per trial (0 = X, 1 = Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0]. Higher means more anxious.
    model_parameters : array-like of float
        [alpha_win, alpha_loss, beta, stick_base]
        - alpha_win in [0,1]: learning rate when reward == 1.
        - alpha_loss in [0,1]: learning rate when reward == 0.
        - beta in [0,10]: inverse temperature.
        - stick_base in [0,1]: baseline perseveration weight.
          Effective stickiness = stick_base * (0.5 + stai/2), increasing with anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha_win, alpha_loss, beta, stick_base = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 MF values
    q_stage2 = np.zeros((2, 2))

    # Perseveration terms (initialized neutral)
    prev_a1 = None
    prev_a2 = {0: None, 1: None}

    # Anxiety-modulated stickiness
    stick = stick_base * (0.5 + 0.5 * np.clip(st, 0.0, 1.0))  # increases with stai

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]
        alpha_t = alpha_win if r >= 0.5 else alpha_loss

        # MB evaluation at stage 1 from current Q(s2)
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        # Stage-1 policy and likelihood
        logits1 = beta * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration within each planet
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick

        q2_s = q_stage2[s]
        logits2 = beta * q2_s + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates at stage 2
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_t * delta2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive learning with anxiety-modulated volatility and MB/MF mixing.
    
    This model adapts its learning rate on each trial based on a simple
    volatility proxy: the absolute prediction error. Anxiety increases the
    assumed volatility, leading to larger adaptive learning rates. First-stage
    choices are a hybrid of model-based and model-free values with a fixed
    mixing weight. Second-stage values are learned model-free with adaptive alpha.
    
    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = A, 1 = U).
    state : array-like of int
        Second-stage state per trial (0 = X, 1 = Y).
    action_2 : array-like of int
        Second-stage choices per trial (0/1).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0]. Higher means more anxious.
    model_parameters : array-like of float
        [base_alpha, beta, vol_base, vol_sens, w_mb]
        - base_alpha in [0,1]: minimum learning rate.
        - beta in [0,10]: inverse temperature for both stages.
        - vol_base in [0,1]: baseline gain on PE for adaptive alpha.
        - vol_sens in [0,1]: how much anxiety increases that gain.
          Effective volatility gain g = vol_base + vol_sens * stai.
        - w_mb in [0,1]: weight on model-based values at stage 1 (1-w_mb = MF weight).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    base_alpha, beta, vol_base, vol_sens, w_mb = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Effective volatility gain
    g = vol_base + vol_sens * np.clip(st, 0.0, 1.0)
    g = np.clip(g, 0.0, 1.0)  # keep within [0,1] to respect overall bounds

    # Eligibility fixed modest for stability
    lam = 0.5 + 0.3 * np.clip(st, 0.0, 1.0)

    for t in range(n_trials):
        # MB estimate from current MF stage-2 values
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid Q at stage 1
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2_s = q2_mf[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observed outcome
        r = reward[t]

        # Compute PE and adaptive learning rate
        pe2 = r - q2_mf[s, a2]
        alpha_t = base_alpha + g * np.abs(pe2)
        alpha_t = np.clip(alpha_t, 0.0, 1.0)

        # Update stage-2
        q2_mf[s, a2] += alpha_t * pe2

        # Update stage-1 MF via bootstrapping and eligibility to outcome
        # Bootstrapped target from updated Q2
        pe1_boot = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_t * pe1_boot
        # Eligibility to final reward
        q1_mf[a1] += alpha_t * lam * (r - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll