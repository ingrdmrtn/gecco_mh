def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated arbitration and stickiness.
    
    This model combines model-free (MF) and model-based (MB) action values at stage 1.
    Anxiety reduces the MB arbitration weight and increases perseveration.
    Stage 2 values are learned via TD(0), and MF stage-1 values are updated with an eligibility trace.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed choices at stage 1 (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Observed choices at stage 2 (0 or 1; alien index within visited planet).
    reward : array-like of float
        Reward received on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Trait anxiety score scaled to [0,1]; we use stai[0] as a scalar.
    model_parameters : array-like of float
        Parameters with bounds:
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w0 in [0,1]: baseline MB arbitration weight.
        - lam in [0,1]: eligibility trace for MF credit from stage 2 to stage 1.
        - kappa in [0,1]: base perseveration strength (converted to bias term).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """
    alpha, beta, w0, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # rows: action1 (A,U), cols: next state (X,Y)

    # Q-values
    q1_mf = np.zeros(2)          # MF values for stage-1 actions A,U
    q2 = np.zeros((2, 2))        # Q2[state, action2]

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration traces (last choices), implemented as bias terms
    prev_a1 = 0
    prev_a2 = np.zeros(2, dtype=int)  # per-state last action2

    # Anxiety modulation:
    # - Reduce MB weight with anxiety (e.g., higher anxiety shifts toward MF control)
    w_eff = np.clip(w0 * (1.0 - 0.5 * stai_val), 0.0, 1.0)
    # - Increase stickiness with anxiety
    kappa_eff = kappa * (1.0 + stai_val)

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values from current stage-2 values
        max_q2 = np.max(q2, axis=1)        # value of each state
        q1_mb = T @ max_q2                 # MB value for A,U

        # Hybrid value with perseveration bias
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        bias1 = np.zeros(2)
        bias1[prev_a1] += kappa_eff
        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with state-dependent perseveration
        bias2 = np.zeros(2)
        bias2[prev_a2[s2]] += kappa_eff
        logits2 = beta * q2[s2] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning: Stage 2 TD(0)
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # Eligibility trace from stage 2 to MF stage 1
        target1 = (1.0 - lam) * q1_mf[a1] + lam * q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update perseveration traces
        prev_a1 = a1
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric learning with anxiety-driven exploration bonus for surprising transitions.
    
    Stage-2 values are learned with separate learning rates for positive and negative PEs.
    Stage-1 uses a fixed MB/MF mixture with an additional exploration bonus that is
    triggered by rare transitions; the bonus is scaled by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed stage-1 choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Observed stage-2 choices (alien index).
    reward : array-like of float
        Received reward on each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; we use stai[0].
    model_parameters : array-like of float
        Parameters with bounds:
        - alpha_pos in [0,1]: learning rate for positive PE at stage 2.
        - alpha_neg in [0,1]: learning rate for negative PE at stage 2.
        - beta in [0,10]: inverse temperature for both stages.
        - omega in [0,1]: weight on model-based value at stage 1 (1=fully MB).
        - xi in [0,1]: base coefficient for exploration bonus at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, omega, xi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure
    # A commonly->X, U commonly->Y; rare otherwise
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Simple perseveration at stage 1 to stabilize choices (implicitly handled by MF value carry-over)
    prev_a1 = 0

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Identify whether observed transition was rare (surprise bonus)
        # Mapping: common if (a1==0 and s2==0) or (a1==1 and s2==1)
        is_rare = 1.0 if ((a1 == 0 and s2 == 1) or (a1 == 1 and s2 == 0)) else 0.0

        # Model-based values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Anxiety-driven exploration bonus for rare transitions
        # Larger anxiety => larger bonus weight on both actions after rare transition,
        # with direction favoring the action that would more likely reach the surprising state
        bonus = np.zeros(2)
        if is_rare > 0.5:
            # Reward the chosen action's alternative to encourage exploration after surprise
            # and inject curiosity proportional to anxiety
            bonus[1 - a1] += xi * stai_val

        # Hybrid value
        q1_hybrid = omega * q1_mb + (1.0 - omega) * q1_mf + bonus

        # Stage-1 choice
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 choice
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Stage-2 value update with valence asymmetry
        pe2 = r - q2[s2, a2]
        alpha = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s2, a2] += alpha * pe2

        # Propagate MF credit to stage 1 (simple SARSA-like backup)
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        # Use averaged learning rate to keep both alphas meaningful
        alpha1 = 0.5 * (alpha_pos + alpha_neg)
        q1_mf[a1] += alpha1 * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning MB control with anxiety-weighted uncertainty aversion.
    
    This model learns the transition probabilities online and uses them to compute
    model-based values at stage 1. It blends MB and MF control with an anxiety-dependent
    arbitration (no extra parameter), and penalizes actions with high transition uncertainty
    proportionally to anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed stage-1 choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed stage-2 states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Observed stage-2 choices.
    reward : array-like of float
        Rewards received.
    stai : array-like of float
        Trait anxiety score in [0,1]; we use stai[0].
    model_parameters : array-like of float
        Parameters with bounds:
        - alpha_r in [0,1]: learning rate for stage-2 rewards (Q2).
        - alpha_t in [0,1]: learning rate for transition probabilities.
        - beta in [0,10]: inverse temperature for both stages.
        - rho in [0,1]: perseveration strength (converted to bias) at both stages.
        - eta in [0,1]: scaling of uncertainty aversion term at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_t, beta, rho, eta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix T_hat with weak prior toward common transitions
    T_hat = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration (previous choices)
    prev_a1 = 0
    prev_a2 = np.zeros(2, dtype=int)

    # Arbitration weight as a function of anxiety (higher anxiety -> more MF)
    w_mb = np.clip(1.0 - stai_val, 0.0, 1.0)

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute model-based values using learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2

        # Transition uncertainty penalty per action (entropy of each row of T_hat)
        # H(p) = -sum p log p (use small epsilon for stability)
        epsH = 1e-12
        ent = -(T_hat * (np.log(T_hat + epsH))).sum(axis=1)  # entropy of each action's transition
        unc_penalty = eta * stai_val * ent  # anxiety-weighted uncertainty aversion

        # Stage-1 values with arbitration, perseveration, and uncertainty penalty
        q1_comb = w_mb * q1_mb + (1.0 - w_mb) * q1_mf - unc_penalty
        bias1 = np.zeros(2)
        bias1[prev_a1] += rho
        logits1 = beta * q1_comb + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with perseveration
        bias2 = np.zeros(2)
        bias2[prev_a2[s2]] += rho
        logits2 = beta * q2[s2] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Outcome learning at stage 2
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_r * pe2

        # MF backup to stage 1 (simple copy of stage-2 value)
        pe1 = q2[s2, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

        # Learn transitions online: update only the row corresponding to chosen a1
        # Move T_hat[a1] toward the one-hot of observed s2
        oh = np.array([1.0 if s2 == 0 else 0.0, 1.0 if s2 == 1 else 0.0])
        T_hat[a1] = (1.0 - alpha_t) * T_hat[a1] + alpha_t * oh
        # Keep rows normalized and bounded
        T_hat[a1] = np.clip(T_hat[a1], 1e-6, 1.0)
        T_hat[a1] /= T_hat[a1].sum()

        # Update perseveration traces
        prev_a1 = a1
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll