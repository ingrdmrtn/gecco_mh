def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated arbitration and eligibility traces.
    
    This model blends model-based (MB) and model-free (MF) first-stage values with an arbitration
    weight that depends on the participant's anxiety (stai). Higher STAI can increase or decrease
    the MB weight depending on parameter k_anx. Stage-2 values are learned model-free; stage-1
    MF values receive an eligibility-trace update from the stage-2 prediction error.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien on the planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate MB/MF arbitration.
    model_parameters : list or array
        [alpha, beta, lambd, w0, k_anx]
        Bounds:
          alpha in [0,1]     : learning rate for MF updates
          beta in [0,10]     : inverse temperature for softmax
          lambd in [0,1]     : eligibility trace from stage 2 to stage 1
          w0 in [0,1]        : baseline weight on MB control
          k_anx in [0,1]     : how strongly STAI shifts the MB weight (mapped to a signed slope)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, lambd, w0, k_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])  # rows: A,U; cols: X,Y

    # Value functions
    q_stage1_mf = np.zeros(2)         # MF first-stage Q-values for actions A,U
    q_stage2_mf = np.zeros((2, 2))    # MF second-stage Q-values for states X,Y and their two aliens

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper transforms
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    # Anxiety-modulated MB weight
    # Map k_anx in [0,1] to a signed slope in [-4,4] to allow either direction of modulation
    k_eff = 8.0 * (k_anx - 0.5)
    base_logit = logit(w0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute MB first-stage action values from current second-stage values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)           # size 2 for states X,Y
        q_stage1_mb = transition_matrix @ max_q_stage2        # size 2 for actions A,U

        # Arbitration weight as a function of STAI (trial-invariant here)
        w_mb = sigmoid(base_logit + k_eff * (stai - 0.5))
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # First-stage decision policy: softmax over blended MB/MF Q-values
        q1_blend = (1.0 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb
        q1_centered = q1_blend - np.max(q1_blend)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Second-stage decision policy: softmax over MF Q-values for current state
        q2_s = q_stage2_mf[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        # Stage-2 TD error and update
        delta2 = reward[t] - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Eligibility-trace update to stage-1 MF for the chosen action
        q_stage1_mf[a1] += alpha * lambd * delta2

        # Optional small value decay towards zero for unchosen second-stage action (stability)
        # Not required; omitted to keep parameters within the specified set.

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-modulated asymmetric learning rates and perseveration.
    
    This purely model-free controller learns second-stage values and backs them up to first-stage
    values via an eligibility trace. Anxiety (stai) creates asymmetry between positive and negative
    prediction-error learning rates and scales a perseveration (choice stickiness) bias at stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate learning-rate asymmetry and perseveration.
    model_parameters : list or array
        [alpha_base, beta, pi_base, stai_weight, lambd]
        Bounds:
          alpha_base in [0,1]  : base learning rate
          beta in [0,10]       : inverse temperature
          pi_base in [0,1]     : baseline perseveration strength
          stai_weight in [0,1] : sensitivity of parameters to STAI (0=no effect, 1=max effect)
          lambd in [0,1]       : eligibility trace from stage 2 to stage 1
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_base, beta, pi_base, stai_weight, lambd = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Value functions
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory (stage 1)
    prev_a1 = None

    # Helper to compute anxiety-modulated alphas and pi
    # Map stai and stai_weight to a signed modulation in [-1,1]
    mod = (2.0 * stai - 1.0) * (2.0 * stai_weight - 1.0)
    # Learning-rate asymmetry: alpha_pos increases with mod, alpha_neg decreases with mod
    alpha_pos = np.clip(alpha_base * (1.0 + mod), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 - mod), 0.0, 1.0)
    # Perseveration scaling: stronger stickiness with positive mod, weaker with negative
    pi_eff = pi_base * (1.0 + mod)  # can vary approximately in [0, 2*pi_base]

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage-1 decision with perseveration bias
        q1 = q_stage1_mf.copy()
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            q1 = q1 + pi_eff * stick

        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 decision
        q2_s = q_stage2_mf[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Learning
        # Stage-2 TD error
        pe2 = reward[t] - q_stage2_mf[s, a2]
        if pe2 >= 0:
            a2_lr = alpha_pos
        else:
            a2_lr = alpha_neg
        q_stage2_mf[s, a2] += a2_lr * pe2

        # Back up to stage-1 MF via eligibility trace (use same sign-dependent lr)
        q_stage1_mf[a1] += a2_lr * lambd * pe2

        # Additionally, a direct stage-1 TD towards the active second-stage value (SARSA(0) style)
        td1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # Use base alpha here, modestly modulated by anxiety via stai_weight
        alpha1 = np.clip(alpha_base * (1.0 + 0.5 * mod), 0.0, 1.0)
        q_stage1_mf[a1] += alpha1 * td1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-sensitive hybrid with anxiety-modulated arbitration based on previous transition type.
    
    First-stage choices arise from a hybrid of model-based (MB) and model-free (MF) values, but the
    arbitration weight changes trial-by-trial depending on whether the previous transition was common
    or rare. Anxiety (stai) modulates how strongly rare vs. common previous transitions shift the
    arbitration weight. Second-stage values are learned MF; MF first-stage values receive eligibility
    updates from stage-2 prediction errors.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate sensitivity to rare vs. common transitions.
    model_parameters : list or array
        [alpha, beta, w_mb_base, rare_sens, lambd]
        Bounds:
          alpha in [0,1]       : learning rate
          beta in [0,10]       : inverse temperature
          w_mb_base in [0,1]   : baseline MB weight
          rare_sens in [0,1]   : sensitivity to previous transition type (mapped to signed slope)
          lambd in [0,1]       : eligibility trace from stage 2 to stage 1
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w_mb_base, rare_sens, lambd = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])  # rows: A,U; cols: X,Y

    # Values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper transforms
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    base_logit = logit(w_mb_base)
    # Map rare_sens in [0,1] to signed slope in [-4,4]
    k_sens = 8.0 * (rare_sens - 0.5)

    # Track previous transition type
    prev_is_common = None  # True/False based on previous trial transition, None on first

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute model-based Q at stage 1 from current MF Q2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Determine arbitration weight based on previous transition type and stai
        # If previous transition was rare, direction = +1; if common, direction = -1; if None, 0
        if prev_is_common is None:
            direction = 0.0
        else:
            direction = +1.0 if (not prev_is_common) else -1.0

        # Anxiety scales the impact of transition type on arbitration
        w_mb = sigmoid(base_logit + k_sens * direction * (stai - 0.5))
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # First-stage policy: softmax over blended values
        q1_blend = (1.0 - w_mb) * q_stage1_mf + w_mb * q_stage1_mb
        q1_centered = q1_blend - np.max(q1_blend)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy: softmax over current state's Q2
        q2_s = q_stage2_mf[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        delta2 = reward[t] - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2
        q_stage1_mf[a1] += alpha * lambd * delta2

        # Update previous transition type using current trial's experienced transition
        # Common if (A->X) or (U->Y)
        prev_is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll