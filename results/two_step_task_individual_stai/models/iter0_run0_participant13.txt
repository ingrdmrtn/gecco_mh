def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free SARSA(Î») with anxiety-modulated arbitration and perseveration.
    
    This model learns second-stage Q-values (per planet/alien) and a model-free first-stage value.
    First-stage choices are driven by a convex combination of model-based and model-free values,
    where the arbitration weight is modulated by the participant's anxiety (STAI). A perseveration
    bias at the first stage is also scaled by anxiety.

    Parameters (model_parameters):
    - alpha in [0,1]: Learning rate for value updates at both stages.
    - beta in [0,10]: Inverse temperature for softmax choice at both stages.
    - lam in [0,1]: Eligibility trace strength propagating second-stage RPE to first stage.
    - w_mb in [0,1]: Baseline weight for model-based values at first stage (before anxiety modulation).
    - stickiness in [0,1]: Strength of first-stage perseveration bias (scaled by STAI).

    Inputs:
    - action_1: array of length n_trials with first-stage actions (0 = A, 1 = U).
    - state: array of length n_trials with observed second-stage state (0 = X, 1 = Y).
    - action_2: array of length n_trials with second-stage actions (0/1 for the two aliens on that planet).
    - reward: array of length n_trials with received reward (e.g., 0 or 1).
    - stai: array-like with a single float in [0,1], the participant's anxiety score.
    - model_parameters: array-like with the parameters [alpha, beta, lam, w_mb, stickiness].

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, lam, w_mb, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A commonly -> X, U commonly -> Y
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage2 = np.zeros((2, 2))      # Q2[state, action2]
    q_stage1_mf = np.zeros(2)        # model-free Q at stage 1

    # Choice probabilities to accumulate likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration: transform w_mb via logistic shift by (0.5 - stai)
    eps = 1e-10
    w_mb = np.clip(w_mb, eps, 1 - eps)
    logit = np.log(w_mb) - np.log(1 - w_mb)
    w_eff = 1 / (1 + np.exp(-(logit + 2.0 * (0.5 - stai))))  # increases when stai < 0.5, decreases when stai > 0.5
    w_eff = float(np.clip(w_eff, 0.0, 1.0))

    prev_a1 = -1  # for stickiness

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # First-stage action values: MB from transition and current Q2; MF from q_stage1_mf
        max_q2 = np.max(q_stage2, axis=1)                  # best alien per planet
        q1_mb = transition_matrix @ max_q2                 # expected value per spaceship
        q1 = w_eff * q1_mb + (1 - w_eff) * q_stage1_mf     # arbitration

        # Add anxiety-scaled perseveration bias
        if prev_a1 >= 0:
            kappa_eff = stickiness * (0.5 + stai)          # stronger bias with higher anxiety
            bias = np.array([0.0, 0.0])
            bias[prev_a1] += kappa_eff
            q1 = q1 + bias

        # First-stage policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Learning
        # Second-stage update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # First-stage MF update with eligibility trace
        td_to_stage1 = (q_stage2[s, a2] - q_stage1_mf[a1])  # bootstrapping from stage 2
        q_stage1_mf[a1] += alpha * td_to_stage1 + alpha * lam * delta2

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based first-stage with uncertainty- and anxiety-adaptive learning rate, decay, and choice kernel.
    
    This model selects first-stage actions purely via a model-based evaluation of second-stage values,
    but learns second-stage values with an adaptive learning rate that increases with the magnitude of the
    reward prediction error and anxiety. A decay term pulls second-stage values toward 0.5 (environmental drift),
    and a first-stage choice kernel (perseveration) is scaled by anxiety. Anxiety also reduces the inverse temperature
    (more exploratory behavior).

    Parameters (model_parameters):
    - alpha0 in [0,1]: Baseline learning rate for second-stage value updates.
    - alpha_gain in [0,1]: Gain on learning rate from absolute prediction error (uncertainty sensitivity).
    - beta in [0,10]: Baseline inverse temperature for softmax at both stages.
    - omega in [0,1]: Decay toward 0.5 for second-stage Q-values on each trial.
    - choice_stick in [0,1]: Strength of first-stage choice kernel (scaled by STAI).

    Inputs:
    - action_1: array of length n_trials with first-stage actions (0 = A, 1 = U).
    - state: array of length n_trials with observed second-stage state (0 = X, 1 = Y).
    - action_2: array of length n_trials with second-stage actions (0/1 for the two aliens).
    - reward: array of length n_trials with received reward (0/1).
    - stai: array-like with a single float in [0,1], the participant's anxiety score.
    - model_parameters: array-like with the parameters [alpha0, alpha_gain, beta, omega, choice_stick].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha0, alpha_gain, beta, omega, choice_stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize second-stage values closer to neutral with decay to 0.5
    q_stage2 = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Apply decay toward 0.5 at second stage to capture drifting environment
        q_stage2 = (1 - omega) * q_stage2 + omega * 0.5

        # First-stage MB values from current Q2
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = transition_matrix @ max_q2
        q1 = q1_mb.copy()

        # Choice kernel at stage 1 scaled by anxiety
        if prev_a1 >= 0:
            kappa_eff = choice_stick * (0.5 + stai)
            bias = np.array([0.0, 0.0])
            bias[prev_a1] += kappa_eff
            q1 = q1 + bias

        # Anxiety lowers inverse temperature (more noise)
        beta_eff = beta * (1.0 - 0.5 * stai)

        # First-stage policy
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta_eff * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Second-stage learning with uncertainty- and anxiety-adaptive learning rate
        pe = r - q_stage2[s, a2]
        alpha_t = alpha0 + alpha_gain * abs(pe) * (1.0 + stai)
        alpha_t = float(np.clip(alpha_t, 0.0, 1.0))
        q_stage2[s, a2] += alpha_t * pe

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with surprise- and anxiety-modulated eligibility and stickiness.
    
    This model learns both second-stage rewards and first-stage transition probabilities. First-stage
    decisions are guided by model-based values computed from the learned transition matrix and are
    combined with a learned first-stage model-free value. Eligibility to update the first-stage model-free
    value from second-stage outcomes is amplified by transition surprise and anxiety, capturing altered
    credit assignment. Perseveration at stage 1 is stronger for lower anxiety.

    Parameters (model_parameters):
    - alpha_r in [0,1]: Learning rate for reward values (Q2) and first-stage MF value.
    - alpha_T in [0,1]: Learning rate for transition probabilities.
    - beta in [0,10]: Inverse temperature for softmax at both stages.
    - lam in [0,1]: Baseline eligibility trace weight from stage 2 to stage 1.
    - rho in [0,1]: Perseveration strength at stage 1 (repetition bias, attenuated by anxiety).

    Inputs:
    - action_1: array of length n_trials with first-stage actions (0 = A, 1 = U).
    - state: array of length n_trials with observed second-stage state (0 = X, 1 = Y).
    - action_2: array of length n_trials with second-stage actions (0/1 for the two aliens).
    - reward: array of length n_trials with received reward (0/1).
    - stai: array-like with a single float in [0,1], the participant's anxiety score.
    - model_parameters: array-like with the parameters [alpha_r, alpha_T, beta, lam, rho].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_T, beta, lam, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize transition beliefs near the instructed common transitions
    T = np.array([[0.7, 0.3],   # P(X|A), P(Y|A)
                  [0.3, 0.7]])  # P(X|U), P(Y|U)

    q_stage2 = np.zeros((2, 2))   # Q2[state, action2]
    q_stage1_mf = np.zeros(2)     # MF Q at stage 1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # First-stage values: model-based from learned transitions and model-free from q_stage1_mf
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = T @ max_q2
        q1 = q1_mb + q_stage1_mf

        # Perseveration bias at stage 1, stronger for low anxiety
        if prev_a1 >= 0:
            rho_eff = rho * (1.0 - stai)
            bias = np.array([0.0, 0.0])
            bias[prev_a1] += rho_eff
            q1 = q1 + bias

        # First-stage policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Transition learning from observed transition (a1 -> s)
        # Move T[a1, s] toward 1 and the other state toward 0
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            T[a1, ss] += alpha_T * (target - T[a1, ss])
        # Ensure each row remains a valid probability distribution
        T[a1, :] = np.clip(T[a1, :], 0.0, 1.0)
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

        # Reward learning at stage 2
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * delta2

        # Surprise- and anxiety-modulated eligibility to update MF stage-1 value
        surprise = 1.0 - T[a1, s]  # high when transition was unexpected
        lam_eff = lam + (1.0 - lam) * stai * surprise
        lam_eff = float(np.clip(lam_eff, 0.0, 1.0))

        # Update MF first-stage value with both bootstrapping and outcome
        td_boot = (q_stage2[s, a2] - q_stage1_mf[a1])
        q_stage1_mf[a1] += alpha_r * td_boot + alpha_r * lam_eff * delta2

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)