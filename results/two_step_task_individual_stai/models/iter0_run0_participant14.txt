def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration.
    
    This model blends model-based (MB) and model-free (MF) values at stage 1.
    The arbitration weight toward MB control is modulated by participant anxiety
    (stai). Higher anxiety increases MB control (linear, clamped).
    Stage 2 uses standard MF Q-learning. Stage 1 MF is updated via backup from
    stage-2 value (TD(1)-style).
    
    Parameters (model_parameters):
    - alpha: learning rate for Q updates, in [0,1]
    - beta1: inverse temperature for stage 1 softmax, in [0,10]
    - beta2: inverse temperature for stage 2 softmax, in [0,10]
    - w_base: baseline MB weight (before anxiety modulation), in [0,1]
    - k_anx: anxiety modulation strength of MB weight, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: spaceship A, 1: spaceship U)
    - state: array of reached planet indices (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1; within-planet aliens)
    - reward: array of rewards (coins) obtained on each trial (float)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta1, beta2, w_base, k_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: A->X common (0), U->Y common (1)
    transition_matrix = np.array([[0.7, 0.3],   # from A to [X, Y]
                                  [0.3, 0.7]])  # from U to [X, Y]

    # Initialize choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize Q-values
    q1_mf = np.zeros(2)           # model-free values for spaceships [A, U]
    q2 = np.zeros((2, 2))         # second-stage values per planet and alien

    # Anxiety-modulated arbitration weight (toward MB)
    # Center around medium/high boundary (0.51) and clamp to [0,1]
    w_mb = np.clip(w_base + k_anx * (stai - 0.51), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based stage-1 values via one-step lookahead using fixed transitions
        max_q2 = np.max(q2, axis=1)                 # best alien per planet
        q1_mb = transition_matrix @ max_q2          # expected value per spaceship

        # Hybrid value
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta1 * q1_hybrid
        logits1 -= np.max(logits1)  # numerical stability
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (within reached planet)
        s = state[t]
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 TD update
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update via backup from stage-2 chosen value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Learned-transition model-based RL with anxiety-modulated perseveration.
    
    This model learns the transition matrix from experience and uses a purely
    model-based (MB) first-stage policy computed from the learned transition and
    second-stage Q-values. Stage 2 uses MF Q-learning.
    
    Anxiety (stai) amplifies a perseveration bias to repeat the previous
    spaceship choice, capturing heightened habit-like inertia under higher
    anxiety while still planning via MB values.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2 updates, in [0,1]
    - alpha_t: transition learning rate, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - stick: base perseveration strength added to stage-1 logits, in [0,1]
    - gamma_anx: anxiety gain on perseveration, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, alpha_t, beta, stick, gamma_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition matrix close to uniform (uninformative)
    T = np.ones((2, 2)) * 0.5  # rows: spaceships [A,U], cols: planets [X,Y]
    q2 = np.zeros((2, 2))      # second-stage values per planet-alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    # Anxiety-modulated perseveration
    stick_eff = stick * np.clip(1.0 + gamma_anx * (stai - 0.31), 0.0, 2.0)

    for t in range(n_trials):
        # Compute MB values from learned transitions
        max_q2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        # Add perseveration bias to stage-1 logits
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within reached planet
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Reward learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Transition learning for chosen spaceship a1 toward observed state s
        # One-hot target: 1 for observed state, 0 for the other
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += alpha_t * (target - T[a1])

        # Ensure rows remain normalized and within [0,1] (numerical safety)
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated uncertainty bonus for directed exploration (MB at stage 1).
    
    Stage 2 is MF Q-learning. An uncertainty bonus (based on inverse visit counts)
    is added to stage-2 action values, encouraging exploration of less-sampled
    aliens. The bonus propagates to stage 1 via a model-based lookahead that uses
    the fixed transition structure. Anxiety reduces the exploration bonus
    (higher stai -> lower directed exploration). A mild perseveration bias is
    also included at stage 1.
    
    Parameters (model_parameters):
    - alpha: learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - bonus_base: baseline uncertainty bonus weight, in [0,1]
    - anx_gain: strength of anxiety suppression of exploration, in [0,1]
    - zeta: perseveration strength (stage 1 logits), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, bonus_base, anx_gain, zeta = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure
    T_fixed = np.array([[0.7, 0.3],   # A -> [X, Y]
                        [0.3, 0.7]])  # U -> [X, Y]

    # Q-values and visit counts for uncertainty
    q2 = np.zeros((2, 2))
    counts = np.zeros((2, 2))  # visit counts per planet-alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    # Anxiety-modulated exploration bonus weight
    # Higher stai reduces bonus linearly, clamped to [0, 1]
    bonus_w = np.clip(bonus_base * (1.0 - anx_gain * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Compute uncertainty as inverse sqrt of visit counts
        # u = 1/sqrt(n+1) in [~0,1], higher when less sampled
        uncertainty = 1.0 / np.sqrt(counts + 1.0)

        # Stage-2 augmented values with uncertainty bonus
        q2_aug = q2 + bonus_w * uncertainty

        # Model-based lookahead for stage 1 using augmented values
        max_q2_aug = np.max(q2_aug, axis=1)   # best augmented alien per planet
        q1_mb = T_fixed @ max_q2_aug

        # Add perseveration to stage-1 logits
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = zeta

        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within reached planet using augmented values
        s = state[t]
        logits2 = beta * q2_aug[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2
        counts[s, a2] += 1.0

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll