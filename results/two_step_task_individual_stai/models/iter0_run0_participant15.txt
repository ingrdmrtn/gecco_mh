def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    Parameters (all used; total=5):
    - alpha: [0,1] Stage-2 learning rate for rewards.
    - beta: [0,10] Inverse temperature for softmax at both stages.
    - lamb: [0,1] Eligibility trace controlling credit assignment to Stage-1.
    - w0: [0,1] Baseline model-based weight.
    - anx_influence: [0,1] How strongly STAI shifts arbitration toward model-based control.
    
    Anxiety use:
    - The effective model-based weight is w = (1 - anx_influence)*w0 + anx_influence*stai,
      so higher anxiety increases MB control when anx_influence > 0.
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha, beta, lamb, w0, anx_influence = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition: rows are first-stage actions (0=A, 1=U); cols are states (0=X, 1=Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)            # model-free Stage-1 values
    q2 = 0.5 * np.ones((2, 2))     # Stage-2 values initialized neutral

    # Anxiety-modulated arbitration weight (kept constant over trials here)
    w = (1.0 - anx_influence) * w0 + anx_influence * stai
    w = min(1.0, max(0.0, w))

    for t in range(n_trials):
        s = state[t]

        # Model-based Stage-1 values from transition structure and current Stage-2 values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Hybrid Stage-1 values
        q1_hybrid = w * mb_q1 + (1.0 - w) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1_hybrid - np.max(q1_hybrid))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        # TD errors
        delta2 = reward[t] - q2[s, a2]
        delta1 = q2[s, a2] - q1_mf[a1]

        # Update Stage-2 values
        q2[s, a2] += alpha * delta2

        # Update Stage-1 MF with direct TD and eligibility-trace propagation from reward
        q1_mf[a1] += alpha * delta1 + alpha * lamb * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Attention-weighted learning and anxiety-sensitive arbitration under rare transitions.
    Parameters (all used; total=5):
    - alpha0: [0,1] Base learning rate.
    - beta: [0,10] Inverse temperature for both stages.
    - w_mb: [0,1] Baseline model-based weight.
    - phi: [0,1] Scaling of STAI into learning-rate gain.
    - rho: [0,1] Anxiety-sensitivity to rare transitions (down-weights MB after rare transitions).
    
    Anxiety use:
    - Trial-wise learning rate alpha_t = alpha0 * ((1 - phi) + phi*stai) * g(|delta2|),
      where g(x)=clip(0.5 + 0.5*x, 0, 1) increases learning with surprise.
    - MB weight is suppressed on trials with rare transitions by factor (1 - rho*stai).
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha0, beta, w_mb, phi, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Precompute common destination for each action (argmax over transition probs)
    common_dest = np.argmax(transition_matrix, axis=1)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage-2 policy (use current q2)
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Compute TD at stage 2 to set trial-wise attention/alpha before stage-1 update
        delta2 = reward[t] - q2[s, a2]
        # Surprise-driven gain in [0,1]
        gain = 0.5 + 0.5 * abs(delta2)
        gain = min(1.0, max(0.0, gain))
        # Anxiety-inflated base learning rate
        alpha_t = alpha0 * ((1.0 - phi) + phi * stai)
        alpha_t = max(0.0, min(1.0, alpha_t))
        # Final adaptive learning rate
        alpha_eff = max(0.0, min(1.0, alpha_t * gain))

        # Update stage-2 values
        q2[s, a2] += alpha_eff * delta2

        # Stage-1 model-based and model-free values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_eff * delta1

        # Determine if transition was rare given chosen action
        was_common = (s == common_dest[a1])
        rare = 0 if was_common else 1

        # Anxiety-sensitive arbitration: down-weight MB after rare transitions
        w_t = w_mb * (1.0 - rho * stai * rare)
        w_t = min(1.0, max(0.0, w_t))
        q1_hybrid = w_t * mb_q1 + (1.0 - w_t) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1_hybrid - np.max(q1_hybrid))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric learning with anxiety-skew, MB planning at stage-1,
    and anxiety-dependent stickiness and forgetting.
    Parameters (all used; total=5):
    - alpha: [0,1] Base learning rate.
    - beta: [0,10] Inverse temperature for both stages.
    - z: [0,1] Degree of anxiety-skewed asymmetry in learning from positive vs negative outcomes.
    - pi: [0,1] Stickiness strength on Stage-1, reduced by higher anxiety.
    - f: [0,1] Forgetting rate toward 0.5 at Stage-2, amplified by anxiety.
    
    Anxiety use:
    - Learning rates: alpha_plus = alpha * [(1 - z) + z * stai], alpha_minus = alpha * [(1 - z) + z * (1 - stai)].
    - Stickiness bias on previous Stage-1 choice is pi*(1 - stai).
    - Effective forgetting at Stage-2 is f_eff = f * stai.
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha, beta, z, pi, f = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values
    q2 = 0.5 * np.ones((2, 2))

    # Previous Stage-1 choice for stickiness (None -> no bias first trial)
    prev_a1 = None

    # Precompute anxiety-modulated quantities that are constant across trials
    stickiness_weight = pi * (1.0 - stai)
    f_eff = f * stai  # forgetting strength increases with anxiety
    f_eff = min(1.0, max(0.0, f_eff))

    for t in range(n_trials):
        s = state[t]

        # Model-based Stage-1 values from current q2
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Add stickiness bias to chosen action from previous trial
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stickiness_weight

        # Stage-1 policy
        z1 = beta * (mb_q1 + bias - np.max(mb_q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy for observed state
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-2 learning with valence asymmetry and anxiety-dependent forgetting
        r = reward[t]
        pe = r - q2[s, a2]
        # Select learning rate by valence, skewed by anxiety through z
        alpha_plus = alpha * ((1.0 - z) + z * stai)
        alpha_minus = alpha * ((1.0 - z) + z * (1.0 - stai))
        alpha_eff = alpha_plus if pe >= 0.0 else alpha_minus
        alpha_eff = min(1.0, max(0.0, alpha_eff))

        # Apply forgetting toward 0.5 baseline before update
        q2[s, :] = (1.0 - f_eff) * q2[s, :] + f_eff * 0.5

        # Update chosen action value
        q2[s, a2] += alpha_eff * pe

        # Track previous Stage-1 choice for next-trial stickiness
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll