Below are three standalone cognitive models as Python functions. Each function:

- Implements a two-step decision model with softmax choice at both stages.
- Uses the provided parameters (all meaningfully).
- Returns the negative log-likelihood of the observed choices.
- Uses the participantâ€™s STAI anxiety score to modulate specific mechanisms.
- Uses at most 5 parameters in total.

Notes on inputs:
- action_1: array of ints in {0,1} indicating first-stage choice (0=A, 1=U).
- state: array of ints in {0,1} indicating reached planet (0=X, 1=Y).
- action_2: array of ints in {0,1} indicating second-stage alien choice.
- reward: array of floats in [0,1] (coins).
- stai: array-like with a single float in [0,1].
- model_parameters: tuple/list with parameter values within required bounds.

Model 1: Hybrid MB/MF with anxiety-modulated arbitration, eligibility, and stickiness
def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated arbitration and eligibility.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed first-stage choices: 0=A, 1=U.
    state : array-like of int (0 or 1)
        Observed second-stage states (planets): 0=X, 1=Y.
    action_2 : array-like of int (0 or 1)
        Observed second-stage choices (aliens).
    reward : array-like of float in [0,1]
        Observed rewards (coins).
    stai : array-like with single float in [0,1]
        Anxiety score. Higher values imply higher anxiety.
        Here, anxiety reduces model-based weight and eligibility trace, and increases stickiness impact.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for Q-updates (both stages).
        - beta in [0,10]: softmax inverse temperature for both stages.
        - w0 in [0,1]: baseline model-based arbitration weight.
        - lambda0 in [0,1]: baseline eligibility trace from stage-2 PE to stage-1 MF value.
        - kappa in [0,1]: baseline perseveration (stickiness) strength at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w0, lambda0, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # rows: actions (A,U), cols: states (X,Y)

    # Anxiety-modulated parameters
    # Higher anxiety shifts arbitration away from MB, and damps eligibility
    w = np.clip(w0 * (1.0 - 0.6 * stai) + 0.3 * stai, 0.0, 1.0)
    lam = np.clip(lambda0 * (1.0 - 0.5 * stai), 0.0, 1.0)
    stickiness_scale = 1.0 + 0.5 * stai  # anxious participants rely more on perseveration
    kappa_eff = kappa * stickiness_scale

    # Value functions
    q_stage1_mf = np.zeros(2)        # model-free at stage 1
    q_stage2 = np.zeros((2, 2))      # state-action values at stage 2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None  # for stickiness

    for t in range(n_trials):
        # Model-based plan at stage 1 from current stage-2 values
        max_q_stage2 = np.max(q_stage2, axis=1)  # per state
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MB and MF with anxiety-modulated arbitration
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # Add stickiness to first-stage logits
        logits1 = beta * q1_combined
        if prev_a1 is not None:
            for a in range(2):
                if a == prev_a1:
                    logits1[a] += kappa_eff

        # Softmax for first-stage choice
        maxl = np.max(logits1)
        exp_q1 = np.exp(logits1 - maxl)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice policy
        s = state[t]
        logits2 = beta * q_stage2[s]
        maxl2 = np.max(logits2)
        exp_q2 = np.exp(logits2 - maxl2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcomes
        r = reward[t]

        # Learning
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility
        # Immediate consistency error for stage-1 chosen action
        delta1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1 + lam * delta2)

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


Model 2: Transition-learning planner with anxiety-modulated transition learning and uncertainty bonus
def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planner with learned transitions. Anxiety speeds transition learning
    and increases attraction to uncertain choices; includes perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Received rewards.
    stai : array-like with single float in [0,1]
        Anxiety score. Higher anxiety increases transition learning rate and boosts
        exploration via an uncertainty bonus on first-stage values.
    model_parameters : iterable of 5 floats
        - alpha_R in [0,1]: reward learning rate (stage-2 Q-values).
        - beta in [0,10]: softmax inverse temperature for both stages.
        - alpha_Tbase in [0,1]: baseline transition learning rate.
        - phi in [0,1]: perseveration strength for first-stage choice.
        - omega in [0,1]: baseline weight of uncertainty bonus in first-stage values.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_R, beta, alpha_Tbase, phi, omega = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-modulated parameters
    alpha_T = np.clip(alpha_Tbase * (0.5 + 0.5 * stai), 0.0, 1.0)  # anxious -> faster transition learning
    unc_w = omega * (0.3 + 0.7 * stai)  # anxious -> higher weight on uncertainty bonus
    phi_eff = phi * (1.0 + 0.5 * stai)  # anxious -> more perseveration

    # Initialize learned transitions as uniform (agnostic)
    T_hat = np.ones((2, 2)) * 0.5  # rows: actions, cols: states
    # Stage-2 Q-values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    # Helper for entropy-based uncertainty bonus (normalized to [0,1])
    def norm_entropy(p_row):
        # Natural log; normalize by ln(2) for binary outcomes
        eps = 1e-12
        p = np.clip(p_row, eps, 1.0 - eps)
        H = -(p[0] * np.log(p[0]) + p[1] * np.log(p[1]))
        return H / np.log(2.0)

    for t in range(n_trials):
        # Compute MB values from current q2 under learned transitions
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T_hat @ max_q2

        # Add uncertainty bonus per action (higher when transitions are more uncertain)
        bonus = np.zeros(2)
        for a in range(2):
            bonus[a] = unc_w * norm_entropy(T_hat[a])

        logits1 = beta * (q1_mb + bonus)

        # Add perseveration bias
        if prev_a1 is not None:
            for a in range(2):
                if a == prev_a1:
                    logits1[a] += phi_eff

        # First-stage softmax
        maxl = np.max(logits1)
        exp_q1 = np.exp(logits1 - maxl)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage softmax (values from q2)
        s = state[t]
        logits2 = beta * q2[s]
        maxl2 = np.max(logits2)
        exp_q2 = np.exp(logits2 - maxl2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update q2
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_R * delta2

        # Update learned transition for the chosen action toward observed state
        # Simple Rescorla-Wagner on the row with normalization implicit
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T_hat[a1, sp] = (1.0 - alpha_T) * T_hat[a1, sp] + alpha_T * target
        # Ensure numerical stability/normalization
        row_sum = np.sum(T_hat[a1])
        if row_sum > 0:
            T_hat[a1] /= row_sum

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


Model 3: Risk-sensitive second-stage with anxiety-modulated risk aversion and stage-2 determinism
def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive two-step planner. Anxiety increases risk aversion and reduces
    exploitation at stage 2; stage 1 plans over risk-adjusted state values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Received rewards.
    stai : array-like with single float in [0,1]
        Anxiety score. Higher values imply higher risk aversion and lower stage-2 determinism.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for both mean and variance estimates of rewards.
        - beta1 in [0,10]: softmax inverse temperature for first-stage choices.
        - beta2_base in [0,10]: baseline stage-2 inverse temperature.
        - rho_base in [0,1]: baseline risk-aversion weight.
        - kappa in [0,1]: perseveration (stickiness) at first stage.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta1, beta2_base, rho_base, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated parameters
    # Increase risk penalty and reduce stage-2 determinism with anxiety
    gamma = np.clip(rho_base + 0.8 * stai * (1.0 - rho_base), 0.0, 1.0)  # risk penalty weight
    beta2 = max(0.0, beta2_base * (1.0 - 0.5 * stai))

    # Stage-2 estimates: mean and variance of reward per state-action
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.25  # initial uncertainty (Bernoulli variance around 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Risk-adjusted state values for planning: max over risk-adjusted values in each state
        risk_adj = q2_mean - gamma * np.sqrt(np.maximum(q2_var, 1e-8))
        max_risk_adj = np.max(risk_adj, axis=1)  # per state

        q1_mb = T @ max_risk_adj

        # Add first-stage stickiness to logits
        logits1 = beta1 * q1_mb
        if prev_a1 is not None:
            for a in range(2):
                if a == prev_a1:
                    logits1[a] += kappa

        # First-stage softmax
        maxl = np.max(logits1)
        exp_q1 = np.exp(logits1 - maxl)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage softmax uses risk-adjusted values at reached state
        s = state[t]
        logits2 = beta2 * risk_adj[s]
        maxl2 = np.max(logits2)
        exp_q2 = np.exp(logits2 - maxl2)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]

        # Update mean with TD error
        delta = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha * delta

        # Update variance as exponential moving average of squared prediction error
        q2_var[s, a2] = (1.0 - alpha) * q2_var[s, a2] + alpha * (delta ** 2)

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)