def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    The model combines model-based (MB) and model-free (MF) values at stage 1.
    Anxiety down-weights MB control: higher STAI reduces the MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically 0 or 1.
    stai : array-like of float in [0,1]
        Anxiety score (single value array), used to modulate arbitration.
    model_parameters : tuple/list
        Parameters with bounds:
        - alpha: learning rate in [0,1]
        - beta: inverse temperature in [0,10]
        - lam: eligibility trace in [0,1]
        - w0: baseline MB weight in [0,1]
        - anx: anxiety influence on arbitration in [0,1]
          Effective MB weight is w = clip(w0 + anx*(0.5 - stai), 0, 1)
          so higher stai reduces w when anx>0.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, lam, w0, anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)          # model-free values at stage 1
    q2 = np.zeros((2, 2))        # model-free values at stage 2 (per state, per action)

    # Anxiety-modulated arbitration weight (fixed across trials)
    w = w0 + anx * (0.5 - stai)
    w = 0.0 if w < 0.0 else (1.0 if w > 1.0 else w)

    for t in range(n_trials):
        # Stage-1 policy: hybrid MB/MF softmax
        max_q2 = np.max(q2, axis=1)             # values of states X,Y under greedy 2nd-stage
        q1_mb = transition_matrix @ max_q2      # MB action values
        q1 = (1 - w) * q1_mf + w * q1_mb

        # Softmax with stability
        z1 = q1 - np.max(q1)
        exp1 = np.exp(beta * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: purely MF softmax over actions in observed state
        s = state[t]
        q2_s = q2[s]
        z2 = q2_s - np.max(q2_s)
        exp2 = np.exp(beta * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 TD error and update
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF TD error and update with eligibility trace
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1
        q1_mf[a1] += lam * alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric (valence-dependent) learning with anxiety-amplified negative learning,
    hybrid MB/MF stage-1, and anxiety-scaled perseveration bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically 0 or 1.
    stai : array-like of float in [0,1]
        Anxiety score (single value array).
        Used to amplify negative-learning and to scale perseveration.
    model_parameters : tuple/list
        Parameters with bounds:
        - alpha: base learning rate in [0,1]
        - beta: inverse temperature in [0,10]
        - k_neg: anxiety coupling for negative learning in [0,1]
                 alpha_neg = clip(alpha * (1 + k_neg * stai), 0, 1)
                 alpha_pos = alpha
        - persev: base perseveration strength in [0,1]
                  effective stickiness = persev * stai
        - wmb: MB weight at stage 1 in [0,1]
               stage-1 Q = (1 - wmb)*MF + wmb*MB

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, k_neg, persev, wmb = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Effective parameters modulated by anxiety
    alpha_pos = alpha
    alpha_neg = alpha * (1 + k_neg * stai)
    alpha_neg = 1.0 if alpha_neg > 1.0 else (0.0 if alpha_neg < 0.0 else alpha_neg)

    stick = persev * stai  # stronger perseveration under higher anxiety

    prev_a1 = None
    prev_a2 = None
    prev_s = None

    for t in range(n_trials):
        # MB component for stage-1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid Q at stage-1
        q1 = (1 - wmb) * q1_mf + wmb * q1_mb

        # Add perseveration bias at stage-1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        # Stage-1 policy
        z1 = (q1 + bias1) - np.max(q1 + bias1)
        exp1 = np.exp(beta * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration (state-dependent)
        s = state[t]
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2 is not None and prev_s == s:
            bias2[prev_a2] += stick

        z2 = (q2_s + bias2) - np.max(q2_s + bias2)
        exp2 = np.exp(beta * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        delta2 = r - q2[s, a2]
        # Valence-dependent learning at stage-2
        a2_lr = alpha_pos if delta2 >= 0 else alpha_neg
        q2[s, a2] += a2_lr * delta2

        # Stage-1 MF update (bootstrapped from updated q2 and valence-sensitive)
        delta1 = q2[s, a2] - q1_mf[a1]
        a1_lr = alpha_pos if delta1 >= 0 else alpha_neg
        q1_mf[a1] += a1_lr * delta1

        # Update perseveration trackers
        prev_a1 = a1
        prev_a2 = a2
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-sensitive (Pearce-Hall-like) learning with anxiety-scaled gain,
    hybrid MB/MF stage-1, and eligibility trace coupling.
    
    Learning rate adapts from recent surprise: alpha_t = clip((alpha0 + k*|delta_prev|) * g(stai), 0, 1)
    where g(stai) = 0.5 + 0.5*stai increases learning under higher anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically 0 or 1.
    stai : array-like of float in [0,1]
        Anxiety score (single value array) scaling the dynamic learning rate.
    model_parameters : tuple/list
        Parameters with bounds:
        - alpha0: baseline learning rate in [0,1]
        - k: surprise-to-learning coupling in [0,1]
        - beta: inverse temperature in [0,10]
        - wmb: MB weight at stage 1 in [0,1]
        - lam: eligibility trace coupling in [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha0, k, beta, wmb, lam = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Anxiety scaling of learning rate
    g = 0.5 + 0.5 * stai  # in [0.5, 1.0]

    # Initialize previous absolute TD error for dynamic learning
    abs_delta_prev = 0.0

    for t in range(n_trials):
        # Compute dynamic learning rate for this trial
        alpha_t = alpha0 + k * abs_delta_prev
        alpha_t *= g
        alpha_t = 1.0 if alpha_t > 1.0 else (0.0 if alpha_t < 0.0 else alpha_t)

        # Stage-1 hybrid policy
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2
        q1 = (1 - wmb) * q1_mf + wmb * q1_mb

        z1 = q1 - np.max(q1)
        exp1 = np.exp(beta * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        q2_s = q2[s]
        z2 = q2_s - np.max(q2_s)
        exp2 = np.exp(beta * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates with dynamic learning rate
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_t * delta1
        q1_mf[a1] += lam * alpha_t * delta2

        # Update surprise for next trial
        abs_delta_prev = abs(delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll