Below are three standalone cognitive models that compute the negative log-likelihood of the participantâ€™s observed choices in the two-step task. Each model uses the STAI score meaningfully, respects parameter bounds, and uses at most 5 parameters.

Note: Assume numpy as np is already imported by the environment.

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-biased arbitration and stickiness.
    
    Description:
    - Stage 1 choice values are a hybrid of model-based (MB) and model-free (MF) values.
    - The MB weight is interpolated by anxiety: w_mb = (1 - stai) * w_low + stai * w_high,
      so higher anxiety increases the contribution of w_high and lower anxiety favors w_low.
    - A single stickiness parameter biases repeating previous stage-1 and stage-2 actions.
      Its effective strength is scaled by (1 - stai), so lower anxiety yields more stickiness influence.
    - Stage 2 is MF with a softmax policy.
    - MF updates use simple two-step TD with an eligibility trace on stage-1 via lambda fixed to 0.5
      (embedded by propagating the stage-2 value difference once).
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - w_low: MB weight used when stai is low (for stai -> 0) in [0,1]
    - w_high: MB weight used when stai is high (for stai -> 1) in [0,1]
    - stickiness: perseveration strength in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
      (0=W on X or P on Y; 1=S on X or H on Y)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w_low, w_high, stickiness)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, w_low, w_high, stickiness = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition structure (fixed, known)
    # Rows: action (0=A, 1=U), Cols: state (0=X, 1=Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q1_mf = np.zeros(2)            # MF values for stage-1 actions
    Q2 = np.zeros((2, 2))          # MF values for stage-2: state x action

    # Anxiety-modulated arbitration
    w_mb = (1.0 - st) * w_low + st * w_high
    w_mb = min(1.0, max(0.0, w_mb))

    # Stickiness effective strength (more effect for low anxiety)
    stick_eff = stickiness * (1.0 - st)

    # Track previous actions for stickiness
    prev_a1 = None
    prev_a2 = [None, None]  # per state (0=X, 1=Y)

    for t in range(n_trials):
        s = int(state[t])

        # MB estimate at stage 1 from current Q2
        max_Q2 = np.max(Q2, axis=1)     # best action value at each state
        Q1_mb = T @ max_Q2              # expected value under transitions

        # Hybrid stage-1 value with stickiness bias
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Add stickiness bias to stage-1 preferences
        pref1 = Q1_hyb.copy()
        if prev_a1 is not None:
            pref1[prev_a1] += stick_eff

        # Softmax for stage-1
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[int(action_1[t])]

        # Stage-2 policy with stickiness
        pref2 = Q2[s].copy()
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += stick_eff

        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[int(action_2[t])]

        # Observe reward and update values (MF/SARSA-style)
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 TD towards current stage-2 value (eligibility = 0.5)
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * 0.5 * delta1

        # Stage-2 TD to reward
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Optional second eligibility step to propagate reward to stage-1
        Q1_mf[a1] += alpha * 0.5 * delta2

        # Update stickiness trackers
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated temperature and learned transition model (MB-MF hybrid with lambda).
    
    Description:
    - Learns the transition function T from experienced transitions with learning rate eta.
      This captures potential uncertainty or idiosyncratic beliefs.
    - Hybrid MB/MF valuation at stage-1 with fixed mixing 0.5 (implicit through MF+MB sum via softmax).
      We keep MF Q1 separately through eligibility.
    - Inverse temperature is amplified for lower anxiety:
        beta_eff = beta * (1 + k_beta * (1 - stai))
      So lower anxiety sharpens choices; higher anxiety softens them.
    - Eligibility trace lambda is also anxiety-scaled to reflect reduced credit assignment under anxiety:
        lambda_eff = lambda_ * (1 - stai)
    - Stage-2 is MF with softmax.
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: base inverse temperature in [0,10]
    - lambda_: eligibility trace in [0,1]
    - eta: transition learning rate in [0,1]
    - k_beta: temperature gain factor in [0,1] scaling anxiety effect
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship
    - state: array-like of ints in {0,1}; observed planet
    - action_2: array-like of ints in {0,1}; chosen alien at second stage
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, lambda_, eta, k_beta)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, lambda_, eta, k_beta = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize transition model to uninformative (0.5/0.5)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    beta_eff = beta * (1.0 + k_beta * (1.0 - st))
    lambda_eff = lambda_ * (1.0 - st)

    for t in range(n_trials):
        s = int(state[t])

        # MB value via current learned transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Combine MB and MF at stage 1 by summing their normalized contributions
        # Here we use a softmax directly over (Q1_mb + Q1_mf) with equal implicit weighting.
        Q1_comb = 0.5 * Q1_mb + 0.5 * Q1_mf

        # Stage-1 softmax
        exp1 = np.exp(beta_eff * (Q1_comb - np.max(Q1_comb)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[int(action_1[t])]

        # Stage-2 softmax
        pref2 = Q2[s].copy()
        exp2 = np.exp(beta_eff * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[int(action_2[t])]

        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Update transition model T toward observed state for chosen action
        # Move probability mass toward the observed state using eta
        current = T[a1, s]
        T[a1, s] = current + eta * (1.0 - current)
        T[a1, 1 - s] = 1.0 - T[a1, s]

        # MF updates with eligibility
        # Stage-1 towards stage-2 value
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * lambda_eff * delta1

        # Stage-2 towards reward
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Propagate reward further to stage-1 depending on lambda_eff
        Q1_mf[a1] += alpha * lambda_eff * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-dependent lapse probability.
    
    Description:
    - Stage-1 uses a hybrid of MB and MF values with fixed weight w (parameter).
    - Both stages include a lapse mechanism: with probability epsilon(stai), the agent chooses randomly.
      The lapse probability is linearly interpolated by anxiety:
         epsilon = (1 - stai) * eps_low + stai * eps_high
      so higher anxiety increases lapses if eps_high > eps_low.
    - Stage-2 is MF with softmax.
    - MF updates use two-step TD with simple eligibility propagation from stage-2 to stage-1.
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - w: MB weight (stage-1 hybrid) in [0,1]
    - eps_low: lapse probability for low anxiety in [0,1]
    - eps_high: lapse probability for high anxiety in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship
    - state: array-like of ints in {0,1}; observed planet
    - action_2: array-like of ints in {0,1}; chosen alien at second stage
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w, eps_low, eps_high)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, w, eps_low, eps_high = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed, known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-dependent lapse probability
    epsilon = (1.0 - st) * eps_low + st * eps_high
    epsilon = min(1.0, max(0.0, epsilon))

    for t in range(n_trials):
        s = int(state[t])

        # MB component at stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid value
        Q1_hyb = w * Q1_mb + (1.0 - w) * Q1_mf

        # Stage-1 softmax with lapse
        pref1 = Q1_hyb.copy()
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        soft1 = exp1 / np.sum(exp1)
        probs1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        p_choice_1[t] = probs1[int(action_1[t])]

        # Stage-2 softmax with lapse
        pref2 = Q2[s].copy()
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        soft2 = exp2 / np.sum(exp2)
        probs2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        p_choice_2[t] = probs2[int(action_2[t])]

        # TD updates
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 update towards stage-2 value (eligibility=0.5)
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * 0.5 * delta1

        # Stage-2 update to reward
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Propagate reward to stage-1
        Q1_mf[a1] += alpha * 0.5 * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

How anxiety is used:
- Model 1: Anxiety interpolates the MB weight (w_low vs w_high) and scales stickiness strength down as anxiety increases.
- Model 2: Anxiety softens choices by reducing effective inverse temperature and lowers eligibility credit assignment; transitions are learned from experience.
- Model 3: Anxiety increases a lapse probability that mixes the softmax with uniform random choice.