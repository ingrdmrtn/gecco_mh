def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and perseveration.
    Parameters:
    - model_parameters = [alpha, beta, w0, kappa, rho]
      alpha: learning rate for value updates (stage 1 and 2), in [0,1]
      beta: inverse temperature for softmax choice, in [0,10]
      w0: baseline weight for model-based control, in [0,1]
      kappa: strength of anxiety modulation of arbitration, in [0,1] (effective weight shifts with stai)
      rho: perseveration strength at stage 1, in [0,1]
    Other inputs:
      action_1: array-like of first-stage choices (0 or 1)
      state: array-like of second-stage states (0 or 1)
      action_2: array-like of second-stage choices (0 or 1)
      reward: array-like of rewards (float)
      stai: array-like with single value in [0,1] indicating anxiety score
    Returns:
      Negative log-likelihood of observed first- and second-stage choices.
    Model details:
      - First-stage action values are a convex combination of model-based and model-free values.
      - Arbitration weight w is modulated by stai: w = clip(w0 + kappa*(stai - 0.5), 0, 1).
      - Perseveration bias added to previously chosen first-stage action scaled by rho and stai.
      - Eligibility-trace-like credit from stage 2 to stage 1 depends on stai: lambda_et = clip(0.5 + 0.5*stai, 0, 1).
      - Transition structure is known (A->X and U->Y common with prob .7).
    """
    alpha, beta, w0, kappa, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition matrix: rows are first-stage actions (A=0, U=1), cols are states (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    Q1_mf = np.zeros(2)       # model-free first-stage values
    Q2 = np.zeros((2, 2))     # second-stage state-action values

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration tracker
    last_a1 = None

    # Anxiety-modulated arbitration and eligibility
    w = np.clip(w0 + kappa * (stai - 0.5), 0.0, 1.0)
    lambda_et = np.clip(0.5 + 0.5 * stai, 0.0, 1.0)
    rho_eff = rho * (0.5 + stai)  # stronger perseveration with higher stai

    for t in range(n_trials):
        # Model-based first-stage values: back up from second-stage max Q
        max_Q2 = np.max(Q2, axis=1)              # shape (2,)
        Q1_mb = T @ max_Q2                       # shape (2,)

        # Combine MF and MB with perseveration bias
        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] += rho_eff
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf + bias

        # First-stage policy
        q1 = Q1 - np.max(Q1)
        exp_q1 = np.exp(beta * q1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = int(state[t])
        q2 = Q2[s, :] - np.max(Q2[s, :])
        exp_q2 = np.exp(beta * q2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # TD errors and updates
        # Stage 2 update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage 1 MF update with eligibility from stage 2
        # Add both the bootstrapped difference and a portion of stage-2 prediction error
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1 + alpha * lambda_et * delta2

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive learning with anxiety-modulated volatility and learned transitions.
    Parameters:
    - model_parameters = [alpha0, beta, vsi, alpha_T, pi]
      alpha0: base learning rate for values, in [0,1]
      beta: inverse temperature for softmax choices, in [0,10]
      vsi: volatility sensitivity index (scales |RPE| into learning-rate boosts), in [0,1]
      alpha_T: learning rate for transition model updates, in [0,1]
      pi: first-stage perseveration strength, in [0,1]
    Other inputs:
      action_1: array-like of first-stage choices (0 or 1)
      state: array-like of second-stage states (0 or 1)
      action_2: array-like of second-stage choices (0 or 1)
      reward: array-like of rewards (float)
      stai: array-like with single value in [0,1] indicating anxiety score
    Returns:
      Negative log-likelihood of observed first- and second-stage choices.
    Model details:
      - Learns the transition matrix T_hat online; higher stai -> faster transition learning.
      - Learning rate for values increases with absolute RPE scaled by stai and vsi.
      - First-stage policy uses an equal-weight blend of MB and MF values plus perseveration bias scaling with stai.
      - Second-stage policy is softmax over Q2.
    """
    alpha0, beta, vsi, alpha_T, pi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix to neutral (0.5/0.5) for each action
    T_hat = np.full((2, 2), 0.5)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    pi_eff = pi * (0.5 + stai)  # stronger perseveration with anxiety
    lambda_et = 0.5  # fixed trace; keep parameters <=5

    for t in range(n_trials):
        # Model-based using learned transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_hat @ max_Q2

        # Blend MB and MF
        Q1_base = 0.5 * Q1_mb + 0.5 * Q1_mf

        # Perseveration bias
        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] += pi_eff
        Q1 = Q1_base + bias

        # First-stage softmax
        q1 = Q1 - np.max(Q1)
        exp_q1 = np.exp(beta * q1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage softmax
        s = int(state[t])
        q2 = Q2[s, :] - np.max(Q2[s, :])
        exp_q2 = np.exp(beta * q2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # Stage-2 update with adaptive learning-rate
        delta2 = r - Q2[s, a2]
        alpha_t = np.clip(alpha0 + vsi * stai * abs(delta2), 0.0, 1.0)
        Q2[s, a2] += alpha_t * delta2

        # Stage-1 MF update with partial eligibility
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha_t * td1 + alpha_t * lambda_et * delta2

        # Update learned transition model for the taken action
        # Anxiety speeds up transition learning
        aT_eff = np.clip(alpha_T * (0.5 + stai), 0.0, 1.0)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T_hat[a1, :] = (1.0 - aT_eff) * T_hat[a1, :] + aT_eff * target

        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric valence learning with anxiety-modulated MB weight and exploration.
    Parameters:
    - model_parameters = [alpha_pos, alpha_neg, beta, mb_weight, gamma]
      alpha_pos: learning rate when RPE is positive, in [0,1]
      alpha_neg: learning rate when RPE is negative, in [0,1]
      beta: inverse temperature baseline, in [0,10]
      mb_weight: baseline weight for model-based control, in [0,1]
      gamma: anxiety sensitivity scaling MB weight and exploration, in [0,1]
    Other inputs:
      action_1: array-like of first-stage choices (0 or 1)
      state: array-like of second-stage states (0 or 1)
      action_2: array-like of second-stage choices (0 or 1)
      reward: array-like of rewards (float)
      stai: array-like with single value in [0,1] indicating anxiety score
    Returns:
      Negative log-likelihood of observed first- and second-stage choices.
    Model details:
      - Uses fixed known transitions (common=0.7).
      - MB/MF hybrid at stage 1; MB weight increases with stai if gamma>0.
      - Exploration increases with anxiety: beta_eff = beta * (1 - 0.5*gamma*stai).
      - Asymmetric learning rates by valence; negative updates amplified by stai.
      - Rare transition credit assignment: when a rare transition occurs, MF value of the unchosen
        first-stage action is nudged toward the alternative MB value, scaled by w and stai.
    """
    alpha_pos, alpha_neg, beta, mb_weight, gamma = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration and exploration
    w = np.clip(mb_weight + gamma * (stai - 0.5), 0.0, 1.0)
    beta_eff = max(1e-6, beta * (1.0 - 0.5 * gamma * stai))

    for t in range(n_trials):
        # MB first-stage values
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid first-stage values
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # First-stage policy
        q1 = Q1 - np.max(Q1)
        exp_q1 = np.exp(beta_eff * q1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = int(state[t])
        q2 = Q2[s, :] - np.max(Q2[s, :])
        exp_q2 = np.exp(beta_eff * q2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # Stage-2 update with asymmetric valence and anxiety scaling
        delta2 = r - Q2[s, a2]
        if delta2 >= 0:
            a_eff = np.clip(alpha_pos * (1.0 - 0.5 * stai), 0.0, 1.0)
        else:
            a_eff = np.clip(alpha_neg * (1.0 + stai), 0.0, 1.0)
        Q2[s, a2] += a_eff * delta2

        # Stage-1 MF update towards backed-up value
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += a_eff * td1

        # Rare transition credit assignment to the unchosen first-stage action
        # Common if s == a1 given the task structure; otherwise rare
        is_rare = (s != a1)
        if is_rare:
            alt = 1 - a1
            # Nudge unchosen MF toward the MB value of the alternative action
            target_alt = Q1_mb[alt]
            Q1_mf[alt] += np.clip(w * gamma * stai, 0.0, 1.0) * (target_alt - Q1_mf[alt])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll