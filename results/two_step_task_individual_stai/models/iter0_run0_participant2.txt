def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration and eligibility traces.
    
    This model combines model-free Q-values with model-based estimates from the known transition
    structure. The arbitration weight favoring model-based control is modulated by STAI (anxiety)
    such that lower anxiety increases model-based weighting. Stage-1 Q-values are updated with an
    eligibility trace from the stage-2 prediction error.
    
    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like, shape (n_trials,)
        Second-stage states (0 = planet X, 1 = planet Y) actually reached after action_1.
    action_2 : array-like, shape (n_trials,)
        Second-stage choices (0/1; X: W/S, Y: P/H).
    reward : array-like, shape (n_trials,)
        Obtained reward (e.g., coins; typically 0/1).
    stai : array-like, shape (1,) or scalar-like
        Participant STAI score in [0,1]. Lower values indicate lower anxiety.
    model_parameters : iterable
        Model parameters (total <= 5):
        - alpha in [0,1]: learning rate for value updates
        - beta1 in [0,10]: inverse temperature at stage 1
        - beta2 in [0,10]: inverse temperature at stage 2
        - lam in [0,1]: eligibility trace strength from stage 2 PE to stage 1 MF value
        - w0 in [0,1]: baseline arbitration weight that is modulated by STAI

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta1, beta2, lam, w0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(np.asarray(stai)[0])

    # Known transition structure (rows: A,U; cols: X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2)           # model-free stage-1 action values (A,U)
    q2 = np.zeros((2, 2))         # stage-2 state-action values: rows X,Y; cols actions 0/1

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration: lower anxiety -> more MB control.
    # Use a squashed combination of baseline w0 and STAI. Fixed slope ensures STAI is used.
    # w in [0,1]
    # Center STAI around 0.5 and add to a logit formed from w0 to make modulation meaningful.
    eps = 1e-10
    w0 = min(max(w0, 0.0), 1.0)
    # Convert w0 to logit safely
    w0_logit = np.log((w0 + eps) / (1.0 - (w0 + eps)))
    w_logit = w0_logit + 3.0 * (0.5 - stai_val)  # lower STAI -> larger w
    w = 1.0 / (1.0 + np.exp(-w_logit))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values = expected max over stage-2 by transition structure
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = transition_matrix @ max_q2  # shape (2,)

        # Hybrid action values for stage-1
        q1 = (1.0 - w) * q1_mf + w * q1_mb

        # Softmax policy at stage-1
        q1_shift = q1 - np.max(q1)
        exp_q1 = np.exp(beta1 * q1_shift)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Softmax policy at stage-2 (state-specific)
        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta2 * q2_shift)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF TD from stage-2 value and eligibility trace from reward PE
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1
        q1_mf[a1] += alpha * lam * delta2

    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid model with anxiety-modulated perseveration at stage 1.
    
    The model learns the transition structure online and uses it for model-based evaluation.
    It combines MB with MF values implicitly via softmax over MB values, while MF influences
    both stages via standard TD learning. Additionally, there is a perseveration bias at
    stage 1 that increases with higher STAI (greater tendency to repeat the previous action).
    
    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like, shape (n_trials,)
        Second-stage states (0 = planet X, 1 = planet Y) actually reached after action_1.
    action_2 : array-like, shape (n_trials,)
        Second-stage choices (0/1; X: W/S, Y: P/H).
    reward : array-like, shape (n_trials,)
        Obtained reward (e.g., coins; typically 0/1).
    stai : array-like, shape (1,) or scalar-like
        Participant STAI score in [0,1].
    model_parameters : iterable
        Model parameters (total = 5):
        - alpha_r in [0,1]: reward learning rate for Q updates
        - alpha_t in [0,1]: transition learning rate for updating transition beliefs
        - beta1 in [0,10]: inverse temperature at stage 1
        - beta2 in [0,10]: inverse temperature at stage 2
        - kappa in [0,1]: baseline perseveration strength at stage 1 (bias to repeat last a1)
          Effective perseveration = kappa * (0.5 + STAI), so higher anxiety increases bias.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_r, alpha_t, beta1, beta2, kappa = model_parameters
    n_trials = len(action_1)
    stai_val = float(np.asarray(stai)[0])

    # Initialize transition belief: rows A,U; cols X,Y
    T = np.full((2, 2), 0.5, dtype=float)

    # Value functions
    q1_mf = np.zeros(2)           # model-free stage-1 values
    q2 = np.zeros((2, 2))         # stage-2 values

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration strength modulated by STAI
    kappa_eff = kappa * (0.5 + stai_val)
    last_a1 = None

    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Update transition belief for the chosen action using alpha_t
        # Move the row for chosen action toward one-hot of observed state
        one_hot = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0], dtype=float)
        T[a1] = (1.0 - alpha_t) * T[a1] + alpha_t * one_hot
        # Ensure row normalization (guard against numerical drift)
        T[a1] = T[a1] / np.sum(T[a1])

        # Model-based stage-1 value via expected max Q2 under learned T
        max_q2 = np.max(q2, axis=1)  # over actions at each state
        q1_mb = T @ max_q2

        # Combine with perseveration bias at stage 1
        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] += kappa_eff

        # Softmax at stage 1 over MB values plus bias (MF contributes through learning updates)
        q1_input = q1_mb + bias
        q1_shift = q1_input - np.max(q1_input)
        exp_q1 = np.exp(beta1 * q1_shift)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Softmax at stage 2 (state-specific)
        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta2 * q2_shift)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning (model-free)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * delta1

        last_a1 = a1

    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Directed exploration at stage 2 with anxiety-modulated uncertainty bonus and stickiness.
    
    This model is primarily model-free, but it adds a directed exploration (UCB-like) bonus at
    stage 2 that is stronger when anxiety (STAI) is lower. It also includes a choice stickiness
    bias at both stages. The first stage uses a model-based evaluation from the known transition
    structure combined with stickiness; the second stage uses MF values plus an uncertainty bonus.
    
    Parameters
    ----------
    action_1 : array-like, shape (n_trials,)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like, shape (n_trials,)
        Second-stage states (0 = planet X, 1 = planet Y) actually reached after action_1.
    action_2 : array-like, shape (n_trials,)
        Second-stage choices (0/1; X: W/S, Y: P/H).
    reward : array-like, shape (n_trials,)
        Obtained reward (e.g., coins; typically 0/1).
    stai : array-like, shape (1,) or scalar-like
        Participant STAI score in [0,1]. Lower values imply stronger exploration bonus here.
    model_parameters : iterable
        Model parameters (total = 5):
        - alpha in [0,1]: learning rate for Q updates at stage 2 (and bootstrapped stage 1 MF)
        - beta1 in [0,10]: inverse temperature at stage 1
        - beta2 in [0,10]: inverse temperature at stage 2
        - bonus_scale in [0,1]: base magnitude of directed exploration bonus
        - rho in [0,1]: choice stickiness strength (applied at both stages to repeat last action)

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta1, beta2, bonus_scale, rho = model_parameters
    n_trials = len(action_1)
    stai_val = float(np.asarray(stai)[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and counts
    q1_mf = np.zeros(2)          # stage-1 MF values for (A,U), used for bootstrapping only
    q2 = np.zeros((2, 2))        # stage-2 state-action values
    n_visits = np.ones((2, 2))   # visit counts per state-action (start at 1 to avoid div by zero)

    # Stickiness trackers
    last_a1 = None
    last_a2 = {0: None, 1: None}  # per-state stickiness at stage 2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # Anxiety-modulated directed exploration: lower STAI -> larger bonus
    bonus_eff = bonus_scale * (1.0 - stai_val)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 evaluation: MB expected value + stickiness bias
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += rho

        q1_input = q1_mb + bias1
        q1_shift = q1_input - np.max(q1_input)
        exp_q1 = np.exp(beta1 * q1_shift)
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: MF values + uncertainty bonus (UCB-like) + stickiness
        # Bonus inversely proportional to sqrt of visits in the current state
        bonus = bonus_eff / np.sqrt(n_visits[s])
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += rho

        q2_input = q2[s] + bonus + bias2
        q2_shift = q2_input - np.max(q2_input)
        exp_q2 = np.exp(beta2 * q2_shift)
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Increment visit counts after observing and choosing at stage 2
        n_visits[s, a2] += 1.0

        # Stage-1 MF bootstrapping towards stage-2 chosen value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update stickiness trackers
        last_a1 = a1
        last_a2[s] = a2

    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)