def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration, eligibility trace, and stickiness.
    
    Parameters
    ----------
    action_1 : 1D array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) per trial.
    state : 1D array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : 1D array-like of int (0 or 1)
        Second-stage choices (alien index within the reached planet) per trial.
    reward : 1D array-like of float (0 or 1)
        Obtained coins per trial.
    stai : 1D array-like of float in [0,1]
        Participant STAI score; higher means higher anxiety. Used to modulate arbitration and stickiness.
    model_parameters : iterable of floats
        Parameters (bounds):
          - alpha in [0,1]: learning rate for action values
          - beta in [0,10]: inverse temperature for softmax
          - w0 in [0,1]: base model-based weight (anxiety reduces it)
          - lam in [0,1]: eligibility trace mixing second-stage PE into first-stage MF update
          - pers in [0,1]: base first-stage choice stickiness strength (scaled up by anxiety)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w0, lam, pers = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities of the actually observed choices (for NLL)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)          # model-free first-stage Q
    q_stage2_mf = np.zeros((2, 2))     # model-free second-stage Q per state
    prev_choice1 = None                # for stickiness

    for t in range(n_trials):

        # Model-based first-stage Q from current second-stage MF values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)     # best alien per planet
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Anxiety-modulated arbitration and stickiness
        # Higher anxiety reduces reliance on model-based control and increases stickiness
        w_eff = np.clip(w0 * (1.0 - 0.6 * stai), 0.0, 1.0)
        kappa = pers * (0.5 + stai)  # stickiness scales up with anxiety

        # Combine MB and MF values
        q1_combined = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add stickiness bias to first-stage preferences
        pref1 = beta * q1_combined
        if prev_choice1 is not None:
            stick = np.zeros(2)
            stick[prev_choice1] = 1.0
            pref1 = pref1 + kappa * stick

        # First-stage policy and likelihood
        exp_q1 = np.exp(pref1 - np.max(pref1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy and likelihood (state is observed)
        s = state[t]
        pref2 = beta * q_stage2_mf[s]
        exp_q2 = np.exp(pref2 - np.max(pref2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Prediction errors
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]          # bootstrapped MF TD error at stage 1
        delta2 = reward[t] - q_stage2_mf[s, a2]                # reward PE at stage 2

        # Updates
        # Second-stage MF update
        q_stage2_mf[s, a2] += alpha * delta2
        # First-stage MF update with eligibility trace from stage 2
        q_stage1_mf[a1] += alpha * (delta1 + lam * delta2)

        # Book-keeping
        prev_choice1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pearce-Hall associability model: trial-by-trial learning rate adapts to surprise, scaled by anxiety.
    
    Stage 1 choice uses an MB/MF hybrid; Stage 2 MF values update with associability-driven learning rates.
    Anxiety increases the gain of associability, reducing reliance on MB planning.
    
    Parameters
    ----------
    action_1 : 1D array-like of int (0 or 1)
        First-stage choices per trial.
    state : 1D array-like of int (0 or 1)
        Second-stage state per trial.
    action_2 : 1D array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : 1D array-like of float (0 or 1)
        Rewards per trial.
    stai : 1D array-like of float in [0,1]
        STAI score; scales associability and reduces MB weight.
    model_parameters : iterable of floats
        Parameters (bounds):
          - alpha0 in [0,1]: base learning rate for action values
          - kappa in [0,1]: associability update rate (how quickly associability tracks |PE|)
          - beta in [0,10]: inverse temperature for softmax
          - w in [0,1]: base MB weight at stage 1 (anxiety reduces it)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha0, kappa, beta, w = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values and associability
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))
    associability = np.zeros((2, 2))  # per-state, per-alien associability (initialized at 0)

    for t in range(n_trials):

        # Model-based component at stage 1
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Anxiety reduces MB arbitration weight
        w_eff = np.clip(w * (1.0 - 0.5 * stai), 0.0, 1.0)
        q1_combined = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Stage 1 policy
        pref1 = beta * q1_combined
        exp_q1 = np.exp(pref1 - np.max(pref1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        pref2 = beta * q_stage2_mf[s]
        exp_q2 = np.exp(pref2 - np.max(pref2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Stage 2 PE and associability
        pe2 = reward[t] - q_stage2_mf[s, a2]
        # Update associability toward |PE|, amplified by anxiety
        associability[s, a2] = (1.0 - kappa) * associability[s, a2] + kappa * abs(pe2)
        # Trial-specific learning rate, boosted by both associability and anxiety
        alpha_t = np.clip(alpha0 * (1.0 + stai * associability[s, a2]), 0.0, 1.0)

        # Stage 2 MF update
        q_stage2_mf[s, a2] += alpha_t * pe2

        # Stage 1 MF bootstrapping from updated stage 2 values
        pe1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # Let stage 1 learning also adapt mildly to associability, with reduced anxiety gain
        alpha1_t = np.clip(alpha0 * (1.0 + 0.5 * stai * associability[s, a2]), 0.0, 1.0)
        q_stage1_mf[a1] += alpha1_t * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid model with anxiety-sensitive uncertainty aversion and eligibility trace.
    
    The agent learns transition probabilities and uses them for model-based planning.
    Anxiety induces aversion to transition uncertainty at stage 1 (penalizing high-entropy actions),
    and modulates the eligibility trace credit assignment from stage 2 to stage 1.
    
    Parameters
    ----------
    action_1 : 1D array-like of int (0 or 1)
        First-stage choices per trial.
    state : 1D array-like of int (0 or 1)
        Second-stage state per trial.
    action_2 : 1D array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : 1D array-like of float (0 or 1)
        Rewards per trial.
    stai : 1D array-like of float in [0,1]
        STAI score; higher anxiety increases uncertainty aversion and eligibility trace.
    model_parameters : iterable of floats
        Parameters (bounds):
          - alpha_r in [0,1]: reward learning rate for MF Q updates
          - alpha_t in [0,1]: transition learning rate
          - beta in [0,10]: inverse temperature
          - w in [0,1]: MB weight mixing learned MB values with MF at stage 1
          - lam in [0,1]: base eligibility trace (scaled up by anxiety)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_t, beta, w, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model T_hat[action, state]
    T_hat = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for t in range(n_trials):

        # Model-based Q using learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)   # best alien per planet
        q_stage1_mb = T_hat @ max_q_stage2

        # Uncertainty aversion term: for each action, compute transition uncertainty
        # Using quadratic uncertainty u = 4 p (1 - p) which is maximal at 0.5
        pA = T_hat[0, 0]
        pU = T_hat[1, 1]
        uA = 4.0 * pA * (1.0 - pA)
        uU = 4.0 * pU * (1.0 - pU)
        uncertainty_penalty = np.array([uA, uU])
        # Anxiety penalizes uncertainty (subtract from preferences)
        unc_weight = stai  # stronger penalty with higher anxiety

        # Combine MB and MF, then apply uncertainty penalty in preference space
        q1_combined = w * q_stage1_mb + (1.0 - w) * q_stage1_mf
        pref1 = beta * q1_combined - unc_weight * uncertainty_penalty

        # First-stage policy
        exp_q1 = np.exp(pref1 - np.max(pref1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        pref2 = beta * q_stage2_mf[s]
        exp_q2 = np.exp(pref2 - np.max(pref2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward PE and updates
        pe2 = reward[t] - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * pe2

        pe1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        # Anxiety increases eligibility trace influence from stage 2 onto stage 1
        lam_eff = np.clip(lam * (0.5 + 0.5 * stai), 0.0, 1.0)
        q_stage1_mf[a1] += alpha_r * (pe1 + lam_eff * pe2)

        # Transition learning for the chosen first-stage action
        # Move T_hat[a1] toward the observed state (one-hot target)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T_hat[a1] = (1.0 - alpha_t) * T_hat[a1] + alpha_t * target

        # Keep rows normalized (they are by construction) and bounded
        T_hat[a1] = np.clip(T_hat[a1], 1e-6, 1.0 - 1e-6)
        T_hat[a1] /= np.sum(T_hat[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll