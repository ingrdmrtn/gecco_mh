def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free SARSA(Î») with anxiety-modulated arbitration.
    
    This model blends model-based and model-free values at stage 1. The arbitration
    weight is modulated by the participant's anxiety (stai), consistent with evidence
    that higher anxiety reduces model-based control. Model-free values are updated
    via an eligibility trace from stage 2. Choices at both stages are generated
    via a softmax with inverse temperature beta.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for value updates
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = lam (0 to 1): eligibility trace strength from stage 2 to stage 1
    - model_parameters[3] = w0 (0 to 1): baseline weight on model-based values at stage 1
    - model_parameters[4] = gamma (0 to 1): strength of anxiety modulation on model-based weight
      Effective weight: w_eff = (1 - gamma) * w0 + gamma * (1 - stai), so higher anxiety reduces w

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha, beta, lam, w0, gamma = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Value stores
    q_stage1_mf = np.zeros(2)          # model-free Q at stage 1 (spaceships)
    q_stage2 = np.zeros((2, 2))        # model-free Q at stage 2 (aliens within planet)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight (higher anxiety -> lower MB weight)
    w_eff = (1.0 - gamma) * w0 + gamma * (1.0 - stai_val)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based evaluation for stage 1
        max_q_stage2 = np.max(q_stage2, axis=1)              # best alien per planet
        q_stage1_mb = transition_matrix @ max_q_stage2       # MB value for each spaceship

        # Hybrid action values
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Stage 1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        q2_s = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage 2 TD error and update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage 1 model-free update via eligibility trace from stage 2
        # Propagate the stage-2 prediction error backward
        q_stage1_mf[a1] += alpha * lam * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free asymmetric learning with anxiety-modulated learning and perseveration.
    
    This model is a purely model-free SARSA-style learner. It features:
    - Asymmetric learning rates for positive vs negative outcomes.
    - Anxiety (stai) increases sensitivity to negative outcomes (higher learning from losses)
      and decreases learning from positive outcomes.
    - Learning is further up-weighted on rare transitions, scaled by the same anxiety factor.
    - First-stage perseveration bias encourages repeating the previous first-stage choice,
      with bias strength increasing with anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha_pos_base (0 to 1): baseline learning rate for positive PE
    - model_parameters[1] = alpha_neg_base (0 to 1): baseline learning rate for negative PE
    - model_parameters[2] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[3] = kappa (0 to 1): baseline perseveration bias magnitude at stage 1
    - model_parameters[4] = gamma_anx (0 to 1): strength of anxiety modulation for learning and transition rarity

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha_pos_base, alpha_neg_base, beta, kappa, gamma_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Transition structure for identifying rare transitions
    # Common: A->X, U->Y
    def is_rare(a1, s):
        return (a1 == 0 and s == 1) or (a1 == 1 and s == 0)

    # Values
    q1_mf = np.zeros(2)        # stage-1 MF values
    q2 = np.zeros((2, 2))      # stage-2 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated perseveration strength (more anxiety -> more stickiness)
    prev_a1 = None

    for t in range(n_trials):
        # Perseveration bias vector over stage-1 actions
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = kappa * stai_val

        # Stage 1 policy (with perseveration bias)
        q1_eff = q1_mf + bias
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        q2_s = q2[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and rarity
        r = reward[t]
        rare = 1.0 if is_rare(a1, s) else 0.0

        # Anxiety-modulated asymmetric learning rates
        alpha_pos = alpha_pos_base * (1.0 - gamma_anx * stai_val)
        alpha_neg = alpha_neg_base * (1.0 + gamma_anx * stai_val)
        # Up-weight learning on rare transitions
        alpha_pos *= (1.0 + gamma_anx * rare)
        alpha_neg *= (1.0 + gamma_anx * rare)
        # Clip to [0,1]
        alpha_pos = float(np.clip(alpha_pos, 0.0, 1.0))
        alpha_neg = float(np.clip(alpha_neg, 0.0, 1.0))

        # Stage 2 update
        pe2 = r - q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * pe2

        # Stage 1 update driven by downstream PE (no explicit lambda)
        pe1 = pe2
        lr1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        q1_mf[a1] += lr1 * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Kalman-filtered model-based planning with anxiety-modulated volatility and stickiness.
    
    Stage-2 values are tracked with a simple Kalman filter per state-action,
    capturing gradual reward drift. The process noise (volatility) is increased
    by anxiety (stai), yielding larger Kalman gains and faster updating when anxious.
    First-stage choices use model-based planning through the known transition matrix.
    A small perseveration bias at stage 1 is stronger for lower anxiety (assuming
    anxious participants rely less on habits in this variant).

    Parameters (bounds):
    - model_parameters[0] = v_base (0 to 1): baseline process noise (volatility) of rewards
    - model_parameters[1] = r_base (0 to 1): observation noise
    - model_parameters[2] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[3] = kappa (0 to 1): baseline perseveration bias magnitude at stage 1
    - model_parameters[4] = eta (0 to 1): strength of anxiety modulation on process noise
      Effective volatility: v_eff = v_base * (1 + eta * stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    v_base, r_base, beta, kappa, eta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Kalman filter parameters
    v_eff = v_base * (1.0 + eta * stai_val)   # anxiety increases assumed volatility
    r_eff = r_base

    # Means and variances for stage-2 rewards
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 1.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Model-based stage-1 values from current stage-2 means
        max_q2 = np.max(q2_mean, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Perseveration bias stronger for lower anxiety in this variant
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = kappa * (1.0 - stai_val)

        # Stage 1 policy
        q1_eff = q1_mb + bias
        exp_q1 = np.exp(beta * (q1_eff - np.max(q1_eff)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy from current means
        s = state[t]
        q2_s_mean = q2_mean[s]
        exp_q2 = np.exp(beta * (q2_s_mean - np.max(q2_s_mean)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Kalman update for chosen state-action
        prior_var = q2_var[s, a2] + v_eff
        gain = prior_var / (prior_var + r_eff + 1e-12)
        pred = q2_mean[s, a2]
        q2_mean[s, a2] = pred + gain * (r - pred)
        q2_var[s, a2] = (1.0 - gain) * prior_var

        # Optional small diffusion for unchosen pairs to reflect drift
        # (kept small to avoid dominating; ensures volatility meaningfully applied)
        for ss in range(2):
            for aa in range(2):
                if not (ss == s and aa == a2):
                    q2_var[ss, aa] = q2_var[ss, aa] + 0.1 * v_eff

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll