def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and first-stage stickiness.
    
    This model blends model-free (MF) and model-based (MB) values for the stage-1 choice.
    Anxiety (stai) down-weights the MB contribution via an anx_gain parameter.
    A first-stage perseveration bias (kappa) adds a bonus to repeating the previous stage-1 action.
    Second-stage decisions are purely MF with softmax.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for MF updates at both stages.
    - beta: [0,10] inverse temperature for softmax at both stages.
    - w0: [0,1] baseline MB weight (before anxiety modulation).
    - kappa: [0,1] perseveration strength for repeating the previous stage-1 action.
    - anx_gain: [0,1] scales how much anxiety reduces MB weight; w_eff = clip(w0*(1 - anx_gain*stai)).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array of ints in {0,1}, reached planet (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on that planet (0 vs 1).
    - reward: array of floats, obtained coins per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, w0, kappa, anx_gain].
    
    Returns:
    - Negative log-likelihood of the observed stage-1 and stage-2 choices under the model.
    """
    alpha, beta, w0, kappa, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q_stage1_mf = np.zeros(2)          # values for A vs U
    q_stage2_mf = np.zeros((2, 2))     # rows planets X/Y, cols aliens

    # Perseveration memory
    prev_a1 = None

    # Anxiety-modulated arbitration weight
    w_eff = w0 * (1.0 - anx_gain * stai)
    w_eff = min(1.0, max(0.0, w_eff))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Model-based stage-1 Q: expected max MF value at stage-2 via transition model
        max_q_stage2 = np.max(q_stage2_mf, axis=1)          # size 2 (planets)
        q_stage1_mb = transition_matrix @ max_q_stage2      # size 2 (spaceships)

        # Hybrid Q for stage-1
        q1_hybrid = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add perseveration bias for repeating previous stage-1 choice
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += kappa

        # Stage-1 policy
        logits1 = beta * q1_hybrid + bias
        logits1 -= np.max(logits1)  # numerical stability
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure MF)
        logits2 = beta * q_stage2_mf[s, :]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # TD errors
        # Stage-2 update (MF)
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update using a "bootstrapped" signal from stage-2 chosen action
        # and an eligibility component from outcome
        # First, immediate bootstrapped update towards the chosen second-stage value
        target1 = q_stage2_mf[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Outcome-eligibility bonus: propagate part of outcome back to stage-1
        q_stage1_mf[a1] += alpha * 0.5 * delta2  # fixed lambda-like factor 0.5

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with eligibility traces, anxiety-modulated exploration and stickiness.
    
    This model is a two-stage MF RL with eligibility trace lambda for credit assignment
    from stage-2 outcomes to stage-1 values. Anxiety reduces the effective inverse temperature
    (more exploratory under higher anxiety) and also reduces the effective lambda.
    A perseveration bias applies to both stages.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for stage-2 and stage-1 updates.
    - beta: [0,10] baseline inverse temperature (before anxiety modulation).
    - lam0: [0,1] baseline eligibility trace parameter.
    - gamma_anx: [0,1] scales how much anxiety reduces inverse temperature: beta_eff = beta*(1 - gamma_anx*stai).
    - persev: [0,1] perseveration strength added to the previously chosen action at each stage.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship.
    - state: array of ints in {0,1}, reached planet.
    - action_2: array of ints in {0,1}, chosen alien.
    - reward: array of floats, coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, lam0, gamma_anx, persev].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, lam0, gamma_anx, persev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective inverse temperature and eligibility under anxiety
    beta_eff = beta * (1.0 - gamma_anx * stai)
    # Keep positive range
    beta_eff = max(0.0, min(10.0, beta_eff))
    lam_eff = lam0 * (1.0 - 0.5 * stai)
    lam_eff = min(1.0, max(0.0, lam_eff))

    # Q-values
    q1 = np.zeros(2)          # stage-1 MF Q
    q2 = np.zeros((2, 2))     # stage-2 MF Q by state

    # Probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 policy with perseveration
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += persev
        logits1 = beta_eff * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration (state-specific)
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += persev
        logits2 = beta_eff * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # TD updates
        # Stage-2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 with eligibility trace from outcome
        # Update towards immediate second-stage value and propagate outcome with lambda
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        q1[a1] += alpha * delta1
        q1[a1] += alpha * lam_eff * delta2

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid with anxiety-driven transition uncertainty and perseveration.
    
    This model assumes participants may misrepresent the transition structure.
    Anxiety increases transition noise, mixing the true transition matrix with a uniform matrix.
    Stage-1 decisions are a hybrid of model-based (using the noisy transition belief)
    and model-free values, with a fixed MB weight omega. Perseveration bias applies to stage-1.
    Stage-2 is purely MF.
    
    Parameters (model_parameters):
    - alpha: [0,1] MF learning rate.
    - beta: [0,10] inverse temperature for softmax.
    - phi: [0,1] scales anxiety-driven transition noise; noise = phi * stai.
    - omega: [0,1] weight of MB values in the hybrid for stage-1.
    - persev: [0,1] perseveration strength for repeating stage-1 action.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship.
    - state: array of ints in {0,1}, reached planet.
    - action_2: array of ints in {0,1}, chosen alien.
    - reward: array of floats, coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, phi, omega, persev].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, phi, omega, persev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # True transition matrix
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]])
    # Anxiety-driven transition noise: mix with uniform (0.5,0.5) per row
    noise = phi * stai
    noise = min(1.0, max(0.0, noise))
    T_uniform = np.array([[0.5, 0.5],
                          [0.5, 0.5]])
    T_belief = (1.0 - noise) * T_true + noise * T_uniform

    # Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # MB evaluation with noisy transition belief
        max_q2 = np.max(q2_mf, axis=1)      # value of best alien per planet
        q1_mb = T_belief @ max_q2

        # Hybrid Q for stage-1
        q1_h = omega * q1_mb + (1.0 - omega) * q1_mf

        # Perseveration on stage-1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += persev

        # Stage-1 choice
        logits1 = beta * q1_h + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= (np.sum(p1) + 1e-16)
        p_choice_1[t] = p1[a1]

        # Stage-2 choice (MF)
        logits2 = beta * q2_mf[s, :]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= (np.sum(p2) + 1e-16)
        p_choice_2[t] = p2[a2]

        # MF updates
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # First-stage MF update: towards second-stage chosen value + eligibility from outcome
        target1 = q2_mf[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1
        q1_mf[a1] += alpha * 0.5 * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll