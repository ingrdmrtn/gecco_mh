def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration and stickiness.
    
    This model blends a model-based first-stage value with a model-free value and adds
    an anxiety-scaled choice stickiness bias at both stages. Second-stage values are
    learned with a standard TD rule; the first-stage model-free value is updated via
    an eligibility trace from the second stage. Anxiety (stai) reduces the model-based
    weight and increases perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0/1)
        Observed second-stage state (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0/1)
        Second-stage choices within the visited state (two aliens per planet).
    reward : array-like of float
        Rewards obtained on each trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score scaled to [0,1]; only the first element is used.
    model_parameters : list/tuple of floats
        [alpha, beta, w0, lam, stick]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - w0 in [0,1]: baseline model-based weight at stage 1 (reduced by anxiety).
        - lam in [0,1]: eligibility trace weighting from stage 2 to stage 1 MF value.
        - stick in [0,1]: baseline stickiness magnitude (scaled up by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w0, lam, stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # True transition structure (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free action values
    q1_mf = np.zeros(2)        # first-stage MF
    q2_mf = np.zeros((2, 2))   # second-stage MF: state x action

    # Stickiness traces (last choices); initialize to "no bias"
    last_a1 = None
    last_a2 = {0: None, 1: None}

    # Anxiety-modulated arbitration (reduce MB as anxiety increases)
    w_mb = max(0.0, min(1.0, w0 * (1.0 - stai)))  # stays within [0,1]
    # Anxiety-modulated stickiness scale
    stick_scale = stick * stai

    for t in range(n_trials):
        # Model-based first-stage Q via one-step lookahead
        max_q2 = np.max(q2_mf, axis=1)  # best action per state
        q1_mb = transition_matrix @ max_q2

        # Hybrid Q at stage 1
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add stickiness bias at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stick_scale

        # Softmax policy stage 1
        logits1 = beta * q1_hybrid + bias1
        # numerically stable softmax
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])

        p_choice_1[t] = probs1[a1]

        # Second stage policy: softmax over MF Q with stickiness within state
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += stick_scale

        logits2 = beta * q2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Stage 2 TD update (model-free)
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage 1 MF update via eligibility trace from stage 2 value
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * lam * delta1

        # Update stickiness memories
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-sensitive hybrid RL with valence-asymmetric learning and anxiety effects.

    This model uses separate learning rates for positive vs. negative second-stage
    prediction errors, and adjusts credit assignment and arbitration depending on
    whether the transition was common or rare. Anxiety reduces model-based arbitration
    and down-weights eligibility after rare transitions.

    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0/1)
        Observed second-stage state (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0/1)
        Second-stage choices within the visited state (two aliens per planet).
    reward : array-like of float
        Rewards obtained on each trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score scaled to [0,1]; only the first element is used.
    model_parameters : list/tuple of floats
        [alpha_pos, alpha_neg, beta, omega0, persev]
        - alpha_pos in [0,1]: learning rate when second-stage PE is positive.
        - alpha_neg in [0,1]: learning rate when second-stage PE is negative.
        - beta in [0,10]: inverse temperature for both stages.
        - omega0 in [0,1]: baseline model-based weight at stage 1.
        - persev in [0,1]: baseline perseveration bias (scaled by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, omega0, persev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Perseveration traces
    last_a1 = None
    last_a2 = {0: None, 1: None}

    # Anxiety reduces MB arbitration weight
    omega = max(0.0, min(1.0, omega0 * (1.0 - stai)))
    # Perseveration magnitude increases with anxiety
    persev_scale = persev * stai

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Determine if transition was common or rare given the chosen ship
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

        # Model-based planning at stage 1
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid Q1 with anxiety-reduced MB weight
        q1_hybrid = omega * q1_mb + (1.0 - omega) * q1_mf

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += persev_scale

        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second stage policy with perseveration within state
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += persev_scale

        logits2 = beta * q2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Valence-asymmetric second-stage learning
        pe2 = r - q2_mf[s, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        # Anxiety modestly reduces learning rate overall
        lr2_eff = lr2 * (1.0 - 0.5 * stai)
        q2_mf[s, a2] += lr2_eff * pe2

        # Eligibility trace with transition sensitivity and anxiety
        # After rare transitions, anxious participants discount credit assignment
        lam_eff = 1.0 if is_common else (1.0 - stai)
        pe1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += lr2_eff * lam_eff * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-adjusted transition belief, softmax gain, and forgetting.

    This model assumes the agent plans with an internal transition belief that may be
    biased toward ambiguity with higher anxiety. It also includes anxiety-amplified
    softmax gain and global forgetting. Second-stage values are learned by TD; the
    first-stage action values are computed purely model-based from the believed
    transition matrix.

    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0/1)
        Observed second-stage state (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0/1)
        Second-stage choices within the visited state (two aliens per planet).
    reward : array-like of float
        Rewards obtained on each trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score scaled to [0,1]; only the first element is used.
    model_parameters : list/tuple of floats
        [alpha, beta, k_beta, rho, decay]
        - alpha in [0,1]: TD learning rate at the second stage.
        - beta in [0,10]: base inverse temperature.
        - k_beta in [0,1]: anxiety gain on inverse temperature (beta_eff = beta*(1+k_beta*stai)).
        - rho in [0,1]: confidence in common transitions (maps to p_common belief).
        - decay in [0,1]: global forgetting factor per trial (scaled up by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, k_beta, rho, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-adjusted transition belief:
    # p_common ranges from 0.5 (ambiguous) to 0.7 (true) depending on rho and reduced by anxiety
    p_common = 0.5 + 0.2 * rho * (1.0 - stai)
    p_rare = 1.0 - p_common
    T_belief = np.array([[p_common, p_rare],
                         [p_rare,  p_common]])

    # Anxiety-amplified softmax gain (capped implicitly by problem constraints on beta input)
    beta_eff = beta * (1.0 + k_beta * stai)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Only second-stage is learned; first-stage is computed from believed transitions
    q2 = np.zeros((2, 2))  # state x action

    # Effective forgetting rate increases with anxiety
    forget = decay * (0.5 + 0.5 * stai)

    for t in range(n_trials):
        # Apply global forgetting before acting
        q2 *= (1.0 - forget)

        # Compute MB first-stage Q from transition belief and current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_belief @ max_q2

        # Stage 1 policy
        logits1 = beta_eff * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Second-stage TD update with anxiety-stable learning rate
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)