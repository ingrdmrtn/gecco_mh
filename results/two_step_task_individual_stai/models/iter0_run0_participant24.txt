def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration and eligibility trace.
    
    This model blends a model-based (MB) plan using the known transition matrix with a model-free (MF)
    system. The arbitration weight favoring MB control decreases with anxiety (stai). An eligibility
    trace propagates the reward prediction error from stage 2 back to stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., coins; typically 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, w_base, lam, gamma_anx]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - w_base in [0,1]: baseline MB weight in the hybrid action value.
        - lam in [0,1]: eligibility trace scaling the backpropagation of the stage-2 RPE to stage 1.
        - gamma_anx in [0,1]: strength by which anxiety (stai) shifts control toward MF (reduces MB weight).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w_base, lam, gamma_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q1_mf = np.zeros(2)         # stage-1 MF values for actions A/U
    q2 = np.zeros((2, 2))       # stage-2 MF values for states X/Y and their two actions

    # Anxiety-modulated arbitration weight (more anxiety -> more MF)
    w_mb = np.clip(w_base - gamma_anx * s, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based Q at stage 1: expected max over next-state Q2 under transitions
        max_q2_per_state = np.max(q2, axis=1)              # shape (2,)
        q1_mb = transition_matrix @ max_q2_per_state       # shape (2,)

        # Hybrid Q for stage 1 policy
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy and probability of observed choice
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy conditioned on observed state
        st = state[t]
        exp_q2 = np.exp(beta * (q2[st] - np.max(q2[st])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD error and update
        r = reward[t]
        delta2 = r - q2[st, a2]
        q2[st, a2] += alpha * delta2

        # Stage-1 MF TD error: bootstrap from the chosen stage-2 action value
        delta1 = q2[st, a2] - q1_mf[a1]
        # Backpropagate both via direct TD(0) and eligibility-trace-scaled stage-2 RPE
        q1_mf[a1] += alpha * (delta1 + lam * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-free SARSA(0) with anxiety-modulated learning asymmetry and choice stickiness.
    
    This model learns MF values for both stages. Learning rates differ for positive vs. negative
    prediction errors, and anxiety shifts the positivity bias (higher anxiety reduces the difference).
    Choice policies at both stages include a perseveration (stickiness) bias that grows with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha_pos, alpha_neg, beta, kappa0, kappa_stai]
        - alpha_pos in [0,1]: base learning rate for positive prediction errors.
        - alpha_neg in [0,1]: base learning rate for negative prediction errors.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa0 in [0,1]: baseline perseveration bias added to the last chosen action.
        - kappa_stai in [0,1]: how much anxiety increases perseveration (stickiness).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, kappa0, kappa_stai = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # MF action values
    q1 = np.zeros(2)            # stage-1 MF values
    q2 = np.zeros((2, 2))       # stage-2 MF values

    # Perseveration state
    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    # Anxiety-modulated stickiness
    kappa = kappa0 + kappa_stai * s

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Build stickiness biases
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        st = state[t]
        bias2 = np.zeros(2)
        if last_a2_by_state[st] is not None:
            bias2[last_a2_by_state[st]] += kappa

        # Stage-1 policy with stickiness
        prefs1 = q1 + bias1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness (within current state)
        prefs2 = q2[st] + bias2
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning with anxiety-modulated asymmetry:
        # Anxiety s blends the two alphas, reducing positivity bias at higher s
        # Effective alphas for this trial:
        # For positive PE: move alpha_pos toward alpha_neg as s increases
        eff_alpha_pos = (1 - s) * alpha_pos + s * alpha_neg
        # For negative PE: move alpha_neg toward alpha_pos as s increases
        eff_alpha_neg = (1 - s) * alpha_neg + s * alpha_pos

        # Stage-2 update
        r = reward[t]
        pe2 = r - q2[st, a2]
        a2_lr = eff_alpha_pos if pe2 >= 0 else eff_alpha_neg
        q2[st, a2] += a2_lr * pe2

        # Stage-1 update: bootstrap from chosen stage-2 value (pure MF SARSA(0))
        pe1 = q2[st, a2] - q1[a1]
        a1_lr = eff_alpha_pos if pe1 >= 0 else eff_alpha_neg
        q1[a1] += a1_lr * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2_by_state[st] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with learned transitions, anxiety-modulated reward sensitivity and lapse.
    
    This model learns the first-stage transition probabilities and plans by projecting second-stage
    values through the learned transition model. Anxiety increases subjective reward sensitivity and
    also increases a choice lapse rate (randomness) that mixes the softmax policy with uniform choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha_r, beta, alpha_t, rho0, eps0]
        - alpha_r in [0,1]: learning rate for second-stage value updates.
        - beta in [0,10]: inverse temperature for softmax.
        - alpha_t in [0,1]: learning rate for first-stage transition probabilities.
        - rho0 in [0,1]: baseline reward sensitivity (scales reward).
        - eps0 in [0,1]: baseline lapse rate that is scaled by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, beta, alpha_t, rho0, eps0 = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize learned transition model close to uninformative (0.5/0.5)
    T = np.full((2, 2), 0.5)  # rows: actions (A,U), cols: states (X,Y)

    # Second-stage values
    q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    rho = np.clip(rho0 * (0.5 + 0.5 * s), 0.0, 1.0)       # higher anxiety -> higher reward impact
    eps_lapse = np.clip(eps0 * (0.2 + 0.8 * s), 0.0, 0.49)  # higher anxiety -> larger lapse

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based Q at stage 1 from learned transitions
        max_q2 = np.max(q2, axis=1)     # per state
        q1_mb = T @ max_q2

        # Stage-1 softmax with lapse
        logits1 = beta * (q1_mb - np.max(q1_mb))
        exp1 = np.exp(logits1)
        soft1 = exp1 / np.sum(exp1)
        probs1 = (1 - eps_lapse) * soft1 + eps_lapse * np.array([0.5, 0.5])
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with lapse (within current state)
        st = state[t]
        logits2 = beta * (q2[st] - np.max(q2[st]))
        exp2 = np.exp(logits2)
        soft2 = exp2 / np.sum(exp2)
        probs2 = (1 - eps_lapse) * soft2 + eps_lapse * np.array([0.5, 0.5])
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward with anxiety-modulated sensitivity
        r = rho * reward[t]

        # Update second-stage value
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_r * pe2

        # Update transition model for chosen action based on observed state
        # Move probability of observed transition toward 1, the alternative toward 0
        T[a1, st] += alpha_t * (1.0 - T[a1, st])
        other = 1 - st
        T[a1, other] += alpha_t * (0.0 - T[a1, other])

        # Keep rows within [0,1] and normalized (optional normalization to reduce drift)
        T[a1] = np.clip(T[a1], 0.001, 0.999)
        T[a1] /= np.sum(T[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll