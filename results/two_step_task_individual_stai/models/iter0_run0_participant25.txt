def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and eligibility traces.
    
    This model blends model-based (MB) and model-free (MF) control at stage 1,
    uses a softmax policy at both stages, and learns with TD(λ). Anxiety (stai)
    down-weights model-based control and increases perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) on each trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y) on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., alien indices) on each trial.
    reward : array-like of float
        Reward received on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Anxiety score(s). Uses the first element as the participant's anxiety.
        Interpretation in this model:
          - Higher stai reduces model-based weight and increases perseveration strength.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, w_mb, lam, kappa)
        - alpha in [0,1]: learning rate.
        - beta in [0,10]: inverse temperature for softmax.
        - w_mb in [0,1]: baseline model-based weight (before anxiety).
        - lam in [0,1]: eligibility trace parameter λ.
        - kappa in [0,1]: baseline perseveration weight.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, w_mb, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0]) if hasattr(stai, "__len__") else float(stai)

    # Transition: rows are first-stage actions (0=A,1=U), columns are states (0=X,1=Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage1_mf = np.zeros(2)         # MF values for stage-1 actions
    q_stage2_mf = np.zeros((2, 2))    # MF values for stage-2 actions per state

    # For likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration tracking (previous choices)
    prev_a1 = -1
    prev_a2_state0 = -1
    prev_a2_state1 = -1

    # Anxiety-modulated arbitration and perseveration
    # Higher anxiety reduces model-based control
    w_eff = w_mb * (1.0 - stai_val)
    # Higher anxiety increases perseveration influence
    kappa_eff = kappa * (1.0 + stai_val)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]

        # Model-based values for stage 1 via one-step lookahead (using MF stage-2 values)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)        # size 2: best value at each state
        q_stage1_mb = transition_matrix @ max_q_stage2     # size 2

        # Hybrid stage-1 values
        q1_hybrid = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa_eff

        # Softmax for stage 1
        logits1 = beta * q1_hybrid + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = max(probs1[a1], eps)

        # Stage-2 policy (state-conditional)
        # Add perseveration bias at stage 2 (state-specific)
        bias2 = np.zeros(2)
        prev_a2 = prev_a2_state0 if s == 0 else prev_a2_state1
        if prev_a2 in (0, 1):
            bias2[prev_a2] += kappa_eff

        logits2 = beta * q_stage2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = max(probs2[a2], eps)

        r = reward[t]

        # TD updates
        # Stage-2 update
        q2_old = q_stage2_mf[s, a2]
        delta2 = r - q2_old
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace
        delta1 = q2_old - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1 + lam * delta2)

        # Update perseveration memory
        prev_a1 = a1
        if s == 0:
            prev_a2_state0 = a2
        else:
            prev_a2_state1 = a2

    neg_ll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-uncertainty-sensitive planner with anxiety-modulated trust and bias.
    
    This model treats the transition structure as potentially uncertain and blends
    the known transition matrix with a uniform one. Anxiety (stai) increases
    transition uncertainty, leading to more MF-like behavior. It also increases
    a soft bias toward spaceship U (action=1) and enhances perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage action.
    reward : array-like of float
        Reward per trial.
    stai : array-like of float
        Anxiety score(s). Uses the first element.
        Interpretation in this model:
          - Higher stai increases perceived transition uncertainty (less MB trust),
            increases perseveration, and amplifies a directional bias at stage 1.
    model_parameters : tuple/list of 5 floats
        (alpha, beta, u_base, kappa, bias)
        - alpha in [0,1]: learning rate.
        - beta in [0,10]: inverse temperature.
        - u_base in [0,1]: baseline uncertainty weight for transitions.
        - kappa in [0,1]: perseveration weight.
        - bias in [0,1]: baseline directional bias toward action 1 (U); transformed to [-1,1].
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, u_base, kappa, bias = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0]) if hasattr(stai, "__len__") else float(stai)

    # True transition matrix
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]])
    # Uniform transition (max uncertainty)
    T_uniform = np.array([[0.5, 0.5],
                          [0.5, 0.5]])

    # Anxiety-modulated uncertainty: higher stai => weight more uniform transitions
    u_eff = np.clip(u_base * stai_val, 0.0, 1.0)
    T_eff = (1.0 - u_eff) * T_true + u_eff * T_uniform

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Perseveration trackers
    prev_a1 = -1
    prev_a2_state0 = -1
    prev_a2_state1 = -1

    # Anxiety-modulated perseveration
    kappa_eff = kappa * (1.0 + stai_val)

    # Directional bias toward action 1 (U) at stage 1, anxiety-amplified
    # bias_raw in [-1,1], then scaled by stai to strengthen with anxiety
    bias_raw = 2.0 * bias - 1.0
    dir_bias = bias_raw * stai_val  # added to action 1 and subtracted from action 0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = state[t]

        # MB planning with uncertain transitions
        v2 = np.max(q2_mf, axis=1)
        q1_mb = T_eff @ v2

        # Combine MB and MF via linear mixture where uncertainty reduces MB implicitly
        # Here we let uncertainty shift weight toward MF without extra parameter:
        w_mb_eff = 1.0 - u_eff
        q1 = w_mb_eff * q1_mb + (1.0 - w_mb_eff) * q1_mf

        # Add perseveration and directional bias at stage 1
        bias1 = np.array([-dir_bias, dir_bias])  # pushes toward action 1 if positive
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa_eff

        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = max(probs1[a1], eps)

        # Stage-2 choice with perseveration
        bias2 = np.zeros(2)
        prev_a2 = prev_a2_state0 if s == 0 else prev_a2_state1
        if prev_a2 in (0, 1):
            bias2[prev_a2] += kappa_eff

        logits2 = beta * q2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = max(probs2[a2], eps)

        r = reward[t]

        # Learning
        q2_old = q2_mf[s, a2]
        delta2 = r - q2_old
        q2_mf[s, a2] += alpha * delta2

        delta1 = q2_old - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update perseveration memories
        prev_a1 = a1
        if s == 0:
            prev_a2_state0 = a2
        else:
            prev_a2_state1 = a2

    neg_ll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive learning-rate hybrid with anxiety-gated volatility and exploration.
    
    This model uses a meta-learning volatility signal v_t to adapt the learning rate:
      alpha_t = alpha0 + gamma * v_t, where v_t tracks unsigned prediction error.
    Anxiety (stai) initializes volatility higher and reduces exploitation (softmax beta).
    Stage-1 action values are a hybrid of MB and MF, with a fixed MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Second-stage state.
    action_2 : array-like of int (0 or 1)
        Second-stage actions.
    reward : array-like of float
        Rewards per trial.
    stai : array-like of float
        Anxiety score(s). Uses the first element.
        Interpretation in this model:
          - Higher stai increases initial volatility (higher early learning rate)
            and reduces beta (encourages exploration).
    model_parameters : tuple/list of 5 floats
        (alpha0, gamma, beta, w_mb, kappa)
        - alpha0 in [0,1]: baseline learning rate.
        - gamma in [0,1]: volatility-to-learning-rate gain.
        - beta in [0,10]: inverse temperature (before anxiety scaling).
        - w_mb in [0,1]: model-based weight for stage-1 values.
        - kappa in [0,1]: perseveration weight.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence.
    """
    alpha0, gamma, beta_base, w_mb, kappa = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0]) if hasattr(stai, "__len__") else float(stai)

    # Transition matrix for MB planning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Volatility and adaptive learning rate
    v = max(0.0, min(1.0, stai_val))  # initialize volatility to anxiety level in [0,1]

    # Perseveration trackers
    prev_a1 = -1
    prev_a2_state0 = -1
    prev_a2_state1 = -1

    # Anxiety reduces effective beta (more exploration under anxiety)
    beta_eff = beta_base * (1.0 - 0.5 * stai_val)
    beta_eff = max(0.0, min(10.0, beta_eff))

    # Anxiety also increases perseveration slightly
    kappa_eff = kappa * (1.0 + 0.5 * stai_val)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = state[t]

        # MB planning
        v2 = np.max(q2_mf, axis=1)
        q1_mb = T @ v2

        # Hybrid stage-1 values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Perseveration biases
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa_eff

        logits1 = beta_eff * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = max(probs1[a1], eps)

        bias2 = np.zeros(2)
        prev_a2 = prev_a2_state0 if s == 0 else prev_a2_state1
        if prev_a2 in (0, 1):
            bias2[prev_a2] += kappa_eff

        logits2 = beta_eff * q2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = max(probs2[a2], eps)

        r = reward[t]

        # Adaptive learning rate from volatility
        alpha_t = alpha0 + gamma * v
        alpha_t = max(0.0, min(1.0, alpha_t))

        # Learning updates
        q2_old = q2_mf[s, a2]
        delta2 = r - q2_old
        q2_mf[s, a2] += alpha_t * delta2

        delta1 = q2_old - q1_mf[a1]
        q1_mf[a1] += alpha_t * delta1

        # Volatility update (Pearce-Hall style): track unsigned PE with inertia
        # v_{t+1} = (1 - eta) * v_t + eta * |delta2|
        # Choose eta tied to gamma to keep params bounded/used: eta = gamma
        eta = gamma
        v = (1.0 - eta) * v + eta * abs(delta2)
        v = max(0.0, min(1.0, v))

        # Update perseveration memories
        prev_a1 = a1
        if s == 0:
            prev_a2_state0 = a2
        else:
            prev_a2_state1 = a2

    neg_ll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return float(neg_ll)