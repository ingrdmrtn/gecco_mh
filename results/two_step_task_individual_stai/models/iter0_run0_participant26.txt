def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated hybrid model-based/model-free RL with eligibility traces.
    
    Overview:
    - Computes a convex combination of model-based (MB) and model-free (MF) action values at stage 1.
    - MB uses the known transition matrix (common=0.7, rare=0.3).
    - MF values are learned from reward via stage-2 TD errors and an eligibility trace for stage 1.
    - Anxiety (stai) controls the arbitration weight between MB and MF via a sigmoid gate.

    Parameters (model_parameters):
    - alpha: learning rate for value updates in [0,1].
    - beta: inverse temperature for softmax in [0,10].
    - w0: base arbitration bias (pre-sigmoid) in [0,1] mapped through sigmoid to [0,1].
    - k_stai: strength of anxiety modulation of arbitration in [0,1]; larger means anxiety pushes policy more toward MB (positive) or MF (negative if you set w0 accordingly).
    - lam: eligibility trace parameter in [0,1], scales how the stage-2 TD error backs up to stage-1 MF values.

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (0=A, 1=U).
    - state: array of ints in {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}, second-stage choices (0 or 1 for the two aliens on that planet).
    - reward: array of floats (typically 0 or 1), coins returned.
    - stai: array-like with a single float in [0,1], anxiety score for the participant.
    - model_parameters: list or array containing [alpha, beta, w0, k_stai, lam].

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    alpha, beta, w0, k_stai, lam = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure: A->X common; U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)       # MF values at stage 1 (for A vs U)
    q_stage2_mf = np.zeros((2, 2))  # MF values at stage 2: state in {X=0, Y=1} x action in {0,1}

    # Anxiety-modulated arbitration weight (0..1)
    # w(stai) = sigmoid(w0 + k_stai*(stai - 0.5))
    gate = w0 + k_stai * (st - 0.5)
    w = 1.0 / (1.0 + np.exp(-5.0 * gate))  # steeper sigmoid for sensitivity while respecting bounds

    eps = 1e-12

    for t in range(n_trials):
        # Model-based values at stage 1 from current stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)         # value of best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2     # plan through transitions

        # Arbitration
        q_stage1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # First-stage policy
        a1 = int(action_1[t])
        exp_q1 = np.exp(beta * (q_stage1 - np.max(q_stage1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = int(state[t])
        a2 = int(action_2[t])
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        pe2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * pe2

        # Stage-1 MF bootstrap (SARSA-style) and eligibility trace from stage-2 PE
        # Bootstrap toward observed stage-2 action value
        pe1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * pe1
        # Eligibility: back up a fraction of stage-2 PE to stage1 MF
        q_stage1_mf[a1] += lam * alpha * pe2

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-modulated learning asymmetry and perseveration.
    
    Overview:
    - Learns an internal transition model T_est for each first-stage action and plans model-based values through it.
    - Combines MB and MF values at stage 1 with a simple anxiety-based arbitration: weight on MF increases with anxiety.
    - Stage-2 learning uses valence-asymmetric learning rates that scale with anxiety (more anxious -> stronger learning from non-reward).
    - Includes an anxiety-amplified perseveration bias at stage 1.

    Parameters (model_parameters):
    - alpha: base learning rate in [0,1].
    - beta: inverse temperature for softmax in [0,10].
    - kappa0: base learning rate for updating the transition model in [0,1].
    - phi: perseveration strength in [0,1], bias to repeat previous stage-1 action.
    - a_neg: asymmetry coefficient in [0,1] scaling how anxiety increases learning from negative outcomes.

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (0=A, 1=U).
    - state: array of ints in {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}, second-stage choices on the observed planet.
    - reward: array of floats (typically 0 or 1).
    - stai: array-like with a single float in [0,1], anxiety score for the participant.
    - model_parameters: list/array [alpha, beta, kappa0, phi, a_neg].

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    alpha, beta, kappa0, phi, a_neg = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize internal transition model with instructed/common structure
    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Perseveration kernel (one-hot of previous action)
    prev_a1 = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-modulated quantities
    # MF weight increases with anxiety; MB weight decreases correspondingly
    w_mf = st
    w_mb = 1.0 - w_mf
    # Transition learning rate increases with anxiety (more anxious -> more volatile belief updates)
    kappa = kappa0 * (0.25 + 0.75 * st)

    # Valence-asymmetric learning rates modulated by anxiety
    alpha_pos = np.clip(alpha * (1.0 - a_neg * st), 0.0, 1.0)
    alpha_neg = np.clip(alpha * (1.0 + a_neg * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 MB values via learned transitions
        max_q2 = np.max(q_stage2, axis=1)      # best alien on each planet
        q1_mb = T_est @ max_q2

        # Perseveration bias (encourages repeating last first-stage choice), amplified by anxiety
        persev_bias = np.zeros(2)
        if prev_a1 is not None:
            persev_bias[prev_a1] = phi * (0.5 + 0.5 * st)

        # Combine MB and MF for stage 1
        q1_total = w_mb * q1_mb + w_mf * q_stage1_mf + persev_bias

        # Policy stage 1
        exp_q1 = np.exp(beta * (q1_total - np.max(q1_total)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Policy stage 2
        q2_vec = q_stage2[s]
        exp_q2 = np.exp(beta * (q2_vec - np.max(q2_vec)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Learn transitions T_est for the chosen first-stage action from observed state
        # Move the chosen row toward one-hot of observed state
        target = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1] = (1.0 - kappa) * T_est[a1] + kappa * target
        # Keep rows normalized (numerical safety)
        T_est[a1] = T_est[a1] / (np.sum(T_est[a1]) + eps)

        # Stage-2 value update with valence asymmetry
        pe2 = r - q_stage2[s, a2]
        lr2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q_stage2[s, a2] += lr2 * pe2

        # Stage-1 MF bootstrap toward updated stage-2 value
        pe1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += lr2 * pe1

        prev_a1 = a1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-driven exploration and forgetting with MB-MF mixture and reward-sensitivity.
    
    Overview:
    - Combines model-based planning with model-free values at stage 1 using a fixed, anxiety-dependent weight.
    - Implements anxiety-increased exploration by reducing effective beta at both stages as anxiety rises.
    - Introduces forgetting (value decay) that strengthens with anxiety.
    - Scales TD updates by an anxiety-dependent outcome sensitivity.

    Parameters (model_parameters):
    - alpha: learning rate in [0,1].
    - beta1: inverse temperature at stage 1 in [0,10].
    - beta2: inverse temperature at stage 2 in [0,10].
    - decay0: base forgetting rate in [0,1]; higher -> more decay each trial.
    - gamma: anxiety gain in [0,1] controlling both exploration increase and outcome sensitivity.

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (0=A,1=U).
    - state: array of ints in {0,1}, second-stage state reached (0=X,1=Y).
    - action_2: array of ints in {0,1}, second-stage choices on the observed planet.
    - reward: array of floats (0/1).
    - stai: array-like with a single float in [0,1], anxiety score for the participant.
    - model_parameters: list/array [alpha, beta1, beta2, decay0, gamma].

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    alpha, beta1, beta2, decay0, gamma = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-dependent components
    # More anxiety -> more exploration (lower effective beta)
    beta1_eff = beta1 / (1.0 + gamma * st + eps)
    beta2_eff = beta2 / (1.0 + gamma * st + eps)
    # More anxiety -> more forgetting (higher decay)
    decay = np.clip(decay0 * (0.5 + 0.5 * st), 0.0, 1.0)
    # Outcome sensitivity: amplifies TD errors with anxiety
    sens = 1.0 + gamma * (st - 0.5)

    for t in range(n_trials):
        # Model-based values at stage 1 from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Anxiety-dependent arbitration: higher anxiety -> rely less on MB
        w_mb = 1.0 - st
        w_mf = st
        q1 = w_mb * q1_mb + w_mf * q1_mf

        # Stage 1 policy
        a1 = int(action_1[t])
        exp_q1 = np.exp(beta1_eff * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = int(state[t])
        a2 = int(action_2[t])
        q2_vec = q2[s]
        exp_q2 = np.exp(beta2_eff * (q2_vec - np.max(q2_vec)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        p_choice_2[t] = probs_2[a2]

        # Global forgetting before updates
        q1_mf = (1.0 - decay) * q1_mf
        q2 = (1.0 - decay) * q2

        # Learning with outcome sensitivity
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * sens * pe2

        # Stage-1 MF bootstrap toward updated stage-2 value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * sens * pe1

    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss