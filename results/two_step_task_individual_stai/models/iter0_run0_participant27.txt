def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-weighted arbitration, learned transitions, and eligibility trace.

    This model learns second-stage Q-values from rewards, a model-free first-stage Q via TD with eligibility,
    and a model-based first-stage value via a learned transition model. The arbitration between model-based
    and model-free control is modulated by anxiety (stai): higher stai shifts control toward model-free.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state each trial (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (0/1 correspond to the two aliens on that planet).
    reward : array-like of float
        Obtained reward on each trial (e.g., coins; typically 0/1).
    stai : array-like of float
        Array containing a single standardized anxiety score in [0,1]. Higher means more anxious.
    model_parameters : list or array of floats
        [alpha, beta, w_base, lam, k_stai]
        - alpha in [0,1]: learning rate for both transition and value updates.
        - beta in [0,10]: inverse temperature for softmax policies.
        - w_base in [0,1]: baseline weight on model-based control at stage 1.
        - lam in [0,1]: eligibility trace strength to back-propagate reward to stage 1.
        - k_stai in [0,1]: strength with which anxiety shifts arbitration toward model-free.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w_base, lam, k_stai = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model; rows: action1, cols: state
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Action values
    Q1_mf = np.zeros(2)              # model-free first stage
    Q2 = 0.5 * np.ones((2, 2))       # second-stage action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation: expected value of each first-stage action via transition model
        max_Q2 = np.max(Q2, axis=1)  # value of best action at each second-stage state
        Q1_mb = T @ max_Q2

        # Anxiety-weighted arbitration: higher stai -> less model-based weight
        w_mb = np.clip(w_base * (1.0 - k_stai * stai), 0.0, 1.0)
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # First-stage policy
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy at observed state
        s = int(state[t])
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Updates
        # Second-stage TD
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # First-stage model-free with eligibility: combine value prediction error and backpropagated reward
        delta1_val = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * (delta1_val + lam * delta2)

        # Learn transition model from observed transition (chosen a1 led to s)
        # Move T[a1, s] toward 1 and the other toward 0
        T[a1, s] += alpha * (1.0 - T[a1, s])
        T[a1, 1 - s] += alpha * (0.0 - T[a1, 1 - s])

        # Keep each row normalized
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric learning with anxiety-modulated arbitration and choice stickiness.

    The model uses separate learning rates for positive vs. negative second-stage prediction errors,
    propagates them to first stage, blends model-based and model-free values with an anxiety-modulated
    weight, and includes an anxiety-scaled perseveration bias on choices at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Rewards each trial.
    stai : array-like of float
        Single anxiety score in [0,1].
    model_parameters : list or array of floats
        [alpha_pos, alpha_neg, beta, w_mb0, stickiness]
        - alpha_pos in [0,1]: learning rate when second-stage PE is positive.
        - alpha_neg in [0,1]: learning rate when second-stage PE is negative.
        - beta in [0,10]: inverse temperature.
        - w_mb0 in [0,1]: baseline model-based weight.
        - stickiness in [0,1]: base perseveration strength; scaled by anxiety for effective bias.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, w_mb0, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition model (common: 0->0 and 1->1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = 0.5 * np.ones((2, 2))

    # Perseveration traces (previous choices, start neutral)
    last_a1 = None
    last_a2 = np.array([None, None], dtype=object)  # track per-state last second-stage action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated perseveration scale (higher stai -> stronger stickiness)
    kappa = stickiness * (0.25 + stai)  # in [0, 1.25*stickiness]

    for t in range(n_trials):
        # Model-based value at stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Anxiety-modulated model-based weight: higher anxiety reduces MB weight
        w_mb = np.clip(w_mb0 - 0.5 * (stai - 0.5), 0.0, 1.0)

        # Combine values
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Add perseveration bias to stage-1 logits
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with state-specific perseveration bias
        s = int(state[t])
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += kappa

        logits2 = beta * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning rates based on sign of second-stage PE
        pe2 = r - Q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        Q2[s, a2] += alpha2 * pe2

        # Propagate to first stage (use same asymmetry)
        pe1 = Q2[s, a2] - Q1_mf[a1]
        alpha1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        Q1_mf[a1] += alpha1 * pe1

        # Update perseveration traces
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-sensitive exploration, rare-transition down-weighting, and Pavlovian safety bias.

    This model blends model-free and model-based control with a fixed weight, but:
    - Down-weights model-based control specifically after rare transitions, more so with higher anxiety.
    - Uses anxiety to reduce effective beta (more exploration with higher anxiety).
    - Adds a Pavlovian 'safety' bias toward action 0 at stage 2 that grows with anxiety and uncertainty.
    - Includes value forgetting toward 0.5 to capture drift in unchosen/unstimulated values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Rewards each trial.
    stai : array-like of float
        Single anxiety score in [0,1].
    model_parameters : list or array of floats
        [alpha, beta, mb_weight, bias_safe, forget]
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: base inverse temperature.
        - mb_weight in [0,1]: baseline model-based mixing weight at stage 1.
        - bias_safe in [0,1]: strength of Pavlovian bias favoring action 0 under uncertainty.
        - forget in [0,1]: forgetting rate pulling Qs toward 0.5 each trial.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, mb_weight, bias_safe, forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure for common vs rare inference
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = 0.5 * np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Identify whether the observed transition was common or rare under fixed structure
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Anxiety-dependent down-weighting of model-based control following rare transitions
        mb_w_eff = mb_weight if is_common else mb_weight * (1.0 - 0.7 * stai)
        mb_w_eff = np.clip(mb_w_eff, 0.0, 1.0)

        # Model-based value from fixed transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_fixed @ max_Q2
        Q1 = mb_w_eff * Q1_mb + (1.0 - mb_w_eff) * Q1_mf

        # Anxiety-reduced beta: higher anxiety -> more exploration
        beta_eff = beta * (1.0 - 0.5 * stai)

        # Stage 1 policy
        logits1 = beta_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Pavlovian safety bias at stage 2: favor action 0 when uncertain
        uncertainty = 1.0 - abs(Q2[s, 0] - Q2[s, 1])  # in [0,1]
        bias_vec = np.array([bias_safe * stai * uncertainty, 0.0])

        logits2 = beta_eff * Q2[s] + bias_vec
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Forgetting toward 0.5 before applying TD updates
        Q2 = (1.0 - forget) * Q2 + forget * 0.5
        Q1_mf = (1.0 - forget) * Q1_mf + forget * 0.0  # MF baseline around 0

        # Second-stage TD
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # First-stage MF update with eligibility component
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * (0.5 * delta1 + 0.5 * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll