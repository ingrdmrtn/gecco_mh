def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and eligibility traces.
    
    This model blends model-based (MB) and model-free (MF) values at stage 1.
    Anxiety (stai) modulates the arbitration weight between MB and MF via kappa,
    shifting control toward MF as anxiety increases (or vice versa depending on kappa),
    and uses an eligibility trace to propagate second-stage TD errors back to the first stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within each state (two aliens per state).
    reward : array-like of float in [0,1]
        Obtained reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; higher means higher anxiety.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, w_base, lam, kappa]
        - alpha (learning rate for MF and second-stage Q; [0,1])
        - beta (inverse temperature for softmax; [0,10])
        - w_base (baseline MB weight at stai=0.5; [0,1])
        - lam (eligibility trace for backpropagating second-stage PE; [0,1])
        - kappa (strength by which stai modulates MB weight; [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w_base, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure known to participant
    transition_matrix = np.array([[0.7, 0.3],  # A -> (X,Y)
                                 [0.3, 0.7]]) # U -> (X,Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free action values
    q_stage1_mf = np.zeros(2)        # for A,U
    q_stage2 = np.zeros((2, 2))      # for state X,Y and two aliens each

    eps = 1e-10

    for t in range(n_trials):
        # Anxiety-modulated arbitration weight
        # w_eff increases/decreases linearly with stai with slope controlled by kappa
        # w_eff = clip( w_base + kappa * (stai - 0.5), 0, 1 )
        w_eff = w_base + kappa * (stai - 0.5)
        w_eff = 0.0 if w_eff < 0.0 else (1.0 if w_eff > 1.0 else w_eff)

        # Model-based stage-1 value: expected max of stage-2 by transitions
        max_q_stage2 = np.max(q_stage2, axis=1)        # size 2: states
        q_stage1_mb = transition_matrix @ max_q_stage2 # size 2: actions

        # Blend MB and MF for stage-1
        q1 = (1.0 - w_eff) * q_stage1_mf + w_eff * q_stage1_mb

        # Softmax for stage-1
        q1_centered = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_centered)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in observed state
        s = int(state[t])
        q2_s = q_stage2[s]
        q2_centered = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2_centered)
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: second-stage TD error and update
        pe2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        # Model-free first-stage update via eligibility trace using stage-2 PE
        q_stage1_mf[a1] += alpha * lam * pe2

        # Also a direct MF update toward the chosen second-stage value (SARSA(0)-like bootstrapping)
        boot = q_stage2[s, a2]
        pe1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (1.0 - lam) * pe1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-scaled perseveration and asymmetric learning rates.
    
    A purely model-free controller learns both stages with separate learning rates
    for positive vs negative RPEs. Choice perseveration (stickiness) biases both
    stages toward repeating the last chosen action; its strength is scaled by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens within the current state).
    reward : array-like of float in [0,1]
        Obtained reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; scales perseveration strength.
    model_parameters : list or array-like of 5 floats
        [alpha_pos, alpha_neg, beta, rho, omega]
        - alpha_pos (learning rate for positive RPEs; [0,1])
        - alpha_neg (learning rate for negative RPEs; [0,1])
        - beta (inverse temperature; [0,10])
        - rho (baseline perseveration strength; [0,1])
        - omega (anxiety modulation of perseveration; [0,1])
          effective stickiness = rho * (1 + omega * (stai - 0.5))
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, rho, omega = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Perseveration traces: last chosen action for each stage/state
    last_a1 = None
    last_a2 = np.array([None, None], dtype=object)  # per state

    eps = 1e-10

    # Anxiety-scaled perseveration coefficient
    rho_eff = rho * (1.0 + omega * (stai - 0.5))

    for t in range(n_trials):
        # Stage-1 preferences include stickiness
        pref1 = q_stage1.copy()
        if last_a1 is not None:
            stick = np.array([0.0, 0.0])
            stick[last_a1] = 1.0
            pref1 = pref1 + rho_eff * stick

        # Softmax for stage-1
        pref1_c = pref1 - np.max(pref1)
        probs_1 = np.exp(beta * pref1_c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 preferences include state-specific stickiness
        s = int(state[t])
        pref2 = q_stage2[s].copy()
        if last_a2[s] is not None:
            stick2 = np.array([0.0, 0.0])
            stick2[int(last_a2[s])] = 1.0
            pref2 = pref2 + rho_eff * stick2

        pref2_c = pref2 - np.max(pref2)
        probs_2 = np.exp(beta * pref2_c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning rates by sign of RPE
        pe2 = r - q_stage2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q_stage2[s, a2] += alpha2 * pe2

        # Bootstrapped MF update of stage-1 toward realized stage-2 value
        pe1 = q_stage2[s, a2] - q_stage1[a1]
        alpha1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        q_stage1[a1] += alpha1 * pe1

        # Update perseveration memories
        last_a1 = a1
        last_a2[s] = a2

    neg_log_lik = -(np.sum(np.log(p_choice_1 + 1e-10)) + np.sum(np.log(p_choice_2 + 1e-10)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning MB/MF hybrid with anxiety-shaped priors and risk-sensitive utility.
    
    This model learns the transition structure online and uses it for MB planning.
    Anxiety shapes the initial transition beliefs (higher anxiety -> more uncertainty,
    i.e., closer to 0.5/0.5) and increases reward concavity via a risk-sensitivity parameter.
    MB and MF are blended at stage 1 with a deterministic weight 1 - stai (more anxiety -> more MF).
    An eligibility trace propagates second-stage PEs to stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state.
    reward : array-like of float in [0,1]
        Obtained reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; used in priors, risk sensitivity, and arbitration.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, tau, kappa_risk, lam]
        - alpha (learning rate for Q-values; [0,1])
        - beta (inverse temperature; [0,10])
        - tau (transition learning rate; [0,1])
        - kappa_risk (baseline risk-sensitivity exponent; [0,1])
          effective exponent gamma = kappa_risk + stai * (1 - kappa_risk)
        - lam (eligibility trace for MF backpropagation; [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, tau, kappa_risk, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-shaped initial transition beliefs: shrink toward 0.5 as stai increases
    base_common = 0.7
    init_common = (1.0 - stai) * base_common + stai * 0.5
    # Rows: actions (A,U); Cols: states (X,Y). Start with common/rare structure but softened by stai
    trans_est = np.array([[init_common, 1.0 - init_common],   # A -> (X,Y)
                          [1.0 - init_common, init_common]])  # U -> (X,Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Anxiety-modulated risk exponent for utility
    gamma = kappa_risk + stai * (1.0 - kappa_risk)  # in [kappa_risk, 1], higher stai -> more concavity

    eps = 1e-10

    for t in range(n_trials):
        # Model-based Q1 from current transition estimates
        max_q2 = np.max(q_stage2, axis=1)  # per state
        q1_mb = trans_est @ max_q2

        # Arbitration weight: w_mb = 1 - stai (higher anxiety -> more MF)
        w_mb = 1.0 - stai
        q1 = (1.0 - w_mb) * q_stage1_mf + w_mb * q1_mb

        # Stage-1 policy
        q1_c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2_s = q_stage2[s]
        q2_c = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Risk-sensitive utility transformation of reward
        r = reward[t]
        u = r ** gamma

        # Second-stage update
        pe2 = u - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        # MF first-stage updates: eligibility-trace using PE2 and direct bootstrap
        q_stage1_mf[a1] += alpha * lam * pe2
        boot = q_stage2[s, a2]
        pe1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (1.0 - lam) * pe1

        # Transition learning: update estimated row for chosen action toward observed state
        # Simple delta rule on categorical distribution (two states)
        # Move probability mass toward the observed state for chosen action
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        trans_est[a1, :] = trans_est[a1, :] + tau * (target - trans_est[a1, :])

        # Ensure row-stochastic and within [0,1] bounds by normalization
        row_sum = np.sum(trans_est[a1, :])
        if row_sum > 0:
            trans_est[a1, :] = trans_est[a1, :] / row_sum

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik