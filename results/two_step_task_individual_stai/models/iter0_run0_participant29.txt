def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    The agent blends a model-based (MB) plan using the known common transitions with a model-free (MF) cached value, and uses an eligibility trace to propagate reward back to the first-stage MF values. Anxiety (stai) reduces reliance on MB planning.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices (alien within state).
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score (this participant’s score).
    - model_parameters: iterable of 5 floats
        learning_rate: [0,1] — reward learning rate for MF Q-values
        beta: [0,10] — inverse temperature for softmax at both stages
        w0: [0,1] — baseline weight on MB planning at stage 1
        lam: [0,1] — eligibility trace for MF update from stage 2 to stage 1
        anx_influence: [0,1] — how strongly anxiety reduces MB weight

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    learning_rate, beta, w0, lam, anx_influence = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (common: 0.7; rare: 0.3)
    # Rows: first-stage action (0=A,1=U). Cols: state (0=X,1=Y).
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices each trial
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)           # for A vs U
    q_stage2_mf = np.zeros((2, 2))      # for states X,Y and two aliens per state

    # Anxiety-modulated MB weight: higher anxiety reduces MB reliance
    w_effective = np.clip(w0 * (1.0 - anx_influence * stai_val), 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):
        # Compute MB action values at stage 1 via one-step lookahead over states
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # value of each second-stage state
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Hybrid value
        q_stage1 = w_effective * q_stage1_mb + (1.0 - w_effective) * q_stage1_mf

        # Softmax policy for stage 1
        q1 = q_stage1 - np.max(q_stage1)  # stability
        exp_q1 = np.exp(beta * q1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (pure MF at the visited state)
        s = state[t]
        q2_row = q_stage2_mf[s]
        q2c = q2_row - np.max(q2_row)
        exp_q2 = np.exp(beta * q2c)
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD learning
        # Stage 2 update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += learning_rate * delta2

        # Stage 1 MF update with eligibility trace using the stage-2 prediction error
        # Also allow direct bootstrapping toward the stage-2 chosen action value
        delta1 = (q_stage2_mf[s, a2] - q_stage1_mf[a1])
        q_stage1_mf[a1] += learning_rate * (lam * delta2 + (1.0 - lam) * delta1)

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """MF with valence-asymmetric learning and anxiety-modulated perseveration.
    The agent is model-free at both stages, but uses different learning rates for positive and negative outcomes. Anxiety increases sensitivity to negative outcomes and increases choice perseveration (tendency to repeat previous choices).

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha_base: [0,1] — base learning rate
        beta: [0,10] — inverse temperature
        valence_bias: [0,1] — scales difference between positive vs negative learning
        perseveration: [0,1] — base perseveration strength
        anx_gain: [0,1] — how strongly anxiety increases neg. learning and perseveration

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha_base, beta, valence_bias, perseveration, anx_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective learning rates
    # Positive learning rate slightly boosted; negative learning rate boosted by anxiety
    alpha_pos = np.clip(alpha_base * (1.0 + 0.5 * valence_bias), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 + valence_bias * anx_gain * stai_val), 0.0, 1.0)

    # Perseveration terms: bias toward repeating previous actions; anxiety amplifies
    pers1_strength = perseveration * (1.0 + anx_gain * stai_val)
    pers2_strength = perseveration * 0.5 * (1.0 + anx_gain * stai_val)

    # MF Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Last choices for perseveration signals
    last_a1 = None
    last_a2 = np.array([None, None], dtype=object)  # per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    for t in range(n_trials):
        # Stage 1 logits with perseveration bias
        logits1 = q1.copy()
        if last_a1 is not None:
            logits1[last_a1] += pers1_strength

        # Softmax stage 1
        logits1_c = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1_c)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 logits with state-specific perseveration
        s = state[t]
        logits2 = q2[s].copy()
        if last_a2[s] is not None:
            logits2[last_a2[s]] += pers2_strength

        logits2_c = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2_c)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning rates by valence
        pe2 = r - q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        q2[s, a2] += lr2 * pe2

        # Stage 1 MF bootstrapping toward the value of the selected second-stage action
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        q1[a1] += lr1 * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based with learned transition matrix, lapse-exploration modulated by anxiety.
    The agent learns second-stage rewards (MF at stage 2) and also learns the transition probabilities from first-stage actions to states. Stage-1 decisions are model-based using the learned transition matrix. Anxiety increases lapse/exploration and transition learning rate (assuming higher perceived volatility).

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha_r: [0,1] — reward learning rate for second-stage Q
        beta: [0,10] — inverse temperature
        alpha_t: [0,1] — base transition learning rate
        epsilon0: [0,1] — base lapse probability (uniform choice mixture)
        anx_weight: [0,1] — how strongly anxiety increases alpha_t and epsilon

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha_r, beta, alpha_t, epsilon0, anx_weight = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix T[a, s], rows sum to 1. Start near uniform.
    T = np.full((2, 2), 0.5)

    # Second-stage MF Q-values
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    epsilon = np.clip(epsilon0 * (0.5 + stai_val * anx_weight), 0.0, 1.0)
    alpha_t_eff = np.clip(alpha_t * (1.0 + stai_val * anx_weight), 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):
        # Model-based values at stage 1 using learned transitions
        V_state = np.max(Q2, axis=1)  # value for each state
        Q1_mb = T @ V_state

        # Softmax with lapse at stage 1
        q1c = Q1_mb - np.max(Q1_mb)
        sm1 = np.exp(beta * q1c)
        sm1 /= (np.sum(sm1) + eps)
        # Lapse mixture toward uniform
        probs1 = (1.0 - epsilon) * sm1 + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax with lapse at the visited state (same epsilon)
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        sm2 = np.exp(beta * q2c)
        sm2 /= (np.sum(sm2) + eps)
        probs2 = (1.0 - epsilon) * sm2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model T using delta rule toward one-hot observation
        # Observation o_s = 1 for reached state; 0 for the other. Keep row normalized.
        o = np.array([0.0, 0.0])
        o[s] = 1.0
        T[a1] = T[a1] + alpha_t_eff * (o - T[a1])
        # Ensure numerical stability and row normalization
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        # Update second-stage Q-values
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * pe2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)