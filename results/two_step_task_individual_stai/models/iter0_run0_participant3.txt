def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated planning, temperature, and perseveration.
    
    This model mixes model-free (first- and second-stage Q-learning with eligibility trace)
    and model-based evaluation of first-stage actions via the known transition matrix. Anxiety
    down-weights planning (model-based), lowers effective inverse temperature (more noise),
    and increases perseveration/stickiness.

    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: array-like of int in {0,1}; chosen spaceship at stage 1 for each trial
    - state:    array-like of int in {0,1}; reached planet (0=X, 1=Y) for each trial
    - action_2: array-like of int in {0,1}; chosen alien at stage 2 for each trial
    - reward:   array-like of float in [0,1]; coins received on each trial
    - stai:     array-like with one float in [0,1]; anxiety score (higher = higher anxiety)
    - model_parameters: tuple/list of five parameters:
        alpha: learning rate for both stages (0..1)
        beta: inverse temperature for softmax (0..10)
        w_base: baseline model-based weight (0..1)
        kappa_base: baseline perseveration strength (0..1)
        lam: eligibility trace scaling credit assignment from stage 2 to stage 1 (0..1)
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w_base, kappa_base, lam = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize value functions
    q1_mf = np.zeros(2)          # model-free stage-1 values for actions {A, U}
    q2 = np.zeros((2, 2))        # second-stage Q-values: states {X,Y} x actions {0,1}

    # Perseveration memory
    prev_a1 = -1
    prev_a2 = -1  # last second-stage action (state-unspecific stickiness for simplicity)

    # Anxiety modulations
    # Higher anxiety -> less planning, more perseveration, more noise
    w = np.clip(w_base * (1.0 - 0.7 * s), 0.0, 1.0)
    kappa = kappa_base * (0.5 + 0.5 * s)
    beta_eff = max(1e-8, beta * (1.0 - 0.4 * s))  # never negative/zero

    for t in range(n_trials):
        # Model-based first-stage values via one-step lookahead
        max_q2 = np.max(q2, axis=1)            # best alien per planet
        q1_mb = transition_matrix @ max_q2     # expected value per spaceship

        # Hybrid action values
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 stickiness bias (one-hot for previous action)
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Policy for the first choice (softmax with stickiness)
        logits1 = beta_eff * q1_hybrid + kappa * stick1
        logits1 -= np.max(logits1)  # numerical stability
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (state-dependent) with stickiness on last second-stage action
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta_eff * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 TD update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

        # Stage-1 model-free TD update toward the realized second-stage action value
        delta1_mf = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1_mf

        # Eligibility trace: propagate stage-2 RPE back to stage-1 choice
        q1_mf[a1] += alpha * lam * delta2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive learning and transition-belief distortion with anxiety.
    
    Anxiety modulates:
    - Perceived transition structure: more anxious participants assume less reliable transitions.
    - Adaptive learning rate: increases with surprise magnitude and anxiety.
    - Perseveration: increased with anxiety.
    - Planning weight: decreases with anxiety.

    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: array-like int {0,1}; first-stage choice per trial
    - state:    array-like int {0,1}; reached second-stage state per trial
    - action_2: array-like int {0,1}; second-stage choice per trial
    - reward:   array-like float [0,1]; reward per trial
    - stai:     array-like with one float [0,1]; anxiety score
    - model_parameters: tuple/list of five parameters:
        alpha0: baseline learning rate (0..1)
        beta: inverse temperature (0..10)
        tau: sensitivity of learning rate to unsigned RPE and anxiety (0..1)
        ps: perseveration baseline strength (0..1)
        zeta: strength of anxiety-induced transition unreliability belief (0..1)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha0, beta, tau, ps, zeta = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Perceived transition reliability (common probability)
    p_common = 0.7 - 0.2 * s * zeta
    p_common = np.clip(p_common, 0.5, 0.7)
    transition_matrix = np.array([[p_common, 1.0 - p_common],
                                  [1.0 - p_common, p_common]])

    # Initialize
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Perseveration and planning weight
    kappa = ps * (0.5 + 0.5 * s)
    w = np.clip(1.0 - s, 0.0, 1.0)  # higher anxiety -> less planning
    beta_eff = beta  # keep base temperature here

    # For adaptive learning rate, track last unsigned RPE magnitude
    last_abs_rpe2 = 0.0

    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Combine MB and MF
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 stickiness
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Policy stage 1
        logits1 = beta_eff * q1 + kappa * stick1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta_eff * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Adaptive learning rate based on surprise and anxiety
        alpha_eff = alpha0 * (1.0 - 0.5 * s) + tau * s * last_abs_rpe2
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Learning updates
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += alpha_eff * delta2

        delta1 = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_eff * delta1

        # Modest eligibility to propagate outcome surprise to stage 1 as well
        q1_mf[a1] += alpha_eff * 0.5 * delta2

        # Update trackers
        last_abs_rpe2 = abs(delta2)
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Affective utility and anxiety-modulated credit assignment with targeted avoidance bias.
    
    This model assumes anxiety distorts utility and credit assignment:
    - Outcome utility is shifted down by a pessimism baseline proportional to anxiety.
    - Outcome sensitivity is reduced with anxiety (dampened affective impact).
    - Eligibility trace for credit assignment from stage 2 to stage 1 is reduced by anxiety.
    - A targeted avoidance bias at stage 2: with anxiety, participants avoid one alien
      on planet Y (state=1, action=1), implemented as a negative bias on that logit.
    - First-stage decisions mix MB and MF with anxiety-reduced planning weight.

    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: array-like int {0,1}; first-stage choices
    - state:    array-like int {0,1}; second-stage states
    - action_2: array-like int {0,1}; second-stage choices
    - reward:   array-like float [0,1]; obtained rewards
    - stai:     array-like with one float [0,1]; anxiety score
    - model_parameters: tuple/list of five parameters:
        alpha: learning rate (0..1)
        beta: inverse temperature (0..10)
        lam_base: baseline eligibility trace (0..1)
        bias_ref: pessimism reference for utility shift (0..1)
        omega: strength of anxiety-driven avoidance/sensitivity (0..1)
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, lam_base, bias_ref, omega = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed true transition
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Anxiety modulations
    # Utility transformation: pessimistic shift and dampening
    # r_eff = (reward - bias_ref * s) scaled by (1 - 0.5 * s * omega)
    scale = 1.0 - 0.5 * s * omega
    scale = max(scale, 1e-4)
    lam = lam_base * (1.0 - 0.5 * s)  # reduced credit assignment with anxiety
    w = np.clip(1.0 - 0.5 * s, 0.0, 1.0)  # reduced planning with anxiety

    for t in range(n_trials):
        # Model-based Q for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Combine MB and MF
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with targeted avoidance bias on planet Y, action=1
        s_idx = state[t]
        logits2 = beta * q2[s_idx].copy()
        if s_idx == 1:
            # Anxiety-driven avoidance of action 1 on planet Y
            logits2[1] -= beta * omega * s
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Affective utility transformation
        r_raw = reward[t]
        r_eff = scale * (r_raw - bias_ref * s)

        # Learning updates
        delta2 = r_eff - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

        delta1 = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Anxiety-reduced eligibility trace
        q1_mf[a1] += alpha * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll