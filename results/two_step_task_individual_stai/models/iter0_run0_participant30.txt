def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with eligibility trace and perseveration.
    
    Anxiety use: Higher STAI reduces model-based control weight (w), biasing toward model-free control.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (spaceship: 0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state per trial (planet: 0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien on the planet; index within the planet).
    reward : array-like of float (0 or 1)
        Received reward per trial.
    stai : array-like of float
        Trait anxiety score; stai[0] used here. Interpreted in [0,1].
    model_parameters : array-like of float
        [alpha, beta, w, lambda_, perseveration]
        - alpha in [0,1]: learning rate for Q-value updates.
        - beta in [0,10]: inverse temperature for softmax choice.
        - w in [0,1]: baseline model-based weight; anxiety reduces this.
        - lambda_ in [0,1]: eligibility trace mixing stage-2 PE into stage-1 update.
        - perseveration in [0,1]: tendency to repeat previous action (applied at both stages).
        
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w, lambda_, perseveration = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: rows are first-stage actions (A,U), cols are states (X,Y).
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action-value tables
    q_stage1_mf = np.zeros(2)           # MF Q for first-stage actions (A,U)
    q_stage2_mf = np.zeros((2, 2))      # MF Q for second-stage actions at each state

    # Perseveration memory (previous actions), initialized to neutral (no bias)
    prev_a1 = None
    prev_a2_by_state = [None, None]

    # Anxiety-modulated model-based weight: higher anxiety reduces MB control.
    w_eff = np.clip(w * (1.0 - 0.8 * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])

        # Compute model-based action values at stage 1 from stage-2 MF values (one-step lookahead)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)      # size 2: best alien at X, Y
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value of A and U

        # Hybrid stage-1 values
        q1 = (1.0 - w_eff) * q_stage1_mf + w_eff * q_stage1_mb

        # Add perseveration bias for repeating previous stage-1 action
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += perseveration

        # Softmax for stage 1
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (softmax over MF Q at reached state), with perseveration bias
        q2 = q_stage2_mf[s].copy()
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += perseveration

        logits2 = beta * q2 + bias2
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # Temporal-difference errors
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        delta2 = r - q_stage2_mf[s, a2]

        # Updates: stage-2 MF
        q_stage2_mf[s, a2] += alpha * delta2

        # Updates: stage-1 MF with eligibility trace (includes both delta1 and lambda*delta2)
        q_stage1_mf[a1] += alpha * (delta1 + lambda_ * delta2)

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive learning-rate hybrid RL, where anxiety amplifies sensitivity to surprising outcomes.
    
    Anxiety use: STAI scales how much unsigned prediction error inflates the learning rate on each trial.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Received reward per trial.
    stai : array-like of float
        Trait anxiety score; stai[0] used here. Interpreted in [0,1].
    model_parameters : array-like of float
        [alpha_base, beta, w, lambda_, v]
        - alpha_base in [0,1]: baseline learning rate.
        - beta in [0,10]: inverse temperature for softmax.
        - w in [0,1]: model-based weight (fixed baseline).
        - lambda_ in [0,1]: eligibility trace mixing stage-2 PE into stage-1 update.
        - v in [0,1]: volatility sensitivity; scales how much |PE| (times anxiety) boosts learning rate.
        
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_base, beta, w, lambda_, v = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for t in range(n_trials):
        s = int(state[t])

        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q1 = (1.0 - w) * q_stage1_mf + w * q_stage1_mb

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2 = q_stage2_mf[s].copy()
        logits2 = beta * q2
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # TD errors using current values
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        delta2 = r - q_stage2_mf[s, a2]

        # Anxiety-amplified adaptive learning rate from unsigned stage-2 PE
        alpha_t = alpha_base * (1.0 + v * stai * abs(delta2))
        alpha_t = min(1.0, max(0.0, alpha_t))

        # Updates with adaptive alpha
        q_stage2_mf[s, a2] += alpha_t * delta2
        q_stage1_mf[a1] += alpha_t * (delta1 + lambda_ * delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk- and valence-asymmetric hybrid RL with anxiety-weighted loss aversion.
    
    Anxiety use: Missed rewards (0 outcome) are treated as aversive with magnitude scaled by STAI and rho.
                 Learning is also valence-asymmetric (different alphas for gains vs losses).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial.
    state : array-like of int (0 or 1)
        Second-stage state per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float (0 or 1)
        Received reward per trial.
    stai : array-like of float
        Trait anxiety score; stai[0] used here. Interpreted in [0,1].
    model_parameters : array-like of float
        [alpha_pos, alpha_neg, beta, w, rho]
        - alpha_pos in [0,1]: learning rate when utility PE is positive (better than expected).
        - alpha_neg in [0,1]: learning rate when utility PE is negative (worse than expected).
        - beta in [0,10]: inverse temperature for softmax.
        - w in [0,1]: model-based planning weight.
        - rho in [0,1]: loss-aversion/risk parameter that scales the aversive utility of no-reward via STAI.
        
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, w, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    for t in range(n_trials):
        s = int(state[t])

        # Model-based lookahead from MF stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2
        q1 = (1.0 - w) * q_stage1_mf + w * q_stage1_mb

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2 = q_stage2_mf[s].copy()
        logits2 = beta * q2
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Anxiety-weighted utility: reward=1 -> +1; reward=0 -> negative utility scaled by rho*stai
        r_bin = float(reward[t])
        if r_bin >= 0.5:
            util = 1.0
        else:
            util = -rho * stai

        # TD errors in utility space
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        delta2 = util - q_stage2_mf[s, a2]

        # Valence-asymmetric learning rates
        alpha2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        alpha1 = alpha_pos if (delta1 + delta2) >= 0.0 else alpha_neg

        # Update second stage
        q_stage2_mf[s, a2] += alpha2 * delta2

        # Update first stage (simple bootstrapping plus utility-driven credit)
        q_stage1_mf[a1] += alpha1 * (0.5 * delta1 + 0.5 * delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll