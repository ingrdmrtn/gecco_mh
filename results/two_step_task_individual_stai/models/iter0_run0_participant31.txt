def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated arbitration and eligibility trace.
    
    This model blends a model-free (MF) first-stage value with a model-based (MB) plan,
    with the arbitration weight w increasing or decreasing as a function of anxiety (stai).
    A single learning rate governs updates, and an eligibility trace propagates second-stage
    reward prediction errors (RPEs) back to first-stage MF values. Likelihoods are computed
    from softmax policies at each stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float (typically 0 or 1)
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values indicate higher anxiety.
    model_parameters : array-like of floats, length 5
        [alpha, beta, w0, k_stai, lam]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w0 in [0,1]: base model-based weight.
        - k_stai in [0,1]: strength with which anxiety modulates the MB weight.
                           Positive values increase MB weight with higher anxiety if stai>0.5,
                           and decrease it if stai<0.5.
        - lam in [0,1]: eligibility trace parameter for propagating second-stage RPE to stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, w0, k_stai, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X and U->Y are common (0.7), cross are rare (0.3)
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)       # MF Q for first-stage actions [A,U]
    q_stage2_mf = np.zeros((2, 2))  # MF Q for second-stage actions in states [X,Y] x [0,1]

    # Anxiety-modulated arbitration weight (kept in [0,1])
    w = w0 + k_stai * (stai - 0.5)
    w = 0.0 if w < 0.0 else (1.0 if w > 1.0 else w)

    for t in range(n_trials):
        # Model-based Q for stage 1: expected max second-stage value under transition model
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [max in X, max in Y]
        q_stage1_mb = transition_matrix @ max_q_stage2  # [A, U]

        # Hybrid value for policy
        q1_hybrid = (1 - w) * q_stage1_mf + w * q_stage1_mb

        # Stage-1 policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)  # stability
        exp_q1 = np.exp(logits1)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy within observed state
        s = state[t]
        logits2 = beta * q_stage2_mf[s]
        logits2 -= np.max(logits2)
        exp_q2 = np.exp(logits2)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        # TD at stage 1 (toward current second-stage chosen value)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # TD at stage 2 with reward
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Eligibility trace: propagate stage-2 RPE to stage-1 MF value
        q_stage1_mf[a1] += lam * alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free SARSA(Î») with valence-asymmetric learning and anxiety-modulated exploration and perseveration.

    This purely model-free account allows different effective learning rates for positive
    versus negative RPEs (via a single asymmetry parameter). Anxiety reduces the inverse
    temperature (more exploration with higher anxiety) and increases perseveration (stickiness).
    Likelihoods are from softmax at each stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher scores => more exploration, more perseveration.
    model_parameters : array-like of floats, length 5
        [alpha_base, beta, k_posneg, pi0, k_stai]
        - alpha_base in [0,1]: base learning rate.
        - beta in [0,10]: base inverse temperature.
        - k_posneg in [0,1]: controls asymmetry between positive and negative RPE learning.
                             alpha_pos = alpha_base*(1 + k_posneg), alpha_neg = alpha_base*(1 - k_posneg).
        - pi0 in [0,1]: base perseveration strength added to the last chosen first-stage action.
        - k_stai in [0,1]: anxiety modulation strength; higher stai decreases beta and increases perseveration.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha_base, beta, k_posneg, pi0, k_stai = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective learning rates for valence
    alpha_pos = alpha_base * (1.0 + k_posneg)
    alpha_neg = alpha_base * (1.0 - k_posneg)
    alpha_pos = 1.0 if alpha_pos > 1.0 else (0.0 if alpha_pos < 0.0 else alpha_pos)
    alpha_neg = 1.0 if alpha_neg > 1.0 else (0.0 if alpha_neg < 0.0 else alpha_neg)

    # Anxiety-modulated exploration: higher stai -> lower beta_eff
    beta_eff = beta * (1.0 + (0.5 - stai) * k_stai * 1.5)
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Anxiety-modulated perseveration: higher stai -> more stickiness
    persev = pi0 + k_stai * stai
    persev = 0.0 if persev < 0.0 else (1.0 if persev > 1.0 else persev)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q1 = np.zeros(2)        # first-stage MF Q
    q2 = np.zeros((2, 2))   # second-stage MF Q

    prev_a1 = None

    for t in range(n_trials):
        # Stage-1 policy with perseveration bias on previous first-stage action
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = persev
        logits1 = beta_eff * q1 + bias
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (no perseveration here)
        s = state[ t ]
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning
        # Stage-1 TD toward second-stage chosen value (SARSA-style)
        delta1 = q2[s, a2] - q1[a1]
        a1_lr = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1[a1] += a1_lr * delta1

        # Stage-2 TD with reward
        r = reward[t]
        delta2 = r - q2[s, a2]
        a2_lr = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += a2_lr * delta2

        # Eligibility trace variant: propagate second-stage RPE to first stage
        q1[a1] += a2_lr * delta2  # uses the valence-appropriate rate

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-dependent credit assignment (TDL) with anxiety-modulated transition sensitivity and perseveration.

    This model implements a transition-outcome interaction at the first stage:
    rewards after common transitions reinforce the chosen first-stage action,
    whereas the same rewards after rare transitions shift credit toward the unchosen action.
    Anxiety increases or decreases the strength of this transition-dependent credit assignment.
    A standard MF learner operates at stage 2. Likelihoods come from softmax policies.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety modulates the transition sensitivity.
    model_parameters : array-like of floats, length 5
        [alpha, beta, gamma0, k_stai, rho]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - gamma0 in [0,1]: base transition-dependent credit assignment strength.
        - k_stai in [0,1]: modulation of gamma by anxiety: gamma = clip(gamma0 + k_stai*(stai-0.51), 0, 1).
        - rho in [0,1]: perseveration strength at stage 1 (bias toward repeating previous first-stage choice).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices under the model.
    """
    alpha, beta, gamma0, k_stai, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-modulated transition sensitivity
    gamma = gamma0 + k_stai * (stai - 0.51)
    gamma = 0.0 if gamma < 0.0 else (1.0 if gamma > 1.0 else gamma)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_tdl = np.zeros(2)      # first-stage values shaped by transition-dependent credit assignment
    q2 = np.zeros((2, 2))     # second-stage MF values

    prev_a1 = None

    for t in range(n_trials):
        # Stage-1 softmax with perseveration
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = rho
        logits1 = beta * q1_tdl + bias
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 softmax
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning at stage 2
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Transition-dependent credit assignment at stage 1
        # Common if chosen spaceship typically goes to the observed state (A->X, U->Y)
        is_common = 1 if (a1 == s) else 0
        sign_c = 1 if is_common == 1 else -1

        # Use the immediate second-stage RPE to drive first-stage credit assignment
        pe = delta2  # incorporating outcome surprise
        # Reinforce chosen action on common; shift to unchosen on rare
        q1_tdl[a1] += alpha * gamma * sign_c * pe
        q1_tdl[1 - a1] -= alpha * gamma * sign_c * pe

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll