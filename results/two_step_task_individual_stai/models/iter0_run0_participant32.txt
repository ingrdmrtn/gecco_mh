def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid model-based/model-free two-step learner with anxiety-modulated arbitration and stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (for the observed state; 0 or 1) for each trial.
    reward : array-like of float
        Obtained reward (gold coins) on each trial (typically in [0,1]).
    stai : array-like of float
        Anxiety score array; uses stai[0]. Higher values indicate higher anxiety.
    model_parameters : list or array-like of float
        [alpha, beta, omega, phi, lam]
        Bounds:
        - alpha: learning rate in [0,1]
        - beta: softmax inverse temperature in [0,10]
        - omega: model-based weight in [0,1]
        - phi: choice stickiness strength in [0,1]
        - lam: eligibility trace strength in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, omega, phi, lam = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize choice likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q_stage1_mf = np.zeros(2)          # model-free values for A/U
    q_stage2 = np.zeros((2, 2))        # second-stage Q-values per state (X,Y) x action (0,1)

    # Stickiness (previous choices)
    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    for t in range(n_trials):
        # Model-based evaluation for stage 1
        max_q2 = np.max(q_stage2, axis=1)  # best action per planet
        q_stage1_mb = transition_matrix @ max_q2

        # Anxiety-modulated arbitration: higher anxiety reduces MB control
        omega_eff = np.clip(omega * (1.0 - stai), 0.0, 1.0)

        # Combine MB and MF values
        q1_base = omega_eff * q_stage1_mb + (1.0 - omega_eff) * q_stage1_mf

        # Add anxiety-modulated stickiness at stage 1
        stick1 = phi * stai
        if prev_a1 is not None:
            bias_vec = np.array([1.0 if i == prev_a1 else 0.0 for i in range(2)])
        else:
            bias_vec = np.zeros(2)
        q1_policy = q1_base + stick1 * bias_vec

        # Softmax policy for stage 1
        exp_q1 = np.exp(beta * (q1_policy - np.max(q1_policy)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with stickiness within the reached state
        s = state[t]
        q2_state = q_stage2[s].copy()
        stick2 = phi * stai
        prev_a2 = prev_a2_by_state[s]
        if prev_a2 is not None:
            bias2 = np.array([1.0 if i == prev_a2 else 0.0 for i in range(2)])
        else:
            bias2 = np.zeros(2)
        q2_policy = q2_state + stick2 * bias2

        exp_q2 = np.exp(beta * (q2_policy - np.max(q2_policy)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2 (standard Q-learning)
        r = reward[t]
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Model-free learning at stage 1 with eligibility trace
        # Move Q1 toward the chosen second-stage value and additionally backpropagate reward via lambda
        td_to_q2 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * td_to_q2 + alpha * lam * delta2

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Pure model-free SARSA(λ)-style learner with asymmetric learning rates and anxiety-modulated learning/exploration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) for each trial.
    reward : array-like of float
        Obtained reward on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0]. Higher values indicate higher anxiety.
    model_parameters : list or array-like of float
        [alpha_pos, alpha_neg, beta, rho, eta]
        Bounds:
        - alpha_pos: learning rate for positive prediction errors in [0,1]
        - alpha_neg: learning rate for negative prediction errors in [0,1]
        - beta: softmax inverse temperature in [0,10]
        - rho: reward sensitivity scaling in [0,1]
        - eta: anxiety impact strength in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, rho, eta = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Value functions
    q1 = np.zeros(2)         # first-stage MF values
    q2 = np.zeros((2, 2))    # second-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated inverse temperature: higher anxiety -> more stochastic
    beta_eff = beta * (1.0 - 0.5 * eta * stai)

    # Effective SARSA(λ) trace strength as a function of anxiety and eta
    lam_eff = np.clip(0.1 + eta * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Stage 1 policy
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        exp_q2 = np.exp(beta_eff * (q2[s] - np.max(q2[s])))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward sensitivity and anxiety-modulated learning rate
        r = reward[t]
        r_eff = rho * r

        # Second-stage update (asymmetric learning rates)
        pe2 = r_eff - q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0 else alpha_neg
        # Anxiety reduces effective learning rate proportionally to eta
        alpha2 *= max(0.0, 1.0 - eta * stai)
        q2[s, a2] += alpha2 * pe2

        # First-stage update via SARSA(λ): credit assignment from stage 2
        # Target for Q1 is the chosen Q2 value; also backpropagate immediate outcome via lambda
        td1 = q2[s, a2] - q1[a1]
        # Use the same sign-based alpha for consistency
        alpha1 = alpha_pos if td1 >= 0 else alpha_neg
        alpha1 *= max(0.0, 1.0 - eta * stai)
        q1[a1] += alpha1 * td1 + alpha1 * lam_eff * pe2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model-based planner with anxiety-modulated transition certainty and epsilon-soft exploration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices in the reached state.
    reward : array-like of float
        Obtained reward on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0]. Higher values indicate higher anxiety.
    model_parameters : list or array-like of float
        [alpha, beta, kappa, epsilon, w2]
        Bounds:
        - alpha: second-stage learning rate baseline in [0,1]
        - beta: softmax inverse temperature in [0,10]
        - kappa: transition certainty weight in [0,1] (higher -> more confident in common transitions)
        - epsilon: baseline epsilon-greedy exploration in [0,1]
        - w2: scaling of alpha for second-stage learning in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, kappa, epsilon, w2 = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Anxiety reduces certainty about transitions: p_common pulls toward 0.5 as stai increases
    # Base common prob centered at 0.7, range toward 0.5 controlled by kappa and stai
    # p_common = 0.5 + (0.7 - 0.5) * (1 - stai) * (0.5 + 0.5 * kappa)
    p_common = 0.5 + 0.2 * (1.0 - stai) * (0.5 + 0.5 * kappa)
    p_common = float(np.clip(p_common, 0.5, 0.9))

    transition_matrix = np.array([[p_common, 1.0 - p_common],
                                  [1.0 - p_common, p_common]])

    # Epsilon-greedy level increases with anxiety
    eps_eff = float(np.clip(epsilon * (0.5 + stai), 0.0, 1.0))

    # Second-stage learning rate scaled by w2 and anxiety
    alpha2 = np.clip(alpha * (0.5 + 0.5 * w2) * (0.5 + 0.5 * stai), 0.0, 1.0)

    # Values
    q2 = np.zeros((2, 2))  # state-action values at stage 2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation for stage 1 (expectimax over Q2)
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Softmax for stage 1 then apply epsilon-soft mixing
        logits1 = beta * (q1_mb - np.max(q1_mb))
        sm1 = np.exp(logits1)
        sm1 /= np.sum(sm1)
        probs_1 = (1.0 - eps_eff) * sm1 + eps_eff * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy for reached state
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        sm2 = np.exp(logits2)
        sm2 /= np.sum(sm2)
        probs_2 = (1.0 - eps_eff) * sm2 + eps_eff * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Update second-stage Q-values
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Note: No model-free Q1 learning; planning updates Q1 implicitly via q2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik