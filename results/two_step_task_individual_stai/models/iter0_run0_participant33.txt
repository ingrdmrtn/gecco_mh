def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration, eligibility trace, and perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0/1)
        Second-stage state reached on each trial (0=X, 1=Y).
    action_2 : array-like of int (0/1)
        Second-stage choices (0/1) on each trial (e.g., W vs S on X; P vs H on Y).
    reward : array-like of float
        Reward received on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Trait anxiety score; use stai[0]. Higher means higher anxiety.
    model_parameters : tuple/list of floats
        (alpha, beta, w_base, lambda_e, persev)
        - alpha in [0,1]: learning rate for value updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w_base in [0,1]: baseline weight on model-based control at stage 1.
        - lambda_e in [0,1]: eligibility trace for backpropagating reward to stage 1 MF.
        - persev in [0,1]: perseveration strength added to the previously chosen first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w_base, lambda_e, persev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known (fixed) transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage2 = np.zeros((2, 2))     # Q at second stage: state x action
    q_stage1_mf = np.zeros(2)       # Model-free first-stage values

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    prev_a1 = None

    # Anxiety-modulated arbitration: higher anxiety reduces model-based weight
    # w in [0,1]; at stai=1, reduce w_base by 70%
    w = w_base * (1.0 - 0.7 * stai)
    w = max(0.0, min(1.0, w))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values via Bellman backup from stage 2
        mb_values = transition_matrix @ np.max(q_stage2, axis=1)  # shape (2,)

        # Perseveration bias (added to preferences)
        persev_eff = persev * (1.0 + 0.5 * stai)
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += persev_eff

        # Hybrid action values for stage 1
        q1 = w * mb_values + (1.0 - w) * q_stage1_mf + bias

        # Stage-1 policy and likelihood
        q1_shift = q1 - np.max(q1)  # numerical stability
        exp_q1 = np.exp(beta * q1_shift)
        probs_1 = exp_q1 / (np.sum(exp_q1) + 1e-12)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy and likelihood
        q2 = q_stage2[s, :].copy()
        q2_shift = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_shift)
        probs_2 = exp_q2 / (np.sum(exp_q2) + 1e-12)
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        # Second stage TD
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # First stage MF TD toward realized stage-2 value
        delta1 = q_stage2[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Eligibility trace backprop of reward prediction error to stage 1 MF
        q_stage1_mf[a1] += alpha * lambda_e * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-directed exploration with anxiety-modulated exploration bonus and hybrid arbitration.
    
    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0/1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0/1)
        Second-stage choices.
    reward : array-like of float
        Reward outcome per trial.
    stai : array-like of float
        Trait anxiety score; use stai[0].
    model_parameters : tuple/list of floats
        (alpha, beta, rho, lambda_e, tau)
        - alpha in [0,1]: learning rate for Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - rho in [0,1]: baseline uncertainty bonus weight (UCB-style).
        - lambda_e in [0,1]: eligibility trace from stage 2 to stage 1 MF.
        - tau in [0,1]: learning rate for tracking uncertainty (recent absolute PE).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, rho, lambda_e, tau = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure known to the agent
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values and uncertainty trackers
    q_stage2 = np.zeros((2, 2))   # second-stage Q
    u_stage2 = np.ones((2, 2))    # second-stage uncertainty proxy, init high
    q_stage1_mf = np.zeros(2)     # model-free first-stage values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety increases directed exploration bonus
    rho_eff = rho + stai * (1.0 - rho)
    rho_eff = max(0.0, min(1.0, rho_eff))

    # Arbitration: higher anxiety shifts away from MB (uses stai meaningfully)
    w_mb = 1.0 - 0.5 * stai  # in [0.5,1] when stai in [0,1]
    w_mb = max(0.0, min(1.0, w_mb))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Uncertainty-augmented second-stage values (UCB-style)
        q2_aug = q_stage2 + rho_eff * u_stage2

        # Model-based stage-1 values from augmented second-stage values
        mb_values = transition_matrix @ np.max(q2_aug, axis=1)

        # Hybrid first-stage values
        q1 = w_mb * mb_values + (1.0 - w_mb) * q_stage1_mf

        # Stage-1 policy
        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / (np.sum(probs_1) + 1e-12)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy uses augmented values for choice
        q2_s = q2_aug[s, :].copy()
        q2_s_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_s_shift)
        probs_2 = probs_2 / (np.sum(probs_2) + 1e-12)
        p_choice_2[t] = probs_2[a2]

        # Learning
        # Second stage TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Update uncertainty as exponentially weighted mean absolute PE
        u_stage2[s, a2] = (1.0 - tau) * u_stage2[s, a2] + tau * abs(delta2)

        # First stage MF updates: toward realized second-stage value + eligibility from reward PE
        q_stage1_mf[a1] += alpha * (q_stage2[s, a2] - q_stage1_mf[a1])
        q_stage1_mf[a1] += alpha * lambda_e * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid RL with anxiety-modulated transition learning and stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0/1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0/1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0/1)
        Second-stage choices.
    reward : array-like of float
        Reward outcome per trial.
    stai : array-like of float
        Trait anxiety score; use stai[0].
    model_parameters : tuple/list of floats
        (alpha_r, beta, alpha_t, stick, w_base)
        - alpha_r in [0,1]: reward learning rate for Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_t in [0,1]: transition learning rate for updating the transition matrix.
        - stick in [0,1]: first-stage choice stickiness (perseveration) strength.
        - w_base in [0,1]: baseline weight on model-based values at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_r, beta, alpha_t, stick, w_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix with prior consistent with task (can adapt)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q_stage2 = np.zeros((2, 2))
    q_stage1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight (higher anxiety -> less MB control)
    w = w_base * (1.0 - 0.6 * stai)
    w = max(0.0, min(1.0, w))

    # Anxiety-modulated stickiness
    stick_eff = stick * (1.0 + 0.5 * stai)

    prev_a1 = None

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based values using learned transitions
        mb_values = T @ np.max(q_stage2, axis=1)

        # Add stickiness bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += stick_eff

        # Hybrid stage-1 values
        q1 = w * mb_values + (1.0 - w) * q_stage1_mf + bias

        # Stage-1 policy
        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / (np.sum(probs_1) + 1e-12)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2_s = q_stage2[s, :].copy()
        q2_s_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_s_shift)
        probs_2 = probs_2 / (np.sum(probs_2) + 1e-12)
        p_choice_2[t] = probs_2[a2]

        # Learning: reward
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * delta2

        # First-stage MF TD toward realized second-stage value (no separate lambda here)
        q_stage1_mf[a1] += alpha_r * (q_stage2[s, a2] - q_stage1_mf[a1])
        # Also small backprop of reward PE to MF stage-1 to capture direct credit
        q_stage1_mf[a1] += alpha_r * 0.5 * delta2

        # Learning: transitions (row of chosen action toward observed state one-hot)
        onehot = np.array([0.0, 0.0])
        onehot[s] = 1.0
        alpha_t_eff = alpha_t * (1.0 + stai)  # anxiety increases sensitivity to transition evidence
        alpha_t_eff = max(0.0, min(1.0, alpha_t_eff))
        T[a1, :] = T[a1, :] + alpha_t_eff * (onehot - T[a1, :])

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)