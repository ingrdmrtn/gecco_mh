def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-weighted arbitration and eligibility trace.
    
    The agent learns second-stage action values and combines a model-based (MB)
    evaluation of first-stage actions with a model-free (MF) first-stage value.
    Anxiety (STAI) shifts the arbitration weight away from MB control.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial, indexed within the reached state.
    reward : array-like of float
        Reward obtained on each trial (e.g., coins).
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, lam, w0, w_stai]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature (softmax) for both stages.
        - lam in [0,1]: eligibility trace strength propagating reward to first-stage MF value.
        - w0 in [0,1]: baseline MB weight in arbitration.
        - w_stai in [0,1]: linear scaling of how anxiety shifts arbitration
                           (effective MB weight is clipped to [0,1]).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, lam, w0, w_stai = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))  # Q2[state, action]

    for t in range(n_trials):
        # Model-based first-stage action values: expect max Q2 under transitions
        max_q2 = np.max(q_stage2, axis=1)             # shape (2,)
        q_stage1_mb = transition_matrix @ max_q2      # shape (2,)

        # Anxiety-weighted arbitration (higher anxiety -> lower MB weight if w_stai>0)
        w_mb = w0 + w_stai * (0.5 - stai_val)
        w_mb = min(1.0, max(0.0, w_mb))

        # Hybrid first-stage Q
        q1_hybrid = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # First-stage policy
        q1 = q1_hybrid
        q1 = q1 - np.max(q1)
        probs1 = np.exp(beta * q1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        q2_logits = q_stage2[s].copy()
        q2_logits = q2_logits - np.max(q2_logits)
        probs2 = np.exp(beta * q2_logits)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace
        # Bootstrapped update towards Q2 plus additional reward-based eligibility
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + lam * alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-free with anxiety-modulated exploration and first-stage stickiness.
    
    The agent learns model-free action values at both stages. Anxiety reduces the
    effective inverse temperature (more anxious -> more random choices).
    A first-stage choice stickiness bias captures perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha1, beta, k_stai_beta, phi, alpha2]
        - alpha1 in [0,1]: learning rate for first-stage MF value.
        - beta in [0,10]: base inverse temperature.
        - k_stai_beta in [0,1]: scales how much anxiety reduces beta (beta_eff = beta*(1 - k*stai)).
        - phi in [0,1]: first-stage stickiness weight added to the previously chosen action.
        - alpha2 in [0,1]: second-stage learning rate.
        
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha1, beta, k_stai_beta, phi, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective inverse temperature reduced by anxiety
    beta_eff = beta * (1.0 - k_stai_beta * stai_val)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    prev_a1 = None

    for t in range(n_trials):
        # First-stage policy with stickiness
        logits1 = q_stage1.copy()
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            logits1 = logits1 + phi * stick
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta_eff * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (no stickiness)
        s = state[t]
        logits2 = q_stage2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta_eff * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning
        # Stage-2 update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Stage-1 bootstrapped MF update towards the obtained second-stage value
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1[a1]
        q_stage1[a1] += alpha1 * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid with learned transitions and anxiety-sensitive surprise arbitration.
    
    The agent learns the transition model online and uses it to compute model-based
    first-stage values. Arbitration between MB and MF control is reduced following
    surprising transitions, especially under higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, alpha_t, w0, k_surprise]
        - alpha in [0,1]: learning rate for MF values at both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_t in [0,1]: learning rate for the transition matrix.
        - w0 in [0,1]: baseline MB arbitration weight.
        - k_surprise in [0,1]: scales how much anxiety amplifies surprise-induced reduction
                               of MB control (effective reduction âˆ k_surprise*stai*surprise).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, alpha_t, w0, k_surprise = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transitions T[action, state], rows sum to 1
    T = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Surprise from previous trial (start neutral)
    prev_surprise = 0.0

    for t in range(n_trials):
        # Model-based valuation from currently learned transitions
        max_q2 = np.max(q_stage2, axis=1)     # (2,)
        q_stage1_mb = T @ max_q2              # (2,)

        # Arbitration weight reduced by previous surprise scaled by anxiety
        # w_mb = clip(w0 - k_surprise * stai * prev_surprise, 0, 1)
        w_mb = w0 - k_surprise * stai_val * prev_surprise
        w_mb = min(1.0, max(0.0, w_mb))

        q1_hybrid = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # First-stage policy
        logits1 = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = q_stage2[s] - np.max(q_stage2[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Compute transition surprise BEFORE updating T
        prob_trans = T[a1, s]
        eps = 1e-12
        prev_surprise = -np.log(prob_trans + eps)

        # Learning: transitions
        # Target is one-hot for observed state
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T[a1, s_idx] += alpha_t * (target - T[a1, s_idx])
        # Re-normalize to guard against numerical drift
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, 0] /= row_sum
            T[a1, 1] /= row_sum

        # Learning: values
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * delta2

        # Stage-1 MF update (bootstrapped + full reward eligibility)
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)