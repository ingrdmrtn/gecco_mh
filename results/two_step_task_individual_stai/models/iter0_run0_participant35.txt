def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with eligibility trace and anxiety-weighted arbitration.

    The model combines a model-based (MB) planner that uses the known transition matrix
    with a model-free (MF) learner at the first stage. The arbitration weight placed on
    the MB component decreases with anxiety (stai). Second-stage values are learned via
    TD learning; first-stage MF values receive an eligibility-trace update from the
    second-stage TD error.

    Inputs
    - action_1: array of ints in {0,1}, first-stage choice (0=A, 1=U)
    - state: array of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array of ints in {0,1}, second-stage choice on the reached state
    - reward: array of floats (e.g., 0/1), reward received on each trial
    - stai: array-like of length 1; scalar anxiety score in [0,1]
    - model_parameters: tuple/list of 5 parameters:
        learning_rate: alpha in [0,1] for both stages
        beta1: inverse temperature for first-stage softmax in [0,10]
        beta2: inverse temperature for second-stage softmax in [0,10]
        w_base: baseline MB weight in [0,1]
        lam: eligibility trace parameter in [0,1]

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.

    Notes on anxiety usage:
    - Effective MB weight at stage 1 is w_mb = w_base * (1 - stai), so higher anxiety
      shifts arbitration toward model-free control.
    """
    alpha, beta1, beta2, w_base, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition matrix: rows are actions (A=0, U=1), columns are next states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of the chosen actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize value functions
    q_stage1_mf = np.zeros(2)        # MF Q-values for first stage actions
    q_stage2_mf = np.zeros((2, 2))   # MF Q-values for second stage actions per state

    # Anxiety-modulated arbitration
    w_mb = max(0.0, min(1.0, w_base * (1.0 - stai)))  # clamp to [0,1]

    for t in range(n_trials):
        # Model-based first-stage Q: expectation over next-state max Q2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)         # size 2 (states)
        q_stage1_mb = transition_matrix @ max_q_stage2     # size 2 (actions)

        # Hybrid first-stage Q
        q1 = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # First-stage policy
        q1_centered = q1 - np.max(q1)  # numerical stability
        exp_q1 = np.exp(beta1 * q1_centered)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in observed state
        s = int(state[t])
        q2 = q_stage2_mf[s]
        q2_centered = q2 - np.max(q2)
        exp_q2 = np.exp(beta2 * q2_centered)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # TD updates
        r = reward[t]

        # Second-stage update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # First-stage MF update with eligibility trace from second stage
        # Immediate TD term plus bootstrapped credit from the outcome
        # delta1 uses the pre-update Q2 value (as in SARSA-like propagation)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + alpha * lam * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free RL with decay, transition-surprise-modulated choice precision,
    and anxiety-amplified choice stickiness.

    The model learns MF Q-values at both stages with forgetting (decay) to track
    nonstationarity. First-stage choice precision (beta) is modulated by how
    surprising the observed transition is, and overall precision is reduced by
    higher anxiety. There is also a perseveration (stickiness) bias on first-stage
    choices that is amplified by anxiety.

    Inputs
    - action_1: array of ints in {0,1}, first-stage choice (0=A, 1=U)
    - state: array of ints in {0,1}, reached state (0=X, 1=Y)
    - action_2: array of ints in {0,1}, second-stage action
    - reward: array of floats, reward each trial
    - stai: array-like of length 1; scalar anxiety score in [0,1]
    - model_parameters: tuple/list of 5 parameters:
        alpha: learning rate in [0,1]
        beta_base: baseline inverse temperature in [0,10]
        k_surprise: modulation of beta by transition surprise in [0,1]
        stickiness: strength of choice perseveration in [0,1]
        gamma_decay: value decay per trial in [0,1] (0=no decay, 1=full decay to 0)

    Returns
    - Negative log-likelihood of observed choices.

    Notes on anxiety usage:
    - Effective beta is scaled by (1 - 0.5*stai), so anxiety lowers choice precision.
    - Stickiness is amplified by anxiety: bias_strength = stickiness*(0.5 + 0.5*stai).
    """
    alpha, beta_base, k_surprise, stickiness, gamma_decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure for surprise computation
    # P(state=X | A)=0.7, P(state=Y | U)=0.7
    def p_state_given_action(s, a):
        if a == 0:  # A
            return 0.7 if s == 0 else 0.3
        else:       # U
            return 0.7 if s == 1 else 0.3

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Stickiness memory (which action was taken previously)
    last_a1 = None
    bias_vec = np.zeros(2)

    for t in range(n_trials):
        # Decay values toward zero to track drifting rewards
        q1 *= (1.0 - gamma_decay)
        q2 *= (1.0 - gamma_decay)

        # Compute surprise for current trial from observed transition
        a1 = int(action_1[t])
        s = int(state[t])
        pr = p_state_given_action(s, a1)  # probability of observed state given chosen action
        surprise = 1.0 - pr               # rare transition -> 0.3 prob -> surprise=0.7; common -> 0.3

        # Effective beta: baseline scaled by surprise and anxiety
        beta_eff = beta_base * (1.0 + k_surprise * (surprise - 0.5))
        beta_eff *= (1.0 - 0.5 * stai)
        beta_eff = min(10.0, max(1e-6, beta_eff))

        # First-stage bias from stickiness
        if last_a1 is None:
            bias_vec[:] = 0.0
        else:
            bias_strength = stickiness * (0.5 + 0.5 * stai)  # anxiety amplifies perseveration
            bias_vec[:] = 0.0
            bias_vec[last_a1] = bias_strength
            bias_vec[1 - last_a1] = -bias_strength

        # First-stage policy with bias added to Q
        q1_biased = q1 + bias_vec
        q1c = q1_biased - np.max(q1_biased)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 /= np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in observed state (use same beta_eff for parsimony)
        a2 = int(action_2[t])
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 /= np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning updates (pure MF)
        r = reward[t]
        # Stage 2 TD
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage 1 TD bootstrapping from the reached state's max Q2
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

        # Update last choice for stickiness
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model with valence-asymmetric learning rates and anxiety-weighted belief in transitions.

    This model uses:
    - Valence-asymmetric learning at the second stage (separate alphas for positive vs. negative PE).
    - Anxiety-weighted belief in the correctness of the transition model to compute model-based
      first-stage values. As anxiety increases, the agent trusts the transition model less and
      blends toward a uniform (uninformative) transition model.
    - An eligibility-like backpropagation from second-stage outcomes to first-stage MF values,
      with a stronger credit assignment on common transitions (eta_common) than on rare ones.

    Inputs
    - action_1: array of ints in {0,1}, first-stage choice (0=A, 1=U)
    - state: array of ints in {0,1}, reached state (0=X, 1=Y)
    - action_2: array of ints in {0,1}, second-stage action
    - reward: array of floats, reward each trial
    - stai: array-like of length 1; scalar anxiety score in [0,1]
    - model_parameters: tuple/list of 5 parameters:
        alpha_pos: learning rate for positive PE in [0,1]
        alpha_neg: learning rate for negative PE in [0,1]
        beta: inverse temperature for both stages in [0,10]
        eta_common: eligibility for common transitions in [0,1]
        k_anx: anxiety impact on transition-belief and negative-learning tilt in [0,1]

    Returns
    - Negative log-likelihood of observed choices.

    Notes on anxiety usage:
    - Belief in transitions: belief = 1 - stai * k_anx. Effective transition is
      belief*T + (1-belief)*U, where U is uniform (0.5,0.5).
    - Learning-rate tilt: negative PEs get relatively larger updates as anxiety increases.
    """
    alpha_pos, alpha_neg, beta, eta_common, k_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Base transition model and uniform fallback
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    U = np.array([[0.5, 0.5],
                  [0.5, 0.5]])

    # Anxiety-weighted belief in transition structure
    belief = 1.0 - stai * k_anx
    belief = max(0.0, min(1.0, belief))
    T_eff = belief * T + (1.0 - belief) * U

    # Initialize value functions
    q1_mf = np.zeros(2)      # first-stage MF Q-values
    q2 = np.zeros((2, 2))    # second-stage Q-values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper to choose eligibility based on whether transition is common
    def is_common_transition(a, s):
        # common if (A -> X) or (U -> Y)
        return (a == 0 and s == 0) or (a == 1 and s == 1)

    for t in range(n_trials):
        # Model-based first-stage Q from effective transitions and current Q2
        max_q2 = np.max(q2, axis=1)           # per-state max
        q1_mb = T_eff @ max_q2                # expected max value for each action

        # Arbitration weight equals belief; combine MB and MF
        w_mb = belief
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage policy
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = int(state[t])
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        exp_q2 = np.exp(beta * q2c)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        r = reward[t]
        pe2 = r - q2[s, a2]

        # Valence-asymmetric learning rates, anxiety tilts negative PEs upward
        if pe2 >= 0:
            alpha_eff = alpha_pos * (1.0 - 0.5 * stai * k_anx)
        else:
            alpha_eff = alpha_neg * (1.0 + 0.5 * stai * k_anx)
        # clamp to [0,1]
        alpha_eff = max(1e-8, min(1.0, alpha_eff))
        q2[s, a2] += alpha_eff * pe2

        # Backpropagate to first-stage MF with transition-dependent eligibility
        common = is_common_transition(a1, s)
        eta = eta_common if common else (1.0 - eta_common)
        # Use updated q2 value as bootstrap target
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll