def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated arbitration and lapse.
    
    This model maintains second-stage Q-values (Q2) and a model-free first-stage Q (Q1_mf).
    A model-based first-stage value (Q1_mb) is computed via the known transition model.
    Anxiety (stai) down-weights the model-based contribution (more anxiety -> more model-free).
    A small lapse mixes softmax choice with uniform randomness.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; e.g., alien index within the planet).
    reward : array-like of float (e.g., 0.0 or 1.0)
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s). We take stai[0] as the participant's score in [0,1] scale.
        Higher stai reduces the model-based arbitration weight and increases the effective lapse impact.
    model_parameters : list or array
        [alpha, beta, w_base, lambda_elig, xi]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax choice.
        - w_base in [0,1]: baseline model-based weight at stai=0.
        - lambda_elig in [0,1]: eligibility trace strength for backing up Q1 from Q2.
        - xi in [0,1]: lapse rate mixing softmax with uniform choice at stage 1.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, w_base, lambda_elig, xi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed, known transition structure: A->X and U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q1_mf = np.zeros(2)           # model-free first-stage values
    Q2 = np.zeros((2, 2))         # second-stage values: state x action

    # Anxiety-modulated arbitration and lapse
    # High anxiety reduces the model-based contribution and slightly boosts the lapse effect.
    w_eff_scale = max(0.0, 1.0 - stai)  # bounded scaling
    xi_eff = np.clip(xi * (1.0 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based first-stage Q from current second-stage values via known transitions
        max_Q2 = np.max(Q2, axis=1)              # best action at each state
        Q1_mb = transition_matrix @ max_Q2       # expected value for each first-stage action

        # Anxiety-adjusted arbitration weight
        w = np.clip(w_base * w_eff_scale, 0.0, 1.0)

        # Combine MB and MF for first-stage decision
        Q1 = (1.0 - w) * Q1_mf + w * Q1_mb

        # First-stage policy with lapse
        logits1 = beta * Q1
        logits1 = logits1 - np.max(logits1)  # numerical stability
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - xi_eff) * probs1 + xi_eff * 0.5  # mix with uniform over 2 actions
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * Q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage 2 TD update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage 1 TD(Î») update towards current second-stage value of chosen branch
        # Back up the chosen second-stage action value into first-stage chosen action
        target1 = Q2[s, a2]
        delta1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * lambda_elig * delta1

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-free RL with anxiety-modulated learning rates and transition-outcome-dependent perseveration.
    
    The agent learns Q2 at the second stage and a model-free Q1 at the first stage via TD backup from Q2.
    Anxiety reduces learning rates and increases perseveration/stay tendencies, especially after
    common rewarded vs. rare rewarded transitions (transition-outcome interaction via a bias term).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float
        Rewards obtained on each trial.
    stai : array-like of float
        Anxiety score(s). We use stai[0] in [0,1]. Higher stai reduces learning rates and
        amplifies perseveration/transition-outcome bias.
    model_parameters : list or array
        [alpha1, alpha2, beta, rho, k_alpha]
        - alpha1 in [0,1]: first-stage learning rate.
        - alpha2 in [0,1]: second-stage learning rate.
        - beta in [0,10]: inverse temperature for both stages.
        - rho in [0,1]: base perseveration strength (stay bias) at stage 1.
        - k_alpha in [0,1]: anxiety sensitivity scaling; effective alphas decrease with stai.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha1, alpha2, beta, rho, k_alpha = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective learning rates reduced by anxiety
    alpha1_eff = np.clip(alpha1 * (1.0 - k_alpha * stai), 1e-6, 1.0)
    alpha2_eff = np.clip(alpha2 * (1.0 - 0.5 * k_alpha * stai), 1e-6, 1.0)

    # Transition structure for determining common vs. rare (for bias only)
    # A->X, U->Y are common (0.7), the other outcomes are rare (0.3).
    def is_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    # Values
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous first-stage action and previous transition-outcome
    prev_a1 = None
    prev_common = 0.0
    prev_reward = 0.0

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Build bias vector for perseveration and transition-outcome interaction
        bias = np.zeros(2)
        if prev_a1 is not None:
            # Base perseveration enhanced by anxiety
            stay_strength = rho * (1.0 + stai)
            # Transition-outcome interaction: after common+reward, stronger stay; after rare+reward, encourage switch.
            toi = (1.0 if prev_common > 0.5 else -1.0) * (2.0 * prev_reward - 1.0)  # in {-1, +1}
            # Anxiety amplifies this effect
            toi_strength = stay_strength * 0.5 * stai * toi
            bias[prev_a1] += stay_strength + toi_strength

        # Stage 1 policy with bias
        logits1 = beta * Q1 + bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * Q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Second stage
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha2_eff * delta2

        # First stage (TD backup from second stage chosen value)
        target1 = Q2[s, a2]
        delta1 = target1 - Q1[a1]
        Q1[a1] += alpha1_eff * delta1

        # Update previous markers for next trial's bias
        prev_a1 = a1
        prev_common = 1.0 if is_common(a1, s) else 0.0
        prev_reward = r

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with uncertainty-sensitive arbitration and anxiety-modulated temperature.
    
    The agent learns both reward values (Q2) and transition probabilities (T_hat).
    First-stage values combine model-based planning using T_hat with a model-free Q1 via arbitration.
    Arbitration depends on transition uncertainty (entropy) and is down-weighted by anxiety.
    Anxiety also reduces the effective inverse temperature (more exploration when anxious).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Observed second-stage state.
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float
        Rewards obtained on each trial.
    stai : array-like of float
        Anxiety score(s). We use stai[0] in [0,1]. Higher stai reduces arbitration weight and inverse temperature.
    model_parameters : list or array
        [alpha_r, beta, alpha_t, w, k_temp]
        - alpha_r in [0,1]: reward learning rate for both stages (Q updates).
        - beta in [0,10]: base inverse temperature.
        - alpha_t in [0,1]: transition learning rate for updating T_hat.
        - w in [0,1]: base arbitration weight (MB fraction) at low uncertainty and low anxiety.
        - k_temp in [0,1]: anxiety sensitivity for temperature: beta_eff = beta * (1 - k_temp * stai).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha_r, beta, alpha_t, w, k_temp = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize transition estimates (start from the typical structure but allow learning)
    T_hat = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-reduced inverse temperature (more anxiety -> more exploration)
    beta_eff = beta * max(0.0, 1.0 - k_temp * stai)

    def entropy_row(p_row):
        # p_row is length-2 probs; entropy normalized to [0,1] by dividing by ln(2)
        p = np.clip(p_row, 1e-8, 1.0)
        p = p / np.sum(p)
        H = -np.sum(p * np.log(p)) / np.log(2.0)
        return np.clip(H, 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute MB Q1 from current T_hat and Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_hat @ max_Q2

        # Uncertainty-sensitive arbitration:
        # Use mean entropy across the two first-stage actions;
        # higher entropy -> rely less on MB; anxiety further down-weights MB.
        H_mean = 0.5 * (entropy_row(T_hat[0]) + entropy_row(T_hat[1]))
        w_unc = w * (1.0 - H_mean)            # reduce MB when transitions are uncertain
        w_anx = w_unc * (1.0 - stai)          # reduce MB further with anxiety
        w_eff = np.clip(w_anx, 0.0, 1.0)

        Q1 = (1.0 - w_eff) * Q1_mf + w_eff * Q1_mb

        # Stage 1 policy
        logits1 = beta_eff * Q1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta_eff * Q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Reward learning updates
        # Second stage
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * delta2

        # First stage (model-free backup from second stage value)
        target1 = Q2[s, a2]
        delta1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_r * delta1

        # Transition learning update for chosen action based on observed state
        # Move T_hat[a1] toward one-hot of observed s
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T_hat[a1, sp] += alpha_t * (target - T_hat[a1, sp])
        # Ensure row remains normalized and within bounds
        T_hat[a1] = np.clip(T_hat[a1], 1e-6, 1.0)
        T_hat[a1] = T_hat[a1] / np.sum(T_hat[a1])

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)