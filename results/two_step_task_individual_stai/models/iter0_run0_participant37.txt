def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated arbitration and eligibility.
    
    This model blends model-based (MB) and model-free (MF) values at stage 1.
    The blend weight is modulated by anxiety (stai). A TD(Î») style eligibility trace
    propagates reward prediction errors from stage 2 to stage 1.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0: planet X, 1: planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1: alien on the current planet) for each trial.
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; used to modulate arbitration weight.
    model_parameters : iterable of floats
        [alpha, beta, lambda_, w_base, w_stai]
        - alpha in [0,1]: learning rate for MF values.
        - beta in [0,10]: inverse temperature for both stages.
        - lambda_ in [0,1]: eligibility trace strength from stage 2 to stage 1.
        - w_base in [0,1]: baseline MB weight at stage 1.
        - w_stai in [0,1]: how much anxiety shifts MB weight (positive -> more MB with higher stai).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, lambda_, w_base, w_stai = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    # Storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q_stage1_mf = np.zeros(2)          # MF values for spaceships [A,U]
    q_stage2_mf = np.zeros((2, 2))     # MF values for aliens by planet: [X,Y] x [alien0, alien1]

    # Anxiety-modulated MB weight (clipped to [0,1])
    weight_mb = w_base + w_stai * (stai - 0.51)
    weight_mb = min(1.0, max(0.0, weight_mb))

    for t in range(n_trials):
        # MODEL-BASED stage-1 values: expected max value after transition
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [V(X), V(Y)]
        q_stage1_mb = transition_matrix @ max_q_stage2  # [Q_MB(A), Q_MB(U)]

        # Hybrid stage-1 values
        q1 = weight_mb * q_stage1_mb + (1.0 - weight_mb) * q_stage1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy for the reached state
        s = state[t]
        q2 = q_stage2_mf[s, :]
        logits2 = beta * q2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Stage-2 MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility trace from stage-2
        delta1_bootstrap = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1_bootstrap + lambda_ * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-modulated transition learning and stickiness.
    
    Participants learn the first-stage transition probabilities over time. Anxiety (stai)
    increases assumed volatility: higher stai -> larger transition learning rate. A
    perseveration (stickiness) bias at stage 1 is also scaled by anxiety. Hybrid MB/MF
    control is combined with an anxiety-determined MB weight (no extra parameter).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage action on the reached planet.
    reward : array-like of float
        Coins received each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; used to modulate transition learning and stickiness.
    model_parameters : iterable of floats
        [alpha, beta, alpha_T0, alpha_Tgain, kappa]
        - alpha in [0,1]: reward learning rate for MF Q values.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_T0 in [0,1]: baseline transition learning rate.
        - alpha_Tgain in [0,1]: how much anxiety increases transition learning rate.
        - kappa in [0,1]: baseline perseveration strength (adds bias toward last chosen ship).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, alpha_T0, alpha_Tgain, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T[a, s]
    T = np.full((2, 2), 0.5)  # start uncertain

    # MF values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate
    alpha_T = alpha_T0 + alpha_Tgain * stai
    alpha_T = min(1.0, max(0.0, alpha_T))

    # Anxiety-modulated perseveration magnitude
    stick = kappa * (1.0 + 0.5 * (stai - 0.51))

    # Anxiety-determined MB weight (no extra parameter): higher anxiety -> less MB
    weight_mb = 0.5 + 0.4 * (0.51 - stai)
    weight_mb = min(1.0, max(0.0, weight_mb))

    prev_a1 = -1  # no previous choice initially

    for t in range(n_trials):
        # MB values from learned transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [V(X), V(Y)]
        q_stage1_mb = T @ max_q_stage2

        # Hybrid stage-1 values
        q1 = weight_mb * q_stage1_mb + (1.0 - weight_mb) * q_stage1_mf

        # Add perseveration bias toward previous choice
        bias = np.zeros(2)
        if prev_a1 >= 0:
            bias[prev_a1] = stick

        logits1 = beta * q1 + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        q2 = q_stage2_mf[s, :]
        logits2 = beta * q2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update learned transitions for the chosen first-stage action
        # Move probability mass toward the observed state
        # T[a, s_obs] += alpha_T*(1 - T[a, s_obs]); T[a, s_other] = 1 - T[a, s_obs]
        T[a1, s] += alpha_T * (1.0 - T[a1, s])
        T[a1, 1 - s] = 1.0 - T[a1, s]

        # MF learning
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Eligibility trace from stage 2 to stage 1: strength = stai (uses anxiety without extra param)
        lam = stai
        delta1_bootstrap = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1_bootstrap + lam * delta2)

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated exploration and outcome sensitivity in a pure model-free agent.
    
    This model uses a two-step MF learner with an eligibility trace equal to the stai score.
    Anxiety modulates:
      - exploratory temperature: beta_eff = beta * (1 - tau * stai)
      - outcome sensitivity: rewards are scaled as r_eff = r * (eta0 + eta1 * stai)

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: A, 1: U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0: X, 1: Y).
    action_2 : array-like of int (0 or 1)
        Second-stage action on the reached planet.
    reward : array-like of float
        Coins received each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; sets eligibility strength and modulates beta and outcomes.
    model_parameters : iterable of floats
        [alpha, beta, tau, eta0, eta1]
        - alpha in [0,1]: learning rate for MF values.
        - beta in [0,10]: base inverse temperature.
        - tau in [0,1]: how much anxiety reduces beta (higher tau -> more randomness with anxiety).
        - eta0 in [0,1]: baseline outcome sensitivity multiplier.
        - eta1 in [0,1]: how much anxiety increases outcome sensitivity.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, tau, eta0, eta1 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective inverse temperature and eligibility trace
    beta_eff = beta * (1.0 - tau * stai)
    beta_eff = max(0.0, min(10.0, beta_eff))
    lam = stai  # eligibility trace equal to anxiety (bounded in [0,1])

    # MF values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage-1 policy
        logits1 = beta_eff * q_stage1_mf
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        q2 = q_stage2_mf[s, :]
        logits2 = beta_eff * q2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Anxiety-modulated subjective reward
        r_eff = reward[t] * (eta0 + eta1 * stai)

        # Learning updates
        delta2 = r_eff - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        delta1_bootstrap = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1_bootstrap + lam * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll