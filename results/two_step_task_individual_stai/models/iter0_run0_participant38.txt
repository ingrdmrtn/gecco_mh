def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB–MF with anxiety-dependent arbitration and MF eligibility.
    This model blends model-based (MB) and model-free (MF) action values at stage 1,
    with the arbitration weight decreasing as anxiety (stai) increases. Stage-2 values
    are learned via TD; stage-1 MF values use an eligibility trace from the stage-2 TD error.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (two aliens available on each planet).
    reward : array-like of float (typically 0 or 1)
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Participant anxiety score. Higher values indicate higher anxiety.
    model_parameters : iterable of floats
        [alpha, lambd, beta, base_w, anx_gain]
        - alpha: [0,1] learning rate for TD updates at both stages.
        - lambd: [0,1] eligibility trace scaling MF credit from stage-2 back to stage-1.
        - beta: [0,10] inverse temperature for softmax choice at both stages.
        - base_w: [0,1] baseline MB weight in the hybrid value at stage 1.
        - anx_gain: [0,1] how strongly anxiety reduces MB arbitration weight.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, lambd, beta, base_w, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: rows = first-stage actions (A=0, U=1); cols = states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize MF action values
    q1_mf = np.zeros(2)          # stage-1 MF Q for A/U
    q2 = np.zeros((2, 2))        # stage-2 MF Q for each state (X/Y) and action (two aliens)

    for t in range(n_trials):
        # Model-based stage-1 values from current stage-2 MF values
        max_q2 = np.max(q2, axis=1)             # value of best alien on each planet
        q1_mb = transition_matrix @ max_q2      # MB forward planning

        # Anxiety-dependent arbitration weight: higher anxiety -> lower MB weight
        w = base_w * (1.0 - anx_gain) + (1.0 - stai) * anx_gain
        w = min(max(w, 0.0), 1.0)

        # Hybrid stage-1 values
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy and likelihood of observed choice
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy given observed state
        s = state[t]
        exp_q2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # TD updates
        r = reward[t]

        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update: SARSA(λ)-style credit from stage-2 back to chosen stage-1 action
        # Immediate consistency term plus eligibility trace on reward
        # First a consistency update towards q2 value at visited state-action
        delta1_consistency = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1_consistency
        # Then eligibility credit from reward
        q1_mf[a1] += alpha * lambd * delta2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-modulated exploration with anxiety-damped inverse temperature.
    Model-based values drive stage-1 choices; stage-2 values are learned with TD.
    Exploration temperature is reduced by both estimated uncertainty and anxiety.
    Includes choice stickiness at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (two aliens available per state).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score. Higher values increase temperature damping.
    model_parameters : iterable of floats
        [alpha, beta, kappa_u, anx_temp, stickiness]
        - alpha: [0,1] learning rate for updating stage-2 values.
        - beta: [0,10] base inverse temperature.
        - kappa_u: [0,1] uncertainty learning rate (EWMA of squared TD error).
        - anx_temp: [0,1] scales how much anxiety reduces effective beta.
        - stickiness: [0,1] choice perseveration weight added to previously chosen action.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, kappa_u, anx_temp, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 value estimates and uncertainty (per state-action)
    q2 = np.zeros((2, 2))
    unc2 = np.zeros((2, 2))  # running estimate of variance via EWMA of squared TD error

    # For stickiness
    last_a1 = None
    last_a2 = [None, None]  # per state

    for t in range(n_trials):
        # Uncertainty-weighted inverse temperature factor for stage 1:
        # use the expected uncertainty under the transition model, using max-uncertain action per state
        max_q2 = np.max(q2, axis=1)
        # approximate relevant uncertainty for each next-state as uncertainty of its greedy action
        greedy_idx = np.argmax(q2, axis=1)
        unc_next = np.array([unc2[0, greedy_idx[0]], unc2[1, greedy_idx[1]]])
        exp_unc_stage1 = transition_matrix @ unc_next
        # anxiety reduces beta; higher uncertainty further reduces it
        beta1_eff = beta * (1.0 - anx_temp * stai) / (1.0 + np.mean(exp_unc_stage1))

        # MB stage-1 values
        q1_mb = transition_matrix @ max_q2

        # Add stickiness bias to the previously chosen first-stage action
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stickiness

        # Stage-1 policy
        logits1 = beta1_eff * q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy given observed state, with uncertainty- and anxiety-damped beta
        s = state[t]
        # local uncertainty for current state based on its greedy action's uncertainty
        g_idx = int(np.argmax(q2[s]))
        local_unc = unc2[s, g_idx]
        beta2_eff = beta * (1.0 - anx_temp * stai) / (1.0 + local_unc)

        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += stickiness

        logits2 = beta2_eff * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observed reward
        r = reward[t]

        # Stage-2 TD error and updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update uncertainty as EWMA of squared TD error
        unc2[s, a2] = (1.0 - kappa_u) * unc2[s, a2] + kappa_u * (delta2 ** 2)

        # Update trackers for stickiness
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """MF SARSA(λ) with transition-surprise credit assignment and anxiety-dependent asymmetry.
    Model-free values drive both stages. Learning rate is asymmetric for positive vs negative
    TD errors, with asymmetry increasing as anxiety decreases (i.e., low anxiety -> more positivity bias).
    Credit assignment back to stage 1 is scaled by transition surprise, amplified by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = A, 1 = U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = X, 1 = Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Coins received per trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score. Higher anxiety amplifies transition-surprise reweighting and
        reduces positive/negative learning asymmetry.
    model_parameters : iterable of floats
        [alpha_base, beta, rho, eta_asym, stickiness]
        - alpha_base: [0,1] base learning rate for TD updates.
        - beta: [0,10] inverse temperature for softmax at both stages.
        - rho: [0,1] strength of transition-surprise scaling of eligibility.
        - eta_asym: [0,1] maximum asymmetry factor between positive and negative learning.
        - stickiness: [0,1] choice perseveration weight at both stages.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, beta, rho, eta_asym, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition mapping for detecting common vs rare:
    # A(0)->X(0) common, U(1)->Y(1) common
    def is_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)        # MF stage-1 values
    q2 = np.zeros((2, 2))   # MF stage-2 values

    last_a1 = None
    last_a2 = [None, None]

    for t in range(n_trials):
        # Softmax with stickiness at stage 1
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stickiness
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy given state
        s = state[t]
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += stickiness
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Asymmetric learning rates modulated by anxiety:
        # When anxiety is low, asymmetry is larger; when high, closer to symmetric.
        asym_scale = eta_asym * (1.0 - stai)
        alpha_pos = np.clip(alpha_base * (1.0 + asym_scale), 0.0, 1.0)
        alpha_neg = np.clip(alpha_base * (1.0 - asym_scale), 0.0, 1.0)

        # Stage-2 TD error and update with asymmetry
        delta2 = r - q2[s, a2]
        if delta2 >= 0:
            q2[s, a2] += alpha_pos * delta2
            alpha2_eff = alpha_pos
        else:
            q2[s, a2] += alpha_neg * delta2
            alpha2_eff = alpha_neg

        # Transition surprise scaling for eligibility:
        # common -> scale = 1 - rho*stai (downweight credit), rare -> 1 + rho*stai (upweight)
        if is_common(a1, s):
            elig_scale = 1.0 - rho * stai
        else:
            elig_scale = 1.0 + rho * stai
        # keep within reasonable bounds
        elig_scale = max(0.0, min(2.0, elig_scale))

        # Stage-1 MF update: propagate stage-2 TD error via eligibility
        q1[a1] += alpha2_eff * elig_scale * delta2

        # Update stickiness trackers
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood