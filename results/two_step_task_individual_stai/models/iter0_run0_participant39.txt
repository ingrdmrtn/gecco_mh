def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated planning weight and stickiness.
    
    The agent learns second-stage action values model-free and forms first-stage
    model-based values via the transition model. First-stage choice uses a hybrid
    mix of model-based and model-free values plus a perseveration bias. Anxiety
    (stai) reduces planning weight and increases perseveration strength.

    Parameters
    - action_1: np.array of shape (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state: np.array of shape (n_trials,), second-stage state indices (0=planet X, 1=planet Y)
    - action_2: np.array of shape (n_trials,), second-stage actions (0/1; e.g., alien choices)
    - reward: np.array of shape (n_trials,), outcomes (e.g., coins; scaled 0/1)
    - stai: np.array of shape (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha1: [0,1] learning rate for stage-1 MF values
        alpha2: [0,1] learning rate for stage-2 MF values
        beta:   [0,10] inverse temperature for both stages
        w_mb_base: [0,1] baseline weight on model-based control (before anxiety)
        kappa: [0,1] perseveration strength at stage 1 (bias toward previous a1)
    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha1, alpha2, beta, w_mb_base, kappa = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])  # trait

    # Transition structure: rows = actions (A,U), cols = states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage1_mf = np.zeros(2)          # MF values for A/U
    q_stage2_mf = np.zeros((2, 2))     # MF values for aliens within each planet

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated control weight and perseveration strength
    # Higher anxiety -> less planning weight, more perseveration
    w_mb = np.clip(w_mb_base * (1.0 - stai0), 0.0, 1.0)
    kappa_eff = kappa * (1.0 + stai0)

    prev_a1 = None  # for stickiness

    eps = 1e-10
    for t in range(n_trials):

        # Compute model-based Q at stage 1 from current MF Q at stage 2
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # size 2 (states)
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value per action

        # Hybrid Q for stage 1
        q1_hybrid = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # Add perseveration bias toward previous first-stage action
        if prev_a1 is not None:
            stickiness = np.zeros(2)
            stickiness[prev_a1] = 1.0
            q1_hybrid = q1_hybrid + kappa_eff * stickiness

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at realized state
        s2 = state[t]
        q2 = q_stage2_mf[s2]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha2 * pe2

        # Stage-1 MF update via eligibility from stage-2 chosen value (no separate lambda param)
        # TD target uses realized second-stage chosen value
        td_target1 = q_stage2_mf[s2, a2]
        pe1 = td_target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha1 * pe1

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free RL with anxiety-modulated associability and eligibility trace.
    
    The agent learns values using a Pearce–Hall-like associability: the learning
    rate increases with the magnitude of recent prediction errors. Anxiety
    amplifies associability (greater sensitivity to surprising outcomes) and
    reduces credit assignment across stages via a lower eligibility trace.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0/1)
    - state: np.array (n_trials,), second-stage state (0/1)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward: np.array (n_trials,), outcomes (0/1)
    - stai: np.array (1,) or (n_trials,), anxiety in [0,1]
    - model_parameters: iterable of 4 parameters
        alpha_base: [0,1] baseline learning rate
        beta: [0,10] inverse temperature for both stages
        lam_base: [0,1] baseline eligibility trace for backing up to stage 1
        persev: [0,1] perseveration bias strength (applied to both stages)
    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_base, beta, lam_base, persev = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Model-free values
    q1 = np.zeros(2)         # stage-1 MF values
    q2 = np.zeros((2, 2))    # stage-2 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated eligibility and perseveration
    # Higher anxiety -> lower cross-stage credit assignment, stronger perseveration
    lam = np.clip(lam_base * (1.0 - 0.7 * stai0), 0.0, 1.0)
    persev_eff = persev * (1.0 + stai0)

    # Keep track of previous actions for perseveration
    prev_a1 = None
    prev_a2 = np.array([None, None])  # per state

    eps = 1e-10
    # Running associability scalar (per action/state-action); start at alpha_base
    assoc1 = np.ones(2) * alpha_base
    assoc2 = np.ones((2, 2)) * alpha_base

    for t in range(n_trials):
        s = state[t]

        # Stage-1 policy with perseveration
        q1_bias = q1.copy()
        if prev_a1 is not None:
            q1_bias[prev_a1] += persev_eff

        exp_q1 = np.exp(beta * (q1_bias - np.max(q1_bias)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with within-state perseveration
        q2_bias = q2[s].copy()
        if prev_a2[s] is not None:
            q2_bias[prev_a2[s]] += persev_eff

        exp_q2 = np.exp(beta * (q2_bias - np.max(q2_bias)))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 update with Pearce–Hall associability amplified by anxiety
        pe2 = r - q2[s, a2]
        # associability grows with surprise; anxiety amplifies this growth
        assoc2[s, a2] = np.clip((1 - alpha_base) * assoc2[s, a2] + (alpha_base * (1 + stai0)) * abs(pe2), 0.0, 1.0)
        alpha2_eff = np.clip(alpha_base * (0.5 + assoc2[s, a2]), 0.0, 1.0)
        q2[s, a2] += alpha2_eff * pe2

        # Stage-1 update via eligibility from stage-2 PE
        pe1 = q2[s, a2] - q1[a1]
        # associability for stage 1 also increases with surprise at stage 2
        assoc1[a1] = np.clip((1 - alpha_base) * assoc1[a1] + (alpha_base * (1 + stai0)) * abs(pe1), 0.0, 1.0)
        alpha1_eff = np.clip(alpha_base * (0.5 + assoc1[a1]), 0.0, 1.0)
        q1[a1] += (lam * alpha1_eff) * pe1

        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid RL with anxiety-modulated transition credit assignment bias.
    
    The agent blends model-based and model-free control at stage 1. Credit
    assignment from stage 2 to stage 1 depends on whether the transition was
    common or rare: anxiety increases the impact of rare transitions on MF
    backups (and reduces it for common transitions), capturing altered learning
    from surprising events.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0/1; A/U)
    - state: np.array (n_trials,), second-stage state (0/1; X/Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward: np.array (n_trials,), outcomes (0/1)
    - stai: np.array (1,) or (n_trials,), anxiety in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha: [0,1] learning rate for MF updates (both stages)
        beta: [0,10] inverse temperature
        w_mb_base: [0,1] baseline weight on model-based control
        lam_base: [0,1] base eligibility for MF backup to stage 1
        b_common: [0,1] bias toward actions whose common destination looks valuable
    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, w_mb_base, lam_base, b_common = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations:
    # - Reduce reliance on MB control
    w_mb = np.clip(w_mb_base * (1.0 - 0.5 * stai0), 0.0, 1.0)
    # - Adjust eligibility differently for common vs rare transitions
    #   Anxiety increases backup after rare transitions and decreases after common ones
    lam_common = np.clip(lam_base * (1.0 - 0.7 * stai0), 0.0, 1.0)
    lam_rare = np.clip(lam_base * (1.0 + 0.7 * stai0), 0.0, 1.0)
    # - Value-guided bias toward actions whose common destination is currently valuable;
    #   bias decreases with anxiety (less reliance on this heuristic)
    b_eff = b_common * (1.0 - stai0)

    eps = 1e-10
    for t in range(n_trials):
        # Model-based Q from current MF second-stage values
        max_q2 = np.max(q2_mf, axis=1)  # per state
        q1_mb = transition_matrix @ max_q2

        # Value-guided bias: add b_eff to the action whose common planet currently has higher value
        # Common planet for action 0 is state 0; for action 1 is state 1
        bias_vec = np.zeros(2)
        if max_q2[0] > max_q2[1]:
            bias_vec[0] += b_eff
        elif max_q2[1] > max_q2[0]:
            bias_vec[1] += b_eff
        # If equal, no bias added

        # Hybrid Q
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias_vec

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / (np.sum(exp_q1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        exp_q2 = np.exp(beta * (q2_mf[s] - np.max(q2_mf[s])))
        probs_2 = exp_q2 / (np.sum(exp_q2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 MF learning
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Determine transition type (common if action index equals state index here)
        # Action 0 commonly -> state 0; action 1 commonly -> state 1
        is_common = 1 if (a1 == s) else 0
        lam = lam_common if is_common else lam_rare

        # Stage-1 MF update with transition-contingent eligibility
        td_target1 = q2_mf[s, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += (lam * alpha) * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll