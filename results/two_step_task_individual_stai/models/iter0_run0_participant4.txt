def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-weighted perseveration (stickiness).
    
    This model blends model-based (MB) and model-free (MF) action values at stage 1.
    The MF system uses an eligibility trace to propagate stage-2 prediction errors
    back to stage 1. Anxiety (stai) scales an action perseveration bias that adds
    to the softmax logits at both stages, capturing increased habit/stickiness
    with higher anxiety. Stage-2 learning is simple Rescorla-Wagner.
    
    Inputs
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, observed planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial (planet-specific)
    - reward: array of floats in [0,1], obtained coins per trial
    - stai: array of length 1 with the participant's anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters:
        learning_rate (alpha): [0,1]         -- value learning rate
        beta: [0,10]                          -- inverse temperature for softmax
        lambda_elig (lam): [0,1]              -- eligibility trace for MF credit assignment
        w_mb: [0,1]                           -- weight on MB values at stage 1
        kappa_stick: [0,1]                    -- base perseveration strength; scaled by stai
    
    Returns
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, lam, w_mb, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure (fixed, known): A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to X/Y
                                  [0.3, 0.7]]) # from U to X/Y

    # Initialize value functions
    q1_mf = np.zeros(2)        # MF Q-values at stage 1 (spaceships)
    q2 = np.zeros((2, 2))      # stage-2 Q-values for each planet and alien

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration terms (previous chosen actions), initialized to zero
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)  # one per planet

    # Anxiety-weighted stickiness effective strength
    kappa_eff = kappa * stai

    for t in range(n_trials):
        s = state[t]

        # Model-based action values at stage 1: expect max Q2 on each planet via transitions
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = transition_matrix @ max_q2  # shape (2,)

        # Hybrid stage-1 values
        q1_hyb = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Add perseveration biases to logits (both options get +kappa if they match prev choice)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_eff

        # Stage-1 policy
        logits1 = beta * q1_hyb + bias1
        logits1 -= np.max(logits1)  # numerical stability
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in the reached state, with perseveration on that planet
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa_eff
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning: Stage-2 TD
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # MF backpropagation to stage 1 with eligibility trace
        q1_mf[a1] += alpha * lam * delta2

        # Update perseveration memories
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-gated learning with anxiety-modulated gain and reduced stickiness.
    
    This model uses a dynamic learning rate driven by surprise (unsigned prediction error)
    tracked with a Pearce-Hallâ€“like running average. Anxiety (stai) increases the impact
    of surprise on the learning rate, capturing heightened sensitivity to unexpected
    outcomes with higher anxiety. Perseveration is present but attenuated by anxiety.
    Stage-1 values are hybrid MB/MF (with fixed 0.5 mixing).
    
    Inputs
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, observed planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial (planet-specific)
    - reward: array of floats in [0,1], obtained coins per trial
    - stai: array of length 1 with the participant's anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters:
        alpha0: [0,1]               -- base learning rate
        beta: [0,10]                -- inverse temperature
        eta_surprise: [0,1]         -- update rate for running surprise
        lambda_elig (lam): [0,1]    -- eligibility trace to stage 1 for MF
        rho_stick: [0,1]            -- base perseveration strength (reduced by anxiety)
    
    Returns
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha0, beta, eta_surprise, lam, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure for MB planning
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness attenuated by anxiety: higher stai -> lower perseveration
    rho_eff = rho * (1.0 - stai)

    # Running surprise (unsigned PE), initialize moderate
    S = 0.5

    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        s = state[t]

        # MB values via transitions to max second-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid values with fixed mixing 0.5
        q1_hyb = 0.5 * q1_mf + 0.5 * q1_mb

        # Perseveration biases (reduced when anxiety is high)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_eff

        logits1 = beta * q1_hyb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += rho_eff
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Stage-2 PE and dynamic (anxiety-weighted) learning rate
        delta2 = r - q2[s, a2]
        # Update running surprise S in [0,1]
        S = (1.0 - eta_surprise) * S + eta_surprise * abs(delta2)
        # Anxiety gates learning-rate amplification by surprise
        alpha_t = alpha0 * ((1.0 - stai) + stai * S)
        # Clip for safety
        if alpha_t < 0.0:
            alpha_t = 0.0
        if alpha_t > 1.0:
            alpha_t = 1.0

        # Learn stage 2
        q2[s, a2] += alpha_t * delta2

        # Eligibility-trace credit to stage 1 MF
        q1_mf[a1] += alpha_t * lam * delta2

        # Update prev choices
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Structure learning of transitions with anxiety-biased arbitration.
    
    This model learns the transition matrix online and uses it for model-based planning.
    Arbitration between MB and MF at stage 1 depends on anxiety: higher anxiety reduces
    reliance on MB (more MF). The initial transition prior is also anxiety-biased:
    low anxiety assumes stronger common transitions (closer to 0.7), high anxiety starts
    more uncertain (closer to 0.5). MF uses an eligibility trace from stage 2 PE.
    
    Inputs
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, observed planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial (planet-specific)
    - reward: array of floats in [0,1], obtained coins per trial
    - stai: array of length 1 with the participant's anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters:
        alphaQ: [0,1]             -- learning rate for Q-values
        beta: [0,10]              -- inverse temperature
        lambda_elig (lam): [0,1]  -- eligibility trace for MF
        w0: [0,1]                 -- baseline MB weight at stage 1 (before anxiety)
        alphaT: [0,1]             -- learning rate for transition probabilities
    
    Returns
    - Negative log-likelihood of observed choices across both stages.
    """
    alphaQ, beta, lam, w0, alphaT = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-biased initial transition prior:
    # common prob starts at 0.5 + 0.2*(1 - stai) in [0.5, 0.7]
    p_common_init = 0.5 + 0.2 * (1.0 - stai)
    T = np.array([[p_common_init, 1.0 - p_common_init],   # for A
                  [1.0 - p_common_init, p_common_init]])  # for U

    # Arbitration weight: high anxiety -> more MF (lower MB weight)
    w_mb = np.clip(w0 * (1.0 - stai), 0.0, 1.0)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]

        # MB action values via learned transition matrix
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid stage-1 values
        q1_hyb = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 choice
        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice in observed state
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Stage-2 PE and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alphaQ * delta2

        # MF backprop to stage 1
        q1_mf[a1] += alphaQ * lam * delta2

        # Learn transition matrix row for chosen action a1 from observed state s
        # One-hot target: P(s | a1) = 1 for observed s, 0 for other
        # Update both columns to keep row stochastic
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alphaT * (target - T[a1, sp])
        # Ensure numerical stability of probabilities
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)