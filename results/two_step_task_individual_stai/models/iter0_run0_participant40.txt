def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated control and stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha, beta, w, lam, kappa)
        - alpha in [0,1]: learning rate for model-free values at both stages.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - w in [0,1]: baseline weight on model-based control.
        - lam in [0,1]: eligibility trace to backpropagate value to stage 1.
        - kappa in [0,1]: first-stage choice stickiness strength.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    
    Notes
    -----
    - Anxiety (stai) reduces model-based control: w_eff = w * (1 - stai).
    - Transition structure is assumed known (common: 0.7).
    - Stickiness adds a bias toward repeating the previous first-stage choice.
    """
    alpha, beta, w, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: rows = actions (A,U), cols = states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize Q-values
    q_stage1_mf = np.zeros(2)         # model-free action values at stage 1
    q_stage2_mf = np.zeros((2, 2))    # model-free values at stage 2: Q[state, action]

    # Anxiety-modulated MB weight
    w_eff = w * (1.0 - stai)

    prev_a1 = None  # for stickiness

    for t in range(n_trials):
        # Compute model-based stage-1 values from stage-2 MF values (myopic MB)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)             # best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2          # MB projection
        # Combine MB and MF with stickiness
        q1_combined = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add first-stage stickiness bias to logits
        stickiness_bias = np.zeros(2)
        if prev_a1 is not None:
            stickiness_bias[prev_a1] = kappa
        logits1 = beta * q1_combined + stickiness_bias
        exp_q1 = np.exp(logits1 - np.max(logits1))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        q2 = q_stage2_mf[s2]
        logits2 = beta * q2
        exp_q2 = np.exp(logits2 - np.max(logits2))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # TD updates
        # Stage-2 update from reward
        r = reward[t]
        delta2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha * delta2

        # Stage-1 MF update via eligibility trace using value at stage 2
        # Use the bootstrap target as current second-stage value after update
        delta1 = q_stage2_mf[s2, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * lam * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid model with anxiety-modulated transition updating and lapse.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state visited each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Outcome each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha_r, beta, w, alpha_t, epsilon)
        - alpha_r in [0,1]: learning rate for second-stage rewards (Q-learning).
        - beta in [0,10]: inverse temperature for both stages.
        - w in [0,1]: weight on model-based control at stage 1.
        - alpha_t in [0,1]: base learning rate for transition probabilities.
        - epsilon in [0,1]: lapse probability that mixes uniform random choice.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    
    Notes
    -----
    - The agent learns the actionâ†’state transition matrix online.
    - Anxiety increases perceived volatility, scaling transition learning:
      alpha_t_eff = alpha_t * (0.5 + stai).
    - Lapse epsilon injects uniform choice noise at both stages.
    """
    alpha_r, beta, w, alpha_t, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition beliefs to the canonical 0.7/0.3 structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate
    alpha_t_eff = alpha_t * (0.5 + stai)  # in [0,1.5], but inputs should keep it <=1

    for t in range(n_trials):
        # Model-based projection using current transition beliefs
        max_q2 = np.max(q_stage2_mf, axis=1)
        q1_mb = T @ max_q2
        q1 = w * q1_mb + (1 - w) * q_stage1_mf

        # First-stage policy with lapse
        logits1 = beta * q1
        exp1 = np.exp(logits1 - np.max(logits1))
        soft1 = exp1 / np.sum(exp1)
        probs_1 = (1 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with lapse
        s2 = state[t]
        logits2 = beta * q_stage2_mf[s2]
        exp2 = np.exp(logits2 - np.max(logits2))
        soft2 = exp2 / np.sum(exp2)
        probs_2 = (1 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Update second-stage Q
        delta2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha_r * delta2

        # Stage-1 MF update towards current second-stage value (eligibility = 1 here)
        target1 = q_stage2_mf[s2, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Learn transition probabilities from the observed transition (a1 -> s2)
        # Move the chosen-action row toward a one-hot on the observed state
        for s in (0, 1):
            target = 1.0 if s == s2 else 0.0
            T[a1, s] += alpha_t_eff * (target - T[a1, s])
        # Ensure row normalization (numerical safety)
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive hybrid model with anxiety-modulated loss aversion and asymmetric learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state visited each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Outcome each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, w, gamma)
        - alpha_pos in [0,1]: learning rate when second-stage prediction error >= 0.
        - alpha_neg in [0,1]: learning rate when second-stage prediction error < 0.
        - beta in [0,10]: inverse temperature for both stages.
        - w in [0,1]: weight on model-based control at stage 1.
        - gamma in [0,1]: scales anxiety-driven loss aversion: rho = 1 + gamma * stai.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    
    Notes
    -----
    - Utility transform: u(r) = r if r >= 0 else -rho * |r|, with rho increasing in anxiety.
    - Stage-2 updates use asymmetric learning rates based on the sign of the RPE in utility space.
    - Stage-1 MF is updated toward the current second-stage value (eligibility-like update).
    """
    alpha_pos, alpha_neg, beta, w, gamma = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss aversion
    rho = 1.0 + gamma * stai  # >= 1

    for t in range(n_trials):
        # Model-based projection
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2
        q1 = w * q1_mb + (1 - w) * q1_mf

        # First-stage policy
        logits1 = beta * q1
        exp1 = np.exp(logits1 - np.max(logits1))
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        logits2 = beta * q2_mf[s2]
        exp2 = np.exp(logits2 - np.max(logits2))
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Utility transform of reward with anxiety-modulated loss aversion
        r = reward[t]
        util = r if r >= 0 else -rho * (-r)

        # Stage-2 update with asymmetric learning rate in utility space
        pe2 = util - q2_mf[s2, a2]
        alpha2 = alpha_pos if pe2 >= 0 else alpha_neg
        q2_mf[s2, a2] += alpha2 * pe2

        # Stage-1 MF update toward current stage-2 value (eligibility-like, no extra param)
        target1 = q2_mf[s2, a2]
        pe1 = target1 - q1_mf[a1]
        # Use the average of alpha_pos/alpha_neg for stage-1 update to keep within param bounds
        alpha1 = 0.5 * (alpha_pos + alpha_neg)
        q1_mf[a1] += alpha1 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll