def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated arbitration and perseveration.
    
    This model combines model-free (MF) and model-based (MB) values at the first stage,
    with an eligibility trace propagating second-stage outcomes back to first-stage values.
    Anxiety (stai) increases the reliance on perseveration and decreases the MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (per planet; 0 or 1) for each trial.
    reward : array-like of float
        Obtained reward on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Participant's anxiety score in [0,1]; here, single-element array with stai[0].
        Higher stai reduces model-based control and increases perseveration.
    model_parameters : list or array of floats
        [alpha, lambda_, beta, w_base, pers_base]
        - alpha in [0,1]: learning rate for both stages.
        - lambda_ in [0,1]: eligibility trace; propagates second-stage TD error to first stage.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w_base in [0,1]: baseline MB/MF mixing weight; anxiety reduces this weight.
        - pers_base in [0,1]: baseline perseveration magnitude; anxiety increases its effect.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, lambda_, beta, w_base, pers_base = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value functions
    q_stage1_mf = np.zeros(2)       # MF values for A/U
    q_stage2_mf = np.zeros((2, 2))  # MF values for second-stage actions in states X/Y

    # Likelihood tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1  # for perseveration bias

    # Anxiety-modulated arbitration and perseveration
    # Higher stai -> lower MB weight (tilt toward MF)
    w_eff_scale = 1.0 - 0.5 * st
    # Bound within [0,1] by construction of scale in [0.5,1]
    w_eff_base = w_base * w_eff_scale

    # Higher stai -> stronger perseveration
    pers_eff = pers_base * (0.5 + 0.5 * st)

    for t in range(n_trials):
        # Compute MB first-stage values via expected max second-stage value
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # shape (2,)
        q_stage1_mb = transition_matrix @ max_q_stage2  # shape (2,)

        # Combine MB and MF with anxiety-modulated weight
        w_eff = w_eff_base
        q1_combined = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Add perseveration bias to the previously chosen action
        bias = np.zeros(2)
        if prev_a1 >= 0:
            bias[prev_a1] = pers_eff
        q1_policy_vals = q1_combined + bias

        # First-stage policy and likelihood
        exp_q1 = np.exp(beta * (q1_policy_vals - np.max(q1_policy_vals)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy and likelihood
        s = state[t]
        q2_policy_vals = q_stage2_mf[s, :]
        exp_q2 = np.exp(beta * (q2_policy_vals - np.max(q2_policy_vals)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        # TD at second stage
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # TD for first-stage MF values:
        # 1) Bootstrapped update towards the second-stage action value (state-action TD)
        delta1_boot = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1_boot

        # 2) Eligibility trace to propagate outcome to first-stage value
        q_stage1_mf[a1] += alpha * lambda_ * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric learning with anxiety-modulated learning rates and exploration.
    
    This model implements separate learning rates for positive and negative outcomes.
    Higher anxiety increases learning from negative outcomes and decreases exploration
    temperature (more stochasticity), consistent with heightened sensitivity to losses.
    First-stage action values are a weighted combination of model-free (MF) and
    model-based (MB) values with a fixed MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Outcome per trial (0.0 or 1.0).
    stai : array-like of float
        Anxiety score in [0,1]; single-element array with stai[0].
        Higher stai increases learning from negative outcomes and lowers inverse temperature.
    model_parameters : list or array of floats
        [alpha_pos, alpha_neg, beta, w]
        - alpha_pos in [0,1]: learning rate when reward is positive.
        - alpha_neg in [0,1]: learning rate when reward is zero/negative.
        - beta in [0,10]: base inverse temperature for softmax at both stages.
        - w in [0,1]: weight of MB values in first-stage decision (1 = pure MB).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, w = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated inverse temperature (higher stai -> lower beta_eff)
    beta_eff = beta * (0.6 + 0.4 * (1.0 - st))  # in [0.6*beta, 1.0*beta]

    for t in range(n_trials):
        # MB estimate for first-stage actions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MF and MB
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf

        # First-stage policy
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2_mf[s, :]
        exp_q2 = np.exp(beta_eff * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Choose valence-specific learning rate with anxiety modulation
        # Positive outcomes: slightly reduced with anxiety
        alpha_pos_eff = alpha_pos * (1.0 - 0.3 * st)
        # Negative/zero outcomes: increased with anxiety
        alpha_neg_eff = alpha_neg * (0.5 + 0.5 * st)

        alpha_t = alpha_pos_eff if r > 0.5 else alpha_neg_eff

        # Second-stage update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_t * delta2

        # First-stage MF bootstrapped update
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_t * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Learned-transition model with anxiety-weighted pessimism about uncertainty.
    
    This model learns the transition probabilities from first-stage actions to states,
    and uses them to compute model-based (MB) values. It penalizes uncertain actions
    via an entropy-based risk term scaled by anxiety (stai) and a risk parameter rho.
    MB values are combined with model-free (MF) values at the first stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the planet.
    reward : array-like of float
        Obtained reward (0.0 or 1.0).
    stai : array-like of float
        Anxiety score in [0,1]; single-element array with stai[0].
        Higher stai increases transition learning rate and pessimism about uncertainty.
    model_parameters : list or array of floats
        [alpha_r, alpha_T, beta, w, rho]
        - alpha_r in [0,1]: reward learning rate (both stages).
        - alpha_T in [0,1]: transition learning rate.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w in [0,1]: weight on MB values at first stage (1 = pure MB).
        - rho in [0,1]: scale of uncertainty penalty; combined with stai.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_T, beta, w, rho = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Initialize learned transition probabilities close to task structure
    T_hat = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate (more weight to observed transitions)
    alpha_T_eff = alpha_T * (0.5 + 0.5 * st)
    # Anxiety-weighted uncertainty penalty coefficient; normalized by log(2)
    # so that maximal binary entropy yields a penalty up to rho*st.
    norm_log2 = np.log(2.0)
    penalty_scale = (rho * st) / (norm_log2 + 1e-12)

    for t in range(n_trials):
        # MB values using learned transitions (before penalty)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # shape (2,)
        q_mb = T_hat @ max_q_stage2  # shape (2,)

        # Compute entropy-based uncertainty per first-stage action and penalize
        # Binary entropy H(p) = -p log p - (1-p) log (1-p), p = P(s=0 | a1)
        p0 = T_hat[:, 0]  # probability of state 0 given action 0/1
        # Clip to avoid log(0)
        p0 = np.clip(p0, 1e-6, 1.0 - 1e-6)
        H = -(p0 * np.log(p0) + (1.0 - p0) * np.log(1.0 - p0))
        q_mb_adj = q_mb - penalty_scale * H  # pessimistic correction with anxiety

        # First-stage combined values
        q1 = w * q_mb_adj + (1.0 - w) * q_stage1_mf

        # First-stage policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2_mf[s, :]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Reward learning updates
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * delta2

        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Transition learning: update T_hat row for chosen first-stage action
        # Move probabilities toward the one-hot of observed state
        one_hot = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T_hat[a1, :] = (1.0 - alpha_T_eff) * T_hat[a1, :] + alpha_T_eff * one_hot

        # Ensure numerical stability and row-stochasticity
        T_hat[a1, :] = np.clip(T_hat[a1, :], 1e-6, 1.0)
        T_hat[a1, :] /= np.sum(T_hat[a1, :])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll