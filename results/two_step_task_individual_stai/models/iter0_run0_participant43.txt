def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-weighted hybrid model-based/model-free controller with separate stage temperatures.
    
    Idea:
    - Stage-2 values are learned with a TD rule (model-free).
    - Stage-1 choice values combine a model-free estimate and a model-based plan computed
      from a fixed transition structure (A->X, U->Y common).
    - The arbitration weight (omega) is attenuated by anxiety (higher STAI -> less model-based control).
    
    Parameters (all in [0,1] except betas in [0,10]):
    - model_parameters[0]: alpha (0..1), learning rate for value updates
    - model_parameters[1]: beta1 (0..10), inverse temperature for stage-1 choices
    - model_parameters[2]: beta2 (0..10), inverse temperature for stage-2 choices
    - model_parameters[3]: omega_base (0..1), baseline weight on model-based value at stage-1
    - model_parameters[4]: k_anx_mb (0..1), strength of anxiety modulation of omega
         omega_eff = omega_base * (1 - k_anx_mb * stai)
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship on each trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet on each trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on each trial (on reached planet)
    - reward: array of floats, coin outcome on each trial
    - stai: array-like with a single float in [0,1], the participant's STAI score
    - model_parameters: list/array of parameter values as above
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """
    alpha, beta1, beta2, omega_base, k_anx_mb = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed two-step transition structure: rows = first-stage action, cols = planet
    # A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Storage for choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)          # model-free values at stage 1 (A,U)
    q2 = np.zeros((2, 2))        # stage-2 MF values: rows=planet (X,Y), cols=alien (W/S at X, P/H at Y)

    eps = 1e-10

    for t in range(n_trials):
        # Compute stage-1 model-based values from current stage-2 values
        max_q2 = np.max(q2, axis=1)               # value of best alien on each planet
        q1_mb = transition_matrix @ max_q2        # plan value for (A,U)

        # Anxiety-weighted arbitration weight
        omega_eff = omega_base * (1.0 - k_anx_mb * stai_val)
        omega_eff = min(1.0, max(0.0, omega_eff))

        # Combined stage-1 action values
        q1 = (1.0 - omega_eff) * q1_mf + omega_eff * q1_mb

        # Stage-1 policy
        # Softmax with temperature beta1
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)  # numerical stability
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (conditional on reached state)
        s = int(state[t])
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        q2_sa_prev = q2[s, a2]
        delta2 = r - q2_sa_prev
        q2[s, a2] = q2_sa_prev + alpha * delta2

        # Stage-1 MF update toward (pre-update) stage-2 chosen value (SARSA-style bootstrap)
        q1_mf[a1] = q1_mf[a1] + alpha * (q2_sa_prev - q1_mf[a1])

    # Negative log-likelihood of observed choices across both stages
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-asymmetric model-free learner with eligibility and perseveration.
    
    Idea:
    - Pure model-free controller: stage-2 values learned by TD; stage-1 values updated via
      an eligibility trace from the stage-2 prediction error.
    - Anxiety modulates learning asymmetry: higher STAI reduces learning from rewards
      and increases learning from non-rewards (pessimistic updating).
    - Perseveration bias at stage-1 captures action stickiness.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha_base (0..1), baseline learning rate
    - model_parameters[1]: beta (0..10), inverse temperature for both stages
    - model_parameters[2]: lam (0..1), eligibility trace from stage-2 PE to stage-1 value
    - model_parameters[3]: k_anx_asym (0..1), strength of anxiety asymmetry on learning rate
         alpha_pos = alpha_base * (1 - k_anx_asym * stai) for rewards
         alpha_neg = alpha_base * (1 + k_anx_asym * stai), capped to 1, for non-rewards
    - model_parameters[4]: pers (0..1), perseveration bias magnitude at stage-1
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on that planet
    - reward: array of floats, coin outcome
    - stai: array-like with a single float in [0,1], STAI score
    - model_parameters: list/array of parameter values as above
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """
    alpha_base, beta, lam, k_anx_asym, pers = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Values
    q1 = np.zeros(2)       # model-free stage-1
    q2 = np.zeros((2, 2))  # stage-2 MF

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        # Perseveration bias vector (additive to logits)
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[int(prev_a1)] = pers

        # Stage-1 policy
        logits1 = beta * q1 + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning rates modulated by anxiety and outcome valence
        r = reward[t]
        if r > 0.5:
            alpha_t = alpha_base * (1.0 - k_anx_asym * stai_val)
        else:
            alpha_t = alpha_base * (1.0 + k_anx_asym * stai_val)
        if alpha_t > 1.0:
            alpha_t = 1.0
        if alpha_t < 0.0:
            alpha_t = 0.0

        # Stage-2 TD update
        q2_sa_prev = q2[s, a2]
        delta2 = r - q2_sa_prev
        q2[s, a2] = q2_sa_prev + alpha_t * delta2

        # Stage-1 MF update via eligibility trace from stage-2 PE
        q1[a1] = q1[a1] + lam * alpha_t * delta2

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Learning transitions with anxiety-driven noise and arbitration.
    
    Idea:
    - The agent learns both transition probabilities and stage-2 values.
    - Model-based plan uses the learned transition matrix to evaluate first-stage actions.
    - Arbitration between model-based and model-free at stage-1.
    - Anxiety increases lapse/noise at stage-2 and reduces both arbitration weight and
      transition learning rate.
    
    Parameters (all in [0,1] except betas in [0,10]):
    - model_parameters[0]: alpha (0..1), learning rate for rewards and transitions
    - model_parameters[1]: beta1 (0..10), inverse temperature for stage-1
    - model_parameters[2]: beta2 (0..10), inverse temperature for stage-2
    - model_parameters[3]: omega_base (0..1), baseline MB weight at stage-1
    - model_parameters[4]: k_anx_noise (0..1), anxiety strength for lapse and arbitration reduction
         eps_lapse = 0.5 * k_anx_noise * stai    (mixed with uniform at stage-2)
         omega_eff = omega_base * (1 - k_anx_noise * stai)
         alpha_trans = alpha * (1 - k_anx_noise * stai)
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on that planet
    - reward: array of floats, coin outcome
    - stai: array-like with a single float in [0,1], STAI score
    - model_parameters: list/array of parameter values as above
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices under the model.
    """
    alpha, beta1, beta2, omega_base, k_anx_noise = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition model to uniform uncertainty
    # Rows correspond to first-stage actions (A,U), columns to planets (X,Y)
    p_trans = np.full((2, 2), 0.5)

    # Values
    q1_mf = np.zeros(2)       # model-free stage-1 values
    q2 = np.zeros((2, 2))     # stage-2 values

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    omega_eff_base = omega_base * (1.0 - k_anx_noise * stai_val)
    omega_eff_base = min(1.0, max(0.0, omega_eff_base))
    alpha_trans_base = alpha * (1.0 - k_anx_noise * stai_val)
    if alpha_trans_base < 0.0:
        alpha_trans_base = 0.0
    eps_lapse = 0.5 * k_anx_noise * stai_val  # 0..0.5

    eps = 1e-10

    for t in range(n_trials):
        # Current MB first-stage values from learned transitions and stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = p_trans @ max_q2

        # Arbitration
        q1 = (1.0 - omega_eff_base) * q1_mf + omega_eff_base * q1_mb

        # Stage-1 policy
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with anxiety-driven lapse
        s = int(state[t])
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        soft2 = exp2 / (np.sum(exp2) + eps)
        uniform2 = np.array([0.5, 0.5])
        probs2 = (1.0 - eps_lapse) * soft2 + eps_lapse * uniform2
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Reward learning (stage-2 TD)
        r = reward[t]
        q2_sa_prev = q2[s, a2]
        delta2 = r - q2_sa_prev
        q2[s, a2] = q2_sa_prev + alpha * delta2

        # Stage-1 MF update toward (pre-update) stage-2 chosen value
        q1_mf[a1] = q1_mf[a1] + alpha * (q2_sa_prev - q1_mf[a1])

        # Transition learning for chosen first-stage action
        # Move the chosen action's transition row toward the observed state
        onehot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        p_trans[a1, :] = p_trans[a1, :] + alpha_trans_base * (onehot_s - p_trans[a1, :])

        # Ensure rows remain normalized and within [0,1] (should be true by construction)
        # Minor numerical clip
        p_trans[a1, :] = np.clip(p_trans[a1, :], 0.0, 1.0)
        row_sum = p_trans[a1, :].sum()
        if row_sum > 0:
            p_trans[a1, :] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll