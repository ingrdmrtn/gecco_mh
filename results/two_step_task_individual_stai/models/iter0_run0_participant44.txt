def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-weighted arbitration, eligibility trace, and stickiness.
    
    Idea:
    - Stage-1 action values combine model-free (MF) and model-based (MB) values.
    - Arbitration weight w is reduced by anxiety (higher STAI -> more MF).
    - Choice stickiness (perseveration) increases with anxiety.
    - MF values use an eligibility trace to propagate second-stage outcomes to the first stage.

    Parameters (with bounds):
    - learning_rate (alpha) in [0,1]: learning rate for Q updates.
    - beta in [0,10]: inverse temperature for both stages.
    - lambda_etrace in [0,1]: eligibility trace strength from stage 2 to stage 1.
    - w0 in [0,1]: baseline MB weight (anxiety reduces this).
    - kappa in [0,1]: baseline stickiness strength (anxiety amplifies this).

    Inputs:
    - action_1: array-like of ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1} indicating reached planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1} for second-stage choices (aliens W/S on X, P/H on Y).
    - reward: array-like of floats, typically 0/1 coins received.
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, lambda_etrace, w0, kappa].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, lambda_etrace, w0, kappa = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q_stage1_mf = np.zeros(2)           # MF Q for A/U
    q_stage2_mf = np.zeros((2, 2))      # MF Q for aliens on each planet

    # Anxiety-modulated arbitration and stickiness
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)         # higher anxiety -> lower MB weight
    kappa_eff = kappa * (0.5 + 0.5 * stai)               # higher anxiety -> more stickiness

    prev_a1 = None
    prev_a2 = None

    for t in range(n_trials):
        # Model-based Q at stage 1: T @ max_a Q2(s,a)
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # [X, Y]
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Combine MF/MB
        q1_combined = (1.0 - w_eff) * q_stage1_mf + w_eff * q_stage1_mb

        # Add stage-1 stickiness bias to the previously chosen action
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_eff

        # Stage-1 policy and likelihood
        logits1 = beta * q1_combined + bias1
        logits1 -= np.max(logits1)  # numerical stability
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy given observed state
        s = state[t]
        # Stage-2 stickiness bias
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += kappa_eff

        logits2 = beta * q_stage2_mf[s, :] + bias2
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD updates
        # Stage-1 MF bootstraps on stage-2 action value
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Stage-2 MF learns from reward
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Eligibility trace to propagate outcome up to stage 1
        q_stage1_mf[a1] += alpha * lambda_etrace * delta2

        # Update history for stickiness
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated temperature, learned transitions, and value spillover.
    
    Idea:
    - Agent learns transition probabilities with a transition learning rate.
    - MB/MF hybrid at stage 1 with anxiety reducing MB weight.
    - Inverse temperature decreases with anxiety (more exploratory).
    - Choice stickiness increases with anxiety.
    - Second-stage learning includes "spillover" to the other planet's same action,
      scaled by anxiety (capturing overgeneralization).

    Parameters (with bounds):
    - alpha in [0,1]: learning rate for Q updates.
    - beta in [0,10]: base inverse temperature.
    - w0 in [0,1]: baseline MB arbitration weight (reduced by anxiety).
    - tau in [0,1]: transition learning rate.
    - kappa in [0,1]: baseline stickiness (amplified by anxiety).

    Inputs:
    - action_1: array-like of ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1} indicating reached planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1} for second-stage choices.
    - reward: array-like of floats, typically 0/1.
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, w0, tau, kappa].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, w0, tau, kappa = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize transition beliefs (rows: actions A/U, cols: states X/Y)
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Anxiety effects
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)
    beta_eff = beta * (1.0 - 0.5 * stai)  # more anxiety -> lower temperature (more noise)
    kappa_eff = kappa * (0.5 + 0.5 * stai)
    spillover_scale = 0.5 * stai  # fraction of delta applied to other state (anxiety-driven)

    prev_a1 = None
    prev_a2 = None

    for t in range(n_trials):
        # Model-based values: expected max Q2 under learned transitions
        max_q2 = np.max(q2, axis=1)   # [X, Y]
        q1_mb = T @ max_q2

        # Combine MB/MF
        q1 = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Stage-1 stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_eff

        logits1 = beta_eff * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Observe transition and update transition belief
        s = state[t]
        # Exponential moving average update toward one-hot of observed state
        T[a1, :] = (1.0 - tau) * T[a1, :]
        T[a1, s] += tau
        # Renormalize row to guard against drift
        T[a1, :] /= np.sum(T[a1, :])

        # Stage-2 policy with stickiness
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += kappa_eff
        logits2 = beta_eff * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF updates
        # Stage-1 MF bootstraps on stage-2 value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Stage-2 update with spillover to other state's same action (overgeneralization)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2
        other_s = 1 - s
        q2[other_s, a2] += alpha * spillover_scale * delta2

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Prospect-adjusted outcomes, anxiety-driven transition-outcome bias, and hybrid control.
    
    Idea:
    - Rewards are transformed by a concavity parameter rho (risk/utility sensitivity).
    - Anxiety induces pessimistic updating at stage 2 by penalizing uncertain value states.
    - Stage-1 incorporates MB/MF hybrid control with anxiety reducing MB weight.
    - Additionally, an anxiety-weighted transition-outcome bias adds a first-stage stay/switch
      tendency based on whether the previous transition was common or rare and rewarded.

    Parameters (with bounds):
    - alpha in [0,1]: learning rate for Q updates.
    - beta in [0,10]: inverse temperature for both stages.
    - rho in [0,1]: reward sensitivity (concavity; lower => more diminishing returns).
    - w0 in [0,1]: baseline MB weight (reduced by anxiety).
    - gamma in [0,1]: strength of transition-outcome bias at stage 1 (amplified by anxiety).

    Inputs:
    - action_1: array-like of ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1} indicating reached planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1} for second-stage choices.
    - reward: array-like of floats, typically 0/1.
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, rho, w0, gamma].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, rho, w0, gamma = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure
    T_fixed = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Anxiety effects
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)
    beta_eff = beta  # keep constant here; other effects captured via biases and updates
    gamma_eff = gamma * stai  # anxiety amplifies transition-outcome bias

    # Keep track of previous trial info for transition-outcome bias
    prev_a1 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        # Model-based component at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Hybrid Q
        q1 = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Transition-outcome bias on repeating previous first-stage action:
        # If previous trial: rewarded+common -> bias to repeat; rewarded+rare -> bias to switch.
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            is_common = None
            if prev_s is not None:
                # Common if A->X or U->Y; rare otherwise
                is_common = int((prev_a1 == 0 and prev_s == 0) or (prev_a1 == 1 and prev_s == 1))
            signed_bias = 0.0
            if prev_r is not None and is_common is not None:
                if prev_r > 0.0:
                    signed_bias = 1.0 if is_common == 1 else -1.0
                else:
                    # If unrewarded, invert the sign (common+no-reward -> switch; rare+no-reward -> stay)
                    signed_bias = -1.0 if is_common == 1 else 1.0
            bias1[prev_a1] += gamma_eff * signed_bias

        # Stage-1 policy
        logits1 = beta_eff * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (no explicit stickiness; bias captured at stage 1 here)
        s = state[t]
        logits2 = beta_eff * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome transformation and anxiety-driven pessimism penalty
        r_raw = reward[t]
        r_eff = (r_raw ** rho)  # concave utility for gains
        # Uncertainty penalty: penalize high local uncertainty q*(1-q), scaled by anxiety
        unc = q2[s, a2] * (1.0 - q2[s, a2])
        r_pess = r_eff - stai * unc

        # MF updates
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        delta2 = r_pess - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update previous trial record for next bias computation
        prev_a1 = a1
        prev_s = s
        prev_r = r_raw

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik