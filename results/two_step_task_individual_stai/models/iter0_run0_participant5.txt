def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid model-based/model-free SARSA(Î») learner with anxiety-weighted arbitration and separate stage temperatures.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = the two aliens available on the planet).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; here stai[0] used. Higher anxiety down-weights model-based control.
    model_parameters : array-like of float
        [alpha, beta1, beta2, w0, lam]
        - alpha in [0,1]: learning rate for both stages.
        - beta1 in [0,10]: inverse temperature for stage 1 softmax.
        - beta2 in [0,10]: inverse temperature for stage 2 softmax.
        - w0 in [0,1]: baseline model-based weight.
        - lam in [0,1]: eligibility trace weighting from stage 2 to stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, w0, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: rows = stage1 actions (A=0, U=1), cols = states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)            # model-free action values at stage 1
    q2 = np.zeros((2, 2))          # model-free action values at stage 2 (state x action)

    # Likelihood tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-weighted arbitration: higher anxiety reduces MB weight
    # w_eff is a convex combination pushing weight toward model-free as stai increases
    w_eff = np.clip((1.0 - stai) * w0, 0.0, 1.0)

    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values = T @ max_a Q2(s,a)
        max_q2 = np.max(q2, axis=1)          # best alien per planet
        q1_mb = transition_matrix @ max_q2   # expected value under transition dynamics

        # Hybrid action values at stage 1
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Policy and likelihood for stage 1
        exp1 = np.exp(beta1 * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Policy and likelihood for stage 2 (state-dependent)
        q2_s = q2[s]
        exp2 = np.exp(beta2 * (q2_s - np.max(q2_s)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 TD error using stage-2 value (SARSA-style bootstrap with chosen a2)
        delta1 = q2[s, a2] - q1_mf[a1]
        # Update stage-1 MF with blend of immediate bootstrap (delta1) and eligibility from outcome (delta2)
        q1_mf[a1] += alpha * ((1.0 - lam) * delta1 + lam * delta2)

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Transition-sensitive model-free learner with valence-asymmetric learning and stickiness,
    where anxiety amplifies credit assignment after rare transitions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached planet on each trial (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = aliens).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; higher anxiety increases learning from rare transitions at stage 1.
    model_parameters : array-like of float
        [alpha_pos, alpha_neg, beta, kappa, carry]
        - alpha_pos in [0,1]: learning rate when reward prediction error is positive.
        - alpha_neg in [0,1]: learning rate when reward prediction error is negative or zero.
        - beta in [0,10]: inverse temperature for both stages.
        - kappa in [0,1]: stickiness for repeating previous action (applied to both stages).
        - carry in [0,1]: fraction of stage-2 TD error carried back to stage-1 update baseline.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, kappa, carry = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure for detecting common vs rare
    # Common when (A->X) or (U->Y)
    def is_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    q1 = np.zeros(2)         # model-free values for stage 1 actions
    q2 = np.zeros((2, 2))    # model-free values for aliens (state x action)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = 0  # initialize previous actions; contributes only via kappa so value doesn't bias first trial much
    prev_a2 = 0

    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 policy: Q1 plus stickiness to previous a1
        stick1 = np.array([0.0, 0.0])
        stick1[prev_a1] += kappa
        q1_policy = q1 + stick1
        exp1 = np.exp(beta * (q1_policy - np.max(q1_policy)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: Q2[s] plus stickiness to previous a2
        stick2 = np.array([0.0, 0.0])
        stick2[prev_a2] += kappa
        q2_s_policy = q2[s] + stick2
        exp2 = np.exp(beta * (q2_s_policy - np.max(q2_s_policy)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning rates by valence
        pe2 = r - q2[s, a2]
        alpha2 = alpha_pos if pe2 > 0 else alpha_neg
        q2[s, a2] += alpha2 * pe2

        # Stage-1 update: transition-sensitive credit assignment
        # Compute whether common vs rare; anxiety increases learning from rare, reduces from common
        common = is_common(a1, s)
        # Weight eta ranges in [0,1]; when common, scale by (1 - stai); when rare, scale by stai
        eta = (1.0 - stai) if common else stai

        # Backpropagate a fraction of stage-2 PE to stage 1 (carry), using same valence-specific alpha
        pe1 = carry * pe2 - (1.0 - carry) * q1[a1]  # simple baseline-corrected target using carried PE
        alpha1 = alpha_pos if pe1 > 0 else alpha_neg
        q1[a1] += eta * alpha1 * pe1

        # Update stickiness memory
        prev_a1 = a1
        prev_a2 = a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Volatility-adaptive hybrid learner: dynamic learning rate from outcome surprise and anxiety,
    with forgetting for unchosen second-stage actions and anxiety-modulated model-based weight.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = aliens).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; increases volatility sensitivity and reduces model-based arbitration weight.
    model_parameters : array-like of float
        [alpha_base, beta, w0, tau_forget]
        - alpha_base in [0,1]: base learning rate (minimum).
        - beta in [0,10]: inverse temperature for both stages.
        - w0 in [0,1]: baseline model-based weight at stage 1 (down-weighted by anxiety).
        - tau_forget in [0,1]: forgetting rate toward 0.5 for unchosen stage-2 actions.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_base, beta, w0, tau_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X and U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)             # model-free stage-1 values
    q2 = 0.5 * np.ones((2, 2))      # initialize around neutral prior
    # Running volatility proxy per state-action: exponential moving average of squared prediction errors
    vol = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight: higher anxiety reduces reliance on model-based control
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)

    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q at stage 1 from current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid action values
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage 1 policy and likelihood
        exp1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy and likelihood
        exp2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Stage-2 learning with volatility-adaptive rate
        pe2 = r - q2[s, a2]
        # Update volatility estimate for chosen state-action: combine old vol with new PE^2
        # Use anxiety as the update gain for volatility tracking
        vol_gain = np.clip(stai, 0.0, 1.0)
        vol[s, a2] = (1.0 - vol_gain) * vol[s, a2] + vol_gain * (pe2 ** 2)

        # Adaptive learning rate increases with surprise and anxiety
        alpha_t = np.clip(alpha_base + stai * np.sqrt(vol[s, a2]), 0.0, 1.0)
        q2[s, a2] += alpha_t * pe2

        # Forgetting for unchosen aliens in the visited state (toward 0.5)
        unchosen_a2 = 1 - a2
        q2[s, unchosen_a2] += tau_forget * (0.5 - q2[s, unchosen_a2])

        # Stage-1 MF update using carried stage-2 value target
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_t * pe1  # use the same adaptive rate to keep coherence across stages

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)