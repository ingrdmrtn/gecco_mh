def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with anxiety-modulated control, eligibility traces, and choice stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien index within planet; 0 or 1).
    reward : array-like of float
        Received coins (can be negative or positive).
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0]. Lower is less anxious.
    model_parameters : sequence of floats
        [alpha, beta, w_mb, lam, kappa]
        - alpha (learning rate) in [0,1]
        - beta (inverse temperature) in [0,10]
        - w_mb (base model-based weight) in [0,1]
        - lam (eligibility trace) in [0,1]
        - kappa (first-stage choice stickiness) in [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w_mb, lam, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q_stage2_mf = np.zeros((2, 2))  # MF values at second stage: state x action
    q_stage1_mf = np.zeros(2)       # MF values for first-stage actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness: track last first-stage choice (one-hot)
    last_a1 = np.zeros(2)

    # Anxiety modulation:
    # - Reduce model-based weight with higher anxiety toward 0.5 (less goal-directed extremity)
    w_eff = np.clip(w_mb * (1.0 - stai) + 0.5 * stai, 0.0, 1.0)
    # - Reduce eligibility trace with anxiety (shorter credit assignment window)
    lam_eff = lam * (1.0 - 0.5 * stai)
    # - Reduce stickiness with anxiety (more variable choices)
    kappa_eff = kappa * (1.0 - stai)

    for t in range(n_trials):
        # Model-based first-stage Q from second-stage MF estimates
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # best action at each planet
        q_stage1_mb = transition_matrix @ max_q_stage2  # expected value per spaceship

        # Hybrid action values + stickiness for stage 1
        q1_hybrid = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf + kappa_eff * last_a1

        # Softmax policy stage 1
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = int(state[t])
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning
        # Stage 2 TD update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage 1 MF update with eligibility trace using second-stage TD error
        # Classic eligibility-trace style credit assignment: propagate delta2 back
        # and also bootstrap from the reached state's action value.
        # First, compute a TD-like prediction at stage 1 toward the second-stage chosen value
        target1 = q_stage2_mf[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * (delta1 + lam_eff * delta2)

        # Update stickiness memory
        last_a1 = np.zeros(2)
        last_a1[a1] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid RL with learned transition model and anxiety-modulated valence-specific learning.
    
    Key features:
    - Separate learning rates for positive and negative outcomes at stage 2.
    - Learned transition probabilities for model-based planning at stage 1.
    - Anxiety increases sensitivity to negative outcomes (higher alpha_neg) and reduces model-based weighting mildly.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha_pos, alpha_neg, beta, tau, w_mb]
        - alpha_pos in [0,1]: base learning rate for positive outcomes
        - alpha_neg in [0,1]: base learning rate for negative/zero outcomes
        - beta in [0,10]: inverse temperature for both stages
        - tau in [0,1]: learning rate for transitions (Dirichlet-like exponential smoothing)
        - w_mb in [0,1]: base model-based weight
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, tau, w_mb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T[a, s']: P(state=s' | action=a)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # start from prior consistent with task, update over time

    # Values
    q_stage2 = np.zeros((2, 2))
    q_stage1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation of learning rates and MB weight
    # - Increase learning from negative outcomes with anxiety; decrease for positive
    alpha_pos_eff = np.clip(alpha_pos * (1.0 - 0.5 * stai), 0.0, 1.0)
    alpha_neg_eff = np.clip(alpha_neg * (0.5 + 0.5 * stai), 0.0, 1.0)
    # - Mild reduction of model-based weighting with anxiety
    w_eff = np.clip(w_mb * (1.0 - 0.3 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based first-stage values using learned transitions
        max_q2 = np.max(q_stage2, axis=1)  # best action at each planet
        q1_mb = T @ max_q2

        # Hybrid value
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q_stage1_mf

        # Stage 1 policy
        exp_q1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = int(state[t])
        q2 = q_stage2[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage 2 update with valence-specific learning rate
        delta2 = r - q_stage2[s, a2]
        lr2 = alpha_pos_eff if delta2 > 0 else alpha_neg_eff
        q_stage2[s, a2] += lr2 * delta2

        # MF backup to stage 1
        target1 = q_stage2[s, a2]
        delta1 = target1 - q_stage1_mf[a1]
        # Use a blended learning rate reflecting the sign of second-stage PE
        q_stage1_mf[a1] += lr2 * delta1

        # Update learned transition model T with simple exponential smoothing
        # Move probability mass for action a1 toward the observed next state s.
        T[a1, :] = (1.0 - tau) * T[a1, :]
        T[a1, s] += tau
        # Ensure normalization
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-bonus model-based RL with anxiety-modulated exploration and common-transition bias.
    
    Key features:
    - Stage-2 values learned with exponential smoothing; track outcome variance per state-action
      to derive an uncertainty bonus for exploration (UCB-like).
    - Model-based planning at stage 1 uses expected values augmented by uncertainty bonus.
    - Anxiety reduces the exploration bonus allocation to uncertain options.
    - Includes a bias toward common transitions (habitual schema) at stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, phi, common_bias, kappa2]
        - alpha in [0,1]: learning rate for stage-2 value and variance estimates
        - beta in [0,10]: inverse temperature
        - phi in [0,1]: base uncertainty bonus weight (UCB coefficient)
        - common_bias in [0,1]: strength of bias favoring the spaceship with higher common transition prob
        - kappa2 in [0,1]: second-stage choice stickiness
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, phi, common_bias, kappa2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition model per task instructions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 estimates: value and running variance per state-action
    q2 = np.zeros((2, 2))
    mean_sq = np.zeros((2, 2))  # running estimate of E[r^2] to derive variance
    # Stickiness at stage 2
    last_a2 = np.zeros((2,))  # last chosen action in previous trial's reached state; encoded for 2 actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation of exploration: lower anxiety -> more bonus from uncertainty
    phi_eff = phi * (1.0 - stai)
    # Prepare common-transition bias term for stage 1 actions:
    # bias vector b[a] = +common_bias for the spaceship with higher common prob to its frequent planet,
    # and -common_bias for the other. Anxiety attenuates reliance on this heuristic.
    bias_scale = (1.0 - 0.5 * stai)
    b = bias_scale * common_bias * np.array([+1.0, +1.0])  # start symmetric
    # Make bias asymmetric to favor "common" transitions: assign + to action aligning with its common mapping
    # Here both ships have a common mapping but to different states; to keep it simple,
    # we bias both equally toward their own common destinations by adding the same +bias to both actions.
    # The asymmetry manifests through MB values via T @ ...
    # (Note: b remains a simple action-prior bonus.)

    # Track last second-stage action per state for stickiness
    last_a2_onehot = np.zeros((2, 2))  # state x action one-hot

    for t in range(n_trials):
        # Compute uncertainty (std) per state-action from running moments
        var = np.maximum(mean_sq - q2**2, 0.0)
        std = np.sqrt(var)

        # For each state, get exploration-augmented values
        q2_ucb = q2 + phi_eff * std + kappa2 * last_a2_onehot

        # Stage 1: MB planning using augmented second-stage values
        max_q2_ucb = np.max(q2_ucb, axis=1)
        q1_mb = T @ max_q2_ucb

        # Add common-transition bias prior
        q1_aug = q1_mb + b

        # Stage 1 policy
        exp_q1 = np.exp(beta * (q1_aug - np.max(q1_aug)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state with UCB and stickiness
        s = int(state[t])
        q2_s = q2_ucb[s]
        exp_q2 = np.exp(beta * (q2_s - np.max(q2_s)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage 2 learning: update mean and second moment (for variance)
        # Update mean q2[s,a2]
        delta = r - q2[s, a2]
        q2[s, a2] += alpha * delta
        # Update second moment E[r^2] with same rate (tracks nonstationarity)
        r2 = r * r
        mean_sq[s, a2] += alpha * (r2 - mean_sq[s, a2])

        # Update stickiness memory for the reached state
        last_a2_onehot[s, :] = 0.0
        last_a2_onehot[s, a2] = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)