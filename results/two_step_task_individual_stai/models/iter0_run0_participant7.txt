def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated planning weight and eligibility trace.
    
    This model mixes model-based (MB) and model-free (MF) action values at the first stage.
    The mixture weight is reduced as anxiety (stai) increases, modeling reduced reliance on planning
    under higher anxiety. A model-free eligibility trace propagates outcome prediction errors back
    to the first-stage choice. Stage-2 values are learned via TD(0).
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates at both stages.
    - beta: [0,10] inverse temperature for softmax choice at both stages.
    - w_base: [0,1] baseline weight on model-based control at stage 1 (when stai=0).
    - k_stai: [0,1] strength of anxiety modulation of model-based weight (effective w decreases with stai).
    - lam: [0,1] eligibility trace parameter for MF backpropagation to stage 1.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship on each trial (0=A, 1=U).
    - state: array of ints in {0,1}, planet reached on each trial (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on each trial (per planet).
    - reward: array of floats, obtained coins on each trial (can be negative/zero/positive).
    - stai: array-like with a single float in [0,1], participant anxiety score (used to modulate w).
    - model_parameters: tuple/list of 5 params (alpha, beta, w_base, k_stai, lam).
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w_base, k_stai, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (commonly A->X, U->Y)
    p_common = 0.7
    transition_matrix = np.array([[p_common, 1 - p_common],
                                  [1 - p_common, p_common]])

    # Value functions
    q_stage2_mf = np.zeros((2, 2))  # Q(s, a) at stage 2 (MF)
    q_stage1_mf = np.zeros(2)       # MF value for first-stage actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight
    w_eff = max(0.0, min(1.0, w_base * (1.0 - k_stai * stai)))

    for t in range(n_trials):
        # Model-based values at stage 1 from current stage-2 MF values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # per state best alien
        q_stage1_mb = transition_matrix @ max_q_stage2

        # Policy for first choice (hybrid)
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf
        q1_center = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_center)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Policy for second choice (softmax over current state's Q)
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        q2_center = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_center)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning signals
        # Stage-2 TD error and update
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update: bootstrapped from stage-2 chosen value + eligibility to outcome
        boot = q_stage2_mf[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1 + alpha * lam * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-modulated transition learning and exploration.
    
    This model learns action-specific transition probabilities online and uses them for
    model-based planning. Anxiety (stai) reduces both the effective transition learning rate
    and the exploitation level (by reducing beta), modeling less confident structure learning
    and more exploratory behavior under higher anxiety. A small MF component (via eligibility)
    complements MB planning at stage 1 with a weight tied to the same modulation to keep
    parameter count constrained.
    
    Parameters (model_parameters):
    - alpha_r: [0,1] reward learning rate for value updates at stage 2 and MF backprop.
    - beta: [0,10] base inverse temperature for softmax choices.
    - alpha_T_base: [0,1] baseline transition learning rate.
    - k_T: [0,1] strength of anxiety modulation; higher stai reduces alpha_T and beta.
    - lam: [0,1] eligibility trace parameter for MF backpropagation to stage 1.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship on each trial (0=A, 1=U).
    - state: array of ints in {0,1}, planet reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on the reached planet.
    - reward: array of floats, obtained coins on each trial.
    - stai: array-like with a single float in [0,1], participant anxiety score.
    - model_parameters: tuple/list of 5 params (alpha_r, beta, alpha_T_base, k_T, lam).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_r, beta, alpha_T_base, k_T, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model T[a, s]: P(s | a)
    T = np.full((2, 2), 0.5)  # start uncertain
    # Value functions
    q_stage2_mf = np.zeros((2, 2))
    q_stage1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    alpha_T = max(0.0, min(1.0, alpha_T_base * (1.0 - k_T * stai)))
    beta_eff = max(0.0, beta * (1.0 - 0.6 * k_T * stai))  # stronger modulation of exploration under anxiety
    # Tie MB/MF mixture to the same modulation without adding a new parameter
    w_mb = max(0.0, min(1.0, 1.0 - 0.5 * k_T * stai))  # less MB under higher anxiety
    w_mf = 1.0 - w_mb

    for t in range(n_trials):
        # Model-based values from learned transitions
        max_q2 = np.max(q_stage2_mf, axis=1)  # per state
        q_stage1_mb = T @ max_q2

        # First-stage policy: hybrid using learned T
        q1 = w_mb * q_stage1_mb + w_mf * q_stage1_mf
        q1_center = q1 - np.max(q1)
        exp_q1 = np.exp(beta_eff * q1_center)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2_mf[s].copy()
        q2_center = q2 - np.max(q2)
        exp_q2 = np.exp(beta_eff * q2_center)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward learning at stage 2
        r = reward[t]
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_r * delta2

        # MF backprop to stage 1
        boot = q_stage2_mf[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1 + alpha_r * lam * delta2

        # Transition learning for the chosen action: move probability mass toward observed state
        # T[a1, s] increases, T[a1, 1-s] decreases
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            T[a1, ss] += alpha_T * (target - T[a1, ss])
        # Keep rows normalized (they should remain so due to symmetric update, but clip for safety)
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive hybrid with anxiety-shaped utility and perseveration.
    
    Rewards are transformed by a concavity parameter to capture risk sensitivity; anxiety (stai)
    makes utility more concave (risk-averse) by interacting with the risk parameter. A perseveration
    bias favors repeating the previous first- and second-stage choices, and its strength is amplified
    by anxiety. First-stage decisions blend model-based and model-free values.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates.
    - beta: [0,10] inverse temperature for softmax choices.
    - rho: [0,1] base risk parameter; larger -> more concave utility; anxiety amplifies concavity.
    - phi: [0,1] baseline perseveration strength; anxiety increases effective perseveration.
    - w: [0,1] weight on model-based value at stage 1 (1=fully MB, 0=fully MF).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship on each trial (0=A, 1=U).
    - state: array of ints in {0,1}, planet reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on the reached planet.
    - reward: array of floats, obtained coins on each trial.
    - stai: array-like with a single float in [0,1], participant anxiety score.
    - model_parameters: tuple/list of 5 params (alpha, beta, rho, phi, w).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, rho, phi, w = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure
    p_common = 0.7
    transition_matrix = np.array([[p_common, 1 - p_common],
                                  [1 - p_common, p_common]])

    # Values
    q_stage2_mf = np.zeros((2, 2))
    q_stage1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-shaped utility curvature: gamma in (0,1], smaller = more concave
    gamma = max(1e-3, 1.0 - rho * (0.5 + 0.5 * stai))  # anxiety increases concavity
    # Anxiety-amplified perseveration strength
    phi_eff = phi * (1.0 + stai)

    # Previous choices for perseveration
    prev_a1 = None
    prev_a2_by_state = [None, None]

    def u(x):
        # Risk-sensitive utility preserving sign
        if x >= 0:
            return x ** gamma
        else:
            return -((-x) ** gamma)

    for t in range(n_trials):
        # Model-based values at stage 1
        max_q2 = np.max(q_stage2_mf, axis=1)
        q_stage1_mb = transition_matrix @ max_q2

        # Perseveration biases
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            # Map phi_eff in [0, >=1] to a bounded bias via tanh-like scaling without extra params
            stick = np.tanh(phi_eff)
            bias1[prev_a1] += stick
            bias1[1 - prev_a1] -= stick

        # First-stage policy: hybrid + perseveration
        q1 = w * q_stage1_mb + (1.0 - w) * q_stage1_mf + bias1
        q1_center = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_center)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with state-specific perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            stick2 = np.tanh(phi_eff)
            bias2[prev_a2_by_state[s]] += stick2
            bias2[1 - prev_a2_by_state[s]] -= stick2

        q2 = q_stage2_mf[s] + bias2
        q2_center = q2 - np.max(q2)
        exp_q2 = np.exp(beta * q2_center)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Utility-transformed reward
        r_util = u(reward[t])

        # Stage-2 update (on utility)
        delta2 = r_util - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Stage-1 MF update with eligibility toward utility outcome
        boot = q_stage2_mf[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        # Use same alpha and an eligibility coefficient that shares curvature influence via gamma
        lam_eff = 1.0 - gamma  # more concavity -> larger backprop
        q_stage1_mf[a1] += alpha * delta1 + alpha * lam_eff * delta2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll