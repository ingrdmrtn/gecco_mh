def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration, exploration, and perseveration.
    
    This model mixes model-based (MB) and model-free (MF) first-stage values.
    Anxiety (stai) reduces reliance on the model-based controller, increases exploration,
    and increases perseveration toward the previous first-stage choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage states (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = one of the two aliens on the visited planet).
    reward : array-like of float
        Rewards received on each trial (typically 0/1).
    stai : array-like of float
        Anxiety score(s); only stai[0] is used. Must be in [0,1].
    model_parameters : iterable of floats
        [alpha, beta, w_base, lambda_elig, kappa]
        - alpha (learning rate, [0,1])
        - beta (inverse temperature, [0,10])
        - w_base (baseline MB weight, [0,1])
        - lambda_elig (eligibility trace strength for MF credit, [0,1])
        - kappa (perseveration strength, [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w_base, lambda_elig, kappa = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q_stage1_mf = np.zeros(2)
    q_stage2_mf = np.zeros((2, 2))

    # Anxiety-modulated arbitration and exploration
    w_eff = np.clip(w_base * (1.0 - st), 0.0, 1.0)  # higher anxiety -> less MB
    beta_eff = beta * (1.0 - 0.3 * st)             # higher anxiety -> more exploration

    prev_a1 = None

    for t in range(n_trials):
        # Model-based Q at stage 1 from current MF second-stage values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)                 # value of each state
        q_stage1_mb = transition_matrix @ max_q_stage2             # plan with transitions

        # Combine MB and MF with anxiety-modulated perseveration bias
        q1 = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        bias = np.zeros(2)
        if prev_a1 is not None:
            # perseveration scaled by anxiety (stronger with higher st)
            bias[prev_a1] += kappa * st
        q1_biased = q1 + bias

        # First-stage policy
        exp_q1 = np.exp(beta_eff * (q1_biased - np.max(q1_biased)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta_eff * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning: second-stage MF update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # Learning: first-stage MF with eligibility trace to reflect outcome
        bootstrapped = q_stage2_mf[s, a2]
        td1 = (bootstrapped + lambda_elig * delta2) - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * td1

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planner with anxiety-modulated transition trust and valence-asymmetric learning.
    
    This model plans at stage 1 using a convex combination of the true transitions
    and a uniform transition, where anxiety (stai) reduces trust in the transition model.
    At stage 2, it uses valence-asymmetric reward learning rates that are themselves
    modulated by anxiety (higher anxiety -> larger negative learning rate, smaller positive).
    Q-values at stage 2 are softly decayed toward 0.5 with an anxiety-scaled rate.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions within the reached state.
    reward : array-like of float
        Trial outcomes (0/1).
    stai : array-like of float
        Anxiety score(s); only stai[0] is used. Must be in [0,1].
    model_parameters : iterable of floats
        [alpha_pos, alpha_neg, beta, rho_decay, pi_bias]
        - alpha_pos (learning rate for positive PE, [0,1])
        - alpha_neg (learning rate for negative PE, [0,1])
        - beta (inverse temperature, [0,10])
        - rho_decay (decay toward 0.5 at stage 2 per trial, [0,1])
        - pi_bias (first-stage bias strength toward spaceship A, [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, rho_decay, pi_bias = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Ground-truth transition; planner will attenuate trust based on anxiety
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage2 = np.zeros((2, 2)) + 0.5  # initialize neutral expectations

    # Anxiety-modulated trust in transition structure
    # trust=1 uses T_true; trust=0 uses uniform transitions
    trust = np.clip(1.0 - 0.5 * st, 0.0, 1.0)
    Tunif = np.ones_like(T_true) * 0.5
    T_eff = trust * T_true + (1.0 - trust) * Tunif

    # Anxiety-modulated exploration
    beta_eff = beta * (1.0 - 0.3 * st)

    # First-stage static bias toward A, scaled by anxiety
    bias_vec = np.array([+1.0, -1.0]) * (pi_bias * (1.0 + st))

    for t in range(n_trials):
        # MB planning using attenuated transitions
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = T_eff @ max_q2

        # Apply bias
        q1 = q1_mb + bias_vec

        # First-stage policy
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2[s]
        exp_q2 = np.exp(beta_eff * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 learning with anxiety-modulated valence asymmetry
        pe = r - q_stage2[s, a2]
        if pe >= 0:
            alpha_eff = alpha_pos * (1.0 - 0.5 * st)  # anxious: reduce positive updating
        else:
            alpha_eff = alpha_neg * (0.5 + 0.5 * st)  # anxious: enhance negative updating
        q_stage2[s, a2] += alpha_eff * pe

        # Soft decay of all second-stage Q toward 0.5, scaled by anxiety
        decay = rho_decay * (0.5 + 0.5 * st)
        q_stage2 = (1.0 - decay) * q_stage2 + decay * 0.5

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-sensitive outcome utility.
    
    The agent learns the transition probabilities online and uses them to plan (MB).
    First-stage choice values are a hybrid of MB and MF values. The MF system learns
    via bootstrapping from stage 2. Anxiety reduces the utility of rewards following
    rare transitions (ambiguity/violation aversion), down-weighting their impact.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float
        Outcomes (0/1).
    stai : array-like of float
        Anxiety score(s); only stai[0] is used. Must be in [0,1].
    model_parameters : iterable of floats
        [alpha_r, beta, alpha_t, w]
        - alpha_r (reward learning rate for Q2 and Q1-MF, [0,1])
        - beta (inverse temperature, [0,10])
        - alpha_t (transition learning rate, [0,1])
        - w (MB weight in first-stage hybrid, [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_r, beta, alpha_t, w = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition probabilities T_learn[a, s']
    # Start neutral: 0.5 to each state
    T_learn = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Anxiety-modulated exploration
    beta_eff = beta * (1.0 - 0.2 * st)

    # Rare-transition utility penalty strength increases with anxiety
    # Utility u = r * (1 - phi*st*rare), where phi in [0,1] is implicitly 1
    phi = 1.0

    for t in range(n_trials):
        # MB planning from learned transitions
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = T_learn @ max_q2

        # Hybrid first-stage value
        q1 = np.clip(w, 0.0, 1.0) * q1_mb + (1.0 - np.clip(w, 0.0, 1.0)) * q_stage1_mf

        # First-stage policy
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2 = q_stage2[s]
        exp_q2 = np.exp(beta_eff * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r_raw = reward[t]

        # Determine if transition was rare under the canonical mapping (A->X, U->Y)
        common_state = a1  # 0 for A->X, 1 for U->Y
        rare = 1.0 if s != common_state else 0.0

        # Anxiety-sensitive utility shaping for outcome
        r_util = r_raw * (1.0 - phi * st * rare)

        # Update second-stage values with utility-shaped reward
        pe2 = r_util - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * pe2

        # Update first-stage MF by bootstrapping from second-stage chosen value
        bootstrapped = q_stage2[s, a2]
        pe1 = bootstrapped - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * pe1

        # Update learned transitions T_learn using observed (a1 -> s)
        # Simple delta rule toward one-hot of observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T_learn[a1] += alpha_t * (target - T_learn[a1])

        # Keep rows normalized and within [0,1]
        T_learn[a1] = np.clip(T_learn[a1], 1e-6, 1.0)
        T_learn[a1] /= np.sum(T_learn[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll