Below are three standalone cognitive models tailored to the two-step task. Each function:

- Takes arrays: action_1, state, action_2, reward, stai, model_parameters
- Uses all parameters meaningfully (with bounds noted in docstrings)
- Returns the negative log-likelihood of the observed choices
- Incorporates the participantâ€™s stai score in a psychologically-motivated way
- Avoids imports (assumes numpy as np is already available)

Model overviews:
- cognitive_model1: Hybrid model-based/model-free with eligibility trace; low anxiety increases planning weight.
- cognitive_model2: Pure model-free with surprise-sensitive learning and anxiety-modulated surprise and perseveration.
- cognitive_model3: Hybrid with learned transition model; anxiety modulates both transition learning and arbitration.

Code:

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with eligibility trace; anxiety (stai) modulates MB arbitration.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices: 0=A, 1=U.
    state : array-like of int {0,1}
        Second-stage states reached: 0=planet X, 1=planet Y.
    action_2 : array-like of int {0,1}
        Second-stage choices: for X: 0=W,1=S; for Y: 0=P,1=H.
    reward : array-like of float
        Scalar rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; lower = less anxious. Used to increase planning weight when anxiety is low.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, lam, w_base, w_stai]
        - alpha (learning rate for Q-values): [0,1]
        - beta (inverse temperature): [0,10]
        - lam (eligibility trace from stage-2 PE to stage-1 MF): [0,1]
        - w_base (baseline weight on model-based values at stage 1): [0,1]
        - w_stai (how strongly low anxiety increases MB weight): [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, lam, w_base, w_stai = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])  # rows: action_1 (A,U); cols: states (X,Y)
    # Value functions
    q_stage1_mf = np.zeros(2)       # MF values for first-stage actions A,U
    q_stage2_mf = np.zeros((2, 2))  # MF values for second-stage actions in each state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Arbitration weight increases as anxiety decreases
        w_mb = w_base + w_stai * (1.0 - stai_val)
        w_mb = max(0.0, min(1.0, w_mb))

        # Model-based values for first-stage: expected max over each planet
        max_q_stage2 = np.max(q_stage2_mf, axis=1)  # size 2: [max at X, max at Y]
        q_stage1_mb = transition_matrix @ max_q_stage2  # size 2: expected values for choosing A or U

        # Hybrid at stage 1
        q1 = w_mb * q_stage1_mb + (1.0 - w_mb) * q_stage1_mf

        # Policy for stage 1
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Policy for stage 2 (softmax within realized state)
        s = state[t]
        q2 = q_stage2_mf[s]
        exp_q2 = np.exp(beta * (q2 - np.max(q2)))
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning: stage-2 TD error and update
        r = reward[t]
        pe2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * pe2

        # Backpropagate to stage-1 MF via eligibility trace using observed stage-2 value
        target1 = q_stage2_mf[s, a2]  # after update (SARSA style)
        pe1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * lam * pe1

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-sensitive model-free control with anxiety-modulated surprise and perseveration.
    
    Idea: Learning accelerates after rare transitions (surprise), with sensitivity stronger when anxiety is low.
    Perseveration bias also depends on anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices: 0=A, 1=U.
    state : array-like of int {0,1}
        Second-stage states reached: 0=planet X, 1=planet Y.
    action_2 : array-like of int {0,1}
        Second-stage choices.
    reward : array-like of float
        Rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; lower -> stronger modulation by surprise and stronger perseveration reduction.
    model_parameters : list/tuple of 5 floats
        [alpha1, alpha2, beta, pi_base, surprise_sens]
        - alpha1: [0,1] learning rate for stage-1 MF
        - alpha2: [0,1] learning rate for stage-2 MF
        - beta: [0,10] inverse temperature (both stages)
        - pi_base: [0,1] baseline perseveration strength (applied at both stages)
        - surprise_sens: [0,1] scales how much rare transitions boost alpha1 when anxiety is low
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha1, alpha2, beta, pi_base, surprise_sens = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Model-free Q-values
    q1_mf = np.zeros(2)       # for A,U
    q2_mf = np.zeros((2, 2))  # for (state, action2)

    # Perseveration memory (last chosen action)
    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Determine if transition was common or rare (A->X common, U->Y common)
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        is_rare = 1 - is_common

        # Anxiety-modulated surprise boost to stage-1 learning
        # Low anxiety -> stronger boost on rare transitions
        boost = surprise_sens * (1.0 - stai_val) * is_rare
        alpha1_eff = max(0.0, min(1.0, alpha1 * (1.0 + boost)))

        # Perseveration strength modulated by anxiety (lower anxiety -> less perseveration)
        pi = pi_base * (0.5 + 0.5 * stai_val)  # scales down when stai is low

        # Stage-1 policy with perseveration bias
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += pi
        logits1 = beta * q1_mf + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with within-state perseveration
        bias2 = np.zeros(2)
        la2 = last_a2_by_state[s]
        if la2 is not None:
            bias2[la2] += pi
        logits2 = beta * q2_mf[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning stage-2 MF
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha2 * pe2

        # Back up to stage-1 MF (one-step SARSA target is the post-update q2 of the chosen a2)
        target1 = q2_mf[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1_eff * pe1

        # Update perseveration memories
        last_a1 = a1
        last_a2_by_state[s] = a2

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid with learned transition model; anxiety modulates transition learning and MB/MF arbitration.
    
    The agent learns transition probabilities for each first-stage action and uses them
    to compute model-based values. Anxiety reduces both transition learning and MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices.
    state : array-like of int {0,1}
        Observed second-stage states.
    action_2 : array-like of int {0,1}
        Second-stage choices.
    reward : array-like of float
        Rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; higher anxiety reduces planning and transition learning.
    model_parameters : list/tuple of 5 floats
        [alpha_r, beta, w_base, omega_stai, alpha_tr]
        - alpha_r: [0,1] learning rate for second-stage rewards (MF update)
        - beta: [0,10] inverse temperature
        - w_base: [0,1] baseline MB arbitration weight
        - omega_stai: [0,1] how strongly higher anxiety reduces MB weight
        - alpha_tr: [0,1] baseline transition learning rate (for learning P(s|a1))
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_r, beta, w_base, omega_stai, alpha_tr = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Learned transition matrix P(state|action1); initialize to weak prior (common ~0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # MF value functions
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Anxiety-modulated arbitration: higher anxiety -> less MB
        w_mb = w_base * (1.0 - omega_stai * stai_val)
        w_mb = max(0.0, min(1.0, w_mb))

        # Model-based Q at stage 1 from learned transitions and stage-2 MF values
        max_q2 = np.max(q2_mf, axis=1)      # [max at X, max at Y]
        q1_mb = T @ max_q2                  # expected values for A,U

        # Hybrid first-stage values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within state
        logits2 = beta * (q2_mf[s] - np.max(q2_mf[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learn transitions: update row corresponding to chosen action toward observed state (one-hot)
        # Anxiety reduces transition learning rate
        alpha_tr_eff = alpha_tr * (1.0 - 0.75 * stai_val)
        alpha_tr_eff = max(0.0, min(1.0, alpha_tr_eff))
        target_T = np.array([0.0, 0.0])
        target_T[s] = 1.0
        T[a1] = (1.0 - alpha_tr_eff) * T[a1] + alpha_tr_eff * target_T
        # Re-normalize for numerical stability
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

        # Learn second-stage MF
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha_r * pe2

        # Update stage-1 MF toward observed post-update q2 value (simple backup)
        target1 = q2_mf[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use same reward learning rate for MF backup to keep parameter count bounded
        q1_mf[a1] += alpha_r * pe1

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))