def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning MB-MF hybrid with anxiety-driven transition forgetting and MF eligibility.
    
    Idea:
    - The agent learns its own transition model T(a->s) online and uses it for model-based values.
    - Higher anxiety increases transition forgetting (less reliance on learned structure) and reduces
      MF eligibility propagation from stage 2 to stage 1.
    - Stage 1 choices use a weighted combination of MB and MF values.
    
    Parameters (bounds):
    - alpha: [0,1] stage-2 reward learning rate
    - lambda_tr: [0,1] transition learning/retention (higher = faster learning, lower = more forgetting)
    - w0: [0,1] baseline MB weight at stage 1 before anxiety modulation
    - beta: [0,10] inverse temperature (both stages)
    
    Anxiety usage:
    - Transition learning rate: alpha_tr = lambda_tr * (1 - 0.5*stai)  # higher anxiety => slower consolidation
      plus complementary forgetting toward a uniform transition (more uncertainty when anxious).
    - MF eligibility propagation from stage 2 to stage 1 is scaled by elig = (1 - stai)  # higher anxiety => weaker MF bootstrapping
    - Arbitration weight: w_eff = clip(w0 * (1 - stai), 0, 1)  # higher anxiety => more MF at stage 1
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T with entries in {0,1} except reward in [0,1]
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, lambda_tr, w0, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, lambda_tr, w0, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Initialize learned transition model T_est[a, s], start uninformative uniform
    T_est = np.full((2, 2), 0.5)

    # Model-free values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Track choice probabilities for NLL
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    alpha_tr = np.clip(lambda_tr * (1.0 - 0.5 * stai_score), 0.0, 1.0)
    w_eff = float(np.clip(w0 * (1.0 - stai_score), 0.0, 1.0))
    elig = float(np.clip(1.0 - stai_score, 0.0, 1.0))  # MF eligibility

    for t in range(n_trials):
        # MB stage-1 values from current transition estimate and Q2
        max_Q2 = np.max(Q2, axis=1)          # best action at each state
        Q1_mb = T_est @ max_Q2               # expected max value given T_est

        # Arbitration: mix MB and MF
        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf

        # Stage-1 policy
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        Q2_s = Q2[s]
        logits2 = beta * (Q2_s - np.max(Q2_s))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model toward observed transition with anxiety-modulated forgetting
        # Target one-hot next-state given a1
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        # Move row a1 toward target with alpha_tr; softly forget toward uniform (0.5,0.5) with weight proportional to stai
        T_est[a1] = (1.0 - alpha_tr) * T_est[a1] + alpha_tr * target
        # Anxiety-driven uncertainty: shrink toward uniform
        T_est[a1] = (1.0 - 0.3 * stai_score) * T_est[a1] + 0.3 * stai_score * np.array([0.5, 0.5])

        # Keep rows normalized
        T_est[a1] = np.clip(T_est[a1], 1e-6, 1.0)
        T_est[a1] /= np.sum(T_est[a1])

        # Stage-2 MF update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Stage-1 MF bootstrapping with anxiety-scaled eligibility
        Q1_mf[a1] += (alpha * elig) * (Q2[s, a2] - Q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-free SARSA(0) with risk-sensitive utility, anxiety-modulated curvature,
    and stage-1 perseveration; inverse temperature also shifts with anxiety.
    
    Idea:
    - MF values at stage 2 learn from a risk-transformed utility u(r) = r^rho_eff.
    - Anxiety increases concavity of utility (more risk-averse on gains) and reduces softmax
      inverse temperature (more noise); stage-1 perseveration bias grows with anxiety.
    - Stage 1 learns from bootstrapped stage-2 value (standard two-step MF).
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for value updates
    - rho: [0,1] base utility curvature (0 -> very concave; 1 -> linear)
    - kappa1: [0,1] baseline stage-1 perseveration strength (added to last chosen action's logit)
    - delta_beta: [0,1] strength of anxiety impact on beta (how much beta drops/increases with stai)
    - beta: [0,10] base inverse temperature (both stages)
    
    Anxiety usage:
    - Utility curvature: rho_eff = clip(rho * (1 - 0.5*stai), 0, 1)  # more concave with higher anxiety
    - Inverse temperature: beta_eff = beta * (1 - delta_beta*stai)   # higher anxiety -> more random
    - Perseveration at stage 1: stick1 = kappa1 * (0.5 + stai)       # stronger with higher anxiety
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, rho, kappa1, delta_beta, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, rho, kappa1, delta_beta, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Values
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Perseveration memory for stage 1
    last_a1 = -1

    # Track choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated params
    rho_eff = np.clip(rho * (1.0 - 0.5 * stai_score), 0.0, 1.0)
    beta_eff = max(1e-6, beta * (1.0 - delta_beta * stai_score))
    stick1 = kappa1 * (0.5 + stai_score)

    for t in range(n_trials):
        # Stage-1 policy with perseveration bonus on previous action
        pref1 = Q1.copy()
        if last_a1 != -1:
            pref1[last_a1] += stick1
        logits1 = beta_eff * (pref1 - np.max(pref1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta_eff * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward (risk sensitivity)
        r = reward[t]
        u = (r + 1e-12) ** rho_eff

        # MF updates
        pe2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Bootstrapped update for stage 1 toward stage-2 value
        Q1[a1] += alpha * (Q2[s, a2] - Q1[a1])

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """MB-MF hybrid with directed exploration (UCB) at stage 2 and anxiety-damped exploration.
    
    Idea:
    - Stage 1 uses MB-MF arbitration with a fixed baseline weight that is attenuated by anxiety.
    - Stage 2 adds an uncertainty bonus (UCB style) that decays with visit count; anxiety reduces this bonus,
      capturing reduced directed exploration under higher anxiety.
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for MF value updates
    - bonus0: [0,1] initial uncertainty bonus scale at stage 2
    - tau_decay: [0,1] decay rate of the uncertainty bonus with visits (higher -> faster decay)
    - w_mb0: [0,1] baseline MB arbitration weight at stage 1
    - beta: [0,10] inverse temperature (both stages)
    
    Anxiety usage:
    - Stage-1 arbitration: w_eff = clip(w_mb0 * (1 - 0.5*stai), 0, 1)  # higher anxiety => more MF
    - Stage-2 exploration: bonus_eff = bonus0 * (1 - stai)             # higher anxiety => smaller UCB bonus
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, bonus0, tau_decay, w_mb0, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, bonus0, tau_decay, w_mb0, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Fixed (known) transition structure for MB planning
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Values and visit counts
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))
    N_visits = np.zeros((2, 2))  # counts for UCB bonus

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    w_eff = float(np.clip(w_mb0 * (1.0 - 0.5 * stai_score), 0.0, 1.0))
    bonus_eff = float(np.clip(bonus0 * (1.0 - stai_score), 0.0, 1.0))
    tau = float(np.clip(tau_decay, 1e-6, 1.0))

    for t in range(n_trials):
        # MB evaluation from known transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_known @ max_Q2

        # Stage-1 arbitration and policy
        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with UCB bonus
        s = state[t]
        # Uncertainty bonus decays with visits: bonus ~ bonus_eff / (1 + tau * N)
        bonus = bonus_eff / (1.0 + tau * N_visits[s])
        pref2 = Q2[s] + bonus
        logits2 = beta * (pref2 - np.max(pref2))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update MF stage-1 toward updated stage-2 value
        Q1_mf[a1] += alpha * (Q2[s, a2] - Q1_mf[a1])

        # Update visit counts after the choice
        N_visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll