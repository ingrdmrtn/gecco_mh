def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Learned transitions with anxiety-modulated surprise bonus and choice stickiness.
    
    Core idea:
    - First-stage policy is purely model-based using a learned transition matrix.
    - Anxiety increases the impact of transition surprise on first-stage preferences.
    - Also includes first-stage choice stickiness (perseveration), scaled by anxiety.
    - Second-stage policy is model-free Q-learning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; aliens).
    reward : array-like of float (0 or 1)
        Coins received each trial.
    stai : array-like of float in [0,1]
        Anxiety score used to scale surprise bonus and stickiness.
    model_parameters : list or array
        [alpha, beta, alpha_t, kappa_stick, phi_anx]
        - alpha in [0,1]: learning rate for second-stage Q-value updates.
        - beta in [0,10]: inverse temperature at both stages.
        - alpha_t in [0,1]: transition learning rate for updating the transition matrix.
        - kappa_stick in [0,1]: strength of first-stage choice perseveration.
        - phi_anx in [0,1]: scales the surprise-to-bonus mapping as a function of anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, alpha_t, kappa_stick, phi_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix with common transitions (A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: actions (A,U), cols: states (X,Y)

    # Model-free second-stage Q-values
    q2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # First-stage stickiness vector; adds bias to previously chosen action
    prev_a1 = None
    stickiness = np.zeros(2, dtype=float)

    for t in range(n_trials):
        # Compute model-based first-stage values: expected max Q2 under learned transitions
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T @ max_q2  # shape (2,)

        # Surprise bonus based on transition prediction error from previous trial (applied to current chosen action)
        # Compute current soft bonus vector; default zeros
        bonus = np.zeros(2, dtype=float)
        if t > 0:
            # Surprise of last transition under then-current T (approximate by current T for tractability)
            a1_prev = action_1[t - 1]
            s2_prev = state[t - 1]
            p_obs = T[a1_prev, s2_prev]
            surprise = 1.0 - p_obs  # higher when transition was rare/unexpected
            # Apply bonus to the action chosen this trial to capture carryover salience
            # Anxiety scales the mapping from surprise to bonus magnitude
            a1_now = action_1[t]
            bonus[a1_now] += phi_anx * stai_val * surprise

        # Update stickiness bias based on previous first-stage action
        if prev_a1 is not None:
            stickiness = np.zeros(2, dtype=float)
            # Anxiety increases perseveration strength
            stickiness[prev_a1] = kappa_stick * (1.0 + stai_val)

        # First-stage decision values with biases
        q1_eff = q1_mb + bonus + stickiness
        q1_eff -= np.max(q1_eff)  # softmax stability
        exp_q1 = np.exp(beta * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage decision
        s2 = state[t]
        q2_s = q2[s2].copy()
        q2_s -= np.max(q2_s)
        exp_q2 = np.exp(beta * q2_s)
        probs_2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Second-stage Q-learning
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # Transition learning: update the row for chosen first-stage action toward observed state
        # Simple delta rule on the two probabilities to preserve row sum 1
        # Move probability mass toward the observed state
        a1_row = T[a1]
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s2 else 0.0
            a1_row[s_idx] += alpha_t * (target - a1_row[s_idx])
        # Normalize for numerical safety
        T[a1] = a1_row / np.sum(a1_row)

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model 2: Anxiety-dependent asymmetric learning, forgetting, and entropy-based arbitration.
    
    Core idea:
    - Hybrid model-based/model-free at stage 1 with dynamic arbitration weight.
    - Arbitration weight decreases with both outcome uncertainty (entropy) and anxiety.
    - Second-stage learning is asymmetric (different for gains vs losses), with asymmetry amplified by anxiety.
    - Unchosen second-stage actions decay (forgetting) within the visited state.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; aliens).
    reward : array-like of float (0 or 1)
        Coins received each trial.
    stai : array-like of float in [0,1]
        Anxiety score modulating arbitration and learning asymmetry.
    model_parameters : list or array
        [alpha, beta, w_entropy, zeta_forget, anx_slope]
        - alpha in [0,1]: base learning rate.
        - beta in [0,10]: inverse temperature at both stages.
        - w_entropy in [0,1]: base weight for model-based control when entropy is low.
        - zeta_forget in [0,1]: forgetting rate of unchosen second-stage actions in visited state.
        - anx_slope in [0,1]: scales how anxiety increases loss-learning and decreases gain-learning and MB weight.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, w_entropy, zeta_forget, anx_slope = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed (known) transition structure for MB component
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Model-free Q-values
    q1_mf = np.zeros(2, dtype=float)      # stage-1 MF
    q2_mf = np.zeros((2, 2), dtype=float) # stage-2 MF

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Helper: binary entropy of second-stage policy per state (approx via softmax over q2_mf)
    def policy_entropy(qvec):
        qn = qvec - np.max(qvec)
        pq = np.exp(beta * qn)
        pq /= np.sum(pq)
        # entropy in [0, log(2)], normalize to [0,1]
        h = -(pq[0] * np.log(max(pq[0], 1e-12)) + pq[1] * np.log(max(pq[1], 1e-12)))
        return h / np.log(2.0)

    for t in range(n_trials):
        # Compute MB action values: expect max Q2 via transitions
        max_q2 = np.max(q2_mf, axis=1)  # size 2
        q1_mb = T @ max_q2

        # Entropy-based arbitration: higher entropy -> lower MB weight
        hX = policy_entropy(q2_mf[0])
        hY = policy_entropy(q2_mf[1])
        h_avg = 0.5 * (hX + hY)  # in [0,1]
        # Base MB weight reduced by entropy and anxiety
        w = w_entropy * (1.0 - h_avg) * (1.0 - anx_slope * stai_val)
        w = max(0.0, min(1.0, w))

        # Combine MB and MF at stage 1
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # First-stage softmax
        q1c = q1 - np.max(q1)
        p1 = np.exp(beta * q1c)
        p1 /= np.sum(p1)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage softmax
        s2 = state[t]
        q2c = q2_mf[s2] - np.max(q2_mf[s2])
        p2 = np.exp(beta * q2c)
        p2 /= np.sum(p2)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning with anxiety-dependent asymmetry at stage 2
        r = reward[t]
        pe2 = r - q2_mf[s2, a2]
        # Anxiety increases sensitivity to negative outcomes and reduces to positive ones
        alpha_plus = min(1.0, max(0.0, alpha * (1.0 - anx_slope * stai_val)))
        alpha_minus = min(1.0, max(0.0, alpha * (1.0 + anx_slope * stai_val)))
        lr = alpha_plus if pe2 >= 0.0 else alpha_minus
        q2_mf[s2, a2] += lr * pe2

        # Forgetting of the unchosen action in the visited state toward 0
        other_a2 = 1 - a2
        q2_mf[s2, other_a2] *= (1.0 - zeta_forget)

        # Stage-1 model-free update bootstrapped from observed state-action value
        pe1 = q2_mf[s2, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model 3: Uncertainty-driven exploration with anxiety-amplified UCB bonus and beta attenuation.
    
    Core idea:
    - Maintain per-alien value and uncertainty (variance-like) estimates.
    - Add an exploration bonus proportional to uncertainty (UCB-style) at stage 2.
    - Anxiety increases the exploration bonus and simultaneously reduces exploitation (effective beta).
    - Stage 1 uses model-based values computed from the expected max over uncertainty-augmented stage-2 values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; aliens).
    reward : array-like of float (0 or 1)
        Coins received each trial.
    stai : array-like of float in [0,1]
        Anxiety score that scales uncertainty bonus and dampens beta.
    model_parameters : list or array
        [alpha, beta0, tau_vol, kappa_uct, anx_gain]
        - alpha in [0,1]: learning rate for Q-value updates.
        - beta0 in [0,10]: base inverse temperature before anxiety attenuation.
        - tau_vol in [0,1]: learning rate for updating uncertainty (variance-like) from squared prediction errors.
        - kappa_uct in [0,1]: weight on the uncertainty (UCB) exploration bonus.
        - anx_gain in [0,1]: strength of anxiety's amplification of exploration and reduction of beta.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta0, tau_vol, kappa_uct, anx_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure for MB computation
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 value and uncertainty trackers
    q2 = np.zeros((2, 2), dtype=float)
    v2 = np.ones((2, 2), dtype=float) * 0.25  # initial uncertainty

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Anxiety-adjusted beta: higher anxiety -> lower beta (more stochastic)
    beta_eff = beta0 * (1.0 - 0.5 * anx_gain * stai_val)
    beta_eff = max(1e-6, beta_eff)

    for t in range(n_trials):
        # Compute uncertainty-augmented values for stage 2 (UCB bonus)
        bonus2 = kappa_uct * (1.0 + anx_gain * stai_val) * np.sqrt(np.maximum(v2, 0.0))
        q2_aug = q2 + bonus2

        # Stage 1 MB values as expectation over max augmented Q2 via transitions
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_q2_aug

        # First-stage softmax using beta_eff
        q1c = q1_mb - np.max(q1_mb)
        p1 = np.exp(beta_eff * q1c)
        p1 /= np.sum(p1)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage softmax using augmented values and beta_eff
        s2 = state[t]
        q2c = q2_aug[s2] - np.max(q2_aug[s2])
        p2 = np.exp(beta_eff * q2c)
        p2 /= np.sum(p2)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning from reward
        r = reward[t]
        pe = r - q2[s2, a2]
        # Update value
        q2[s2, a2] += alpha * pe
        # Update uncertainty (variance-like) with squared PE
        v2[s2, a2] = (1.0 - tau_vol) * v2[s2, a2] + tau_vol * (pe * pe)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)