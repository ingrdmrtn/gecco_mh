def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid RL with learned transitions and anxiety-weighted arbitration and forgetting.
    
    This model learns the first-stage transition structure online and arbitrates
    between model-based (MB) and model-free (MF) control at stage 1. The arbitration
    weight increases with transition certainty but is down-weighted by anxiety.
    Both stage-1 and stage-2 Q-values undergo anxiety-scaled forgetting.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial on the planet (0/1 for the two aliens).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0] in [0,1]. Higher indicates more anxious.
    model_parameters : array-like of float
        [alpha2, beta, alpha_T, w0, decay]
        - alpha2 in [0,1]: stage-2 MF learning rate.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - alpha_T in [0,1]: learning rate for the transition matrix.
        - w0 in [0,1]: baseline arbitration weight for MB control at stage 1.
        - decay in [0,1]: base forgetting rate applied each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha2, beta, alpha_T, w0, decay = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T[a, s], start uninformative (0.5/0.5)
    T = np.full((2, 2), 0.5)

    # Q-values
    q1_mf = np.zeros(2)        # model-free stage-1
    q2_mf = np.zeros((2, 2))   # model-free stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-scaled forgetting factor (stronger forgetting with higher anxiety)
    decay_eff = np.clip(decay * (0.5 + 0.5 * np.clip(st, 0.0, 1.0)), 0.0, 1.0)

    for t in range(n_trials):
        # Apply forgetting to both stages before acting
        q1_mf = (1.0 - decay_eff) * q1_mf
        q2_mf = (1.0 - decay_eff) * q2_mf

        # Model-based evaluation using learned transitions and current q2
        max_q2 = np.max(q2_mf, axis=1)      # value of each planet
        q1_mb = T @ max_q2                  # MB value of each spaceship

        # Certainty of transitions (0=uncertain, 1=certain). For action a: c_a=2*|T[a,0]-0.5|
        c_actions = 2.0 * np.abs(T[:, 0] - 0.5)
        certainty = np.mean(c_actions)  # overall certainty in [0,1]

        # Anxiety-weighted arbitration: more anxious (higher st) dampens MB usage when certainty < 1
        # w_eff increases with certainty, but raised to (1-st) to reduce MB under anxiety.
        w_eff = w0 * (certainty ** np.clip(1.0 - st, 0.0, 1.0))
        w_eff = np.clip(w_eff, 0.0, 1.0)

        # Stage-1 policy
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in observed state
        s = int(state[t])
        q2 = q2_mf[s]
        q2_centered = q2 - np.max(q2)
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn transitions from observed (a1 -> s)
        # Move T[a1] toward one-hot for observed s by alpha_T
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_T * (target - T[a1, sp])
        # Keep rows normalized (should already be due to update form, but ensure numerical stability)
        T[a1] = T[a1] / np.sum(T[a1])

        # Stage-2 MF update
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha2 * delta2

        # Stage-1 MF bootstrapping from realized second-stage value
        v2 = q2_mf[s, a2]
        delta1 = v2 - q1_mf[a1]
        q1_mf[a1] += alpha2 * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-free RL with anxiety-modulated exploration and perseveration, with eligibility trace.
    
    This model uses MF Q-learning at both stages. The inverse temperature is reduced
    by anxiety (more anxious participants explore more), and a choice perseveration
    bias increases with anxiety. An eligibility trace carries reward back to the
    stage-1 MF values.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial on the planet (0/1 for the two aliens).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0] in [0,1]. Higher indicates more anxious.
    model_parameters : array-like of float
        [alpha, beta_base, beta_sens, stick, lam]
        - alpha in [0,1]: learning rate for both stages.
        - beta_base in [0,10]: baseline inverse temperature.
        - beta_sens in [0,1]: fraction of beta reduction per unit anxiety (effective beta = beta_base*(1 - beta_sens*stai)).
        - stick in [0,1]: baseline perseveration strength added to the last chosen action's logit.
        - lam in [0,1]: eligibility trace from reward to stage-1 MF values.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha, beta_base, beta_sens, stick, lam = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Effective temperature and perseveration scaled by anxiety
    beta_eff = max(1e-6, beta_base * (1.0 - beta_sens * np.clip(st, 0.0, 1.0)))
    stick_eff = stick * np.clip(st, 0.0, 1.0)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    for t in range(n_trials):
        # Stage-1 preferences with perseveration
        pref1 = q1.copy()
        if last_a1 is not None:
            pref1[last_a1] += stick_eff

        pref1_centered = pref1 - np.max(pref1)
        probs1 = np.exp(beta_eff * pref1_centered)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 preferences with state-specific perseveration
        s = int(state[t])
        pref2 = q2[s].copy()
        if last_a2_by_state[s] is not None:
            pref2[last_a2_by_state[s]] += stick_eff

        pref2_centered = pref2 - np.max(pref2)
        probs2 = np.exp(beta_eff * pref2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Bootstrapped update for stage-1 value toward realized stage-2 value
        v2 = q2[s, a2]
        delta1_boot = v2 - q1[a1]
        q1[a1] += alpha * delta1_boot

        # Eligibility trace: additional update from final reward directly to stage-1
        q1[a1] += alpha * lam * (r - q1[a1])

        # Update perseveration memory
        last_a1 = a1
        last_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid RL with anxiety-weighted surprise (rare transition) sensitivity and forgetting.
    
    Uses fixed known transitions (A→X common, U→Y common). Stage-1 values combine
    model-based and model-free estimates. Rewards are reweighted on trials with rare
    transitions, with the strength of this surprise weighting increasing with anxiety.
    Includes small forgetting on Q-values.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0 = spaceship A, 1 = spaceship U).
    state : array-like of int
        Second-stage state per trial (0 = planet X, 1 = planet Y).
    action_2 : array-like of int
        Second-stage choices per trial on the planet (0/1 for the two aliens).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score(s). Uses stai[0] in [0,1]. Higher indicates more anxious.
    model_parameters : array-like of float
        [alpha, beta, omega, rare_sens, forget]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - omega in [0,1]: model-based weight at stage 1 (1 = fully MB).
        - rare_sens in [0,1]: strength of reward upweighting on rare-transition trials.
        - forget in [0,1]: forgetting rate applied each trial to Q-values.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha, beta, omega, rare_sens, forget = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition matrix reflecting common (0.7) / rare (0.3)
    T_fixed = np.array([[0.7, 0.3],  # A→(X,Y)
                        [0.3, 0.7]]) # U→(X,Y)

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective forgetting (slightly amplified by anxiety)
    forget_eff = np.clip(forget * (0.5 + 0.5 * np.clip(st, 0.0, 1.0)), 0.0, 1.0)

    for t in range(n_trials):
        # Forgetting before choice
        q1_mf = (1.0 - forget_eff) * q1_mf
        q2_mf = (1.0 - forget_eff) * q2_mf

        # MB evaluation from fixed transitions and current q2
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T_fixed @ max_q2

        # Stage-1 policy with hybrid arbitration
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2 = q2_mf[s]
        q2_centered = q2 - np.max(q2)
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Determine whether the observed transition was rare or common
        # Common: A→X or U→Y; Rare: A→Y or U→X
        common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        is_rare = 0 if common else 1

        # Anxiety-weighted surprise scaling on reward
        # On rare transitions, upweight reward prediction errors more when anxiety is higher.
        surprise_gain = 1.0 + rare_sens * (0.5 + np.clip(st, 0.0, 1.0)) * is_rare
        r_eff = r * surprise_gain

        # Stage-2 MF update with surprise-weighted reward
        delta2 = r_eff - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Stage-1 MF bootstrapping toward realized stage-2 value (also surprise-weighted)
        v2 = q2_mf[s, a2]
        delta1 = v2 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll