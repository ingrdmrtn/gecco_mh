def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-penalized hybrid with anxiety-modulated arbitration and stickiness.
    
    This model blends model-free (MF) and model-based (MB) values at stage 1, and penalizes
    first-stage actions that lead to higher transition uncertainty. Anxiety reduces the
    arbitration weight on MB control and amplifies a stickiness bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed first-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Observed second-stage choices within the encountered state.
    reward : array-like of float
        Rewards received on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Anxiety score in [0,1]; uses stai[0] as scalar.
    model_parameters : array-like of float
        Parameters with bounds:
        - alpha in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - psi in [0,1]: baseline MB arbitration weight.
        - gamma in [0,1]: weight on uncertainty penalty at stage 1.
        - kappa1 in [0,1]: baseline first-stage stickiness strength.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, psi, gamma, kappa1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure: rows are A/U, cols are X/Y
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Precompute action-wise transition entropies H(T[a])
    eps_h = 1e-12
    H = -np.sum(T * np.log(T + eps_h), axis=1)  # shape (2,)

    # Value tables
    q1_mf = np.zeros(2)     # MF values for A,U
    q2 = np.zeros((2, 2))   # Q2[state, action]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness: previous action at stage 1
    prev_a1 = 0

    # Anxiety-modulated arbitration weight (less MB with higher anxiety)
    w_eff = np.clip(psi * (1.0 - 0.6 * stai_val), 0.0, 1.0)
    # Anxiety-modulated stickiness strength
    kappa_eff = kappa1 * (1.0 + stai_val)

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based estimates from stage-2 values
        max_q2 = np.max(q2, axis=1)       # value of X and Y
        q1_mb = T @ max_q2               # MB value for A,U

        # Hybrid action values
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Bias terms for stage 1: stickiness and uncertainty penalty
        bias1 = np.zeros(2)
        bias1[prev_a1] += kappa_eff

        # Uncertainty penalty: higher entropy transitions are penalized more
        # Scaled by anxiety and gamma; we subtract as a bias term
        unc_penalty = gamma * stai_val * H
        logits1 = beta * q1_hybrid + bias1 - unc_penalty
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (no added biases here; beta shared)
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning updates
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # MF first-stage credit assignment via bootstrapped value at stage 2
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric outcome learning with anxiety-modulated exploration and transition-dependent bias.
    
    This model uses separate learning rates for rewarding vs. non-rewarding outcomes at stage 2,
    propagates MF credit to stage 1, reduces inverse temperature with anxiety, and includes a
    bias to repeat the previous first-stage action after common transitions but to switch after rare.
    The strength of this transition-dependent bias decreases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        Observed first-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Observed second-stage actions within the visited state.
    reward : array-like of float
        Rewards (0.0/1.0).
    stai : array-like of float
        Anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like of float
        Parameters with bounds:
        - a_plus in [0,1]: learning rate when reward >= 0.5.
        - a_minus in [0,1]: learning rate when reward < 0.5.
        - beta0 in [0,10]: baseline inverse temperature for softmax.
        - theta in [0,1]: outcome sensitivity scaling with anxiety.
        - chi in [0,1]: transition-dependent bias strength (reduced by anxiety).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    a_plus, a_minus, beta0, theta, chi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = 0
    prev_common = True  # initialize as common

    # Anxiety reduces effective beta (more exploration under higher anxiety)
    beta_eff = beta0 * (1.0 - 0.6 * stai_val)
    beta_eff = max(beta_eff, 1e-6)

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Transition-dependent bias on repeating vs switching first-stage choice
        # Repeat after common, switch after rare; attenuated by anxiety
        bias_strength = chi * (1.0 - stai_val) * (1.0 if prev_common else -1.0)
        bias1 = np.zeros(2)
        bias1[prev_a1] += bias_strength

        logits1 = beta_eff * q1_mf + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        logits2 = beta_eff * q2[s2]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Outcome sensitivity: anxiety reduces effective reward impact by theta
        r_eff = r * (1.0 - 0.5 * theta * stai_val)

        # Choose learning rate based on outcome valence
        alpha2 = a_plus if r >= 0.5 else a_minus

        # Update stage 2
        pe2 = r_eff - q2[s2, a2]
        q2[s2, a2] += alpha2 * pe2

        # Propagate MF credit to stage 1 using same valence-specific learning rate
        pe1 = q2[s2, a2] - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Determine whether current transition was common or rare
        # Common if (A->X) or (U->Y)
        curr_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        prev_common = curr_common
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-shaped eligibility and win/lose perseveration.
    
    This model learns the transition probabilities online and uses them for model-based
    evaluation at stage 1. Model-free values are updated at both stages, with an eligibility
    factor that increases with anxiety (more credit assignment to antecedent choices).
    Additionally, the first-stage policy includes win-stay and lose-stay biases that are
    differentially modulated by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage actions within the visited state.
    reward : array-like of float
        Rewards received.
    stai : array-like of float
        Anxiety in [0,1]; stai[0] used.
    model_parameters : array-like of float
        Parameters with bounds:
        - alpha_q in [0,1]: learning rate for Q updates (both stages).
        - beta in [0,10]: inverse temperature for softmax.
        - kappa_ws in [0,1]: win-stay bias magnitude at stage 1 (reduced by anxiety).
        - kappa_ls in [0,1]: lose-stay bias magnitude at stage 1 (increased by anxiety).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_q, beta, kappa_ws, kappa_ls = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize subjective transition model with symmetric Dirichlet(1,1) prior
    # Maintain counts for A and U to X(0)/Y(1)
    trans_counts = np.ones((2, 2), dtype=float)  # rows: action1, cols: state
    # Current estimate T_est[a] = trans_counts[a] / sum
    def get_T_est():
        row_sums = np.sum(trans_counts, axis=1, keepdims=True)
        return trans_counts / row_sums

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = 0
    prev_r = 0.0

    # Anxiety-shaped eligibility: more anxiety -> more credit assignment to stage 1
    elig = 0.3 + 0.7 * stai_val  # in [0.3, 1.0]

    for t in range(n_trials):
        s2 = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        T_est = get_T_est()

        # Model-based value via learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_est @ max_q2

        # Combine MB and MF with fixed 0.5 arbitration but tilted by anxiety via eligibility in learning
        q1_comb = 0.5 * q1_mb + 0.5 * q1_mf

        # Win/Lose stay biases: anxiety reduces win-stay, increases lose-stay
        bias_ws = kappa_ws * (1.0 - stai_val)
        bias_ls = kappa_ls * (stai_val)

        bias1 = np.zeros(2)
        if prev_r >= 0.5:
            bias1[prev_a1] += bias_ws
        else:
            bias1[prev_a1] += bias_ls

        logits1 = beta * q1_comb + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Update Q-values
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_q * pe2

        # MF stage-1 update with anxiety-shaped eligibility on the stage-2 TD error
        q1_mf[a1] += alpha_q * elig * pe2

        # Update subjective transition model with observed transition (a1 -> s2)
        trans_counts[a1, s2] += 1.0

        prev_a1 = a1
        prev_r = r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll