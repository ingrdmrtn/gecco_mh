def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MBâ€“MF with anxiety-modulated arbitration and eligibility trace.
    
    This model combines a model-free (MF) controller that learns second-stage Q-values and
    backs them up to the first stage via an eligibility trace, with a model-based (MB) controller
    that plans using the known transition structure. Anxiety (stai) shifts the arbitration weight
    toward MB or MF control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial at the visited state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration between MB and MF.
    model_parameters : list or array
        [alpha, beta, w0, k_stai, lambd]
        Bounds:
          alpha in [0,1]    : learning rate for MF values
          beta in [0,10]    : inverse temperature for softmax
          w0 in [0,1]       : baseline MB arbitration weight
          k_stai in [0,1]   : sensitivity of MB weight to STAI (positive shifts MB if stai>0.5)
          lambd in [0,1]    : eligibility trace from stage 2 to stage 1

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, w0, k_stai, lambd = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure: rows = actions (A=0, U=1), cols = states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Model-free Q-values
    q1_mf = np.zeros(2)           # first-stage MF values for A/U
    q2_mf = np.zeros((2, 2))      # second-stage MF values for states X/Y and aliens

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration toward MB
    w_eff = w0 + k_stai * (stai - 0.5)
    w_eff = np.clip(w_eff, 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based Q1 computed from current MF second-stage values (planning)
        max_q2_per_state = np.max(q2_mf, axis=1)  # best alien on each planet
        q1_mb = transition_matrix @ max_q2_per_state

        # Hybrid first-stage action values
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage 1 choice probabilities
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 choice probabilities at visited state
        q2s = q2_mf[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2 (MF)
        pe2 = reward[t] - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Eligibility-trace update to stage 1 MF for chosen a1
        q1_mf[a1] += alpha * lambd * pe2

        # Optional direct TD(0) consistency between stages for MF
        td1 = (q2_mf[s, a2] - q1_mf[a1])
        q1_mf[a1] += alpha * (1.0 - lambd) * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-based controller with anxiety-modulated risk aversion and uncertainty learning.
    
    This purely model-based controller maintains both expected value and outcome variance for each
    second-stage option. Choices maximize a risk-adjusted utility: U = Q - rho * sqrt(Var).
    Anxiety increases risk sensitivity (rho) linearly. Planning uses the known transition structure.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial at the visited state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate risk sensitivity.
    model_parameters : list or array
        [alpha, beta, rho_base, rho_stai, kappa]
        Bounds:
          alpha in [0,1]     : learning rate for mean reward (Q)
          beta in [0,10]     : inverse temperature
          rho_base in [0,1]  : baseline risk sensitivity
          rho_stai in [0,1]  : additional risk sensitivity per unit STAI
          kappa in [0,1]     : learning rate for variance (uncertainty tracking)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, rho_base, rho_stai, kappa = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Maintain mean and variance for each second-stage option
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.25  # start with moderate uncertainty for Bernoulli rewards

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated risk sensitivity
    rho = rho_base + rho_stai * stai

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Risk-adjusted utilities at stage 2
        util2 = q2_mean - rho * np.sqrt(np.maximum(q2_var, 1e-8))

        # Stage 1 MB values via planning over risk-adjusted utilities
        max_util_per_state = np.max(util2, axis=1)
        q1_mb = transition_matrix @ max_util_per_state

        # Stage 1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy at visited state (risk-adjusted)
        u2s = util2[s].copy()
        u2c = u2s - np.max(u2s)
        probs_2 = np.exp(beta * u2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning: update mean and variance of observed second-stage option
        r = reward[t]
        m_old = q2_mean[s, a2]
        q2_mean[s, a2] = m_old + alpha * (r - m_old)

        # Exponential moving average of squared error as variance proxy
        se = (r - m_old) ** 2
        q2_var[s, a2] = (1.0 - kappa) * q2_var[s, a2] + kappa * se

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with anxiety-modulated heuristic (MB-consistent) stay/shift and lapse.
    
    The controller plans using second-stage MF values and the known transition structure (MB).
    Additionally, a heuristic bias at stage 1 promotes MB-consistent stay/shift:
      - After common transitions, reward encourages staying with the previous first-stage choice;
        non-reward encourages switching.
      - After rare transitions, this pattern flips (MB signature).
    Anxiety scales the strength of this heuristic and also increases a lapse (noise) rate.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial at the visited state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (typically 0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Modulates heuristic strength and lapse.
    model_parameters : list or array
        [alpha, beta, eta_base, stai_gain, epsilon_base]
        Bounds:
          alpha in [0,1]        : learning rate for second-stage MF values used by MB planning
          beta in [0,10]        : inverse temperature
          eta_base in [0,1]     : baseline strength of heuristic stay/shift bias
          stai_gain in [0,1]    : scaling of heuristic by STAI (centered around 0.5)
          epsilon_base in [0,1] : baseline lapse rate (mixed with uniform choice)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, eta_base, stai_gain, epsilon_base = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Second-stage MF values that MB planning relies on
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_common = None
    prev_rew = None

    # Anxiety-modulated heuristic strength and lapse
    eta_eff = np.clip(eta_base + stai_gain * (stai - 0.5), 0.0, 1.0)
    epsilon_eff = np.clip(epsilon_base * (0.5 + stai), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # MB planning values at stage 1 from current q2
        max_q2_per_state = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2_per_state

        # Add heuristic bias based on previous trial outcome and transition type
        bias = np.zeros(2)
        if prev_a1 is not None:
            # MB-consistent stay/shift sign: +1 promotes staying with prev_a1, -1 promotes switching
            # After common transition: reward->stay, no reward->switch
            # After rare transition: reward->switch, no reward->stay
            sign = 0.0
            if prev_common is not None and prev_rew is not None:
                if prev_common:
                    sign = 1.0 if prev_rew > 0.0 else -1.0
                else:
                    sign = -1.0 if prev_rew > 0.0 else 1.0
            bias[prev_a1] = eta_eff * sign

        q1_aug = q1_mb + bias

        # Stage 1 softmax with lapse (mixture with uniform)
        q1c = q1_aug - np.max(q1_aug)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        probs_1 = (1.0 - epsilon_eff) * probs_1 + epsilon_eff * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage 2 softmax with lapse at visited state
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        probs_2 = (1.0 - epsilon_eff) * probs_2 + epsilon_eff * 0.5
        p_choice_2[t] = probs_2[a2]

        # Learning: update second-stage values
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Track whether the observed transition was common or rare for the chosen first-stage action
        # Common if (A->X) or (U->Y)
        prev_a1 = a1
        prev_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        prev_rew = reward[t]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll