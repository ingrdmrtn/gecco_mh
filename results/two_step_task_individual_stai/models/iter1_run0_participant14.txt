def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Pure model-free SARSA(λ) with anxiety-modulated trace persistence and choice temperature.
    
    This model treats the task as a two-step Markov decision process but learns purely model-free
    Q-values. Stage-1 Q-values are updated using a two-step SARSA(λ) target that blends the
    immediately bootstrapped second-stage value with the terminal reward. Anxiety increases both
    the eligibility trace persistence (λ) and the effective inverse temperature, capturing more
    persistent credit assignment and more deterministic choices under higher anxiety.
    
    Parameters (model_parameters):
    - alpha: learning rate for all Q-updates, in [0,1]
    - beta: base inverse temperature for both stages, in [0,10]
    - lam0: base eligibility trace parameter λ, in [0,1]
    - g_beta: anxiety gain on temperature (scales beta), in [0,1]
    - g_lam: anxiety gain on λ, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0 or 1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, lam0, g_beta, g_lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-modulated effective parameters
    beta_eff = beta * (1.0 + g_beta * max(0.0, stai - 0.31))
    lam_eff = np.clip(lam0 + g_lam * max(0.0, stai - 0.31), 0.0, 1.0)

    # Model-free Q-values
    q1 = np.zeros(2)        # stage-1 action values for spaceships [A,U]
    q2 = np.zeros((2, 2))   # stage-2 action values per planet [X,Y] x aliens [0,1]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Policy stage 1
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Policy stage 2 (conditioned on observed planet)
        s = state[t]
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Updates
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Two-step SARSA(λ) target for stage-1:
        # target1 = (1-λ)*Q2(s,a2) + λ*r
        target1 = (1.0 - lam_eff) * q2[s, a2] + lam_eff * r
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model 2: Model-based planning with risk-sensitive utility and asymmetric learning.
    
    The model uses a fixed transition structure (A→X common, U→Y common) to act in a model-based
    manner at stage 1 (computing expected values from second-stage Q-values). Stage-2 Q-values are
    updated with asymmetric learning rates for positive vs. negative prediction errors. Rewards are
    transformed by a risk/utility curvature parameter ρ. Anxiety modulates the utility curvature,
    capturing increased risk aversion or seeking in value learning and decision making.
    
    Parameters (model_parameters):
    - alpha_pos: learning rate for positive RPEs at stage 2, in [0,1]
    - alpha_neg: learning rate for negative RPEs at stage 2, in [0,1]
    - beta: inverse temperature (both stages), in [0,10]
    - rho0: base utility curvature (0=extremely concave, 1=linear), in [0,1]
    - g_rho: anxiety gain on curvature (adds to rho0), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0 or 1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_pos, alpha_neg, beta, rho0, g_rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions: rows [A,U] to cols [X,Y]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Anxiety-modulated risk curvature
    rho_eff = np.clip(rho0 + g_rho * (stai - 0.51), 0.0, 1.0)

    q2 = np.zeros((2, 2))  # second-stage values per planet/alien
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage values from current second-stage values
        max_q2 = np.max(q2, axis=1)       # best alien per planet
        q1_mb = T @ max_q2               # expected value per spaceship

        # Stage 1 policy
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy conditioned on observed planet
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward and asymmetric learning
        r = reward[t]
        u = r ** rho_eff  # in [0,1], curvature depends on anxiety

        delta2 = u - q2[s, a2]
        alpha = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model 3: Model-based planning with anxiety-sensitive aversion to previously rare transitions
    and value forgetting.
    
    The model uses fixed transitions for planning at stage 1, but adds a dynamic bias that penalizes
    repeating a spaceship if it most recently produced a rare transition. This bias decays over
    trials and is stronger under higher anxiety. Stage-2 values are learned model-free with
    forgetting (decay toward zero), capturing limited memory. The same decay also shrinks the
    rare-transition bias over time to reuse the parameter efficiently and meaningfully.
    
    Parameters (model_parameters):
    - alpha: reward learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - eta0: base strength of rare-transition aversion bias, in [0,1]
    - g_eta: anxiety gain on rare-transition aversion, in [0,1]
    - decay: forgetting/decay rate applied to Q2 and bias each trial, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0 or 1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, eta0, g_eta, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions: A commonly to X, U commonly to Y
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Anxiety-modulated rare-transition aversion strength
    eta_eff = np.clip(eta0 * (1.0 + g_eta * max(0.0, stai - 0.31)), 0.0, 2.0)

    q2 = np.zeros((2, 2))   # stage-2 values per planet/alien
    bias = np.zeros(2)      # dynamic bias on stage-1 actions [A,U]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Apply decay to values and bias
        q2 *= (1.0 - decay)
        bias *= (1.0 - decay)

        # Model-based q1 from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add dynamic rare-transition bias
        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update Q2 MF with learning rate
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update rare-transition aversion bias based on observed transition rarity
        # Common if (A->X) or (U->Y), rare otherwise
        common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        if common == 0:
            # Penalize repeating the action that just yielded a rare transition
            bias[a1] -= eta_eff
        else:
            # Small rebound toward zero for common transitions (already handled by decay implicitly)

            # Optionally provide a tiny positive nudge to counterbalance decay; keep it zero to
            # avoid adding extra effective parameters.

            pass

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll