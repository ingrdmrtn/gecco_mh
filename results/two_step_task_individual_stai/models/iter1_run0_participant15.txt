def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with learned transition model and anxiety-damped information-seeking.
    
    Idea:
    - Stage-2 values (Q2) are learned model-free from rewards.
    - A transition model T is learned with its own learning rate (alpha_t).
    - Stage-1 values are a mixture of model-based planning via T and model-free bootstrap from Q2.
    - An exploration bonus adds the entropy of the predicted next-state distribution to the MB values.
    - Anxiety decreases both the MB weight and the strength of the exploration bonus.
    
    Parameters (all used; total=5):
    - alpha_r: [0,1] Reward learning rate for Stage-2 values.
    - beta:   [0,10] Inverse temperature for both stages.
    - alpha_t:[0,1] Learning rate for the transition model.
    - w_mb0:  [0,1] Baseline weight on model-based value at Stage-1 (before anxiety).
    - kappa:  [0,1] Coefficient for entropy-based exploration bonus at Stage-1.
    
    Anxiety use:
    - Effective MB weight: w_mb = w_mb0 * (1 - stai).
    - Exploration bonus: bonus_scale = kappa * (1 - stai).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1 (0=A, 1=U).
    - state:    array of ints in {0,1}, reached planet at Stage-2 (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on that planet (0 or 1).
    - reward:   array of floats in [0,1], received coins.
    - stai:     array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [alpha_r, beta, alpha_t, w_mb0, kappa].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha_r, beta, alpha_t, w_mb0, kappa = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition model T(a, s') with weak bias toward 0.5-0.5
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Stage-2 Q-values: Q2[state, action]
    Q2 = 0.5 * np.ones((2, 2), dtype=float)
    # Stage-1 MF values: Q1_mf[action]
    Q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Effective weights modulated by anxiety
    w_mb = w_mb0 * (1.0 - st)
    bonus_scale = kappa * (1.0 - st)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB value at Stage-1 via current learned transitions and Stage-2 max values
        maxQ2 = np.max(Q2, axis=1)  # shape (2,)
        V_mb = T @ maxQ2  # shape (2,)

        # Entropy bonus of next-state prediction for each Stage-1 action
        # H(p) = - sum p log p; encourage actions with uncertain transitions (exploration)
        H = np.zeros(2, dtype=float)
        for a in range(2):
            p = T[a]
            # numerically stable entropy
            p_clip = np.clip(p, 1e-12, 1.0)
            H[a] = -np.sum(p_clip * np.log(p_clip))
        V_mb_bonus = V_mb + bonus_scale * H

        # Blend MB with MF at Stage-1
        Q1 = w_mb * V_mb_bonus + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        z1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning: update Q2 (model-free) from reward
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * pe2

        # Update Stage-1 MF by bootstrapping from Stage-2 value (TD)
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_r * pe1

        # Update transition model from observed transition (a1 -> s)
        # Move T[a1] toward one-hot of observed state s
        oh = np.array([0.0, 0.0])
        oh[s] = 1.0
        T[a1] = (1.0 - alpha_t) * T[a1] + alpha_t * oh

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-free with eligibility trace, anxiety-scaled outcome sensitivity,
    and anxiety-amplified Stage-2 perseveration.
    
    Idea:
    - Both stages are learned via model-free temporal-difference updates.
    - Eligibility trace (gamma_e) credits Stage-1 action with Stage-2 prediction error.
    - Outcome sensitivity is reduced by anxiety via xi, damping learning from both gains and losses.
    - Perseveration at Stage-2 biases repeating the previous Stage-2 action, amplified by anxiety.
    - Stage-1 choice is also model-free (Q1), with its own softmax; anxiety can slightly reduce its precision.
    
    Parameters (all used; total=5):
    - alpha:  [0,1] Base learning rate for value updates.
    - beta:   [0,10] Base inverse temperature.
    - gamma_e:[0,1] Eligibility trace strength propagating Stage-2 PE to Stage-1.
    - psi:    [0,1] Perseveration strength at Stage-2 (amplified by anxiety).
    - xi:     [0,1] Anxiety scaling of outcome sensitivity and Stage-1 precision.
    
    Anxiety use:
    - Outcome sensitivity factor: k_out = 1 - xi * stai, scaling the TD-errors used for learning.
    - Stage-2 perseveration bias magnitude: stick2 = psi * stai.
    - Stage-1 inverse temperature: beta1 = beta * (1 - 0.5 * xi * stai); Stage-2 uses beta.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1 (0=A, 1=U).
    - state:    array of ints in {0,1}, reached planet at Stage-2 (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on that planet (0 or 1).
    - reward:   array of floats in [0,1], received coins.
    - stai:     array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [alpha, beta, gamma_e, psi, xi].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha, beta, gamma_e, psi, xi = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Values
    Q2 = 0.5 * np.ones((2, 2), dtype=float)  # Q2[state, action]
    Q1 = np.zeros(2, dtype=float)            # Q1[action]

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Anxiety effects
    k_out = 1.0 - xi * st
    k_out = max(0.0, min(1.0, k_out))
    stick2 = psi * st
    beta1 = beta * (1.0 - 0.5 * xi * st)

    prev_a2 = None

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 policy (model-free)
        z1 = beta1 * (Q1 - np.max(Q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration bias
        bias2 = np.zeros(2, dtype=float)
        if prev_a2 is not None:
            bias2[prev_a2] = stick2

        z2 = beta * (Q2[s] + bias2 - np.max(Q2[s] + bias2))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * k_out * pe2

        # Eligibility trace to update Stage-1 from Stage-2 PE
        Q1[a1] += alpha * gamma_e * k_out * pe2

        prev_a2 = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-weighted MB/MF arbitration with anxiety bias and optimistic priors.
    
    Idea:
    - Stage-2 values (Q2) are learned model-free with learning rate alpha.
    - Each state-action also tracks an uncertainty U2 (running absolute PE), updated with alpha_u.
    - Model-based value at Stage-1 comes from a fixed transition model (0.7/0.3).
    - Arbitration weight for MB at Stage-1 decreases with:
        (a) higher predicted successor uncertainty, and
        (b) higher anxiety.
      Thus, w_mb = arb * (1 - predicted_uncertainty) * (1 - stai).
    - Q2 initialized with optimism q0.
    - Stage-1 MF values (Q1_mf) are updated via TD bootstrapping from Stage-2.
    
    Parameters (all used; total=5):
    - alpha:  [0,1] Learning rate for Q2 and Stage-1 MF bootstrap.
    - beta:   [0,10] Inverse temperature for both stages.
    - arb:    [0,1] Overall strength of uncertainty-based arbitration.
    - q0:     [0,1] Optimistic prior for all Stage-2 Q-values.
    - alpha_u:[0,1] Learning rate for uncertainty (U2) updates.
    
    Anxiety use:
    - Effective MB weight: w_mb = arb * (1 - stai) * (1 - U_pred[a]) for each Stage-1 action a.
      Here U_pred[a] is the transition-weighted successor uncertainty.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1 (0=A, 1=U).
    - state:    array of ints in {0,1}, reached planet at Stage-2 (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien on that planet (0 or 1).
    - reward:   array of floats in [0,1], received coins.
    - stai:     array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [alpha, beta, arb, q0, alpha_u].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha, beta, arb, q0, alpha_u = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition model (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Initialize optimistic Q2 and zero-mean uncertainties
    Q2 = q0 * np.ones((2, 2), dtype=float)  # Q2[state, action]
    U2 = np.zeros((2, 2), dtype=float)      # running abs PE per state-action
    Q1_mf = np.zeros(2, dtype=float)        # Stage-1 MF values

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based value via transitions and current Q2
        maxQ2 = np.max(Q2, axis=1)  # value for each successor state
        V_mb = T @ maxQ2            # per action at Stage-1

        # Predicted successor uncertainty for each Stage-1 action
        # Define state uncertainty as the uncertainty of its best action
        u_state = np.max(U2, axis=1)  # uncertainty of best option in each state
        U_pred = T @ u_state          # per Stage-1 action

        # Arbitration weight per action: higher uncertainty and higher anxiety -> lower MB weight
        w_mb = arb * (1.0 - st) * (1.0 - U_pred)
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # Final Stage-1 values: action-wise arbitration between MB and MF
        Q1 = w_mb * V_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        z1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning Stage-2 values and uncertainty
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update uncertainty as an exponential moving average of |PE|
        U2[s, a2] = (1.0 - alpha_u) * U2[s, a2] + alpha_u * abs(pe2)
        # Small decay for other entries to avoid stale high uncertainty
        for ss in range(2):
            for aa in range(2):
                if not (ss == s and aa == a2):
                    U2[ss, aa] = (1.0 - 0.1 * alpha_u) * U2[ss, aa]

        # Update Stage-1 MF by bootstrapping from Stage-2 chosen value
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll